{"raw_generations": {"sample_427": ["    def test_formset_error_messages(self):\n        class CustomFormSet(BaseFormSet):\n            error_messages = {\"required\": \"This field is required (custom message)\"}\n            \n        data = {\n            \"choices-TOTAL_FORMS\": \"2\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"\",\n            \"choices-0-votes\": \"1\",\n            \"choices-1-choice\": \"One\",\n            \"choices-1-votes\": \"\",\n        }\n        formset = CustomFormSet(data, auto_id=False, prefix=\"choices\")\n        self.assertFalse(formset.is_valid())\n        self.assertIn(\"This field is required (custom message)\", formset.errors)  \n", "    def test_formset_can_have_custom_templates(self):\n        class CustomTemplatesRenderer:\n                return f'<div>{form.as_p()}</div>'\n\n        ChoiceFormSet = formset_factory(Choice, renderer=CustomTemplatesRenderer())\n        formset = ChoiceFormSet()\n        self.assertEqual(formset.template_name, None)\n        self.assertEqual(str(formset), '<div>Choice: <input type=\"text\" name=\"choices-0-choice\" value=\"\"></div>')\n\n", "    def test_formset__html_safe_with_template_name(self):\n        from django.forms.renderers import Jinja2\n        ChoiceFormSet = formset_factory(Choice, renderer=Jinja2)\n        formset = ChoiceFormSet(data={\"choices-TOTAL_FORMS\": \"1\"}, prefix=\"choices\")\n        formset.template_name = \"my_template\"\n        self.assertTrue(hasattr(formset, \"__html__\"))\n        self.assertEqual(str(formset), formset.__html__())\n\n\n", "    def test_formset_widget_overrides_are_respected(self):\n        class CustomTextInput(TextInput):\n            pass\n\n        form = ChoiceForm()\n        form.fields['choice'].widget = CustomTextInput()\n        formset = formset_factory(form, extra=2)\n        rendered_formset = formset().as_p()\n        self.assertIn('widget=\"CustomTextInput\"', rendered_formset) \n", "    def test_error_messages_updated(self):\n        data = {\n            \"choices-TOTAL_FORMS\": \"2\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"Zero\",\n            \"choices-0-votes\": \"\",\n            \"choices-1-choice\": \"One\",\n            \"choices-1-votes\": \"\",\n        }\n        ChoiceForm = Form\n        class ChoiceFormSet(BaseFormSet):\n            error_messages = {\"required\": \"This field is required.\"}\n        ChoiceFormSet = formset_factory(ChoiceForm, error_messages={'required': 'This field is required.'})\n        formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(formset.errors, [{\"votes\": [\"This field is required.\"] * 2}])\n"], "sample_900": ["compilation error", "def test_early_stopping_with_validation_fraction():\n    X = X_digits_binary[:200]\n    y = y_digits_binary[:200]\n    \n    # Test early stopping with validation fraction\n    for val_fraction in [0.2, 0.3, 0.5]:\n        clf = MLPClassifier(tol=0.01, max_iter=1000, solver='sgd',\n                            early_stopping=True, validation_fraction=val_fraction,\n                            random_state=1)\n        clf.fit(X, y)\n        assert clf.n_iter_ < 1000\n        assert clf.best_validation_score_ > 0.9\n\n\n", "compilation error", "compilation error", "compilation error"], "sample_708": ["def test_leading_whitespace() -> None:\n    source = Source(\"\"\"\n         pass\n    \"\"\")\n    assert str(source) == \"def f():\\n    pass\"\n\n\n    source = Source(\"\"\"\n         pass\n     \"\"\")\n    assert str(source) == \"def f():\\n    pass\"\n", "def test_multiline_string_as_statement() -> None:\n    source = \"\"\"\\", "  def f(a=1, b=2, c=3):\n        print(a+b+c)\n    \"\"\"\n    source = getstatement(0, source)\n    assert \"def f(a=1, b=2, c=3):\" in str(source)\n", "    def test_trailing_comment_block() -> None:\n        source = \"\"\"\\", "    def test_trailing_comments() -> None:\n        s = \"\"\"\\"], "sample_1039": ["compilation error", "def test_presentation_mathml_matrix_size():\n    A = MatrixSymbol('A', 2, 3)\n    B = MatrixSymbol('B', 4, 4)\n    assert mpp._print(A).childNodes[0].childNodes[0].tagName == 'mfenced'\n    assert mpp._print(A).childNodes[0].childNodes[1].childNodes[0].nodeValue == '2'\n    assert mpp._print(A).childNodes[0].childNodes[1].childNodes[1].nodeValue == '3'\n    assert mp._print(B).childNodes[0].childNodes[0].tagName == 'mfenced'\n    assert mp._print(B).childNodes[0].childNodes[1].childNodes[0].nodeValue == '$4$'\n    assert mp._print(B).childNodes[0].childNodes[1].childNodes[1].nodeValue == '$4$' \n", "def test_print_matrix_element():\n    A = MatrixSymbol('A', 2, 2)\n    A[0, 0] = x\n    A[1, 1] = y\n    assert mpp._print(A) == '<mtable><mtr><mtd><mi>x</mi></mtd><mtd><mi></mi></mtd></mtr><mtr><mtd><mi></mi></mtd><mtd><mi>y</mi></mtd></mtr></mtable>'\n    assert mp._print(A) == '<matrix><apply><ci>A</ci><cn>0</cn><cn>0</cn></apply><apply><ci>A</ci><cn>1</cn><cn>1</cn></apply></matrix>'\n", "def test_print_matrix():\n    A = Matrix([[1, 2], [3, 4]])\n    assert mpp.doprint(A) == '<mtable><mtr><mtd><mn>1</mn></mtd><mtd><mn>2</mn></mtd></mtr><mtr><mtd><mn>3</mn></mtd><mtd><mn>4</mn></mtd></mtr></mtable>'\n    assert mp.doprint(A) == '<matrix><row><cn>1</cn><cn>2</cn></row><row><cn>3</cn><cn>4</cn></row></matrix>'\n", "compilation error"], "sample_754": ["def test_callback_mini_batch(norm_comp):\n        n_calls += 1\n        if n_iter == 10:\n            assert_true(n_calls == 10)\n\n    n_calls = 0\n    alpha = 1\n    rng = np.random.RandomState(0)\n    Y, _, _ = generate_toy_data(3, 10, (8, 8), random_state=rng)\n    pca = MiniBatchSparsePCA(n_components=3, n_jobs=1, random_state=0,\n                                   alpha=alpha, callback=callback_counter,\n                                   normalize_components=norm_comp)\n    pca.fit(Y)\n", "def test_sparsepca_with_small_alpha():\n    rng = np.random.RandomState(0)\n    X = rng.randn(12, 10)\n    spca = SparsePCA(n_components=8, alpha=0.001, random_state=rng)\n    U = spca.fit_transform(X)\n    assert_true(np.sum(spca.components_ == 0) < 8)\n", "def test_dictionary_learning_consistency(norm_comp):\n    n_components = 5\n    n_samples = 100\n    n_features = 10\n    random_state = 0\n    alpha = 0.1\n\n    rng = np.random.RandomState(random_state)\n    X = rng.rand(n_samples, n_features)\n    \n    # Fit SparsePCA\n    spca = SparsePCA(n_components=n_components, alpha=alpha,\n                      random_state=random_state,\n                      normalize_components=norm_comp)\n    spca.fit(X)\n    spca_components = spca.components_\n\n    # Fit dictionary learning with the same components\n    dict_params = {'n_components': n_components, 'alpha': alpha,\n                   'method': 'lars', 'random_state': random_state}\n\n    dict_learning = dict_learning(X.T, **dict_params)\n    dict_components = dict_learning.components_\n\n    assert_array_almost_equal(spca_components, dict_components)\n\n\n\n", "def test_sparse_components_are_sparse(norm_comp):\n    rng = np.random.RandomState(0)\n    X = rng.randn(12, 10)\n    spca = SparsePCA(n_components=8, alpha=1, random_state=rng,\n                     normalize_components=norm_comp)\n    spca.fit(X)\n    components = spca.components_\n    non_zero_values = np.count_nonzero(components)\n    assert_true(non_zero_values < components.size * 0.5)\n", "def test_sparse_components_positive_variance(norm_comp):\n    rng = np.random.RandomState(0)\n    X = rng.randn(100, 10)\n    spca = SparsePCA(n_components=10, alpha=0.1, random_state=rng,\n                     normalize_components=norm_comp)\n    spca.fit(X)\n    for i in range(spca.n_components_):\n        assert_true(np.all(spca.components_[i, :] != 0))  \n"], "sample_168": ["    def test_include_stale_apps_true(self):\n        \"\"\"\n        `--include-stale-apps=true` deletes content types from removed apps.\n        \"\"\"\n        ContentType.objects.create(app_label='empty_models', model='Fake 1')\n        with self.assertNumQueries(0):\n            call_command('remove_stale_contenttypes', verbosity=2, include_stale_apps=True)\n        self.assertEqual(ContentType.objects.count(), self.before_count + 1)\n", "    def test_include_stale_apps_true(self):\n        \"\"\"\n        `--include-stale-apps` includes content types from removed apps.\n        \"\"\"\n        ContentType.objects.create(app_label='empty_models', model='Fake')\n        with mock.patch('builtins.input', return_value='yes'):\n            with captured_stdout() as stdout:\n                call_command(\n                    'remove_stale_contenttypes',\n                    verbosity=2,\n                    include_stale_apps=True,\n                )\n        self.assertIn(\n            \"Deleting stale content type 'empty_models | Fake'\",\n            stdout.getvalue(),\n        )\n        self.assertEqual(ContentType.objects.count(), self.before_count)\n", "    def test_include_stale_apps_true(self):\n        \"\"\"\n        When include_stale_apps is True, stale content types from uninstalled apps are deleted.\n        \"\"\"\n        ContentType.objects.create(app_label='empty_models', model='Fake')\n\n        with mock.patch('builtins.input', return_value='yes'):\n            with captured_stdout() as stdout:\n                call_command('remove_stale_contenttypes', verbosity=2, include_stale_apps=True)\n        self.assertIn('Deleting stale content type', stdout.getvalue())\n        self.assertEqual(ContentType.objects.count(), self.before_count + 1)\n\n", "    def test_include_stale_apps_true(self):\n        \"\"\"\n        --include-stale-apps=True deletes stale content types from\n        previously installed apps.\n        \"\"\"\n        ContentType.objects.create(app_label='empty_models', model='Fake 1')\n        with mock.patch('builtins.input', return_value='yes'):\n            with captured_stdout() as stdout:\n                call_command('remove_stale_contenttypes', verbosity=2, include_stale_apps=True)\n        self.assertIn(\n            'Deleting stale content type \\'empty_models | Fake 1\\'',\n            stdout.getvalue(),\n        )\n        self.assertEqual(ContentType.objects.count(), self.before_count + 1)\n", "    def test_include_stale_apps_true(self):\n        ContentType.objects.create(app_label='empty_models', model='Fake 1')\n        with mock.patch('builtins.input', return_value='yes'), captured_stdout() as stdout:\n            call_command('remove_stale_contenttypes', verbosity=2, include_stale_apps=True)\n        self.assertIn(\"Deleting stale content type 'empty_models | Fake 1'\", stdout.getvalue())\n        self.assertEqual(ContentType.objects.count(), self.before_count + 1)\n\n\n"], "sample_92": ["    def test_authenticate_with_username_only(self):\n        self.assertFalse(self.user.is_active)\n        self.assertEqual(authenticate(username=self.user.username), self.user)\n", "    def test_authenticate_twice_fails(self):\n        user = authenticate(**self.user_credentials)\n        self.assertEqual(user, self.user)\n        with self.assertRaises(AuthenticationError):\n            authenticate(**self.user_credentials)\n\n\n", "    def test_inactive_user_login_status(self):\n        self.user = User.objects.create_user(\n            email='test@example.com', is_active=False,\n            **self.user_credentials\n        )\n        authenticate(**self.user_credentials)  \n        self.assertFalse(self.user.is_active) \n        self.assertEqual(self.user.backend, 'django.contrib.auth.backends.AllowAllUsersModelBackend')   \n", "    def test_anonymous_user(self):\n        self.assertIs(authenticate(username='anon', password=''), AnonymousUser())\n", "    def test_password_hashing(self):\n        self.user.set_password('new_password')\n        self.user.save()\n        \n        auth_backend = AllowAllUsersModelBackend()\n        user = auth_backend.authenticate(\n            username=self.user.username, password='new_password'\n        )\n\n        self.assertIs(user, self.user)\n        self.assertTrue(check_password_hash(self.user.password, 'new_password'))\n\n\n\n"], "sample_580": ["compilation error", "def test_categorical_order():\n    s = pd.Series([\"a\", \"b\", \"c\", \"a\"])\n    assert categorical_order(s) == [\"a\", \"b\", \"c\"]\n\n    s = pd.Series([\"b\", \"a\", \"c\", \"b\"])\n    assert categorical_order(s) == [\"a\", \"b\", \"c\"]\n\n    s = pd.Series([\"cat1\", \"cat2\", \"cat1\", \"cat3\"])\n    assert categorical_order(s) == [\"cat1\", \"cat2\", \"cat3\"]\n\n    s = pd.Series([1, 2, 3, 1])\n    assert categorical_order(s) == [1, 2, 3]\n\n    s = pd.Series([1, 2, np.nan, 1])\n    assert categorical_order(s) == [1, 2, np.nan]\n\n    s = pd.Series([\"A\", \"B\", \"C\"])\n    assert categorical_order(s) == [\"A\", \"B\", \"C\"]\n\n    s = pd.Series([\"a\", 1, \"b\", 2])\n    assert categorical_order(s) == [\"a\", 1, \"b\", 2]\n\n    s = pd.Series([\"A\", 1, \"B\", 2])\n    assert categorical_order(s) == [\"A\", 1, \"B\", 2]\n", "compilation error", "compilation error", "def test_categorical_order():\n    s = pd.Series([\"A\", \"B\", \"C\", \"A\", \"B\"])\n    assert categorical_order(s) == [\"A\", \"B\", \"C\"]\n\n    s = pd.Series([\"A\", \"B\", \"C\", \"A\", \"B\"], dtype=\"category\")\n    assert categorical_order(s) == [\"A\", \"B\", \"C\"]\n\n    s = pd.Series([1, 2, 3, 1, 2])\n    assert categorical_order(s) == [1, 2, 3]\n\n    s = pd.Series([1, 2, 3, 1, 2], dtype=\"category\")\n    assert categorical_order(s) == [1, 2, 3]\n\n    s = pd.Series([np.nan, np.nan, np.nan])\n    assert categorical_order(s) == []\n\n    s = pd.Series([np.nan, 1, 2, np.nan])\n    assert categorical_order(s) == [1, 2]\n"], "sample_660": ["    def test_skip_reason_with_newline(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n            @pytest.mark.skip(reason='Line 1\\nLine 2')\n                pass\n            \"\"\"\n        )\n        _, dom = runandparse(testdir)\n        node = dom.find_first_by_tag(\"testcase\")\n        snode = node.find_first_by_tag(\"skipped\")\n        assert \"Line 1\\nLine 2\" in snode.text\n        snode.assert_attr(message=\"Line 1\\nLine 2\")\n", "def test_record_property_unicode(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n            record_property(\"foo\", \"\u4f60\u597d\\\\U0001F600\")\n    \"\"\"\n    )\n    result, dom = runandparse(testdir, \"-rw\")\n    node = dom.find_first_by_tag(\"testsuite\")\n    tnode = node.find_first_by_tag(\"testcase\")\n    psnode = tnode.find_first_by_tag(\"properties\")\n    pnodes = psnode.find_by_tag(\"property\")\n    pnodes[0].assert_attr(name=\"foo\", value=\"\u4f60\u597d\\U0001F600\")\n", "    def test_escaped_skip_reason_issue3533_newlines(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n            @pytest.mark.skip(reason='line 1\\nline 2')\n                pass\n        \"\"\"\n        )\n        _, dom = runandparse(testdir)\n        node = dom.find_first_by_tag(\"testcase\")\n        snode = node.find_first_by_tag(\"skipped\")\n        assert \"line 1\\nline 2\" in snode.text\n        snode.assert_attr(message=\"line 1\\nline 2\")\n", "def test_skip_reason_unicode(testdir, reason):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.skip(reason=reason)\n            pass\n    \"\"\"\n    )\n    _, dom = runandparse(testdir)\n    node = dom.find_first_by_tag(\"testcase\")\n    snode = node.find_first_by_tag(\"skipped\")\n    assert reason in snode.text\n    snode.assert_attr(message=reason)\n", "    def test_record_testsuite_property_duplicate_names(testdir):\n        testdir.makepyfile(\n            \"\"\"\n                record_testsuite_property(\"foo\", \"bar1\")\n                record_testsuite_property(\"foo\", \"bar2\")\n            \"\"\"\n        )\n        result, dom = runandparse(testdir)\n        assert result.ret == 0\n        node = dom.find_first_by_tag(\"testsuite\")\n        properties_node = node.find_first_by_tag(\"properties\")\n        p1_node = properties_node.find_nth_by_tag(\"property\", 0)\n        p2_node = properties_node.find_nth_by_tag(\"property\", 1)\n        p1_node.assert_attr(name=\"foo\", value=\"bar1\")\n        p2_node.assert_attr(name=\"foo\", value=\"bar2\")\n\n"], "sample_452": ["    def test_references_field_by_m2m_through(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ManyToManyField(\n                \"Other\", through=\"Through\", related_name=\"related_field\"\n            ),\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"whatever\", \"migrations\"), True\n        )\n        self.assertIs(\n            operation.references_field(\"Through\", \"whatever\", \"migrations\"), True\n        )\n        self.assertIs(\n            operation.references_field(\"Missing\", \"whatever\", \"migrations\"), False\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"related_field\", \"migrations\"), True\n        )\n", "    def test_references_field_by_related_name(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ForeignKey(\"Other\", models.CASCADE, related_name=\"related_field\"),\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"related_field\", \"migrations\"),\n            True,\n        )\n        self.assertIs(operation.references_field(\"Other\", \"whatever\", \"migrations\"), False)\n", "    def test_references_field_by_through_with_fields(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ManyToManyField(\"Other\", through=\"Through\", through_fields=(\"through_field\",)),\n        )\n        self.assertIs(\n            operation.references_field(\"Through\", \"through_field\", \"migrations\"), True\n        )\n", "    def test_references_field_by_one_to_many(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.fields.related.ManyToOneRel(\n                to=\"Other\",\n                field=models.ForeignKey(\"Other\", models.CASCADE),\n            ),\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"id\", \"migrations\"), True\n        )\n        self.assertIs(operation.references_field(\"Model\", \"field\", \"migrations\"), False)\n\n\n\n", "    def test_references_field_by_through_with_extra_args(self):\n        operation = FieldOperation(\n            \"Model\", \"field\", models.ManyToManyField(\"Other\", through=\"Through\", db_constraint=False)\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"whatever\", \"migrations\"), True\n        )\n        self.assertIs(operation.references_field(\"Through\", \"whatever\", \"migrations\"), True)\n        self.assertIs(\n            operation.references_field(\"Missing\", \"whatever\", \"migrations\"), False\n        )\n"], "sample_646": ["def test_do_class_cleanups_on_exception_during_test(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n        class MyTestCase(unittest.TestCase):\n            values = []\n            @classmethod\n                    cls.values.append(1)\n                cls.addClassCleanup(cleanup)\n                raise Exception(\"intentional\")\n                pass\n            assert MyTestCase.values == [1]\n    \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    passed, skipped, failed = reprec.countoutcomes()\n    assert failed == 1\n    assert passed == 1\n", "    def test_do_class_cleanups_after_test_method_failure(pytester: Pytester) -> None:\n        testpath = pytester.makepyfile(\n            \"\"\"\n            import unittest\n            class MyTestCase(unittest.TestCase):\n                values = []\n                @classmethod\n                        cls.values.append(1)\n                    cls.addClassCleanup(cleanup)\n                    assert False\n                    pass\n                assert MyTestCase.values == [1]\n            \"\"\"\n        )\n        reprec = pytester.inline_run(testpath)\n        passed, skipped, failed = reprec.countoutcomes()\n        assert failed == 1\n        assert passed == 1\n\n\n\n", "    def test_traceback_pruning_with_multi_instance(pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import unittest\n            import pdb\n\n            class MyTestCase(unittest.TestCase):\n\n                    pdb.set_trace()\n                \n                    pass\n\n            class TestIt(MyTestCase):\n                    pass\n\n            class TestTwo(BaseTestCase):\n                    pass\n\n            \"\"\"\n        )\n        reprec = pytester.inline_run()\n        passed, skipped, failed = reprec.countoutcomes()\n        assert passed == 3\n        assert failed == 0\n        assert reprec.ret == 0\n", "def test_do_cleanups_with_yield(pytester: PyTester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n        class MyTestCase(unittest.TestCase):\n            values = []\n                    self.values.append(1)\n                self.addCleanup(cleanup)\n                yield\n                yield\n            instance = MyTestCase()\n            instance.test_one()\n            assert instance.values == [1]\n            instance = MyTestCase()\n            instance.test_two()\n            assert instance.values == [1]\n\n    \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    passed, skipped, failed = reprec.countoutcomes()\n    assert failed == 0\n    assert passed == 2\n", "    def test_remove_duplicated_test_names(pytester: Pytester) -> None:\n        # Issue: https://github.com/pytest-dev/pytest/issues/11679\n        pytester.makepyfile(\n            \"\"\"\n            import unittest\n\n            class TestSameName(unittest.TestCase):\n                    pass\n\n            class TestDifferentName(unittest.TestCase):\n                    pass\n            \"\"\"\n        )\n        result = pytester.runpytest()\n        result.stdout.fnmatch_lines(\n            [\n                \"*2 passed*\",\n            ]\n        )\n"], "sample_343": ["    def test_get_prefetch_queryset_with_related_manager(self):\n        question = Question.objects.create(text='Who?')\n        answer1 = Answer.objects.create(question=question, text='Who knows?')\n        answer2 = Answer.objects.create(question=question, text='Nobody knows.')\n        post = Post.objects.create(title='Answer', parent=question)\n\n        answers = Answer.objects.filter(question=question)\n        prefetch_querysets = list(answer1.question.get_prefetch_queryset(answers))\n        self.assertEqual(len(prefetch_querysets), 1)\n        self.assertEqual(prefetch_querysets[0][0], (question.pk, question.pk))\n        self.assertEqual([obj.text for obj in prefetch_querysets[0][1]], [answer1.text, answer2.text])\n\n\n\n", "    def test_get_related_objects_after_deletion(self):\n        question1 = Question.objects.create(text='Question 1')\n        question2 = Question.objects.create(text='Question 2')\n        answer1 = Answer.objects.create(question=question1, content='Answer 1')\n\n        self.assertEqual(list(answer1.question.all()), [question1])\n        question1.delete()\n        self.assertEqual(list(answer1.question.all()), [])\n", "    def test_get_object_for_this_type_returns_correct_object(self):\n        with self.subTest(\"Content type is a valid one\"):\n            question = Question.objects.create(text='Who?')\n            answer = Answer.objects.create(text='You!', parent=question)\n\n            content_type = ContentType.objects.get_for_model(Question)\n            self.assertEqual(content_type.get_object_for_this_type(pk=question.pk), question)\n\n            self.assertEqual(\n                content_type.get_object_for_this_type(pk=answer.pk),\n                None,\n            )\n\n            with self.assertRaises(ObjectDoesNotExist):\n                content_type.get_object_for_this_type(pk=12345)\n\n\n\n", "    def test_bulk_related_objects_with_m2m_field(self):\n        Answer.objects.create(question=Question.objects.create(text='What is the meaning of life?'),\n                             content='42')\n        Answer.objects.create(question=Question.objects.create(text='Who is the Queen?'),\n                             content='Elizabeth')\n        with self.assertNumQueries(1):\n            qs = Question.objects.filter(answers__content='42').all()\n            self.assertEqual(len(qs), 1)\n            self.assertEqual(qs[0].text, 'What is the meaning of life?')\n\n\n\n        with self.assertNumQueries(1):\n            qs = Question.objects.filter(answers__content__in=['42', 'Elizabeth']).all()\n            self.assertEqual(len(qs), 2)\n        \n\n\n\n", "    def test_get_object_for_this_type_with_wrong_content_type(self):\n        question = Question.objects.create(text='Who?')\n        post = Post.objects.create(title='Answer', parent=question)\n\n        with self.assertRaises(ObjectDoesNotExist):\n            answer = Answer.objects.create(question=question, text='...')\n\n            # Try to get the answer through a different content type\n            wrong_content_type = ContentType.objects.create(model=Answer)\n            wrong_answer = Answer.objects.get_object_for_this_type(\n                content_type=wrong_content_type,\n                pk=answer.pk\n            )\n"], "sample_142": ["    def test_check_fields_for_duplicates(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            fields = ['title', 'title', 'album']\n\n        errors = MyModelAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'fields' contains duplicate field(s).\",\n                obj=MyModelAdmin,\n                id='admin.E006'\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_check_lookup_parameters_in_autocomplete_field(self):\n        class SongAdmin(admin.ModelAdmin):\n            search_fields = ['title', 'keywords__name']\n\n        errors = SongAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"Search field 'keywords__name' contains invalid lookup parameters.\",\n                obj=SongAdmin,\n                id='admin.E204',\n            )\n        ]\n        self.assertEqual(errors, expected)\n\n", "    def test_check_for_invalid_field_types(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            fields = ['title', {'invalid_field': ''}]\n\n        errors = MyModelAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"'invalid_field' is not a field on 'admin_checks.Song'.\",\n                obj=MyModelAdmin,\n                id='admin.E002',\n            )\n        ]\n        self.assertEqual(errors, expected) \n\n", "    def test_check_complex_inlines(self):\n        class Inline1(admin.TabularInline):\n            model = Song\n            extra = 1\n            fields = ['title', 'album']\n\n        class Inline2(admin.TabularInline):\n            model = Song\n            extra = 1\n            fields = ['artist', 'duration']\n\n        class AlbumAdmin(admin.ModelAdmin):\n            inlines = [Inline1, Inline2]\n\n        errors = AlbumAdmin(Album, AdminSite()).check()\n        self.assertEqual(errors, [])\n", "    def test_check_for_nonexistent_fields_in_fieldsets(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            fieldsets = [\n                (None, {\n                    'fields': ['nonexistent_field']\n                }),\n            ]\n\n        errors = MyModelAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'fieldsets[0][1]['fields'][0]' is not an attribute of 'admin_checks.Song'.\",\n                obj=MyModelAdmin,\n                id='admin.E002',\n            )\n        ]\n        self.assertEqual(errors, expected) \n"], "sample_30": ["def test_bad_version():\n    with pytest.raises(ValueError):\n        parse(get_pkg_data_filename(\"data/bad_version.xml\"))\n", "    def test_timesys_with_duplicate_ids(tmp_path):\n        with open(tmp_path / \"timesys_duplicate_ids.xml\", \"w\") as fd:\n            fd.write(\"\"\"<?xml version=\"1.0\"?>\n            <VOTable xmlns=\"http://www.ivoa.net/xml/VOTable/v2\">\n              <RESOURCE>\n                <TIMESYS id=\"duplicates\">\n                  <timeorigin>2455197.5</timeorigin>\n                  <timescale>TCB</timescale>\n                  <refposition>BARYCENTER</refposition>\n                </TIMESYS>\n                <TIMESYS id=\"duplicates\">\n                  <timeorigin>2455197.5</timeorigin>\n                  <timescale>TCB</timescale>\n                  <refposition>BARYCENTER</refposition>\n                </TIMESYS>\n              </RESOURCE>\n            </VOTable>\n            \"\"\")\n\n        with pytest.warns(VOTableWarning):\n            votable = parse(tmp_path / \"timesys_duplicate_ids.xml\")\n\n        assert len(list(votable.iter_timesys())) == 1  \n", "def test_table_data_roundtrip():\n    votable = parse(get_pkg_data_filename(\"data/regression.xml\"))\n    table = votable.get_first_table()\n\n    # Check if table data can be round tripped \n    bio = io.BytesIO()\n    votable.to_xml(bio)\n    bio.seek(0)\n    votable_roundtrip = parse(bio)\n    table_roundtrip = votable_roundtrip.get_first_table()\n\n    assert_array_equal(table.array, table_roundtrip.array)\n    assert_array_equal(table.mask, table_roundtrip.mask)\n", "def test_timesys_custom_values():\n    votable = parse(get_pkg_data_filename(\"data/custom_timesys.xml\"))\n    timesys = votable.get_timesys_by_id(\"custom_origin\")\n    assert timesys.timeorigin == \"custom_value\"\n    assert timesys.timescale == \"custom_scale\"\n    assert timesys.refposition == \"custom_position\"\n", "    def test_timesys_missing_attribute():\n        votable = parse(get_pkg_data_filename(\"data/timesys_missing_attribute.xml\"))\n        timesys = votable.get_timesys_by_id(\"missing_attribute\")\n        assert timesys.timeorigin is None\n        assert timesys.timescale is None\n        assert timesys.refposition is None\n"], "sample_44": ["    def test_outer(self):\n        for mag1, mag2 in itertools.product(self.mags, repeat=2):\n            a = mag1.outer(mag2)\n            assert a.shape == mag1.shape + mag2.shape\n            assert np.all(a.value == mag1.value.reshape(-1, 1).dot(\n                mag2.value.reshape(1, -1)))\n            assert a.unit == mag1.unit * mag2.unit\n", "    def test_log_magnitude(self):\n        lq1 = u.Magnitude(np.arange(1., 10.)*u.Jy)\n        assert_allclose(lq1.log10(), np.log10(lq1.value))\n        assert_allclose(lq1.log10(base=2), np.log2(lq1.value))\n        assert lq1.log10().unit == u.dimensionless_unscaled\n        assert lq1.log10(base=2).unit == u.dimensionless_unscaled\n", "    def test_multiply(self):\n        for mag in self.mags:\n            other = 1.23 * u.mag() if mag.unit.physical_unit == u.dimensionless_unscaled else 1.23 * mag.unit\n            result = mag * other\n            assert np.all(result.value == mag.value * other.value)\n            assert result.unit == mag.unit if mag.unit.physical_unit == u.dimensionless_unscaled else mag.unit * other.unit\n            result = mag * other.to(mag.unit)\n            assert np.all(result.value == mag.value * other.value)\n            assert result.unit == mag.unit\n", "compilation error", "    def test_logarithmic_operations(self):\n        lq1 = u.Magnitude(np.arange(1., 4.)*u.Jy)\n        lq2 = u.Magnitude(3.*u.Jy)\n        assert np.allclose(np.log10(lq1), lq1.log10())\n        assert np.allclose(10**np.log10(lq1), lq1)\n        with pytest.raises(u.UnitsError):\n            np.log(lq1)\n        with pytest.raises(u.UnitsError):\n            10**lq1\n"], "sample_1103": ["def test_issue_16475():\n    x, y = symbols('x y', negative=False, real=True)\n    z = x**2 + y**2\n    assert z.is_nonnegative\n    assert z.is_positive is False\n\n\n\n\n", "compilation error", "def test_issue_15001():\n    from sympy import sqrt\n    x, y = symbols('x y')\n    assert (sqrt(x**2) + sqrt(y**2)).simplify() == abs(x) + abs(y)\n    assert (sqrt(x**2) - sqrt(y**2)).simplify() == abs(x) - abs(y) \n    assert (-sqrt(x**2) + sqrt(y**2)).simplify() == -abs(x) + abs(y)\n    assert (-sqrt(x**2) - sqrt(y**2)).simplify() == -abs(x) - abs(y)\n\n", "def test_issue_15799():\n    a, b = symbols('a b')\n    expr = (a/b) * (b/a)\n    assert expr.as_simple() == 1\n\n\n", "def test_issue_11331():\n    assert Pow(x, 2, evaluate=False).as_base_exp() == (x, 2)\n    assert Pow(x, 2).as_base_exp() == (x, 2)\n    assert Pow(2, 2*x, evaluate=False).as_base_exp() == (2, 2*x)\n    assert Pow(2, 2*x).as_base_exp() == (2, 2*x)\n"], "sample_601": ["def test_cftime_round_accessor(cftime_rounding_dataarray, cftime_date_type, use_dask):\n    import dask.array as da\n\n    expected = xr.DataArray(\n        [\n            [cftime_date_type(1, 1, 1, 0), cftime_date_type(1, 1, 1, 0)],\n            [cftime_date_type(1, 1, 1, 0), cftime_date_type(1, 1, 2, 0)],\n        ],\n        name=\"round\",\n    )\n\n    if use_dask:\n        chunks = {\"dim_0\": 1}\n        # Currently a compute is done to inspect a single value of the array\n        # if it is of object dtype to check if it is a cftime.datetime (if not\n        # we raise an error when using the dt accessor).\n        with raise_if_dask_computes(max_computes=1):\n            result = cftime_rounding_dataarray.chunk(chunks).dt.round(parameters)\n        expected = expected.chunk(chunks)\n        assert isinstance(result.data, da.Array)\n        assert result.chunks == expected.chunks\n    else:\n        result = cftime_rounding_dataarray.dt.round(parameters)\n\n    assert_identical(result, expected)\n", "def test_cftime_accessor_methods(data, method, parameters):\n    expected = getattr(data.time, method)(parameters)\n    actual = getattr(data.time.dt, method)(parameters)\n    assert_equal(expected, actual)\n", "    def test_cftime_round_accessor(cftime_rounding_dataarray, cftime_date_type, use_dask):\n        import dask.array as da\n\n        freq = \"D\"\n        expected = xr.DataArray(\n            [\n                [cftime_date_type(1, 1, 1, 0), cftime_date_type(1, 1, 1, 0)],\n                [cftime_date_type(1, 1, 1, 0), cftime_date_type(1, 1, 2, 0)],\n            ],\n            name=\"round\",\n        )\n\n        if use_dask:\n            chunks = {\"dim_0\": 1}\n            # Currently a compute is done to inspect a single value of the array\n            # if it is of object dtype to check if it is a cftime.datetime (if not\n            # we raise an error when using the dt accessor).\n            with raise_if_dask_computes(max_computes=1):\n                result = cftime_rounding_dataarray.chunk(chunks).dt.round(freq)\n            expected = expected.chunk(chunks)\n            assert isinstance(result.data, da.Array)\n            assert result.chunks == expected.chunks\n        else:\n            result = cftime_rounding_dataarray.dt.round(freq)\n\n        assert_identical(result, expected)\n", "def test_cftime_round_accessor(cftime_rounding_dataarray, cftime_date_type, use_dask):\n    import dask.array as da\n\n    freq = \"D\"\n    expected = xr.DataArray(\n        [\n            [cftime_date_type(1, 1, 1, 0), cftime_date_type(1, 1, 1, 0)],\n            [cftime_date_type(1, 1, 1, 0), cftime_date_type(1, 1, 2, 0)],\n        ],\n        name=\"round\",\n    )\n\n    if use_dask:\n        chunks = {\"dim_0\": 1}\n        # Currently a compute is done to inspect a single value of the array\n        # if it is of object dtype to check if it is a cftime.datetime (if not\n        # we raise an error when using the dt accessor).\n        with raise_if_dask_computes(max_computes=1):\n            result = cftime_rounding_dataarray.chunk(chunks).dt.round(freq)\n        expected = expected.chunk(chunks)\n        assert isinstance(result.data, da.Array)\n        assert result.chunks == expected.chunks\n    else:\n        result = cftime_rounding_dataarray.dt.round(freq)\n\n    assert_identical(result, expected)\n", "def test_cftime_dt_accessor_with_invalid_arguments(data):\n    with raises_regex(\n        TypeError, \"Invalid argument type for 'cftime.datetime'\"\n    ):\n        data.time.dt.isocalendar(\"invalid_parameter\")\n\n\n"], "sample_818": ["def test_spectral_clustering_with_precomputed_affinity_matrix():\n    # Test the spectral clustering with a precomputed affinity matrix\n    X, y = make_blobs(n_samples=50, centers=2, random_state=0)\n    affinity_matrix = pairwise_distances(X, metric='rbf',\n                                        gamma=1).T  # Precomputed affinity matrix\n    model = SpectralClustering(n_clusters=2, affinity='precomputed',\n                               random_state=0).fit(affinity_matrix)\n    labels = model.labels_\n    assert adjusted_rand_score(y, labels) == 1\n\n\n\n", "def test_spectral_clustering_with_random_state():\n    X, y = make_blobs(n_samples=300, centers=4, random_state=0,\n                      cluster_std=0.6)\n\n    # Random state checks for spectral_clustering and kmeans\n    for i in range(10):\n        random_state = np.random.RandomState(i)\n        sklearn_cluster = SpectralClustering(n_clusters=4,\n                                       random_state=random_state,\n                                       n_init=10, assign_labels='kmeans').fit(X)\n\n        # Second run with same random state should produce the same clusters\n        sklearn_cluster_same = SpectralClustering(n_clusters=4,\n                                          random_state=random_state,\n                                          n_init=10, assign_labels='kmeans').fit(X)\n        assert_array_equal(sklearn_cluster.labels_, sklearn_cluster_same.labels_)\n\n\n\n        \n", "    def test_spectral_affinity_scaling(affinity):\n        X, y = make_blobs(n_samples=20, centers=2, random_state=0)\n\n        if affinity == 'precomputed':\n            affinity_matrix = pairwise_distances(X)\n            affinity_matrix = -affinity_matrix  # Negative affinity\n        elif affinity == 'rbf':\n            affinity_matrix = rbf_kernel(X, gamma=1)\n        else:\n            affinity_matrix = kneighbors_graph(X, n_neighbors=5)\n\n        model1 = SpectralClustering(n_clusters=2, affinity=affinity,\n                                    random_state=0).fit(X)\n        model2 = SpectralClustering(n_clusters=2, affinity=affinity,\n                                    random_state=0).fit(affinity_matrix)\n        assert_array_equal(model1.labels_, model2.labels_)\n", "def test_spectral_clustering_affinity_matrix_precomputed():\n    X, y = make_blobs(n_samples=20, random_state=0,\n                      centers=[[1, 1], [-1, -1]], cluster_std=0.01)\n    S = pairwise_distances(X)\n    S = np.max(S) - S\n    S = S.astype(float)  # Ensure it's float and not doubles\n\n    sc = SpectralClustering(n_clusters=2, affinity='precomputed',\n                           random_state=0)\n    labels = sc.fit(S).labels_\n    assert adjusted_rand_score(y, labels) == 1\n    \n", "def test_spectral_clustering_affinity_matrix():\n    n_samples = 10\n    random_state = check_random_state(42)\n    X = random_state.rand(n_samples, 5)\n\n    # Generate a random affinity matrix\n    affinity_matrix = rbf_kernel(X, gamma=2)\n\n    # Fit SpectralClustering with precomputed affinity\n    sp = SpectralClustering(n_clusters=2, affinity='precomputed',\n                            random_state=random_state)\n\n    sp.fit(affinity_matrix)\n    labels = sp.labels_\n\n    # Fit SpectralClustering with provided data and affinity='rbf'\n    sp2 = SpectralClustering(n_clusters=2, affinity='rbf',\n                            gamma=2, random_state=random_state)\n    sp2.fit(X)\n    labels2 = sp2.labels_\n\n    # Assert that the labels are equal for both methods\n    assert (labels == labels2).all()\n"], "sample_105": ["    def test_template_response_mixin_with_object(self):\n        view = views.TemplateResponseWithObjectView()\n        response = view(self.rf.get('/'))\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.context['object'], {'id': 1, 'name': 'Example Object'})\n", "    def test_template_no_object(self):\n        \"\"\"\n        Make sure that the view renders without raising an exception\n        when the object is not assigned to the context\n        \"\"\"\n        response = self.client.get('/template/no_object/')\n        self.assertEqual(response.status_code, 200)\n\n", "    def test_context_from_view(self):\n        test_view = views.CustomContextView()\n        context = test_view.get_context_data()\n        self.assertEqual(context['custom_key'], 'custom_value')\n", "    def test_context_object_name(self):\n        \"\"\"Test different ways to set the context object name.\"\"\"\n        response = self.client.get('/template/object_name/my_object/')\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.context['my_object'], 'My Object')\n\n        response = self.client.get('/template/object_name_from_view/my_other_object/')\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.context['target'], 'My Other Object')\n", "    def test_template_params_with_request(self):\n        \"\"\"\n        A generic template view can access request attributes\n        \"\"\"\n        request = self.rf.get('/template/simple/bar/', HTTP_HOST=\"example.com\")\n        response = self.client.get('/template/simple/bar/', **request.query_params)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.context['host'], 'example.com')\n\n"], "sample_1140": ["def test_issue_18367():\n    from sympy.logic.boolalg import Or\n    assert pretty(Or(Symbol('x') > 1, Symbol('x') < 0)) == '(x > 1) \u2228 (x < 0)'\n    assert upretty(Or(Symbol('x') > 1, Symbol('x') < 0)) == '(x > 1) \u2228 (x < 0)'\n", "def test_pretty_matrix_powers():\n    m, n = symbols('m n', integer=True)\n    A = MatrixSymbol('A', m, n)\n    B = MatrixSymbol('B', m, n)\n\n    assert pretty(A**2) == 'A\u00b2'\n    assert upretty(A**2) == 'A\u00b2'\n\n    assert pretty(A**-2) == 'A\u207b\u00b2'\n    assert upretty(A**-2) == 'A\u207b\u00b2'\n\n    assert pretty(A**(1/2)) == '\u221aA'\n    assert upretty(A**(1/2)) == 'A^(1/2)'\n\n    assert pretty((A*B)**3) == '(A\u22c5B)\u00b3'\n    assert upretty((A*B)**3) == '(A\u22c5B)\u00b3'\n\n\n", "def test_issue_18273():\n    from sympy import integrate\n    x, y = symbols('x y')\n    assert pretty(integrate(1/(x + y), (x, 0, 1))) == '\u222b\u2081\u2070(1/(x + y))dx'\n    assert upretty(integrate(1/(x + y), (x, 0, 1))) == '\u222b\u2080\u00b9(1 / (x + y)) dx'\n\n\n", "def test_issue_18609():\n    from sympy.physics.quantum.density import DensityMatrix\n    dm = DensityMatrix([[1, 0], [0, 0]])\n    assert pretty(dm) == \"\"\"\\", "def test_issue_18436():\n    a = Symbol('a')\n    b = Symbol('b')\n    expr = (a*b**2 + a*b + 1) / (a*b**2 + a*b)\n    assert pretty(expr) == '((a\u22c5b\u00b2 + a\u22c5b + 1))/(a\u22c5b\u00b2 + a\u22c5b)'\n    assert upretty(expr) == '((a\u22c5b\u00b2 + a\u22c5b + 1))/(a\u22c5b\u00b2 + a\u22c5b)'\n"], "sample_661": ["    def test_skipped_string_reason(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.skip(reason=\"my reason\")\n                pass\n            \"\"\"\n        )\n        result, dom = runandparse(testdir)\n        assert result.ret == 0\n        node = dom.find_first_by_tag(\"testcase\")\n        snode = node.find_first_by_tag(\"skipped\")\n        assert \"my reason\" in snode.text\n        snode.assert_attr(message=\"my reason\")\n", "    def test_unicode_issue368_non_ascii_characters(testdir):\n        testdir.makeini(\n            \"\"\"\n            [pytest]\n            junit_family = xunit1\n        \"\"\"", "def test_multiple_skips(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.skip(reason=\"Reason 1\")\n            pass\n        @pytest.mark.skip(reason=\"Reason 2\")\n            pass\n        @pytest.mark.skip(reason=\"Combined reason: Reason 1 Reason 2\")\n            pass\n    \"\"\"\n    )\n    _, dom = runandparse(testdir)\n    assert len(dom.findall(\".//skipped\")) == 3\n\n", "    def test_nested_skipreasons_issue3533(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n            @pytest.mark.skip(reason='reason 1')\n                @pytest.mark.skip(reason='reason 2')\n                    pass\n            \"\"\"\n        )\n        _, dom = runandparse(testdir)\n        node = dom.find_first_by_tag(\"testcase\")\n        snode = node.find_first_by_tag(\"skipped\")\n        assert \"reason 1\" in snode.text\n        snode.assert_attr(message=\"reason 1\")\n\n\n", "def test_custom_xml_elements(testdir):\n    \"\"\"\n    Test the functionality of custom XML elements in the report.\n    \"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        class MyCustomData(object):\n                self.value = value\n\n        @pytest.fixture\n            return MyCustomData(\"fixture_data\")\n\n            assert custom_data.value == \"fixture_data\"\n\n            pytest.mark.custom_xml(\n                \"custom_element\", {\"data\": custom_data.value}\n            )\n            \n    \"\"\"\n    )\n    result, dom = runandparse(testdir, \"-rw\", '--junit-xml=test.xml')\n    assert result.ret == 0\n\n    node = dom.find_first_by_tag(\"testsuite\")\n    testcase_nodes = node.findall(\"testcase\")\n    assert len(testcase_nodes) == 1\n\n    custom_element = testcase_nodes[0].find(\"custom_element\")\n    assert custom_element is not None\n    assert custom_element.get(\"data\") == \"fixture_data\"\n"], "sample_194": ["    def test_name_with_include(self):\n        constraints = get_constraints(UniqueConstraintInclude._meta.db_table)\n        expected_name = 'name_color_include_uniq'\n        self.assertIn(expected_name, constraints)\n\n", "    def test_opclasses_database_constraint(self):\n        UniqueConstraintProduct.objects.create(name='p1', color='red')\n        with self.assertRaises(IntegrityError):\n            UniqueConstraintProduct.objects.create(name='p1', color='red')\n", "    def test_opclasses_database_constraint(self):\n        with self.assertRaises(IntegrityError):\n            UniqueConstraintProduct.objects.create(\n                name='p1', color='red', opclasses='text_pattern_ops'\n            )\n", "    def test_opclasses_empty_list(self):\n        with self.assertRaisesMessage(ValueError, 'UniqueConstraint.opclasses must have at least one element'):\n            models.UniqueConstraint(\n                name='uniq_opclasses',\n                fields=['field'],\n                opclasses=()\n            )\n", "    def test_cannot_create_unique_constraint_with_duplicate_fields(self):\n        with self.assertRaisesMessage(ValueError, 'UniqueConstraint.fields must not contain duplicates.'):\n            models.UniqueConstraint(\n                fields=['name', 'name'],\n                name='name_unique',\n            )\n\n\n"], "sample_820": ["def test_predict_proba_proba_on_toy_problem():\n    \"\"\"Calculate probabilities predicted by each estimator separately on toy dataset.\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    clf3 = GaussianNB()\n    X = np.array([[-1.1, -1.5], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])\n    y = np.array([1, 1, 2, 2])\n\n    eclf = VotingClassifier(estimators=[\n                            ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n                            voting='soft')\n    eclf.fit(X, y)\n    \n    res_eclf = eclf.predict_proba(X)\n\n    res_lr = clf1.predict_proba(X)\n    res_rf = clf2.predict_proba(X)\n    res_gnb = clf3.predict_proba(X)\n\n    assert_array_almost_equal(res_eclf.sum(axis=1), np.ones(4), decimal=2) \n    assert_almost_equal(res_eclf[:, 0].sum(), res_lr[:, 0].sum() + res_rf[:, 0].sum() + res_gnb[:, 0].sum(), decimal=2) \n    assert_almost_equal(res_eclf[:, 1].sum(), res_lr[:, 1].sum() + res_rf[:, 1].sum() + res_gnb[:, 1].sum(), decimal=2) \n", "    def test_transform_with_different_estimators():\n        \"\"\"\n        Check transform method of VotingClassifier when estimators have different output shapes\n        \"\"\"\n        clf1 = LogisticRegression(random_state=123, multi_class='multinomial', solver='lbfgs')\n        clf2 = RandomForestClassifier(random_state=123, n_jobs=-1)\n        clf3 = KNeighborsClassifier()\n\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        y = np.array([0, 1, 2])\n\n        eclf = VotingClassifier(estimators=[\n            ('lr', clf1), ('rf', clf2), ('knn', clf3)],\n            voting='soft', flatten_transform=False).fit(X, y)\n\n        transformed_data = eclf.transform(X)\n\n        assert transformed_data.shape == (3, 3, 3)  # Each estimator output 3 classes\n\n", "def test_partial_fit():\n    \"\"\"Check partial_fit functionality of VotingClassifier.\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    clf3 = GaussianNB()\n    eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n                            voting='soft')\n\n    X_train = X[:10]\n    y_train = y[:10]\n    X_test = X[10:]\n    y_test = y[10:]\n\n    eclf.partial_fit(X_train, y_train)\n\n    assert_array_almost_equal(eclf.predict(X_test), np.array([0, 1, 1, 0]))\n    assert_array_almost_equal(\n        eclf.predict_proba(X_test),\n        np.array([[0.7, 0.3], [0.1, 0.9], [0.1, 0.9], [0.8, 0.2]]))\n\n\n    eclf.partial_fit(X_train, y_train, classes=['a', 'b'])\n\n    assert_array_equal(eclf.predict(X_test), np.array([0, 1, 1, 0]))\n\n\n\n", "def test_voting_classifier_with_pipeline():\n    from sklearn.pipeline import Pipeline\n\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    clf3 = GaussianNB()\n    eclf = VotingClassifier(estimators=[\n                            ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n                            voting='soft')\n\n    stages = [('scaler', StandardScaler()), ('clf', eclf)]\n    pipeline = Pipeline(stages)\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n    pipeline.fit(X_train, y_train)\n    y_pred = pipeline.predict(X_test)\n\n    assert_array_equal(y_pred, eclf.predict(X_test))\n", "def test_voting_with_two_estimators():\n    \"\"\"Test VotingClassifier with two estimators.\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)],\n                            voting='soft')\n    eclf.fit(X, y)\n    assert_array_equal(eclf.predict(X),\n                       np.array([0, 0, 1, 1]))\n    assert_array_almost_equal(eclf.predict_proba(X),\n                              np.array([[0.7, 0.3],\n                                        [0.8, 0.2],\n                                        [0.2, 0.8],\n                                        [0.4, 0.6]]))\n"], "sample_1043": ["def test_complex_expressions():\n    assert mcode(x*sin(x)**(x + 1) + 2*cos(x) + 1) == \"x*Sin[x]^(x + 1) + 2*Cos[x] + 1\"\n    assert mcode(Exp(x*x*y) + 2*x**2 - 3*y) == \"Exp[x^2*y] + 2 x^2 - 3 y\"\n\n\n\n\n", "def test_known_functions():\n    assert mcode(sin(x) ** cos(x)) == \"Sin[x]^Cos[x]\"\n    assert mcode(exp(x**2)) == \"Exp[x^2]\"\n    assert mcode(log(x**2)) == \"Log[x^2]\"\n", "compilation error", "def test_complex_numbers():\n    assert mcode(1 + 2*I) == \"1 + 2 I\"\n    assert mcode(2 + 3*I * (4 - I)) == \"10 + 11 I\"\n    assert mcode(-1 - 2*I) == \"-1 - 2 I\"\n    assert mcode(complex(1, 2)) == \"1 + 2 I\"\n", "def test_user_functions():\n        return x**y \n\n    known_functions['my_function'] = [(lambda x, y: True), 'my_function']\n    assert mcode(my_function(x, y)) == \"my_function[x, y]\"\n"], "sample_1112": ["def test_is_palindromic():\n    assert is_palindromic(121)\n    assert is_palindromic(121, 8)\n    assert not is_palindromic(123, 10)\n    assert not is_palindromic(123, 8)\n    assert is_palindromic(0o121, 8)\n    assert not is_palindromic(0o121, 10)\n    assert is_palindromic(88, 10)\n    assert not is_palindromic(88, 8)\n", "def test_is_palindromic():\n    assert is_palindromic(121)\n    assert is_palindromic(121, 8)\n    assert not is_palindromic(123, 10)\n    assert not is_palindromic(123, 8)\n    assert is_palindromic(0o121, 8)\n    assert not is_palindromic(0o121)\n    assert is_palindromic(-121)\n", "def test_is_palindromic():\n    assert is_palindromic(121)\n    assert is_palindromic(-121)\n    assert not is_palindromic(123)\n    assert is_palindromic(12321)\n    assert is_palindromic(0, 10)\n    assert is_palindromic(1, 10)\n    assert is_palindromic(88, 10)\n    assert not is_palindromic(88, 8)\n    assert is_palindromic(0o121, 8)\n    assert not is_palindromic(0b10101, 2)\n    assert is_palindromic(121, 8)\n", "compilation error", "compilation error"], "sample_422": ["    def test_foreignkey_forward(self):\n        books = Book.objects.order_by(\"title\")\n        with self.assertNumQueries(3):\n            authors = list(\n                Author.objects.prefetch_related(\n                    Prefetch(\n                        \"first_book\",\n                        books,\n                    ),\n                    Prefetch(\n                        \"first_book\",\n                        books[1:],\n                        to_attr=\"first_book_sliced\",\n                    ),\n                )\n            )\n        for author in authors:\n            with self.subTest(author=author):\n                self.assertEqual(\n                    author.first_book_sliced, list(author.first_book.all())[1:]\n                )\n", "    def test_foreignkey_forward(self):\n        books = Book.objects.order_by(\"title\")\n        with self.assertNumQueries(3):\n            authors = list(\n                Author.objects.prefetch_related(\n                    Prefetch(\"first_book\", books),\n                    Prefetch(\"first_book\", books[1:], to_attr=\"first_book_sliced\"),\n                )\n            )\n        for author in authors:\n            with self.subTest(author=author):\n                self.assertEqual(\n                    author.first_book_sliced, list(author.first_book.all())[1:]\n                )\n", "    def test_prefetch_related_with_distinct_related_objects_multiple_instances(self):\n        \"\"\"\n        Tests that prefetching works correctly when there are multiple instances of distinct related objects\n        \"\"\"\n        for _ in range(3):\n            AuthorWithAge.objects.create(name=f\"Author {_+1}\", first_book=None, age=40)\n        authors = AuthorWithAge.objects.all()\n        first_book = BookWithYear.objects.create(title=\"First Book\", published_year=2020)\n        second_book = BookWithYear.objects.create(title=\"Second Book\", published_year=2021)\n        authors[0].first_book = first_book\n        authors[1].first_book = second_book\n        authors[2].first_book = first_book\n        with self.assertNumQueries(2):\n            books = list(\n                BookWithYear.objects.prefetch_related(\n                    Prefetch(\"authorwithage_set\", authors[0:2], to_attr=\"author_set\"),\n                    Prefetch(\"authorwithage_set\", to_attr=\"author_set_2\"),\n                )\n            )\n        for book in books:\n            with self.subTest(book=book):\n                self.assertEqual(len(book.author_set), 2)\n                self.assertEqual(len(book.author_set_2), 2)\n\n\n\n", "    def test_complex_m2m_prefetch(self):\n        \"\"\"\n        Test a more complex scenario involving multiple m2m relationships\n        with nested prefetches.\n        \"\"\"\n        # Create a bunch of stuff\n        author1 = Author.objects.create(name=\"Jane\")\n        author2 = Author.objects.create(name=\"John\")\n        author3 = Author.objects.create(name=\"Mary\")\n        book1 = Book.objects.create(title=\"Hamlet\")\n        book2 = Book.objects.create(title=\"Romeo and Juliet\")\n        book3 = Book.objects.create(title=\"The Tempest\")\n\n        # Establish relationships\n        author1.favorites.add(book1, book2)\n        author2.favorites.add(book2, book3)\n        author3.favorites.add(book3)\n        # Add some more complex nested relationships\n        favorite_author1 = Author.objects.create(name=\"FavAuthor1\")\n        favorite_author2 = Author.objects.create(name=\"FavAuthor2\")\n        author1.favorite_authors.add(favorite_author1)\n\n        with self.assertNumQueries(4):\n            books = Book.objects.prefetch_related(\n                Prefetch(\n                    \"favorites\",\n                    queryset=Book.objects.prefetch_related(\n                        Prefetch(\n                            \"authors\",\n                            queryset=Author.objects.prefetch_related(\n                                Prefetch(\n                                    \"favorites\",\n                                    queryset=Book.objects.values(\"title\"),\n                                ),\n                            ),\n                        ),\n                    ),\n                ),\n            )\n\n        # Access the nested prefetched data\n        for book in books:\n            with self.subTest(book=book):\n                # Check if the favorite_authors relation is fetched\n                self.assertEqual(len(book.favorites.all()), len(book.authors.all()))\n                # Check if the nested favorite author is fetched\n                for author in book.favorites.all():\n                    self.assertEqual(len(author.favorites.all()), 1)\n                    self.assertEqual(author.favorites.first()[\"title\"], \"Hamlet\")\n\n", "    def test_prefetch_related_with_ordering(self):\n        # Test ordering when prefetching related objects\n        authors = Author.objects.order_by(\"name\")\n        with self.assertNumQueries(3):\n            books = list(\n                Book.objects.prefetch_related(\n                    Prefetch(\n                        \"first_time_authors\", authors, queryset=authors.order_by(\"age\")\n                    )\n                )\n            )\n\n        for book in books:\n            with self.subTest(book=book):\n                for author in book.first_time_authors.all():\n                    with self.subTest(author=author):\n                        self.assertEqual(\n                            list(authors).index(author),\n                            list(book.first_time_authors.all()).index(author),\n                        )\n"], "sample_296": ["    def test_not_finished_sentinel(self):\n        storage = self.get_storage()\n        response = self.get_response()\n\n        # Add some messages to the storage\n        storage.add(constants.INFO, 'Message 1')\n        storage.add(constants.INFO, 'Message 2')\n        storage.add(constants.INFO, 'Message 3')\n\n        # Store the messages in the cookie\n        storage.update(response)\n\n        # Retrieve the messages from the cookie\n        messages = self.stored_messages_count(storage, response)\n\n        # Assert that we retrieved all messages\n        self.assertEqual(messages, 3)\n\n        # Add another message\n        storage.add(constants.INFO, 'Message 4')\n\n        # Update the cookie again\n        storage.update(response)\n\n        # Assert that the cookie now contains the not_finished sentinel\n        messages = self.stored_messages_count(storage, response)\n        self.assertEqual(messages, 3)\n        self.assertTrue(storage._loaded_data[-1] == CookieStorage.not_finished) \n\n", "    def test_not_finished_sentinel(self):\n        storage = self.storage_class(self.get_request())\n        example_messages = ['test', 'me']\n        set_cookie_data(storage, example_messages)\n        messages, all_retrieved = storage._get()\n        self.assertEqual(messages, example_messages[:-1])\n        self.assertFalse(all_retrieved)\n\n        # Verify the stored data reflects the sentinel removal.\n        set_cookie_data(storage, example_messages[:-1])\n        messages, all_retrieved = storage._get()\n        self.assertEqual(messages, example_messages[:-1])\n        self.assertTrue(all_retrieved)\n", "    def test_not_finished_sentinel(self):\n        storage = self.get_storage()\n        response = self.get_response()\n\n        # Add a message, store it, and check that the sentinel is present.\n        storage.add(constants.INFO, 'Test message 1')\n        storage.update(response)\n        decoded_messages = storage._decode(response.cookies['messages'].value)\n        self.assertEqual(decoded_messages[-1], CookieStorage.not_finished)\n\n        # Simulate a partially retrieved set of messages by removing some\n        # messages from the cookie and check that the sentinel is still\n        # present.\n        response.cookies['messages'].value = response.cookies['messages'].value[:-1]\n        decoded_messages = storage._decode(response.cookies['messages'].value)\n        self.assertEqual(decoded_messages[-1], CookieStorage.not_finished)\n\n        # Retrieve the remaining messages and remove the sentinel.\n        storage.update(response)\n        decoded_messages = storage._decode(response.cookies['messages'].value)\n        self.assertEqual(len(decoded_messages), 1)\n", "    def test_not_finished(self):\n        storage = self.storage_class(self.get_request())\n        messages = [\n            Message(constants.INFO, 'message 1'),\n            Message(constants.INFO, 'message 2'),\n            Message(constants.INFO, 'message 3'),\n        ]\n        set_cookie_data(storage, messages[:2])\n        \n\n        messages_part_one = list(storage)\n        self.assertEqual(messages_part_one, messages[:2])\n\n        response = self.get_response()\n        storage.update(response, messages=messages[2:])\n\n        messages_part_two = list(storage)\n        self.assertEqual(messages_part_two, messages[2:])\n\n        self.assertEqual(stored_cookie_messages_count(storage, response), 3)\n        \n        storage.add(constants.INFO, 'message 4')\n        response = self.get_response()\n        storage.update(response)\n\n        self.assertEqual(stored_cookie_messages_count(storage, response), 4)\n", "    def test_not_finished_sentinel(self):\n        storage = self.get_storage()\n        response = self.get_response()\n        messages = ['test', 'me']\n\n        # Send a message to the storage and ensure it stores correctly\n        storage.add(constants.INFO, 'test')\n        storage.update(response)\n        self.assertEqual(self.stored_messages_count(storage, response), 1)\n\n        # Simulate a scenario where not all messages could be stored due to\n        # the cookie size limit.\n        for i in range(CookieStorage.max_cookie_size // 50):\n            storage.add(constants.INFO, 'message')\n        unstored_messages = storage.update(response)\n\n        # Validate that the 'not_finished' sentinel was added\n        self.assertEqual(self.stored_messages_count(storage, response),\n                         CookieStorage.max_cookie_size // 50)\n        self.assertTrue(unstored_messages)\n\n        # Ensure that the remaining messages are retrieved correctly\n        storage._loaded_data = None  # Reset the internal cache\n        messages_after_retrieval = list(storage)\n        self.assertEqual(len(messages_after_retrieval), \n                         CookieStorage.max_cookie_size // 50)\n        self.assertEqual(unstored_messages[0].message, 'message')\n"], "sample_788": ["def test_bin_edges_size(n_bins, strategy):\n    X = np.arange(100).reshape(-1, 1)\n    kbd = KBinsDiscretizer(n_bins=n_bins, strategy=strategy)\n    kbd.fit(X)\n    assert len(kbd.bin_edges_[0]) == n_bins + 1\n", "    def test_invalid_n_bins_with_constant_features(n_bins):\n        X = np.array([[1, 1, 1, 1],\n                      [1, 1, 1, 1],\n                      [1, 1, 1, 1],\n                      [1, 1, 1, 1]])\n        kbd = KBinsDiscretizer(n_bins=n_bins)\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            kbd.fit(X)\n        assert_array_equal(kbd.bin_edges_[0], np.array([1, 1]))\n\n\n", "def test_n_bins_zero_feature():\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    with pytest.raises(ValueError) as e:\n        KBinsDiscretizer(n_bins=0).fit(X)\n    assert \"n_bins must be at least 2\" in str(e.value)\n\n", "def test_identical_points(strategy):\n    X = np.array([[1], [1], [1], [2], [2]]).reshape(-1, 1)\n    kbd = KBinsDiscretizer(n_bins=3, strategy=strategy, encode='ordinal')\n    Xt = kbd.fit_transform(X)\n    assert_array_equal(Xt, np.array([[0], [0], [0], [1], [1]]))\n\n    # test that inverse_transform works correctly\n    Xinv = kbd.inverse_transform(Xt)\n    assert_array_almost_equal(Xinv, np.array([[1. ], [1. ], [1. ], [2. ], [2. ]]))\n\n", "def test_fit_transform_with_nan():\n    X = np.array([[1, np.nan], [2, 3], [4, np.nan]])\n    est = KBinsDiscretizer(n_bins=2, strategy='uniform')\n    est.fit_transform(X)\n    # Test that nan values are handled correctly\n    assert_array_equal(est.transform(X), np.array([[0, 0], [1, 1], [1, 1]]))\n"], "sample_704": ["def test_prunetraceback_removes_initial_paths(\n    pytester: Pytester", "def test_node_get_closest_marker_with_defaults(pytester: Pytester) -> None:\n    items = pytester.getitems(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.skip\n            pass\n    \"\"\"\n    )\n    item = items[0]\n    assert item.get_closest_marker(\"skip\") == pytest.skip\n    assert item.get_closest_marker(\"x\") is None\n    assert item.get_closest_marker(\"x\", default=pytest.mark.parametrize) == pytest.mark.parametrize\n", "def test_iterparentnodeids_with_fspath_sep() -> None:\n    \"\"\"Test iterparentnodeids with fspath that may contain separators.\"\"\"\n    for sep in [os.sep, \"/\"]:\n        for nodeid in (\n            f\"a{sep}b{sep}c\",\n            f\"a{sep}b{sep}c::D\",\n            f\"d{sep}e{sep}f::g{sep}h\",\n        ):\n            result = list(nodes.iterparentnodeids(nodeid))\n            expected = result  # Initialize with empty list for first iteration\n            for part in nodeid.split(sep):\n                expected.append(sep.join(expected))\n            assert result == expected\n\n", "    def test_iterparentnodeids_handles_leading_separator(nodeid: str, expected: List[str]) -> None:\n        result = list(nodes.iterparentnodeids(nodeid))\n        assert result == expected\n", "def test_node_repr_failure_with_fulltrace(pytester: Pytester) -> None:\n    result = pytester.runpytest_subprocess(\n        \"--fulltrace\", \"-s\", \"-v\", \"test_node.py\", capture=True\n    )\n    assert result.ret == 0\n    assert \"stack\" in result.stdout\n\n"], "sample_74": ["    def test_empty_db_name(self):\n        with self.assertRaises(subprocess.CalledProcessError):\n            self._run_it({\n                'database': '',\n                'user': 'someuser',\n                'password': 'somepassword',\n                'host': 'somehost',\n                'port': '444',\n            })\n", "    def test_empty_params(self):\n        self.assertEqual(\n            self._run_it({}), (\n                ['psql'],\n                {},\n            )\n        )\n", "    def test_empty_connection_params(self):\n        self.assertEqual(self._run_it({}), (['psql'], {}))\n", "    def test_empty_params(self):\n        with self.assertRaises(ValueError):\n            DatabaseClient.runshell_db({})\n", "    def test_empty_params(self):\n        self.assertEqual(\n            self._run_it({}),\n            (['psql'], {}),\n        )\n"], "sample_714": ["def test_log_loss_with_binary_output_and_multiclass_labels():\n    y_true = [0, 1, 2]\n    y_pred = [[0.2, 0.8], [0.1, 0.9], [0.6, 0.4]]\n    loss = log_loss(y_true, y_pred, labels=[0, 1, 2])\n    assert_almost_equal(loss, 0.9877844)\n\n\n", "def test_log_loss_sparse_input():\n    y_true = np.array([1, 0, 2])\n    y_pred = sp.csr_matrix([[0.2, 0.7, 0.1], [0.6, 0.2, 0.2], [0.6, 0.1, 0.3]])\n    loss = log_loss(y_true, y_pred, normalize=True)\n    assert_almost_equal(loss, 0.6904911)\n", "def test_log_loss_multiclass_with_empty_classes():\n    y_true = [0, 1, 2]\n    y_pred = [[0.2, 0.7, 0.1], [0.6, 0.2, 0.2], [0.6, 0.1, 0.3]]\n    \n    # Empty classes in y_true\n    y_true_empty = [0, 1, 2, -1]\n    y_pred_empty = [[0.2, 0.7, 0.1], [0.6, 0.2, 0.2], [0.6, 0.1, 0.3]]\n    \n    for y_true, y_pred in [(y_true, y_pred), (y_true_empty, y_pred_empty)]:\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(\"always\")\n            loss = log_loss(y_true, y_pred)\n        assert_equal(len(w), 1)\n        assert_equal(str(w[-1].message),\n                     \"Encountered labels not present in the prediction matrix. \"\n                     \"Consider using the labels argument to specify the \"\n                     \"correct mapping.\")\n\n\n\n", "def test_log_loss_multiclass_multioutput():\n    y_true = np.array([[1, 0, 2],\n                       [1, 1, 0],\n                       [0, 2, 2],\n                       [1, 0, 2]])\n    y_pred = np.array([[[0.1, 0.8, 0.1],\n                        [0.5, 0.3, 0.2],\n                        [0.6, 0.2, 0.2]],\n\n                       [[0.3, 0.4, 0.3],\n                        [0.2, 0.6, 0.2],\n                        [0.7, 0.1, 0.2]],\n\n                       [[0.2, 0.1, 0.7],\n                        [0.3, 0.5, 0.2],\n                        [0.4, 0.3, 0.3]],\n\n                       [[0.2, 0.6, 0.2],\n                        [0.4, 0.3, 0.3],\n                        [0.5, 0.2, 0.3]]])\n    loss = log_loss(y_true, y_pred, normalize='true')\n    assert_almost_equal(loss, 1.2499999999999999)\n\n\n    \n\n", "def test_log_loss_multiclass_with_different_number_of_classes():\n    # This test checks if the function raises a ValueError when the number of classes in y_true and y_pred is different.\n    y_true = [1, 0, 2]\n    y_pred = [[0.1, 0.2, 0.7], [0.5, 0.3, 0.2], [0.3, 0.6, 0.1]]\n    assert_raises(ValueError, log_loss, y_true, y_pred)\n"], "sample_507": ["    def test_plot_with_empty_data(self, plotter):\n        ax = plt.figure().subplots()\n        plotter(ax, [], [1, 2])\n        axis_test(ax.xaxis, [])\n", "    def test_plot_with_existing_units(self, plotter):\n        ax = plt.figure().subplots()\n        # Set existing units\n        ax.set_xunits([\"a\", \"b\"])\n        plotter(ax, [\"a\", \"b\"], [1, 2])\n        axis_test(ax.xaxis, [\"a\", \"b\"])\n\n", "    def test_plot_with_nan(self, plotter):\n        ax = plt.figure().subplots()\n        data = ['a', 'b', 'c', np.nan]\n        counts = [1, 2, 3, np.nan]\n        plotter(ax, data, counts)\n        # check that the axes labels are as they should be\n        axis_test(ax.xaxis, data)\n", "    def test_plot_mixed_types(self, plotter):\n        ax = plt.figure().subplots()\n        plotter(ax, [\"A\", 1, 2.5], [1, 2, 3])\n        axis_test(ax.xaxis, [\"A\", 1, 2.5])\n        axis_test(ax.yaxis, [1, 2, 3])\n", "    def test_plot_with_dates(self, plotter):\n        ax = plt.figure().subplots()\n        dates = [datetime(2023, 1, 1), datetime(2023, 1, 2), datetime(2023, 1, 3)]\n        plotter(ax, dates, range(len(dates)))\n        axis_test(ax.xaxis, dates)    \n"], "sample_431": ["    def test_refresh_clears_cached_relations_with_prefetch(self):\n        article = Article.objects.prefetch_related(\"related_article\").first()\n        self.assertIsNotNone(article.related_article)\n        article.refresh_from_db()\n        self.assertIsNotNone(article.related_article)\n        \n        # This assert is to confirm the prefetch_related is working\n        with self.assertNumQueries(0):\n            self.assertEqual(len(article.related_article.headline), 20)\n", "    def test_refresh_with_only_fields(self):\n        a = Article.objects.create(headline=\"Parrot programs in Python\", pub_date=datetime(2005, 7, 28))\n        a.headline = \"Parrot programs in Python 2.0\"\n        a.save()\n\n        with self.assertNumQueries(1):\n            a_refreshed = Article.objects.get(pk=a.pk)\n            a_refreshed.refresh_from_db(fields=[\"headline\"])\n        self.assertEqual(a_refreshed.headline, \"Parrot programs in Python 2.0\")\n        self.assertNotEqual(a_refreshed.pub_date, datetime.now())\n\n", "    def test_refresh_cascade_related_objects(self):\n        parent = Parent.objects.create(name=\"parent\")\n        child = Child.objects.create(parent=parent, name=\"child\")\n        with self.assertNumQueries(1):\n            parent.refresh_from_db()\n\n        with self.assertNumQueries(1):\n            child.refresh_from_db(fields=[\"parent\"])\n\n        self.assertEqual(child.parent, parent)\n", "    def test_refresh_multiple_fields(self):\n        a = Article.objects.create(\n            headline=\"Parrot programs in Python\",\n            pub_date=datetime(2005, 7, 28),\n            teaser=\"This is a teaser\",\n        )\n        with self.assertNumQueries(1):\n            a.refresh_from_db(fields=[\"headline\", \"teaser\"])\n        self.assertEqual(a.headline, \"Parrot programs in Python\")\n        self.assertEqual(a.teaser, \"This is a teaser\")\n", "    def test_refresh_with_select_related(self):\n        a = Article.objects.create(\n            headline=\"Parrot programs in Python\",\n            pub_date=datetime(2005, 7, 28),\n        )\n        a.featured_article.update(headline=\"New headline\")\n        a.refresh_from_db(select_related=[\"featured_article\"])\n        self.assertEqual(a.featured_article.headline, \"New headline\")\n\n"], "sample_66": ["    def test_case_insensitive(self):\n        environ = {\n            'Content-Type': 'text/html',\n            'CONTENT_LENGTH': '100',\n            'hOst': 'example.com',\n        }\n        headers = HttpHeaders(environ)\n        self.assertEqual(sorted(headers), ['Content-Length', 'Content-Type', 'Host'])\n        self.assertEqual(headers, {\n            'Content-Type': 'text/html',\n            'Content-Length': '100',\n            'Host': 'example.com',\n        })\n", "    def test_header_case_insensitivity(self):\n        environ = {\n            'HTTP_ACCEPT': 'application/json',\n            'content-type': 'text/html',\n            'content-length': '100',\n        }\n        headers = HttpHeaders(environ)\n        self.assertEqual(sorted(headers), [\n            'Accept', 'Content-Length', 'Content-Type',\n        ])\n        self.assertEqual(headers, {\n            'Accept': 'application/json',\n            'Content-Length': '100',\n            'Content-Type': 'text/html'\n        })\n\n", "    def test_remove_invalid_non_http_header_names(self):\n        environ = {\n            'BAD_HEADER': 'hello',\n            'CONTENT_TYPE': 'text/html',\n            'CONTENT_LENGTH': '100',\n            'HTTP_HOST': 'example.com',\n        }\n        headers = HttpHeaders(environ)\n        self.assertEqual(sorted(headers), ['Content-Length', 'Content-Type', 'Host'])\n        self.assertEqual(headers, {\n            'Content-Type': 'text/html',\n            'Content-Length': '100',\n            'Host': 'example.com',\n        })\n", "    def test_non_ascii_characters(self):\n        environ = {\n            'CONTENT_TYPE': 'text/html; charset=utf-8',\n            'HTTP_HOST': 'example.com',\n            'HTTP_USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n        }\n        headers = HttpHeaders(environ)\n        self.assertEqual(sorted(headers), ['Content-Type', 'Host', 'User-Agent'])\n        self.assertEqual(headers['Content-Type'], 'text/html; charset=utf-8')\n        self.assertEqual(headers['User-Agent'], 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3')\n", "    def test_empty_headers(self):\n        environ = {}\n        headers = HttpHeaders(environ)\n        self.assertEqual(sorted(headers), [])\n"], "sample_1204": ["compilation error", "compilation error", "compilation error", "def test_order():\n    G = PermutationGroup(Permutation(0,1,2))\n    assert G.order() == 3\n    G = PermutationGroup(Permutation(0,1,2,3))\n    assert G.order() == 4\n    G = PermutationGroup(Permutation(0,1,2,3,4,5,6,7))\n    assert G.order() == 8\n    G = SymmetricGroup(2)\n    assert G.order() == 2\n    G = SymmetricGroup(3)\n    assert G.order() == 6\n    G = AlternatingGroup(3)\n    assert G.order() == 3\n    G = AlternatingGroup(4)\n    assert G.order() == 12\n    G = DihedralGroup(4)\n    assert G.order() == 8\n    G = AbelianGroup(2, 3)\n    assert G.order() == 6\n\n\n\n", "def test_cyclic_quotient():\n    G = SymmetricGroup(3)\n    H = G.subgroup([Permutation(0,1,2)])\n    Q = G.cyclic_quotient(H)\n    assert Q.is_cyclic and Q.order() == 2\n\n    G = PermutationGroup(Permutation(1, 2, 3), Permutation(1, 2))\n    H = G.subgroup([Permutation(1, 2)])\n    Q = G.cyclic_quotient(H)\n    assert Q.is_cyclic and Q.order() == 3\n    \n\n"], "sample_961": ["    def test_show_sourcelink(app, status, warning):\n        text = (\".. py:class:: MyClass\\n\"\n                \"   :source-link: <https://example.com/myclass.py>\\n\")\n        restructuredtext.parse(app, text)\n        app.build()\n\n        content = (app.outdir / 'index.html').read_text()\n        assert '<a class=\"source-link\" href=\"https://example.com/myclass.py\">Source</a>' in content\n\n\n\n", "    def test_param_type_list(app):\n        text = (\".. py:class:: Class\\n\"\n                \"\\n\"\n                \"   :param str name: blah blah\\n\"\n                \"   :param age: blah blah\\n\"\n                \"   :type age: int, float\\n\"\n                \n                \"   :param items: blah blah\\n\"\n                \"   :type items: Tuple[str, ...]\\n\"\n                \"   :type items: List[int]\\n\")\n        doctree = restructuredtext.parse(app, text)\n\n        assert_node(doctree, (nodes.target,\n                              addnodes.index,\n                              addnodes.index,\n                              [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                           [desc_addname, \"example.\"],\n                                           [desc_name, \"Class\"])],\n                         [desc_content, nodes.field_list, nodes.field, (nodes.field_name,\n                                [nodes.field_body, nodes.paragraph,\n                \n                ])]))\n        assert_node(doctree[3][1][0][0][1][0][0], ([nodes.paragraph,\n                ([addnodes.literal_strong, \"age\"],\n                 \" (\",\n                 [pending_xref, addnodes.literal_emphasis, \"int\"],\n                 [addnodes.literal_emphasis, \" | \"],\n                 [pending_xref, addnodes.literal_emphasis, \"float\"],\n                 \")\",\n                 \" -- \",\n                 \"blah blah\")],))\n        assert_node(doctree[3][1][0][0][1][0][0][2], pending_xref,\n                refdomain=\"py\", reftype=\"class\", reftarget=\"int\",\n                **{\"py:module\": \"example\", \"py:class\": \"Class\"})\n        assert_node(doctree[3][1][0][0][1][0][0][4], pending_xref,\n                refdomain=\"py\", reftype=\"class\", reftarget=\"float\",\n                **{\"py:module\": \"example\", \"py:class\": \"Class\"})\n\n\n\n", "def test_python_python_use_unqualified_type_names_enabled(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"n\"><span class=\"pre\">Name</span></span>' in content)\n    assert ('<span class=\"n\"><span class=\"pre\">Age</span></span>' in content)\n    assert ('<p><strong>name</strong> (<span class=\"pre\">Name</span>) \u2013 blah blah</p>' in content)\n    assert '<p><strong>age</strong> (<em>Age</em>) \u2013 blah blah</p>' in content\n", "def test_py_attribute_explicit_type(app, status, warning):\n    app.build()\n\n    app.srcdir = 'domain-py-attribute-explicit-type'\n    with app.expect_warning(category=Warning,\n                           message=\"The 'type' argument is deprecated in py:attribute; use 'annotation' instead.\"):\n        app.build()\n\n    content = (app.outdir / 'index.html').read_text()\n\n    assert ('<span class=\"p\"><a class=\"reference internal\" href=\"#SomeModule.attr\" title=\"SomeModule.attr\">'\n            '<span class=\"pre\">attr</span></a> (<a class=\"reference internal\" href=\"#int\" title=\"int\">'\n            '<em>int</em></a>) \u2013 blah blah</span>' in content)\n\n    assert ('<span class=\"p\"><a class=\"reference internal\" href=\"#SomeModule.attr2\" title=\"SomeModule.attr2\">'\n            '<span class=\"pre\">attr2</span></a> (<a class=\"reference internal\" href=\"#str\" title=\"str\">'\n            '<em>str</em></a>) \u2013 blah blah</span>' in content)\n", "def test_python_python_use_unqualified_type_names_enabled(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"n\"><span class=\"pre\">Name</span></span>' in content)\n    assert ('<p><strong>name</strong> (<span class=\"n\"><span class=\"pre\">Name</span></span>) \u2013 blah blah</p>' in content)\n\n"], "sample_689": ["def test_pytest_warning_captured_is_deprecated(testdir: Testdir) -> None:\n    testdir.makepyfile(\n        \"\"\"\n            pass\n        \"\"\"\n    )\n    with pytest.warns(deprecated.WARNING_CAPTURED_HOOK):\n        result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*pytest_warning_captured is deprecated*\"])\n\n\n", "def test_pytest_warning_captured_is_deprecated(testdir: Testdir) -> None:\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n            pytest_warning: str, caplog: pytest.CaptureFixture,\n        ) -> None:\n            pass\n        \"\"\"\n    )\n    with pytest.warns(deprecated.WARNING_CAPTURED_HOOK):\n        result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*pytest_warning_captured is deprecated*\"])\n", "def test_strict_option_is_deprecated(testdir: Testdir) -> None:\n    with pytest.warns(pytest.PytestDeprecationWarning, match=\"The `--strict` option is deprecated\"):\n        result = testdir.runpytest(\"--strict\")\n    result.stdout.fnmatch_lines([\"*The `--strict` option is deprecated*\"])\n\n", "def test_pytest_warning_captured_is_deprecated(testdir: Testdir) -> None:\n    testdir.makepyfile(\n        \"\"\"\n            # Do something with the warning\n            pass\n        \"\"\"\n    )\n\n    with pytest.warns(deprecated.WARNING_CAPTURED_HOOK) as warning:\n        testdir.runpytest()\n\n    assert warning[0].message == \"The pytest_warning_captured is deprecated and will be removed in a future release.\\nPlease use pytest_warning_recorded instead.\"\n", "def test_strict_option_is_deprecated(testdir: Testdir) -> None:\n    result = testdir.runpytest(\"--strict\")\n    result.stdout.fnmatch_lines([\"*The `--strict` option*deprecated*\"])\n    result = testdir.runpytest(\"--strict-markers\")\n    result.stdout.fnmatch_lines([])\n"], "sample_1115": ["def test_tensor_contraction():\n    L1 = TensorIndexType(\"L1\")\n    L2 = TensorIndexType(\"L2\")\n    i, j, k = tensor_indices(\"i j k\", L1)\n    A = TensorHead(\"A\", [L1, L1])\n    B = TensorHead(\"B\", [L2])\n    C = TensorHead(\"C\", [L2, L1])\n\n    expr = A(i, j) * B(k) * C(k, i)\n\n    result = expr.contract()\n    assert result ==  Sum(A(i, j) * B(k) * C(k, i), (i, 1))\n    assert repr(result) == \"Sum(A(i_0, i_1) * B(k) * C(k, i_0), (i_0, 1))\"\n\n\n", "compilation error", "compilation error", "compilation error", "def test_tensor_ordering():\n    L = TensorIndexType(\"L\")\n    i, j, k = tensor_indices(\"i j k\", L)\n\n    A = TensorHead(\"A\", [L, L])\n    B = TensorHead(\"B\", [L, L, L])\n\n    assert A(i, j).ordering == (i, j)\n    assert B(i, j, k).ordering == (i, j, k)\n\n    assert A(i,-j).ordering == (i, -j)\n    assert B(i,-j,k).ordering == (i, -j, k)\n\n    assert A(j,i).ordering == (j, i)\n    assert B(j,i,k).ordering == (j, i, k)\n"], "sample_359": ["    def test_references_field_by_through_multiple_models(self):\n        operation = FieldOperation(\n            'Model',\n            'field',\n            models.ManyToManyField(\n                'Other',\n                through='Through',\n                related_name='through_other_field',\n            )\n        )\n        self.assertIs(operation.references_field('Other', 'whatever', 'migrations'), True)\n        self.assertIs(operation.references_field('Through', 'whatever', 'migrations'), True)\n        self.assertIs(operation.references_field('Missing', 'whatever', 'migrations'), False)\n", "    def test_references_field_by_through_field(self):\n        operation = FieldOperation(\n            'Model', 'field', models.ManyToManyField('Other', through='Through', through_fields=('many_to_many_field',))\n        )\n        self.assertIs(operation.references_field('Through', 'many_to_many_field', 'migrations'), True)\n        self.assertIs(operation.references_field('Other', 'whatever', 'migrations'), True)\n        self.assertIs(operation.references_field('Missing', 'whatever', 'migrations'), False)\n", "    def test_references_field_by_through_related_name(self):\n        operation = FieldOperation('Model', 'field', models.ManyToManyField('Other', through='Through', related_name='through_relationship'))\n        self.assertIs(operation.references_field('Through', 'through_relationship', 'migrations'), True)\n        self.assertIs(operation.references_field('Other', 'through_relationship', 'migrations'), True)\n        self.assertIs(operation.references_field('Missing', 'through_relationship', 'migrations'), False)\n", "    def test_references_field_by_m2m_through_field(self):\n        operation = FieldOperation('Model', 'field', models.ManyToManyField('Other', through='Through', related_name=\"through_related_name\"))\n        self.assertIs(operation.references_field('Other', 'whatever', 'migrations'), True)\n        self.assertIs(operation.references_field('Through', 'whatever', 'migrations'), True)\n        self.assertIs(operation.references_field('Missing', 'whatever', 'migrations'), False)\n", "    def test_references_field_by_related_name(self):\n        operation = FieldOperation(\n            'Model', 'field', models.ForeignKey('Other', models.CASCADE, related_name='related_to_model')\n        )\n        self.assertIs(operation.references_field('Other', 'related_to_model', 'migrations'), True)\n        self.assertIs(operation.references_field('Other', 'whatever', 'migrations'), False)\n\n\n\n"], "sample_988": ["def test_issue_11239():\n    x = symbols('x', real=True)\n    f = symbols('f', cls=Function)\n    assert (f(x) > 0).is_comparable is True\n    assert (f(x) <= 0).is_comparable is True\n    assert (f(x) == 0).is_comparable is True\n\n\n\n", "def test_issue_13981():\n    x = symbols('x', nonzero=True)\n    assert (x/x).evalf() == 1\n    assert (x/x).simplify() == 1\n    assert (2*x/x).evalf() == 2\n    assert (2*x/x).simplify() == 2\n    assert (x/x**2).evalf() == 1/x\n    assert (x/x**2).simplify() == 1/x\n", "def test_issue_14018():\n    x = symbols('x', positive=True)\n    y = symbols('y', positive=True)\n\n    assert (x/y).is_comparable  == True\n    assert (x/y).is_strict_equivalence is S.false\n\n", "def test_issue_11117():\n    x = symbols('x', real=True)\n    assert (x > 0).as_expr() == x > 0\n    assert (x < 0).as_expr() == x < 0\n    assert (x >= 0).as_expr() == x >= 0\n    assert (x <= 0).as_expr() == x <= 0\n    assert (x == 0).as_expr() == x == 0\n    assert (x != 0).as_expr() == x != 0\n\n\n\n\n", "compilation error"], "sample_1046": ["def test_tensor_contraction():\n    L = TensorIndexType(\"L\")\n    i, j, k, l = tensor_indices(\"i j k l\", L)\n    A, B, C, D = tensorhead(\"A B C D\", [L], [[1]])\n    H = tensorhead(\"H\", [L, L, L], [[1], [1], [1]])\n\n    expr = A(i)*B(j)*C(i)*D(-j)\n    assert expr.contract(i, j) == A(i)*B(i)*C(i)*D(-i)\n    assert expr.contract(i, -i) == A(i)*B(i)*C(i)*D(i)\n    assert expr.contract(j, -j) == A(i)*B(i)*C(i)*D(i)\n\n    expr = A(i)*B(j)*C(j)*D(k)\n    assert expr.contract(j, k) == A(i)*B(j)*C(j)*D(j)\n\n    expr = H(i, j, k)\n    assert expr.contract(i, j) == H(j, j, k)\n    assert expr.contract(i, -j) == H(j, -j, k)\n    assert expr.contract(j, i) == H(i, i, k)\n\n    expr = A(i)*B(j)*C(i)*D(j)\n    assert expr.contract(i, j) != A(i)*B(i)*C(i)*D(i)\n    assert expr.contract(i, -j) != A(i)*B(i)*C(i)*D(i)\n    assert expr.contract(j, i) != A(i)*B(i)*C(i)*D(i)\n    assert expr.contract(i, k) != A(i)*B(j)*C(i)*D(k)\n    assert expr.contract(j, l) != A(i)*B(j)*C(i)*D(l)\n\n    # Test with mixed indices:\n    expr = A(i)*B(j)*C(k)*D(l)\n    assert expr", "compilation error", "compilation error", "def test_tensor_contraction_indices():\n    L = TensorIndexType('L')\n    i, j, k = tensor_indices(\"i j k\", L)\n    A, B = tensorhead(\"A B\", [L, L], [[1], [1]])\n    C = tensorhead(\"C\", [L], [[1]])\n    expr = A(i)*B(j)*C(k)\n\n    # Contraction over i\n    contract_indices = [i]\n    result = expr.contract(contract_indices)\n    assert result.indices == [j, k]\n    assert str(result) == \"B(j)*C(k)\"\n\n    # Contraction over i and j\n    contract_indices = [i, j]\n    result = expr.contract(contract_indices)\n    assert result.indices == [k]\n    assert str(result) == \"C(k)\"\n\n    # Contraction over non-existing index\n    contract_indices = [l]\n    raises(ValueError, lambda: expr.contract(contract_indices))\n\n    # Two contractions\n    expr = A(i)*B(j)*C(j)*D(k)\n    contract_indices = [j, j]\n    result = expr.contract(contract_indices)\n    assert result.indices == [i, k]\n    assert str(result) == \"A(i)*C(k)*D(k)\"\n\n\n\n", "compilation error"], "sample_61": ["    def test_ascii_validator(self):\n        valid_usernames = ['joe', 'john_doe', 'john.doe']\n        invalid_usernames = ['Jos\u00e9', '\u0418\u0432\u0430\u043d', '\u03b1\u03b2\u03b3', '\u4f60\u597d', '\u00df\u00fc\u00f1\u00f1\u00f8']\n        v = validators.ASCIIUsernameValidator()\n        for valid in valid_usernames:\n            with self.subTest(valid=valid):\n                v(valid)\n        for invalid in invalid_usernames:\n            with self.subTest(invalid=invalid):\n                with self.assertRaises(ValidationError):\n                    v(invalid)\n", "    def test_ascii_validator(self):\n        valid_usernames = ['joe', 'john.doe', 'johndoe123', '_john_doe']\n        invalid_usernames = ['\u0410\u043b\u0451\u043d\u0430', '\u2603', '\u4f60\u597d', '\u00f6\u00e4\u00fc']\n        v = validators.ASCIIUsernameValidator()\n        for valid in valid_usernames:\n            with self.subTest(valid=valid):\n                v(valid)\n        for invalid in invalid_usernames:\n            with self.subTest(invalid=invalid):\n                with self.assertRaises(ValidationError):\n                    v(invalid)\n", "    def test_ascii_validator(self):\n        valid_usernames = ['joe', 'john_doe123', 'Alice1990']\n        invalid_usernames = ['\u4f60\u597d', ' Ren\u00e9 ', '\u00e4\u00f6\u00fc', '\u4f60\u597d\\u00A0']\n        v = validators.ASCIIUsernameValidator()\n        for valid in valid_usernames:\n            with self.subTest(valid=valid):\n                v(valid)\n        for invalid in invalid_usernames:\n            with self.subTest(invalid=invalid):\n                with self.assertRaises(ValidationError):\n                    v(invalid)\n", "    def test_ascii_validator(self):\n        valid_usernames = ['joe', 'JohnDoe', 'username123']\n        invalid_usernames = ['\u4f60\u597d', '\u0391\u03c5\u03c4\u03cc', '\u00dcberschrift', '\u00f1\u00f1\u00f1']\n        v = validators.ASCIIUsernameValidator()\n        for valid in valid_usernames:\n            with self.subTest(valid=valid):\n                v(valid)\n        for invalid in invalid_usernames:\n            with self.subTest(invalid=invalid):\n                with self.assertRaises(ValidationError):\n                    v(invalid)\n", "    def test_ascii_validator(self):\n        valid_usernames = ['joe', 'john.doe', 'admin', 'user123']\n        invalid_usernames = ['\u00c9lie', '\u00c1ngel', '\u4f60\u597d', 'j\\xf1']\n        v = validators.ASCIIUsernameValidator()\n        for valid in valid_usernames:\n            with self.subTest(valid=valid):\n                v(valid)\n        for invalid in invalid_usernames:\n            with self.subTest(invalid=invalid):\n                with self.assertRaises(ValidationError):\n                    v(invalid)\n"], "sample_392": ["    def test_key_transform_with_empty_json(self):\n        obj = NullableJSONModel.objects.create(value={})\n        self.assertIsNone(\n            NullableJSONModel.objects.filter(\n                value__a=KeyTransform(\"a\", KeyTransform(\"b\", \"value\"))\n            ).first()\n        )\n        self.assertIsNone(\n            NullableJSONModel.objects.filter(\n                value__a=KeyTransform(\"a\", KeyTransform(\"b\", \"value\"))\n            ).first()\n        )\n", "    def test_contains_with_key_transform_expression(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__contains=KeyTransform(\n                    \"baz\", KeyTransform(\"x\", \"value\")\n                )\n            ),\n            [self.objs[7]],\n        )\n\n", "    def test_contains_within_list(self):\n        obj = NullableJSONModel.objects.create(value=[{\"a\": 1}, {\"b\": 2}])\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__1__contains={\"a\": 1}),\n            [obj],\n        )\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__0__contains={\"a\": 1}),\n            [obj],\n        )\n", "    def test_contains_none_value(self):\n        tests = [\n            (\"value__a\", None),\n            (\"value__d__0\", None),\n            (\"value__d__1__f\", None),\n            (\"value__baz__a\", None),\n        ]\n        for lookup, value in tests:\n            with self.subTest(lookup=lookup):\n                self.assertIs(\n                    NullableJSONModel.objects.filter(**{lookup: value}).exists(),\n                    True,\n                )\n", "    def test_contains_contained_by_with_key_transform_failed(self):\n        tests = [\n            (\"value__d__contains\", \"f\"),\n            (\"value__d__contains\", [\"f\", \"g\"]),\n            (\"value__contains\", KeyTransform(\"baz\", \"value\")),\n            (\"value__contained_by\", {\"a\": \"b\", \"c\": \"d\", \"e\": \"f\"}),\n            (\n                \"value__contained_by\",\n                KeyTransform(\n                    \"x\",\n                    RawSQL(\n                        self.raw_sql,\n                        ['{\"y\": {\"a\": \"b\", \"c\": 1, \"d\": \"e\"}}'],  # Mismatch\n                    ),\n                ),\n            ),\n        ]\n        for lookup, value in tests:\n            with self.subTest(lookup=lookup, value=value):\n                self.assertIs(\n                    not NullableJSONModel.objects.filter(**{lookup: value}).exists(),\n                    True,\n                )\n"], "sample_868": ["def test_empty_arrays(metric):\n    # Check that metrics handle empty input arrays gracefully.\n    with pytest.raises(ValueError, match=\"Input arrays are empty\"):\n        metric([], [])\n\n    with pytest.raises(ValueError, match=\"Input arrays are empty\"):\n        metric([0], [])\n    \n    with pytest.raises(ValueError, match=\"Input arrays are empty\"):\n        metric([], [0])\n\n\n\n", "def test_empty_input(metric):\n    X = []\n    for y in [None, []]:\n        with pytest.raises(ValueError):\n            metric(X, y)\n", "def test_empty_arrays(metric_name):\n    metric = SUPERVISED_METRICS[metric_name]\n    with pytest.raises(ValueError):\n        metric([], [])\n    with pytest.raises(ValueError):\n        metric(np.array([]), np.array([0]))\n    with pytest.raises(ValueError):\n        metric(np.array([0]), np.array([]))\n", "def test_perfect_flip(metric_name, y_true, y_pred, expected_result):\n    metric = SUPERVISED_METRICS[metric_name]\n    assert metric(y_true, y_pred) == expected_result\n", "def test_empty_input(metric):\n    X = np.array([])\n    y = np.array([])\n    with pytest.raises(ValueError):\n        metric(X, y) \n"], "sample_830": ["def test_blas_info():\n    blas_info = _get_blas_info()\n\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info \n", "def test_show_versions():\n    show_versions()\n", "def test_show_versions():\n    show_versions()\n    # Assertions should be added based on the expected output of show_versions()\n", "def test_show_versions():\n    show_versions()\n    \n    # assert that each section was printed\n    import re\n    from io import StringIO\n    \n    captured_output = StringIO()\n    \n    print_to_string = lambda text: print(text, file=captured_output)\n    with open(sys.stdout.fileno(), 'w') as original_stdout:\n        sys.stdout = captured_output\n        show_versions()\n        sys.stdout = original_stdout\n    \n    captured_content = captured_output.getvalue()\n    assert 'System:' in captured_content\n    assert 'BLAS:' in captured_content\n    assert 'Python deps:' in captured_content\n", "def test_show_versions():\n    show_versions()\n    \n    # These assertions are not exhaustive and depend on the user's system.\n    # Adjust them based on your expected output format.\n    assert 'System:' in sys.stdout.getvalue()\n    assert 'Python:' in sys.stdout.getvalue()\n    assert 'BLAS:' in sys.stdout.getvalue()\n    assert 'Python deps:' in sys.stdout.getvalue()\n"], "sample_946": ["def test_py_alias_definition(app, status, warning):\n    app.config.add_data_files({'modules.txt': 'import module_alias as ma'})\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert '<span class=\"n\">ma</span>' in content\n    assert '<span class=\"o\">module_alias</span>' in content\n", "def test_python_python_use_unqualified_type_names_enabled(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"n\"><a class=\"reference internal\" href=\"#Name\" title=\"Name\">'\n            '<span class=\"pre\">Name</span></a></span>' in content)\n    assert '<span class=\"n\"><span class=\"pre\">Age</span></span>' in content \n", "def test_python_python_use_unqualified_type_names_enabled(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"n\"><span class=\"pre\">Name</span></span>' in content)\n    assert ('<span class=\"n\"><span class=\"pre\">Age</span></span>' in content)\n", "def test_alias(app):\n    text = (\".. py:alias:: alias_name\\n\"\n            \"   :py:class:: class_name\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                    [desc_name, \"alias_name\"])],\n                                  [desc_content, desc_content])]))\n    assert 'alias_name' in domain.objects\n    assert domain.objects['alias_name'] == ('index', 'alias_name', 'class', False)\n", "def test_index_inheritance(app):\n    text = (\n        \".. py:class:: Base\\n\"\n        \"   :noindexentry:\\n\"\n        \"\\n\"\n        \".. py:class:: Sub\\n\"\n        \"   :inherits: Base\\n\"\n    )\n    restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    assert index.generate() == (\n        [('s', [IndexEntry('Sub', 1, 'index', 'module-Sub', '', '', '')])],\n        False\n    )\n\n"], "sample_207": ["    def test_key_transform_complex_lookup(self):\n        tests = (\n            (\n                'value__baz__contains',\n                KeyTextTransform('a', KeyTextTransform('b', 'value')),\n            ),\n            ('value__baz__d__startswith', 'a'),\n            ('value__baz__d__iexists', True),\n        )\n        for lookup, value in tests:\n            with self.subTest(lookup=lookup):\n                self.assertIs(NullableJSONModel.objects.filter(\n                    **{lookup: value},\n                ).exists(), True)\n", "    def test_key_transform_and_contains(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__baz__contains=KeyTransform('a', 'value')),\n            [self.objs[7]],\n        )", "    def test_key_transform_with_null(self):\n        obj = NullableJSONModel.objects.create(value={'a': None})\n        self.assertIs(NullableJSONModel.objects.filter(\n            value__a=KeyTransform('a', KeyTextTransform('a', 'value')).as_expression()\n        ).exists(), True)\n\n\n\n", "    def test_key_transform_with_expression(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__c__gt=KeyTransform('1', KeyTransform('d', 'value')),\n            ),\n            [self.objs[3], self.objs[4]],\n        )\n", "    def test_empty_object(self):\n        obj = NullableJSONModel.objects.create(value={})\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__isnull=False),\n            [obj],\n        )\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__a=None),\n            [obj],\n        )\n\n"], "sample_649": ["        def test_logger_propagation_to_root(caplog):\n            with caplog.at_level(logging.DEBUG):\n                logging.getLogger('root').warning('some root logger message')\n                logging.getLogger('test').debug('some test logger message')\n                assert len(caplog.record_tuples) == 2\n                assert caplog.record_tuples[0][0] == 'root'\n                assert caplog.record_tuples[0][2] == 'some root logger message'\n                assert caplog.record_tuples[1][0] == 'test'", "        def test_logger_propagation_to_parent_with_disable_in_test(caplog):\n            logging.getLogger(\"parent\").setLevel(logging.WARNING)\n            logging.getLogger(\"parent.child\").setLevel(logging.ERROR)\n            with caplog.at_level(logging.DEBUG):\n                logging.getLogger(\"parent.child\").warning(\"some child logger message\")\n                logging.getLogger(\"parent\").warning(\"some parent logger message\")\n                assert len(caplog.record_tuples) == 2\n                assert caplog.record_tuples[0][0] == \"parent\"\n                assert caplog.record_tuples[0][2] == \"some parent logger message\"\n                assert caplog.record_tuples[1][0] == \"parent.child\"\n                assert caplog.record_tuples[1][2] == \"some child logger message\"", "compilation error", "def test_disable_loggers_inheritance_chain(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import os\n\n        logger1 = logging.getLogger('logger1')\n        logger2 = logging.getLogger('logger2')\n        logger3 = logger2.getChild('logger3')\n\n            with caplog.at_level(logging.DEBUG):\n                logger1.warning(\"logger1 message\")\n                logger2.warning(\"logger2 message\")\n                logger3.warning(\"logger3 message\")\n                assert len(caplog.record_tuples) == 3\n                assert caplog.record_tuples[0][0] == 'logger1'\n                assert caplog.record_tuples[1][0] == 'logger2'\n                assert caplog.record_tuples[2][0] == 'logger3'\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--log-disable=logger2\", \"-s\")\n    assert result.ret == ExitCode.OK\n    assert len(caplog.record_tuples) == 2\n    assert caplog.record_tuples[0][0] == 'logger1'\n    assert caplog.record_tuples[1][0] == 'logger3'\n", "def test_disable_loggers_from_module(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import os\n        import my_module\n        \n            with caplog.at_level(logging.DEBUG):\n                my_module.my_logger.warning(\"some message from module\")\n                assert caplog.record_tuples == []\n    \n    \"\"\"\n    )\n    testdir.makeconftest(\n        \"\"\"\n        import logging\n        my_module = os.path.join(os.path.dirname(__file__), \"my_module.py\") \n        module_logger = logging.getLogger(\"my_module\")\n        \n            config.addinivalue_line('logging', 'disable', 'my_module')\n    \"\"\"\n    ).copy(\"my_module.py\")\n    result = testdir.runpytest(\"--log-disable=my_module\", \"-s\")\n    assert result.ret == ExitCode.OK\n    assert not result.stderr.lines\n"], "sample_1052": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_266": ["    def test_load_invalid_pyc(self):\n        \"\"\"\n        Ensure that loading .pyc files with invalid content raises an exception.\n        \"\"\"\n        with self.temporary_migration_module(module='migrations.test_migrations') as migration_dir:\n            # Compile .py files to .pyc files and delete .py files.\n            compileall.compile_dir(migration_dir, force=True, quiet=1, legacy=True)\n            for name in os.listdir(migration_dir):\n                if name.endswith('.py'):\n                    os.remove(os.path.join(migration_dir, name))\n            # Modify the .pyc file to be invalid (e.g., remove the magic number)\n            with open(os.path.join(migration_dir, 'migrations\\\\0001_initial.pyc'), 'wb') as f:\n                f.write(b'invalid_magic_number')\n            loader = MigrationLoader(connection)\n            with self.assertRaises(ImportError):\n                loader.load_disk()\n", "    def test_unmigrated_dependencies(self):\n        loader = MigrationLoader(connection)\n        with self.assertRaisesMessage(AmbiguityError, \"Migration 'migrations.0001_initial' depends on nonexistent node ('migrations', 'migrations\\\\.0002_second')\"):\n            loader.build_graph()\n", "    def test_no_migration(self):\n        \"\"\"\n        If a migration directory exists but contains no migrations, it should be\n        ignored.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        loader.load_disk()\n        self.assertEqual(loader.disk_migrations, [])\n\n", "    def test_corrupted_pyc(self):\n        with self.temporary_migration_module(module='migrations.test_migrations') as migration_dir:\n            # Create a corrupted .pyc file\n            with open(os.path.join(migration_dir, '0001_initial.pyc'), 'wb') as f:\n                f.write(b'This is not valid python bytecode')\n            loader = MigrationLoader(connection)\n            with self.assertRaises(ImportError):\n                loader.load_disk()\n", "    def test_missing_auto_migration(self):\n        \"\"\"\n        Tests that MigrationLoader doesn't raise an error when\n        a migration doesn't contain an 'auto' migration\n        \"\"\"\n        loader = MigrationLoader(connection)\n        loader.load_disk()\n        self.assertIn(('migrations', '0002_second'), loader.disk_migrations)\n\n\n\n"], "sample_810": ["def test_pipeline_with_sparse_data():\n    iris = load_iris()\n    X = sparse.csr_matrix(iris.data)\n    y = iris.target\n\n    # Test with a transformer and SVC on sparse data\n    transf = DummyTransf()\n    clf = SVC(gamma='scale', probability=True, random_state=0)\n    pipe = Pipeline([('transf', clone(transf)), ('svc', clf)])\n    pipe.fit(X, y)\n    predicted_y = pipe.predict(X)\n\n    assert_array_equal(predicted_y, clf.predict(transf.fit_transform(X)))\n", "def test_pipeline_nested_pipeline_memory():\n    cachedir = mkdtemp()\n    if LooseVersion(joblib_version) < LooseVersion('0.12'):\n        # Deal with change of API in joblib\n        memory = Memory(cachedir=cachedir, verbose=10)\n    else:\n        memory = Memory(location=cachedir, verbose=10)\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n\n    clf = SVC(gamma='scale', probability=True, random_state=0)\n    transf = DummyTransf()\n    nested_pipe = Pipeline([\n        ('outer', Pipeline([('inner', transf), ('svc', clf)], memory=memory))\n    ])\n\n    cached_nested_pipe = Pipeline([\n        ('outer', Pipeline([('inner', transf), ('svc', clf)], memory=memory))\n    ], memory=memory)\n\n    cached_nested_pipe.fit(X, y)\n    nested_pipe.fit(X, y)\n    assert_array_equal(nested_pipe.predict(X), cached_nested_pipe.predict(X))\n\n    # Cache should be accessed only for the outer pipeline\n    assert cached_nested_pipe.named_steps['outer'].timestamp_ is not None\n    assert not hasattr(nested_pipe.named_steps['outer'].named_steps['inner'], 'means_')\n\n    shutil.rmtree(cachedir)\n", "    def test_pipeline_memory_errors():\n        cachedir = mkdtemp()\n        try:\n            if LooseVersion(joblib_version) < LooseVersion('0.12'):\n                # Deal with change of API in joblib\n                memory = Memory(cachedir=cachedir, verbose=10)\n            else:\n                memory = Memory(location=cachedir, verbose=10)\n\n            # Test invalid memory argument types\n            assert_raises_regex(TypeError,\n                                \"'memory' should be None, a string or\"\n                                \" have the same interface as joblib.Memory.\"\n                                \" Got memory='1' instead.\",\n                                Pipeline, DummyTransf(), SVC(), memory=1)\n            assert_raises_regex(TypeError,\n                                \"'memory' should be None, a string or\"\n                                \" have the same interface as joblib.Memory.\"\n                                \" Got memory='abc' instead.\",\n                                Pipeline, DummyTransf(), SVC(), memory='abc')\n\n            # Test memory with incompatible joblib version\n            joblib.__version__ = '0.11.1'\n            try:\n                with assert_raises_regex(AttributeError,\n                                         \"Memory object has no attribute: 'location'\"):\n                    Pipeline(DummyTransf(), SVC(), memory=memory)\n            finally:\n                joblib.__version__ = joblib_version\n\n        finally:\n            shutil.rmtree(cachedir) \n", "def test_pipeline_with_memory_none():\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    pipe = Pipeline([('dummy', DummyTransf()), ('svc', SVC())], memory=None)\n    pipe.fit(X, y)\n    # Check that the transformed features are not cached\n    assert not hasattr(pipe.named_steps['dummy'], 'cache_')\n    assert hasattr(pipe.named_steps['svc'], 'cache_')\n", "def test_pipeline_with_invalid_memory():\n    # Test that an error is raised when using a non-existent cache directory\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n\n    cachedir = 'nonexistent_cache_dir'\n    try:\n        memory = Memory(location=cachedir)\n        # Define an invalid cache directory\n        clf = SVC(gamma='scale', probability=True, random_state=0)\n        transf = DummyTransf()\n        pipe = Pipeline([('transf', transf), ('svc', clf)],\n                        memory=memory)\n        assert_raises_regex(FileNotFoundError,\n                            \"Error during memory initialization: \"\n                            \"The directory 'nonexistent_cache_dir'\"\n                            \" does not exist\", pipe.fit, X, y)\n    except FileNotFoundError:\n        pass\n\n\n"], "sample_598": ["compilation error", "    def test_format_array_repr_large_array(self):\n        da = xr.DataArray(np.random.randn(1000, 1000))\n        max_width = 80\n        actual = formatting.format_array_repr(da, max_width=max_width)\n\n        assert len(actual.splitlines()) < 10\n\n        assert \"array(...)\" in actual\n", "compilation error", "compilation error", "compilation error"], "sample_147": ["    def test_union_with_joins(self):\n        ReservedName.objects.create(name='a', order=1)\n        qs1 = Number.objects.select_related('reserved_name').filter(num=1)\n        qs2 = Number.objects.select_related('reserved_name').filter(num=2)\n        self.assertEqual(\n            list(qs1.union(qs2).values_list('num', 'reserved_name__name')),\n            [(1, 'a'), (2, None)],\n        )\n\n\n", "    def test_union_with_aggregate_function(self):\n        qs1 = Number.objects.filter(num__lte=1).annotate(count=Count('id'))\n        qs2 = Number.objects.filter(num__gte=2, num__lte=3).annotate(count=Count('pk'))\n        self.assertEqual(qs1.union(qs2).values_list('count').get(), (1,))\n        self.assertEqual(qs2.union(qs1).values_list('count').get(), (1,))\n", "    def test_union_with_distinct_and_order_by(self):\n        qs1 = Number.objects.values('num').distinct()\n        qs2 = Number.objects.values('num')\n        self.assertNumbersEqual(list(qs1.union(qs2).order_by('num')), list(range(10)))\n\n", "    def test_union_with_distinct_and_ordering(self):\n        ReservedName.objects.create(name='a', order=2)\n        ReservedName.objects.create(name='a', order=3)\n        qs1 = ReservedName.objects.all()\n        qs2 = ReservedName.objects.filter(order=2)\n        self.assertCountEqual(list(qs1.union(qs2, all=True).distinct().order_by('name', 'order')), [\n            ('a', 2),\n            ('a', 3),\n        ])\n", "    def test_union_with_distinct_and_ordering(self):\n        qs1 = Number.objects.filter(num__lte=1).values('num').distinct()\n        qs2 = Number.objects.filter(num__gte=2, num__lte=3).values('num')\n        self.assertEqual(\n            list(qs1.union(qs2).order_by('num')), [1, 2, 3],\n        )\n\n"], "sample_157": ["    def test_clone_test_db(self, mocked_migrate, mocked_ensure_connection):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST']['MIGRATE'] = True\n        creation = test_connection.creation_class(test_connection)\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_clone_test_db'):\n                creation.clone_test_db('suffix', verbosity=0, autoclobber=True)\n            mocked_migrate.assert_called_once()\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0)\n", "    def test_clone_test_db(self, mocked_migrate, mocked_ensure_connection):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST'] = {'NAME': 'test_hodor'}\n        creation = test_connection.creation_class(test_connection)\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            creation.clone_test_db(suffix='_clone')\n            new_settings = creation.get_test_db_clone_settings('_clone')\n            self.assertEqual(new_settings['NAME'], 'test_hodor_clone')\n        finally:\n            creation.destroy_test_db(old_database_name)  \n", "    def test_clone_test_db_suffix(self, mocked_migrate, mocked_ensure_connection):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST'] = {'NAME': 'hodor'}\n        creation = test_connection.creation_class(test_connection)\n        new_suffix = '2'\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            creation.clone_test_db(new_suffix, verbosity=0, autoclobber=True)\n            new_settings = creation.get_test_db_clone_settings(new_suffix)\n            self.assertEqual(new_settings['NAME'], 'hodor_2')\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0)\n\n\n", "    def test_clone_test_db(self, mocked_migrate, mocked_ensure_connection):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST']['MIGRATE'] = True\n        original_name = test_connection.settings_dict['NAME']\n        creation = test_connection.creation_class(test_connection)\n        creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n        try:\n            suffix = '2'\n            creation.clone_test_db(suffix, verbosity=0)\n            cloned_settings = creation.get_test_db_clone_settings(suffix)\n            self.assertNotEqual(cloned_settings['NAME'], original_name)\n        finally:\n            creation.destroy_test_db(original_name, verbosity=0)\n", "    def test_clone_test_db(self, mocked_migrate, mocked_ensure_connection):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST']['MIGRATE'] = True\n        creation = test_connection.creation_class(test_connection)\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_clone_test_db'):\n                creation.clone_test_db('suffix', verbosity=0, autoclobber=True)\n            mocked_migrate.assert_called_once()\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0)\n"], "sample_466": ["    def test_serialize_custom_field(self):\n        class MyCustomField(models.Field):\n                super().__init__(*args, **kwargs)\n\n                return \"MyCustomType\"\n\n                return \"CharField\"\n\n                return value\n\n        class MyCustomModel(models.Model):\n            field = MyCustomField(max_length=50)\n\n        with self.assertRaises(\n            ValueError,\n            msg=\"Could not serialize field of type 'migrations.test_writer.MyCustomField'\",\n        ):\n            MigrationWriter.serialize(MyCustomModel)\n", "    def test_serialize_custom_managers_with_options(self):\n        class FoodManager(models.Manager):\n                return super().get_query_set().filter(category=\"veg\")\n\n        class Food(models.Model):\n            name = models.CharField(max_length=100)\n            category = models.CharField(max_length=100)\n\n            objects = FoodManager()\n\n        string, imports = MigrationWriter.serialize(Food)\n        self.assertEqual(\n            string,\n            \"class Food(models.Model):\\n\"\n            \"    name = models.CharField(max_length=100)\\n\"\n            \"    category = models.CharField(max_length=100)\\n\"\n            \"\\n\"\n            \"    class Meta:\\n\"\n            \"        managed = True\\n\"\n            \"\\n\"\n            \"    objects = migrations.models.Manager()\\n\",\n        )\n        self.assertEqual(imports, {\"from django.db import models\"})\n\n\n", "    def test_serialize_enum_from_string(self):\n        class E(models.TextChoices):\n            A = \"a\", \"A value\"\n            B = \"b\", \"B value\"\n        string = MigrationWriter.serialize(E.A)[0]\n        self.assertEqual(string, \"'migrations.test_writer.E.A'\")\n        self.assertSerializedResultEqual(\n            E(\"a\"),\n            (\"migrations.test_writer.E.A\", {\"import migrations.test_writer\"}),\n        )", "    def test_serialize_custom_field(self):\n        class CustomField(models.Field):\n            description = \"My own custom field\"\n\n                super().__init__(*args, **kwargs)\n\n                return (\n                    self.__class__.__name__,\n                    self.attrs,\n                    None,  # No path because not in a module\n                )\n\n                pass\n\n                pass\n\n                return value\n\n                return \"CharField\"\n\n                return value\n\n        field = CustomField()\n        string = MigrationWriter.serialize(field)[0]\n        self.assertEqual(string, \"migrations.test_writer.CustomField(attrs={})\")\n\n", "    def test_deserialize_custom_type(self):\n        class CustomType(object):\n                self.value = value\n\n        serialized = (\n            \"complex('CustomType(value=42)')\",\n            {},\n        )\n        deserialized = MigrationWriter.deserialize(serialized)\n        self.assertIsInstance(deserialized, CustomType)\n        self.assertEqual(deserialized.value, 42)\n"], "sample_386": ["    def test_mark_safe_unicode_string(self):\n        # Test that mark_safe handles unicode strings correctly.\n        unicode_string = \"\\u03B1\\u03B2\"\n        safe_string = mark_safe(unicode_string)\n        self.assertRenderEqual(\"{{ safe_string }}\", unicode_string, safe_string=safe_string)\n", "    def test_mark_safe_with_invalid_input(self):\n        with self.assertRaises(TypeError):\n            mark_safe(123)\n\n        with self.assertRaises(TypeError):\n            mark_safe(True)\n\n        with self.assertRaises(TypeError):\n            mark_safe([1, 2, 3])\n\n        with self.assertRaises(TypeError):\n            mark_safe({\"a\": 1, \"b\": 2})\n", "compilation error", "    def test_mark_safe_decorator_with_arguments(self):\n            return f\"<{text}>\"\n\n        decorated_add_to_string = mark_safe(add_to_string)\n        self.assertEqual(decorated_add_to_string(\"safe\"), \"<safe>\")\n", "    def test_mark_safe_decorator_with_callable_returning_lazy(self):\n            return lazystr(\"a&b\")\n\n        wrapped_func = mark_safe(lazy_string_factory)\n        self.assertIsInstance(wrapped_func(), Promise)\n        self.assertRenderEqual(\"{{ s }}\", \"a&b\", s=wrapped_func())\n"], "sample_1002": ["compilation error", "def test_issue_10610():\n    assert Float(1)**0 == 1\n    assert Float(0)**0 == 1\n    assert Float(-1)**0 == 1\n    assert Float(0.0)**0 == 1\n    assert Float(-0.0)**0 == 1\n    assert Float(Float('inf')).__pow__(0) == 1\n    assert Float(Float('-inf')).__pow__(0) == 1\n", "def test_int_to_complex():\n    assert int_to_complex(5) == 5 + 0j\n    assert int_to_complex(-5) == -5 + 0j\n    assert int_to_complex(0) == 0 + 0j\n    assert int_to_complex(1234567890) == 1234567890 + 0j\n\n\n\n", "def test_issue_10975():\n    assert  Float('-inf', 10) == Float('-inf') \n    assert Float('inf', 10) == Float('inf')\n    assert Float('nan', 10) == Float('nan') \n", "def test_issue_10821():\n    assert Float('1e+100').as_real() == Float('1e+100')\n    assert Float('1e-100').as_real() == Float('1e-100')\n"], "sample_148": ["    def test_quote(self):\n        self.assertEqual(quote(\"This is a test\"), \"This is a test\")\n        self.assertEqual(quote(123), \"123\")\n        self.assertEqual(quote(True), \"True\")\n        self.assertEqual(quote(None), \"None\")\n        self.assertEqual(quote(datetime.now()), repr(datetime.now()))  \n", "    def test_display_for_value_null(self):\n        self.assertEqual(\n            display_for_value(None, self.empty_value),\n            self.empty_value\n        )\n\n\n\n", "    def test_quote(self):\n        self.assertEqual(quote(\"\"), '')\n        self.assertEqual(quote(\"a\"), \"'a'\")\n        self.assertEqual(quote(\"ab'c\"), \"'ab'\\\\'c'\")\n        self.assertEqual(quote(\"a'b\\\"c\"), \"'a'\\\\'b\\\\\\\"c'\")\n\n\n\n", "    def test_quote(self):\n        self.assertEqual(quote('single'), \"single\")\n        self.assertEqual(quote('double \"quote\"'), 'double \"quote\"')\n        self.assertEqual(quote(\"single 'quote'\"), \"single 'quote'\")\n        self.assertEqual(quote(\"a \\\"quoted\\\" string\"), 'a \\\"quoted\\\" string')\n\n", "    def test_quote(self):\n        self.assertEqual(quote('abc'), '\"abc\"')\n        self.assertEqual(quote('a\"b\\'c'), '\"a\\\\\"b\\\\\\'c\"')"], "sample_372": ["    def test_lookahead_and_lookbehind_together(self):\n        test_urls = [\n            ('/lookahead-lookbehind-/a-city/', {'city': 'a-city'}),\n            ('/lookahead-lookbehind+/a-city/', {'city': 'a-city'}),\n        ]\n        for test_url, kwargs in test_urls:\n            with self.subTest(url=test_url):\n                self.assertEqual(resolve(test_url).kwargs, kwargs)\n", "    def test_lookbehind_lookahead_mixed(self):\n        test_urls = [\n            ('/lookbehind-/lookahead+/a-city/', {'city': 'a-city'}),\n            ('/lookahead+/lookbehind-/a-city/', {'city': 'a-city'}),\n            ('/lookbehind+/lookbehind+/a-city/', {'city': 'a-city'}),\n            ('/lookahead-/lookahead-/a-city/', {'city': 'a-city'}),\n        ]\n        for url, kwargs in test_urls:\n            with self.subTest(url=url):\n                self.assertEqual(resolve(url).kwargs, kwargs)\n", "    def test_lookaround_kwargs_order(self):\n        test_urls = [\n            ('lookahead-positive', {'city': 'a-city', 'extra': 'value'}, '/lookahead+/a-city/extra/'),\n            ('lookahead-negative', {'extra': 'value', 'city': 'a-city'}, '/lookahead-/a-city/extra/'),\n        ]\n        for name, kwargs, expected in test_urls:\n            with self.subTest(name=name, kwargs=kwargs):\n                self.assertEqual(reverse(name, kwargs=kwargs), expected)\n", "    def test_lookahead_and_lookbehind_together(self):\n        test_urls = [\n            ('/lookahead-lookbehind-positive', {'city': 'a-city'}, '/lookahead+/lookbehind+/a-city/'),\n            ('/lookahead-lookbehind-negative', {'city': 'a-city'}, '/lookahead-/lookbehind-/a-city/'),\n        ]\n        for url, kwargs, expected in test_urls:\n            with self.subTest(url=url, kwargs=kwargs):\n                self.assertEqual(resolve(url, kwargs=kwargs).kwargs, {'city': 'a-city'})\n                self.assertEqual(reverse(url, kwargs=kwargs), expected)\n\n", "    def test_lookahead_greedy_non_capturing(self):\n        test_urls = [\n            ('/lookahead-?+a-city/', {'city': 'a-city'}, '/lookahead-?+a-city/'),\n        ]\n        for test_url in test_urls:\n            with self.subTest(url=test_url):\n                self.assertEqual(resolve(test_url).kwargs, {'city': 'a-city'})\n"], "sample_363": ["    def test_remove_selected_options(self):\n        from selenium.webdriver.common.by import By\n        self.admin_login(username='super', password='secret', login_url='/')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_event_add'))\n        main_window = self.selenium.current_window_handle\n\n        # Initial state: no band selected\n        self.assertEqual(self.selenium.find_element(By.ID, 'id_main_band').get_attribute('value'), '')\n\n        # Select a band\n        self.selenium.find_element(By.ID, 'lookup_id_main_band').click()\n        self.wait_for_and_switch_to_popup()\n        link = self.selenium.find_element(By.LINK_TEXT, 'Bogey Blues')\n        self.assertIn('/band/42/', link.get_attribute('href'))\n        link.click()\n\n        # The field now contains the selected band's id\n        self.selenium.switch_to.window(main_window)\n        self.wait_for_value('#id_main_band', '42')\n\n        # Open the popup window again\n        self.selenium.find_element(By.ID, 'lookup_id_main_band').click()\n        self.wait_for_and_switch_to_popup()\n\n        # Find the \"Remove\" button for the previously selected band\n        remove_button = self.selenium.find_element(By.CSS_SELECTOR, 'a.remove-option')\n        remove_button.click()\n\n        # The field should now be empty again\n        self.selenium.switch_to.window(main_window)\n        self.assertEqual(self.selenium.find_element(By.ID, 'id_main_band').get_attribute('value'), '')\n", "    def test_clear_button(self):\n        from selenium.webdriver.common.by import By\n        self.admin_login(username='super', password='secret', login_url='/')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_event_add'))\n        main_window = self.selenium.current_window_handle\n\n        # Select a band\n        self.selenium.find_element(By.ID, 'lookup_id_main_band').click()\n        self.wait_for_and_switch_to_popup()\n        link = self.selenium.find_element(By.LINK_TEXT, 'Bogey Blues')\n        self.assertIn('/band/42/', link.get_attribute('href'))\n        link.click()\n\n        # The field now contains the selected band's id\n        self.selenium.switch_to.window(main_window)\n        self.wait_for_value('#id_main_band', '42')\n\n        # Click the clear button\n        clear_button = self.selenium.find_element(By.CSS_SELECTOR, '#id_main_band .clear')\n        clear_button.click()\n\n        # The field is empty again\n        self.assertEqual(self.selenium.find_element(By.ID, 'id_main_band').get_attribute('value'), '')\n\n\n", "    def test_clear_button(self):\n        from selenium.webdriver.common.by import By\n        self.admin_login(username='super', password='secret', login_url='/')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_event_add'))\n        main_window = self.selenium.current_window_handle\n\n        # No value has been selected yet\n        self.assertEqual(self.selenium.find_element(By.ID, 'id_main_band').get_attribute('value'), '')\n\n        # Simulate selecting a band\n        self.selenium.find_element(By.ID, 'lookup_id_main_band').click()\n        self.wait_for_and_switch_to_popup()\n        link = self.selenium.find_element(By.LINK_TEXT, 'Bogey Blues')\n        self.assertIn('/band/42/', link.get_attribute('href'))\n        link.click()\n        self.selenium.switch_to.window(main_window)\n        self.wait_for_value('#id_main_band', '42')\n\n        # Click the clear button\n        self.selenium.find_element(By.ID, 'clear_id_main_band').click()\n\n        # The field should be empty again\n        self.assertEqual(self.selenium.find_element(By.ID, 'id_main_band').get_attribute('value'), '')\n", "    def test_many_to_many_multiple_selections(self):\n        from selenium.webdriver.common.by import By\n        self.admin_login(username='super', password='secret', login_url='/')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_event_add'))\n        main_window = self.selenium.current_window_handle\n\n        # No value has been selected yet\n        self.assertEqual(self.selenium.find_element(By.ID, 'id_supporting_bands').get_attribute('value'), '')\n\n        # Open the popup window and click on multiple bands\n        self.selenium.find_element(By.ID, 'lookup_id_supporting_bands').click()\n        self.wait_for_and_switch_to_popup()\n        self.selenium.find_elements(By.LINK_TEXT, 'Bogey Blues')[0].click()\n        self.selenium.find_elements(By.LINK_TEXT, 'Green Potatoes')[0].click()\n\n        # The field now contains the selected bands' ids\n        self.selenium.switch_to.window(main_window)\n        self.wait_for_value('#id_supporting_bands', '42,98')\n\n", "    def test_empty_values(self):\n        from selenium.webdriver.common.by import By\n        self.admin_login(username='super', password='secret', login_url='/')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_event_add'))\n\n        # Check that the field is empty if no value is selected (no bands in the\n        # 'field-main_band' element)\n        self.assertEqual(self.selenium.find_element(By.ID, 'id_main_band').get_attribute('value'), '')\n\n        # Check that the field will display as empty in the form\n        main_window = self.selenium.current_window_handle\n        self.selenium.find_element(By.ID, 'lookup_id_main_band').click()\n        self.wait_for_and_switch_to_popup()\n        self.selenium.close()\n        self.selenium.switch_to.window(main_window)\n        self.assertEqual(self.selenium.find_element(By.ID, 'id_main_band').get_attribute('value'), '')\n\n        # Check that the field is empty if no values are selected for the\n        # 'supporting_bands' field (empty string)\n        self.assertEqual(self.selenium.find_element(By.ID, 'id_supporting_bands').get_attribute('value'), '')\n\n        #  Check that the help text for empty fields is displayed\n        self.assertEqual(\n            self.selenium.find_element(By.CSS_SELECTOR, '.field-main_band div.help').text,\n            'Main Band.'\n        )\n        self.assertEqual(\n            self.selenium.find_element(By.CSS_SELECTOR, '.field-supporting_bands div.help').text,\n            'Supporting Bands.'\n        )\n"], "sample_383": ["    def test_ticket_24605_ordering(self):\n        \"\"\"\n        Ensure ordering works correctly in queries with subqueries.\n        \"\"\"\n        School.objects.create(name=\"Sc1\")\n        School.objects.create(name=\"Sc2\")\n        Student.objects.create(name=\"St1\", school=School.objects.get(name=\"Sc1\"))\n        Student.objects.create(name=\"St2\", school=School.objects.get(name=\"Sc2\"))\n        Student.objects.create(name=\"St3\", school=School.objects.get(name=\"Sc1\"))\n        qs = (\n            Student.objects.filter(\n                school__name__in=School.objects.filter(\n                    name__startswith=\"Sc\"\n                ).values_list(\"name\", flat=True)\n            )\n            .order_by(\"name\")\n        )\n        self.assertSequenceEqual(\n            list(qs),\n            [\n                Student.objects.get(name=\"St1\"),\n                Student.objects.get(name=\"St2\"),\n                Student.objects.get(name=\"St3\"),\n            ],\n        )\n", "    def test_ticket_24605_complex_filter(self):\n        \"\"\"\n        Subquery table names should be quoted in complex filter scenarios\n        \"\"\"\n        i1 = Individual.objects.create(alive=True)\n        i2 = Individual.objects.create(alive=False)\n        i3 = Individual.objects.create(alive=True)\n        RelatedIndividual.objects.create(related=i1)\n        RelatedIndividual.objects.create(related=i2)\n        RelatedIndividual.objects.create(related=i3)\n        q = Q(related_individual__isnull=False) & Q(\n            related_individual__related__alive=True\n        )\n        self.assertSequenceEqual(Individual.objects.filter(q), [i1, i3])\n\n\n", "    def test_ticket_24605_with_pre_existing_alias(self):\n        i1 = Individual.objects.create(alive=True)\n        RelatedIndividual.objects.create(related=i1)\n        i2 = Individual.objects.create(alive=False)\n        RelatedIndividual.objects.create(related=i2)\n        i3 = Individual.objects.create(alive=True)\n\n        qs = Individual.objects.filter(\n            Q(alive=False) | Q(related_individual__isnull=True, pk=i3.pk)\n        )\n        self.assertSequenceEqual(qs, [i2, i3])  \n", "    def test_ticket_24605_complex(self):\n        \"\"\"\n        Test for complex subqueries with joins and ordering.\n\n        This test is inspired by ticket #24605 which highlighted issues\n        with quoting table aliases in complex subqueries.\n        \"\"\"\n        for _ in range(3):  # Create some data\n            Individual.objects.create(\n                name=f\"Name {_.randint(1, 10)}\", alive=True\n            )\n        for _ in range(2):\n            Individual.objects.create(\n                name=f\"Name {_.randint(1, 10)}\", alive=False\n            )\n\n        # Subqueries with joins and ordering\n        qs = Individual.objects.filter(\n            Q(alive=False)\n            & Q(\n                related_individual__isnull=False\n            )\n        ).order_by(\"related_individual__related__name\").values(\"name\")\n        self.assertQuerysetEqual(qs, [\"Name 1\", \"Name 2\"])\n\n\n\n", "    def test_ticket_24605_subquery_ordering(self):\n        \"\"\"Test for ticket 24605: ordering should work correctly in subquery\"\"\"\n        i1 = Individual.objects.create(alive=True, name=\"i1\")\n        i2 = Individual.objects.create(alive=False, name=\"i2\")\n        i3 = Individual.objects.create(alive=True, name=\"i3\")\n\n        RelatedIndividual.objects.create(related=i1)\n        RelatedIndividual.objects.create(related=i2)\n        RelatedIndividual.objects.create(related=i3)\n\n        qs = Individual.objects.filter(\n            Q(alive=True),\n            Q(related_individual__name__startswith=\"i\")\n        ).order_by(\"name\")\n        self.assertSequenceEqual(\n            qs, [i1, i3]\n        )\n"], "sample_328": ["    def test_jsonfield_nullable(self):\n        json_fields = [JSONFieldNullable.objects.create(\n            json_data={'key': 'value'}\n        ) for _ in range(10)]\n        for json_field in json_fields:\n            json_field.json_data = None\n        JSONFieldNullable.objects.bulk_update(json_fields, ['json_data'])\n        self.assertCountEqual(\n            JSONFieldNullable.objects.filter(json_data__isnull=True),\n            json_fields\n        )\n", "    def test_jsonfield_nullable(self):\n        json_values = [\n            {'key': 'value'}\n            for _ in range(10)\n        ]\n        objects = [\n            JSONFieldNullable.objects.create(data=json_values[i])\n            for i in range(len(json_values))\n        ]\n        for obj in objects:\n            obj.data = {'new_key': 'new_value'}\n        JSONFieldNullable.objects.bulk_update(objects, ['data'])\n        self.assertCountEqual(JSONFieldNullable.objects.values_list('data', flat=True), [\n            {'new_key': 'new_value'}\n            for _ in range(len(objects))\n        ])\n\n", "    def test_json_field(self):\n        json_nullable = JSONFieldNullable.objects.create(data={'key': 'value'})\n        json_nullable.data['key'] = 'updated_value'\n        JSONFieldNullable.objects.bulk_update([json_nullable], ['data'])\n        json_nullable.refresh_from_db()\n        self.assertEqual(json_nullable.data['key'], 'updated_value')\n", "    def test_jsonfield(self):\n        from django.core.serializers import serialize\n        json_data = {\n            'test_key': 'test_value',\n            'other_key': 123\n        }\n        json_objects = [\n            JSONFieldNullable.objects.create(json_field=json_data)\n            for _ in range(10)\n        ]\n        for json_obj in json_objects:\n            json_obj.json_field = {\n                **json_data,\n                'updated_key': 'updated_value',\n            }\n        JSONFieldNullable.objects.bulk_update(json_objects, ['json_field'])\n        self.assertEqual(\n            serialize('json', JSONFieldNullable.objects.all(), use_natural_keys=True),\n            serialize('json', json_objects, use_natural_keys=True)\n        )\n\n\n\n", "    def test_jsonfield_nullable(self):\n        jsonfields = [\n            JSONFieldNullable.objects.create(\n                json_field=None\n            )\n            for _ in range(10)\n        ]\n        for jsonfield in jsonfields:\n            jsonfield.json_field = {\"test\": \"value\"}\n        JSONFieldNullable.objects.bulk_update(jsonfields, ['json_field'])\n        self.assertCountEqual(JSONFieldNullable.objects.filter(json_field={\"test\": \"value\"}), jsonfields)\n"], "sample_408": ["    def test_rename_field(self):\n        changes = self.get_changes(\n            [self.author_empty],\n            [self.author_blank_name_field],\n        )\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameField\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, new_field=\"name\", old_field=\"name1\")\n", "    def test_operation_with_explicit_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\"Person\", fields=[]),\n                migrations.RenameModel(\"Person\", \"User\"),\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"rename_person_to_user\")\n", "    def test_operation_with_custom_name(self):\n        class Migration(migrations.Migration):\n            operations = [migrations.CreateModel(\"Person\", fields=[], name=\"my_person\")]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"my_person\")\n", "    def test_suggest_name_with_field_name_collision(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    \"Book\", fields=[(\"title\", models.CharField(max_length=200))],\n                ),\n                migrations.CreateModel(\n                    \"Author\", fields=[(\"name\", models.CharField(max_length=100))],\n                ),\n                migrations.AlterField(\n                    model_name=\"Book\", old_field=\"title\", new_field=\"author_name\",\n                ),\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"book_author_change_title_to_author_name\")\n\n", "    def test_operation_with_special_characters(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    \"Person_%\", fields=[\n                        (\"name\", models.CharField(max_length=255))\n                    ]\n                ),\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"person_percent\")\n"], "sample_1027": ["def test_issue_14199():\n    assert Poly(1, x, domain='ZZ').is_ground_constant is True\n    assert Poly(x, x, domain='ZZ').is_ground_constant is False\n    assert Poly(1/2, x, domain='RR').is_ground_constant is True\n", "def test_issue_14645():\n    p = Poly(x**2 + 1, x, domain='RR')\n    assert p.is_square_free() is True\n    p = Poly(x**4 + 1, x, domain='RR')\n    assert p.is_square_free() is False\n", "def test_issue_12969():\n    # Regression test for issue 12969: \n    # Poly.subs failing with symbolic expressions in denominator\n    p = Poly(1/(x + y), x, y)\n    assert p.subs({y: x}) == Poly(1/(2*x), x)\n", "def test_issue_14467():\n    # Test for issue 14467, where Poly.evalf() was losing accuracy\n    p = Poly(1/x**3, x)\n    assert abs(p.evalf(50, x=1.01) - 0.9707019997) < 1e-6\n", "def test_issue_14566():\n    p = Poly(x**3 + y**3, x, y)\n    assert p.monoms == [(1, 0, 3), (0, 1, 3)]\n"], "sample_377": ["    def test_sensitive_post_parameters_method_decorator(self):\n        class MyClass:\n            @method_decorator(sensitive_post_parameters())\n                return HttpResponse()\n\n        instance = MyClass()\n        instance.a_view(HttpRequest())\n", "    def test_sensitive_post_parameters_class_method(self):\n        class MyClass:\n            @method_decorator(sensitive_post_parameters())\n                return HttpResponse()\n\n        with self.settings(DEBUG=True):\n            response = MyClass().a_view(self.rf.post(\"/test/\", self.breakfast_data))\n\n        # The sensitive data shouldn't be in the response\n        self.assertNotContains(response, \"sausage-value\", status_code=500)\n        self.assertNotContains(response, \"bacon-value\", status_code=500)\n\n", "    def test_sensitive_post_parameters_method_decorator(self):\n        class MyClass:\n            @method_decorator(sensitive_post_parameters())\n                return HttpResponse()\n\n        my_instance = MyClass()\n        response = my_instance.a_view(HttpRequest())\n        self.assertEqual(response.status_code, 200)\n\n\n", "    def test_sensitive_post_parameters_method_decorator(self):\n        class MyClass:\n            @method_decorator(sensitive_post_parameters())\n                return HttpResponse()\n\n        MyClass().a_view(HttpRequest())\n", "    def test_sensitive_post_parameters_method_decorator(self):\n        class MyClass:\n            @method_decorator(sensitive_post_parameters())\n                return HttpResponse()\n\n        obj = MyClass()\n        response = obj.a_view(HttpRequest())\n        self.assertIsInstance(response, HttpResponse)\n\n\n\n"], "sample_954": ["def test_productionlist_output(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'testproject.1').read_text()\n    assert '.sp\\n.nf\\n' in content\n    assert 'production1 ::= ' in content\n    assert 'production2  ' in content\n    assert 'production3 ::= ' in content\n    assert '.fi\\n' in content\n", "def test_productionlist(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'sphinxtests.1').read_text()\n    assert ' ::= ' in content\n    assert '   ' in content  \n", "def test_custom_nodes(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'custom_nodes.1').read_text()\n    assert r'\\fBcustom_node1\\en\\fP' in content\n    assert r'\\fIcustom_inline\\fP' in content\n    assert r'\\fBcustom_strong\\fP' in content\n", "def test_manpage_translator_modifications(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'test-manpage.1').read_text()\n    assert r'\\fBmodified\\fP' in content\n    assert r'\\fBdescription\\fP\\n' in content\n\n\n", "def test_man_pages_config(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'sphinx.1').read_text()\n\n    assert 'Section: My Special Section' in content\n    assert 'Manual Group: My Special Group' in content\n    assert 'Copyright: Copyright 2023 by The Sphinx Team' in content\n    assert 'Modified: ' in content\n\n\n\n"], "sample_239": ["    def test_invalid(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [{'votes': ['This field is required.']}, {}]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n\n\n\n", "    def test_invalid(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': 'a',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [\n            {'choices-0-votes': ['Enter a valid number.']},\n            {}\n        ]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n", "    def test_invalid(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': 'invalid',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [\n            {'choices-0-votes': ['This field is invalid.']},\n            {}\n        ]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n", "    def test_invalid(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '',  # missing data\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset1, formset2)), False)\n", "    def test_valid_with_non_form_errors_returns_false(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice, extra=1, validate_max=True)\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset,)), False)\n"], "sample_411": ["    def test_normalize_path_patterns(self):\n        patterns = [\n            [\"foo/bar/baz\"],\n            [\"baz\"],\n            [\"foo/bar/baz\"],\n            [\"*/baz\"],\n            [\"*\"],\n            [\"b?z\"],\n            [\"[abc]az\"],\n            [\"*/ba[!z]/baz\"],\n        ]\n        normalized_patterns = normalize_path_patterns(patterns)\n        self.assertEqual(normalized_patterns, patterns)\n", "    def test_normalize_path_patterns(self):\n        tests = [\n            ([\"*\"], [\"*\"]),\n            ([\"foo\"], [\"foo\"]),\n            ([\"foo/bar\", \"baz/\"], [\"foo/bar\", \"baz/\"]),\n            ([\"foo/bar\", \"*\"], [\"foo/bar\", \"*\"]),\n            ([\"*\"], [\"foo/bar\", \"*\"]),\n        ]\n        for patterns, normalized in tests:\n            with self.subTest(patterns=patterns):\n                self.assertEqual(normalize_path_patterns(patterns), normalized)\n\n", "    def test_normalize_path_patterns(self):\n        self.assertEqual(\n            normalize_path_patterns(['foo/bar/baz', 'baz', '*/baz']),\n            ['foo/bar/baz', 'baz', '**/baz'],\n        )\n        self.assertEqual(normalize_path_patterns(['*']), ['*'])\n\n", "    def test_normalize_path_patterns(self):\n        tests = [\n            ({\"a\": \"b\"}, [\"a\"]),\n            ({\"a\": \"b\"}, [\"a/\"]),\n            ({\"a\": \"b\"}, [\"a/c\"]),\n            ({\"a\": \"b\"}, [\"a*\"]),\n            ({\"a\": \"b\"}, [\"a.*\"]),\n            ({\"a\": \"b\"}, [\"*\"]),\n            ({\"a\": \"b\"}, [\"**\"]),\n            ({\"a\": \"b\"}, [\"a/b\"]),\n        ]\n        for patterns_dict, paths in tests:\n            with self.subTest(patterns_dict=patterns_dict, paths=paths):\n                normalized_patterns = normalize_path_patterns(patterns_dict)\n                self.assertEqual(normalized_patterns, patterns_dict)\n\n                for path in paths:\n                    self.assertIn(path, normalized_patterns)\n", "    def test_normalize_path_patterns(self):\n        patterns = [\n            [\"foo/bar/baz\"],\n            [\"baz\"],\n            [\"foo/bar/baz\"],\n            [\"*/baz\"],\n            [\"*\"],\n            [\"b?z\"],\n            [\"[abc]az\"],\n            [\"*/ba[!z]/baz\"],\n        ]\n        for ignore_patterns in patterns:\n            with self.subTest(ignore_patterns=ignore_patterns):\n                normalized_patterns = normalize_path_patterns(ignore_patterns)\n                self.assertEqual(normalized_patterns, ignore_patterns)\n\n\n\n"], "sample_96": ["    def test_actions_are_callable(self):\n        class BandAdmin(ModelAdmin):\n            actions = ('NotCallable',)\n\n        self.assertIsInvalid(\n            BandAdmin, Band,\n            'Action \"NotCallable\" must be callable.',\n            id='admin.E131',\n        )\n", "    def test_actions_not_callable(self):\n        class BandAdmin(ModelAdmin):\n            actions = 'hello'\n\n        self.assertIsInvalid(\n            BandAdmin, Band,\n            \"The value of 'actions' must be a set of callable objects.\",\n            id='admin.E131',\n        )\n", "    def test_actions_contain_callable(self):\n        class BandAdmin(ModelAdmin):\n            actions = ('hello',)\n\n        self.assertIsInvalid(\n            BandAdmin, Band,\n            'Each element in Admin.actions must be a callable.',\n            id='admin.E131',\n        )\n", "    def test_actions_callable_type_error(self):\n        class BandAdmin(ModelAdmin):\n            actions = ('invalid')\n\n        self.assertIsInvalid(\n            BandAdmin, Band,\n            \"The value of 'actions[0]' must be a callable.\",\n            id='admin.E125',\n        )\n", "    def test_actions_are_callable(self):\n        class BandAdmin(ModelAdmin):\n            actions = ('not_a_callable')\n\n        self.assertIsInvalid(\n            BandAdmin, Band,\n            'All actions in <class \\'modeladmin.test_checks.ActionsCheckTests.'\n            'test_actions_are_callable.<locals>.BandAdmin\\'> must be callable.',\n            id='admin.E131',\n        )\n\n\n\n\n"], "sample_432": ["    def test_bulk_edit_form_field_validation(self):\n        from django.core.exceptions import ValidationError\n\n        Parent.objects.create(name=\"parent\")\n        self.admin_login(username=\"super\", password=\"secret\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_changelist_parent_changelist\")\n        )\n\n        # Select the parent\n        self.selenium.find_element(\n            By.CSS_SELECTOR, \"#result_list tbody tr:first-child input[type='checkbox']\"\n        ).click()\n\n        # Open the bulk edit form\n        self.selenium.find_element(By.ID, \"action-toggle\").click()\n        select = Select(self.selenium.find_element(By.NAME, \"action\"))\n        select.select_by_value(\"bulk_edit\")\n\n        # Check that the form fields are populated\n        self.assertIsNotNone(\n            self.selenium.find_element(By.ID, \"id_form-0-name\")\n        )\n\n        # Simulate invalid input and check for error messages\n        name_input = self.selenium.find_element(By.ID, \"id_form-0-name\")\n        name_input.clear()\n        name_input.send_keys(\"invalid\")  # Replace with your specific validation rules\n\n        # Submit the form\n        self.selenium.find_element(By.NAME, \"_save\").click()\n\n        # Check for error messages\n        error_messages = self.selenium.find_elements(By.CSS_SELECTOR, \".errorlist li\")\n        self.assertEqual(len(error_messages), 1)\n        self.assertIn(\"Invalid\", error_messages[0].text)\n\n", "    def test_bulk_edit_with_no_fields(self):\n        \"\"\"\n        Bulk edit should work even if there are no editable fields.\n        \"\"\"\n        Parent.objects.create(name=\"parent\")\n        self.admin_login(username=\"super\", password=\"secret\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_changelist_parent_changelist\")\n        )\n\n        select_all = self.selenium.find_element(By.ID, \"action-toggle\")\n        select_all.click()\n\n        bulk_edit_button = self.selenium.find_element(\n            By.CSS_SELECTOR,\n            \".actions > .bulk-edit-btn\",\n        )\n        bulk_edit_button.click()\n\n        bulk_edit_form = self.selenium.find_element(By.ID, \"bulk-edit-form\")\n        self.assertEqual(bulk_edit_form.find_elements(By.TAG_NAME, \"input\").__len__(), 0)\n", "    def test_no_save_button_when_editing_disabled(self):\n        from selenium.webdriver.common.by import By\n\n        class EditableAdmin(admin.ModelAdmin):\n            list_display = [\"name\"]\n            list_per_page = 10\n            actions = None\n            save_on_top = False\n            change_list_template = \"admin/change_list_with_edit_disabled.html\"\n\n        custom_site = site.Site(name=\"custom\")\n        custom_site.register(Parent, EditableAdmin)\n        Parent.objects.create(name=\"parent\")\n\n        self.admin_login(username=\"super\", password=\"secret\")\n        self.selenium.get(self.live_server_url + reverse(\"admin:admin_changelist_parent_changelist\"))\n        self.selenium.find_element(By.CSS_SELECTOR, \"input[name=_submit]\").click()\n        form = self.selenium.find_element(By.ID, \"changelist-form\")\n        self.assertFalse(\n            form.find_element(By.CSS_SELECTOR, \"input[type='submit'][name='_save']\").is_displayed()\n        )\n", "    def test_actions_with_no_selection(self):\n        from selenium.webdriver.common.by import By\n\n        Parent.objects.create(name=\"foo\")\n\n        self.admin_login(username=\"super\", password=\"secret\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_changelist_parent_changelist\")\n        )\n\n        self.selenium.find_element(By.NAME, \"action\").click()\n        select = Select(self.selenium.find_element(By.NAME, \"action\"))\n        select.select_by_value(\"delete_selected\")\n        self.selenium.find_element(By.NAME, \"_save\").click()\n        alert = self.selenium.switch_to.alert\n        try:\n            self.assertEqual(alert.text, \"No items selected.\")\n        finally:\n            alert.dismiss()\n", "    def test_ordering_by_non_field_attribute(self):\n        class MyModel(models.Model):\n            name = models.CharField(max_length=255)\n            pk = models.IntegerField(primary_key=True)\n\n        MyModel.objects.create(name=\"Test 1\", pk=1)\n        MyModel.objects.create(name=\"Test 2\", pk=2)\n\n        m = MyModelAdmin(MyModel, custom_site)\n        request = self._mocked_authenticated_request(\"/\", superuser)\n\n        with self.assertRaises(AttributeError):\n            m._get_ordering_list(request)\n"], "sample_781": ["def test_random_state_deterministic(name):\n    X, y = hastie_X, hastie_y  \n    for i in range(5):\n        est1 = FOREST_ESTIMATORS[name](n_estimators=10, random_state=i)\n        est2 = FOREST_ESTIMATORS[name](n_estimators=10, random_state=i)\n        assert_array_almost_equal(est1.estimators_[0].tree_.children_left,\n                                        est2.estimators_[0].tree_.children_left)\n        assert_array_almost_equal(\n                est1.estimators_[0].tree_.feature_importances_,\n                est2.estimators_[0].tree_.feature_importances_\n            )\n", "compilation error", "def test_sample_weight_forest_fit_predict(name):\n    X, y = datasets.make_regression(n_samples=100, n_features=20,\n                                    random_state=42)\n    ForestEstimator = FOREST_ESTIMATORS[name]\n    est = ForestEstimator(n_estimators=5, max_depth=3, random_state=42)\n\n    #Test with sample weights\n    sample_weight = np.random.rand(X.shape[0])\n    est.fit(X, y, sample_weight=sample_weight)\n    y_pred = est.predict(X)\n\n    #Test that prediction is affected by sample_weight\n    est_no_weight = ForestEstimator(n_estimators=5, max_depth=3,\n                                     random_state=42)\n    est_no_weight.fit(X, y)\n    y_pred_no_weight = est_no_weight.predict(X)\n    assert_array_not_equal(y_pred, y_pred_no_weight) \n", "def test_check_missing_values_warnings():\n    X, y = datasets.make_classification(n_samples=100, random_state=0)\n    X[:, 0] = np.nan\n\n    # Test with RandomForestClassifier\n    with pytest.warns(UserWarning, match=\"Scikit-learn does not support missing values\"):\n        clf = RandomForestClassifier().fit(X, y)\n\n    # Test with RandomForestRegressor\n    with pytest.warns(UserWarning, match=\"Scikit-learn does not support missing values\"):\n        reg = RandomForestRegressor().fit(X, y)\n\n\n", "def test_check_feature_importances_with_sparse_data(name):\n    X, y = make_classification(n_samples=100, n_features=20, random_state=0)\n    X_sparse = csr_matrix(X)\n    clf = FOREST_CLASSIFIERS[name](random_state=0)\n    clf.fit(X, y)\n    assert_array_almost_equal(clf.feature_importances_,\n                             clf.fit(X_sparse, y).feature_importances_)\n\n"], "sample_1192": ["def test_disambiguate():\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n    s1 = disambiguate(x, x, y, y, z, z)\n    s2 = disambiguate(x, x, y, y, z, z, cls=Dummy)\n\n    assert s1 == (x_2, x_1, y_1, y_2, z_1, z_2)\n\n    assert s2 == (x_2, x_1, y_1, y_2, z_1, z_2)\n\n\n\n\n", "def test_symbols_with_parentheses():\n    from sympy import sympify\n    x, y, z = symbols('x,y,z')\n    assert symbols('(x,y)') == (x, y)\n    assert symbols('(x, y)') == (x, y)\n    assert symbols(' (x,y) ') == (x, y)\n    assert symbols('(x, y ,z)') == (x, y, z)\n    assert symbols(' (x, y ,z) ') == (x, y, z)\n    assert symbols('(x y z)') == (x, y, z)\n    assert symbols('( x y z )') == (x, y, z)\n    test_cases = [\n        ('(x, y, z)', (x, y, z)),\n        ('(x, y, z ,)', (x, y, z)),\n        ('(x y z)', (x, y, z)),\n        ('( x y z )', (x, y, z)),\n        ('((x, y), z)', ((x, y), z)),\n        ('((x, y), z,)', ((x, y), z)),\n        ('(( x, y ), z)', ((x, y), z)),\n        ('(( x, y ), z ,)', ((x, y), z)),\n    ]\n\n    for expr, expected in test_cases:\n        assert symbols(expr) == expected\n    \n    raises(ValueError, lambda: symbols('(x,y,)')) \n    raises(ValueError, lambda: symbols('(x,y,z,)'))\n    raises(ValueError, lambda: symbols('()'))\n\n\n\n", "def test_disambiguate():\n    x, y = symbols('x,y')\n    assert disambiguate('x_0', 'x_1') == (x, x_1)\n    assert disambiguate('x_0', 'x_1', 'y') == (x, x_1, y)\n    assert disambiguate(x, y, 'x_0', 'x_1') == (x, y, x_0, x_1)\n\n\n", "def test_disambiguate():\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n    a = Symbol('_a')\n    b = Symbol('_b')\n    c = Symbol('_c')\n\n    expr = x + y + z\n    expr_dis = disambiguate(expr)\n\n    assert expr_dis == x + y + z\n    \n    expr_dis = disambiguate(x + y + a + b + c)\n    assert expr_dis == (x + y) + (a + b + c)\n    \n    expr_dis = disambiguate((x + y) + (z + a) - (b + c))\n    assert expr_dis == (x + y) + (z + a) - (b + c)\n\n    expr_dis = disambiguate(x + y + z + a + b + c)\n\n    assert expr_dis == (x + y + z) + (a + b + c)\n\n    expr_dis = disambiguate((x + y) + (z + a) - (b + c) + (x + y))\n    assert expr_dis == ((x + y) + (z + a) - (b + c)) + (x + y)\n\n\n\n", "def test_symbols_with_leading_and_trailing_spaces():\n    x = Symbol('   x   ')\n    assert x.name == 'x'\n    y = Symbol(' x   ') \n    assert y.name == 'x'\n    z = Symbol('  z  ')\n    assert z.name == 'z'\n\n\n\n"], "sample_984": ["def test_And():\n    assert str(And(True, False)) == \"And(True, False)\"\n    assert str(And(Eq(x, 1), Eq(y, 2))) == \"And(Eq(x, 1), Eq(y, 2))\"\n    assert str(And(x > 1, y < 5)) == \"x > 1 & y < 5\"\n\n\n\n", "def test_cond_expr():\n    from sympy import cond, symbols\n\n    x, y = symbols('x y')\n    assert str(cond(x > 0, x, y)) == \"cond(x > 0, x, y)\"\n    assert str(cond(x > y, x**2, y**2)) == \"cond(x > y, x**2, y**2)\"\n", "def test_issue_7781():\n    from sympy.calculus import limit\n    assert str(limit(sin(x)/x, x, 0)) == 'limit(sin(x)/x, x, 0)'\n\n\n", "def test_MatrixRank():\n    from sympy import Matrix\n    A = Matrix([[1, 2], [3, 4]])\n    assert str(MatrixRank(A)) == 'MatrixRank(Matrix([[1, 2], [3, 4]]))'  \n", "def test_Interval():\n    assert str(Interval(1, 3, included=False, open_left=True)) == '(1, 3)'\n    assert str(Interval(1, 3, included=True, open_left=True)) == '[1, 3)'\n    assert str(Interval(1, 3, included=True, open_left=False)) == '[1, 3]'\n    assert str(Interval(1, 3, included=False, open_left=False)) == '(1, 3]'\n    assert str(Interval(1, 3, left_open=True, right_open=True)) == '(1, 3)'\n    assert str(Interval(1, 3, left_open=True, right_closed=True)) == '[1, 3)'\n    assert str(Interval(1, 3, left_closed=True, right_open=True)) == '(1, 3]'\n    assert str(Interval(1, 3, left_closed=True, right_closed=True)) == '[1, 3]'\n    assert str(Interval(3, 1, included=False, open_left=True)) == '(3, 1)'\n    assert str(Interval(3, 1, included=True, open_left=True)) == '[3, 1)'\n    assert str(Interval(3, 1, included=True, open_left=False)) == '[3, 1]'\n    assert str(Interval(3, 1, included=False, open_left=False)) == '(3, 1]'\n\n\n\n"], "sample_678": ["    def test_get_extended_length_path_str_relative_path(self):\n        assert get_extended_length_path_str(\"c:\\\\foo\") == r\"\\\\?\\c:\\foo\"\n        assert get_extended_length_path_str(\"foo\") == r\"\\foo\"\n", "def test_ensure_deletable_with_dead_lock(tmp_path):\n    path = tmp_path / \"temp-1\"\n    path.mkdir()\n    lock_path = get_lock_path(path)\n    lock_path.touch()\n    lock_file_creation_time = lock_path.stat().st_mtime\n    with open(str(lock_path), 'w') as f:\n        f.write(\"dead\")  \n    # Wait a bit exceeding the lock timeout to make it \"dead\"\n    os.utime(str(lock_path), (lock_file_creation_time - 60, lock_file_creation_time - 60))\n    assert not ensure_deletable(path, lock_file_creation_time - 60)\n    assert lock_path.exists()\n", "    def test_ensure_extended_length_path(path, expected_path):\n        assert (\n            ensure_extended_length_path(Path(path)).as_posix() == expected_path\n        )\n\n", "def test_ensure_deletable():\n    with pytest.raises(py.warning.PytestWarning):\n        ensure_deletable(Path(\"\"), 10.0)\n\n\n\n", "    def test_get_extended_length_path_str_for_various_inputs(\n        self, path_str, expected_path_str"], "sample_409": ["    def test_unicode_handling_in_blocktranslate(self):\n        with translation.override(\"de\"):\n            t = self.get_template(\n                \"{% load i18n %}{% blocktranslate %}\u20ac {number} \"\n                \"{% endblocktranslate %}\"\n            )\n            self.assertEqual(\n                t.render(Context({\"number\": 123})), \"\u20ac 123 \" \n            )\n", "    def test_html_escape_blocktranslate(self):\n        t = self.get_template(\n            \"{% load i18n %}{% blocktranslate %}This is &lt;a&gt;link&lt;/a&gt;.\"\n            \"{% endblocktranslate %}\"\n        )\n        with translation.override(\"de\"):\n            self.assertEqual(t.render(Context({})), \"This is &lt;a&gt;link&lt;/a&gt;.\")\n\n\n\n", "    def test_unicode_in_translatable_block(self):\n        t = self.get_template(\n            \"{% load i18n %}{% blocktranslate %}This is a test with \"\n            \"a unicode character: \\u03A9{% endblocktranslate %}\"\n        )\n        with translation.override(\"de\"):\n            self.assertEqual(t.render(Context({})), \"Dieser Test enth\u00e4lt einen Unicode-Charakter: \u03a9\")\n", "    def test_percent_in_translatable_block_with_escaping(self):\n        t = self.get_template(\n            \"{% load i18n %}{% blocktranslate %}{{ percent }}%%{% endblocktranslate %}\"\n        )\n        with translation.override(\"de\"):\n            self.assertEqual(t.render(Context({\"percent\": 42})), \"42%\")\n", "    def test_safe_html_interpolation(self):\n        # Ensure safe HTML interpolation works as expected\n        with translation.override(\"de\"):\n            t = self.get_template(\n                \"{% load i18n %}{% blocktranslate %}\"\n                \"Click <a href=\\\"{{ url }}\\\">{{ text }}</a> to continue.\"\n                \"{% endblocktranslate %}\"\n            )\n            context = {\"url\": \"https://www.example.com\", \"text\": \"Learn More\"}\n            rendered = t.render(context)\n            self.assertEqual(\n                rendered, '<a href=\"https://www.example.com\">Learn More</a> to continue.'\n            )\n"], "sample_797": ["compilation error", "def test_power_transformer_sparse_input(method):\n    X = sparse.csc_matrix([[1, 2], [3, 4], [5, 6]])\n    pt = PowerTransformer(method=method)\n    \n    # Test that the fit and transform methods work with sparse matrices\n    pt.fit(X)\n    X_trans = pt.transform(X)\n    assert isinstance(X_trans, sparse.csr_matrix)\n    \n    # Test inverse_transform with sparse matrices\n    X_inv = pt.inverse_transform(X_trans)\n    assert isinstance(X_inv, sparse.csc_matrix)\n", "def test_power_transformer_multivariate_sparse():\n    n_samples = 1000\n    n_features = 5\n    X = sparse.csr_matrix((np.random.rand(n_samples, n_features),\n                          (np.random.randint(0, n_features, size=n_samples),\n                           np.random.randint(0, n_features, size=n_samples))))\n    pt = PowerTransformer(method='box-cox')  \n    pt.fit(X)\n    assert_array_almost_equal(pt.lambdas_, np.zeros(n_features), decimal=5)\n    X_trans = pt.transform(X)\n    assert_array_almost_equal(X.shape, X_trans.shape)\n\n\n", "def test_power_transformer_sparse_input():\n    X_sparse = sparse.csr_matrix([[1, 2], [3, 4]])\n    for method in ['box-cox', 'yeo-johnson']:\n        pt = PowerTransformer(method=method)\n        X_trans = pt.fit_transform(X_sparse)\n        assert isinstance(X_trans, sparse.csr_matrix)\n\n        X_inv_trans = pt.inverse_transform(X_trans)\n        assert isinstance(X_inv_trans, sparse.csr_matrix)\n\n        assert_array_almost_equal(X_sparse.toarray(),\n                                  X_inv_trans.toarray())\n\n\n\n", "def test_power_transformer_sparse_matrix():\n    rng = np.random.RandomState(0)\n    X = sparse.csr_matrix((rng.rand(100, 5), (rng.arange(100), rng.arange(5))))\n    pt = PowerTransformer(method='box-cox', standardize=False)\n    X_trans = pt.fit_transform(X)\n    assert isinstance(X_trans, sparse.csr_matrix)\n\n    pt = PowerTransformer(method='yeo-johnson', standardize=False)\n    X_trans = pt.fit_transform(X)\n    assert isinstance(X_trans, sparse.csr_matrix)\n\n\n\n"], "sample_1161": ["def test_RelationalPredicate():\n    assert sstr(le(x, y)) == 'x <= y'\n    assert sstr(gt(x, y)) == 'x > y'\n    assert sstr(ne(x, y)) == 'x != y'\n    assert sstr(eq(x, y)) == 'x == y'\n", "def test_Lambda_printing():\n    x, y = symbols(\"x y\")\n    assert str(Lambda(x, x**2 + y)) == 'Lambda(x, x**2 + y)'\n    assert str(Lambda(x, 2*x + 1)) == 'Lambda(x, 2*x + 1)'\n    assert str(Lambda(x, sin(x))) == 'Lambda(x, sin(x))'\n", "def test_Piecewise():\n    from sympy.functions import piecewise, Heaviside\n    x = symbols('x')\n    assert str(piecewise((x, x > 0), (0, True))) == \"Piecewise((x, x > 0), (0, True))\"\n    assert str(piecewise((x, x < 0), (1, True))) == \"Piecewise((x, x < 0), (1, True))\"\n    assert str(piecewise((x**2, x < 0), (x, x >= 0))) == \"Piecewise((x**2, x < 0), (x, x >= 0))\"\n    assert str(piecewise((Heaviside(x), x > 0), (0, True))) == \"Piecewise((Heaviside(x), x > 0), (0, True))\"\n", "def test_is_commutative():\n    A, B = symbols('A B', commutative=False)\n    assert str(is_commutative(A*B)) == 'False'\n    C, D = symbols('C D', commutative=True)\n    assert str(is_commutative(C*D)) == 'True'\n", "def test_FunctionClass():\n    from sympy.utilities.functions import FunctionClass\n    f = FunctionClass('f')(x)\n    g = FunctionClass('g')(x, y)\n    assert str(f) == 'f(x)'\n    assert str(g) == 'g(x, y)'\n"], "sample_139": ["    def test_dynamic_list_filter_with_empty_value(self):\n        \"\"\"\n        Regression tests for ticket #17646: Dynamic list_filter with empty values in URL query parameters.\n        \"\"\"\n        parent = Parent.objects.create(name='parent')\n        for i in range(10):\n            Child.objects.create(name='child %s' % i, parent=parent)\n\n        user_noparents = self._create_superuser('noparents')\n        m = DynamicListFilterChildAdmin(Child, custom_site)\n        request = self._mocked_authenticated_request('/child/?parent_id=', user_noparents)\n        response = m.changelist_view(request)\n        self.assertEqual(response.context_data['cl'].list_filter, ['name', 'age'])\n\n        request = self._mocked_authenticated_request('/child/?parent_id=', user_noparents, data={'parent': ''})\n        response = m.changelist_view(request)\n        self.assertEqual(response.context_data['cl'].list_filter, ['name', 'age'])\n", "    def test_dynamic_list_filter_multiple_values(self):\n        \"\"\"\n        Regression tests for ticket #17646: dynamic list_filter support\n        with multiple values.\n        \"\"\"\n        parent = Parent.objects.create(name='parent')\n        for i in range(10):\n            Child.objects.create(name='child %s' % i, parent=parent)\n\n        user_noparents = self._create_superuser('noparents')\n        user_parents = self._create_superuser('parents')\n\n        # Test with user 'noparents'\n        m = DynamicListFilterChildAdmin(Child, custom_site)\n\n        for user in [user_noparents, user_parents]:\n            request = self._mocked_authenticated_request('/child/', user)\n            response = m.changelist_view(request)\n\n            # Check that the filter is correctly applied.\n            self.assertIn('<select name=\"filter_parent\" id=\"filter_parent\">', response.rendered_content)\n\n            # Select a parent and simulate a changelist request.\n            for child in parent.child_set.all():\n                m.get_query_set_from_request(request, {'filter_parent': [str(child.parent.pk)]})\n                self.assertEqual(response.context_data['cl'].filter_queryset.count(), 1)\n\n                self.assertIn(str(child.id), response.context_data['cl'].result_list[0].id)\n\n", "    def test_pagination_large_datasets(self):\n        \"\"\"\n        Verify pagination works correctly with a large dataset,\n        handling results across multiple pages.\n        \"\"\"\n        # Generate a large dataset\n        for i in range(500):\n            Group.objects.create(name=f'Group {i}')\n\n        # Access the changelist view\n        superuser = self._create_superuser('superuser')\n        with self.client.login(username='superuser', password='secret'):\n            response = self.client.get('/group/')\n\n        # Check the total number of pages\n        self.assertContains(response,\n                           f'Showing {Group.objects.count()} results')\n        self.assertContains(response,\n                           f'Page 1 of {len(response.context['object_list'].paginator.page_range)}')\n\n        # Navigate to subsequent pages\n        for page_num in range(2, len(response.context['object_list'].paginator.page_range) + 1):\n            with self.subTest(page=page_num):\n                response = self.client.get('/group/?page=%d' % page_num)\n                self.assertContains(response, f'Showing {Group.objects.count()} results')\n                self.assertContains(response, f'Page {page_num} of {len(response.context[\"object_list\"].paginator.page_range)}')\n\n\n\n", "    def test_dynamic_list_filter_choices(self):\n        \"\"\"Dynamic list filters should return choices based on the current context, not on a fixed set.\"\"\"\n        class ChildWithDynamicChoices(Child):\n            class Meta:\n                permissions = [\n                    ('view_dynamic_choices', 'Can view dynamic choices'),\n                ]\n\n        class ChildWithDynamicChoicesAdmin(ChildAdmin):\n                if field_name == 'parent':\n                    return [\n                        ('parent_one', 'Parent One'),\n                        ('parent_two', 'Parent Two'),\n                    ]\n                return super().get_list_filter_choices(request, field_name)\n\n        custom_site.register(ChildWithDynamicChoices, ChildWithDynamicChoicesAdmin)\n        superuser = self._create_superuser('superuser')\n        request = self._mocked_authenticated_request('/childwithdynamicchoices/', superuser)\n        admin_object = ChildWithDynamicChoicesAdmin(ChildWithDynamicChoices, custom_site)\n        choices = admin_object.get_list_filter_choices(request, 'parent')\n        self.assertEqual(choices, [('parent_one', 'Parent One'), ('parent_two', 'Parent Two')])\n        custom_site.unregister(ChildWithDynamicChoices)\n\n", "    def test_pagination_on_changelist_view(self):\n        superuser = self._create_superuser('super')\n        \n        # Create more items than will fit on the first page\n        for i in range(30):\n            Swallow.objects.create(origin='Swallow %s' % i, load=i, speed=i)\n\n        self.client.force_login(superuser)\n        response = self.client.get('/swallow/')\n\n        # Check that the response content only includes pages 1-2\n        self.assertContains(response, 'page=1')\n        self.assertContains(response, 'page=2')\n        self.assertNotContains(response, 'page=3')\n        \n        # Navigate to page 2\n        response = self.client.get('/swallow/?page=2')\n        \n        # Check that items from the second page are displayed\n        self.assertContains(response, 'Swallow 10')\n\n\n\n"], "sample_1176": ["def test_issue_13214():\n    from sympy import Float, sqrt, oo\n\n    assert Float(sqrt(-1)).is_imaginary == True\n    assert Float(sqrt(-oo)).is_imaginary == True\n    assert Float(sqrt(oo)).is_imaginary == False\n\n", "def test_log_of_zero_and_negative():\n    assert log(0, 2) is S.NegativeInfinity\n    assert log(0, 10) is S.NegativeInfinity\n    assert log(-1, 2) is S.ComplexInfinity\n    assert log(-1, 10) is S.ComplexInfinity\n", "def test_issue_12810():\n    assert Rational(1, 3)**Rational(2, 3) == Float('1.0' , precision=10)\n    assert (Rational(1, 3)**Rational(2, 3)).n(10) == 1.0\n", "def test_issue_10373():\n    assert Float(0).is_finite is True\n    assert Float(oo).is_finite is False\n    assert Float(-oo).is_finite is False\n    assert Float(nan).is_finite is False\n", "compilation error"], "sample_764": ["def test_column_transformer_invalid_remainder_transformer():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n    with pytest.raises(ValueError):\n        ct = ColumnTransformer([('trans1', StandardScaler(), [0])],\n                               remainder=object())\n\n", "def test_column_transformer_sparse_threshold_behavior():\n\n    X_array = np.array([[0, 1, 2],\n                        [2, 4, 6],\n                        [8, 6, 4]]).astype('float').T\n\n    ct = ColumnTransformer([('trans1', StandardScaler(), [0]),\n                           ('trans2', StandardScaler(), [1, 2])],\n                           sparse_threshold=0.5)\n    X_trans = ct.fit_transform(X_array)\n    assert not sparse.issparse(X_trans)\n\n    ct = ColumnTransformer([('trans1', SparseMatrixTrans(), [0]),\n                           ('trans2', StandardScaler(), [1, 2])],\n                           sparse_threshold=0.5)\n    X_trans = ct.fit_transform(X_array)\n    assert sparse.issparse(X_trans)\n\n    ct = ColumnTransformer([('trans1', StandardScaler(), [0]),\n                           ('trans2', SparseMatrixTrans(), [1, 2])],\n                           sparse_threshold=0.2)\n    X_trans = ct.fit_transform(X_array)\n    assert sparse.issparse(X_trans)\n", "def test_column_transformer_sparse_output():\n    X_array = np.array([[0, 1, 2],\n                        [2, 4, 6],\n                        [8, 6, 4]]).astype('float').T\n\n    ct = ColumnTransformer([('trans1', SparseMatrixTrans(), [0])],\n                           sparse_output=True)\n\n    X_trans = ct.fit_transform(X_array)\n    assert sparse.issparse(X_trans)\n    assert X_trans.shape == (3, 3)\n\n    ct = ColumnTransformer([('trans1', StandardScaler(), [0])],\n                           sparse_output=True)\n\n    X_trans = ct.fit_transform(X_array)\n    assert not sparse.issparse(X_trans)\n\n", "def test_column_transformer_non_numpy_array_remainder_transformer():\n    X_array = np.array([[0, 1, 2],\n                        [2, 4, 6],\n                        [8, 6, 4]]).T\n    ct = ColumnTransformer(\n        [('trans1', 'drop', [0])],\n        remainder=lambda X: X[:, 1:3] * 2\n    )\n\n    X_trans = ct.fit_transform(X_array)\n    assert_array_equal(X_trans, 2 * X_array[:, 1:3])\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == 'remainder'\n    assert callable(ct.transformers_[-1][1])\n    assert_array_equal(ct.transformers_[-1][2], [1, 2])\n", "def test_column_transformer_partial_fit():\n\n    X_array = np.array([[[0, 1], [2, 4]], [[8, 6], [4, 2]]])\n    y_array = np.array([2, 1])\n    ct = ColumnTransformer([('trans', StandardScaler(), [0]),\n                           ('trans2', StandardScaler(), [1])])\n\n    ct.partial_fit(X_array[0], y_array[0])\n    \n    # Check that fit method is now called without raising\n    ct.fit(X_array[1], y_array[1])\n\n"], "sample_226": ["    def test_serialization_large_dataset(self):\n        # Test serialization of a large dataset.\n\n        # Create a large dataset of Objects with references.\n        for i in range(1000):\n            obj_1 = Object.objects.create(name=f'Object {i}')\n            obj_2 = ObjectReference.objects.create(obj=obj_1)\n        # Serialize the dataset.\n        data = connection.creation.serialize_db_to_string()\n\n        # Deserialize the dataset.\n        Object.objects.all().delete()\n        ObjectReference.objects.all().delete()\n        connection.creation.deserialize_db_from_string(data)\n\n        # Verify that the deserialized dataset is identical to the original.\n        self.assertEqual(Object.objects.count(), 1000)\n        self.assertEqual(ObjectReference.objects.count(), 1000)\n", "    def test_circular_dependent_objects(self):\n        # deserialize_db_from_string() handles circular dependencies\n        # between objects.\n        data = \"\"\"\n        [\n            {\n                \"model\": \"backends.circulara\",\n                \"pk\": 1,\n                \"fields\": {\"circularb\": 2}\n            },\n            {\n                \"model\": \"backends.circularb\",\n                \"pk\": 2,\n                \"fields\": {\"circulara\": 1}\n            }\n        ]\n        \"\"\"\n        connection.creation.deserialize_db_from_string(data)\n        obj_a = CircularA.objects.get()\n        obj_b = CircularB.objects.get()\n        self.assertEqual(obj_a.circularb, obj_b)\n        self.assertEqual(obj_b.circulara, obj_a)\n", "    def test_clone_test_db(self, mocked_sync_apps, mocked_migrate, *mocked_objects):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST']['MIGRATE'] = True\n        creation = test_connection.creation_class(test_connection)\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db'):\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n            # Migrate the initial database\n            mocked_migrate.assert_called()\n\n            # Clone the database\n            suffix = '1'\n            creation.clone_test_db(suffix, verbosity=0, keepdb=False)\n\n            # Ensures that the cloned database settings are correct\n            cloned_settings = creation.get_test_db_clone_settings(suffix)\n            self.assertEqual(cloned_settings['NAME'],\n                             f'{test_connection.settings_dict[\"NAME\"]}_{suffix}')\n\n            # Verify that the cloned database has been migrated\n            mocked_migrate.assert_called_with([], ['backends'], verbosity=0, interactive=False, run_syncdb=True)\n            \n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0)\n\n\n\n\n", "    def test_circular_references_with_foreign_key(self):\n        # serialize_db_to_string() and deserialize_db_from_string() handles\n        # circular references with ForeignKey.\n        obj_a = CircularA(name=\"A\")\n        obj_b = CircularB(name=\"B\")\n        obj_a.circularb = obj_b\n        obj_b.circulara = obj_a\n        obj_a.save()\n        obj_b.save()\n\n        data = connection.creation.serialize_db_to_string()\n        CircularA.objects.all().delete()\n        CircularB.objects.all().delete()\n        connection.creation.deserialize_db_from_string(data)\n        obj_a = CircularA.objects.get()\n        obj_b = CircularB.objects.get()\n        self.assertEqual(obj_a.circularb, obj_b)\n        self.assertEqual(obj_b.circulara, obj_a)\n", "    def test_clone_test_db(self, mocked_sync_apps, mocked_migrate, *mocked_objects):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST']['MIGRATE'] = True\n        creation = test_connection.creation_class(test_connection)\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db'):\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n            creation.clone_test_db('suffix', verbosity=0)\n            # Migrations run for the new clone\n            mocked_migrate.assert_called()\n            args, kwargs = mocked_migrate.call_args\n            self.assertEqual(args, ([('app_unmigrated', '0001_initial')],))\n            self.assertEqual(len(kwargs['plan']), 1)\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0)\n\n"], "sample_787": ["def test_balanced_accuracy_score_empty_target():\n    assert_equal(balanced_accuracy_score(y_true=None, y_pred=None), 0)\n    assert_equal(balanced_accuracy_score(y_true=[], y_pred=[]), 0)\n    assert_equal(balanced_accuracy_score(y_true=None, y_pred=[0.1, 0.2, 0.3]), 0)\n    assert_equal(balanced_accuracy_score(y_true=[0, 1, 2], y_pred=None), 0)\n\n", "def test_balanced_accuracy_score_empty_arrays():\n    assert_equal(balanced_accuracy_score(y_true=[], y_pred=[]), 0.0)\n    assert_equal(balanced_accuracy_score(y_true=[], y_pred=[0]), np.nan)\n    assert_equal(balanced_accuracy_score(y_true=[0], y_pred=[]), np.nan)\n\n\n", "compilation error", "def test_balanced_accuracy_score_empty():\n    assert_equal(balanced_accuracy_score([], []), 0.0)\n    assert_equal(balanced_accuracy_score([0], []), 0.0)\n    assert_equal(balanced_accuracy_score([], [0]), 0.0)\n    \n    assert_equal(balanced_accuracy_score([0, 1], [0, 1]), 1.0)\n    assert_equal(balanced_accuracy_score([0, 1, 1], [0, 1, 1]), 1.0)\n    assert_equal(balanced_accuracy_score([0, 1, 1], [0, 0, 1]), 0.6666666666666666)\n\n\n\n", "def test_balanced_accuracy_score_multiclass_multilabel():\n    y_true = np.array([1, 0, 0], dtype=bool)\n    y_pred = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    assert_equal(balanced_accuracy_score(y_true, y_pred), 0.6666666666666666)\n"], "sample_597": ["    def test_merge_identical(self):\n        ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"b\": (\"x\", [3, 4]), \"x\": [0, 1]})\n        ds2 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"b\": (\"x\", [3, 4]), \"x\": [0, 1]})\n        expected = ds1.copy() \n\n        assert expected.identical(ds1.merge(ds2, compat=\"identical\"))\n        assert expected.identical(ds2.merge(ds1, compat=\"identical\"))\n\n        with pytest.raises(xr.MergeError):\n            ds3 = xr.Dataset({\"a\": (\"x\", [99, 2]), \"b\": (\"x\", [3, 4]), \"x\": [0, 1]})\n            ds1.merge(ds3, compat=\"identical\")\n\n        with pytest.raises(xr.MergeError):\n            ds3 = xr.Dataset({\"a\": (\"y\", [2, 3]), \"b\": (\"x\", [3, 4]), \"x\": [0, 1]})\n            ds1.merge(ds3, compat=\"identical\")\n", "    def test_merge_broadcast_equals_nan(self):\n        ds1 = xr.Dataset({\"x\": np.nan})\n        ds2 = xr.Dataset({\"x\": (\"y\", [np.nan, np.nan])})\n        actual = ds1.merge(ds2)\n        assert ds2.identical(actual)\n\n        actual = ds2.merge(ds1)\n        assert ds2.identical(actual)\n\n        actual = ds1.copy()\n        actual.update(ds2)\n        assert ds2.identical(actual)\n\n\n        ds1 = xr.Dataset({\"x\": np.nan})\n        ds2 = xr.Dataset({\"x\": (\"y\", [0, np.nan])})\n        actual = ds1.merge(ds2)\n        assert ds2.identical(actual)\n\n        actual = ds2.merge(ds1)\n        assert ds2.identical(actual)\n", "    def test_merge_broadcast_equals_coords(self):\n        ds1 = xr.Dataset({\"x\": 0, \"y\": ([], 0)})\n        ds2 = xr.Dataset({\"x\": (\"y\", [0, 0]), \"y\": ([], 1)})\n        actual = ds1.merge(ds2)\n        assert ds2.identical(actual)\n\n        actual = ds2.merge(ds1)\n        assert ds2.identical(actual)\n\n\n        ds1 = xr.Dataset({\"x\": np.nan, \"y\": ([], 0)})\n        ds2 = xr.Dataset({\"x\": (\"y\", [np.nan, np.nan]), \"y\": ([], 1)})\n        actual = ds1.merge(ds2)\n        assert ds2.identical(actual)\n\n\n        ds1 = xr.Dataset({\"x\": 0, \"y\": ([], 0)})\n        ds2 = xr.Dataset({\"x\": (\"y\", [0]), \"y\": ([], 1)})\n        with pytest.raises(xr.MergeError):\n            ds1.merge(ds2)\n\n\n\n", "    def test_merge_broadcast_equals_with_attrs(self):\n        ds1 = xr.Dataset({\"x\": 0, \"a\": {\"foo\": \"bar\"}}, dims=[\"x\"])\n        ds2 = xr.Dataset({\"x\": (\"y\", [0, 0]), \"b\": {\"foo\": \"baz\"}}, dims=[\"y\"])\n        actual = ds1.merge(ds2)\n        assert ds2.identical(actual)\n        assert actual.attrs == {\"foo\": \"bar\"}\n\n        actual = ds2.merge(ds1)\n        assert ds2.identical(actual)\n        assert actual.attrs == {\"foo\": \"bar\"}\n\n\n", "    def test_merge_preserve_attrs_with_existing(self):\n        ds1 = xr.Dataset({\"x\": 0, \"foo\": \"bar\"})\n        ds2 = xr.Dataset({\"y\": 1, \"foo\": \"baz\"})\n        merged = ds1.merge(ds2)\n        assert merged.attrs == {\"foo\": \"baz\"}\n\n        ds1 = xr.Dataset({\"x\": 0, \"foo\": {\"a\": 1}})\n        ds2 = xr.Dataset({\"y\": 1, \"foo\": \"baz\"})\n        merged = ds1.merge(ds2)\n        assert merged.attrs == {\"foo\": \"baz\"}\n"], "sample_642": ["def test_preprocess_options_no_arg() -> None:\n    \"\"\"Test preprocess_options with options that don't take an argument.\"\"\"\n    with tempdir() as chroot:\n        with fake_home():\n            chroot_path = Path(chroot)\n            testutils.create_files([\"a/b/c/d/__init__.py\"])\n            os.chdir(chroot_path / \"a/b/c\")\n            args = [\"--verbose\", \"--enable-all-extensions\"]\n            processed_args = _preprocess_options(Run(), args)\n            assert processed_args == []\n\n\n", "def test_preprocess_options_store_true(  # pylint: disable=too-many-arguments\n    capsys: CaptureFixture,", "def test_preprocess_options_store_true() -> None:\n    with tempdir() as chroot:\n        with fake_home():\n            chroot_path = Path(chroot)\n            testutils.create_files([\"a/b/c/d/__init__.py\"])\n            os.chdir(chroot_path / \"a/b/c\")\n            run = Run([\"--my-option\"])\n            assert run.options[\"my_option\"] is True\n", "def test_preprocess_options_no_takearg(capsys: CaptureFixture) -> None:\n    \"\"\"Test that we correctly handle options without takearg.\"\"\"\n    with tempdir() as chroot:\n        with fake_home():\n            chroot_path = Path(chroot)\n            testutils.create_files([\"a/b/c/d/__init__.py\"])\n            os.chdir(chroot_path / \"a/b/c\")\n            with pytest.raises(SystemExit):\n                Run([\"--enable-all-extensions\"])\n            out = capsys.readouterr()\n            # This output only exists when launched in verbose mode\n            assert \"No config file found, using default configuration\" in out.err\n\n\n", "def test_preprocess_options_no_args() -> None:\n    \"\"\"Test that no args is handled correctly.\"\"\"\n    with tempdir() as chroot:\n        with fake_home():\n            chroot_path = Path(chroot)\n            testutils.create_files([\"a/b/c/d/__init__.py\"])\n            os.chdir(chroot_path / \"a/b/c\")\n            processed_args = _preprocess_options(Run(), [])\n            assert processed_args == []\n\n\n\n"], "sample_261": ["    def test_invalid_duration_formats(self):\n        invalid_inputs = (\n            'invalid',\n            '1 day 10:15:30.abc',\n            '1 day 10:15.30',\n            '10:15:30:30',\n            '1 day 10:15',\n            '+1 day 10:15:30',\n            '-1 day 10:15:30,1',\n            '1.23',\n            '',\n            None,\n        )\n        for source in invalid_inputs:\n            with self.subTest(source=source):\n                self.assertIsNone(parse_duration(source))\n", "    def test_iso_format(self):\n        test_values = (\n            ('P1D', timedelta(days=1)),\n            ('P1DT2H3M4S', timedelta(days=1, hours=2, minutes=3, seconds=4)),\n            ('PT2H3M4S', timedelta(hours=2, minutes=3, seconds=4)),\n            ('P1DT2H3M4S.123\u03bcs', timedelta(days=1, hours=2, minutes=3, seconds=4, microseconds=123)),\n            ('PT2H3M4S.123\u03bcs', timedelta(hours=2, minutes=3, seconds=4, microseconds=123)),\n            ('P-1D', timedelta(days=-1)),\n            ('P-1DT2H3M4S', timedelta(days=-1, hours=2, minutes=3, seconds=4)),\n            ('PT2H3M4S.123\u03bcs', timedelta(hours=2, minutes=3, seconds=4, microseconds=123)),\n            ('P1Y', timedelta(days=365)),\n            ('P1Y2M', timedelta(days=365, seconds=59 * 60 * 60 * 2)),\n            ('P1Y2M3DT12H30M', timedelta(days=365, seconds=59 * 60 * 60 * 2, hours=12, minutes=30)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n\n", "    def test_iso8601_duration(self):\n        test_values = (\n            ('P4D', timedelta(days=4)),\n            ('P2DT3H', timedelta(days=2, hours=3)),\n            ('P3DT12H30M15S', timedelta(days=3, hours=12, minutes=30, seconds=15)),\n            ('PT1H30M45S', timedelta(hours=1, minutes=30, seconds=45)),\n            ('PT12H30M', timedelta(hours=12, minutes=30)),\n            ('PT12H30M15S.123456789', timedelta(hours=12, minutes=30, seconds=15, microseconds=123456789)),\n            ('P-2D', timedelta(days=-2)),\n            ('PT-1H30M', timedelta(hours=-1, minutes=-30)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n", "    def test_iso8601_duration(self):\n        test_values = (\n            ('P1D', timedelta(days=1)),\n            ('P1D2H30M', timedelta(days=1, hours=2, minutes=30)),\n            ('PT1H30M', timedelta(hours=1, minutes=30)),\n            ('PT30S', timedelta(seconds=30)),\n            ('P1DT2H30M', timedelta(days=1, hours=2, minutes=30)),\n            ('PT123.456S', timedelta(seconds=123, microseconds=456000)),\n            ('P-1D', timedelta(days=-1)),\n            ('P-1D2H30M', timedelta(days=-1, hours=-2, minutes=-30)),\n            ('P0DT1H15M', timedelta(hours=1, minutes=15)),\n            ('P10Y', timedelta(days=3650)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n\n\n\n", "    def test_iso8601(self):\n        test_values = (\n            ('P3DT10H30M', timedelta(days=3, hours=10, minutes=30)),\n            ('PT10H30M', timedelta(hours=10, minutes=30)),\n            ('PT10H30M1S', timedelta(hours=10, minutes=30, seconds=1)),\n            ('P1D', timedelta(days=1)),\n            ('P-1D', timedelta(-1)),\n            ('PT10H30M1S.123456', timedelta(hours=10, minutes=30, seconds=1, microseconds=123456)),\n            ('P1D00:00', timedelta(days=1)),\n            ('P1DT10H30M', timedelta(days=1, hours=10, minutes=30)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n\n"], "sample_1182": ["def test_hypergeom():\n    from sympy.functions.special.hypergeometric import hyper\n    \n    expr = hyper((1, 2), (3, 4), x)\n    \n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'scipy.special.hyperu(x, (1, 2), (3, 4))[0]'\n    \n    prntr = NumPyPrinter()\n    assert \"Not supported\" in prntr.doprint(expr)\n    \n    prntr = PythonCodePrinter()\n    assert \"Not supported\" in prntr.doprint(expr)\n    \n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr) ==  'mpmath.hyper(x, (1, 2), (3, 4))'   \n", "def test_log1p_and_log2():\n    from sympy.functions.special.log_functions import log1p, log2\n\n    expr1 = log1p(x)\n    expr2 = log2(x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr1) == 'scipy.special.log1p(x)'\n    assert prntr.doprint(expr2) == 'numpy.log2(x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr1) == 'numpy.log1p(x)'\n    assert prntr.doprint(expr2) == 'numpy.log2(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr1) == 'math.log1p(x)'\n    assert prntr.doprint(expr2) == 'math.log2(x)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr1) == 'mpmath.log1p(x)'\n    assert prntr.doprint(expr2) == 'mpmath.log2(x)'\n\n\n\n", "def test_log1p():\n    from sympy.functions.elementary.exponential import log1p\n\n    expr = log1p(x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'scipy.special.log1p(x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.log1p(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == 'math.log1p(x)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr) == 'mpmath.log1p(x)'\n\n", "def test_hypergeometric_functions():\n    from sympy.functions.special.hypergeometric import (\n        hyper,\n        MeijerG,\n    )\n    \n    x, a, b, c = symbols('x a b c')\n    expr1 = hyper([a, b], [c], x)\n    expr2 = MeijerG([[a, b], ], [[c], ], x)\n    \n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr1) == 'scipy.special.hyper([a, b], [c], x)'\n    assert prntr.doprint(expr2) == 'scipy.special.meijerg([a, b], [c], x)'\n    \n    prntr = NumPyPrinter()\n    assert \"Not supported\" in prntr.doprint(expr1)\n    assert \"Not supported\" in prntr.doprint(expr2)\n    \n    prntr = PythonCodePrinter()\n    assert \"Not supported\" in prntr.doprint(expr1)\n    assert \"Not supported\" in prntr.doprint(expr2)\n\n", "def test_issue_19497():\n    from sympy.tensor import IndexedBase\n\n    p = IndexedBase('p')\n    expr = p[0, 1]\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == 'p[0, 1]'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr) == 'p[0, 1]'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == 'p[0, 1]'\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'p[0, 1]'\n\n    prntr = SymPyPrinter()\n    assert prntr.doprint(expr) == 'p[0, 1]'\n"], "sample_1056": ["compilation error", "def test_custom_printed_object():\n    obj = CustomPrintedObject()\n    assert lambdarepr(obj) == 'lambda'\n    assert lambdarepr(obj, method='tensorflow') == 'tensorflow'\n    assert lambdarepr(obj, method='numpy') == 'numpy'\n    assert lambdarepr(obj, method='numexpr') == 'numexpr'\n    assert lambdarepr(obj, method='mpmath') == 'mpmath'\n    with raises(TypeError), \\\n         raises.TypeError(r\"Unsupported method.*\"):\n        lambdarepr(obj, method='garbage')\n", "def test_custom_printed_object():\n    obj = CustomPrintedObject()\n    assert lambdarepr(obj) == 'lambda'\n    assert lambdarepr(obj, method=\"tensorflow\") == 'tensorflow'\n    assert lambdarepr(obj, method=\"numpy\") == 'numpy'\n    assert lambdarepr(obj, method=\"mpmath\") == 'mpmath'\n\n    with raises(TypeError):\n        lambdarepr(obj, method=\"garbage\")\n", "def test_custom_printed_object():\n    obj = CustomPrintedObject()\n    assert lambdarepr(obj) == 'lambda'\n    assert lambdarepr(obj, method='tensorflow') == 'tensorflow'\n    assert lambdarepr(obj, method='numpy') == 'numpy'\n    assert lambdarepr(obj, method='numexpr') == 'numexpr'\n    assert lambdarepr(obj, method='mpmath') == 'mpmath'\n    with raises(TypeError):\n        lambdarepr(obj, method='invalid')\n", "def test_custom_printer():\n    o = CustomPrintedObject()\n    assert lambdarepr(o) == 'lambda'\n"], "sample_696": ["def test_argument_percent_default_is_deprecated(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import argparse\n\n        parser = argparse.ArgumentParser()\n        parser.add_argument(\"--foo\", type=str, default=\"%default\")\n        args = parser.parse_args()\n        print(args.foo)\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: pytest now uses argparse.*\",\n        ]\n    )\n", "def test_argument_percent_default(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import argparse\n        parser = argparse.ArgumentParser()\n        parser.add_argument('--arg', type=str, default='%', dest='value')\n        args = parser.parse_args()\n        assert args.value == '%'\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: pytest now uses argparse. \\\"\\\\%default\\\" should be changed to \\\"\\\\%(default)s\\\"\",\n        ]\n    )\n\n\n", "def test_argument_percent_default_is_deprecated(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import argparse\n\n            parser = argparse.ArgumentParser()\n            parser.add_argument('--foo', type=int, default=\"%default\")\n            args = parser.parse_args()\n            print(args.foo)\n\n        if __name__ == \"__main__\":\n            main()\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: pytest now uses argparse. \\\"\\\\%default\\\" should be changed to \\\"\\\\%(default)s\\\"\",\n        ]\n    )\n\n\n\n\n", "def test_argument_percent_default(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import argparse\n\n        parser = argparse.ArgumentParser()\n        parser.add_argument('--my_arg', type=str, default='%(default)s')\n        args = parser.parse_args()\n\n        assert args.my_arg == 'default'\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\"*PytestDeprecationWarning: pytest now uses argparse. \\\"\\\\%default\\\" should be changed to \\\"\\\\%(default)s\\\"*\"]\n    )\n", "def test_argument_percent_default(pytester: Pytester) -> None:\n    pytester.scribble(\n        \"\"\"\n            pytest.skip()\n    \n    \n            parser = argparse.ArgumentParser()\n            parser.add_argument(\"--arg\", type=str, default=\"%default\")\n            args = parser.parse_args([\"--arg\"])\n            assert args.arg == args.arg\n    \n    \n            parser = argparse.ArgumentParser()\n            parser.add_argument(\"--arg\", type=str, default=\"%default\")\n            args = parser.parse_args([\"--arg\"])\n            assert args.arg == \"%default\"\n    \n    \n            parser = argparse.ArgumentParser()\n            parser.add_argument(\"--arg\", type=str)\n            args = parser.parse_args([\"--arg\", \"some_value\"])\n            assert args.arg == \"some_value\"\n    \n    \n            parser = argparse.ArgumentParser()\n            parser.add_argument(\"--arg\", type=str, default=None)\n            args = parser.parse_args()\n            assert args.arg is None\n            args = parser.parse_args([\"--arg\", \"some_value\"])\n            assert args.arg == \"some_value\"\n\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*pytest.warning.PytestDeprecationWarning: pytest now uses argparse.*\",\n            \"'%default' should be changed to '%(default)s'\",\n        ]\n    )\n\n\n"], "sample_295": ["    def test_empty_select(self):\n        expr = ExpressionWrapper(Value(3), output_field=IntegerField())\n        self.assertEqual(expr.get_select_cols(), [])\n", "    def test_output_field_with_non_string(self):\n        with self.assertRaises(TypeError):\n            ExpressionWrapper(Value('foo'), output_field=None)\n", "    def test_output_field_with_null(self):\n        with self.assertRaisesMessage(TypeError, \"output_field must be a Field class\"):\n            ExpressionWrapper(Value(3), output_field=None)\n\n\n", "    def test_field_group_by(self):\n        expr = ExpressionWrapper(F('cost'), output_field=IntegerField())\n        self.assertEqual(expr.get_group_by_cols(alias=None), ['cost'])\n", "    def test_no_expression(self):\n        with self.assertRaises(ValueError):\n            ExpressionWrapper(None, output_field=IntegerField())\n"], "sample_399": ["    def test_aggregation_subquery_annotation_complex_grouping(self):\n        subquery_qs = (\n            Subquery(\n                Author.objects.filter(\n                    books__publisher=OuterRef(\"pk\")\n                ).values(\"id\")\n            )\n            .defer()\n            .alias(\"authors_id\")\n        )\n        books_counts = (\n            Book.objects.annotate(\n                authors_count=Count(\"authors\")\n            )\n            .annotate(\n                authors_subquery=subquery_qs\n            )\n            .values(\"pk\", \"authors_count\", \"authors_subquery\")\n        )\n        self.assertSequenceEqual(\n            list(books_counts),\n            [\n                {\"pk\": 1, \"authors_count\": 1, \"authors_subquery\": [1]},\n                {\"pk\": 2, \"authors_count\": 1, \"authors_subquery\": [2]},\n                {\"pk\": 3, \"authors_count\": 1, \"authors_subquery\": [3]},\n                {\"pk\": 4, \"authors_count\": 1, \"authors_subquery\": [4]},\n                {\"pk\": 5, \"authors_count\": 1, \"authors_subquery\": [5]},\n                {\"pk\": 6, \"authors_count\": 1, \"authors_subquery\": [6]},\n            ],\n        )", "    def test_aggregation_subquery_annotation_with_filter(self):\n        long_books_qs = Book.objects.filter(pages__gt=400)\n        subquery_qs = Subquery(long_books_qs.values(\"publisher\"))\n        authors_count = Author.objects.annotate(\n            long_book_count=Count(\n                OuterRef(\"book\"),\n                filter=Q(book__in=subquery_qs),\n            )\n        ).values_list(\"long_book_count\", flat=True)\n        self.assertSequenceEqual(authors_count, [1, 1, 0, 0, 0, 1])\n", "    def test_aggregation_with_outerref_on_subquery_with_exists(self):\n        with self.assertNumQueries(1):\n            result = Publisher.objects.annotate(\n                has_book=Exists(\n                    Book.objects.filter(\n                        pubdate__lt=F(\"pk\"),\n                        publisher=OuterRef(\"pk\"),\n                    )\n                ),\n            ).values(\"pk\", \"name\", \"has_book\")\n        self.assertSequenceEqual(\n            list(result),\n            [\n                {\"pk\": 1, \"name\": \"Apress\", \"has_book\": True},\n                {\"pk\": 2, \"name\": \"Jonno's House of Books\", \"has_book\": True},\n                {\"pk\": 3, \"name\": \"Morgan Kaufmann\", \"has_book\": True},\n                {\"pk\": 4, \"name\": \"Prentice Hall\", \"has_book\": True},\n                {\"pk\": 5, \"name\": \"Sams\", \"has_book\": True},\n            ],\n        )\n\n\n\n", "    def test_aggregation_with_subquery_on_empty_set(self):\n        with self.assertNumQueries(1):\n            result = Book.objects.annotate(\n                dummy=Subquery(\n                    Publisher.objects.filter(name=\"This publisher does not exist\").values(\"id\")\n                )\n            ).aggregate(dummy_count=Count(\"dummy\"))\n        self.assertEqual(result[\"dummy_count\"], 0)\n\n\n\n", "    def test_aggregation_default_with_nested_subqueries(self):\n        nested_qs = (\n            Author.objects.filter(age__gt=30)\n            .values(\"name\")\n            .annotate(author_count=Count(\"book\"))\n        )\n        expr = Avg(\"book__rating\", default=Max(nested_qs.annotate(\n            avg_rating=Avg(\"rating_new\")\n        ).values_list(\"author_count\", flat=True), output_field=FloatField()))\n        result = Book.objects.annotate(avg_rating=expr).values(\"avg_rating\")\n        self.assertEqual(list(result), [{'avg_rating': 4.0}])\n"], "sample_1164": ["def test_hamiltonian():\n    H = Hamiltonian('H')\n    H += 0.5 * (Jz**2 +Jz**2) + Pow(Jz, Integer(3))\n    H += Commutator(Jz,Operator('A'))\n    assert str(H) == 'H = 0.5*(Jz**2 + Jz**2) + Jz**3 + [Jz,A]'\n    ascii_str = \\", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_571": ["    def test_lmplot_style(self):\n\n        with plt.style.context(\"ggplot\"):\n            g = lm.lmplot(x=\"x\", y=\"y\", data=self.df, hue=\"h\")\n            assert g.axes[0, 0].get_lines()[0].get_marker() == \"o\"\n            assert g.axes[0, 0].grid() is True\n            assert g.axes[0, 0].get_xlabel() == \"x\"\n            assert g.axes[0, 0].get_ylabel() == \"y\"\n\n", "    def test_lmplot_err(self):\n\n        with pytest.raises(ValueError):\n            lm.lmplot(x=\"x\", y=\"y\", data=self.df, err=\"invalid\")\n\n        g = lm.lmplot(x=\"x\", y=\"y\", data=self.df, err=\"y_na\", hue=\"h\")\n        for ax in g.axes.flat:\n            assert len(ax.lines) == 2\n            assert len(ax.collections) == 4\n", "    def test_lmplot_custom_legend(self):\n\n        g = lm.lmplot(x=\"x\", y=\"y\", data=self.df, hue=\"h\", legend=None)\n        assert g._legend is None\n        \n\n        g = lm.lmplot(\n            x=\"x\", y=\"y\", data=self.df, hue=\"h\", legend=dict(\n                loc=\"upper right\", title=\"My Legend Title\"\n            ),\n        )\n        assert g._legend.get_title().get_text() == \"My Legend Title\"\n        assert g._legend.get_location() == \"upper right\"\n\n        g = lm.lmplot(\n            x=\"x\", y=\"y\", data=self.df, hue=\"h\", legend=None\n        )\n        assert g._legend is None\n", "    def test_lmplot_color_alpha(self):\n\n        g = lm.lmplot(x=\"x\", y=\"y\", data=self.df, hue=\"h\",\n                      alpha=0.5)\n        collections = g.axes[0, 0].collections\n        for collection in collections:\n            npt.assert_equal(collection._alpha, 0.5)\n", "    def test_residual_plot_with_yerr(self):\n\n        x = self.df.x\n        y = self.df.y + self.rs.randn(len(y)) * 0.5 \n        yerr = self.rs.randn(len(y)) * 0.2\n\n        ax = lm.residplot(x=x, y=y, yerr=yerr)\n        _, y_resid = ax.collections[0].get_offsets().T\n\n        # Check that the points are plotted with the specified yerr\n        for i in range(len(y_resid)):\n            expected_y_lower = y[i] - yerr[i]\n            expected_y_upper = y[i] + yerr[i]\n            assert ax.collections[0].get_offsets()[i][1] == y_resid[i]\n            assert ax.collections[0].get_offsets()[i][1] == y_resid[i]\n\n"], "sample_1191": ["    def test_hermite_normal_form_zeros_in_rows():\n        m = DM([[12, 0, 4], [0, 9, 6], [0, 0, 14]], ZZ)\n        hnf = DM([[12, 0, 0], [0, 9, 0], [0, 0, 14]], ZZ)\n        assert hermite_normal_form(m).to_dense() == hnf\n", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_424": ["    def test_references_field_by_related_name(self):\n        operation = FieldOperation(\n            \"Model\", \"field\", models.ForeignKey(\"Other\", models.CASCADE, related_name=\"other_field\")\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"other_field\", \"migrations\"), True\n        )\n        self.assertIs(operation.references_field(\"Other\", \"field\", \"migrations\"), False)\n\n\n", "    def test_references_field_by_related_name(self):\n        operation = FieldOperation(\n            \"Model\", \"field\", models.ForeignKey(\"Other\", models.CASCADE, related_name=\"related_model\")\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"related_model\", \"migrations\"), True\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"whatever\", \"migrations\"), False\n        )\n", "    def test_references_field_by_through_reverse(self):\n        operation = FieldOperation(\n            \"Model\", \"field\", models.ManyToManyField(\"Other\", through=\"Through\")\n        )\n        self.assertIs(\n            operation.references_field(\"Through\", \"model\", \"migrations\"), True\n        )\n        self.assertIs(\n            operation.references_field(\"Model\", \"reverse_field\", \"migrations\"), True\n        )\n        self.assertIs(\n            operation.references_field(\"Missing\", \"whatever\", \"migrations\"), False\n        )\n\n", "    def test_references_field_by_related_name(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ForeignKey(\"Other\", models.CASCADE, related_name=\"models\"),\n        )\n        self.assertIs(operation.references_field(\"Other\", \"models\", \"migrations\"), True)\n", "    def test_references_model_mixin(self):\n        migrations.CreateModel(\n            \"name\",\n            fields=[],\n            bases=(Mixin, models.Model),\n        ).references_model(\"other_model\", \"migrations\")\n"], "sample_14": ["def test_angle_invalid_unit():\n    with pytest.raises(ValueError) as excinfo:\n        Angle('1d', unit='invalid')\n    assert 'Invalid unit: invalid' in str(excinfo.value)\n\n", "    def test_latitude_too_large(value, dtype):\n        with pytest.raises(ValueError, match=r\"Latitude angle\\(s\\) must be within.*\"):\n            Latitude(value, u.rad, dtype=dtype)\n", "def test_angle_tostring_with_unit():\n    a1 = Angle(10, u.deg)\n    assert a1.to_string(unit=u.deg) == '10.0d'\n    assert a1._to_string(unit=u.deg) == '10.0d'\n\n    a2 = Angle(10, u.hourangle)\n    assert a2.to_string(unit=u.deg) == '10.0h'\n    assert a2._to_string(unit=u.deg) == '10.0h'\n\n    a3 = Angle(10 * np.pi / 180, u.rad)\n    assert a3.to_string(unit=u.deg) == '10.0d'\n    assert a3._to_string(unit=u.deg) == '10.0d'\n", "def test_latitude_out_of_limits_small_tolerance(value, dtype):\n    \"\"\"\n    Test that Latitude angles slightly larger than pi/2 are rejected \n    with a small tolerance, covering different data types.\n    \"\"\"\n    with pytest.raises(ValueError, match=r\"Latitude angle\\(s\\) must be within.*\"):\n        Latitude(value, u.rad, dtype=dtype)\n\n", "def test_angle_with_different_units():\n    \"\"\"\n    Tests Angle creation from mixed units.\n    \"\"\"\n    with pytest.raises(ValueError) as excinfo:\n        Angle(1 * u.deg, 2 * u.rad)\n    assert \"incompatible units\" in str(excinfo.value)\n\n    with pytest.raises(ValueError) as excinfo:\n        Angle([1 * u.deg, 1 * u.rad])\n    assert \"incompatible units\" in str(excinfo.value)\n\n    with pytest.raises(ValueError) as excinfo:\n        Angle({\n            \"a\": 1 * u.deg,\n            \"b\": 1 * u.rad,\n        })\n    assert \"incompatible units\" in str(excinfo.value)\n\n    with pytest.raises(ValueError) as excinfo:\n        Angle(['1d', 1 * u.rad])\n    assert \"incompatible units\" in str(excinfo.value)\n\n"], "sample_89": ["    def test_snapshot_files_handles_same_mtime(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n            snapshot1 = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot1)\n            # Simulate same mtime for the file\n            self.set_mtime(self.existing_file, snapshot1[self.existing_file])\n            snapshot2 = dict(self.reloader.snapshot_files())\n            self.assertEqual(snapshot1[self.existing_file], snapshot2[self.existing_file])\n", "    def test_snapshot_files_handles_non_existing_files(self):\n        self.reloader.snapshot_files()  # Initial snapshot shouldn't raise errors\n        non_existent_file = Path(self.tempdir, 'does_not_exist.py').absolute()\n        self.reloader.watched_files = mock.MagicMock(return_value=[non_existent_file])\n        snapshot = dict(self.reloader.snapshot_files())\n        self.assertNotIn(non_existent_file, snapshot)\n\n\n\n", "    def test_notify_file_changed_calls_with_correct_paths(self):\n        with mock.patch.object(self.reloader, 'process_changed_files') as mocked_process:\n            self.reloader.notify_file_changed(self.existing_file)\n        self.assertEqual(mocked_process.call_count, 1)\n        self.assertEqual(mocked_process.call_args[0][0], [self.existing_file])\n", "    def test_snapshot_files_for_existing_and_new_files(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n            snapshot1 = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot1)\n            new_file = self.ensure_file(self.tempdir / 'new_file.py')\n            self.increment_mtime(self.existing_file)\n            self.increment_mtime(new_file)\n            snapshot2 = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot2)\n            self.assertIn(new_file, snapshot2)\n            self.assertNotEqual(snapshot1, snapshot2)\n\n\n", "    def test_snapshot_files_handles_new_files(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n            snapshot1 = dict(self.reloader.snapshot_files())\n        self.assertNotIn(self.nonexistent_file, snapshot1)\n\n        self.ensure_file(self.nonexistent_file)\n        snapshot2 = dict(self.reloader.snapshot_files())\n        self.assertIn(self.nonexistent_file, snapshot2)\n\n\n\n"], "sample_479": ["    def test_rename_index_field(self):\n        self.assertOptimizesTo(\n            [\n                migrations.RenameIndex(\n                    \"Pony\", old_name=\"weight_index\", old_fields=(\"weight\", \"pink\")\n                ),\n                migrations.RenameIndex(\n                    \"Pony\", new_name=\"new_weight_index\", old_name=\"weight_index\",\n                    new_fields=(\"weight\", \"pink\")\n                ),\n            ],\n            [\n                migrations.RenameIndex(\n                    \"Pony\", new_name=\"new_weight_index\", old_fields=(\"weight\", \"pink\")\n                ),\n            ],\n        )\n\n\n", "    def test_rename_index_with_other_operations(self):\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    \"Foo\", [(\"name\", models.CharField(max_length=255))]\n                ),\n                migrations.CreateIndex(\n                    model_name=\"Foo\",\n                    name=\"idx_name\",\n                    fields=[\"name\"],\n                ),\n                migrations.RenameIndex(\"Foo\", \"idx_name\", \"new_idx_name\"),\n                migrations.RemoveField(\"Foo\", \"name\"),\n            ],\n            [\n                migrations.CreateModel(\n                    \"Foo\", [(\"name\", models.CharField(max_length=255))]\n                ),\n                migrations.CreateIndex(\n                    model_name=\"Foo\",\n                    name=\"new_idx_name\",\n                    fields=[\"name\"],\n                ),\n            ],\n        )\n", "    def test_delete_model_with_fk(self):\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\"Foo\", [(\"name\", models.CharField(max_length=255))]),\n                migrations.CreateModel(\"Bar\", [(\"name\", models.CharField(max_length=255))]),\n                migrations.AddField(\"Bar\", \"foo\", models.ForeignKey(\"migrations.Foo\", models.CASCADE)),\n                migrations.DeleteModel(\"Foo\"),\n            ],\n            [],\n        )\n\n", "    def test_rename_index_conflict(self):\n        self.assertDoesNotOptimize(\n            [\n                migrations.RenameIndex(\"Pony\", new_name=\"name_idx\", old_fields=(\"name\",)),\n                migrations.RenameIndex(\"Pony\", new_name=\"name_idx\", old_fields=(\"name\",)),\n            ],\n            [\n                migrations.RenameIndex(\"Pony\", new_name=\"name_idx\", old_fields=(\"name\",)),\n            ],\n        )\n", "    def test_rename_index_with_order(self):\n        self.assertOptimizesTo(\n            [\n                migrations.RenameIndex(\n                    \"Pony\",\n                    new_name=\"mid_name_order\",\n                    old_name=\"my_index\",\n                    order=migrations.Index(\n                        fields=[\"weight\", \"pink\"],\n                        order_by=[\"weight\"],\n                    ),\n                ),\n                migrations.RenameIndex(\n                    \"Pony\",\n                    new_name=\"new_name_order\",\n                    old_name=\"mid_name_order\",\n                    order=migrations.Index(\n                        fields=[\"weight\", \"pink\"],\n                        order_by=[\"pink\"],\n                    ),\n                ),\n            ],\n            [\n                migrations.RenameIndex(\n                    \"Pony\",\n                    new_name=\"new_name_order\",\n                    old_name=\"my_index\",\n                    order=migrations.Index(\n                        fields=[\"weight\", \"pink\"],\n                        order_by=[\"pink\"],\n                    ),\n                ),\n            ],\n        )\n\n\n\n"], "sample_995": ["compilation error", "def test_issue_7710():\n    assert simplify(Rational(0.5, 1000000000)) == Rational(1, 2000000000)\n    assert simplify(Rational(1, 1000000000)) == Rational(1, 1000000000)\n    assert simplify(Rational(0.0001, 1000000000)) == Rational(1, 10000000)\n", "compilation error", "def test_issue_10738():\n    assert isinstance(Float('1.23', precision=10).as_mpf(), mpmath.mpf)\n    assert isinstance(Float('1.23e10', precision=10).as_mpf(), mpmath.mpf)\n", "def test_issue_12272():\n    assert Float('1.23456789012345678901234567890123', 30).n(10) == '1.234567890'\n"], "sample_593": ["def test_attr_section_limiting_items(dataset):\n    formatted = fh.attr_section(dataset.attrs)\n    assert formatted.count(\"<li class='xr-section-item'>\") == len(dataset.attrs)\n    assert formatted.count(\"disabled\") == 0 \n\n    new_dataset = dataset.assign(\n        description=\"A more verbose description of the data attributes.\"\n    )\n    formatted = fh.attr_section(new_dataset.attrs)\n    assert formatted.count(\"<li class='xr-section-item'>\") == 10 \n    assert formatted.count(\"collapsed\") == 1 \n", "def test_array_repr_with_empty_dims(dataarray):\n    ds = dataarray.expand_dims(dim=\"foo\")\n    formatted = fh.array_repr(ds)\n    assert \"dim_0\" in formatted\n    assert \"foo\" in formatted\n", "def test_summarize_attrs_no_attrs():\n    attrs = {}\n    formatted = fh.summarize_attrs(attrs)\n    assert formatted == \"\"\n", "def test_summarize_attrs_empty(dataset):\n    formatted = fh.summarize_attrs({})\n    assert formatted == \"\"\n", "def test_summarize_attrs_empty_attrs(dataset):\n    ds = dataset.copy()\n    ds.attrs = {}\n    formatted = fh.summarize_attrs(ds.attrs)\n    assert formatted == \"\"\n"], "sample_999": ["def test_TensorProduct_printing_commutative():\n    from sympy.tensor.functions import TensorProduct\n    A = MatrixSymbol(\"A\", 3, 3)\n    B = MatrixSymbol(\"B\", 3, 3)\n    assert latex(TensorProduct(B, A)) == r\"A \\otimes B\"\n    \n", "def test_Lambda():\n    from sympy.functions import Lambda\n    lamda = Lambda(x, x**2)\n    assert latex(lamda) == r\"\\lambda x: x^{2}\"\n    assert latex(lamda(2)) == r\"2^{2}\"\n    assert latex(lamda(x + 1)) == r\"\\lambda x: \\left(x + 1\\right)^{2}\"\n\n\n\n", "def test_TensorProduct_with_symbols():\n    a, b, c, d = symbols('a:d')\n    A = TensorProduct(a, b)\n    B = TensorProduct(c, d)\n    assert latex(A + B) == r\"a \\otimes b + c \\otimes d\"\n", "def test_Function_printing():\n    from sympy.functions import *\n    assert latex(sin(x)) == r\"\\sin{\\left (x \\right )}\"\n    assert latex(cos(x)) == r\"\\cos{\\left (x \\right )}\"\n    assert latex(tan(x)) == r\"\\tan{\\left (x \\right )}\"\n    assert latex(log(x)) == r\"\\log{\\left (x \\right )}\"\n    assert latex(sinh(x)) == r\"\\sinh{\\left (x \\right )}\"\n    assert latex(cosh(x)) == r\"\\cosh{\\left (x \\right )}\"\n    assert latex(tanh(x)) == r\"\\tanh{\\left (x \\right )}\"\n    assert latex(exp(x)) == r\"\\exp{\\left (x \\right )}\"\n    assert latex(sqrt(x)) == r\"\\sqrt{x}\"\n    assert latex(abs(x)) == r\"\\left|{x}\\right|\"\n    assert latex(round(x)) == r\"\\left\\lfloor x \\right\\rfloor\"\n    assert latex(ceiling(x)) == r\"\\left\\lceil x \\right\\rceil\"\n    assert latex(floor(x)) == r\"\\left\\lfloor x \\right\\rfloor\"\n\n", "def test_issue_15516():\n    from sympy.tensor.tensor import Tensor, einstein\n    a, b, c, d = symbols('a b c d')\n    T = Tensor((a, b), 1, 'T')\n    assert latex(T[0, 0]) == r\"T_{0, 0}\"\n    T_contracted = einstein(T, [0, 1], [1, 0])\n    assert latex(T_contracted) == r\"a b\"\n"], "sample_459": ["    def test_enum_choices_raises_error_on_invalid_input(self):\n        f = models.IntegerField(choices=self.Choices.choices)\n        with self.assertRaises(ValidationError):\n            f.clean(\"not_a_key\", None)\n\n", "    def test_integerfield_cleans_string_representation_of_int(self):\n        f = models.IntegerField()\n        self.assertEqual(f.clean(\"2\", None), 2)\n\n\n", "    def test_enum_choices_raises_error_on_invalid_input(self):\n        f = models.IntegerField(choices=self.Choices.choices)\n        with self.assertRaises(ValidationError):\n            f.clean(\"a\", None)\n\n\n\n", "    def test_enum_choices_raises_error_on_invalid_input(self):\n        f = models.IntegerField(choices=self.Choices.choices)\n        with self.assertRaises(ValidationError):\n            f.clean(\"a\", None)\n\n\n\n\n", "    def test_enum_choices_raises_error_on_invalid_intput(self):\n        f = models.IntegerField(choices=self.Choices.choices)\n        with self.assertRaises(ValidationError):\n            f.clean(\"a\", None)\n"], "sample_827": ["def test_csc_row_median_errors():\n    csc = sp.csc_matrix([[1,2,3],[4,5,6]])\n    assert_raises(ValueError, csc_median_axis_0, csc, axis=1) \n\n\n", "def test_inplace_row_scale_lil():\n    rng = np.random.RandomState(0)\n    X = sp.rand(100, 200, 0.05).tolil()\n    scale = rng.rand(100)\n    XA = X * scale.reshape(-1, 1)\n\n    inplace_row_scale(X, scale)\n    assert_array_almost_equal(X.toarray(), XA)\n    assert_raises(TypeError, inplace_row_scale, X.tocsc(), scale)\n    assert_raises(TypeError, inplace_row_scale, X.tocsr(), scale)\n\n    X = X.astype(np.float32)\n    scale = scale.astype(np.float32)\n    XA = X * scale.reshape(-1, 1)\n    inplace_row_scale(X, scale)\n    assert_array_almost_equal(X.toarray(), XA)\n    assert_raises(TypeError, inplace_row_scale, X.tocsc(), scale)\n    assert_raises(TypeError, inplace_row_scale, X.tocsr(), scale)\n", "def test_csc_median_axis_1():\n    # Test csc_median_axis_1 correctly calculates the median along axis 1\n    rng = np.random.RandomState(0)\n    X = rng.rand(50, 100)\n    dense_median = np.median(X, axis=1)\n    csc = sp.csc_matrix(X)\n    sparse_median = csc_median_axis_1(csc)\n    assert_array_almost_equal(sparse_median, dense_median)\n\n    # Test with sparse matrix\n    X = rng.rand(10, 50)\n    X[X < 0.7] = 0.0\n    ind = rng.randint(0, 50, 10)\n    X[ind] = -X[ind]\n    csc = sp.csc_matrix(X)\n    dense_median = np.median(X, axis=1)\n    sparse_median = csc_median_axis_1(csc)\n    assert_array_almost_equal(sparse_median, dense_median)\n\n    # Test for toy data.\n    X = [[0, -2], [-1, -1], [1, 0], [2, 1]]\n    csc = sp.csc_matrix(X)\n    assert_array_almost_equal(csc_median_axis_1(csc), np.array([0.5, -0.5]))\n    X = [[0, -2], [-1, -5], [1, -3]]\n    csc = sp.csc_matrix(X)\n    assert_array_almost_equal(csc_median_axis_1(csc), np.array([0., -3.0]))\n\n    # Test that it raises an Error for non-csc matrices.\n    assert_raises(TypeError, csc_median_axis_1, sp.csr_matrix(X))\n", "def test_csc_row_median_nan():\n    # Test that csc_row_median handles NaN values correctly.\n    X = np.array([[0, -2, np.nan], [-1, -1, 0], [1, 0, np.nan]])\n    csc = sp.csc_matrix(X)\n    median = csc_median_axis_0(csc)\n    assert_array_almost_equal(median, np.array([0.0, -1.0]))\n    \n    X = np.array([[0, -2, np.nan], [-1, -1, np.nan], [1, 0, np.nan]])\n    csc = sp.csc_matrix(X)\n    median = csc_median_axis_0(csc, ignore_nan=True)\n    assert_array_almost_equal(median, np.array([0.0, -1.0]))\n", "def test_inplace_csr_row_normalize_l1():\n    X = np.array([[0, 3, 0],\n                  [2, -1, 0],\n                  [0, 0, 0],\n                  [9, 8, 7],\n                  [4, 0, 5]], dtype=np.float64)\n    X_csr = sp.csr_matrix(X)\n\n    inplace_csr_row_normalize_l1(X_csr, inplace=True)\n    expected_norms = np.linalg.norm(X_csr.toarray(), axis=1)\n    for i in range(X_csr.shape[0]):\n        assert_almost_equal(np.linalg.norm(X_csr[i], ord=1), 1)\n\n    inplace_csr_row_normalize_l1(X_csr, inplace=False)\n\n    assert_raises(TypeError, inplace_csr_row_normalize_l1, X_csr.tolil())\n\n\n\n"], "sample_976": ["def test_symbols_with_cls():\n    x = Symbol('x')\n    y = Symbol('y')\n\n    assert symbols('x', cls=int) == 0\n    assert symbols('y', cls=int) == 0\n    assert symbols('x,y', cls=int) == (0, 0)\n\n    assert symbols('x,y', cls=Function) != (x, y)\n    assert isinstance(symbols('x,y', cls=Function)[0], Function)\n    assert isinstance(symbols('x,y', cls=Function)[1], Function)\n\n    assert symbols('x:2', cls=int) == (0, 1)\n    assert symbols(('x:2', 'y:3'), cls=int) == ((0, 1), (0, 1, 2))\n\n    with raises(TypeError):\n        symbols('x:2', cls=list)\n\n", "def test_symbols_string_manipulation():\n    from sympy import sympify\n\n    assert symbols('x,y,z', cls=str) == (str(Symbol('x')), str(Symbol('y')), str(Symbol('z')))\n    assert symbols('x,y,z', cls=str).is_real is False\n\n    assert symbols('x,y,z', cls=int).is_integer\n\n    # test string evaluation\n    assert symbols('x:2', cls=int) == [0, 1]\n    assert symbols('x:2', cls = str) == [\"x0\", \"x1\"]\n\n\n    expr = symbols('x', cls=str)\n    assert expr != 'x'\n    assert expr == Symbol('x').as_string()\n\n\n    expr2 = symbols('x', cls=int)\n    assert expr2 != 'x'\n    assert expr2 == int(Symbol('x'))\n\n\n    # test symbols with complex type\n\n    raises(TypeError, lambda: symbols('x', cls=Complex))\n    raises(TypeError, lambda: symbols('x', cls=list))\n\n\n\n", "compilation error", "def test_symbols_with_cls():\n    from sympy.core import Function\n\n    x, y = symbols('x,y')\n\n    f = Function('f')\n\n    assert symbols('f(x,y)', cls=f) == f(x, y)\n    assert symbols('f(x,y)', cls=Function) == f(x, y)\n    assert symbols('g(x,y)', cls=Function) == Function('g')(x, y)\n\n\n\n", "def test_symbol_as_dummy():\n    x = Symbol('x')\n    d = x.as_dummy()\n    assert d.is_Dummy is True\n    assert d.name != x.name\n    assert not x.is_Dummy\n\n    x = Symbol('x', commutative=False)\n    d = x.as_dummy()\n    assert d.is_Dummy is True\n    assert d.name != x.name\n    assert not x.is_Dummy\n    assert d.is_commutative is False\n\n    y = Symbol('y')\n    assert x.as_dummy() != y.as_dummy()\n\n"], "sample_1118": ["def test_pow_simplification():\n    assert (A**2).doit() == MatPow(A, 2)\n    assert (A**3).doit() == MatPow(A, 3)\n    assert (A**0).doit() == Identity(A.shape[0])\n    assert (A**S.One).doit() == A\n    assert (A**-1).doit() == Inverse(A)\n    assert (A*B)**2 == MatPow(A*B, 2)\n    assert (A*B)**3 == MatPow(A*B, 3)\n    assert (A*B)**0 == Identity(A.shape[0])\n    assert (A*B)**S.One == A*B\n    assert (A*B)**-1 == Inverse(A*B)\n\n    \n    a = symbols('a', real=True)\n    mat = MatrixSymbol('M', 2, 2)\n    assert (mat**a).doit() == MatPow(mat, a)\n    assert (mat**a).is_commutative\n    \n", "def test_matpow():\n    assert (C**2).shape == (n, n)\n    assert (C**0).shape == (n, n)\n    assert (C**1).shape == (n, n)\n    assert  (C**S.Half).shape == (n, n)\n    assert (A**0).shape == (n, m)\n    assert (A**-1).shape == (m, n) if m != n else (n, n)\n    assert (A**-2).shape == (m, n) if m != n else (n, n)\n\n\n    assert (C*D)**2 == C**2 * D**2\n    assert C**2 * D**2 == (C*D)**2\n    assert (C**2).doit() == C*C\n    assert (MatPow(C, 2)).doit() == C*C\n    assert (C**(-1)).doit() == C.inverse()\n\n\n    assert (A**3).doit() == (A*A*A)\n    assert (A**(S.Half)).doit() == A**(1/2)\n    \n\n    assert (A*B)**2 == A**2 * B**2\n    assert (A*B)**2 == (A*B) * (A*B)\n    \n    assert (A*B)**-1 == (A*B).inverse()\n    assert Inverse(A*B) == (A*B).inverse()\n    \n    assert (A**2)**3 == A**6\n    assert (A**2)**(-1) == A**(-2)\n    assert (C**2)**(-1) == (C.inverse())**2\n", "compilation error", "def test_matpow_symbolic_powers():\n    assert MatPow(A, 2).shape == (n, m)\n    assert MatPow(A, 3).shape == (n, m)\n    assert MatPow(A, S.Two).shape == (n, m)\n    assert MatPow(A, S.Three).shape == (n, m)\n    assert MatPow(A, S.Half).shape == (n, m)\n", "def test_matpow_zero():\n    assert MatPow(A, 0).shape == A.shape\n    assert MatPow(A, 0) == Identity(A.shape[0])\n    assert MatPow(ZeroMatrix(A.shape), -1) is None\n\n    assert MatPow(Inverse(A), 0) == Identity(A.shape[0])\n    assert MatPow(Identity(A.shape), 0) == Identity(A.shape[0])\n\n"], "sample_867": ["def test_refit_with_single_parameter():\n    param_grid = {'C': [0.1, 1.0]}\n    X, y = make_classification(n_samples=10, random_state=0)\n    clf = LinearSVC()\n    grid_search = GridSearchCV(clf, param_grid, cv=2, refit=True)\n\n    grid_search.fit(X, y)\n\n    # Check that the best estimator has been refitted with the best parameter\n    assert grid_search.best_params_ == grid_search.best_estimator_.get_params()[\n        'C']\n\n\n\n", "    def custom_scorer(y_true, y_pred):\n        return y_pred.shape[0]\n", "def test_str_repr():\n    X, y = make_classification(n_samples=10, n_features=2, random_state=0)\n    \n    clf = GridSearchCV(SVC(random_state=0), param_grid={'C': [0.1, 1, 10]}, cv=3)\n    \n    # Test that repr is concise and includes key info\n    repr_output = repr(clf)\n    expected_output = (\n        \"<GridSearchCV(estimator=SVC(C=1.0, kernel='rbf', \"\n        \"random_state=0), param_grid={'C': [0.1, 1, 10]}, cv=3)>\"\n    )\n    assert repr_output in expected_output\n\n\n", "def test_grid_search_fit_pre_fit_estimators():\n    # Test that GridSearchCV works correctly when the estimator has\n    # been partially fitted.\n    X, y = make_classification(random_state=0)\n    pipe = Pipeline([\n        ('scaler', StandardScaler()),\n        ('clf', LinearSVC())\n    ])\n    param_grid = {'clf__C': [0.1, 1, 10]}\n    gs = GridSearchCV(pipe, param_grid, cv=3, n_jobs=1)\n    scaler = StandardScaler()\n    scaler.fit(X)\n    pipe.set_params(scaler=scaler)\n\n    gs.fit(X, y)\n    # Check that the estimator's score is correctly computed\n    # after partial fitting.\n    assert isinstance(gs.best_estimator_, Pipeline)\n    assert isinstance(gs.best_estimator_.steps[1][1], LinearSVC)\n    assert gs.best_estimator_.score(X, y) == gs.best_score_\n\n\n\n", "def test_get_param_names():\n    gs = GridSearchCV(SVC(), {'C': [0.1, 1], 'kernel': ['linear']}, cv=2)\n    assert_array_equal(gs.param_grid.keys(), ['C', 'kernel'])\n"], "sample_933": ["def test_gettext_index_target(app):\n    app.builder.build(['index'])\n\n    _msgid_getter = re.compile(r'msgid \"(.*)\"').search\n\n        m = _msgid_getter(msgid)\n        if m:\n            return m.groups()[0]\n        return None\n\n    pot = (app.outdir / 'index.pot').read_text()\n    msgids = [_f for _f in map(msgid_getter, pot.splitlines()) if _f]\n\n    expected_msgids = [\n        \"i18n with index targets\",\n        \"index target section\",\n        \"This is an index target paragraph.\",\n        \"See also\",\n        \"This is another index target paragraph.\",\n    ]\n    for expect in expected_msgids:\n        assert expect in msgids\n        msgids.remove(expect)\n\n    # unexpected msgid existent\n    assert msgids == []\n", "compilation error", "def test_gettext_domain_with_spaces(app):\n    # Regression test for #1000\n    app.builder.build(['test_domain'])\n    assert (app.outdir / 'test domain.pot').isfile()\n", "def test_gettext_additional_targets_index(app):\n    app.builder.build(['index_entries'])\n\n    _msgid_getter = re.compile(r'msgid \"(.*)\"').search\n\n        m = _msgid_getter(msgid)\n        if m:\n            return m.groups()[0]\n        return None\n\n    pot = (app.outdir / 'index_entries.pot').read_text()\n    msgids = [_f for _f in map(msgid_getter, pot.splitlines()) if _f]\n\n    expected_msgids = [  # Check for each index entry translation\n        \"i18n with index entries\",\n        \"index target section\",\n        \"this is :index:`Newsletter` target paragraph.\",\n        \"various index entries\",\n        \"That's all.\",\n        \"Mailing List\",\n        \"Newsletter\",\n        \"Recipients List\",\n        \"First\",\n        \"Second\",\n        \"Third\",\n        \"Entry\",\n        \"See\",\n        \"Module\",\n        \"Keyword\",\n        \"Operator\",\n        \"Object\",\n        \"Exception\",\n        \"Statement\",\n        \"Builtin\",\n    ]\n    for expect in expected_msgids:\n        assert expect in msgids\n        msgids.remove(expect)\n\n    # unexpected msgid existent\n    assert msgids == []\n\n\n\n", "def test_gettext_multiple_domains(app):\n    app.builder.build(['index_entries'])\n\n    assert (app.outdir / 'index_entries.pot').isfile()\n    assert (app.outdir / 'special.pot').isfile()\n"], "sample_28": ["    def test_rvkc_with_whitespace(self):\n        \"\"\"\n        Tests handling of record-valued keywords with whitespace in field\n        specifier\n\n        This test was added to address an edge case where whitespace\n        characters in field specifiers within RVKC were being incorrectly\n        handled.\n        \"\"\"\n\n        h = fits.Header()\n        h[\"DP1\"] = \"NAXIS: 2\"\n        h[\"DP1. NA XIS:  1\"] = 1\n        assert h[\"DP1.NAXIS\"] == 2.0\n        assert h[\"DP1.NA XIS: 1\"] == 1.0\n\n\n\n", "    def test_header_fromstring_comments(self):\n        header_str = \"\"\"", "    def test_invalid_card_value(self):\n        with pytest.raises(fits.verify.VerifyWarning):\n            h = fits.Header()\n            h[\"FOO\"] = \"1 2 3\"\n            h.verify(\"fix\")\n", "    def test_replace_and_update_card(self):\n        \"\"\"\n        Test the replace and update functionality of the Header object.\n\n        Specifically, this test checks that replacing a card with a new\n        card with a different value and/or comment works as expected.  It also\n        tests that the .update() method correctly updates the value of an\n        existing card.\n\n        Also, that adding a new keyword with a default value using\n        .set() works as expected.\n        \"\"\"\n        h = fits.Header()\n\n        h.set(\"A\", 1)\n        h.set(\"B\", \"Hello\")\n        h.set(\"C\", 3.14, \"pi\")\n\n        # Replace the card\n        h.replace(\"A\", 2)\n        assert h[\"A\"] == 2\n\n        # Update the card\n        h.update(\"B\", \"Goodbye\")\n        assert h[\"B\"] == \"Goodbye\"\n\n        # Add a new card with a default value\n        h.set(\"D\", \"World\", default_value=\"Astropy\")\n        assert h[\"D\"] == \"World\"\n\n        # Check that the default value is used if the card is not present\n        h = fits.Header()\n        h.set(\"E\", \"Default Value\")\n        new_h = h.copy()\n        new_h.set(\"F\", default_value=\"Test Default\")\n        assert new_h[\"E\"] == \"Default Value\"  \n        assert new_h[\"F\"] == \"Test Default\"\n", "    def test_header_comment_insertion(self):\n        h = fits.Header()\n        h.set(\"SIMPLE\", True)\n        h.set(\"COMMENT\", \"This is a comment.\")\n        h.set(\"HISTORY\", \"This is a history entry.\")\n        assert h[\"COMMENT\"] == \"This is a comment.\"\n        assert h[\"HISTORY\"] == \"This is a history entry.\"\n\n        h.insert(\"COMMENT\", \"A comment added after the previous one.\")\n\n        assert h[\"COMMENT\"] == \"A comment added after the previous one.\"\n        assert h[\"HISTORY\"] == \"This is a history entry.\"\n\n        assert len(h) == 3\n\n        h.insert(0, (\"COMMENT\", \"Another comment inserted at the beginning.\"))\n        assert h[\"COMMENT\"] == \"Another comment inserted at the beginning.\"\n        assert h[\"HISTORY\"] == \"This is a history entry.\"\n\n        h.insert(\"HISTORY\", \"This is another history entry inserted after the previous one.\")\n        assert h[\"HISTORY\"] == \"This is another history entry inserted after the previous one.\"\n\n\n        \n"], "sample_934": ["        def assert_classes(self, tag, expected_classes):\n            assert self.content_classes[tag].issuperset(expected_classes), (\n                f\"Content with tag '{tag}' should have classes \"\n                f\"{expected_classes} but has {self.content_classes[tag]}\"\n            )\n", "        def assert_classes(self, expected_classes, tag=None):\n            if tag:\n                assert self.content_classes[tag] == expected_classes, (\n                    f\"Expected classes for {self.name} with tag {tag}: \"\n                    f\"{expected_classes}, but got {self.content_classes[tag]}\"\n                )\n            else:\n                assert self.classes == expected_classes, (\n                    f\"Expected classes for {self.name}: \"\n                    f\"{expected_classes}, but got {self.classes}\"\n                )\n", "        def assert_consistency(self, expected_classes=None):\n            \"\"\"Assert that the classes assigned to each element are consistent with the expectations.\"\"\"\n\n            if expected_classes is None:\n                return\n\n            for tag, content_classes in self.content_classes.items():\n                assert content_classes == expected_classes, (\n                    f\"Classes for tag '{tag}' in role '{self.name}' \"\n                    f\"are inconsistent: {content_classes} != {expected_classes}\"\n                )\n            assert self.classes == expected_classes, (\n                f\"Classes for root element in role '{self.name}' \"\n                f\"are inconsistent: {self.classes} != {expected_classes}\"\n            )\n", "    def test_xref_consistency_complex(self):\n        roles = ['class', 'struct', 'union', 'func', 'member', 'type', 'concept', 'enum', 'enumerator']\n        roots = ['li', 'span', 'p']\n        contents = ['ul', 'ol', 'dl', 'table']\n        for role in roles:\n            print(f\"Testing role {role}\")\n            obj = RoleClasses(role, roots, contents) \n", "        def assert_classes(self, expected_classes):\n            assert self.classes == expected_classes\n            for tag, classes in self.content_classes.items():\n                assert classes == expected_classes, (\n                    f\"Content classes for tag '{tag}' should be \"\n                    f\"{expected_classes}, got {classes}\"\n                )\n"], "sample_1016": ["compilation error", "def test_MatrixElement_dot():\n    A = MatrixSymbol(\"A\", 2, 3)\n    B = MatrixSymbol(\"B\", 3, 2)\n    C = MatrixSymbol(\"C\", 2, 2)\n    assert mcode(A[0, 0] * B[0, :]) == \"A(1, 1) * B(:, 1)\"\n    assert mcode((A[0, :] * B) * C) == \"(A(1, :) * B) * C\"\n", "def test_matrix_element_symbols_with_functions():\n    A = MatrixSymbol('A', 2, 2)\n    B = Symbol('B')\n    C = Function('C')(x)\n    assert mcode(A[0, 0] * B + sin(C(A[1, 1]))) == (\n        \"A(1, 1)*B + sin(C(A(2, 2)))\"\n    )\n\n\n\n", "def test_octave_polygamma():\n    assert mcode(polygamma(1, x)) == \"polygamma(1, x)\"\n    assert mcode(polygamma(2, x)) == \"polygamma(2, x)\"\n    assert mcode(polygamma(3, x)) == \"polygamma(3, x)\"\n    assert mcode(polygamma(x, x)) == \"polygamma(x, x)\"\n", "def test_octave_log():\n    assert mcode(log(x)) == \"log10(x)\"\n    assert mcode(x**(1/log(x))) == \"x.^(1./log10(x))\"  \n    assert mcode(log(x, 2)) == \"log2(x)\"\n    assert mcode(log(x, base=pi)) == \"log(x, pi)\"\n"], "sample_891": ["def test_label_ranking_average_precision_score_with_sparse_y_score():\n    # Test that label_ranking_average_precision_score accepts sparse y_score.\n    # Non-regression test for #22575\n    y_true = np.array([[1, 0, 0], [0, 0, 1]])\n    y_score = csr_matrix([[0.5, 0.9, 0.6], [0, 0, 1]])\n    result = label_ranking_average_precision_score(y_true, y_score)\n    assert result == pytest.approx(2 / 3)\n", "def test_label_ranking_average_precision_score_with_pos_label(\n    y_true, y_score, pos_label, expected_result", "def test_label_ranking_average_precision_score_handles_ties():\n    y_true = np.array([[1, 0, 0], [0, 1, 1], [0, 0, 1]])\n    y_score = np.array([[0.5, 0.5, 0.5], [0.2, 0.8, 0.8], [0.1, 0.1, 0.9]])\n    ap = label_ranking_average_precision_score(y_true, y_score)\n    assert ap == pytest.approx(0.333)\n\n", "def test_top_k_accuracy_score_invalid_input(\n    y_true, y_score, k, labels, warn", "def test_top_k_accuracy_score_multiclass_labels(n_classes):\n    from sklearn.datasets import make_classification\n\n    X, y = make_classification(n_classes=n_classes, n_samples=100, random_state=42)\n\n    y_pred = np.random.RandomState(42).rand(100, n_classes)\n    labels = list(range(n_classes))\n    for k in range(1, n_classes + 1):\n        score = top_k_accuracy_score(y, y_pred, k=k, labels=labels)\n        assert 0 <= score <= 1\n\n\n"], "sample_153": ["    def test_database_checks_called_with_no_backends(self, mocked_check):\n        # Ensure the check function is not called if no databases are specified.\n        check_database_backends(databases=None)\n        self.assertFalse(mocked_check.called) \n", "    def test_database_checks_called_with_invalid_database(self, mocked_check):\n        mocked_check.side_effect = checks.Error(\"Error!\")\n        with self.assertRaises(checks.Error):\n            check_database_backends(databases=['invalid_database'])\n", "    def test_database_checks_filter_databases(self):\n        mock_check = mock.MagicMock()\n        with mock.patch('django.db.backends.base.validation.BaseDatabaseValidation.check', return_value=mock_check):\n            check_database_backends(databases=['other'])\n            mock_check.assert_called_once()\n            mock_check.assert_any_call('other')\n            mock_check.reset_mock()\n\n            check_database_backends(databases=['default'])\n            mock_check.assert_called_once()\n            mock_check.assert_any_call('default')\n\n\n\n", "    def test_model_checks_called(self, mocked_check):\n        ModelBase._check(using='default', databases=['default'])\n        self.assertFalse(mocked_check.called)\n        ModelBase._check(using='default', databases=self.databases)\n        self.assertTrue(mocked_check.called)\n", "    def test_database_checks_exclude_unusable_databases(self, mock_is_usable):\n        mock_is_usable.side_effect = [True, False]\n        check_database_backends(databases=['default', 'other'])\n        mock_is_usable.assert_has_calls([mock.call('default'), mock.call('other')]) \n"], "sample_98": ["    def test_connection_handling_between_requests(self):\n        \"\"\"\n        Tests that the server correctly handles consecutive requests from the same client,\n        especially when request data might carry over between requests due to persistent\n        connections.\n        \"\"\"\n        conn = HTTPConnection(LiveServerViews.server_thread.host, LiveServerViews.server_thread.port)\n        try:\n            conn.request('POST', '/method_view/', b'{}', headers={\"Connection\": \"keep-alive\"})\n            response = conn.getresponse()\n            self.assertFalse(response.will_close)\n            self.assertEqual(response.status, 200)\n            self.assertEqual(response.read(), b'POST')\n\n            conn.request('GET', '/example_view/', headers={\"Connection\": \"keep-alive\"})\n            response = conn.getresponse()\n            self.assertFalse(response.will_close)\n            self.assertEqual(response.status, 200)\n            self.assertEqual(response.read(), b'example view')\n        finally:\n            conn.close()\n", "    def test_status_code_handling(self):\n        with self.urlopen('/status_code_view/?code=403') as f:\n            self.assertEqual(f.status, 403)\n        with self.assertRaises(HTTPError) as err:\n            self.urlopen('/status_code_view/?code=500')\n        err.exception.close()\n        self.assertEqual(err.exception.code, 500)\n", "    def test_streaming_response_with_content_length_header(self):\n        conn = HTTPConnection(LiverServerThreadedTests.server_thread.host, LiverServerThreadedTests.server_thread.port)\n        try:\n            conn.request('GET', '/streaming_example_view/', headers={'Connection': 'keep-alive'})\n            response = conn.getresponse()\n            self.assertTrue(response.will_close)\n            self.assertEqual(response.getheader('Content-Length'), '10')\n            data = response.read()\n            self.assertEqual(data, b'Iamastream')\n            self.assertEqual(response.status, 200)\n        finally:\n            conn.close()\n", "    def test_connection_error_handled(self):\n        # Simulate a connection error by connecting to a non-existent host\n        try:\n            conn = HTTPConnection('invalid_host', self.port)\n            conn.request('GET', '/')\n            response = conn.getresponse()\n            self.fail(\"Expected an error for an invalid host connect.\")\n        except ConnectionRefusedError:\n            pass\n        except Exception as e:\n            self.fail(f\"Unexpected error: {e}\")\n\n\n", "    def test_environ_with_query_params(self):\n        with self.urlopen('/environ_view/?name=john&age=30') as f:\n            self.assertIn(b\"QUERY_STRING: 'name=john&age=30'\", f.read())\n"], "sample_201": ["    def test_not_finished(self):\n        storage = self.get_storage()\n        response = self.get_response()\n\n        # Store a message, then store more messages in a subsequent call,\n        # ensuring the initial message is part of a larger list.\n        storage.add(constants.INFO, 'message 1')\n        storage.add(constants.INFO, 'message 2')\n        storage.add(constants.INFO, 'message 3')\n\n        # Simulate processing messages and encountering the sentinel value,\n        # ensuring it's removed and the count reflects the remaining messages.\n        for _ in range(2):\n            pass\n        stored_messages = self.stored_messages_count(storage, response)\n        self.assertEqual(stored_messages, 2)\n\n        # Ensure the storage properly handles a cookie with the sentinel value.\n\n        # Ensure the storage properly handles a cookie with the sentinel value.\n\n\n", "    def test_not_finished(self):\n        storage = self.get_storage()\n        response = self.get_response()\n        messages = ['message 1', 'message 2', 'message 3']\n        storage._store(messages, response, remove_oldest=False)\n        self.assertEqual(stored_cookie_messages_count(storage, response), 3)\n        \n        # Messages should be retrieved\n        messages = list(storage)\n        self.assertEqual(messages, ['message 1', 'message 2', 'message 3'])\n        \n        # Now add more messages\n        storage._store([\n            'message 4',\n            'message 5',\n            'message 6'\n        ], response, remove_oldest=False)\n        self.assertEqual(stored_cookie_messages_count(storage, response), 6)\n        \n        # Retrieve messages\n        messages = list(storage)\n        self.assertEqual(messages, ['message 1', 'message 2', 'message 3', 'message 4', 'message 5', 'message 6'])\n", "    def test_removal_of_not_finished_sentinel(self):\n        storage = self.get_storage()\n        response = self.get_response()\n\n        # Add messages, exceeding the max cookie size\n        example_messages = ['test', 'me'] * 100\n        storage.add_all(*example_messages)\n        storage.update(response)\n        self.assertEqual(stored_cookie_messages_count(storage, response), 4)\n\n        # Additional stored messages should be removed\n        storage.add_all(*example_messages)\n        storage.update(response)\n        self.assertEqual(stored_cookie_messages_count(storage, response), 4)\n\n        # Verify the 'not_finished' sentinel was removed\n        self.assertEqual(list(storage), example_messages[:4])\n", "    def test_not_finished_sentinel(self):\n        storage = self.storage_class(self.get_request())\n        messages = [Message(constants.INFO, 'test 1'),\n                    Message(constants.INFO, 'test 2')]\n        set_cookie_data(storage, messages)\n        # Simulate exceeding the cookie size limit\n        messages_for_storage = messages * 5\n        set_cookie_data(storage, messages_for_storage, encode_empty=True)\n        # Verify that the not_finished sentinel is set\n        data = storage._decode(storage.request.COOKIES[storage.cookie_name])\n        self.assertEqual(data[-1], storage.not_finished)\n        # Delete the sentinel value and verify storage\n        storage._store(data[:-1], self.get_response())\n        data = storage._decode(storage.request.COOKIES[storage.cookie_name])\n        self.assertEqual(data[-1], storage.not_finished)\n", "    def test_not_finished_sentinel(self):\n        storage = self.get_storage()\n        response = self.get_response()\n\n        # Store messages which are too long for the cookie\n        messages = [str(x) for x in range(100)]\n        storage.add_many(messages)\n        storage.update(response)\n\n        # Retrieve the messages, and check that the \"not_finished\" sentinel\n        # is present\n        retrieved_messages, all_retrieved = storage._get()\n        self.assertFalse(all_retrieved)\n        self.assertEqual(retrieved_messages[-1], CookieStorage.not_finished)\n\n        # Retrieve the remaining messages\n        messages, all_retrieved = storage._get()\n        self.assertTrue(all_retrieved)\n        self.assertEqual(retrieved_messages + messages, messages)\n"], "sample_798": ["def test_ridge_multi_output():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([[7, 8], [9, 10], [11, 12]])\n\n    reg = Ridge(solver='svd', multi_output='raw_values')\n    reg.fit(X, y)\n    assert_array_almost_equal(reg.coef_,\n                              [[7, 8], [9, 10], [11, 12]])\n\n    reg = Ridge(solver='svd', multi_output='variance_threshold')\n    reg.fit(X, y)\n    assert_array_almost_equal(reg.coef_,\n                              [[7, 8], [9, 10], [11, 12]])\n\n    reg = Ridge(solver='svd', multi_output='variance_threshold',\n                alpha=10.0)\n    reg.fit(X, y)\n    assert_array_almost_equal(reg.coef_,\n                              [[7, 8], [9, 10], [11, 12]])\n", "compilation error", "def test_ridge_classifier_multiclass():\n    X, y = make_classification(n_samples=100, n_features=10, n_classes=3, random_state=0)\n    ridge_classifier = RidgeClassifier(solver='sag', multi_class='multinomial')\n    ridge_classifier.fit(X, y) \n\n    assert_equal(ridge_classifier.classes_, np.unique(y))\n    assert_greater(ridge_classifier.coef_.shape[0],\n                   X.shape[1])\n", "def test_ridge_gcv_sparse_matrix_tol():\n    X = sp.csr_matrix(np.random.rand(100, 5))\n    y = np.random.rand(100)\n\n    ridge_gcv = RidgeCV(cv=3, store_cv_values=True)\n    ridge_gcv.fit(X, y)\n\n    # Test with different tol values\n    ridge_gcv_tol_1e_4 = RidgeCV(cv=3, store_cv_values=True, tol=1e-4)\n    ridge_gcv_tol_1e_4.fit(X, y)\n\n    ridge_gcv_tol_1e_6 = RidgeCV(cv=3, store_cv_values=True, tol=1e-6)\n    ridge_gcv_tol_1e_6.fit(X, y)\n\n    # Check if the best alpha and cv_values_ are different\n    assert_almost_equal(ridge_gcv.alpha_, ridge_gcv_tol_1e_4.alpha_)\n    assert_almost_equal(ridge_gcv.alpha_, ridge_gcv_tol_1e_6.alpha_)\n\n    assert_array_almost_equal(ridge_gcv.cv_values_,\n                             ridge_gcv_tol_1e_4.cv_values_,\n                             decimal=5)\n    assert_array_almost_equal(ridge_gcv.cv_values_,\n                             ridge_gcv_tol_1e_6.cv_values_,\n                             decimal=5)\n\n\n\n", "def test_ridge_multi_fit():\n    X, y = make_regression(n_samples=1000, n_features=2, random_state=42)\n\n    ridge = Ridge(alpha=1.)\n\n    # fit on first 500 samples\n    ridge.fit(X[:500], y[:500])\n    \n    # fit on remaining samples\n    ridge.partial_fit(X[500:], y[500:])\n    assert_array_almost_equal(ridge.coef_, Ridge(alpha=1.).fit(X, y).coef_, decimal=5)\n\n\n\n"], "sample_1061": ["def test_Float_as_content():\n    assert Float(1.2345).as_content() == (0, long(5000), -3, 4) \n    assert Float(1.2345).as_content(precision=10).as_content() == (0, long(12345000), -3, 4) \n    assert Float(1.234567890123456789).as_content(precision=10).as_content() == (0, long(1234567890123456789), -3, 4) \n    assert Float(-1.234567890123456789).as_content(precision=10).as_content() == (0, long(-1234567890123456789), -3, 4) \n\n\n\n\n\n", "def test_issue_11190():\n    assert Float('1e100', 50).as_mpf().__class__ == mpf.mpf  \n    assert Float('1e-100', 50).as_mpf().__class__ == mpf.mpf\n\n", "def test_issue_12063():\n    assert Float('1.00000000000000000000000000000001').n(10) == 1\n    assert Float('1.00000000000000000000000000000002').n(10) == 1 \n    assert Float('1.00000000000000000000000000000003').n(10) == 1\n", "def test_abs_nan():\n    assert abs(nan) == nan\n\n\n", "def test_issue_11401():\n    a = Float('1.234e+100')\n    assert a.is_finite is False \n    assert a.is_infinite is True\n    assert a.is_nan is False\n\n\n\n"], "sample_1160": ["def test_issue_15035():\n    s = ImageSet(Lambda(x, x**2), S.Reals)\n    assert s.is_subset(S.Reals)\n    assert s.is_open == False\n    assert s.is_closed == True\n    assert s.boundary == S.Reals\n    assert s.closure == S.Reals\n", "compilation error", "def test_issue_7201():\n    assert Interval(0, 1).intersection(S.Naturals) == FiniteSet(1)\n    assert Interval(0, 1).intersection(S.Integers) == FiniteSet()\n    assert Interval(-1, 1).intersection(S.Naturals) == FiniteSet()\n    assert Interval(-1, 1).intersection(S.Integers) == Interval(-1, 1)\n", "def test_issue_17858_continue():\n    assert 1 in Range(-oo, oo, 2)\n    assert 0 in Range(oo, -oo, -2)\n    assert 1 not in Range(-oo, oo, 3)\n    assert -1 in Range(-oo, oo, 3)\n", "def test_issue_17858_border_cases():\n    assert 1 in Range(0, oo)\n    assert 0 not in Range(0, oo, -1)\n    assert oo in Range(0, oo)\n    assert -oo not in Range(0, oo)\n\n\n"], "sample_901": ["compilation error", "compilation error", "def test_k_means_with_categorical_features():\n    from sklearn.preprocessing import OneHotEncoder\n\n    # Create some sample data with categorical features\n    n_samples = 100\n    n_features = 5\n    n_categories = [2, 3, 4]\n    X = np.random.randint(0, n_categories[i], size=(n_samples, i + 1))\n    \n    # Encode categorical features\n    encoder = OneHotEncoder(handle_unknown='ignore')\n    X_encoded = encoder.fit_transform(X).toarray()\n\n    # Test KMeans on encoded data\n    km = KMeans(n_clusters=3, random_state=42)\n    km.fit(X_encoded)\n\n    # Assert that clusters make sense\n    assert len(set(km.labels_)) == 3\n", "def test_minibatch_kmeans_partial_fit_single_instance():\n    # check that partial fit with a single instance works\n    X = np.array([[1, 2]])\n    km = MiniBatchKMeans(n_clusters=2)\n    km.partial_fit(X)\n    assert km.cluster_centers_.shape[0] == 1\n    assert km.labels_.shape[0] == 1\n\n\n\n", "def test_minibatch_kmeans_update_centers_sparse():\n    # Test that updating cluster centers in minibatch works on sparse input\n    from sklearn.sparse import csr_matrix\n    mb_km = MiniBatchKMeans(batch_size=2, n_clusters=3, random_state=42)\n    X_sparse = csr_matrix(X)\n\n    # Fit on the sparse data\n    mb_km.fit(X_sparse)\n    # Now update centers\n    mb_km.partial_fit(X_sparse[0:2])\n\n    # Check centers are updated\n    assert not np.array_equal(mb_km.cluster_centers_, mb_km.cluster_centers_)\n"], "sample_117": ["    def test_password_too_short(self):\n        user = User.objects.get(username='testclient')\n        data = {'password1': 'abc', 'password2': 'abc'}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertIn('password1', form.errors)\n        self.assertIn(\n            'This password is too short. It must contain at least 8 characters.', form.errors['password1']\n        )\n\n", "    def test_invalid_password_strength(self):\n        user = User.objects.get(username='testclient')\n        data = {'password1': 'short', 'password2': 'short'}\n        form = AdminPasswordChangeForm(user, data)\n\n        self.assertFalse(form.is_valid())\n        self.assertIn(\n            _('Password must be at least 8 characters long.'),\n            form.errors['password1']\n        )\n        self.assertIn(\n            _('Password must be at least 8 characters long.'),\n            form.errors['password2']\n        )\n\n\n\n", "    def test_bug_19349_bound_password_field(self):\n        user = User.objects.get(username='testclient')\n        form = AdminPasswordChangeForm(data={}, instance=user)\n        # When rendering the bound password field,\n        # ReadOnlyPasswordHashWidget needs the initial\n        # value to render correctly\n        self.assertEqual(form.initial['password'], form['password'].value())\n", "    def test_invalid_password_format(self):\n        user = User.objects.get(username='testclient')\n        data = {'password1': 'password123!', 'password2': 'password123!'}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertIn(\n            _('Invalid password format. Please enter a strong password.'), form.errors['password1']\n        )\n\n\n\n\n", "    def test_password_too_short(self):\n        user = User.objects.get(username='testclient')\n        data = {'password1': 'a1', 'password2': 'a1'}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertEqual(form.errors['password1'], ['Password must contain at least 8 characters.'])  \n        self.assertEqual(form.errors['password2'], ['Password must contain at least 8 characters.']) \n"], "sample_1136": ["def test_issue_19644():\n    from sympy.polys import Poly\n    x = Symbol('x')\n    y = Symbol('y')\n    p = Poly(x**3 + y*x**2 + y**2, x, y)\n    p_copy  = p.copy()\n    assert p == p_copy\n    assert p.gens == p_copy.gens\n    assert p.domain == p_copy.domain\n    assert p.is_ground_poly == p_copy.is_ground_poly\n    assert p.is_commutative == p_copy.is_commutative\n    assert p.is_expr == p_copy.is_expr\n\n", "compilation error", "def test_issue_21421():\n    x = symbols('x')\n    p = Poly(x**2 + x + 1, x)\n    assert p.is_monic() is False\n    assert p.monic().as_expr() == x**2 + x + 1\n", "def test_poly_hash_equals_original():\n    poly = Poly(x + y, x, y, z)\n    copy = poly.copy()\n    assert hash(poly) == hash(copy), (\n        \"Hashes of copied polynomial and original do not match.\")\n", "def test_issue_20104():\n    p = Poly(sin(x)**2 + cos(x)**2, x)\n    assert p == Poly(1, x)\n"], "sample_763": ["        def __len__(self):\n            return 2\n", "        def __len__(self):\n            return 2\n", "        def __getitem__(self, index):\n            return self.shape", "compilation error", "        def __getitem__(self, index):\n            return None\n"], "sample_270": ["    def test_unique_constraint_include_pointing_to_fk(self):\n        class Target(models.Model):\n            pass\n\n        class Model(models.Model):\n            fk_1 = models.ForeignKey(Target, models.CASCADE, related_name='target_1')\n            fk_2 = models.ForeignKey(Target, models.CASCADE, related_name='target_2')\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['fk_1_id', 'fk_2'],\n                        include=['id'],\n                        name='name',\n                    ),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [])\n\n", "    def test_unique_constraint_include_with_joined_fields(self):\n        class Model(models.Model):\n            name = models.CharField(max_length=10)\n            field1 = models.PositiveSmallIntegerField()\n            field2 = models.PositiveSmallIntegerField()\n            parent = models.ForeignKey('self', models.CASCADE)\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['name'],\n                        include=['parent__field1', 'field2'],\n                        name='name',\n                    ),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [\n            Error(\n                \"'constraints' refers to the joined field 'parent__field1'\"\n                \".\",\n                obj=Model,\n                id='models.E041',\n            ),\n        ])\n\n\n\n", "    def test_unique_constraint_include_joined_fields(self):\n        class Model(models.Model):\n            name = models.CharField(max_length=10)\n            field1 = models.PositiveSmallIntegerField()\n            field2 = models.PositiveSmallIntegerField()\n            parent = models.ForeignKey('self', models.CASCADE)\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        name='name',\n                        fields=['field1'],\n                        include=['parent__field2'],\n                    ),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [\n            Error(\n                \"'constraints' refers to the joined field 'parent__field2'.\"\n                , obj=Model,\n                id='models.E041',\n            ),\n        ])\n\n", "    def test_unique_constraint_include_joined_fields(self):\n        class Model(models.Model):\n            name = models.CharField(max_length=10)\n            field1 = models.PositiveSmallIntegerField()\n            field2 = models.PositiveSmallIntegerField()\n            parent = models.ForeignKey('self', models.CASCADE)\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['name'],\n                        include=['parent__field1', 'parent__field2'],\n                        name='name',\n                    ),\n                ]\n\n        errors = Model.check(databases=self.databases)\n        expected_errors = [\n            Error(\n                \"'constraints' refers to the joined fields 'parent__field1', \"\n                \"'parent__field2'.\",\n                obj=Model,\n                id='models.E041',\n            ),\n        ]\n        self.assertCountEqual(errors, expected_errors)\n", "    def test_unique_constraint_include_pointing_to_fk(self):\n        class Target(models.Model):\n            pass\n\n        class Model(models.Model):\n            fk_1 = models.ForeignKey(Target, models.CASCADE, related_name='target_1')\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['id'],\n                        include=['fk_1'],\n                        name='name',\n                    ),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [])\n\n\n\n"], "sample_824": ["def test_check_empty_arrays():\n    # Ensures that checks return valid arrays for empty arrays.\n    XA = np.array([])\n    XB = np.array([])\n    XA_checked, XB_checked = check_pairwise_arrays(XA, XB)\n\n    assert_array_equal(XA, XA_checked)\n    assert_array_equal(XB, XB_checked)  \n\n    XA = np.array([])\n    XB = np.array([1])\n    assert_raises(ValueError, check_pairwise_arrays, XA, XB)\n\n    XA = np.array([[1], [2]])\n    XB = np.array([])\n    assert_raises(ValueError, check_pairwise_arrays, XA, XB) \n\n\n", "def test_pairwise_kernels_with_invalid_input():\n    # Test that pairwise kernels raise ValueError for invalid input types\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    \n    for invalid_input in (1, True, [1, 2],  \n                          {'a': 1, 'b': 2}, list(range(5)),\n                          np.array([1, 2, 3])):\n        with pytest.raises(TypeError):\n            pairwise_kernels(X, Y=invalid_input, metric='linear')\n\n    with pytest.raises(TypeError):\n        pairwise_kernels(\n            X=invalid_input, \n            Y=X, \n            metric='linear'\n        )\n\n\n\n", "def test_check_pairwise_arrays_sparse_inputs():\n    rng = np.random.RandomState(0)\n    XA = rng.random_sample((5, 4)).astype(np.float32)\n    XB = rng.random_sample((5, 4)).astype(np.float32)\n    XA_sparse = csr_matrix(XA)\n    XB_sparse = csr_matrix(XB)\n    XA_checked, XB_checked = check_pairwise_arrays(XA_sparse, XB_sparse)\n    assert_array_almost_equal(XA, XA_checked.toarray())\n    assert_array_almost_equal(XB, XB_checked.toarray())\n", "def test_check_sparse_data_types():\n    # Ensures that sparse matrices are handled correctly.\n    rng = np.random.RandomState(0)\n    XA = rng.random_sample((5, 4)).astype(np.float32)\n    XB = rng.random_sample((5, 4)).astype(np.float32)\n    XA_sparse = csr_matrix(XA)\n    XB_sparse = csr_matrix(XB)\n\n    # Test with float64 matrices\n    XA_checked, XB_checked = check_pairwise_arrays(XA_sparse, XB_sparse)\n    assert_equal(XA_checked.dtype, np.float64)\n    assert_equal(XB_checked.dtype, np.float64)\n\n    # Test with a mixed data type case, where one matrix is dense \n    # while the other is sparse\n    XA_checked, XB_checked = check_pairwise_arrays(XA_sparse, XA)\n    assert_equal(XA_checked.dtype, np.float64)\n    assert_equal(XB_checked.dtype, np.float64)\n\n\n\n", "def test_pairwise_kernels_dense_output_with_njobs():\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((1000, 10))\n    Y = rng.random_sample((500, 10))\n    K = pairwise_kernels(X, Y, metric='cosine')\n    \n    # Test with dense_output=True and n_jobs\n    K_dense = pairwise_kernels(X, Y, metric='cosine', dense_output=True, n_jobs=2)\n    assert_array_almost_equal(K, K_dense)\n    \n"], "sample_1142": ["def test_matrix_addition_simplification():\n    A = MatrixSymbol(\"A\", 2, 2)\n    B = MatrixSymbol(\"B\", 2, 2)\n    assert simplify(A + B) == A + B\n    assert simplify(B + A) == A + B\n    assert simplify(A + ZeroMatrix(2, 2)) == A\n    assert simplify(ZeroMatrix(2, 2) + B) == B\n\n\n", "compilation error", "def test_issue_14633():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n    expr = A + B*C\n    assert expr.func is Add\n    assert expr.args == (A, Mul(B, C))\n    expr = A + B\n    assert expr.func is Add\n    assert expr.args == (A, B)\n", "def test_matrix_element_subs():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    a, b = symbols('a, b')\n    Ai = A[0, 0]\n    Bi = B[0, 0]\n    assert Ai.subs(A, B) == Bi\n    assert Ai.subs({A: B, 0: 1, 0: 1}) == Bi\n    assert Ai.subs({A: B, 0: 0}) == Bi\n    assert Ai.subs({A: B, 1: 1, 1: 1}) == Bi\n    assert Ai.subs({A: B, 0: 1, 1: 1}) == Bi\n\n    with raises(IndexError):\n        Ai.subs({A: B, 3: 0, 0: 0})\n    with raises(IndexError):\n        Ai.subs({A: B, 0: 3, 0: 0})\n\n\n\n", "def test_matrix_add_inverse():\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    assert (A + B).inv() != A.inv() + B.inv()\n    assert (A - A.inv() + B).inv() != A.inv() + B.inv()\n    assert (A + B).inv() == (B + A).inv()\n\n"], "sample_1037": ["def test_transpose_with_symbols():\n    A = MatrixSymbol('A', 2, 3)\n    B = MatrixSymbol('B', 3, 2)\n    assert (A.T * B).shape == (2, 2)\n\n    assert (B.T * A).shape == (2, 3)\n    assert transpose(A.T).equals(A)\n", "def test_transpose():\n    A = MatrixSymbol('A', 3, 2)\n    assert A.T.shape == (2, 3)\n    assert (A.T).T == A\n\n\n\n\n    B = Matrix([[1, 2], [3, 4]])\n    C = B.T\n    assert C.shape == (2, 2)\n    assert (C.T).shape == (2, 2)\n    assert (B.T).T == B\n    assert B[0, 1] == C[1, 0]\n\n    D = MatrixSymbol('D', 2, 2)\n    assert (D.T*D).dot() == (D*D.T).dot()\n    assert (D.T*D).as_explicit() == (D*D.T).as_explicit()\n    \n    E = Matrix([[1, 2], [3, 4]])\n    F = E.T\n    assert (E + F).T == E.T + F.T\n    assert (E*F).T == F.T * E.T\n    assert (E - F).T == E.T - F.T\n\n\n\n\n    G = MatPow(E, 2)\n    assert (G.T).T == G\n    assert (E**2).T == (E.T)**2 \n\n\n", "def test_transpose_symbol():\n    A = MatrixSymbol('A', 3, 2)\n    assert A.T == Transpose(A)\n    assert (A.T).T == A\n    assert isinstance(A.T, Matrix)\n    assert A.T.shape == (2, 3)\n\n", "def test_transpose():\n    A = MatrixSymbol(\"A\", 3, 2)\n    B = MatrixSymbol(\"B\", 2, 3)\n\n    assert transpose(A).shape == (2, 3)\n    assert transpose(B).shape == (3, 2)\n\n    assert transpose(A).transpose() == A\n    assert transpose(B).transpose() == B\n\n    assert transpose(A + B) == transpose(A) + transpose(B)\n    assert transpose(A * B) == transpose(B) * transpose(A)\n\n\n    assert transpose(A**-1) == transpose(A).I\n    assert transpose(A**T) == transpose(A)\n\n\n\n    assert transpose(Matrix([[1,0,0], [0,2,0], [0,0,3]])) == Matrix([[1,0,0], [0,2,0], [0,0,3]])\n\n\n\n    x = MatrixSymbol('x', 2, 1)\n    assert  transpose(x) == x.T\n    assert transpose(transpose(x)) == x\n    assert transpose(x.T) == x\n\n\n\n    assert (A * B).transpose() == transpose(B) * transpose(A)\n\n\n", "def test_transpose():\n    A = MatrixSymbol('A', 3, 2)\n    assert (A.T).shape == (2, 3)\n    assert A.T.T == A\n    assert transpose(A).args == (A,)\n    assert transpose(A).shape == A.T.shape\n    assert transpose(A**2) == transpose(A)**2\n    assert transpose(A*B) == transpose(B)*transpose(A)\n    assert (A + B).T == A.T + B.T\n    assert (A - B).T == A.T - B.T\n    assert (A*scalar).T == scalar*A.T\n    assert (scalar*A).T == scalar*A.T\n\n    assert (transpose(A) + transpose(A)).shape == (2, 2)\n    assert (transpose(A) * transpose(A)).shape == (2, 2)\n\n    assert (A**-1).T == transpose(A)**-1\n\n\n"], "sample_846": ["def test_column_transformer_with_dict_vectorizer():\n    X_array = np.array([[{'a': 1, 'b': 2}, {'c': 3, 'd': 4}],\n                        [{'a': 5, 'e': 6}, {'f': 7, 'g': 8}]])\n    X = pd.DataFrame(X_array, columns=None)\n    ct = ColumnTransformer([('trans', DictVectorizer(), [0])],\n                           remainder='passthrough')\n    exp = pd.DataFrame(dict([\n        ('a', [1, 5]), ('b', [2, 6], ), ('c', [3, None]),\n        ('d', [4, None]), ('e', [None, 6]), ('f', [None, 7]),\n        ('g', [None, 8]),\n        ('Unnamed: 0', [0, 1])\n    ]), columns=['a', 'b', 'c', 'd', 'e', 'f', 'g', 'Unnamed: 0'])\n    assert_frame_equal(ct.fit_transform(X), exp)\n\n", "compilation error", "compilation error", "def test_column_transformer_with_missing_values():\n    X_array = np.array([[0, 1, np.nan], [2, 4, 6]])\n    ct = ColumnTransformer([('trans', Trans(), [0]), ('trans2', StandardScaler(), [1, 2])],\n                           remainder='drop')\n    ct.fit(X_array)\n    assert isinstance(ct.transformers_[0][1], Trans)\n    assert isinstance(ct.transformers_[1][1], StandardScaler)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] != 'remainder'\n    X_trans = ct.transform(X_array)\n    assert X_trans.shape == (2, 1)\n    assert np.isnan(X_trans[:, 0]).any()\n\n\n\n", "compilation error"], "sample_69": ["    def test_snapshot_files_handles_concurrent_modifications(self):\n            self.increment_mtime(self.existing_file)\n\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n            snapshot1 = dict(self.reloader.snapshot_files())\n            with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n                future1 = executor.submit(modify)\n                future2 = executor.submit(modify)\n                future1.result()\n                future2.result()\n            snapshot2 = dict(self.reloader.snapshot_files())\n            # Ensure both modifications are reflected.\n            self.assertNotEqual(snapshot1[self.existing_file], snapshot2[self.existing_file])\n", "    def test_snapshot_files_handles_existing_files_changed_mtime(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n            snapshot1 = dict(self.reloader.snapshot_files())\n            self.increment_mtime(self.existing_file)\n            snapshot2 = dict(self.reloader.snapshot_files())\n            self.assertEqual(snapshot1[self.existing_file], snapshot2[self.existing_file])\n\n", "    def test_snapshot_files_retains_existing_files(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n            snapshot1 = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot1)\n            snapshot2 = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot2)\n            self.assertEqual(snapshot1[self.existing_file], snapshot2[self.existing_file])\n\n", "    def test_snapshot_files_handles_multiple_updates(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n            snapshot1 = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot1)\n            self.increment_mtime(self.existing_file)\n            snapshot2 = dict(self.reloader.snapshot_files())\n            self.assertNotEqual(snapshot1[self.existing_file], snapshot2[self.existing_file])\n            self.increment_mtime(self.existing_file)\n            snapshot3 = dict(self.reloader.snapshot_files())\n            self.assertNotEqual(snapshot2[self.existing_file], snapshot3[self.existing_file])\n\n\n\n", "    def test_snapshot_files_handles_errors(self):\n        with mock.patch.object(os, 'stat', side_effect=OSError()) as mocked_stat:\n            with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n                snapshot = dict(self.reloader.snapshot_files())\n                self.assertNotIn(self.existing_file, snapshot)\n                mocked_stat.reset_mock()\n                mocked_stat.return_value = os.stat(str(self.existing_file))\n                snapshot = dict(self.reloader.snapshot_files())\n                self.assertIn(self.existing_file, snapshot)\n\n\n\n"], "sample_806": ["    def test_gradient_boosting_with_init_invalid_criterion(criterion):\n        # Make sure error is raised if init estimators have invalid criterion\n        X, y = make_regression(random_state=0)\n        init = GradientBoostingRegressor(criterion=criterion)  \n        with pytest.raises(ValueError,\n                           match=r\"The init parameter must be a valid estimator\"):\n            GradientBoostingRegressor(init=init).fit(X, y)\n", "def test_gradient_boosting_set_params(n_estimators):\n    est = GradientBoostingClassifier(n_estimators=10, random_state=42)\n    for param in ['n_estimators', 'learning_rate', 'max_depth', 'criterion',\n                  'init']:\n        if param == 'n_estimators' and n_estimators not in [0, -1, None]:\n            est.set_params(**{param: n_estimators})\n            assert_equal(est.n_estimators, n_estimators)\n        else:\n            est.set_params(**{param: n_estimators})\n            assert_equal(getattr(est, param), n_estimators)\n\n\n    est.set_params()  # test empty set_params\n", "def test_gradient_boosting_with_init_estimator_missing_attributes(gb_clf):\n    # Test that a ValueError is raised when the init estimator is missing\n    # required attributes (fit, predict, predict_proba)\n    class DummyEstimator:\n        pass\n\n    with pytest.raises(ValueError,\n                       match=\"The init estimator must have a 'fit' \"\n                             \"method.\"):\n        gb_clf(init=DummyEstimator()).fit(X, y)  \n", "    def test_gradient_boosting_with_init_unsupported_loss():\n        X, y = make_regression(random_state=0)\n\n        # Test that an error is raised if the init estimator uses\n        # a loss function not supported by GradientBoosting\n        init = GradientBoostingRegressor(loss='absolute_error')  \n        with pytest.raises(ValueError,\n                           match=\"GradientBoostingRegressor does not support\"):\n            GradientBoostingRegressor(init=init).fit(X, y)\n", "    def test_gradient_boosting_with_init_custom_fit(GBEstimator, dataset_maker):\n        from sklearn.base import BaseEstimator, clone\n\n        class CustomFitEstimator(BaseEstimator):\n                pass\n\n        X, y = dataset_maker(random_state=42)\n\n        gb = GBEstimator(init=CustomFitEstimator())\n        gb.fit(X, y)  # Should not raise errors even if init doesn't have a fit\n\n\n\n"], "sample_1066": ["compilation error", "compilation error", "compilation error", "def test_print_Symbol():\n    assert mathml(Symbol('a'), printer='presentation') == '<mi>a</mi>'\n    assert mathml(Symbol('x'), printer='presentation') == '<mi>x</mi>'\n    assert mathml(Symbol('y'), printer='presentation') == '<mi>y</mi>'\n    assert mathml(Symbol('z'), printer='presentation') == '<mi>z</mi>'\n    assert mathml(Symbol('a1'), printer='presentation') == '<mi>a1</mi>'\n    assert mathml(Symbol('x_y'), printer='presentation') == '<mi>x_y</mi>'\n    assert mathml(Symbol('a_b_c'), printer='presentation') == '<mi>a_b_c</mi>'\n", "compilation error"], "sample_495": ["    def test_paginator_with_complex_ordering(self):\n        \"\"\"\n        Test pagination with a QuerySet that uses complex ordering.\n        \"\"\"\n        # Create some models with different ordering fields\n        for x in range(1, 10):\n            a = Article(headline='Article %s' % x, pub_date=datetime(2005, 7, 29),\n                        author='Author %s' % x)\n            a.save()\n\n        # Construct a Paginator with a complex ordering\n        paginator = Paginator(Article.objects.order_by('pub_date', '-headline'), 5)\n        p = paginator.page(1)\n        self.assertQuerysetEqual(p.object_list, [\n            \"<Article: Article 5>\",\n            \"<Article: Article 6>\",\n            \"<Article: Article 7>\",\n            \"<Article: Article 8>\",\n            \"<Article: Article 9>\"\n        ])\n\n", "    def test_cache_paginator_attributes(self):\n        \"\"\"\n        Paginator attributes like count, num_pages, and page_range\n        are cached after the first call.\n        \"\"\"\n        paginator = Paginator(Article.objects.order_by('id'), 5)\n        before = paginator.count\n        paginator.page(1)\n        self.assertEqual(paginator.count, before)\n\n", "    def test_get_page_with_invalid_page_number_string(self):\n        paginator = Paginator(Article.objects.order_by('id'), 5)\n        with self.assertRaises(PageNotAnInteger):\n            paginator.get_page('abc')\n", "    def test_paginating_queryset_with_ordering(self):\n        Article.objects.bulk_create([\n            Article(headline='Article 1', pub_date=datetime(2004, 7, 29)),\n            Article(headline='Article 2', pub_date=datetime(2005, 7, 29)),\n            Article(headline='Article 3', pub_date=datetime(2006, 7, 29)),\n            Article(headline='Article 4', pub_date=datetime(2005, 7, 29)),\n        ])\n        paginator = Paginator(Article.objects.order_by('-pub_date'), 2)\n        p = paginator.page(1)\n        self.assertQuerysetEqual(p.object_list, [\n            \"<Article: Article 3>\",\n            \"<Article: Article 2>\",\n        ])\n\n", "    def test_ordering_for_pagination(self):\n        \"\"\"\n        Test that ordering applied to the queryset is respected by pagination.\n        \"\"\"\n        paginator = Paginator(Article.objects.order_by('-pub_date'), 5)\n        p = paginator.page(1)\n        self.assertEqual(p.object_list[0].headline, 'Article 9')  \n        # Ensure the ordering is preserved even on subsequent pages.\n        p = paginator.page(2)\n        self.assertEqual(p.object_list[0].headline, 'Article 8')\n"], "sample_1200": ["def test_issue_24463():\n    from sympy.physics.units import Quantity, meter, second\n    Q1 = Quantity(\"Q1\")\n    SI.set_quantity_dimension(Q1, meter / second)\n    assert Q1.is_prefixed is False\n    \n    Q2 = Quantity(\"Q2\")\n    SI.set_quantity_dimension(Q2, meter / second**2)\n    assert Q2.is_prefixed is False\n    \n    Q3 = Quantity(\"Q3\", abbrev=\"m/s\")\n    SI.set_quantity_dimension(Q3, meter / second)\n    assert Q3.is_prefixed is False\n", "def test_issue_25214():\n    from sympy.physics.units import Quantity, kilogram, meter, second\n\n    u = Quantity(\"u\")\n    v = Quantity(\"v\")\n\n    SI.set_quantity_dimension(u, kg*meter**2/second**2)\n    SI.set_quantity_dimension(v, kg*meter/second)\n\n    u.set_global_relative_scale_factor(1, kilogram*meter**2/second**2)\n    v.set_global_relative_scale_factor(1, kilogram*meter/second)\n    expr = u.convert_to(v)\n\n    assert expr == u * (1 / (v * second))\n", "def test_unit_conversion_with_non_standard_dimensions(): \n    good_grade = Quantity(\"good_grade\")\n    kilo_good_grade = Quantity(\"kilo_good_grade\")\n    centi_good_grade = Quantity(\"centi_good_grade\")\n\n    kilo_good_grade.set_global_relative_scale_factor(1000, good_grade)\n    centi_good_grade.set_global_relative_scale_factor(S.One/10**5, kilo_good_grade)\n\n    charity_points = Quantity(\"charity_points\")\n    milli_charity_points = Quantity(\"milli_charity_points\")\n    missions = Quantity(\"missions\")\n\n    milli_charity_points.set_global_relative_scale_factor(S.One/1000, charity_points)\n    missions.set_global_relative_scale_factor(251, charity_points)\n    \n    assert convert_to(\n        (kilo_good_grade*milli_charity_points*meter),\n        centi_good_grade * missions * centimeter\n    ) == S.One * 10**5 / (251*1000) * meter / centimeter\n\n\n\n", "def test_issue_24433():\n    from sympy.physics.units import  speed_of_light\n    from sympy.physics.units.definitions import  length, time\n    \n    assert speed_of_light.unit == 'm/s'\n    assert speed_of_light.dimension == length / time\n    assert speed_of_light.scale_factor == 299792458\n    assert speed_of_light.convert_to(meter/second) == speed_of_light  \n", "def test_issue_24482():\n    from sympy.physics.units import Quantity, meter, second\n    u = Quantity('u')\n    v = Quantity('v')\n    SI.set_quantity_dimension(u, meter)\n    SI.set_quantity_dimension(v, second)\n    u.set_global_relative_scale_factor(1, meter)\n    v.set_global_relative_scale_factor(1, second)\n    expr = u / v\n    assert SI._collect_factor_and_dimension(expr)[1] == meter / second \n\n\n"], "sample_100": ["    def test_snapshot_files_with_nested_glob(self):\n        inner_file = self.ensure_file(self.tempdir / 'test' / 'test.py')\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file, inner_file]):\n            snapshot1 = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot1)\n            self.assertIn(inner_file, snapshot1)\n            self.increment_mtime(inner_file)\n            snapshot2 = dict(self.reloader.snapshot_files())\n            self.assertNotEqual(snapshot1[inner_file], snapshot2[inner_file])\n", "    def test_snapshot_files_handles_stat_exceptions(self, mock_os_stat):\n        mock_os_stat.side_effect = OSError(\"Cannot access file\")\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n            snapshot = dict(self.reloader.snapshot_files())\n            self.assertNotIn(self.existing_file, snapshot)\n", "    def test_snapshot_files_handles_empty_watched_files(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[]):\n            self.assertEqual(dict(self.reloader.snapshot_files()), {})\n", "    def test_snapshot_files_handles_removed_files(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n            snapshot1 = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot1)\n            os.remove(self.existing_file)\n            snapshot2 = dict(self.reloader.snapshot_files())\n            self.assertNotIn(self.existing_file, snapshot2)\n\n", "    def test_snapshot_files_handles_multiple_changes(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file, self.nonexistent_file]):\n            snapshot1 = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot1)\n            self.assertNotIn(self.nonexistent_file, snapshot1)\n            self.increment_mtime(self.existing_file)\n            self.increment_mtime(self.nonexistent_file)\n            snapshot2 = dict(self.reloader.snapshot_files())\n            self.assertNotEqual(snapshot1[self.existing_file], snapshot2[self.existing_file])\n"], "sample_470": ["    def test_keep_lazy_function_with_one_lazy_argument(self):\n        @keep_lazy(str)\n            return f\"Result: {arg1} and {arg2}\"\n\n        self.assertEqual(my_func(lazy(lambda: \"hello\"), \"world\"), \"Result: hello and world\")\n        self.assertEqual(my_func(\"hello\", \"world\"), \"Result: hello and world\")\n\n", "    def test_keep_lazy_decorator(self):\n        @keep_lazy(int)\n            return x + 1\n\n        self.assertEqual(add_one(1), 2)\n        self.assertEqual(add_one(lazy(lambda: 2, int)), 3)\n", "    def test_keep_lazy(self):\n        @keep_lazy(str)\n            return text\n\n        @keep_lazy\n            return x + y\n\n        self.assertEqual(keep_lazy_str(\"hello\"), \"hello\")\n        self.assertEqual(keep_lazy_no_args(\"hello\", \"world\"), \"helloworld\")\n        self.assertEqual(keep_lazy_no_args(1, 2), 3)\n        self.assertEqual(keep_lazy_no_args(1, lazy(lambda: 2, int)), 3)\n\n\n\n\n", "    def test_keep_lazy(self):\n            return a + b\n\n        lazy_a = lazy(lambda: 1, int)\n        lazy_b = lazy(lambda: 2, int)\n        \n        result = keep_lazy(int)(my_func)(lazy_a(), lazy_b())\n        self.assertEqual(result, 3)\n\n        result = my_func(lazy_a(), lazy_b())\n        self.assertEqual(result, 3) \n\n        # Test with only one lazy argument\n        result = keep_lazy(int)(my_func)(lazy_a(), 3)\n        self.assertEqual(result, 4)\n\n", "    def test_keep_lazy_mixed_args(self):\n        @lazy(int)\n            return 42\n\n        @lazy(str)\n            return \"hello\"\n\n        @keep_lazy(int, str)\n            return a + b\n\n        self.assertEqual(my_func(lazy_int_arg(), lazy_str_arg()), 42 + \"hello\")\n\n"], "sample_947": ["compilation error", "def test_cattribute(app):\n    text = \".. c:attribute:: PyObject* PyTypeObject.tp_flags\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1], addnodes.desc, desctype=\"attribute\",\n                domain=\"c\", objtype=\"attribute\", noindex=False)\n\n    entry = _get_obj(app, 'PyTypeObject.tp_flags')\n    assert entry == ('index', 'c.PyTypeObject.tp_flags', 'attribute') \n", "def test_cmacro(app):\n    text = \".. c:macro:: MY_MACRO(int i)\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1], addnodes.desc, desctype=\"macro\",\n                domain=\"c\", objtype=\"macro\", noindex=False)\n\n    entry = _get_obj(app, 'MY_MACRO')\n    assert entry == ('index', 'c.MY_MACRO', 'macro')\n", "def test_c_type_specifier(app):\n    text = \".. c:type:: int * const \"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1], addnodes.desc, desctype=\"type\",\n                domain=\"c\", objtype=\"type\", noindex=False)\n\n    entry = _get_obj(app, 'int * const')\n    assert entry == ('index', 'c.int * const', 'type')\n\n", "compilation error"], "sample_994": ["def test_issue_10600():\n    a = Symbol('a', positive=True)\n    b = Symbol('b', positive=True)\n    assert (a**b).rewrite(exp).is_valid\n    assert (a**b).rewrite(exp).limit(a, oo) == oo\n    assert (a**b).rewrite(exp).limit(b, oo) == oo\n    assert (a**b).rewrite(exp).limit(b, 0) == 1\n    assert (a**b).rewrite(exp).limit(a, 0) == 0\n\n\n\n", "def test_issue_10597():\n    assert Float(15.5, 10) == Float(15.5)\n    assert Float(15.5, 10) != Float(15.5, 15)\n", "def test_issue_10835():\n    assert Float('1.2345678901234567890123456789').n(5) == '1.23457'\n    assert Float('1.2345678901234567890123456789').n(10) == '1.234567890123'\n\n\n\n", "def test_issue_10800():\n    assert Float('1e1000').is_finite is True\n    assert Float(S(1000)**1000).is_finite is False\n\n\n", "def test_issue_10730():\n    a = Rational(1, 3)\n    b = sqrt(2)\n    assert (a + b).as_expr() == a + b\n    assert (a - b).as_expr() == a - b\n    assert (a * b).as_expr() == a * b\n    assert (a / b).as_expr() == a / b\n    assert (a**b).as_expr() == a**b\n\n\n\n    \n"], "sample_419": ["    def test_form_instance_creation_error(self):\n        class RaisesExceptionForm(Form):\n                raise ValueError(\"This form can't be instantiated!\")\n\n        with self.assertRaises(ValueError):\n            formset_factory(RaisesExceptionForm)()\n        try:\n            formset_factory(RaisesExceptionForm)(data={'choices-TOTAL_FORMS': '1', 'choices-INITIAL_FORMS': '0'})\n        except ValueError as e:\n            self.assertEqual(str(e), \"This form can't be instantiated!\")\n\n\n\n", "    def test_extra_and_max_num_forms_with_non_form_errors(self):\n        class CustomFormSetWithNonFormError(BaseFormSet):\n                return [\n                    ValidationError(\"This is a non-form error.\")\n                ]\n        \n        data = {\n            \"choices-TOTAL_FORMS\": \"2\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-MAX_NUM_FORMS\": \"1\",\n        }\n        \n        formset = formset_factory(\n            Choice,\n            extra=2,  \n            max_num=1, \n            formset=CustomFormSetWithNonFormError,\n        )(data, auto_id=False, prefix=\"choices\")\n        self.assertEqual(formset.total_form_count(), 1)  \n        self.assertEqual(formset.cleaned_data, [{}])\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(formset._errors, [{\"\": [\"This is a non-form error.\"] }])\n\n\n\n", "    def test_formset_with_explicit_renderer(self):\n        from django.forms.renderers import Jinja2\n\n        ChoiceFormSet = formset_factory(Choice, renderer=Jinja2)\n        formset = ChoiceFormSet()\n        self.assertEqual(formset.renderer, Jinja2)\n\n", "    def test_deprecation_warning_on_explicitly_set_renderer(self):\n        from django.forms.utils import DEFAULT_TEMPLATE_DEPRECATION_MSG\n\n        with isolate_lru_cache(get_default_renderer), self.settings(\n            FORM_RENDERER=\"django.forms.renderers.DjangoTemplates\"\n        ), self.assertRaisesMessage(\n            RemovedInDjango50Warning, DEFAULT_TEMPLATE_DEPRECATION_MSG\n        ):\n            ChoiceFormSet = formset_factory(Choice, renderer='django.forms.renderers.Simple')\n            formset = ChoiceFormSet()\n            str(formset)\n", "    def test_is_bound_after_cleaning_non_form_errors(self):\n        class ChoiceFormSetWithNonFormError(BaseFormSet):\n                raise ValidationError(\"This is a non-form error\")\n\n        data = {\n            \"choices-TOTAL_FORMS\": \"1\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"Zero\",\n            \"choices-0-votes\": \"0\",\n        }\n        formset = ChoiceFormSetWithNonFormError(data, auto_id=False, prefix=\"choices\")\n        formset.full_clean()\n        self.assertTrue(formset.is_bound)\n"], "sample_528": ["def test_load_base_library():\n    with _api.suppress_matplotlib_deprecation_warning():\n        library = style.load_base_library()\n    assert library  # Ensure library is loaded.\n    assert 'default' in library  # Check for existence of 'default' style\n\n\n\n", "def test_default_style():\n    mpl.rcParams[PARAM] = 'gray'\n    with style.context('default'):\n        assert mpl.rcParams[PARAM] == mpl.rcParams['image.cmap']\n    assert mpl.rcParams[PARAM] == 'gray'\n\n\n", "def test_use_invalid_style():\n    with pytest.raises(IOError):\n        style.use('doesnotexist')\n", "def test_reload_library():\n    original_library = style.library.copy()\n    with temp_style('_test_', DUMMY_SETTINGS):\n        style.reload_library()\n        assert set(style.library.keys()) > set(original_library.keys())\n", "def test_reload_library():\n    original_library = style.library.copy()\n    with temp_style('temp', {'foo': 'bar'}) as temp_path:\n        assert 'temp' in style.library\n        style.reload_library()\n        assert style.library == original_library\n"], "sample_880": ["def test_ovr_decision_function():\n    # check if predictions are correct when all classifiers\n    # classify correctly.\n    predictions = np.array([\n        [1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1],\n    ])\n    confidences = np.array([\n        [0.8, 0.2, 0.0],\n        [0.1, 0.9, 0.0],\n        [0.0, 0.1, 0.9],\n    ])\n    n_classes = 3\n    decision_function = _ovr_decision_function(predictions, confidences, n_classes)\n\n    expected_decision_function = np.array([\n        [ 0.6, -0.2,  0.0],\n        [-0.2,  0.9,  0.0],\n        [ 0.0, -0.2,  0.9],\n    ])\n    assert_array_almost_equal(decision_function, expected_decision_function)\n\n    # check if predictions are correct when some classifiers\n    # classify incorrectly.\n    predictions = np.array([\n        [0, 1, 0],\n        [1, 0, 0],\n        [0, 0, 1],\n    ])\n    confidences = np.array([\n        [0.8, 0.2, 0.0],\n        [0.1, 0.9, 0.0],\n        [0.0, 0.1, 0.9],\n    ])\n    n_classes = 3\n    decision_function = _ovr_decision_function(predictions, confidences, n_classes)\n\n    expected_decision_function = np.array([\n        [-0.2,  0.2,  0.0],\n        [ 0.9, -0.1,  0.0],\n        [ 0.0, -0.1,  0.9],\n    ])\n    assert_array_almost_equal(decision_function, expected_decision_function)\n", "def test_class_distribution_multilabel_sparse():\n    # Test class distribution with multiclass sparse data\n\n    data = np.array([\n        [1, 0, 1],\n        [1, 1, 0],\n        [0, 1, 1]\n    ])\n    indices = np.array([\n        [0, 1, 2],\n        [0, 2, 1],\n        [1, 2, 0]\n    ])\n    indptr = np.array([0, 3, 3])\n    y_sparse = coo_matrix((data, indices, indptr), shape=(3, 3))\n\n    classes, n_classes, class_prior = class_distribution(y_sparse)\n    classes_expected = [[0, 1, 2],\n                        [0, 1, 1],\n                        [0, 1, 2]]\n    n_classes_expected = [3, 3, 3]\n\n    for k in range(y_sparse.shape[1]):\n        assert_array_equal(classes[k], classes_expected[k])\n        assert_array_equal(n_classes[k], n_classes_expected[k])\n\n\n", "def test_ovr_decision_function_with_ties():\n    # Test that ties are broken consistently based on confidence\n    # when using OvR decision function\n\n    predictions = np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0], [1, 0, 1]])\n    confidences = np.array([\n        [0.6, 0.4, 0.2],\n        [0.3, 0.7, 0.0],\n        [0.5, 0.5, 0.0],\n        [0.2, 0.8, 0.0],\n    ])\n    n_classes = 3\n\n    votes = _ovr_decision_function(predictions, confidences, n_classes)\n\n    # Check that ties are broken consistently\n    expected_votes = np.array([\n        [1, 0, 0],  # Class 1 is more confident\n        [0, 1, 0],\n        [0, 1, 0],\n        [0, 1, 0],  # Class 2 is more confident even if votes are tied\n    ])\n    assert_array_almost_equal(votes, expected_votes)\n", "def test_ovr_decision_function():\n    n_samples = 10\n    n_classes = 3\n    predictions = np.random.randint(0, 2, size=(n_samples, n_classes * (n_classes - 1) // 2))\n    confidences = np.random.rand(n_samples, n_classes * (n_classes - 1) // 2)\n\n    decisions = _ovr_decision_function(predictions, confidences, n_classes)\n\n    # Check shape\n    assert decisions.shape == (n_samples, n_classes)\n\n    # Ensure decisions are valid\n    for i in range(n_samples):\n        for j in range(n_classes):\n            assert decisions[i, j] in [-1, 1]\n\n\n", "def test_multiclass_class_distribution(\n    y, expected_classes, expected_n_classes, expected_class_prior"], "sample_668": ["def test_deprecated_funcargnames_warning():\n    with pytest.warns(pytest.PytestDeprecationWarning, match=\"`funcargnames` attribute\"):\n        nodes.Node.funcargnames(\"\")\n", "def test_funcargnames_deprecation(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            pass\n\n            assert \"my_fixture\" in funcargnames\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*The 'funcargnames' attribute was an alias for 'fixturenames',\",\n            \"*since pytest 2.3 - use the newer attribute instead.*\",\n        ]\n    )\n", "def test_funcargnames_deprecation(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            pass\n\n            assert funcargnames == [\"my_fixture\"]\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*The 'funcargnames' attribute was an alias for 'fixturenames',\",\n            \"*since pytest 2.3 - use the newer attribute instead.*\",\n        ]\n    )\n", "def test_funcargnames_deprecation():\n    with pytest.warns(deprecated.FUNCARGNAMES):\n        from _pytest import fixture  # noqa: F401\n        fixture.funcargnames  # Trigger the warning\n", "compilation error"], "sample_839": ["compilation error", "def test_empty_vocabulary(Estimator):\n    if issubclass(Estimator, HashingVectorizer):\n        pytest.xfail('HashingVectorizer is not supported on PyPy')\n\n    vectorizer = Estimator(vocabulary=[])\n    with pytest.raises(ValueError):\n        vectorizer.fit(['hello'])\n    with pytest.raises(ValueError):\n        vectorizer.transform(['hello'])\n\n\n", "def test_vectorizer_with_custom_tokenizer_language(vectorizer_class):\n    if issubclass(vectorizer_class, HashingVectorizer):\n        pytest.xfail('HashingVectorizer is not supported on PyPy')\n    \n        return [\"<JP_TOKEN>\"]  # Replace with actual Japanese tokenization\n\n    vectorizer = vectorizer_class(tokenizer=custom_tokenizer_ja)\n    data = [\"\u3053\u308c\u306f\u65e5\u672c\u8a9e\u306e\u30c6\u30ad\u30b9\u30c8\u3067\u3059\"]\n    X = vectorizer.fit_transform(data)\n    tokens = vectorizer.get_feature_names_out()\n\n    assert len(X.toarray()) == 1\n    assert tokens[0] == \"<JP_TOKEN>\"\n\n\n", "def test_input_list_of_strings(Estimator):\n    data = ['this is text', 'another text', 'one more text']\n    vectorizer = Estimator()\n    vectorizer.fit(data)\n    assert isinstance(vectorizer.transform(data), sparse.csr_matrix)\n\n\n\n", "def test_vectorizer_vocabulary_updates_empty_corpus(Estimator):\n    vec = Estimator()\n    # Ensure vocabulary doesn't change with an empty corpus\n    assert len(vec.vocabulary_) == 0\n    vec.fit([])\n    assert len(vec.vocabulary_) == 0\n\n\n"], "sample_844": ["def test_xi_parameter_range():\n    for xi in np.linspace(0, 1, 11):\n        # Computing OPTICS with different xi values.\n        clust = OPTICS(min_samples=5, xi=xi).fit(X)\n        assert (0 <= clust.labels_).all()\n        assert (clust.labels_ < len(set(clust.labels_))).all()\n\n\n\n", "def test_xi_parameter_effects():\n    # test how the xi parameter affects the clustering results\n\n    centers = [[1, 1], [-1, -1], [1, -1]]\n    X, labels_true = make_blobs(n_samples=750, centers=centers,\n                cluster_std=0.4, random_state=0)\n    \n    # Test xi = 0.1\n    clust1 = OPTICS(min_samples=5, cluster_method='xi',\n                    xi=0.1).fit(X)\n    assert_array_equal(clust1.labels_, make_blobs(n_samples=750, centers=centers,\n                cluster_std=0.4, random_state=0)[-1])\n\n    # Test xi = 0.3\n    clust2 = OPTICS(min_samples=5, cluster_method='xi',\n                    xi=0.3).fit(X)\n    # xi = 0.3 should lead to more noise points being clustered\n    assert_array_equal(clust1.labels_, make_blobs(n_samples=750, centers=centers,\n                cluster_std=0.4, random_state=0)[-1])\n\n    # Test xi = 0.5\n    clust3 = OPTICS(min_samples=5, cluster_method='xi',\n                    xi=0.5).fit(X)\n    # xi = 0.5 should lead to even more noise points being clustered\n    assert_array_equal(clust1.labels_, make_blobs(n_samples=750, centers=centers,\n                cluster_std=0.4, random_state=0)[-1])\n\n\n\n", "def test_xi_extraction_edge_cases():\n    # Test xi extraction with very small distances (close to zero)\n    X = np.array([[0, 0], [0, 0.1], [0, -.1]])\n    clust = OPTICS(xi=0.1, min_samples=3).fit(X)\n    assert_array_equal(clust.labels_, np.r_[[-1, -1, -1]])\n\n    # Test xi extraction with all points as noise\n    X = np.random.rand(100, 2)\n    clust = OPTICS(xi=0.1, min_samples=100).fit(X)\n    assert_array_equal(clust.labels_, np.full(100, -1)) \n\n    # Test xi extraction with a single point\n    X = np.array([[0, 0]])\n    clust = OPTICS(xi=0.1, min_samples=1).fit(X)\n    assert_array_equal(clust.labels_, [0])\n", "def test_dbscan_eps_influence():\n    rng = np.random.RandomState(0)\n    n_points_per_cluster = 50\n    C1 = [0, 0] + .2 * rng.randn(n_points_per_cluster, 2)\n    C2 = [5, 5] + .2 * rng.randn(n_points_per_cluster, 2)\n\n    X = np.vstack((C1, C2))\n    clust1 = OPTICS(cluster_method='dbscan', eps=0.3).fit(X)\n    clust2 = OPTICS(cluster_method='dbscan', eps=0.6).fit(X)\n\n    assert_array_equal(clust1.labels_, np.zeros_like(clust1.labels_))\n    assert_array_equal(clust2.labels_, np.array([0, 1]))\n\n\n\n", "def test_xi_with_outliers():\n    # test xi parameter with outliers\n    rng = np.random.RandomState(0)\n    n_points_per_cluster = 100\n    C1 = [0, 0] + 2 * rng.randn(n_points_per_cluster, 2)\n    C2 = [0, 0] + 50 * rng.randn(n_points_per_cluster, 2)\n    C3 = [1, 1] + 2 * rng.randn(n_points_per_cluster, 2)\n    X = np.vstack((C1, C2, C3))\n    # add some outliers\n    X = np.concatenate((X, 10 * np.random.randn(10, 2)))\n\n    expected_labels = np.r_[np.zeros(n_points_per_cluster),\n                            np.zeros(n_points_per_cluster),\n                            np.ones(n_points_per_cluster),\n                            -np.ones(10)]\n    clust = OPTICS(min_samples=10, xi=0.2, cluster_method='xi').fit(X)\n    assert_array_equal(clust.labels_, expected_labels)\n\n\n\n"], "sample_607": ["def test_get_backend_engine_string():\n    backend = plugins.get_backend(\"dummy\")\n    assert isinstance(backend, DummyBackendEntrypointArgs)\n    \n    with pytest.raises(ValueError, match=\"unrecognized engine\"):\n        plugins.get_backend(\"invalid_engine\")\n\n", "def test_no_installed_backend():\n    with pytest.raises(ValueError, match=r\"xarray is unable to open this file\"):\n        plugins.guess_engine(\"not-valid\")\n", "def test_list_engines_cache(mock_iter_entry_points):\n    engines1 = plugins.list_engines()\n    engines2 = plugins.list_engines()\n    assert engines1 == engines2\n\n\n", "def test_guess_engine_handles_missing_backends(mocker):\n    with pytest.raises(ValueError, match=r\"did not find a match in any\"):\n        plugins.guess_engine(\"not-valid\")\n", "def test_list_engines_cache(mocker):\n    plugins.list_engines()\n    plugins.list_engines()\n\n    list_engines_mock = mocker.patch(\"xarray.backends.plugins.list_engines\")\n    list_engines_mock.assert_called_once()\n"], "sample_115": ["    def test_nested_sensitive_functions(self):\n        @sensitive_variables\n            @sensitive_variables\n                return f\"{sensitive_var} {another_sensitive_var}\"\n            return inner_function(\"secret\")\n        with self.settings(DEBUG=True):\n            result = outer_function(\"top secret\")\n            self.assertNotContains(result, 'top secret', status_code=500)\n            self.assertNotContains(result, 'secret', status_code=500)\n\n", "    def test_sensitive_function_arguments_with_default_values(self):\n        \"\"\"\n        Sensitive variables don't leak in the sensitive_variables decorator's\n        frame, when those variables are passed as arguments to the decorated\n        function with default values.\n        \"\"\"\n        @sensitive_variables\n            pass\n\n        with self.settings(DEBUG=True):\n            sensitive_args_function_caller('safe_value', 'super_secret')\n            sensitive_args_function_caller('safe_value')\n\n        with self.settings(DEBUG=False):\n            sensitive_args_function_caller('safe_value', 'super_secret')\n            sensitive_args_function_caller('safe_value')\n", "    def test_sensitive_variables_decorator_chain(self):\n        @sensitive_variables\n        @sensitive_variables\n            return password\n\n        with self.settings(DEBUG=True):\n            response = self.client.get('/sensitive_variables_test/', HTTP_SECRET_HEADER='super_secret')\n            self.assertEqual(response.status_code, 500)\n            self.assertContains(response, 'XXXXXXXXXXXXXXXXXXXX', status_code=500)\n\n        with self.settings(DEBUG=False):\n            response = self.client.get('/sensitive_variables_test/')\n            self.assertEqual(response.status_code, 500)\n            self.assertContains(response, 'XXXXXXXXXXXXXXXXXXXX', status_code=500)\n", "    def test_dont_leak_sensitive_post_parameters_in_html_emails(self):\n        with self.settings(DEBUG=True, ADMINS=[('Admin', 'admin@fattie-breakie.com')]):\n            mail.outbox = []  # Empty outbox\n            request = self.rf.post('/some_url/', self.breakfast_data)\n            non_sensitive_view(request)\n            self.assertEqual(len(mail.outbox), 1)\n            email = mail.outbox[0]\n            body_html = str(email.alternatives[0][0])\n            for k, v in self.breakfast_data.items():\n                self.assertNotIn(v, body_html)    \n\n\n", "    def test_sensitive_variables_decorator_works_with_decorated_function(self):\n        @sensitive_variables\n            return password\n\n        with self.settings(DEBUG=True):\n            response = self.client.post('/some_url/', {'key': 'value', 'password': 'secret'})\n            self.assertIn('XXXXXXXXXXXXXXXXXXXX', response.content.decode('utf-8'))\n"], "sample_294": ["    def test_csrf_failure_redirects_with_message(self):\n        req = self._get_POST_request_with_token()\n        mw = CsrfViewMiddleware(post_form_view)\n        with self.assertLogs('django.security.csrf', 'WARNING') as cm:\n            response = mw.process_view(req, post_form_view, (), {})\n\n        self.assertEqual(response.status_code, 403)\n        self.assertIn('CSRF verification failed.', response.content.decode('utf-8'))\n\n\n\n        msg = (\n            'CSRF verification failed. Request aborted.'  \n        )\n        self.assertEqual(cm.records[0].getMessage(), msg)\n", "    def test_csrf_cookie_domain_with_empty_cookie_domain(self):\n        \"\"\"\n        The CSRF cookie domain is set to '.' if CSRF_COOKIE_DOMAIN is empty.\n        \"\"\"\n        req = self._get_GET_no_csrf_cookie_request()\n        mw = CsrfViewMiddleware(token_view)\n        mw.process_view(req, token_view, (), {})\n        resp = mw(req)\n        \n        self.assertIn(settings.CSRF_COOKIE_NAME, resp.cookies, \"Cookie was not set\")\n        csrf_cookie = resp.cookies[settings.CSRF_COOKIE_NAME]\n        self.assertEqual(csrf_cookie.get('domain'), '.')\n\n", "    def test_cookie_domain_subdomain_mismatch(self):\n        req = self._get_POST_request_with_token()\n        req.META['HTTP_HOST'] = 'foo.example.com'\n        req.META['HTTP_REFERER'] = 'https://www.example.com/'\n        mw = CsrfViewMiddleware(post_form_view)\n        with self.assertRaises(CSRFError) as cm:\n            mw.process_view(req, post_form_view, (), {})\n        self.assertEqual(cm.exception.message, 'CSRF verification failed')\n", "    def test_bare_secret_accepted_and_replaced(self):\n        \"\"\"\n        The csrf token is reset from a bare secret.\n        \"\"\"\n        req = self._get_POST_bare_secret_csrf_cookie_request()\n        mw = CsrfViewMiddleware(token_view)\n        mw.process_request(req)\n        resp = mw.process_view(req, token_view, (), {})\n        self.assertIsNone(resp)\n        csrf_cookie = req.session[CSRF_SESSION_KEY]\n        self._check_token_present(resp, csrf_id=csrf_cookie)\n\n\n", "    def test_bare_secret_accepted_and_replaced(self):\n        \"\"\"\n        The csrf token is reset from a bare secret.\n        \"\"\"\n        req = self._get_POST_bare_secret_csrf_cookie_request()\n        mw = CsrfViewMiddleware(token_view)\n        mw.process_request(req)\n        resp = mw.process_view(req, token_view, (), {})\n        self.assertIsNone(resp)\n        resp = mw(req)\n        csrf_cookie = resp.cookies.get(settings.CSRF_COOKIE_NAME, False)\n        self.assertEqual(len(csrf_cookie.value), CSRF_TOKEN_LENGTH)\n        self._check_token_present(resp, csrf_id=csrf_cookie.value)\n\n\n\n"], "sample_628": ["    def test_skip_sphinx_directive_in_docstring(self):\n        stmt = astroid.extract_node(\n            'class ComentAbc(object):\\n   \"\"\"This is :func:`ComentAbc` with a bad coment\"\"\"\\n   pass'\n        )\n        with self.assertAddsMessages(\n            Message(\n                \"wrong-spelling-in-docstring\",\n                line=2,\n                args=(\n                    \"coment\",\n                    \"This is :func:`ComentAbc` with a bad coment\",\n                    \"                                      ^^^^^^\",\n                    self._get_msg_suggestions(\"coment\"),\n                ),\n            )\n        ):\n            self.checker.visit_classdef(stmt)\n\n", "    def test_docstring_lines_that_look_like_comments_6(self):\n        stmt = astroid.extract_node(\n            '''def f():", "    def test_skip_words_with_underscores_in_docstrings(self):\n        stmt = astroid.extract_node(\n            'class TestClass(object):\\n   \"\"\"This class has an underscore_name\"\"\"\\n   pass'\n        )\n        self.checker.visit_classdef(stmt)\n        assert self.linter.release_messages() == []\n", "    def test_docstring_lines_that_look_like_comments_6(self):\n        stmt = astroid.extract_node(\n            '''def f():", "    def test_docstring_lines_that_look_like_comments_6(self):\n        stmt = astroid.extract_node(\n            '''def f():"], "sample_831": ["compilation error", "def test_export_text_with_class_names():\n    clf = DecisionTreeClassifier(max_depth=2, random_state=0)\n    clf.fit(X, y)\n    class_names = ['Class A', 'Class B']\n    expected_report = dedent(\"\"\"\n    |--- feature_1 <= 0.00\n    |   |--- class: Class A\n    |--- feature_1 >  0.00\n    |   |--- class: Class B\n    \"\"\").lstrip()\n\n    assert export_text(clf, class_names=class_names) == expected_report\n\n\n\n", "def test_export_text_with_class_names():\n    clf = DecisionTreeClassifier(max_depth=2, random_state=0)\n    clf.fit(X, y)\n\n    expected_report = dedent(\"\"\"\n    |--- feature_1 <= 0.00\n    |   |--- class: -1\n    |--- feature_1 >  0.00\n    |   |--- class: 1\n    \"\"\").lstrip()\n\n    assert export_text(clf, class_names=[\"yes\", \"no\"]) == expected_report\n", "compilation error", "compilation error"], "sample_1207": ["    def test_issue_15391():\n        assert parse_expr(\"1.2.3\") == 1.23\n        assert parse_expr(\"1.2.3e-1\") == 0.123\n        assert parse_expr(\"1.2.3e+1\") == 12.3\n        assert parse_expr(\"1.2.3e+1.1\") == 1.23e1.1\n", "compilation error", "def test_issue_13757():\n    assert parse_expr('3*x+x') == 4*x \n", "compilation error", "def test_issue_13676():\n    x = Symbol('x')\n    assert parse_expr('1+2*3+4') == 1 + 2*3 + 4\n    assert parse_expr('1+2*3+4', evaluate=False) ==  1 + 2*3 + 4 \n"], "sample_650": ["def test_log_cli_format(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.warning('text')\n            assert False\n        \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_format=%(asctime)s %(levelname)s: %(message)s\n        log_cli_format=%(message)s\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    assert result.ret == 1\n    result.stdout.fnmatch_lines([\"text\"])\n", "def test_log_format_interpolation_with_args(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.warning(\"Hello, %s!\", \"world\")\n            assert False\n        \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_format=%(asctime)s; %(levelname)s; %(message)s\n        log_date_format=%Y-%m-%d %H:%M:%S\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    assert result.ret == 1\n    result.stdout.re_match_lines([r\"^[0-9-]{10} [0-9:]{8}; WARNING; Hello, world!$\" ])\n", "def test_format_options(pytester: Pytester) -> None:\n    \"\"\"Test that format options in log_format are parsed correctly.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.warning('text')\n            assert False\n        \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_format=%(name)s: %(levelname)s: %(message)s\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    assert result.ret == 1\n    result.stdout.fnmatch_lines(\n        [\"test_format_options: WARNING: text\"]\n    )\n\n\n\n\n", "def test_log_format_custom(pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import logging\n\n            logger = logging.getLogger(__name__)\n\n                logger.warning('text')\n                assert False\n            \"\"\"\n        )\n        pytester.makeini(\n            \"\"\"\n            [pytest]\n            log_format = %(process)d-%(thread)d-%(levelname)s-%(message)s\n            \"\"\"\n        )\n        result = pytester.runpytest()\n        assert result.ret == 1\n        result.stdout.re_match_lines([r\"^[0-9]+-[0-9]+-WARNING-text\"])\n", "    def test_custom_formatter(pytester: Pytester) -> None:\n        class CustomFormatter(logging.Formatter):\n                return f\"{record.levelname} - {record.message}\"\n\n        pytester.makepyfile(\n            \"\"\"\n            import logging\n\n            logger = logging.getLogger(__name__)\n            logger.setLevel(logging.DEBUG)\n            formatter = CustomFormatter()\n            handler = logging.StreamHandler()\n            handler.setFormatter(formatter)\n            logger.addHandler(handler)\n\n                logger.debug(\"This is a debug message\")\n                logger.warning(\"This is a warning message\")\n            \"\"\"\n        )\n        pytester.makeini(\n            \"\"\"\n            [pytest]\n            log_format = []\n            log_date_format=\n            log_cli_level = WARNING\n            \"\"\"\n        )\n        result = pytester.runpytest()\n        result.stdout.fnmatch_lines(\n            [\n                \"WARNING - This is a warning message\",\n            ]\n        )\n        result.stdout.no_fnmatch_line(\"DEBUG - This is a debug message\")\n"], "sample_99": ["    def test_trunc_with_timezone_aware_time_field(self):\n        from django.db import models\n        class TimeAwareModel(models.Model):\n            start_time = models.TimeField(tz_name='Asia/Tokyo')\n            \n        self.create_model()\n        model_instance = TimeAwareModel.objects.create(start_time=timezone.make_aware(datetime(2023, 1, 1, 10, 30), tz=pytz.timezone('Asia/Tokyo')))\n\n        with timezone.override(pytz.timezone('Asia/Tokyo')):\n            model = TimeAwareModel.objects.annotate(\n                truncated_time=TruncHour('start_time')\n            ).get()\n            self.assertEqual(model.truncated_time, timezone.make_aware(datetime(2023, 1, 1, 10), tz=pytz.timezone('Asia/Tokyo')))\n\n\n\n", "    def test_trunc_with_timezone_aware_datetimefield(self):\n        timezone_field = models.DateTimeField(null=True, blank=True, tzinfo=timezone.get_current_timezone())\n        class TimeZoneAwareModel(models.Model):\n            start_datetime = timezone_field\n\n        with self.assertRaises(TypeError):\n            TimeZoneAwareModel.objects.create(start_datetime=datetime(2023, 10, 26, 10, 30, 0))\n", "    def test_trunc_func_with_timezone_ambiguous_times(self):\n\n        utc_tz = pytz.utc\n        sao_tz = pytz.timezone('America/Sao_Paulo')\n\n        start_datetime = utc_tz.localize(datetime(2016, 10, 28, 2, 0))\n        end_datetime = utc_tz.localize(datetime(2016, 10, 28, 2, 30))\n\n        self.create_model(start_datetime, end_datetime)\n\n        with self.assertRaises(pytz.AmbiguousTimeError):\n            # Truncating to minute in a timezone with DST transition\n            list(DTModel.objects.annotate(\n                truncated=TruncMinute('start_datetime', tzinfo=sao_tz)\n            ))\n\n        with self.assertRaises(pytz.AmbiguousTimeError):\n            # Truncating to hour in a timezone with DST transition\n            list(DTModel.objects.annotate(\n                truncated=TruncHour('start_datetime', tzinfo=sao_tz)\n            ))\n", "    def test_trunc_func_with_timezone_and_ambiguous_time(self):\n        start_datetime = datetime(2016, 3, 14, 2, 0, 0, tzinfo=timezone.utc)\n        end_datetime = datetime(2016, 3, 14, 23, 59, 59, tzinfo=timezone.utc)\n        self.create_model(start_datetime, end_datetime)\n        with timezone.override('America/Chicago'):\n            with self.assertRaisesMessage(pytz.AmbiguousTimeError, '2016-03-14 02:00:00'):\n                model = DTModel.objects.annotate(truncated_start=TruncHour('start_datetime')).get()\n", "    def test_trunc_func_with_timezone_time_field(self):\n        start_datetime = datetime(2015, 6, 15, 23, 30, 1, 321)\n        end_datetime = datetime(2015, 6, 16, 13, 11, 27, 123)\n        start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n        end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n        melb = pytz.timezone('Australia/Melbourne')\n\n        with timezone.override(melb):\n            model = DTModel.objects.annotate(\n                truncated_time=TruncTime('start_datetime', tzinfo=melb),\n            ).order_by('start_datetime').get()\n            self.assertEqual(model.truncated_time, truncate_to(start_datetime.time(), 'time', melb))\n\n\n\n"], "sample_277": ["    def test_resolve_expression(self):\n        q = Q(id__in= [1,2])\n        clause, joins = q.resolve_expression()\n        self.assertIsNotNone(clause)\n        self.assertEqual(len(joins), 0)\n", "    def test_select_related_descend(self):\n        from django.db import models\n        class Author(models.Model):\n            name = models.CharField(max_length=100)\n            books = models.ManyToManyField('Book', related_name='authors')\n\n        class Book(models.Model):\n            title = models.CharField(max_length=100)\n\n        # Test cases for select_related_descend() function\n\n        # 1. No remote field\n        field = Author.name\n        restricted = False\n        requested = {}\n        load_fields = set()\n        reverse = False\n        self.assertFalse(select_related_descend(field, restricted, requested, load_fields, reverse))\n\n        # 2. Parent link and not reverse\n        field = Book.authors.field\n        restricted = False\n        requested = {}\n        load_fields = set()\n        reverse = False\n        self.assertTrue(select_related_descend(field, restricted, requested, load_fields, reverse))\n\n        # 3. Restricted and reverse\n        field = Book.authors.field\n        restricted = True\n        requested = {'book_authors'}  \n        load_fields = set()\n        reverse = True\n        self.assertTrue(select_related_descend(field, restricted, requested, load_fields, reverse))\n\n        # 4. Restricted and not reverse\n        field = Author.books.field\n        restricted = True\n        requested = {'author_name'}\n        load_fields = set()\n        reverse = False\n        self.assertTrue(select_related_descend(field, restricted, requested, load_fields, reverse))\n\n        # 5. Null field (should not be loaded)\n        field = Book.authors.field\n        restricted = False\n        requested = {}\n        load_fields = set()\n        reverse = False\n        field.null = True\n        self.assertFalse(select_related_descend(field, restricted, requested, load_fields, reverse))\n\n\n", "    def test_resolve_expression(self):\n        q = Q(name__icontains='test')\n        result = q.resolve_expression()\n        self.assertIsInstance(result, list)\n        self.assertEqual(len(result), 1)\n        self.assertIsInstance(result[0], dict)\n        self.assertEqual(result[0]['expression'], 'name__icontains')\n        self.assertEqual(result[0]['value'], 'test')\n", "    def test_resolve_expression(self):\n        q = Q(pk__in=[1, 2, 3])\n        clause, joins = q._get_q_as_sql()\n        \n        self.assertEqual(clause, 'IN (1, 2, 3)')\n        self.assertFalse(joins)\n", "    def test_resolve_expression(self):\n        q = Q(id=1)\n        result = q.resolve_expression(allow_joins=False)\n        self.assertEqual(result, ['id = 1'])\n\n\n"], "sample_980": ["compilation error", "compilation error", "compilation error", "compilation error", "def test_from_inversion_vector():\n    p = Permutation.from_inversion_vector([2, 4, 1, 3, 0])\n    assert p.array_form == [3, 2, 1, 0, 4]\n    assert Permutation.from_inversion_vector([0]) == Permutation([0])\n    assert Permutation.from_inversion_vector([0, 1]) == Permutation([1, 0])\n    assert Permutation.from_inversion_vector([0, 1, 2, 3]) == \\\n        Permutation([2, 1, 0, 3])\n    raises(ValueError, lambda: Permutation.from_inversion_vector([0, 1, 2, 3, 4, 5]))\n"], "sample_600": ["def test_choose_float_dtype(data, encoding, expected_dtype):\n    coder = variables.CFScaleOffsetCoder()\n    dtype = coder._choose_float_dtype(data.dtype, encoding.get(\"add_offset\", False) is not None)\n    assert dtype == expected_dtype\n", "def test_CFScaleOffsetCoder_negative_scale_factor():\n    encoding = dict(scale_factor=-10)\n    original = xr.Variable((\"x\",), np.arange(10), encoding=encoding)\n    coder = variables.CFScaleOffsetCoder()\n    encoded = coder.encode(original)\n    roundtripped = coder.decode(encoded)\n    assert_allclose(original.values * -1, roundtripped.values) \n", "def test_CFScaleOffsetCoder_decode_with_offset():\n    original = xr.Variable(\n        (\"x\",),\n        np.array([0, 1, 2], dtype=np.float64),\n        encoding={\"add_offset\": 10},\n    )\n    coder = variables.CFScaleOffsetCoder()\n    encoded = coder.encode(original)\n    roundtripped = coder.decode(encoded)\n    assert_allclose(roundtripped.data, np.array([10, 11, 12]))\n", "def test_UnsignedIntegerCoder_roundtrip(dtype):\n    original = xr.Variable((\"x\",), np.arange(10, dtype=dtype), encoding=dict(_Unsigned=\"true\"))\n    coder = variables.UnsignedIntegerCoder()\n    roundtripped = coder.decode(coder.encode(original))\n    assert_identical(original, roundtripped)\n", "def test_CFMaskCoder_decode_mixed_fill_values(data, encoding):\n    original = xr.Variable((\"x\",), data, encoding=encoding)\n    coder = variables.CFMaskCoder()\n    encoded = coder.encode(original)\n    roundtripped = coder.decode(encoded)\n    assert_identical(roundtripped, original)\n"], "sample_1206": ["def test_issue_8564():\n    assert Float(0.1**100).n(50) == float('0.0')\n", "compilation error", "def test_issue_10750():\n    assert Float('_1.234e5', 5)._mpf_ == Float('123400', 5)._mpf_\n    assert Float('_1.234e-5', 5)._mpf_ == Float('0.00001234', 5)._mpf_\n", "def test_issue_10281():\n    assert Float('1e300').invert() is S.Zero\n", "compilation error"], "sample_230": ["    def test_invalid_json_error_message(self):\n        field = JSONField(error_messages={'invalid': 'Invalid JSON format.'})\n        with self.assertRaisesMessage(ValidationError, 'Invalid JSON format.'):\n            field.clean('{some badly formed: json}')\n", "    def test_invalid_encoding(self):\n        field = JSONField()\n        with self.assertRaisesMessage(ValidationError, 'Enter a valid JSON.'):\n            field.clean('{\"a\": \"b\"}')  \n", "    def test_invalid_non_string_input(self):\n        field = JSONField()\n        for value in [123, [], None]:\n            with self.assertRaisesMessage(ValidationError, 'Enter a valid JSON.'):\n                field.clean(value)\n", "    def test_invalid_json_with_custom_decoder(self):\n        class CustomDecoder(json.JSONDecoder):\n                return super().__init__(object_hook=object_hook, *args, **kwargs)\n            \n                try:\n                    return super().decode(s, *args, **kwargs)\n                except json.JSONDecodeError as e:\n                    raise ValidationError(\n                        self.error_messages['invalid'],\n                        code='invalid',\n                        params={'value': s},\n                    ) from e\n\n        field = JSONField(decoder=CustomDecoder)\n        with self.assertRaisesMessage(ValidationError, 'Enter a valid JSON.'):\n            field.clean('{some badly formed: json}')\n", "    def test_invalid_json_input(self):\n        field = JSONField()\n        with self.assertRaisesMessage(ValidationError, 'Enter a valid JSON.'):\n            field.clean('{invalid:json}')\n        with self.assertRaisesMessage(ValidationError, 'Enter a valid JSON.'):\n            field.clean('not valid json')\n"], "sample_583": ["def test_extract_indexers_from_dict_basic():\n    data = indexing.NumpyIndexingAdapter(np.arange(10))\n    indexers = {\n        'x': 0,\n        'y': slice(None),\n        'z': indexing.BasicIndexer((2, 4, 6))\n    }\n    actual = indexing._extract_indexers_from_dict(indexers, data.shape)\n    expected = {\n        'x': indexing.BasicIndexer((0,)),\n        'y': indexing.BasicIndexer((slice(None,))),\n        'z': indexing.BasicIndexer((2, 4, 6))\n    }\n    assert actual == expected\n\n", "def test_create_mask_multi_index_dimension(shape):\n    indexer = indexing.OuterIndexer((np.arange(shape[0]), slice(None),\n                                   np.arange(shape[-1])))\n    actual = indexing.create_mask(indexer, shape)\n    expected = np.all(np.arange(shape[0])[:, None, None] == np.arange(shape[-1])[:, None, None],\n                      axis=-1)\n    np.testing.assert_array_equal(expected, actual)\n", "compilation error", "compilation error", "compilation error"], "sample_416": ["    def test_empty_settings(self):\n        self.assertEqual(\n            self.settings_to_cmd_args_env({}),\n            ([\"psql\", \"postgres\"], None),\n        )\n", "    def test_default_database(self):\n        self.assertEqual(\n            self.settings_to_cmd_args_env({}),\n            ([\"psql\", \"postgres\"], None),\n        )\n", "    def test_default_database(self):\n        self.assertEqual(\n            self.settings_to_cmd_args_env({}),\n            ([\"psql\", \"postgres\"], None),\n        )\n", "    def test_default_database(self):\n        self.assertEqual(\n            self.settings_to_cmd_args_env({}),\n            ([\"psql\", \"postgres\"], None),\n        ) \n", "    def test_default_database(self):\n        self.assertEqual(\n            self.settings_to_cmd_args_env({}),\n            ([\"psql\", \"postgres\"], None),\n        )\n"], "sample_35": ["def test_minversion():\n    assert minversion('astropy', '0.4.4')\n    assert minversion('astropy', '0.12')\n\n    try:\n        import astrid\n    except ImportError:\n        pass\n    else:\n        assert not minversion('astrid', '0.12')\n\n    assert not minversion('astropy', '1.0', inclusive=False)\n    assert minversion('astropy', '0.12', inclusive=False)\n", "def test_minversion():\n    import astropy\n    assert minversion('astropy', '0.4.0', inclusive=True) is True\n    assert minversion('astropy', '0.4.0', inclusive=False) is True\n    assert minversion('astropy', '0.3.0', inclusive=True) is False\n    assert minversion('astropy', '0.3.0', inclusive=False) is False\n\n    try:\n        minversion('nonexistent_module', '0.4.0')\n    except ImportError:\n        pass\n    else:\n        assert False\n\n    # Test with version_path\n    assert minversion('astropy', '0.4.0', version_path='__version__', inclusive=True) is True\n", "def test_minversion():\n    assert minversion('astropy', '0.4.0') is True\n    assert minversion('astropy', '1.0.0') is True\n    assert minversion('astropy', '0.3.0') is False\n    assert minversion('astropy', '0.4', inclusive=False) is True\n    assert minversion('astropy', '0.5', inclusive=False) is False\n    assert minversion('astropy', '0.4.0', version_path='__version__') is True\n    assert minversion('astropy', '0.4.0', version_path='__version__') is True\n\n    with pytest.raises(ImportError):\n        minversion('nonexistent_module', '0.4.0') \n\n\n\n", "def test_minversion():\n    assert minversion('astropy', '0.4.4')\n    assert minversion('astropy', '0.12')\n    assert not minversion('astropy', '1.0.0')\n\n    assert minversion(types, '3.4')\n    assert not minversion(types, '4.0')\n \n    with pytest.raises(ImportError):\n        minversion('nonexistent_module', '1.0')\n\n", "def test_minversion():\n    assert minversion('astropy', '0.4.4')\n    assert minversion('astropy', '0.12')\n    assert not minversion('astropy', '0.13')\n    assert minversion('astropy', '0.4.3', inclusive=False)\n    assert minversion('astropy', '0.4.4', inclusive=False) is True\n    assert not minversion('astropy', '0.4.4', inclusive=False, version_path='__version__')\n    with pytest.raises(ImportError):\n        minversion('nonexistent_module', '0.12')\n\n"], "sample_1087": ["def test_fateman_poly_F_3():\n    f, g, h = fateman_poly_F_3(1)\n    F, G, H = dmp_fateman_poly_F_3(1, ZZ)\n\n    assert [ t.rep.rep for t in [f, g, h] ] == [F, G, H]\n\n    f, g, h = fateman_poly_F_3(3)\n    F, G, H = dmp_fateman_poly_F_3(3, ZZ)\n\n    assert [ t.rep.rep for t in [f, g, h] ] == [F, G, H]\n", "compilation error", "def test_fateman_poly_F_3():\n    f, g, h = fateman_poly_F_3(1)\n    F, G, H = dmp_fateman_poly_F_3(1, ZZ)\n\n    assert [ t.rep.rep for t in [f, g, h] ] == [F, G, H]\n\n    f, g, h = fateman_poly_F_3(3)\n    F, G, H = dmp_fateman_poly_F_3(3, ZZ)\n\n    assert [ t.rep.rep for t in [f, g, h] ] == [F, G, H]\n", "compilation error", "def test_fateman_poly_F_3():\n    f, g, h = fateman_poly_F_3(1)\n    F, G, H = dmp_fateman_poly_F_3(1, ZZ)\n\n    assert [ t.rep.rep for t in [f, g, h] ] == [F, G, H]\n\n    f, g, h = fateman_poly_F_3(3)\n    F, G, H = dmp_fateman_poly_F_3(3, ZZ)\n\n    assert [ t.rep.rep for t in [f, g, h] ] == [F, G, H]\n"], "sample_121": ["    def test_check_constraints_with_expressions(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n            height = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(\n                        check=models.Q(age__gte=18) & models.Q(height__gt=150),\n                        name='is_adult_and_tall',\n                    )\n                ]\n\n        errors = Model.check()\n        warn = Warning(\n            '%s does not support check constraints.' % connection.display_name,\n            hint=(\n                \"A constraint won't be created. Silence this warning if you \"\n                \"don't care about it.\"\n            ),\n            obj=Model,\n            id='models.W027',\n        )\n        expected = [] if connection.features.supports_table_check_constraints else [warn, warn]\n        self.assertCountEqual(errors, expected)\n\n\n\n\n", "    def test_check_constraints_in_multi_inheritance(self):\n        class Base(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                constraints = [models.CheckConstraint(check=models.Q(age__gte=18), name='is_adult')]\n\n        class Parent(Base):\n            pass\n\n        class Child(Parent):\n            pass\n\n        errors = Child.check()\n        warn = Warning(\n            '%s does not support check constraints.' % connection.display_name,\n            hint=(\n                \"A constraint won't be created. Silence this warning if you \"\n                \"don't care about it.\"\n            ),\n            obj=Child,\n            id='models.W027',\n        )\n        expected = [] if connection.features.supports_table_check_constraints else [warn, warn]\n        self.assertCountEqual(errors, expected)\n", "    def test_router_constraints(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                constraints = [models.CheckConstraint(check=models.Q(age__gte=18), name='is_adult')]\n\n        errors = Model.check()\n        expected = [] \n        self.assertCountEqual(errors, expected)\n", "    def test_check_constraints_with_negated_check(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                constraints = [models.CheckConstraint(check=~models.Q(age__gte=18), name='not_adult')]\n\n        errors = Model.check()\n        warn = Warning(\n            '%s does not support check constraints.' % connection.display_name,\n            hint=(\n                \"A constraint won't be created. Silence this warning if you \"\n                \"don't care about it.\"\n            ),\n            obj=Model,\n            id='models.W027',\n        )\n        expected = [] if connection.features.supports_table_check_constraints else [warn, warn]\n        self.assertCountEqual(errors, expected)\n", "    def test_unique_together_with_foreignkeys(self):\n        class Author(models.Model):\n            name = models.CharField(max_length=255)\n\n        class Book(models.Model):\n            title = models.CharField(max_length=255)\n            author = models.ForeignKey(Author, models.CASCADE)\n\n            class Meta:\n                unique_together = ('title', 'author')\n\n        errors = Book.check()\n        self.assertEqual(errors, [])\n  \n"], "sample_921": ["compilation error", "compilation error", "def test_is_builtin_class_method():\n    class MyInt(int):\n            pass\n\n    @classmethod\n        pass\n    \n    assert is_builtin_class_method(MyInt.my_class_method) is False\n\n\n\n", "def test_is_builtin_class_method():\n    class MyInt(int):\n            pass\n\n    class MyStr(str):\n            pass\n\n    assert is_builtin_class_method(MyInt.my_method) is False\n    assert is_builtin_class_method(MyStr.my_method) is False\n", "compilation error"], "sample_307": ["    def test_format_empty_string(self):\n        my_birthday = datetime(1979, 7, 8, 22, 00)\n        self.assertEqual(dateformat.format(my_birthday, ''), '')\n", "    def test_timezone_for_naive_datetime(self):\n        naive_dt = datetime(2023, 10, 26, 10, 0, 0)\n        self.assertEqual(dateformat.format(naive_dt, 'e'), '')\n        self.assertEqual(dateformat.format(naive_dt, 'O'), '')\n        self.assertEqual(dateformat.format(naive_dt, 'T'), '')\n        self.assertEqual(dateformat.format(naive_dt, 'Z'), '')\n\n", "    def test_alternate_month_names(self):\n        my_birthday = datetime(1979, 7, 8, 22, 00)\n\n        self.assertEqual(dateformat.format(my_birthday, 'E'), 'July') \n", "    def test_f_format(self):\n        my_birthday = datetime(1979, 7, 8, 22, 00)\n        self.assertEqual(dateformat.format(my_birthday, 'f'), '10')\n        self.assertEqual(dateformat.format(my_birthday, 'f'), '10') \n\n\n", "    def test_format_with_escaped_characters(self):\n        escaped_string = 'This is a \\\\test string with \\\\\\' escaped characters\\\\\\'\\\\'\n        dt = datetime(2023, 10, 26, 10, 30, 0)\n        self.assertEqual(dateformat.format(dt, escaped_string), 'This is a test string with \\' escaped characters\\'' )\n\n\n"], "sample_559": ["def test_inset_axes_transform():\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.set_xlim(0, 10)\n    ax.set_ylim(0, 10)\n    ax.plot([0, 10], [0, 10], 'k-')\n\n    axins = zoomed_inset_axes(ax, zoom=2, loc='upper left')\n    axins.set_xlim(2, 8)\n    axins.set_ylim(2, 8)\n    axins.plot([0, 8], [0, 8], 'r--')\n\n", "def test_grid_axes_locator():\n    fig = plt.figure()\n    grid = Grid(fig, 111, (3, 2), direction=\"row\")\n    for i, ax in enumerate(grid):\n        ax.text(0.5, 0.5, str(i), ha=\"center\", va=\"center\", transform=ax.transAxes)\n    fig.canvas.draw()\n\n\n", "def test_imagegrid_cbar_mode_shared():\n    arr = np.arange(16).reshape((4, 4))\n\n    fig = plt.figure(figsize=(18, 9))\n\n    positions = (241, 242, 243, 244)\n    directions = ['row']*4\n    cbar_locations = ['left', 'right', 'top', 'bottom']\n\n    for position, direction, location in zip(\n            positions, directions, cbar_locations):\n        grid = ImageGrid(fig, position,\n                         nrows_ncols=(2, 2),\n                         direction=direction,\n                         cbar_location=location, cbar_size='20%',\n                         cbar_mode='shared')\n        ax1, ax2, ax3, ax4 = grid\n\n        ax1.imshow(arr, cmap='nipy_spectral')\n        ax2.imshow(arr.T, cmap='hot')\n        ax3.imshow(np.hypot(arr, arr.T), cmap='jet')\n        ax4.imshow(np.arctan2(arr, arr.T), cmap='hsv')\n\n", "compilation error", "def test_bbox_transform():\n    fig, ax = plt.subplots()\n    bbox = ax.bbox\n\n    # Test that bbox_transform returns the correct transformation matrix for\n    # a simple scaling\n    transform = ax.bbox_transform\n    new_bbox = transform.transform(bbox)\n    assert new_bbox.width == bbox.width * 2\n    assert new_bbox.height == bbox.height * 2\n    assert new_bbox.x0 == bbox.x0 * 2\n    assert new_bbox.y0 == bbox.y0 * 2\n\n    # Test that bbox_transform returns the correct transformation matrix for\n    # a rotation\n    theta = np.pi / 4\n    rotation_matrix = np.array([[np.cos(theta), -np.sin(theta)],\n                                [np.sin(theta), np.cos(theta)]])\n    new_bbox = transform.transform(bbox, rotation_matrix)\n    # Check that the new bbox is rotated by the specified angle.\n    rotated_point = np.array([bbox.x0 + bbox.width / 2, bbox.y0 + bbox.height / 2])\n    rotated_point = rotation_matrix @ rotated_point\n    assert np.allclose(new_bbox.x0 + new_bbox.width / 2, rotated_point[0])\n    assert np.allclose(new_bbox.y0 + new_bbox.height / 2, rotated_point[1])\n\n"], "sample_893": ["def test_export_text_feature_name_handling():\n    clf = DecisionTreeClassifier(max_depth=2, random_state=0)\n    clf.fit(X, y)\n\n    # Test with None feature names\n    assert export_text(clf) == (\n        \"|--- feature_0 <= 0.00\\n\"\n        \"|   |--- class: -1\\n\"\n        \"|--- feature_0 >  0.00\\n\"\n        \"|   |--- class: 1\\n\"\n    ).lstrip()\n\n    # Test with empty string feature names\n    assert export_text(clf, feature_names=[]) == (\n        \"|---  <= 0.00\\n\"\n        \"|   |--- class: -1\\n\"\n        \"|---  >  0.00\\n\"\n        \"|   |--- class: 1\\n\"\n    ).lstrip()\n\n    # Test with feature names including spaces\n    feature_names = [\"feature 1\", \"feature 2\"]\n    assert export_text(clf, feature_names=feature_names) == (\n        \"|--- feature 1 <= 0.00\\n\"\n        \"|   |--- class: -1\\n\"\n        \"|--- feature 1 >  0.00\\n\"\n        \"|   |--- class: 1\\n\"\n    ).lstrip()\n", "def test_export_text_class_weights():\n    rng = RandomState(42)\n    X = rng.rand(100, 2)\n    y = rng.randint(0, 2, size=100)\n    sample_weights = rng.rand(100)\n    clf = DecisionTreeClassifier(class_weight=\"balanced\", random_state=42)\n    clf.fit(X, y, sample_weight=sample_weights)\n    expected_report = dedent(\"\"\"\n    |--- feature_0 <= 0.50\n    |   |--- weights: [45.00, 55.00] class: 0\n    |--- feature_0 >  0.50\n    |   |--- weights: [55.00, 45.00] class: 1\n    \"\"\").lstrip()\n    assert export_text(clf, show_weights=True) == expected_report \n", "def test_export_text_with_decimals():\n    clf = DecisionTreeRegressor(max_depth=2, random_state=0)\n    clf.fit(X, y)\n\n    expected_report = dedent(\"\"\"\n    |--- feature_1 <= 0.00\n    |   |--- value: [-1.000, -1.000]\n    |--- feature_1 >  0.00\n    |   |--- value: [1.000, 1.000]\n    \"\"\").lstrip()\n    assert export_text(clf, decimals=3) == expected_report\n    assert export_text(clf, decimals=0) == dedent(\"\"\"\n    |--- feature_1 <= 0.0\n    |   |--- value: [-1.0, -1.0]\n    |--- feature_1 >  0.0\n    |   |--- value: [1.0, 1.0]\n    \"\"\").lstrip()\n\n\n\n    \n", "def test_export_text_with_feature_names_and_class_names( ):\n    # Check that export_text handles feature_names and class_names correctly\n    clf = DecisionTreeClassifier(max_depth=2, random_state=0)\n    clf.fit(X, y)\n\n    class_names = [\"positive\", \"negative\"]\n    \n    expected_report = dedent(\"\"\"\n    |--- feature_1 <= 0.00\n    |   |--- class: positive\n    |--- feature_1 >  0.00\n    |   |--- class: negative\n    \"\"\").lstrip()\n    \n    assert export_text(clf, feature_names=[\"feature 1\"], class_names=class_names) == expected_report\n\n\n", "def test_truncated_branch_depth():\n    clf = DecisionTreeClassifier(max_depth=2)\n    clf.fit(X, y)\n\n    expected_report = dedent(\"\"\"\n    |--- feature_1 <= 0.00\n    |   |--- truncated branch of depth 1\n    |--- feature_1 >  0.00\n    |   |--- truncated branch of depth 1\n    \"\"\").lstrip()\n\n    assert export_text(clf, max_depth=1) == expected_report\n\n\n\n"], "sample_794": ["def test_ridge_classifier_sparse_data():\n    from sklearn.datasets import make_classification\n    from scipy.sparse import csr_matrix\n\n    X, y = make_classification(n_samples=100, n_features=100,\n                               random_state=42)\n    X_sparse = csr_matrix(X)\n    \n    ridge = RidgeClassifier()\n    ridge.fit(X_sparse, y)\n    \n    assert ridge.coef_.shape == (100,)\n    assert ridge.intercept_ is not None\n    assert_array_almost_equal(ridge.predict(X_sparse), ridge.predict(X))\n\n", "def test_sparse_handle_missing_values():\n    rng = np.random.RandomState(42)\n    X = sp.csr_matrix((rng.rand(10, 5), (rng.randint(0, 5, size=10),\n                                          rng.randint(0, 5, size=10))),\n                      shape=(10, 5))\n    y = rng.randn(10)\n\n    # Test with Ridge\n    ridge = Ridge(alpha=1.0, solver='sag', tol=1e-10)\n    ridge.fit(X, y)\n    assert_array_almost_equal(ridge.coef_, np.array([1, 2, 3, 4, 5]))\n\n    # Test with RidgeCV\n    ridgecv = RidgeCV(alphas=[1e-3, 1e-2, 1e-1], cv=3, scoring='neg_mean_squared_error', tol=1e-10)\n    ridgecv.fit(X, y)\n    assert_array_almost_equal(ridgecv.coef_, np.array([1, 2, 3, 4, 5]))\n\n\n\n", "def test_ridge_with_invalid_cv_params(alpha):\n    # Test that error is raised for invalid cv parameters.\n\n    ridge = Ridge(alpha=alpha)\n    assert_raises(ValueError, ridge.fit, X_iris, y_iris, cv='invalid')\n    assert_raises(ValueError, ridge.fit, X_iris, y_iris, cv=None)\n\n\n\n", "def test_sparse_input_with_solver_lsqr():\n    X, y = make_regression(n_samples=1000, n_features=2, n_informative=2,\n                           bias=10., random_state=42)\n\n    X_csr = sp.csr_matrix(X)\n\n    ridge = Ridge(alpha=1., tol=1.e-15, solver='lsqr')\n    ridge.fit(X_csr, y)\n    assert_array_almost_equal(ridge.coef_, np.linalg.solve(X.T @ X + 1e-15 * np.eye(X.shape[1]), X.T @ y))\n\n\n", "def test_ridge_classifier_multilabel_with_class_weight():\n    # Test RidgeClassifier with multilabel data and class weights.\n\n    X, y = make_multilabel_classification(n_samples=10, n_classes=3, random_state=0)\n    class_weights = {0: 2, 1: 1, 2: 3}\n\n    ridge_classifier = RidgeClassifier(class_weight=class_weights)\n    ridge_classifier.fit(X, y)\n\n    assert ridge_classifier.coef_ is not None\n\n\n\n"], "sample_681": ["        def test_bad_log(monkeypatch):\n            logging.basicConfig(level=logging.ERROR)\n            try:\n                logging.warning('oops', 'first', 2)\n            except ValueError:\n                pass", "def test_logging_emit_error_explicit_handler(testdir: Testdir) -> None:\n    \"\"\"\n    If logging is configured with a custom handler that suppresses errors,\n    pytest respects that configuration.\n    \"\"\"\n    class SuppressingHandler(logging.Handler):\n            pass  # Swallow the error\n\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logger = logging.getLogger(__name__)\n            logger.setLevel(logging.WARNING)\n            logger.addHandler(SuppressingHandler())\n            logger.warning('oops', 'first', 2)\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_logging_warn_level_in_file(testdir):\n    log_file = testdir.tmpdir.join(\"pytest.log\").strpath\n\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file={}\n        log_file_level = WARNING\n        log_cli=true\n        \"\"\".format(\n            log_file\n        )\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        logging.getLogger('catchlog').info(\"This log message will be ignored\")\n        logging.getLogger('catchlog').warning(\"This log message will be logged\")\n        \"\"\"\n    )\n\n    result = testdir.runpytest()\n\n    assert result.ret == 0\n    assert os.path.isfile(log_file)\n    with open(log_file, encoding=\"utf-8\") as rfh:\n        contents = rfh.read()\n        assert \"This log message will be logged\" in contents\n        assert \"This log message will be ignored\" not in contents\n", "        def test_log_custom_format(caplog):\n            logging.basicConfig(format='%(asctime)s - %(levelname)s - %(message)s')\n            logging.info('custom log message')\n            assert 'custom log message' in caplog.text", "    def test_log_in_pytest_finalizer(testdir):\n        log_file = testdir.tmpdir.join(\"pytest.log\").strpath\n\n        testdir.makeini(\n            \"\"\"\n            [pytest]\n            log_file={}\n            log_file_level = INFO\n            log_cli=true\n            \"\"\".format(\n                log_file\n            )\n        )\n        testdir.makeconftest(\n            \"\"\"\n            import logging\n            class MyPlugin:\n                    logging.info('MyPlugin_sessionfinish')\n                    \n                logging.info('pytest_sessionfinish')\n\n                logging.info('pytest_finalizer')\n                \n        \"\"\".format(\n            testdir.tmpdir\n        )\n        )\n        testdir.makepyfile(\n            \"\"\"\n                pass\n        \"\"\"\n        )\n        testdir.runpytest()\n        with open(log_file) as rfh:\n            contents = rfh.read()\n        assert \"pytest_sessionfinish\" in contents\n        assert \"MyPlugin_sessionfinish\" in contents\n        assert \"pytest_finalizer\" in contents\n\n\n"], "sample_832": ["def test_ard_with_highly_correlated_features():\n    # Test ARDRegression with highly correlated features (Github issue #14669)\n    X = np.array([[1, 2],\n                  [2, 4],\n                  [3, 6]])\n    y = np.array([1, 2, 3])\n    clf = ARDRegression(compute_score=True)\n    clf.fit(X, y)\n    # Check that ARD effectively prunes weights when features are highly correlated\n    assert clf.lambda_.min() > 1e-4\n    assert clf.lambda_.max() < 1e10\n\n\n\n", "def test_ARDregression_with_prior_params():\n    # Test ARDRegression with custom hyperparameters for alpha and lambda\n    X = np.array([[1, 0],\n                  [0, 0]])\n    y = np.array([0, 0])\n    desired_alpha = 0.5\n    desired_lambda = 2.0\n    clf = ARDRegression(alpha_1=1, alpha_2=1, lambda_1=1, lambda_2=1,\n                        n_iter=1)\n    clf.fit(X, y)\n\n    assert_almost_equal(clf.alpha_, desired_alpha, decimal=3)\n    assert_almost_equal(clf.lambda_, desired_lambda, decimal=3)\n", "compilation error", "compilation error", "compilation error"], "sample_845": ["def test_vectorizer_with_empty_docs(Estimator):\n    data = []\n    vec = Estimator()\n    with pytest.raises(ValueError):\n        vec.fit_transform(data)\n\n\n", "def test_callable_analyzer_filepath_error_handling(input_type):\n    if issubclass(TfidfVectorizer, Estimator):\n        pytest.xfail('HashingVectorizer is not supported on PyPy')\n\n    with pytest.raises(FileNotFoundError, match='file not found'):\n        TfidfVectorizer(analyzer=lambda x: open(x, 'r'),\n                       input=input_type).fit_transform(['nonexistent_file'])\n", "def test_vectorizer_invalid_analyzer_type(Estimator):\n    if issubclass(Estimator, HashingVectorizer):\n        pytest.xfail('HashingVectorizer is not supported on PyPy')\n\n    data = ['this is some text']\n    invalid_analyzer = {\"invalid\": \"analyzer\"}\n    with pytest.raises(TypeError, match=\"analyzer must be callable\"):\n        Estimator(analyzer=invalid_analyzer).fit_transform(data)\n", "def test_vectorizer_custom_analyzer_with_non_string_output(vectorizer_cls):\n        return [1, 2, 3]\n\n    vec = vectorizer_cls(analyzer=custom_analyzer)\n    with pytest.raises(TypeError):\n        vec.fit_transform(['hello'])\n\n\n\n", "def test_analyzer_custom_function_with_extra_kwargs(Estimator):\n    if issubclass(Estimator, HashingVectorizer):\n        pytest.xfail('HashingVectorizer is not supported on PyPy')\n    data = [\"this is text\", \"another text\"]\n        assert kwargs == {'extra_key': 'extra_value'}\n        return doc.split()\n    vectorizer = Estimator(analyzer=lambda doc, **kwargs: analyzer_with_kwargs(doc,\n                                                                               extra_key='extra_value',\n                                                                               **kwargs))\n    vectorizer.fit_transform(data)\n"], "sample_543": ["def test_polygon_selector_reset(ax, draw_bounding_box):\n    verts = [(50, 50), (150, 50), (50, 150)]\n    tool = widgets.PolygonSelector(ax, onselect=noop,\n                                   draw_bounding_box=draw_bounding_box)\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    tool.reset()\n    assert tool.verts == []\n", "def test_polygon_selector_draw_empty_polygon(ax, draw_bounding_box):\n    tool = widgets.PolygonSelector(ax, onselect=noop, draw_bounding_box=draw_bounding_box)\n    expected_verts = []\n\n    #  trigger the draw_polygon method once\n    tool.draw_polygon(expected_verts)\n\n    assert len(tool._handles_artists) == 0\n    assert tool._selection_artist is None\n\n", "def test_polygon_selector_empty_polygon(ax, draw_bounding_box):\n    event_sequence = [\n        # Create empty polygon and trigger on_select\n        *polygon_place_vertex(50, 50),\n        *polygon_place_vertex(50, 50),   \n    ]\n\n    tool = widgets.PolygonSelector(ax, onselect=noop,\n                                    draw_bounding_box=draw_bounding_box)\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n    assert tool.verts == []\n", "def test_polygon_selector_move_points(ax):\n    verts = [(20, 0), (0, 20), (20, 40), (40, 20)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[3]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    tool = widgets.PolygonSelector(ax, onselect=noop, draw_bounding_box=True)\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    # Move the second vertex\n    MouseEvent(\n        \"button_press_event\", ax.figure.canvas, *ax.transData.transform((0, 20)),\n        1)._process()\n    MouseEvent(\n        \"motion_notify_event\", ax.figure.canvas, *ax.transData.transform((10, 30)))._process()\n    MouseEvent(\n        \"button_release_event\", ax.figure.canvas, *ax.transData.transform((10, 30)),\n        1)._process()\n\n    np.testing.assert_allclose(tool.verts, [(20, 0), (10, 30), (20, 40), (40, 20)])\n\n    # Move the third vertex\n    MouseEvent(\n        \"button_press_event\", ax.figure.canvas, *ax.transData.transform((20, 40)),\n        1)._process()\n    MouseEvent(\n        \"motion_notify_event\", ax.figure.canvas, *ax.transData.transform((30, 50)))._process()\n    MouseEvent(\n        \"button_release_event\", ax.figure.canvas, *ax.transData.transform((30, 50)),\n        1)._process()\n\n    np.testing.assert_allclose(tool.verts, [(20, 0), (10, 30), (30, 50), (40", "def test_polygon_selector_click_outside_polygon(ax, draw_bounding_box):\n    verts = [(50, 50), (150, 50), (50, 150)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    tool = widgets.PolygonSelector(ax, onselect=noop, draw_bounding_box=draw_bounding_box)\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    # Click outside of the polygon\n    x, y = 200, 100\n    MouseEvent(\n        \"button_press_event\", tool.ax.figure.canvas,\n        *tool.ax.transData.transform((x, y)), 1)._process()\n\n    assert tool.verts == verts\n\n\n\n\n"], "sample_1208": ["def test_MatrixStudentT_with_symbolic_parameters():\n    v = symbols('v', positive=True)\n    n, p = symbols('n p', positive=True, integer=True)\n    Omega = MatrixSymbol('Omega', p, p)\n    Sigma = MatrixSymbol('Sigma', n, n)\n    Location = MatrixSymbol('Location', n, p)\n    Y = MatrixSymbol('Y', n, p)\n    M = MatrixStudentT('M', v, Location, Omega, Sigma)\n\n    exprd = gamma(v/2 + 1)*Determinant(Matrix([[1]]) + Sigma**(-1)*(-Location + Y)*Omega**(-1)*(-Location.T + Y.T))**(-v/2 - 1) / \\\n            (pi*gamma(v/2)*sqrt(Determinant(Omega))*Determinant(Sigma))\n    assert density(M)(Y).dummy_eq(exprd)\n\n    subs_dict = {v: 2, n: 3, p: 2}\n    M_subs = M.subs(subs_dict)\n    assert density(M_subs)(Y).doit() == exprd.subs(subs_dict)\n", "def test_MatrixStudentT_pdf_edge_cases():\n    M = MatrixStudentT('M', 2, [[5, 6]], [[2, 1], [1, 2]], [4])\n    x1 = Matrix([[7, 8]])\n    assert density(M)(x1).doit() > 0\n    x2 = Matrix([[1, 2]])\n    assert density(M)(x2).doit() > 0\n\n    M = MatrixStudentT('M', 2, [[5, 6]], [[2, 1], [1, 2]], [4])\n    X = MatrixSymbol('X', 1, 2)\n    x1 = Matrix([[7, 8]])\n    assert density(M)(x1).doit() > density(M)(x1 + Matrix([[0.1, -0.1]])).doit()\n    x2 = Matrix([[1, 2]])\n    assert density(M)(x2).doit() > density(M)(x2 + Matrix([[-0.1, 0.1]])).doit()\n\n\n\n", "def test_MatrixStudentT_pdf_invalid_input():\n    M = MatrixStudentT('M', 2, [[5, 6]], [[2, 1], [1, 2]], [4])\n    raises(ValueError, lambda: density(M)([[1, 2, 3]]))\n\n", "compilation error", "def test_MatrixStudentT_special_case():\n    v = symbols('v', positive=True)\n    n, p = 1, 2\n    Omega = MatrixSymbol('Omega', p, p)\n    Sigma = MatrixSymbol('Sigma', n, n)\n    Location = MatrixSymbol('Location', n, p)\n    Y = MatrixSymbol('Y', n, p)\n    M = MatrixStudentT('M', v, Location, Omega, Sigma)\n    \n    # Test case: Degrees of freedom v = 1\n    v = 1\n    exprd = gamma(v/2 + 1)*Determinant(Matrix([[1]]) + Sigma**(-1)*(-Location + Y)*Omega**(-1)*(-Location.T + Y.T)) \\\n            **(-v/2 - 1) / (pi*gamma(v/2)*sqrt(Determinant(Omega))*Determinant(Sigma))\n    assert density(M)(Y).doit() == exprd\n    \n    # Test case: Empty location matrix\n    Location = Matrix([])\n    exprd = gamma(v/2 + 1)*Determinant(Matrix([[1]]) + Sigma**(-1)*Y*Omega**(-1)*Y.T) \\\n            **(-v/2 - 1) / (pi*gamma(v/2)*sqrt(Determinant(Omega))*Determinant(Sigma))\n    assert density(M)(Y).doit() == exprd\n\n    # Test case: Empty scale matrix \n    Sigma = Matrix([])\n    exprd = gamma(v/2 + 1)*Determinant(Matrix([[1]]) + Omega**(-1)*Y*Omega**(-1)*Y.T) \\\n            **(-v/2 - 1) / (pi*gamma(v/2)*sqrt(Determinant(Omega)))\n    assert density(M)(Y).doit() == exprd\n\n\n\n"], "sample_688": ["    def test_custom_import_mode_respected(self, testdir):\n        testdir.makepyfile(\n            \"import pytest\\n\"\n            \"def test_custom_mode():\\n\"\n            \"    assert pytest.config.inifile.get('testpaths') == ['test_custom_mode.py']\",\n            \"test_custom_mode.py\"\n        )\n        result = testdir.runpytest(\n            \"--import-mode=custom\", '--junitxml=xunit-custom.xml'\n        )\n        result.stdout.fnmatch_lines([\"*test_custom_mode.py::test_custom_mode PASSED*\"])\n", "    def test_multiple_conftest_in_same_dir(self, testdir):\n        \"\"\"Test handling of multiple conftest.py files in the same directory.\"\"\"\n        tests_dir = testdir.tmpdir.join(\"tests\")\n        (tests_dir / \"conftest.py\").write(\"a = 1\")\n        (tests_dir / \"conftest2.py\").write(\"b = 2\")\n        testdir.makepyfile(\n            \"import sys\\n\"\n            \"def test_check():\\n\"\n            \"    assert a == 1\\n\"\n            \"    assert b == 2\",\n            dest=\"tests/test_foo.py\",\n        )\n        result = testdir.runpytest(\n            \"--import-mode=importlib\", \"-v\",\n        )\n        result.stdout.fnmatch_lines(\n            [\"* 1 passed in *\"]\n        )\n", "    def test_collect_conftest_imports(self, testdir):\n        \"\"\"Conftest imports should not affect collection.\"\"\"\n        testdir.makepyfile(\n            \"tests/conftest.py\",\n            \"\"\"\n            import pytest\n                config.pluginmanager.register(\n                    pytest.ImportPlugin)\n            \"\"\"\n        )\n        testdir.makepyfile(\"tests/test_foo.py\", \"def test_foo(): pass\")\n        result = testdir.runpytest(\"-v\", \"--import-mode=importlib\")\n        result.stdout.fnmatch_lines([\"* 1 passed in *\"])\n\n", "    def test_collect_cycles(self, testdir):\n        \"\"\"Test import cycles handling (GH-4001)\n        \"\"\"\n        testdir.makepyfile(\n            \"\"\"\n            import foo\n                foo.test_cyclic()\n            \"\"\",\n            \"foo.py\": \"\"\"\n            import bar\n                bar.test_cyclic()\n            \"\"\",\n            \"bar.py\": \"\"\"\n            import foo\n                foo.test_cyclic()\n            \"\"\"\n        )\n        with pytest.raises(ImportError):\n            testdir.runpytest()\n", "    def test_import_errors(self, testdir):\n        p = testdir.makepyfile(\n            \"import os\\n\"\n            \"os.environ['MY_VAR'] = 'value'\\n\"\n            \"def test_import_error():\\n\"\n            \"    import sys\\n\"\n            \"    assert sys.modules['os'].environ['MY_VAR'] == 'value'\"\n        )\n        result = testdir.runpytest(p, \"--import-mode=importlib\")\n        result.stdout.fnmatch_lines([\"*test_import_error.py::test_import_error PASSED*\"])\n"], "sample_164": ["    def test_server_formatter_date_format(self):\n        formatter = ServerFormatter(datefmt='%Y-%m-%d %H:%M:%S')\n        server_time = '2016-09-25 10:20:30'\n        log_msg = 'log message'\n        record = logging.makeLogRecord({'msg': log_msg, 'server_time': server_time})\n        self.assertEqual(formatter.format(record), f'[{server_time}] {log_msg}')\n\n\n", "    def test_server_formatter_use_default_datetime_format(self):\n        server_time = '2016-09-25 10:20:30'\n        log_msg = 'log message'\n        logger = logging.getLogger('django.server')\n\n        @contextmanager\n            old_stream = logger.handlers[0].stream\n            new_stream = StringIO()\n            logger.handlers[0].stream = new_stream\n            yield new_stream\n            logger.handlers[0].stream = old_stream\n\n        with patch_django_server_logger() as logger_output:\n            logger.info(log_msg, extra={'server_time': server_time})\n            self.assertEqual(f'[{server_time}] {log_msg}\\n', logger_output.getvalue())\n\n", "    def test_server_formatter_uses_default_time_format(self):\n        server_time = '2016-09-25 10:20:30'\n        log_msg = 'log message'\n        logger = logging.getLogger('django.server')\n\n        @contextmanager\n            old_stream = logger.handlers[0].stream\n            new_stream = StringIO()\n            logger.handlers[0].stream = new_stream\n            yield new_stream\n            logger.handlers[0].stream = old_stream\n\n        with patch_django_server_logger() as logger_output:\n            logger.info(log_msg, extra={'server_time': server_time})\n            self.assertEqual('[%s] %s\\n' % (server_time, log_msg), logger_output.getvalue())\n\n\n\n", "    def test_server_formatter_handles_null_time_string(self):\n        formatter = ServerFormatter()\n        log_msg = 'log message'\n        record = logging.makeLogRecord({'msg': log_msg, 'server_time': None})\n        self.assertEqual(formatter.format(record), log_msg)\n\n\n\n", "    def test_server_formatter_formats_time(self):\n        formatter = ServerFormatter()\n        record = logging.makeLogRecord({'msg': 'log message', 'server_time': '2023-10-26 10:20:30'})\n        self.assertEqual(formatter.format(record), '[2023-10-26 10:20:30] log message')\n\n\n"], "sample_573": ["    def test_fit_with_small_unique_x(self, df):\n\n        # Reduce the number of unique x values to be less than order\n        df[\"x\"] = df[\"x\"].sample(frac=0.2, random_state=42) \n        groupby = GroupBy([\"group\"])\n        res = PolyFit(order=2, gridsize=100)(df[[\"x\", \"y\"]], groupby, \"x\", {})\n\n        # Assert that the fit didn't throw an error\n        assert res.shape[0] == 100\n        assert len(res[\"x\"].unique()) == 100 \n", "    def test_order_handling(self, df):\n\n        groupby = GroupBy([\"group\"])\n        res_order_2 = PolyFit(order=2)(df[[\"x\", \"y\"]], groupby, \"x\", {})\n        res_order_1 = PolyFit(order=1)(df[[\"x\", \"y\"]], groupby, \"x\", {})\n\n        for _, part in res_order_2.groupby(\"group\"):\n            assert part[\"y\"].diff().diff().dropna().abs().gt(0).all()\n        for _, part in res_order_1.groupby(\"group\"):\n            assert part[\"y\"].diff().diff().dropna().abs().gt(0).all()\n\n\n", "    def test_polyfit_order(self, df):\n\n        groupby = GroupBy([\"group\"])\n        orders = [1, 2, 3]\n        for order in orders:\n            res = PolyFit(order=order)(df, groupby, \"x\", {})\n            x = res[\"x\"]\n            y = res[\"y\"]\n\n            if order == 1:\n                # Linear fit should have roughly equal spacing between y values\n                assert_array_almost_equal(y.diff().mean(), y.mean() / len(x))\n            elif order == 2:\n                # Quadratic fit should have non-linear spacing between y values\n                assert_array_almost_equal(y.diff().mean(), y.mean() / len(x) * 2)\n            else:  \n                # Cubic fit should have increasingly non-linear spacing between y values\n                assert_array_almost_equal(y.diff().mean(), y.mean() / len(x) * 3)\n", "    def test_data_subset(self, df):\n\n        groupby = GroupBy([\"group\"])\n        df_subset = df[[\"x\", \"y\", \"group\"]].copy()\n        res = PolyFit(order=1, gridsize=100)(df_subset, groupby, \"x\", {})\n\n        assert_frame_equal(res, df_subset.groupby(\"group\").apply(lambda x: pd.DataFrame(\n            dict(x=np.linspace(x[\"x\"].min(), x[\"x\"].max(), 100),\n                 y=np.polyval(np.polyfit(x[\"x\"], x[\"y\"], 1),\n                              np.linspace(x[\"x\"].min(), x[\"x\"].max(), 100)))\n        )))\n\n\n\n", "    def test_data_limitation(self, df):\n\n        df[\"x\"] = df[\"x\"].repeat(10)\n        df[\"y\"] = df[\"y\"].repeat(10)\n\n        groupby = GroupBy([\"group\"])\n        res = PolyFit(order=1, gridsize=100)(df[[\"x\", \"y\"]], groupby, \"x\", {})\n\n        assert_array_equal(res.shape, (1000, 2))\n"], "sample_221": ["    def test_in_lookup_with_object_values(self):\n        e2 = Event.objects.create(title='Event 2', group=Group.objects.get(name='Group 1'))\n        group_ids = [g.pk for g in Group.objects.filter(name__in=['Group 1', 'Group 2'])]\n        events = Event.objects.filter(group_id__in=group_ids)\n        with self.assertNumQueries(0):\n            dumped = pickle.dumps(events.query)\n        with self.assertNumQueries(0):\n            reloaded = pickle.loads(dumped)\n            reloaded_events = Event.objects.none()\n            reloaded_events.query = reloaded\n        self.assertSequenceEqual(reloaded_events, [self.e1, e2])\n", "    def test_in_lookup_with_related_objects(self):\n        \"\"\"\n        The __in lookup should be evaluated correctly even when involving related\n        objects.\n        \"\"\"\n        groups = Group.objects.filter(id=1)\n        events = Event.objects.filter(group__in=groups)\n\n        with self.assertNumQueries(1):\n            events_list = list(events)\n\n        with self.assertNumQueries(0):\n            dumped = pickle.dumps(events.query)\n\n        with self.assertNumQueries(0):\n            reloaded = pickle.loads(dumped)\n            reloaded_events = Event.objects.none()\n            reloaded_events.query = reloaded\n\n        self.assertSequenceEqual(reloaded_events, events_list)\n\n", "    def test_in_lookup_with_prefetch_related(self):\n        \"\"\"\n        Prefetching related objects for an inner queryset should be respected\n        when pickling/unpickling a QuerySet with an __in=inner_qs lookup.\n        \"\"\"\n        groups = Group.objects.prefetch_related('event_set')\n        events = Event.objects.filter(group__in=groups)\n        with self.assertNumQueries(1):  # Initial query for groups\n            dumped = pickle.dumps(events.query)\n\n        with self.assertNumQueries(0):\n            reloaded = pickle.loads(dumped)\n            reloaded_events = Event.objects.none()\n            reloaded_events.query = reloaded\n\n        self.assertSequenceEqual(reloaded_events, [self.e1])\n", "    def test_pickle_subquery_in_filter(self):\n        group = Group.objects.create(name='group')\n        Event.objects.create(title='event', group=group)\n        subquery = Event.objects.filter(group_id=models.OuterRef('id')).values('title').\n\n        qs = Happening.objects.filter(name__in=subquery)\n        self.assert_pickles(qs)\n\n\n\n", "    def test_in_lookup_with_custom_related_manager(self):\n        # Ensure that inner queryset is not evaluated when pickling/unpickling\n        # with a custom related manager that uses an inner queryset for __in.\n        class CustomGroupManager(models.Manager):\n                return super().get_queryset().filter(name__contains=\"Group\")\n\n        Group.objects.add_to_class('custom_group_set', CustomGroupManager())\n        events = Event.objects.filter(group__custom_group_set__in=Group.objects.all())\n\n        with self.assertNumQueries(0):\n            dumped = pickle.dumps(events.query)\n\n        with self.assertNumQueries(0):\n            reloaded = pickle.loads(dumped)\n            reloaded_events = Event.objects.none()\n            reloaded_events.query = reloaded\n\n        self.assertSequenceEqual(reloaded_events, [self.e1])\n\n\n\n"], "sample_352": ["    def test_subquery_with_alias(self):\n        i1 = Individual.objects.create(alive=True)\n        RelatedIndividual.objects.create(related=i1)\n        i2 = Individual.objects.create(alive=False)\n        RelatedIndividual.objects.create(related=i2)\n\n        subquery = Individual.objects.filter(related_individual__isnull=True).values('name')\n        qs = Individual.objects.filter(pk__in=subquery, alive=True)\n        self.assertSequenceEqual(qs, [i1])\n", "    def test_ticket_24605_with_isnull(self):\n        \"\"\"\n        Subquery table names should be quoted.\n        \"\"\"\n        i1 = Individual.objects.create(alive=True)\n        RelatedIndividual.objects.create(related=i1)\n        i2 = Individual.objects.create(alive=False)\n        RelatedIndividual.objects.create(related=i2)\n        i3 = Individual.objects.create(alive=True)\n        i4 = Individual.objects.create(alive=False)\n\n        self.assertSequenceEqual(\n            Individual.objects.filter(\n                Q(alive=False) & Q(related_individual__isnull=True)\n            ),\n            [i4]\n        )\n", "    def test_ticket_24605_subquery(self):\n        \"\"\"\n        Subquery table names should be quoted, even if the outer query already has a\n        FROM clause.\n        \"\"\"\n        i1 = Individual.objects.create(alive=True)\n        RelatedIndividual.objects.create(related=i1)\n        i2 = Individual.objects.create(alive=False)\n        RelatedIndividual.objects.create(related=i2)\n        i3 = Individual.objects.create(alive=True)\n        i4 = Individual.objects.create(alive=False)\n\n        qs = Individual.objects.select_related('related_individual').filter(\n            Q(alive=False),\n            Q(related_individual__isnull=True)\n        ).order_by('pk')\n        self.assertSequenceEqual(qs, [i4])\n", "    def test_ticket_24897(self):\n        # Test ticket 24897: Filter on related object with lookup on parent field\n        class Parent(models.Model):\n            name = models.CharField(max_length=255)\n\n        class Child(models.Model):\n            parent = models.ForeignKey(Parent, on_delete=models.CASCADE)\n            value = models.CharField(max_length=255)\n\n        p1 = Parent.objects.create(name='p1')\n        p2 = Parent.objects.create(name='p2')\n        c1 = Child.objects.create(parent=p1, value='c1')\n        c2 = Child.objects.create(parent=p2, value='c2')\n\n        qs = Child.objects.filter(\n            parent__name=\"p1\",\n            Q(value='c1') | Q(value='c2')\n        )\n        self.assertSequenceEqual(qs, [c1])\n\n        qs = Child.objects.filter(\n            parent__name=\"p1\",\n            Q(value='c1') & Q(value='c2')\n        )\n        self.assertEqual(qs.count(), 0)\n\n\n", "    def test_ticket_24605_nested(self):\n        \"\"\"\n        Subquery table names should be quoted.\n        \"\"\"\n        i1 = Individual.objects.create(alive=True)\n        RelatedIndividual.objects.create(related=i1)\n        i2 = Individual.objects.create(alive=False)\n        RelatedIndividual.objects.create(related=i2)\n        i3 = Individual.objects.create(alive=True)\n        RelatedIndividual.objects.create(related=i3)\n        i4 = Individual.objects.create(alive=False)\n        RelatedIndividual.objects.create(related=i4)\n\n        self.assertSequenceEqual(\n            Individual.objects.filter(\n                Q(alive=True) & Q(related_individual__isnull=False)\n            ).order_by('pk'),  \n            [i1, i3]\n        )\n\n\n"], "sample_757": ["def test_one_hot_encoder_sparse_output():\n    X = np.array([['abc', 55], ['def', 55]])\n    enc = OneHotEncoder(sparse=True)\n    X_tr = enc.fit_transform(X)\n    assert isinstance(X_tr, sparse.csr_matrix)\n    assert X_tr.shape == (2, 3)\n", "def test_one_hot_encoder_sparse_transform():\n    X = np.array([[1, 2], [3, 4]], dtype='int64')\n    enc = OneHotEncoder(sparse=True)\n    X_tr = enc.fit_transform(X)\n    assert isinstance(X_tr, sparse.csr_matrix)\n    assert X_tr.shape == (2, 4)\n    assert_array_equal(X_tr.toarray(),\n                       np.array([[1, 0, 1, 0], [0, 1, 0, 1]]))\n\n\n\n", "def test_encoder_handle_unknown_in_transform():\n    X = np.array([['a', 1, 55], ['def', 1, 55]], dtype=object)\n    # check that handle_unknown='ignore' works as expected\n    enc = OneHotEncoder(handle_unknown='ignore', categories=[['a', 'b'], [1, 2, 3], [54, 55]])\n    exp = np.array([[1., 0., 0., 1., 0., 0., 0., 0., 0.],\n                    [0., 1., 0., 0., 1., 0., 0., 0., 0.]], dtype=float)\n    assert_array_equal(enc.transform(X).toarray(), exp)\n\n    # check that handle_unknown='error' raises exceptions\n    enc = OneHotEncoder(handle_unknown='error', categories=[['a', 'b'], [1, 2, 3], [54, 55]])\n    with pytest.raises(ValueError, match=\"Found unknown categories\"):\n        enc.transform(X)\n", "def test_encoder_fit_transform_with_sparse_output():\n    X = [[1, 2, 3], [4, 5, 6]]\n    enc = OneHotEncoder(sparse=True)\n    trans = enc.fit_transform(X)\n    assert isinstance(trans, sparse.csr_matrix)\n\n", "def test_encoder_handle_unknown_with_categories():\n    X = np.array([['abc', 2, 55], ['def', 1, 55]])\n    enc = OneHotEncoder(categories=[['abc', 'def'], [1, 2], [54, 55]],\n                        handle_unknown='ignore')\n    exp = np.array([[1., 0., 1., 1., 0., 0.],\n                    [0., 1., 1., 0., 0., 0.]],\n                   dtype='float64')\n    assert_array_equal(enc.fit_transform(X), exp)\n\n"], "sample_183": ["    def test_when_order_preservation(self):\n        with self.assertRaises(ValueError):\n            Case(\n                When(pk=1, then=Value('one')),\n                When(pk=2, then=Value('three')),\n                When(pk=1, then=Value('two')),  # Duplicate condition\n            )\n", "    def test_order_by_case_expression(self):\n        o = CaseTestModel.objects.create(integer=1, integer2=1, string='1')\n        qs = CaseTestModel.objects.filter(pk=o.pk).annotate(\n            foo=Case(\n                When(integer=1, then=2),\n                default=3,\n                output_field=IntegerField(),\n            )\n        ).order_by('foo')\n        self.assertQuerysetEqual(qs, [(o, 2)], lambda x: (x, x.foo))\n", "    def test_complex_lookup_chain(self):\n        o1 = CaseTestModel.objects.create(integer=1, integer2=1, string='1')\n        o2 = CaseTestModel.objects.create(integer=2, integer2=2, string='2')\n        o3 = CaseTestModel.objects.create(integer=3, integer2=3, string='3')\n        qs = CaseTestModel.objects.annotate(\n            foo=Case(\n                When(\n                    Q(integer__gt=1) & Q(integer2__lt=2), then=Value('a'),\n                ),\n                default=Value('b'),\n                output_field=CharField(),\n            ),\n        ).order_by('integer')\n        self.assertQuerysetEqual(\n            qs,\n            [(o1, 'b'), (o2, 'a'), (o3, 'b')],\n            lambda x: (x, x.foo)\n        )\n", "    def test_invalid_output_field_type(self):\n        with self.assertRaises(TypeError):\n            Case(\n                When(Q(pk__in=[1]), then=Value(1, output_field=str)),\n            )\n        with self.assertRaises(TypeError):\n            Case(\n                When(Q(pk__in=[1]), then=Value(1, output_field=None)),\n            )\n\n\n\n", "    def test_when_ordering(self):\n        with self.assertRaises(ValueError):\n            Case(When(Q(pk=1) ,then=1), When(Q(pk=2), then=2)).order_by('when')\n\n\n"], "sample_262": ["    def test_classproperty_inheritance(self):\n        class Parent:\n            @classproperty\n                return 'Parent'\n\n        class Child(Parent):\n            @classproperty\n                return 'Child'\n\n        self.assertEqual(Parent.parent_value, 'Parent')\n        self.assertEqual(Child.child_value, 'Child')\n        self.assertEqual(Child.parent_value, 'Parent')\n\n", "    def test_keep_lazy(self):\n        @keep_lazy(str)\n            return a * b\n\n        self.assertEqual(func('hello', 3), 'hellohellohello')\n        self.assertEqual(func(123, 4), 492)\n\n        @keep_lazy_text\n            return a * b\n\n        self.assertEqual(func_text('hello', 3), 'hellohellohello')\n\n        @keep_lazy(int, str)\n            return a + int(b)\n\n        self.assertEqual(func_mixed(123, '45'), 168)\n        self.assertEqual(func_mixed(123, 'abc'), 123)\n\n        @keep_lazy(int)\n  \n            return a\n\n        self.assertEqual(func(func_no_lazy(5), 3), func_no_lazy(5) * 3)\n\n", "    def test_keep_lazy(self):\n        @keep_lazy(str)\n            return a + b\n\n        @keep_lazy(str, int)\n            return a + str(b)\n\n        self.assertEqual(my_func('hello', lazy('world')), 'helloworld')\n        self.assertEqual(another_func('hello', lazy(123)), 'hello123')\n        self.assertEqual(my_func('hello', 'world'), 'helloworld')\n        self.assertEqual(another_func('hello', 123), 'hello123')\n\n        with self.assertRaises(TypeError):\n            keep_lazy(str)(3)\n", "    def test_keep_lazy(self):\n        @keep_lazy(str)\n            return a + b\n\n        @keep_lazy(str)\n            return a + b\n\n        self.assertEqual(my_func(\"hello\", \" world\"), \"hello world\")\n        self.assertEqual(my_func(lazy(\"hello\", str), \" world\"), \"hello world\")\n        self.assertEqual(my_func(\"hello\", lazy(\" world\", str)), \"hello world\")\n        self.assertEqual(my_func(lazy(\"hello\", str), lazy(\" world\", str)), \"hello world\")\n\n        with self.assertRaises(TypeError) as ctx:\n            my_other_func(1, 2)\n        self.assertEqual(str(ctx.exception), 'All arguments to keep_lazy decorated '\n                         'functions must be lazy objects.')\n\n        self.assertEqual(my_other_func(\"hello\", lazy(\" world\", str)), \"hello world\")\n        self.assertEqual(my_other_func(lazy(\"hello\", str), \" world\"), \"hello world\")\n        self.assertEqual(my_other_func(lazy(\"hello\", str), lazy(\" world\", str)), \"hello world\")\n\n\n\n\n\n\n", "    def test_classproperty_unbound(self):\n        class Foo:\n            @classproperty\n                return 123\n\n        self.assertEqual(Foo.foo, 123)\n"], "sample_617": ["compilation error", "def test_polyval_invalid_input():\n    with pytest.raises(TypeError):\n        xr.polyval(\"invalid\", xr.DataArray([1, 2, 3], dims=\"degree\"))\n    with pytest.raises(TypeError):\n        xr.polyval(xr.DataArray([1, 2, 3], dims=\"x\"), \"invalid\")\n    with pytest.raises(ValueError):\n        xr.polyval(xr.DataArray([1, 2, 3], dims=\"x\"), xr.DataArray([1, 2], dims=\"degree\"))\n", "def test_polyval_scalar_input():\n    x = 2\n    coeffs = xr.DataArray([2, 3, 4], dims=\"degree\", coords={\"degree\": [0, 1, 2]})\n    actual = xr.polyval(coord=x, coeffs=coeffs)\n    expected = 2**0 * 2 + 2**1 * 3 + 2**2 * 4\n    assert_allclose(actual, expected)\n", "def test_polyval_missing_coords() -> None:\n    x = xr.DataArray([1, 2, 3], dims=\"x\")\n    coeffs = xr.DataArray([2, 3, 4], dims=\"degree\")\n    with pytest.raises(ValueError, match=r\"must have coord\"):\n        xr.polyval(x, coeffs)\n", "def test_polyfit():\n    x = xr.DataArray([1, 2, 3], dims=\"x\")\n    y = xr.DataArray([2, 4, 6], dims=\"x\")\n    coeffs = xr.polyfit(x, y, 1)\n    assert coeffs.dims == (\"degree\",)\n    assert (coeffs.data == [1.0, 1.0]).all()\n\n    coeffs = xr.polyfit(x, y, 2)\n    assert coeffs.dims == (\"degree\",)\n    assert (coeffs.data == [1.0, 1.0, 0.0]).all()\n\n    with pytest.raises(ValueError):\n        xr.polyfit(x, y, 3)\n\n    with pytest.raises(ValueError):\n        xr.polyfit(x, x, 1)\n\n    with pytest.raises(ValueError):\n        xr.polyfit(x, y, dims=\"a\")\n\n    # test for empty arrays\n    x = xr.DataArray([], dims=\"x\")\n    y = xr.DataArray([], dims=\"x\")\n    coeffs = xr.polyfit(x, y, 1)\n    assert coeffs.dims == (\"degree\",)\n    assert (coeffs.data == [np.nan, np.nan]).all()\n\n    # test for multi-dimensional arrays\n    x = xr.DataArray(\n        [\n            [[1, 2, 3], [4, 5, 6]],\n            [[7, 8, 9], [10, 11, 12]],\n        ],\n        dims=(\"a\", \"b\", \"x\"),\n    )\n    y = xr.DataArray(\n        [\n            [[2, 4, 6], [8, 10, 12]],\n            [[14, 16, 18], [20, 22, 24]],\n        ],\n        dims=(\"a\", \"b\", \"x\"),\n    )\n    coeffs = xr.polyfit(x, y, 1, dims=\"x\")\n    assert coeffs.dims == (\"a\", \"b\", \"degree\")\n\n\n\n"], "sample_489": ["    def test_update_conflicts_with_target_unsupported(self):\n        with self.assertRaisesMessage(\n            NotSupportedError,\n            \"This database backend does not support updating conflicts with a target field\",\n        ):\n            TwoFields.objects.bulk_create(\n                [\n                    TwoFields(f1=1, f2=1),\n                    TwoFields(f1=2, f2=2),\n                ],\n                update_conflicts=True,\n                update_fields=[\"f2\"],\n                unique_fields=[\"f1\"],\n                target_field=\"f1\",\n            )\n\n", "    def test_update_conflicts_unique_fields_wrong_type(self):\n        msg = \"bulk_create() can only use integer primary keys or unique fields.\"\n        with self.assertRaisesMessage(TypeError, msg):\n            UpsertConflict.objects.bulk_create(\n                [\n                    UpsertConflict(number=1, rank=1, name=\"John\"),\n                    UpsertConflict(number=\"a\", rank=2, name=\"Mary\"),\n                ],\n                update_conflicts=True,\n                update_fields=[\"name\", \"rank\"],\n                unique_fields=[\"number\"],\n            )\n", "    def test_bulk_create_with_transaction(self):\n        with transaction.atomic():\n            TwoFields.objects.bulk_create([TwoFields(f1=1, f2=1)])\n            with self.assertNumQueries(2):  # One for each CREATE\n                self.assertEqual(TwoFields.objects.count(), 1)\n                TwoFields.objects.bulk_create([TwoFields(f1=2, f2=2)])\n            self.assertEqual(TwoFields.objects.count(), 2)\n\n", "    def test_update_conflicts_partial_update(self):\n        UpsertConflict.objects.bulk_create(\n            [\n                UpsertConflict(number=1, rank=1, name=\"John\"),\n                UpsertConflict(number=2, rank=2, name=\"Mary\"),\n                UpsertConflict(number=3, rank=3, name=\"Hannah\"),\n            ]\n        )\n        self.assertEqual(UpsertConflict.objects.count(), 3)\n\n        conflicting_objects = [\n            UpsertConflict(number=1, rank=4, name=\"Steve\"),\n            UpsertConflict(number=2, rank=None, name=\"Olivia\"),\n            UpsertConflict(number=3, rank=1, name=\"Hannah\"),\n        ]\n        results = UpsertConflict.objects.bulk_create(\n            conflicting_objects,\n            update_conflicts=True,\n            update_fields=[\"name\", \"rank\"],\n            unique_fields=[\"number\"],\n        )\n        self.assertEqual(len(results), len(conflicting_objects))\n        if connection.features.can_return_rows_from_bulk_insert:\n            for instance in results:\n                self.assertIsNotNone(instance.pk)\n        self.assertEqual(UpsertConflict.objects.count(), 3)\n        self.assertCountEqual(\n            UpsertConflict.objects.values(\"number\", \"rank\", \"name\"),\n            [\n                {\"number\": 1, \"rank\": 4, \"name\": \"Steve\"},\n                {\"number\": 2, \"rank\": 2, \"name\": \"Olivia\"},\n                {\"number\": 3, \"rank\": 1, \"name\": \"Hannah\"},\n            ],\n        )\n", "    def test_update_conflicts_unique_fields_with_pk(self):\n        UpsertConflict.objects.bulk_create(\n            [\n                UpsertConflict(number=1, rank=1, name=\"John\"),\n                UpsertConflict(number=2, rank=2, name=\"Mary\"),\n                UpsertConflict(number=3, rank=3, name=\"Hannah\"),\n            ]\n        )\n        self.assertEqual(UpsertConflict.objects.count(), 3)\n\n        obj1 = UpsertConflict.objects.get(number=1)\n        obj2 = UpsertConflict.objects.get(number=2)\n\n        conflicting_objects = [\n            UpsertConflict(pk=obj1.pk, number=4, rank=4, name=\"Steve\"),\n            UpsertConflict(pk=obj2.pk, number=5, rank=5, name=\"Olivia\"),\n            UpsertConflict(number=3, rank=1, name=\"Hannah\"),\n        ]\n        results = UpsertConflict.objects.bulk_create(\n            conflicting_objects,\n            update_conflicts=True,\n            update_fields=[\"name\", \"rank\"],\n            unique_fields=[\"pk\"],\n        )\n        self.assertEqual(len(results), len(conflicting_objects))\n        if connection.features.can_return_rows_from_bulk_insert:\n            for instance in results:\n                self.assertIsNotNone(instance.pk)\n        self.assertEqual(UpsertConflict.objects.count(), 3)\n        self.assertCountEqual(\n            UpsertConflict.objects.values(\"number\", \"rank\", \"name\"),\n            [\n                {\"number\": 4, \"rank\": 4, \"name\": \"Steve\"},\n                {\"number\": 5, \"rank\": 5, \"name\": \"Olivia\"},\n                {\"number\": 3, \"rank\": 1, \"name\": \"Hannah\"},\n            ],\n        )\n"], "sample_1145": ["def test_refine_matrixelement_non_symmetric():\n    from sympy.matrices.expressions.matexpr import MatrixSymbol\n    X = MatrixSymbol('X', 3, 3)\n    assert refine_matrixelement(X[0, 1], Q.symmetric(X)) == X[0, 1]\n    assert refine_matrixelement(X[1, 0], Q.symmetric(X)) == X[0, 1]  \n", "def test_refine_matrixelement_sym():\n    from sympy.matrices.expressions.matexpr import MatrixSymbol\n    x = MatrixSymbol('X', 3, 3)\n    assert refine_matrixelement(x[0,1], Q.symmetric(x)) == x[0,1]\n    assert refine_matrixelement(x[1,0], Q.symmetric(x)) == x[0,1]\n", "def test_refine_matrixelement_symmetric():\n    from sympy.matrices.expressions.matexpr import MatrixSymbol\n    X = MatrixSymbol('X', 3, 3)\n    assert refine(X[0, 1], Q.symmetric(X)) == X[0, 1]\n    assert refine(X[1, 0], Q.symmetric(X)) == X[0, 1]\n", "def test_refine_matrixelement_non_symmetric():\n    from sympy.matrices.expressions.matexpr import MatrixSymbol\n    X = MatrixSymbol('X', 3, 3)\n    assert refine_matrixelement(X[0, 1], Q.nonpositive(X[0, 0])) == X[0, 1]\n", "def test_matrix_element_non_symmetric():\n    X = MatrixSymbol('X', 2, 2)\n    assert refine(X[0, 1], Q.even(X[1, 0])) != X[0, 1]\n    assert refine(X[1, 0], Q.even(X[0, 1])) != X[1, 0]\n\n\n"], "sample_1180": ["compilation error", "compilation error", "def test_cross():\n    p1 = Point3D(1, 0, 0)\n    p2 = Point3D(0, 1, 0)\n    p3 = Point3D(0, 0, 1)\n    assert p1.cross(p2) == Point3D(0, 0, 1)\n    assert p2.cross(p1) == Point3D(0, 0, -1)\n    assert p1.cross(p3) == Point3D(-1, 0, 1)\n    assert p3.cross(p1) == Point3D(1, -1, 0)\n    with raises(ValueError, match=\"Cannot compute the cross product\"):\n        p1.cross(Point2D(1, 1))\n", "compilation error", "def test_issue_14599():\n    p = Point3D(1, 2, 3)\n    assert p.transform(Matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]])) == Point3D(32, 49, 66)\n\n    # Test inverse transform\n    inverse_transform = p.transform(Matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]]))\n    assert p.transform(inverse_transform) == Point3D(1, 2, 3)"], "sample_75": ["    def test_read_prefetched_objects_cache(self):\n        with self.assertNumQueries(2):\n            books = Book.objects.filter(title__in=['Les confessions Volume I', 'Candide']).prefetch_related('authors')\n        book1, book2 = list(books)\n        \n        with self.assertNumQueries(0):\n            self.assertEqual(book1.authors.all(), [self.author1])\n            self.assertEqual(book2.authors.all(), [self.author2])\n\n        # test that the cache is reused when querying the same object\n        with self.assertNumQueries(0):\n            self.assertEqual(book1.authors.all(), [self.author1])\n            self.assertEqual(book2.authors.all(), [self.author2])\n\n        with self.assertNumQueries(0):\n            self.assertEqual(book1.authors.all()[0].age, 70)\n            self.assertEqual(book2.authors.all()[0].age, 65)\n\n\n", "    def test_read_prefetched_objects_cache(self):\n        with self.assertNumQueries(2):\n            books = Book.objects.filter(title__in=['Les confessions Volume I', 'Candide']).prefetch_related('authors')\n            book1, book2 = list(books)\n\n        with self.assertNumQueries(0):\n            self.assertCountEqual(book1.authors.all(), [self.author1])\n            self.assertCountEqual(book2.authors.all(), [self.author2])\n\n", "    def test_read_prefetched_objects_cache(self):\n        with self.assertNumQueries(2):\n            books = Book.objects.prefetch_related('authors').all()\n        self.assertEqual(len(books), 2)\n        self.assertEqual(books[0].authors.count(), 1)\n        self.assertEqual(books[1].authors.count(), 1)\n\n\n\n\n", "    def test_read_prefetched_objects_cache(self):\n        with self.assertNumQueries(2):\n            books = Book.objects.prefetch_related('authors')\n            book1, book2 = list(books)\n\n        with self.assertNumQueries(0):\n            self.assertCountEqual(book1.authors.all(), [self.author1])\n            self.assertCountEqual(book2.authors.all(), [self.author2])\n\n            # Accessing the prefetched objects should not trigger any new queries.\n\n            book1_authors = list(book1.authors.all())\n            book2_authors = list(book2.authors.all())\n\n            self.assertEqual(book1_authors[0], self.author1)\n            self.assertEqual(book2_authors[0], self.author2)\n        \n", "    def test_remove_clears_prefetched_objects_from_cache(self):\n        bookwithyear = BookWithYear.objects.get(pk=self.bookwithyear1.pk)\n        prefetch_related_objects([bookwithyear], 'bookreview_set')\n\n        self.assertCountEqual(bookwithyear.bookreview_set.all(), [self.bookreview1])\n        bookwithyear.bookreview_set.remove(self.bookreview1)\n        self.assertCountEqual(bookwithyear.bookreview_set.all(), [])\n\n        new_bookreview = BookReview.objects.create()\n        bookwithyear.bookreview_set.add(new_bookreview)\n\n        # Test that the prefetched cache was cleared and the new book review is included\n        prefetch_related_objects([bookwithyear], 'bookreview_set')\n        self.assertCountEqual(bookwithyear.bookreview_set.all(), [new_bookreview])\n\n"], "sample_579": ["    def test_figsize_and_tight_layout(self):\n        kws = self.default_kws.copy()\n        kws['figsize'] = (10, 5)\n        kws['tight_layout'] = True\n\n        g = mat.clustermap(self.df_norm, **kws)\n        assert g.fig.get_size_inches() == (10, 5)\n        g.fig.tight_layout()\n        assert all([ax.get_position().x0 > 0 for ax in g.fig.axes])\n", "    def test_tree_kws_nested_colors(self):\n        colors = [(1, .5, .2), (0.8, 0.2, 0.8)]\n        g = mat.clustermap(self.df_norm, tree_kws=dict(color=colors))\n        for ax in [g.ax_col_dendrogram, g.ax_row_dendrogram]:\n            tree, = ax.collections\n            assert len(tree.get_facecolors()) == len(colors)\n            for i, color in enumerate(colors):\n                assert tuple(tree.get_facecolors()[i].squeeze())[:3] == color\n", "    def test_dendrogram_kws(self):\n        kws = self.default_kws.copy()\n        kws[\"dendrogram_kws\"] = dict(leaf_size=10)\n        g = mat.clustermap(self.df_norm, **kws)\n\n        for dendro in [g.ax_col_dendrogram, g.ax_row_dendrogram]:\n            for subtree in dendro.get_children():\n                for leaf in subtree.get_children():\n                    assert leaf.get_data()[0].shape[0] <= 10\n\n        kws[\"dendrogram_kws\"] = dict(orientation=\"top\")\n        g = mat.clustermap(self.df_norm, **kws)\n        for dendro in [g.ax_col_dendrogram, g.ax_row_dendrogram]:\n            assert dendro.get_position().y0 == 1.0 \n", "    def test_standalone_dendrogram(self):\n        kws = self.default_kws.copy()\n\n        # Test standalone row dendrogram\n        row_dendrogram = mat.dendrogram(\n            self.df_norm.T,\n            ax=plt.axes(),\n            orientation=\"horizontal\",\n            **kws[\"dendrogram_kws\"]\n        )\n        assert row_dendrogram.get_orientation() == \"horizontal\"\n        assert row_dendrogram.is_set_visible()\n\n        # Test standalone column dendrogram\n        col_dendrogram = mat.dendrogram(\n            self.df_norm,\n            ax=plt.axes(),\n            orientation=\"vertical\",\n            **kws[\"dendrogram_kws\"]\n        )\n        assert col_dendrogram.get_orientation() == \"vertical\"\n        assert col_dendrogram.is_set_visible()\n\n        plt.close()\n", "    def test_colormap_and_cmap(self):\n        kws = self.default_kws.copy()\n        kws[\"cmap\"] = mpl.cm.viridis\n        g1 = mat.clustermap(self.df_norm, **kws)\n\n        kws = self.default_kws.copy()\n        kws[\"colormap\"] = mpl.cm.viridis\n        g2 = mat.clustermap(self.df_norm, **kws)\n\n        assert g1.ax_heatmap.get_cmap() == g2.ax_heatmap.get_cmap()\n\n\n\n\n"], "sample_1012": ["compilation error", "def test_ComplexInfinity():\n    p = PythonCodePrinter()\n    assert p.doprint(oo) == 'float('inf')'\n    assert p.doprint(-oo) == 'float('-inf')'\n    assert p.doprint(zoo) == 'float('inf')'\n    assert p.doprint(-zoo) == 'float('-inf')'\n", "    def test_matrix_matmul():\n        p = NumPyPrinter()\n        a = [[1, 2], [3, 4]]\n        b = [[5, 6], [7, 8]]\n        matrix_a = p._parse_matrix(a)\n        matrix_b = p._parse_matrix(b)\n        assert p.doprint(matrix_a.dot(matrix_b)) == \\\n                '(array([[1, 2], [3, 4]]).dot(array([[5, 6], [7, 8]])))'\n", "compilation error", "def test_piecewise_complex():\n    expr = Piecewise((x**2, x > 0), (1, x <= 0),\n                    (x + 1j, x == 0))\n    assert pycode(expr) == '((x**2) if (x > 0) else (1) if (x <= 0) else (x + 1j) if (x == 0) else None)' \n"], "sample_756": ["compilation error", "def test_reachability_plot():\n    # Tests reachability plot generation.\n    points = np.array([[1, 1], [1, 2], [2, 1], [2, 2], [3, 1], [3, 2]])\n    distances = pairwise_distances(points)\n    expected_reachability = np.zeros_like(distances)\n    expected_reachability[0, 1] = 1\n    expected_reachability[1, 0] = 1\n    expected_reachability[0, 2] = np.sqrt(2)\n    expected_reachability[2, 0] = np.sqrt(2)\n    expected_reachability[1, 2] = np.sqrt(2)\n    expected_reachability[2, 1] = np.sqrt(2)\n    expected_reachability[0, 3] = 2\n    expected_reachability[3, 0] = 2\n    expected_reachability[1, 3] = np.sqrt(5)\n    expected_reachability[3, 1] = np.sqrt(5)\n    expected_reachability[2, 3] = np.sqrt(5)\n    expected_reachability[3, 2] = np.sqrt(5)\n\n    # Compute reachability plot using the distances\n    clust = OPTICS(min_samples=2, max_eps=2).fit(points)\n    reachability_plot = clust.reachability_plot(points)\n\n    # Compare the computed reachability plot to the expected one\n    assert_allclose(reachability_plot, expected_reachability)\n\n\n\n", "def test_reachability_ordering():\n    # Tests that reachability ordering is consistent\n    # with core/border/noise classification\n    centers = [[1, 1], [-1, -1], [1, -1]]\n    X, labels_true = make_blobs(n_samples=750, centers=centers,\n                                cluster_std=0.4, random_state=0)\n\n    op = OPTICS(min_samples=5).fit(X)\n    n_samples = len(X)\n    ordering = op.ordering_\n    reachability = op.reachability_\n\n    # Check that core points order before border points\n    for i in range(n_samples):\n        if op.core_samples_indices_.any(i == 0):\n            if i in op.border_samples_indices_:\n                assert ordering[i] < ordering[op.border_samples_indices_][0]\n            for j in range(n_samples):\n                if op.core_samples_indices_.any(j == 0):\n                    assert (reachability[i] <= reachability[j])\n\n\n", "def test_dbscan_optics_paritioning_large_data():\n    # Test that OPTICS clustering labels are <= 5% difference of DBSCAN on a \n    # large dataset (5000 points).\n\n    centers = [[1, 1], [-1, -1], [1, -1]]\n    X, labels_true = make_blobs(n_samples=5000, centers=centers,\n                cluster_std=0.4, random_state=0)\n\n    # calculate optics with dbscan extract at 0.3 epsilon\n    op = OPTICS(min_samples=10).fit(X)\n    core_optics, labels_optics = op.extract_dbscan(0.3)\n\n    # calculate dbscan labels\n    db = DBSCAN(eps=0.3, min_samples=10).fit(X)\n\n    contingency = contingency_matrix(db.labels_, labels_optics)\n    agree = min(np.sum(np.max(contingency, axis=0)),\n                np.sum(np.max(contingency, axis=1)))\n    disagree = X.shape[0] - agree\n\n    # verify core_labels match\n    assert_array_equal(core_optics, db.core_sample_indices_)\n\n    # verify label mismatch is <= 5% labels\n    percent_mismatch = np.round((disagree - 1) / (X.shape[0] - len(db.core_sample_indices_)), 2)\n    assert percent_mismatch <= 0.05\n\n\n\n", "def test_optics_ordering_with_min_samples_and_max_eps():\n    # Test that ordering is preserved when min_samples and max_eps\n    # are adjusted, ensuring consistency with ELKI.\n    X = [[1, 1], [2, 2], [3, 3], [4, 4], [5, 5], [6, 6],\n         [11, 11], [12, 12], [13, 13], [14, 14], [15, 15],\n         [21, 21], [22, 22], [23, 23], [24, 24], [25, 25]]\n    optics = OPTICS(min_samples=3, max_eps=2) # Base case\n    optics.fit(X)\n    base_ordering = optics.ordering_\n\n    # Reduce min_samples, should see ordering change\n    optics_smaller_samples = OPTICS(min_samples=2, max_eps=2)\n    optics_smaller_samples.fit(X)\n    smaller_ordering = optics_smaller_samples.ordering_\n\n    # Increase max_eps, should see less change in ordering\n    optics_larger_eps = OPTICS(min_samples=3, max_eps=3)\n    optics_larger_eps.fit(X)\n    larger_ordering = optics_larger_eps.ordering_\n\n    # assert that smaller samples ordering is a subset of the base\n    assert set(smaller_ordering).issubset(set(base_ordering))\n\n    # assert that larger eps ordering is almost the same as the base\n    assert np.array_equal(larger_ordering, base_ordering)\n\n"], "sample_154": ["    def test_database_checks_called_with_provided_databases(self, mock_connections):\n        mock_connections.return_value = {'default': mock.Mock(spec=connection.Connection),\n                                        'other': mock.Mock(spec=connection.Connection)}\n        issues = check_database_backends(databases=self.databases)\n        self.assertEqual(len(issues), 0)\n", "    def test_database_checks_called_with_kwargs(self, mocked_connections):\n        mocked_connections.return_value = {\n            'default': mock.Mock(),\n            'other': mock.Mock()\n        }\n        mocked_check = mock.patch(\n            'django.db.backends.base.validation.BaseDatabaseValidation.check',\n            return_value=[]\n        )\n\n        check_database_backends(databases=self.databases, backend='postgresql')\n        mocked_check.assert_any_call(backend='postgresql')\n", "    def test_database_checks_with_aliases(self, mocked_check, mocked_connections):\n        mocked_connections.return_value = {'default': mock.MagicMock(), 'other': mock.MagicMock()}\n        issues = check_database_backends(databases=['default', 'other'])\n        self.assertTrue(mocked_check.call_count == 2)\n        self.assertEqual(issues, [])\n", "    def test_database_check_uses_provided_databases(self, mocked_check, mocked_connections):\n        mocked_connections.return_value = {\n            'default': mock.Mock(spec=connection.Connection),\n            'other': mock.Mock(spec=connection.Connection)\n        }\n        issues = check_database_backends(databases=['default'])\n        mocked_check.assert_called_once_with(connection_alias='default', **{})\n        mocked_check.assert_not_called_with(connection_alias='other', **{})\n", "    def test_database_connection_used(self, mocked_connections):\n        mocked_connections.return_value = {'default': mock.Mock(), 'other': mock.Mock()}\n        issues = check_database_backends(databases=self.databases)\n        self.assertEqual(issues, [])  # Assuming no validation errors\n"], "sample_274": ["    def test_modelchoicefield_with_to_field_name(self):\n        ChoiceModel.objects.create(pk=1, name='a', value='v1')\n        ChoiceModel.objects.create(pk=2, name='b', value='v2')\n        ChoiceModel.objects.create(pk=3, name='c', value='v3')\n\n        # ModelChoiceField with to_field_name\n        f = ModelChoiceField(queryset=ChoiceModel.objects.all(),\n                             to_field_name='value', error_messages={'invalid_choice': '%(value)s IS INVALID CHOICE'})\n        self.assertFormErrors(['INVALID CHOICE'], f.clean, {'pk': 4})\n\n\n\n", "    def test_limit_choices_to(self):\n        ChoiceModel.objects.create(pk=1, name='a')\n        ChoiceModel.objects.create(pk=2, name='b')\n        ChoiceModel.objects.create(pk=3, name='c')\n\n        class MyForm(Form):\n            field = ModelChoiceField(\n                queryset=ChoiceModel.objects.all(),\n                limit_choices_to=lambda: ChoiceModel.objects.filter(name__startswith='b'),\n            )\n\n        f = MyForm()\n        # Assert choices are limited\n        self.assertEqual(f.fields['field'].choices, [(2, 'b')])\n\n\n\n", "    def test_custom_validation_error(self):\n        class MyModel(models.Model):\n            name = models.CharField(max_length=20)\n\n        class MyForm(forms.ModelForm):\n            class Meta:\n                model = MyModel\n\n                name = self.cleaned_data['name']\n                if name.startswith('invalid'):\n                    raise forms.ValidationError(\"Name cannot start with 'invalid'\")\n                return name\n\n        form = MyForm(data={'name': 'invalidstart'})\n        self.assertFormErrors(\n            ['Name cannot start with \\'invalid\\''],\n            form.clean,\n        )\n\n\n\n", "    def test_modelchoicefield_pk_value_error(self):\n        # Create choices for the model choice field tests below.\n        ChoiceModel.objects.create(pk=1, name='a')\n        ChoiceModel.objects.create(pk=2, name='b')\n        ChoiceModel.objects.create(pk=3, name='c')\n\n        e = {\n            'invalid_pk_value': '\u201c%(pk)s\u201d is not a valid value.',\n        }\n        f = ModelChoiceField(queryset=ChoiceModel.objects.all(), error_messages=e)\n        self.assertFormErrors(\n            ['\u201c4\u201d is not a valid value.'], f.clean, 4\n        )\n\n\n\n", "    def test_modelchoice_with_invalid_pk(self):\n        e = {\n            'invalid_pk_value': '\u201c%(pk)s\u201d is not a valid value.',\n        }\n        f = ModelChoiceField(queryset=ChoiceModel.objects.all(), error_messages=e)\n        with self.assertRaises(ValidationError):\n            f.clean('invalid_pk') \n"], "sample_429": ["    def test_min_value_validator_custom_message(self):\n        v = MinValueValidator(0, message=\"%(value)s must be greater than or equal to %(limit_value)s.\")\n        with self.assertRaisesMessage(\n            ValidationError, \"123 must be greater than or equal to 0.\"\n        ):\n            v(-123)\n        with self.assertRaisesMessage(\n            ValidationError, \"0 must be greater than or equal to 0.\"\n        ):\n            v(0)\n", "    def test_validate_slug_with_unicode(self):\n        v = validate_slug\n        self.assertEqual(v(\"\u4f60\u597d\"), None)\n        self.assertEqual(v(\"\u4f60\u597d\u4e16\u754c\"), None)\n        self.assertEqual(v(\"\u4f60\u597d-\u4e16\u754c\"), None)\n        self.assertEqual(v(\"\u4f60\u597d-\u4e16\u754c-\"), None)\n        self.assertEqual(v(\"\u4f60\u597d \u4e16\u754c\"), None)\n        self.assertEqual(v(\"\u4f60\u597d\u4e16\u754c!\"), None)\n        self.assertEqual(v(\"\u4f60\u597d\u4e16\u754c/\"), ValidationError)\n        self.assertEqual(v(\"\u4f60\u597d\u4e16\u754c\\\\\"), ValidationError)\n        self.assertEqual(v(\"\u4f60\u597d\u4e16\u754c:\"), ValidationError)\n        self.assertEqual(v(\"\u4f60\u597d\u4e16\u754c\\\"\"), ValidationError)\n        self.assertEqual(v(\"\u4f60\u597d\u4e16\u754c'\"), ValidationError)\n", "    def test_url_validator_with_params(self):\n        v = URLValidator(\n            allowed_schemes=(\"http\", \"https\", \"ftp\"),\n            # This scheme is invalid\n            disallowed_schemes=(\"ftps\",),\n        )\n        with self.assertRaises(ValidationError):\n            v(\"ftps://www.example.com/\")\n        with self.assertRaises(ValidationError):\n            v(\"invalid://www.example.com/\")\n        with self.assertRaises(ValidationError):\n            v(\"ftp://www.example.com/invalid\")\n        self.assertEqual(v(\"http://www.example.com/\"), None)\n        self.assertEqual(v(\"https://www.example.com/\"), None)\n        self.assertEqual(v(\"ftp://www.example.com/\"), None)\n\n\n", "    def test_null_character_validation(self):\n        v = ProhibitNullCharactersValidator()\n        self.assertIn(\"'\\x00'\", str(v(\"\\x00\")))\n        self.assertIn(\"'\\x00'\", str(v(\"\\x00Hello\")))\n        self.assertIn(\"'\\x00'\", str(v(\"Hello\\x00\")))\n\n\n\n", "    def test_url_validator_with_port(self):\n        self.assertTrue(URLValidator()(\n            \"https://www.example.com:8080/path\"\n        ))\n        self.assertTrue(URLValidator()(\n            \"http://example.com:443/path\"\n        ))\n        self.assertTrue(URLValidator()(\n            \"ftp://example.com:21/path\"\n        ))\n        self.assertTrue(URLValidator()(\n            \"https://example.com:443\"\n        ))\n\n\n\n"], "sample_132": ["    def test_sensitive_variables_decorator_function_arguments(self):\n        @sensitive_variables\n            return f\"Hello {username}, your password is {password}\"\n\n        with self.settings(DEBUG=True):\n            response = self.client.post('/sensitive-view/', data={'password': 'super_secret', 'username': 'tester'})\n            self.assertContains(response, 'XXXXXXXXXXXXXXXXXXXX', status_code=500)\n            self.assertIn('tester', response.content)\n\n        with self.settings(DEBUG=False):\n            response = self.client.post('/sensitive-view/', data={'password': 'super_secret', 'username': 'tester'})\n            self.assertContains(response, 'XXXXXXXXXXXXXXXXXXXX', status_code=500)\n            self.assertIn('tester', response.content)\n", "    def test_sensitive_variables_decorator_with_function_call_as_argument(self):\n        @sensitive_variables\n                return func(*args, **kwargs)\n            return inner\n        @wrapper\n            return password\n\n        self.assertEqual(test_func(\"super_secret\"), \"XXXXXXXXXXXXXXXXXXXX\")\n", "    def test_sensitive_variables_on_class_methods(self):\n        class TestClass:\n            @sensitive_variables\n                return user_password\n\n        instance = TestClass()\n        with self.settings(DEBUG=True):\n            with self.assertRaises(ValueError) as context:\n                instance.my_method('secret')\n            self.assertIn('sensitive_variables', str(context.exception))\n\n\n\n", "    def test_sensitive_variables_decorator_context(self):\n        @sensitive_variables\n            return password, args, kwargs\n\n        result = test_func('secret', 1, 2, a=3, b=4)\n        self.assertTupleEqual(result, ('XXXXXXXXXXXXXXXXXXXX', (1, 2), {'a': 3, 'b': 4}))\n", "    def test_sensitive_variables_decorator_with_func_args(self):\n        @sensitive_variables\n            return f'{user} {password} {sensitive_data}'\n\n        with self.settings(DEBUG=True):\n            response = self.client.post('/raises500/', data={'user': 'alice', 'password': 'secret', 'sensitive_data': 'classified'})\n            self.assertNotContains(response, 'secret', status_code=500)\n            self.assertNotContains(response, 'classified', status_code=500)\n\n\n"], "sample_726": ["compilation error", "def test_label_binarize_empty_array():\n    assert_array_equal(label_binarize([], classes=None), np.array([]))\n    assert_array_equal(label_binarize([], classes=[1, 2]), np.array([]))\n\n\n\n", "compilation error", "compilation error", "compilation error"], "sample_214": ["    def test_exclude_key_transform(self):\n        tests = (\n            ('value__baz__has_key', 'c'),\n            ('value__baz__has_keys', ['a', 'c']),\n            ('value__baz__has_any_keys', ['a', 'x']),\n            ('value__has_key', KeyTextTransform('foo', 'value')),\n        )\n        for lookup, value in tests:\n            with self.subTest(lookup=lookup):\n                self.assertIs(\n                    NullableJSONModel.objects.exclude(**{lookup: value}).exists(),\n                    False,\n                )\n", "    def test_key_exists_transform(self):\n        self.assertIs(NullableJSONModel.objects.filter(value__has_key=KeyTransform('d', 'value')).exists(), True)\n        self.assertIs(NullableJSONModel.objects.filter(value__has_key=KeyTransform('e', 'value')).exists(), False)\n", "    def test_key_transform_expression_with_subquery(self):\n        subquery = NullableJSONModel.objects.filter(value__baz__a=1).values('id')\n        query = NullableJSONModel.objects.annotate(\n            id=Subquery(subquery, output_field=models.IntegerField()),\n        ).filter(\n            id=F('value__baz__c'),\n        )\n        self.assertSequenceEqual(query, self.objs[3:5])\n", "    def test_key_transform_with_cast(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(**{\n                'value__foo__a': 1,\n            }),\n            self.objs[7:8],\n        )\n\n\n", "    def test_complex_key_transform(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__d__0__gt=0,\n                value__d__1__lt=10,\n            ),\n            [self.objs[4]],\n        )\n"], "sample_298": ["    def test_token_with_different_algorithm(self):\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        custom_algorithm = 'sha512'\n        with override_settings(PASSWORD_RESET_TOKEN_GENERATOR_ALGORITHM=custom_algorithm):\n            custom_password_generator = PasswordResetTokenGenerator()\n            custom_password_generator.algorithm = custom_algorithm\n            tk_custom = custom_password_generator.make_token(user)\n\n            default_password_generator = PasswordResetTokenGenerator()\n            default_password_generator.algorithm = 'sha256'\n            tk_default = default_password_generator.make_token(user)\n            self.assertNotEqual(custom_password_generator.algorithm, default_password_generator.algorithm)\n            self.assertIs(custom_password_generator.check_token(user, tk_custom), True)\n            self.assertIs(default_password_generator.check_token(user, tk_custom), False)\n", "    def test_token_generation_with_different_secrets(self):\n        \"\"\"\n        Tests that tokens generated with different secrets are not interchangeable.\n        \"\"\"\n        user1 = User.objects.create_user('user1', 'user1@example.com', 'password')\n        user2 = User.objects.create_user('user2', 'user2@example.com', 'password')\n\n        secret1 = 'secret1'\n        secret2 = 'secret2'\n        \n        generator1 = PasswordResetTokenGenerator()\n        generator1.secret = secret1\n        token1 = generator1.make_token(user1)\n\n        generator2 = PasswordResetTokenGenerator()\n        generator2.secret = secret2\n        token2 = generator2.make_token(user2)\n\n        # Verify that tokens are specific to their corresponding secrets\n\n        self.assertIs(generator1.check_token(user1, token1), True)\n        self.assertIs(generator2.check_token(user2, token2), True)\n\n        self.assertIs(generator1.check_token(user2, token2), False)  \n        self.assertIs(generator2.check_token(user1, token1), False)   \n\n", "    def test_token_with_different_algorithm(self):\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        p1 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        p1.algorithm = 'sha512'\n        with self.assertRaises(ImproperlyConfigured):\n            p1.make_token(user)\n        # Verify that the token generated with algorithm 'sha256'\n        # is not valid when using a different algorithm.\n        self.assertIs(p1.check_token(user, tk1), False)\n\n\n", "    def test_token_generation_with_large_timestamp(self):\n        # Test that tokens can be generated with timestamps beyond the\n        # supported range of base36 representation (around 2069)\n        user = User.objects.create_user('largetimestampuser', 'test2@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        # Create a timestamp far exceeding the range for base36 representation.\n        max_timestamp = int(2**36)  # Maximum value representable by int_to_base36\n        try:\n            p0.make_token(user, max_timestamp)\n        except ValueError:\n            self.assertTrue(True)  # Test passes if it raises ValueError\n        else:\n            self.fail(\"Expected ValueError when timestamp exceeds base36 range\")\n\n\n\n", "    def test_token_contains_timestamp(self):\n        user = User.objects.create_user('tokentimestampuser', 'test2@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        ts_b36, _ = tk1.split('-')\n        try:\n            ts = int(ts_b36)\n        except ValueError:\n            self.fail(\"Token timestamp does not look like a valid base36 number\")\n        # Check that the timestamp is within the valid range\n        now = datetime.now()\n        min_ts = int((datetime(2001, 1, 1) - timedelta(seconds=1)).total_seconds())\n        max_ts = int((datetime(2069, 12, 31) - timedelta(seconds=1)).total_seconds())\n        self.assertGreaterEqual(ts, min_ts)\n        self.assertLessEqual(ts, max_ts) \n"], "sample_1044": ["def test_issue_10824():\n    x = Symbol('x')\n    y = Symbol('y')\n    assert (x + y).is_zero is None\n\n\n\n", "def test_issue_10760():\n    x = Symbol('x', nonzero=True)\n    raises(ValueError, lambda: log(x, 0))\n", "def test_issue_10977():\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    assert ((x**2 + 1) * (y**2 + 1)).is_positive\n\n\n\n", "def test_issue_10462():\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    assert (x/y).is_zero is None\n    assert (x/y).is_finite is None \n    assert (x/y).is_positive is None\n    assert (x/y).is_negative is None\n\n\n\n", "def test_issue_10561():\n    x = Symbol('x', finite=True)\n    assert (x**oo).is_infinite is True\n    assert (x**-oo).is_infinite is True\n    assert (x**oo).is_positive is None\n\n\n\n"], "sample_193": ["    def test_proxy_not_in_related_models(self):\n        A = self.create_model(\"A\")\n        B = self.create_model(\"B\", bases=(A,), proxy=True)\n        self.assertRelated(A, [B])\n        self.assertRelated(B, [])\n", "    def test_abstract_with_fk(self):\n        A = self.create_model(\"A\", abstract=True, foreign_keys=[models.ForeignKey('B', models.CASCADE)])\n        B = self.create_model(\"B\")\n        self.assertRelated(A, [B])\n        self.assertRelated(B, [])\n", "    def test_proxy_to_concrete(self):\n        A = self.create_model(\"A\")\n        B = self.create_model(\"B\", bases=(A,), proxy=True)\n        C = self.create_model(\"C\", foreign_keys=[models.ForeignKey('B', models.CASCADE)])\n        self.assertRelated(A, [B, C])\n        self.assertRelated(B, [C])\n        self.assertRelated(C, [B])\n", "    def test_proxy_with_m2m(self):\n        A = self.create_model(\"A\")\n        B = self.create_model(\"B\", bases=(A,), proxy=True)\n        C = self.create_model(\"C\")\n        T = self.create_model(\"T\", foreign_keys=[\n            models.ForeignKey('B', models.CASCADE),\n            models.ForeignKey('C', models.CASCADE),\n        ])\n        self.assertRelated(A, [B, C, T])\n        self.assertRelated(B, [C, T])\n        self.assertRelated(C, [T])\n        self.assertRelated(T, [B, C])\n", "    def test_proxy_m2m(self):\n        A = self.create_model(\"A\", foreign_keys=[models.ManyToManyField('B', through='T')])\n        B = self.create_model(\"B\")\n        T = self.create_model(\"T\", foreign_keys=[\n            models.ForeignKey('A', models.CASCADE),\n            models.ForeignKey('B', models.CASCADE),\n        ])\n        C = self.create_model(\"C\", bases=(A,), proxy=True)\n        self.assertRelated(A, [B, T])\n        self.assertRelated(B, [A, T])\n        self.assertRelated(T, [A, B])\n        self.assertRelated(C, [B, T])\n\n"], "sample_1022": ["compilation error", "def test_repeated_decimals():\n    cases = {\n        '0.2[1]': '0.21111111111111111',\n        '0.1[2]': '0.12',\n        '0.12[34]': '0.1234',\n        '1.23[456]': '1.2345',\n        '0.123[456789]': '0.123456789'\n    }\n    transformations = standard_transformations + (repeated_decimals,\n                                                auto_number)\n    for case, expected in cases.items():\n        assert(parse_expr(case, transformations=transformations) ==\n               sympy.sympify(expected))\n\n", "    def split_custom(symbol):\n        if symbol in ('a*', 'b+', 'c-'):\n            return True\n        else:\n            return False\n", "compilation error", "compilation error"], "sample_1032": ["    def test_issue_12645():\n        from sympy.abc import a, b, c, x, y\n        assert Min(Max(a, b), Max(c, a)) == Min(a, c)\n        assert Max(Min(a, b), Min(c, a)) == Max(a, c)\n", "compilation error", "def test_issue_12163():\n    from sympy.abc import x, y\n\n    assert Min(x, x + y).subs(x, 1).subs(y, 2) == 1\n    assert Max(x, x + y).subs(x, 1).subs(y, 2) == 3\n\n    assert Min(x + y, x).subs(x, 1).subs(y, 2) == 1\n    assert Max(x + y, x).subs(x, 1).subs(y, 2) == 3\n\n\n", "def test_is_positive_is_nonnegative_is_negative():\n    from sympy.abc import x, y, z\n\n    assert Min(1, 2).is_positive is False\n    assert Min(1, 2).is_nonnegative is True\n    assert Min(1, 2).is_negative is False\n\n    assert Min(-1, -2).is_positive is False\n    assert Min(-1, -2).is_nonnegative is False\n    assert Min(-1, -2).is_negative is True\n\n    assert Max(1, 2).is_positive is True\n    assert Max(1, 2).is_nonnegative is True\n    assert Max(1, 2).is_negative is False\n\n    assert Max(-1, -2).is_positive is False\n    assert Max(-1, -2).is_nonnegative is False\n    assert Max(-1, -2).is_negative is True\n\n    assert Min(1, 0).is_positive is True\n    assert Min(1, 0).is_nonnegative is True\n    assert Min(1, 0).is_negative is False\n\n    assert Min(0, 1).is_positive is False\n    assert Min(0, 1).is_nonnegative is True\n    assert Min(0, 1).is_negative is False\n\n    assert Min(0, -1).is_positive is False\n    assert Min(0, -1).is_nonnegative is False\n    assert Min(0, -1).is_negative is True\n\n    assert Min(x, y).is_positive is None\n    assert Min(x, y).is_nonnegative is None\n    assert Min(x, y).is_negative is None\n\n    assert Max(x, y).is_positive is None\n    assert Max(x, y).is_nonnegative is None\n    assert Max(x, y).is_negative is None\n", "    def test_rewrite_with_sympy_functions():\n        from sympy.abc import x, y\n        from sympy.functions import sin, cos, tan, log, exp\n        assert Min(sin(x), y).rewrite(Abs) == Abs(sin(x) - y)\n        assert Max(cos(x), log(y)).rewrite(Abs) == Abs(cos(x) - log(y))\n        assert Min(x + 1, tan(y)).rewrite(Abs) == Abs(x + 1 - tan(y))\n        assert Max(x**2, exp(y)).rewrite(Abs) == Abs(x**2 - exp(y))\n"], "sample_837": ["def test_show_versions():\n    show_versions()\n", "def test_blas_info():\n    blas_info = _get_blas_info()\n\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info \n", "def test_blas_info():\n    blas_info = _get_blas_info()\n\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n", "def test_show_versions():\n    show_versions()\n    \n    # Assertions could be added here to verify the output format\n    # and content, but the core functionality is already tested. \n", "def test_get_blas_info():\n    blas_info = _get_blas_info()\n\n    assert 'macros' in blas_info\n    assert 'lib_dirs' in blas_info\n    assert 'cblas_libs' in blas_info\n"], "sample_662": ["    def test_user_properties(self, testdir, pytestconfig):\n        testdir.makepyfile(\n            \"\"\"\n                assert False\n            \"\"\"\n        )\n        reprec = testdir.inline_run(\n            \"--userprop=user_prop1=1 --userprop=user_prop2=abc\"\n        )\n        reports = reprec.getreports(\"pytest_runtest_logreport\")\n        assert len(reports) == 3\n        for rep in reports:\n            assert rep.user_properties == [\n                (\"user_prop1\", 1),\n                (\"user_prop2\", \"abc\"),\n            ]\n\n            data = pytestconfig.hook.pytest_report_to_serializable(\n                config=pytestconfig, report=rep\n            )\n            new_rep = pytestconfig.hook.pytest_report_from_serializable(\n                config=pytestconfig, data=data\n            )\n            assert new_rep.user_properties == [\n                (\"user_prop1\", 1),\n                (\"user_prop2\", \"abc\"),\n            ]\n", "    def test_hook_extra_data(self, testdir, pytestconfig):\n        testdir.makepyfile(\n            \"\"\"\n                assert False\n        \"\"\"\n        )\n        reprec = testdir.inline_run()\n        reports = reprec.getreports(\"pytest_runtest_logreport\")\n        assert len(reports) == 3\n        for rep in reports:\n            # Add some extra data to the report\n            rep.extra = {\"my_key\": \"my_value\"}\n            data = pytestconfig.hook.pytest_report_to_serializable(\n                config=pytestconfig, report=rep\n            )\n            new_rep = pytestconfig.hook.pytest_report_from_serializable(\n                config=pytestconfig, data=data\n            )\n            # Assert that the extra data is preserved\n            assert new_rep.extra == {\"my_key\": \"my_value\"}\n", "    def test_hook_report_serialization(self, testdir, pytestconfig):\n        testdir.makeclean()  \n        testdir.mkdir(\"subfolder\")\n        testdir.makefile(\"subfolder/__init__.py\", \"\")  \n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n                assert False\n\n                pass\n            \"\"\",\n            \"subfolder/test_mod.py\",\n        )\n        reprec = testdir.inline_run()\n        reports = reprec.getreports(\"pytest_runtest_logreport\") + reprec.getreports(\"pytest_collectreport\")\n        for rep in reports:\n            data = pytestconfig.hook.pytest_report_to_serializable(\n                config=pytestconfig, report=rep\n            )\n            # Check that the  report type is serialized correctly\n            assert data[\"$report_type\"] in [\"TestReport\", \"CollectReport\"]\n            new_rep = pytestconfig.hook.pytest_report_from_serializable(\n                config=pytestconfig, data=data\n            )\n            \n            # Basic assertion that the deserialized report has the same nodeid as the original\n            assert rep.nodeid == new_rep.nodeid\n\n", "    def test_collectreport_nested_collection(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n                pass\n            \n                pass\n\n                pass\n\n            package = Package()\n            \n            class Package:\n                    self.tests = [test_a, test_b, test_c]\n        \"\"\"\n        )\n        reprec = testdir.inline_run()\n        reports = reprec.getreports(\"pytest_collectreport\")\n        assert len(reports) == 3\n        # Verify that the nested collection is handled correctly\n        for i, report in enumerate(reports):\n            assert report.nodeid == f\"package.{i}\" \n", "    def test_report_serialization_custom_data(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n                pass\n        \"\"\"\n        )\n        reprec = testdir.inline_runsource(\"def test_func(): pass\")\n        reports = reprec.getreports(\"pytest_runtest_logreport\")\n        assert len(reports) == 3\n        test_func_call = reports[1]\n        test_func_call.extra = {\"custom_data\": \"hello world\"}\n        data = test_func_call._to_json()\n        newrep = TestReport._from_json(data)\n        assert newrep.extra == {\"custom_data\": \"hello world\"}\n\n"], "sample_1134": ["def test_issue_18258():\n    from sympy import symbols, Function\n    x, y = symbols('x, y')\n    f = Function('f')\n    assert latex(f(x, y)) == r'f{\\left(x, y \\right)}'\n", "def test_issue_18687():\n    from sympy import symbols, Matrix, Function\n    x, y, z = symbols('x y z')\n\n    m = Matrix([[1, 2], [3, 4]])\n\n    assert latex(m.T) == r\" \\begin{bmatrix} 1 & 3 \\\\ 2 & 4 \\end{bmatrix}\"\n    f = Function('f')\n    assert latex(f(m)) == r\"f\\left(\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\\right)\"\n", "def test_issue_17594():\n    from sympy import Function, lambdify, Symbol\n    x = Symbol('x')\n    y = Symbol('y')\n    f = Function('f')\n    g = Function('g')\n    \n    assert latex(lambdify(x, f(x))*g(x)) == r'\\left(\\operatorname{f}\\left(x\\right) g{\\left(x\\right)}\\right)' \n    assert latex(lambdify(x, f(x))*g(y)) == r'\\left(\\operatorname{f}\\left(x\\right) g{\\left(y\\right)}\\right)' \n    assert latex(lambdify(x, f(x)+g(x))) == r'\\operatorname{f}\\left(x\\right) + g{\\left(x\\right)}' \n", "    def test_issue_19864():\n        from sympy import Symbol, Function,  Rational\n        x, y = symbols('x y')\n        f = Function('f')\n        assert latex(f(x + Rational(1, 2)*y, x - Rational(1, 2)*y).subs(x, symbols('a')),\n                     use_unicode=True) == r'f\\left( a + \\frac{1}{2} y, a - \\frac{1}{2} y \\right)'\n", "def test_issue_10922():\n    from sympy import Matrix, Symbol\n    x = Symbol('x')\n    A = Matrix([[x, 1], [2, x]])\n    assert latex(A) == r' \\begin{bmatrix} x & 1 \\\\ 2 & x \\end{bmatrix}'\n"], "sample_951": ["compilation error", "compilation error", "def test_getargspec_from_descriptor():\n    class MyDescriptor:\n            pass\n\n        return inspect.getargspec(func)\n\n    argspec = get_argspec(MyDescriptor.__get__)\n    assert argspec.args == ['self', 'instance', 'owner']\n    assert argspec.varargs is None\n    assert argspec.keywords is None\n    assert argspec.defaults == ()\n", "compilation error", "compilation error"], "sample_405": ["    def test_references_field_with_chained_foreignkeys(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ForeignKey(\n                \"Intermediate\",\n                on_delete=models.CASCADE,\n                to_field=\"intermediary_field\",\n            ),\n        )\n        self.assertIs(\n            operation.references_field(\"Intermediate\", \"intermediary_field\", \"migrations\"),\n            True,\n        )\n        self.assertIs(operation.references_field(\"Model\", \"field\", \"migrations\"), False)\n        self.assertIs(\n            operation.references_field(\"Other\", \"whatever\", \"migrations\"), False\n        )\n\n\n\n", "    def test_references_field_by_related_name(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ForeignKey(\"Other\", models.CASCADE, related_name=\"related_to_model\"),\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"related_to_model\", \"migrations\"), True\n        )\n        self.assertIs(operation.references_field(\"Other\", \"whatever\", \"migrations\"), False)\n", "    def test_references_field_by_through_field(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ManyToManyField(\n                \"Other\", through=\"Through\", through_fields=(\"through_field1\", \"through_field2\")\n            ),\n        )\n        self.assertIs(\n            operation.references_field(\"Through\", \"through_field1\", \"migrations\"), True\n        )\n        self.assertIs(\n            operation.references_field(\"Through\", \"through_field2\", \"migrations\"), True\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"whatever\", \"migrations\"), True\n        )\n        self.assertIs(\n            operation.references_field(\"Missing\", \"whatever\", \"migrations\"), False\n        )\n", "    def test_references_field_by_related_name(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ForeignKey(\"Other\", models.CASCADE, related_name=\"related_name\"),\n        )\n        self.assertIs(operation.references_field(\"Other\", \"related_name\", \"migrations\"), True)\n        self.assertIs(\n            operation.references_field(\"Other\", \"whatever\", \"migrations\"), False\n        )", "    def test_references_field_by_one2many_reverse(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ForeignKey(\"Other\", models.CASCADE, to_field=\"id\", related_name=\"models\"),\n        )\n        self.assertIs(operation.references_field(\"Other\", \"id\", \"migrations\"), True)\n        self.assertIs(operation.references_field(\"Other\", \"whatever\", \"migrations\"), False)\n        self.assertIs(operation.references_field(\"Missing\", \"whatever\", \"migrations\"), False)\n"], "sample_878": ["def test_column_transformer_set_output_after_fitting_with_none(remainder):\n    pd = pytest.importorskip(\"pandas\")\n    df = pd.DataFrame(\n        {\n            \"pet\": pd.Series([\"dog\", \"cat\", \"snake\"], dtype=\"category\"),\n            \"age\": [1.4, 2.1, 4.4],\n            \"height\": [20, 40, 10],\n        }\n    )\n    ct = ColumnTransformer(\n        [\n            (\n                \"color_encode\",\n                OneHotEncoder(sparse_output=False, dtype=\"int16\"),\n                [\"pet\"],\n            ),\n            (\"age\", StandardScaler(), [\"age\"]),\n        ],\n        remainder=remainder,\n        verbose_feature_names_out=False,\n    )\n\n    # fit without calling set_output\n    X_trans = ct.fit_transform(df)  # No transformation yet\n    assert isinstance(X_trans, np.ndarray)\n    assert X_trans.dtype == \"float64\"  \n\n    ct.set_output(transform=\"pandas\")\n    # Calling transform with a DataFrame\n    X_trans_df = ct.transform(df)\n    \n    # Assert that set_output was called\n    assert isinstance(X_trans_df, pd.DataFrame) \n\n\n", "def test_verbose_feature_names_out_false_no_col_dropping_transformers(\n    transformers, remainder, expected_names", "def test_column_transformer_errors(remainder):\n    dftest = pd.DataFrame({\"col1\": [1, 2, 3], \"col2\": [4, 5, 6]})\n    with pytest.raises(ValueError, match=\"transformers.*non-Transformer\"):\n        ct = ColumnTransformer(\n            [(\"a\", \"invalid_transformer\", [\"col1\"])], remainder=remainder\n        )\n        ct.fit(dftest)\n\n", "def test_column_transformer_output_dataframe_type_errors(\n    transformers, remainder, verbose_feature_names_out", "def test_column_transformer_remainder_with_feature_names_out(transformers, remainder):\n    pd = pytest.importorskip(\"pandas\")\n    df = pd.DataFrame({\"a\": [1, 2], \"b\": [3, 4], \"c\": [5, 6]})\n    ct = ColumnTransformer(transformers, remainder=remainder)\n    ct.fit(df)\n    names = ct.get_feature_names_out()\n    expected_names = [\n        \"a\",\n        \"b\",\n        \"x\",\n        \"y\",\n    ] if remainder == \"passthrough\" else [\"a\", \"b\", \"x\", \"y\"]\n    assert_array_equal(names, expected_names)\n"], "sample_906": ["compilation error", "def test_domain_cpp_parse_targetname(app):\n    text = (\".. cpp:function:: void f()\\n\"\n            \"   :targetname: my_target_name\\n\"\n            \".. cpp:function:: void g()\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc, addnodes.index, desc))\n    assert_node(doctree[0], addnodes.index, entries=[('single', 'f (C++ function)', '_CPPv41f', '', None)])\n    assert_node(doctree[2], addnodes.index, entries=[('single', 'g (C++ function)', '_CPPv41g', '', None)])\n    assert doctree[0].get('targetname') == 'my_target_name'\n", "def test_domain_cpp_parse_special_members(app):\n    text = (\n        \".. cpp:class:: MyClass\\n\"\n        \"   :members:\\n\"\n        \"      .. cpp:member:: data\\n\"\n        \"      .. cpp:member:: operator=.\\n\"\n        \"      .. cpp:member:: ~MyClass()\\n\"\n    )\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc, addnodes.index, desc))\n    assert_node(doctree[0], addnodes.index, entries=[('single', 'MyClass (class)', '_CPPv45MyClass', '', None)])\n    assert_node(doctree[2], addnodes.index, entries=[('single', 'data', '_CPPv45MyClass14data', '', None),\n                                                 ('single', 'operator=(MyClass &MyClass)', '_CPPv45MyClass12operator_equalEQ', '', None),\n                                                 ('single', '~MyClass', '_CPPv45MyClass4_MyClass', '', None)])\n", "def test_domain_cpp_parse_cpp_directives(app):\n    text = (\n        \".. cpp::header:: include/my_header.h\\n\"\n        \".. cpp::function:: int add(int a, int b)\\n\"\n        \".. cpp::concept:: template <typename T> Foo<T>\\n\"\n        \".. cpp::type:: int\\n\"\n        ).. cpp::member:: int value\\n\"\n        \".. cpp::var:: int count\\n\"\n        \".. cpp::enum:: MyEnum\\n\"\n        \".. cpp::enumerator:: VALUE_ONE\\n\"\n        ).. cpp::union:: MyUnion\\n\"\n        \".. cpp::struct:: MyStruct\\n\"\n        \".. cpp::class:: MyClass\\n\"\n    )\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.seealso, nodes.literal, 'include/my_header.h'))\n    assert_node(doctree, (addnodes.index, desc, addnodes.index, desc))\n\n    all_entries = [entry for node in doctree for entry in node.entries if node.what() == \"index\"]\n    assert len(all_entries) == 9\n", "def test_domain_cpp_parse_implicit_type_name(app):\n    text = (\".. cpp:function:: void f(int) \\n\"\n            \"    :type: int\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc, addnodes.index, desc))\n    assert_node(doctree[0], addnodes.index, entries=[('single', 'f (C++ function)', '_CPPv41fv', '', None)])\n    assert_node(doctree[0][0], addnodes.desc, text=\"int\")\n"], "sample_520": ["def test_set_box_aspect():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection=\"3d\")\n    ax.set_box_aspect((1, 1, 1))\n    assert ax.get_box_aspect() == (1, 1, 1)\n\n    ax.set_box_aspect((2, 1, 3))\n    assert ax.get_box_aspect() == (2, 1, 3)\n\n    with pytest.raises(ValueError):\n        ax.set_box_aspect((1, 2, 3, 4))\n", "def test_axes_3d_equal():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.set_box_aspect((1,1,1))\n    assert ax.get_aspect() == (1, 1, 1)\n", "def test_collections_3d_with_lines():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n\n    # Create a scatter collection with colors\n    scatter_data = np.random.rand(100, 3)\n    scatter_collection = art3d.Poly3DCollection(\n        [scatter_data], facecolors='b', edgecolors='k', linewidths=1\n    )\n    ax.add_collection3d(scatter_collection)\n\n    # Create a line collection within the same axes\n    line_data = np.random.rand(10, 3)\n    line_collection = art3d.Line3DCollection(\n        [Line2D(line_data[:, :2], line_data[:, 2]) for line_data in\n         np.array_split(line_data, 3)],\n        colors='r', linewidths=2\n    )\n    ax.add_collection3d(line_collection)\n\n    fig.canvas.draw()\n    assert len(ax.collections) == 2 \n\n\n\n", "def test_patch_3d_set_alpha():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    patch = art3d.PathPatch3D(ax.get_xlim(), z=(0, 1), zdir='z')\n    patch.set_alpha(0.5)\n    assert patch.get_alpha() == 0.5\n\n    patch.set_alpha(1.0)\n    assert patch.get_alpha() == 1.0\n\n    with pytest.raises(ValueError):\n        patch.set_alpha(-0.1)\n\n\n", "def test_grid():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    ax.grid(True)\n    assert ax.get_proj()\n    \n    ax.grid(False)\n    assert not ax.get_proj() \n"], "sample_1135": ["compilation error", "compilation error", "def test_issue_14403():\n    from sympy import simplify, Symbol\n    x, y = Symbol('x', real=True), Symbol('y', real=True)\n    assert simplify(x + y - (1 + x/y) * y) == x + y - y - x == x\n    assert simplify(x + y - (1 + x/y) * y) == x\n", "def test_issue_15988():\n    x = symbols('x', real=True)\n    y = symbols('y', real=True)\n    assert (sqrt(-x*y)).find(real=False) is not None\n    assert (sqrt(-x*y)).as_real_imag() == (0j, sqrt(x*y))\n", "compilation error"], "sample_768": ["    def __repr__(self):\n        return f\"MockSplitter(a={self.a}, b={self.b}, c={self.c})\"\n    \n", "        def __repr__(self):\n            return f\"MockSplitter(a={self.a}, b={self.b}, c={self.c})\"\n", "        def __str__(self):\n            return f\"MockSplitter(a={self.a}, b={self.b}, c={self.c})\"\n", "        def __repr__(self):\n            return f\"MockSplitter(a={self.a}, b={self.b}, c={self.c})\"\n", "        def __repr__(self):\n            return f\"{self.__class__.__name__}(a={self.a}, b={self.b}, c={self.c})\"\n"], "sample_930": ["def test_create_index_with_sub_entries(app):\n    text = (\".. index:: pair: docutils; reStructuredText\\n\"\n            \"    :name: ref1\\n\"\n            \".. index:: pair: Python; interpreter\\n\"\n            \"    :name: ref2\\n\"\n            \".. index:: Sphinx\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n\n    # check index is created correctly\n    assert len(index) == 3\n    assert index[0] == ('D', [('docutils', [[('main', '#ref1')], [], None])])\n    assert index[1] == ('P', [('Python', [[('main', '#ref2')], [], None])])\n    assert index[2] == ('S', [('Sphinx', [[('', '#index-0')], [], None])])\n\n    # check the reference labels are created correctly\n    std = app.env.get_domain('std')\n    assert std.anonlabels['ref1'] == ('index', 'ref1')\n    assert std.anonlabels['ref2'] == ('index', 'ref2')\n", "def test_create_index_with_sub_indices(app):\n    text = (\".. index:: single: docutils\\n\"\n            \"   :name: ref1\\n\"\n            \".. index:: pair: Python; interpreter\\n\"\n            \"   :name: ref2\\n\"\n            \".. index:: Sphinx\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n\n    # check index is created correctly\n    assert len(index) == 3\n    assert index[0] == ('D', [('docutils', [[('', '#ref1')], [], None])])\n    assert index[1] == ('P', [('Python', [[], [('interpreter', [('', '#ref2')]),], None])])\n    assert index[2] == ('S', [('Sphinx', [[('', '#index-0')], [], None])])\n\n", "def test_create_index_with_subentry(app):\n    text = (\".. index:: pair: docutils; reStructuredText\\n\"\n            \".. index:: pair: docutils; :ref:`a_docutils_ref`\\n\"\n            \".. index:: pair: Python; interpreter\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n\n    assert len(index) == 3\n    assert index[0] == ('D', [('docutils', [[], [('seealso reStructuredText', [])],\n                None], [('a_docutils_ref', [[], [], None])])])\n    assert index[1] == ('P', [('Python', [[], [('interpreter', [])], None])])\n", "def test_create_index_with_subitems(app):\n    text = (\".. index:: pair: docutils; reStructuredText\\n\"\n            \".. index:: pair: docutils; :ref:`subitem1`\\n\"\n            \".. index:: pair: Sphinx; documentation tool\\n\"\n            \".. index:: pair: Sphinx; :ref:`subitem2`\\n\"\n            \n            \".. index:: single: foo\\n\"\n            \"   :name: foo1\\n\"\n            \".. index:: single: bar\\n\"\n            \"   :name: bar1\\n\"\n            \"   :subitems:\\n\"\n            \"     - baz\\n\"\n            \"     - qux\\n\"\n\n\n            )\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n    assert len(index) == 6\n    assert index[0] == ('D', [('docutils', [[], [('reStructuredText', [('', '#index-0')])], None])])\n    assert index[1] == ('D', [('docutils', [[], [('subitem1', [('', '#index-1')])], None])])\n    assert index[2] == ('S', [('Sphinx', [[], [('documentation tool', [('', '#index-2')])], None])])\n    assert index[3] == ('S', [('Sphinx', [[], [('subitem2', [('', '#index-3')])], None])])\n    assert index[4] == ('F', [('foo', [[('', '#foo1')], [], None])])\n    assert index[5] == ('B', [('bar', [[], [('baz', [('', '#index-4')]), ('qux', [('', '#index-5')])], None])])\n\n", "def test_create_index_with_subindex(app):\n    text = (\".. index:: pair: docutils; reStructuredText\\n\"\n            \"    :name: ref1\\n\"\n            \".. index:: pair: Python; interpreter\\n\"\n            \"    :name: ref2\\n\"\n            \".. index:: Sphinx; documentation tool\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n\n    # check index is created correctly\n    assert len(index) == 4\n    assert index[0] == ('D', [('docutils', [[('', '#ref1')], [], None])])\n    assert index[1] == ('I', [('interpreter', [[], [('Python', [('', '#ref2')])], None])])\n    assert index[2] == ('P', [('Python', [[], [('interpreter', [('', '#ref2')])], None])])\n    assert index[3] == ('S', [('Sphinx', [[], [('documentation tool', [('', '#index-2')])], None])])\n\n    # check the reference labels are created correctly\n    std = app.env.get_domain('std')\n    assert std.anonlabels['ref1'] == ('index', 'ref1')\n    assert std.anonlabels['ref2'] == ('index', 'ref2')\n"], "sample_822": ["def test_check_different_data_types():\n    # Ensure an error is raised if the data types are different.\n    XA = np.array([1, 2, 3, 4], dtype=np.int64)\n    XB = np.array([1, 2, 3, 4], dtype=np.float32)\n    assert_raises(ValueError, check_pairwise_arrays, XA, XB)\n    XA = np.array([1, 2, 3, 4], dtype=np.float64)\n    XB = np.array([1, 2, 3, 4], dtype=np.int32)\n    assert_raises(ValueError, check_pairwise_arrays, XA, XB)\n", "def test_normalised_cosine_with_empty_arrays(dtype):\n    # Test that cosine similarity\n    # handles empty arrays correctly\n    from sklearn.metrics.pairwise import cosine_similarity\n    X = np.array([[1, 2]], dtype=dtype)\n    for shape in [(0, 2), (2, 0)]:  \n        Y = np.empty(shape, dtype=dtype)\n        with pytest.raises(ValueError):\n            cosine_similarity(X, Y)\n    \n    for shape in [(2, 0), (0, 2)]:\n        Y = np.empty(shape, dtype=dtype)\n        with pytest.raises(ValueError):\n            cosine_similarity(X, Y)\n    \n    # test that the empty matrix remains empty\n    Y = np.empty((0, 0), dtype=dtype)\n    cosine_sim = cosine_similarity(X, Y)\n    assert cosine_sim.shape == (X.shape[0], Y.shape[0])\n    assert np.all(np.isnan(cosine_sim))\n    \n", "def test_check_invalid_input_types():\n    # Ensure the function raises an error for invalid input types.\n    XA = np.arange(40).reshape(5, 8)\n    \n    assert_raises(TypeError, check_pairwise_arrays, XA, \"string\")\n    assert_raises(TypeError, check_pairwise_arrays, XA, [1,2,3])\n    assert_raises(TypeError, check_pairwise_arrays, XA, (1,2,3))\n    assert_raises(TypeError, check_pairwise_arrays, None, XA)\n", "def test_missing_dimensions_in_pairwise_arrays():\n    # Ensure that if one array has a missing dimension, the result\n    # contains the NaNs.\n\n    XA = np.reshape(np.arange(40), (5, 8))\n    XB = np.arange(40)\n    XA_checked, XB_checked = check_pairwise_arrays(XA, XB)\n    assert np.isnan(XB_checked).all()\n\n    XB = np.reshape(np.arange(40), (4, 10))\n    assert_raises(ValueError, check_pairwise_arrays, XA, XB)\n\n    XB = np.arange(40).reshape(4, 10)\n    XA_checked, XB_checked = check_pairwise_arrays(XA, XB)\n    assert np.isnan(XA_checked).all()\n\n    XB = np.arange(40).reshape(8, 5)\n    XA_checked, XB_checked = check_pairwise_arrays(XA, XB)\n    assert np.isnan(XB_checked).all()\n", "def test_jaccard_similarity():\n    rng = np.random.RandomState(0)\n    X = rng.randint(0, 2, size=(5, 4))\n    Y = rng.randint(0, 2, size=(3, 4))\n    \n    # Test jaccard similarity\n    J = jaccard_similarity(X, Y)\n    assert np.shape(J) == (5, 3)\n    # check that values are between 0 and 1\n    assert np.all(J >= 0)\n    assert np.all(J <= 1)\n\n\n"], "sample_667": ["    def test_ensure_deletable_already_gone(tmp_path):\n        from _pytest.pathlib import ensure_deletable\n        (tmp_path / \"file\").unlink()\n        with pytest.raises(OSError):\n            ensure_deletable(tmp_path / \"file\")\n", "compilation error", "    def test_relative_to_basetemp_returns_relative_path_always(tmpdir, tmp_path):\n        from _pytest.pathlib import TempPathFactory\n\n        mytemp = tmpdir.mkdir(\"mytemp\")\n        factory = TempPathFactory(basetemp=mytemp)\n        p = factory.mktemp(\"world\")\n        assert p.relative_to(factory.getbasetemp()) == \"world0\"\n", "compilation error", "def test_numbered_dir_with_prefix_collision(tmp_path):\n    from _pytest.pathlib import make_numbered_dir\n\n    root = tmp_path\n    prefix1 = \"foo-\"\n    prefix2 = \"foo-\"\n    d1 = make_numbered_dir(root, prefix=prefix1)\n    d2 = make_numbered_dir(root, prefix=prefix2)\n    assert d1.name.startswith(prefix1)\n    assert d2.name.startswith(prefix2)\n    assert d1.name != d2.name\n"], "sample_364": ["    def test_reverse_type_error_propogates(self):\n        @DynamicConverter.register_to_url\n            raise TypeError('Reverse type error.')\n        with self.assertRaisesMessage(NoReverseMatch, 'Reverse type error.'):\n            reverse('dynamic', kwargs={'value': object()})\n", "    def test_reverse_type_error_propagates(self):\n        @DynamicConverter.register_to_url\n            raise TypeError('This type error propagates.')\n        with self.assertRaisesMessage(TypeError, 'This type error propagates.'):\n            reverse('dynamic', kwargs={'value': object()})\n", "    def test_reverse_type_error_propagates(self):\n        @DynamicConverter.register_to_url\n            raise TypeError('This type error propagates.')\n        with self.assertRaisesMessage(TypeError, 'This type error propagates.'):\n            reverse('dynamic', kwargs={'value': object()})\n", "    def test_resolve_reverse_type_error_results_in_no_match(self):\n        @DynamicConverter.register_to_url\n            raise TypeError\n        with self.assertRaises(NoReverseMatch):\n            reverse('dynamic', kwargs={'value': object()})\n", "    def test_reverse_type_error_means_no_match(self):\n        @DynamicConverter.register_to_url\n            raise TypeError('This type error propagates.')\n        with self.assertRaisesMessage(NoReverseMatch, 'This type error propagates.'):\n            reverse('dynamic', kwargs={'value': object()})\n"], "sample_732": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1162": ["def test_nfloat_dict():\n    from sympy import sympify\n    a = sympify({'x': 2, 'y': 3*I})\n    b = nfloat(a, dkeys=True)\n    assert b == {'x': 2.0, 'y': 3.0*I}\n    c = nfloat(a, dkeys=False)\n    assert c == {'x': 2.0, 'y': 3.0*I}\n\n    a = sympify({1: 2, 'x': 3*I})\n    b = nfloat(a, dkeys=True)\n    assert b == {1: 2.0, 'x': 3.0*I}\n    c = nfloat(a, dkeys=False)\n    assert c == {1: 2.0, 'x': 3.0*I}", "def test_nfloat():\n    from sympy.core.function import nfloat\n    from sympy.abc import x, y\n    from sympy import cos, pi, sqrt\n\n    expr = x**4 + x/2 + cos(pi/3) + 1 + sqrt(y)\n    assert nfloat(expr) == x**4 + 0.5*x + sqrt(y) + 1.5\n\n    expr = x**4 + sqrt(y)\n    assert nfloat(expr, exponent=True) == x**4.0 + y**0.5\n\n    expr = (1, 2)\n    assert type(nfloat(expr)) is tuple\n    assert nfloat(expr) == (1.0, 2.0)\n", "def test_nfloat_dict():\n    d = {Symbol('x'): 2, Symbol('y'): 3}\n    assert nfloat(d) == {Symbol('x'): 2.0, Symbol('y'): 3.0}\n\n\n", "def test_nfloat_exponents():\n    from sympy.abc import x, y\n    from sympy import sqrt, cos, pi\n\n    assert nfloat(x**4 + sqrt(y), exponent=True) == x**4.0 + y**0.5\n    assert nfloat(x**2*sqrt(y), exponent=True) == x**2.0*y**0.5\n    assert nfloat(cos(pi/3)**x) == cos(pi/3)**x\n    assert nfloat(x**y) == x**y  \n", "def test_nfloat_special_cases():\n    from sympy.core.numbers import Float\n    from sympy.core.function import nfloat\n\n    assert nfloat(2*S.GoldenRatio).is_instance(Float)\n    assert nfloat(pi + I).is_instance(Float)\n    assert nfloat(pi**2).is_Float\n    assert nfloat(pi*S.ImaginaryUnit).is_instance(Float)\n"], "sample_39": ["def test_wcs_from_string():\n    # Test loading WCS from string representation\n    header_str = \"\"\"SIMPLE  =                    T / conforms to FITS standard", "        def test_sip_coeff_parsing(self):\n            sip_header = get_pkg_data_contents(os.path.join(\"data\", \"sip.hdr\"), encoding='binary')\n\n            w = wcs.WCS(sip_header)\n\n            a_order = w.sip.a_order\n            b_order = w.sip.b_order\n            ap_order = w.sip.ap_order\n            bp_order = w.sip.bp_order\n            \n            self.assertEqual(a_order, 4)\n            self.assertEqual(b_order, 4)\n            self.assertEqual(ap_order, 0)\n            self.assertEqual(bp_order, 0)\n\n            np.testing.assert_allclose(w.sip.crpix, [2048., 1024.])\n\n\n", "def test_invalid_wcs_with_errors():\n    \"\"\"\n    Test handling of invalid WCS headers with appropriate error messages.\n    \"\"\"\n    # Test missing CTYPE keywords\n    hdr = get_pkg_data_contents(\"data/invalid_header.hdr\")\n    with pytest.raises(wcs.InvalidWCSHeaderError) as exc:\n        wcs.WCS(hdr)\n    assert \"Missing CTYPE keywords\" in str(exc.value)\n\n    # Test incorrect SIP specification\n    hdr = get_pkg_data_contents(\"data/sip-broken.hdr\")\n    with pytest.raises(wcs.InvalidWCSHeaderError) as exc:\n        wcs.WCS(hdr)\n    assert \"Invalid SIP coefficient values\" in str(exc.value)\n\n\n\n", "def test_wcs_from_string():\n    \"\"\"\n    Test reading a WCS from a string.\n    \"\"\"\n    header_str = \"\"\"", "def test_sip_order():\n    \n    hdr = get_pkg_data_contents(\"data/sip-order.hdr\")\n    w = wcs.WCS(hdr)\n\n    assert w.sip.a_order == 4\n    assert w.sip.b_order == 4\n    assert w.sip.ap_order == 0\n    assert w.sip.bp_order == 0\n\n"], "sample_795": ["def test_check_estimator_with_deprecated_attribute():\n    class DeprecatedEstimator(BaseEstimator):\n            self.deprecated_param = deprecated_param\n\n        @deprecated(\n            \"Attribute 'deprecated_param' is deprecated since 0.2.0.\"\n            \" Use 'new_param' instead.\"\n        )\n            return self.deprecated_param\n\n            if 'new_param' in kwargs:\n                self.deprecated_param = kwargs.pop('new_param')\n            return super().set_params(**kwargs)\n\n    # Test that the deprecation warning is raised when deprecated_param is accessed.\n    msg = r\"Attribute 'deprecated_param' is deprecated\"\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(\"always\")\n        estimator = DeprecatedEstimator(deprecated_param=\"value\")\n        estimator.get_deprecated_param()\n        assert len(w) == 1\n        assert msg in str(w[0].message)\n    \n    # Test that the 'new_param' parameter takes precedence.\n    estimator = DeprecatedEstimator(deprecated_param=\"value\")\n    estimator.set_params(new_param=\"new_value\")\n    assert estimator.deprecated_param == \"new_value\"\n\n\n", "def test_check_estimator_sparse_dtype_validation():\n    class SparseDtypeEstimator(BaseEstimator):\n            if X.dtype != np.float64:\n                raise ValueError(\n                    \"Estimator expects float64 dtype for sparse data\"\n                )\n            return X\n\n    check_estimator(SparseDtypeEstimator())\n", "def test_check_no_attributes_set_in_init_with_default_args():\n    class NonConformantEstimatorPrivateSet:\n            self.you_should_not_set_this_ = other\n\n    with ignore_warnings(category=(FutureWarning, DeprecationWarning)):\n        assert_raises_regex(AssertionError,\n                            \"Estimator estimator_name should not set any\"\n                            \" attribute apart from parameters during init.\"\n                            r\" Found attributes \\['you_should_not_set_this_'\\].\",\n                            check_no_attributes_set_in_init,\n                            'estimator_name',\n                            NonConformantEstimatorPrivateSet())\n\n\n", "def test_check_estimator_no_validation_errors():\n    # check estimator with no validation errors\n    class NoValidationEstimator(BaseEstimator):\n            pass\n            return self\n            return X\n\n    check_estimator(NoValidationEstimator())\n", "def test_check_get_params_invariance():\n    # Tests that the get_params() method returns the same parameters\n    # regardless if it is called with  deep = True or deep = False\n    # for estimators with a set_params() method. \n    for Estimator in [GaussianMixture, LinearRegression,\n                      RandomForestClassifier, NMF, SGDClassifier,\n                      MiniBatchKMeans]:\n        with ignore_warnings(category=(FutureWarning, DeprecationWarning)):\n            est = Estimator()\n            set_checking_parameters(est)\n            set_random_state(est)\n            shallow_params = est.get_params(deep=False)\n            deep_params = est.get_params(deep=True)\n            assert_equal(set(shallow_params.keys()),\n                         set(deep_params.keys()))\n            for key in shallow_params:\n                assert_equal(shallow_params[key], deep_params[key])\n"], "sample_1070": ["def test_issue_19371():\n    x = Symbol('x')\n    assert log(exp(x)).expand() == x\n\n", "def test_log_issue_9664():\n    x = Symbol(\"x\")\n    assert log(x, 2).rewrite(log) == log(x) / log(2)\n    assert log(x, 2).rewrite(exp) == -log(2)/log(x)\n\n\n", "def test_log_complex():\n    z = Symbol('z', complex=True)\n    assert log(z).is_complex is True\n    assert log(1).is_real is True\n    assert log(-1).is_real is False\n    assert log(I).is_real is False\n    assert log(-I).is_real is False\n    assert log(1 + I).expand(complex=True) == log(sqrt(2))/2 + I*pi/4\n    assert log(1 - sqrt(2)).expand(complex=True) == log(sqrt(2) - 1) + I*pi\n    assert log(z**2).expand() == 2*log(z)\n    assert log(z*y).expand() == log(z) + log(y)\n    assert log(z/y).expand() == log(z) - log(y)\n    assert log(z**y).expand() == y*log(z)\n\n\n\n    \n", "def test_log_limit():\n    x = Symbol('x')\n    assert limit(log(x), x, 0, '+').is_oo is True\n    assert limit(log(x), x, 0, '-').is_neg_oo is True\n    assert limit(log(x), x, oo).is_oo is True\n    assert limit(log(x), x, 1).is_0 is True\n    assert limit(log(x), x, 2).is_log(2) is True\n\n\n", "def test_log_issue_12854():\n    x = Symbol('x')\n    assert log(x, 0).expand() == S.Infinity\n    assert log(0, x).expand() == S.Infinity\n\n\n\n"], "sample_346": ["    def test_cache_control_decorator_with_kwargs(self):\n        @cache_control(max_age=3600, s_maxage=7200, public=True)\n            return HttpResponse()\n        r = a_view(HttpRequest())\n        self.assertEqual(\n            set(r.headers['Cache-Control'].split(', ')),\n            {'max-age=3600', 's-maxage=7200', 'public'},\n        )\n", "compilation error", "    def test_cache_control_decorator(self):\n        @cache_control(max_age=3600)\n            return HttpResponse()\n        r = a_view(HttpRequest())\n        self.assertEqual(\n            set(r.headers['Cache-Control'].split(', ')),\n            {'max-age=3600', 'private'}\n        )\n\n\n\n", "    def test_cache_control_decorator(self):\n        @cache_control(max_age=3600, must_revalidate=True, no_cache=True)\n            return HttpResponse()\n        r = a_view(HttpRequest())\n        self.assertEqual(\n            set(r.headers['Cache-Control'].split(', ')),\n            {'max-age=3600', 'must-revalidate', 'no-cache'},\n        )\n\n\n", "    def test_cache_control_decorator(self):\n        @cache_control(max_age=3600, public=True, must_revalidate=True)\n            return HttpResponse()\n        r = a_view(HttpRequest())\n        self.assertEqual(\n            set(r.headers['Cache-Control'].split(', ')),\n            {'max-age=3600', 'public', 'must-revalidate'},\n        )\n"], "sample_972": ["def test_stringify_Callable_complex_args(annotation, expected):\n    assert stringify(annotation) == expected\n\n\n\n\n\n", "def test_stringify_type_hints_typedDict():\n    from typing import TypedDict  # type: ignore\n    MyTypedDict = TypedDict('MyTypedDict', {'a': int, 'b': str})\n    assert stringify(MyTypedDict) == \"TypedDict['MyTypedDict', {'a': int, 'b': str}]\"\n    assert stringify(MyTypedDict, \"smart\") == \"~typing.TypedDict['MyTypedDict', {'a': int, 'b': str}]\"\n", "def test_stringify_type_hints_paramspec():\n    from typing import ParamSpec  # type: ignore\n    P = ParamSpec('P') \n    assert stringify(P) == \"ParamSpec['P']\"\n    assert stringify(P, \"smart\") == \"~typing.ParamSpec['P']\"\n", "compilation error", "def test_stringify_type_hints_Annotated_with_NoneType():\n    from typing import Annotated  # type: ignore\n\n    assert stringify(Annotated[int, None]) == \"Annotated[int, None]\"\n    assert stringify(Annotated[int, None], \"fully-qualified\") == \"typing.Annotated[int, None]\"\n    assert stringify(Annotated[int, None], \"smart\") == \"~typing.Annotated[int, None]\"\n"], "sample_941": ["    def test_stringify_type_hints_TypeVar_GenericMeta():\n        from typing import Generic, TypeVar\n\n        class MyGeneric(Generic[T]):\n            pass\n\n        T = TypeVar('T')\n        assert stringify(MyGeneric) == \"tests.test_util_typing.MyGeneric[T]\"\n        assert stringify(type(MyGeneric)) == \"tests.test_util_typing.MyGeneric\"\n", "    def test_stringify_type_hints_GenericMeta_origin_callable():\n        from typing import GenericMeta, Callable  # type: ignore\n        class MyCallable(GenericMeta, Callable):\n            __origin__ = Callable\n            __args__ = (str, int)\n\n        assert stringify(MyCallable) == \"Callable[[str], int]\" \n", "def test_stringify_type_hints_ForwardRef():\n    from typing import ForwardRef  # type: ignore\n    assert stringify(ForwardRef(\"myint\")) == \"myint\" \n\n\n\n", "def test_stringify_type_hints_forward_ref():\n    from typing import ForwardRef  # type: ignore\n    \n    forwarded_type = ForwardRef(\"MyForwardedType\")\n    assert stringify(forwarded_type) == \"MyForwardedType\" \n", "compilation error"], "sample_680": ["def test_importorskip_present(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    pytest.importorskip(\"requests\")\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*test_func*\"])\n", "compilation error", "compilation error", "compilation error", "def test_importorskip_with_module_as_string():\n    with pytest.raises(\n        pytest.skip.Exception,\n        match=\"^could not import '.*test_fixtures.ModuleWithFixtures: .*', module not found$\",\n    ):\n        pytest.importorskip(\"test_fixtures.ModuleWithFixtures\")\n"], "sample_329": ["    def test_serialize_custom_serializer(self):\n        class MyCustomSerializer(BaseSerializer):\n                return 'my_custom(%r)' % self.value, {}\n        with self.subTest(\"CustomSerializer on objects\"):\n            self.assertSerializedResultEqual(\n                MyCustomSerializer(value=\"hello\"),\n                (\"my_custom('hello')\", {})\n            )\n\n        with self.subTest(\"CustomSerializer on field\"):\n            field = models.CharField(default=MyCustomSerializer(value=\"hello\"))\n            string = MigrationWriter.serialize(field)[0]\n            self.assertEqual(string, \"models.CharField(default=my_custom('hello'))\")\n\n", "    def test_serialize_nested_structure(self):\n        class NestedObject:\n                self.value = value\n\n        nested_object = NestedObject(42)\n        self.assertSerializedEqual(nested_object)\n        self.assertSerializedResultEqual(\n            nested_object,\n            ('migrations.test_writer.NestedObject(42)', {'import migrations.test_writer'}),\n        )\n\n\n", "    def test_serialize_custom_field(self):\n        class MyCustomField(models.Field):\n                super().__init__(**kwargs)\n\n                return value\n\n                return 'CharField'\n\n                return 'CharField'\n\n        field = models.Model(\n            myfield=MyCustomField(),\n        )\n        string, imports = MigrationWriter.serialize(field)\n        self.assertIn('MyCustomField', string)\n        self.assertIn('from migrations.test_writer import MyCustomField', imports)\n\n", "    def test_serialize_complex_types(self):\n        class ComplexObject:\n                self.value = value\n\n        serialized = MigrationWriter.serialize(ComplexObject(123))[0]\n        self.assertEqual(serialized, 'complex(123)')\n", "    def test_serialize_function_with_args(self):\n            return arg1 + arg2\n\n        serialized_string, imports = MigrationWriter.serialize(my_function)\n        self.assertEqual(serialized_string, \"migrations.test_writer.my_function\")\n        self.assertEqual(imports, {\"from migrations.test_writer import my_function\"})\n"], "sample_442": ["    def test_compress(self):\n        signer = signing.Signer(key=\"predictable-secret\")\n        data = b\"This is a test string that is longer than 64 characters so it should be compressed.\"\n        compressed_data = signer.sign_object(data, compress=True)\n        self.assertTrue(compressed_data.startswith(\".\"))\n        self.assertEqual(signer.unsign_object(compressed_data), data)\n\n\n", "    def test_cookie_signer(self):\n        cookie_signer = signing.get_cookie_signer()\n        with self.subTest(key=\"predictable-secret\"):\n            signed_cookie = cookie_signer.sign(\"abc\")\n            self.assertIsInstance(signed_cookie, str)\n            self.assertNotEqual(signed_cookie, \"abc\")\n            self.assertEqual(\"abc\", cookie_signer.unsign(signed_cookie))\n\n        with self.subTest(key=\"predictable-secret2\"):\n            cookie_signer = signing.get_cookie_signer(salt=\"another_salt\")\n            signed_cookie = cookie_signer.sign(\"abc\")\n            self.assertIsInstance(signed_cookie, str)\n            self.assertNotEqual(signed_cookie, \"abc\")\n            self.assertEqual(\"abc\", cookie_signer.unsign(signed_cookie))\n", "    def test_compress(self):\n        data = \"This is a test string\" * 1000\n        compressed = zlib.compress(data.encode(\"utf-8\"))\n        signer = signing.Signer(key=\"predictable-secret\")\n\n        # Test compression when it actually saves space\n        compressed_signed = signer.sign_object(data, compress=True)\n        self.assertNotEqual(len(compressed_signed), len(signer.sign_object(data)))\n        self.assertEqual(\n            signer.unsign_object(compressed_signed),\n            data,\n        )\n\n        # Test compression when it doesn't save space\n        data = b\"x\" * 10\n        compressed_signed = signer.sign_object(data, compress=True)\n        self.assertEqual(\n            len(compressed_signed), len(signer.sign_object(data))\n        )  \n        self.assertEqual(\n            signer.unsign_object(compressed_signed),\n            data,\n        )\n", "    def test_compress(self):\n        signer = signing.Signer(key=\"predictable-secret\")\n        data = b\"This is a test string that we are going to compress.\"\n        compressed = zlib.compress(data)\n        with self.subTest(data=data):\n            signed = signer.sign_object(data, compress=True)\n            self.assertTrue(signed.startswith(\".\"))\n            self.assertLess(len(signed), len(signer.sign_object(data)))\n        with self.subTest(data=compressed):\n            signed = signer.sign_object(compressed, compress=True)\n            self.assertTrue(signed.startswith(\".\"))\n            self.assertLess(len(signed), len(signer.sign_object(compressed)))\n", "    def test_compress(self):\n        signer = signing.Signer(key=\"predictable-secret\")\n        data = b\"this is some data\"\n        compressed_data = zlib.compress(data)\n        with self.subTest(data=data):\n            signed_compressed = signer.sign_object(data, compress=True)\n            self.assertTrue(signed_compressed.startswith(\".\"))\n            self.assertEqual(signer.unsign_object(signed_compressed), data)\n        with self.subTest(data=compressed_data):\n            signed_compressed = signer.sign_object(compressed_data, compress=True)\n            self.assertTrue(signed_compressed.startswith(\".\"))\n            self.assertEqual(signer.unsign_object(signed_compressed), compressed_data)\n"], "sample_291": ["    def test_custom_context_object_name(self):\n        test_view = views.CustomMultipleObjectMixinView()\n        test_view.context_object_name = 'my_objects'\n        context = test_view.get_context_data()\n        self.assertEqual(context['my_objects'], test_view.object_list)\n", "    def test_queryset_in_context_data(self):\n        test_view = views.CustomMultipleObjectMixinView()\n        test_view.queryset = [{'name': 'Lennon'}]\n        context = test_view.get_context_data()\n        self.assertEqual(context['object_list'], test_view.queryset)\n\n\n", "    def test_use_queryset_from_kwargs(self):\n        test_view = views.CustomMultipleObjectMixinView()\n        queryset = [{'name': 'Lennon'}, {'name': 'Ono'}]\n        context = test_view.get_context_data(object_list=queryset)\n        self.assertEqual(context['object_list'], queryset)\n", "    def test_custom_context_object_name(self):\n        test_view = views.CustomMultipleObjectMixinView()\n        test_view.context_object_name = 'my_objects'\n        context = test_view.get_context_data()\n        self.assertEqual(context['my_objects'], test_view.object_list)\n\n", "    def test_use_objects_multiple_args(self):\n        test_view = views.CustomMultipleObjectMixinView()\n        test_view.get(self.rf.get('/'))\n        object1 = {'name': 'Lennon'}\n        object2 = {'name': 'Ono'}\n        context = test_view.get_context_data(object1=object1, object2=object2)\n        self.assertEqual(context['object_list'], [object1, object2])\n\n\n\n\n"], "sample_998": ["def test_issue_14639():\n    from sympy.physics.quantum import DensityMatrix, Project, TensorProduct\n\n    rho = DensityMatrix(2)\n\n    proj = Project(1)\n    assert latex(proj) == r\"\\left|{1}\\right\\rangle\\left\\langle{1}\\right|\"\n    assert latex(TensorProduct(proj, rho)) == r\"\\left|{1}\\right\\rangle\\left\\langle{1}\\right| \\otimes \\rho\" \n", "def test_ComplexSymbol_printing():\n    c = Complex('2+3j')\n    assert latex(c) == r'2 + 3 i'\n", "def test_issue_16854():\n    from sympy.matrices import Matrix\n    A, B = MatrixSymbol('A', 2, 2), MatrixSymbol('B', 2, 2)\n    assert latex(A.transpose() * B) == r\"{A}^{T} B\"\n", "def test_trace_printing():\n    A = MatrixSymbol(\"A\", 3, 3)\n    assert latex(Trace(A)) == r\" \\text{tr}\\left(A\\right)\"\n", "def test_TensorProduct_printing_mixed_dimensions():\n    from sympy.tensor.functions import TensorProduct\n    A = MatrixSymbol(\"A\", 2, 3)\n    B = MatrixSymbol(\"B\", 3, 4)\n    assert latex(TensorProduct(A, B)) == r\"A \\otimes B\"\n"], "sample_136": ["    def test_ordering_matters(self):\n        environ = {\n            'HTTP_USER_AGENT': 'python-requests/1.2.0',\n            'CONTENT_TYPE': 'text/html',\n            'CONTENT_LENGTH': '100',\n            'HTTP_HOST': 'example.com',\n        }\n        headers = HttpHeaders(environ)\n        self.assertEqual(sorted(headers), ['Content-Length', 'Content-Type', 'Host', 'User-Agent'])\n        self.assertEqual(headers['User-Agent'], 'python-requests/1.2.0')\n\n\n", "    def test_ordering(self):\n        environ = {\n            'HTTP_X-My-Header': 'value1',\n            'CONTENT_TYPE': 'text/html',\n            'CONTENT_LENGTH': '100',\n            'HTTP_HOST': 'example.com',\n            'HTTP_User-Agent': 'python-requests/1.2.0',\n        }\n        headers = HttpHeaders(environ)\n        self.assertEqual(sorted(headers), [\n            'CONTENT_LENGTH',\n            'CONTENT_TYPE',\n            'HTTP_HOST',\n            'HTTP_User-Agent',\n            'HTTP_X-My-Header',\n        ])\n", "    def test_non_http_headers(self):\n        environ = {\n            'PATH_INFO': '/somepath/',\n            'REQUEST_METHOD': 'get',\n            'wsgi.input': BytesIO(b''),\n            'CONTENT_TYPE': 'text/html',\n            'CONTENT_LENGTH': '100',\n            'HTTP_HOST': 'example.com',\n            # These are non-HTTP headers\n            'MY_HEADER': 'my value',\n        }\n        headers = HttpHeaders(environ)\n        self.assertEqual(sorted(headers), ['Content-Length', 'Content-Type', 'Host', 'MY_HEADER'])\n        self.assertEqual(headers, {\n            'Content-Type': 'text/html',\n            'Content-Length': '100',\n            'Host': 'example.com',\n            'MY_HEADER': 'my value',\n        })\n", "    def test_non_ascii(self):\n        environ = {\n            'CONTENT_TYPE': 'text/html; charset=utf-8',\n            'CONTENT_LENGTH': '100',\n            'HTTP_HOST': 'example.com',\n            'HTTP_USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n        }\n        headers = HttpHeaders(environ)\n        self.assertEqual(sorted(headers), ['Content-Length', 'Content-Type', 'Host', 'User-Agent'])\n        self.assertEqual(headers, {\n            'Content-Type': 'text/html; charset=utf-8',\n            'Content-Length': '100',\n            'Host': 'example.com',\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n        })\n", "    def test_invalid_header_name(self):\n        environ = {\n            'invalid-header': 'value', \n            'CONTENT_TYPE': 'text/html',\n        }\n        with self.assertRaises(ValueError):\n            HttpHeaders(environ)\n"], "sample_1089": ["compilation error", "def test_issue_9257():\n    x, y, z = Symbol(\"x, y, z\")\n    f1 = Function('f1', x, y)\n    f2 = Function('f2', x, y)\n    expr1 = f1(x, y) * f2(y, z) * f1(x, y)\n    expr2 = f1(x, y + z) * f2(y, z)\n    assert factor_nc(expr1) == f1(x, y)**2 * f2(y, z)\n    assert factor(expr2) == f1(x, y + z) * f2(y, z)\n\n\n    expr3 = f1(x, y) * f2(y, z) * (f1(x, y) + f2(y, z))\n    assert factor_nc(expr3) == f1(x, y) * f2(y, z) * (f1(x, y) + f2(y, z))\n    expr4 = f1(x, y) * f2(y, z) * (f1(x, y) + f2(y, z)) * (f1(x, y) + f2(y, z))\n    assert factor_nc(expr4) == f1(x, y) * f2(y, z) * (f1(x, y) + f2(y, z))**2\n\n    expr5 = f1(x, y)*(f2(y, z) + f1(x, y))*(f2(y, z) + f1(x, y))\n    assert factor_nc(expr5) == f1(x, y)*(f2(y, z) + f1(x, y))**2\n\n    expr6 = f1(x, y) * (f2(y, z) + f1", "compilation error", "compilation error", "compilation error"], "sample_683": ["def test_capture_multiple_streams_with_multiple_capturers(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import sys\n        import pytest\n\n        @pytest.fixture\n            yield capture.StdCapture()\n\n        @pytest.fixture\n            yield capture.StdCaptureFD(out=False, err=True)\n\n            print(\"hello\", file=sys.stdout)\n            sys.stderr.write(\"world\\n\")\n            cap1.readouterr()\n            cap2.readouterr()\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        \"\"\"\n        *test_capture_streams*\n    \"\"\"\n    )\n    result.stderr.fnmatch_lines(\n        \"\"\"\n        *hello*\n        *world*\n    \"\"\"\n    )\n", "def test_multicapture_custom_name(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        class MyCapture(pytest.capture.CaptureFixture):\n                super().__init__(*args, **kwargs)\n                self.my_name = \"MyCapture\"\n\n        @pytest.fixture(autouse=True)\n            return MyCapture(request)\n\n            assert my_capture.my_name == \"MyCapture\"\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    assert result.ret == 0\n", "compilation error", "compilation error", "def test_capture_unicode_write(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import sys\n            sys.stdout.write(u\"\u3053\u3093\u306b\u3061\u306f\")\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--capture=sys\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*\u3053\u3093\u306b\u3061\u306f*\",\n        ]\n    )\n\n\n"], "sample_524": ["def test_colorbar_fraction():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.random.randn(10, 10))\n    cb = fig.colorbar(pc, ax=ax, fraction=0.15)\n    fig.draw_without_rendering()\n    assert cb.ax.get_position().width * fig.get_size_inches()[0] \\\n           > 0.15 - 1e-2 and cb.ax.get_position().width * fig.get_size_inches()[0] \\\n           < 0.15 + 1e-2\n\n\n", "    def test_colorbar_no_label():\n        fig, ax = plt.subplots()\n        pc = ax.pcolormesh(np.random.randn(10, 10))\n        cb = fig.colorbar(pc)\n        cb.set_label('')\n\n        assert cb.ax.get_ylabel() == ''\n", "def test_colorbar_label_rotation():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.random.randn(10, 10))\n    cb = fig.colorbar(pc, ax=ax)\n    cb.set_label('My long colorbar label')\n    bbox = cb.ax.get_position()\n    \n\n    # Test rotation with default rotation (0)\n    cb.ax.set_xticklabels(rotation=0)\n    plt.show()\n    assert cb.ax.get_xticklabels()[0].get_rotation() == 0\n\n    # Test rotation with a larger value\n    cb.ax.set_xticklabels(rotation=45)\n    plt.show()\n    assert cb.ax.get_xticklabels()[0].get_rotation() == 45\n\n    # Test rotation with negative value\n    cb.ax.set_xticklabels(rotation=-45)\n    plt.show()\n    assert cb.ax.get_xticklabels()[0].get_rotation() == -45\n\n\n\n", "def test_colorbar_with_ticks_and_label():\n    fig, ax = plt.subplots()\n    im = ax.imshow([[1, 2, 3], [4, 5, 6]])\n    cb = fig.colorbar(im, ax=ax)\n    ticks = [1, 3, 5]\n    labels = ['label1', 'label2', 'label3']\n    cb.set_ticks(ticks)\n    cb.set_ticklabels(labels)\n    cb.ax.set_ylabel('My Ylabel')\n    assert np.allclose(cb.ax.get_yticks(), ticks)\n    assert np.array(cb.ax.get_yticklabels()) == np.array(labels)\n    assert cb.ax.get_ylabel() == 'My Ylabel'\n", "compilation error"], "sample_1120": ["compilation error", "compilation error", "def test_matrix_equality():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    assert A == A\n    assert A != B\n    assert A != 1\n    assert B != 1\n    assert A != zeros(2)\n    assert B != zeros(2)\n    assert (A + 1) == (A + 1)\n    assert (A + 1) != (A + 2)\n    assert (A * 2) == (A * 2)\n    assert (A * 2) != (A * 3)\n", "compilation error", "def test_matrix_symbols_with_indices():\n    A = MatrixSymbol('A', 2, 3)\n    assert A[1, 2] == A[1, 2]\n    assert A[1, 1] != A[1, 2]\n    raises(ValueError, lambda: A[3, 2])\n    raises(ValueError, lambda: A[1, 3])\n    B = MatrixSymbol('B', 3, 2)\n    assert B[2, 0] == B[2, 0]\n    assert B[0, 1] != B[2, 0]\n    raises(ValueError, lambda: B[3, 0])\n    raises(ValueError, lambda: B[0, 2])\n"], "sample_1185": ["def test_compogen():\n    assert compogen([sin(x), cos(x)], x) == sin(cos(x))\n    assert compogen([x**2 + x + 1, sin(x)], x) == sin(x)**2 + sin(x) + 1\n    assert compogen([sqrt(x), 6*x**2 - 5], x) == sqrt(6*x**2 - 5)\n    assert compogen([sin(x), sqrt(x), cos(x), x**2 + 1], x) == sin(sqrt(cos(x**2 + 1)))\n    assert compogen([Abs(x), x**2 + 1], x) == Abs(x**2 + 1)\n    assert compogen([x + 1, 2*x - 3], x) == 2*x + (-1) \n", "compilation error", "compilation error", "def test_compogen():\n    assert compogen([sin(x), cos(x)], x) == sin(cos(x))\n    assert compogen([x**2 + x + 1, sin(x)], x) == sin(x)**2 + sin(x) + 1\n    assert compogen([sqrt(x), 6*x**2 - 5], x) == sqrt(6*x**2 - 5)\n    assert compogen([sin(x), sqrt(x), cos(x), x**2 + 1], x) == sin(sqrt(cos(x**2 + 1)))\n    assert compogen([Abs(x), x**2 + 3*x - 4, cos(x)], x) == Abs(cos(x)**2 + 3*cos(x) - 4)\n    assert compogen([x**2 + x - sqrt(3)/2, sin(x)], x) == sin(x)**2 + sin(x) - sqrt(3)/2\n    assert compogen([Abs(x), 3*x + cos(y)**2 - 4, cos(x)], x) == Abs(3*x + cos(y)**2 - 4)\n", "def test_compogen():\n    assert compogen([sin(x), cos(x)], x) == sin(cos(x))\n    assert compogen([x**2 + x + 1, sin(x)], x) == sin(x)**2 + sin(x) + 1\n    assert compogen([sqrt(x), 6*x**2 - 5], x) == sqrt(6*x**2 - 5)\n    assert compogen([sin(x), sqrt(x), cos(x), x**2 + 1], x) == sin(sqrt(cos(x**2 + 1)))\n    assert compogen([Abs(x), x**2 + 3*x - 4, cos(x)], x) == Abs(cos(x)**2 + 3*cos(x) - 4)\n    assert compogen([x**2 + x - sqrt(3)/2, sin(x)], x) == sin(x)**2 + sin(x) - sqrt(3)/2\n    assert compogen([Abs(x), 3*x + cos(y)**2 - 4, cos(x)], x) == Abs(3*x + cos(y)**2 - 4)\n\n\n\n"], "sample_406": ["    def test_refresh_without_pk(self):\n        a = Article.objects.create(headline=\"Parrot programs in Python\", pub_date=datetime.now())\n        Article.objects.filter(headline=\"Parrot programs in Python\").delete()\n        with self.assertRaises(Exception):\n            a = Article.objects.create(headline=\"Foo\")\n            a.refresh_from_db()\n", "    def test_refresh_from_db_no_query_if_not_modified(self):\n        a = Article.objects.create(headline=\"Foo\", pub_date=datetime.now())\n        with self.assertNumQueries(0):\n            a.refresh_from_db()\n", "    def test_refresh_without_fields(self):\n        a = Article.objects.create(headline=\"Foo\", pub_date=datetime.now())\n        with self.assertNumQueries(1):\n            a.refresh_from_db()\n        with self.assertNumQueries(0):\n            a.refresh_from_db(fields=None)  \n", "    def test_refresh_cascading_deletion(self):\n        a = Article.objects.create(headline=\"Parrot programs in Python\", pub_date=datetime(2005, 7, 28))\n        s1 = SelfRef.objects.create(article=a)\n        s1.delete()\n        a.refresh_from_db()\n        self.assertIsNone(a.selfref)\n", "    def test_refresh_ignores_nonexistent_fields(self):\n        a = Article.objects.create(headline=\"Foo\")\n        with self.assertNumQueries(0):\n            a.refresh_from_db(fields=[\"bogus_field\"])\n        self.fields = a._meta.get_fields() if a else []\n        self.assertNotIn(\"bogus_field\", self.fields)\n\n"], "sample_460": ["    def test_missing_slash_append_slash_true_script_name_without_final_catch_all_view(self):\n        superuser = User.objects.create_user(\n            username=\"staff\",\n            password=\"secret\",\n            email=\"staff@example.com\",\n            is_staff=True,\n        )\n        self.client.force_login(superuser)\n        known_url = reverse(\"admin10:admin_views_article_changelist\")\n        response = self.client.get(known_url[:-1], SCRIPT_NAME=\"/prefix/\")\n        self.assertRedirects(\n            response,\n            \"/prefix\" + known_url,\n            status_code=301,\n            fetch_redirect_response=False,\n        ) \n", "    def test_missing_slash_append_slash_true_script_name_without_final_catch_all_view(self):\n        superuser = User.objects.create_user(\n            username=\"staff\",\n            password=\"secret\",\n            email=\"staff@example.com\",\n            is_staff=True,\n        )\n        self.client.force_login(superuser)\n        known_url = reverse(\"admin10:admin_views_article_changelist\")\n        response = self.client.get(known_url[:-1], SCRIPT_NAME=\"/prefix/\")\n        self.assertRedirects(\n            response,\n            \"/prefix\" + known_url,\n            status_code=301,\n            fetch_redirect_response=False,\n        )\n\n", "    def test_unknown_url_404_if_authenticated_without_final_catch_all_view(self):\n        superuser = User.objects.create_superuser(\n            username=\"super\",\n            password=\"secret\",\n            email=\"super@example.com\",\n        )\n        self.client.force_login(superuser)\n        unknown_url = \"/test_admin/admin10/unknown/\"\n        response = self.client.get(unknown_url)\n        self.assertEqual(response.status_code, 404)\n", "    def test_missing_slash_append_slash_true_script_name_without_final_catch_all_view(self):\n        superuser = User.objects.create_user(\n            username=\"staff\",\n            password=\"secret\",\n            email=\"staff@example.com\",\n            is_staff=True,\n        )\n        self.client.force_login(superuser)\n        known_url = reverse(\"admin10:admin_views_article_changelist\")\n        response = self.client.get(known_url[:-1], SCRIPT_NAME=\"/prefix/\")\n        self.assertRedirects(\n            response,\n            \"/prefix\" + known_url,\n            status_code=301,\n            fetch_redirect_response=False,\n        )\n\n\n\n\n", "    def test_final_catch_all_view_disabled(self):\n        class CustomAdminSite(AdminSite):\n            final_catch_all_view = None\n\n        admin_site = CustomAdminSite(name=\"myadmin\")\n        from django.contrib.admin import site  \n        site.unregister(app)\n        admin_site.register(Article)\n        request = HttpRequest()\n        request.path = \"/test_admin/admin10/unknown/\"\n        response = admin_site.dispatch(request)\n        self.assertEqual(response.status_code, 404)\n"], "sample_871": ["def test_calinski_harabasz_score_equal_clusters():\n    \"\"\"\n    Test for Calinski-Harabasz score when clusters are exactly equal.\n    \"\"\"\n    X = np.array([[0, 0], [1, 1]] * 10)\n    labels = [0] * 10\n    score = calinski_harabasz_score(X, labels)\n    assert score == pytest.approx(0.0)\n\n", "def test_handle_sparse_input():\n    for sparse_type in (csr_matrix, csc_matrix, dok_matrix, lil_matrix):\n        X_sparse = sparse_type(\n            np.array([[0.2, 0.1, 0.1, 0.2, 0.1, 1.6, 0.2, 0.1]], dtype=np.float32).T\n        )\n        X_dense = X_sparse.toarray()\n        y = [0, 0, 0, 0, 1, 1, 1, 1]\n\n        assert silhouette_score(X_sparse, y, metric=\"precomputed\") == silhouette_score(\n            X_dense, y\n        )\n        assert silhouette_samples(X_sparse, y, metric=\"precomputed\") == silhouette_samples(\n            X_dense, y\n        )\n", "compilation error", "compilation error", "compilation error"], "sample_1013": ["def test_issue_15567():\n    # Test for issue 15567 where lambdify with a custom symbol printer\n    # could cause issues.\n    class CustomSymbolPrinter(sympy.printing.HTMLPrinter):\n            return str(expr) + \"!\"\n\n    f = lambdify(x, x, printer=CustomSymbolPrinter())\n    assert f(1) == \"1!\"\n", "def test_issue_15297():\n    # Issue #15297 - Error when using lambdify with 'sympy.integrate'\n    from sympy import integrate\n    x = symbols('x')\n    expr = integrate(x**2, (x, 0, 1))\n    func = lambdify(x, expr)\n    assert func(1) == 1/3\n", "def test_lambdify_with_non_numeric_dtype():\n    if not numpy:\n        skip(\"numpy not installed\")\n    x = symbols('x')\n    f = lambdify(x, x, dtypes=[np.dtype(\"|U10\")])\n    assert f(np.array([\"hello\"])[0]) == \"hello\"\n", "def test_issue_14788():\n    if not mpmath:\n        skip(\"mpmath not installed\")\n\n    f = lambdify(x, I0(x), \"mpmath\")\n    assert Abs(f(1.0) - I0(1.0).evalf()).n() < 1e-15\n", "def test_lambdify_with_string_arguments():\n    x = symbols('x')\n    f = lambdify((x, 'y'), x + 'y')\n    assert f(1, 2) == '12'\n\n\n\n"], "sample_513": ["def test_multiple_lines_labels_with_empty_strings():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='')\n    ax.plot([2, 3], [4, 5], label='')\n    leg = ax.legend()\n    assert len(leg.get_texts()) == 2\n    for text in leg.get_texts():\n        assert text.get_text() == ''\n\n\n\n", "def test_legend_handles_order():\n    fig, ax = plt.subplots()\n    line1, = ax.plot([1, 2, 3], [4, 5, 6], label='line1')\n    line2, = ax.plot([4, 3, 2], [5, 6, 7], label='line2')\n    legend = ax.legend()\n    assert legend.get_lines()[0] is line1\n    assert legend.get_lines()[1] is line2\n", "def test_legend_bbox_to_anchor_negative():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4])\n    leg = ax.legend(bbox_to_anchor=(-0.1, 0.1))\n    assert leg.get_bbox_patch().get_box().get_x() < 0\n", "def test_legend_with_disabled_patches_still_includes_text():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1])\n    legend = ax.legend()\n    for patch in legend.get_patches():\n        patch.set_visible(False)\n    assert len(legend.get_texts()) == 1\n\n\n\n\n", "def test_legend_handles_with_patches():\n    fig, ax = plt.subplots()\n    patches = [plt.Rectangle((0, 0), 1, 1, color='red'),\n               plt.Circle((0.5, 0.5), 0.2, color='green')]\n    ax.add_patch(patches[0])\n    ax.add_patch(patches[1])\n    legend = ax.legend([patches[0], patches[1]], ['Patch 1', 'Patch 2'])\n    assert len(legend.legendHandles) == 2\n    assert legend.legendHandles[0].is_patch()\n    assert legend.legendHandles[1].is_patch()\n"], "sample_68": ["    def test_cleanse_setting_nested_dicts(self):\n        test_dict = {\n            'TEST': 'normal_value',\n            'PASSWORD': 'super_secret',\n            'nested': {\n                'another_password': 'top_secret'\n            }\n        }\n        cleaned_dict = cleanse_setting(test_dict, 'TEST')\n        self.assertEqual(cleaned_dict['TEST'], 'normal_value')\n        self.assertEqual(cleaned_dict['PASSWORD'], CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleaned_dict['nested'], {\n            'another_password': CLEANSED_SUBSTITUTE\n        }) \n", "    def test_cleanse_setting_nested_dict(self):\n        data = {'MY_SETTINGS': {'PASSWORD': 'foobar'}}\n        result = cleanse_setting_in_dict(data, 'MY_SETTINGS', 'PASSWORD')\n        self.assertEqual(result['MY_SETTINGS'], {'PASSWORD': CLEANSED_SUBSTITUTE})\n", "    def test_cleanse_setting_recursive(self):\n        data = {\n            'test': 'ok',\n            'secret': 'super_secret',\n            'nested': {\n                'password': 'dont_show_me',\n                'another_test': 'ok',\n            }\n        }\n        cleaned_data = cleanse_setting(data, 'secret')\n        self.assertEqual(cleaned_data['test'], 'ok')\n        self.assertEqual(cleaned_data['secret'], CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleaned_data['nested']['password'], CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleaned_data['nested']['another_test'], 'ok')\n", "    def test_cleanse_setting_nested_dicts(self):\n        data = {\n            'foo': 'bar',\n            'sensitive': 'secret',\n            'nested': {'password': 'super_secret'}\n        }\n\n        cleaned_data = cleanse_settings(data)\n        self.assertEqual(cleaned_data['foo'], 'bar')\n        self.assertEqual(cleaned_data['sensitive'], CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleaned_data['nested'], {'password': CLEANSED_SUBSTITUTE})\n", "    def test_cleanse_setting_nested_dictionaries(self):\n        data = {'key1': 'value1', 'key2': 'secret_value2', 'key3': {'nested_key': 'very_sensitive'}}\n        cleaned_data = cleanse_setting('data', data)\n        self.assertEqual(cleaned_data['key1'], 'value1')\n        self.assertEqual(cleaned_data['key2'], CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleaned_data['key3'], {'nested_key': CLEANSED_SUBSTITUTE})\n"], "sample_116": ["    def test_different_caches(self):\n        \"\"\"\n        Requesting different cache aliases should yield different instances.\n        \"\"\"\n        cache1 = caches['default']\n        cache2 = caches['other']\n\n        self.assertIsNot(cache1, cache2)\n", "def test_get_server_side_cache_control_headers(self):\n    response = HttpResponse(\"Hello World\")\n    response['Cache-Control'] = 'max-age=3600, must-revalidate'\n    response['Vary'] = 'cookie'\n\n    expected_headers = {\n        'Cache-Control': 'max-age=3600, must-revalidate',\n        'Vary': 'cookie'\n    }\n    self.assertDictEqual(get_server_side_cache_control_headers(response), expected_headers)\n\n\n", "    def test_get_cache_key_with_fragment_and_vary(self):\n        request = self.factory.get(self.path, {'test': 1})\n        template = engines['django'].from_string(\"This is a test\")\n        response = TemplateResponse(HttpRequest(), template, {'fragment': 'foo'})\n        # Expect None if no headers have been set yet.\n        self.assertIsNone(get_cache_key(request))\n        # Set headers to an empty list.\n        learn_cache_key(request, response, vary_headers=['Accept-Encoding'])\n        # The querystring and fragment are taken into account.\n        self.assertEqual(\n            get_cache_key(request),\n            'template.cache.fragment.foo.view.GET.'\n            '58a0a05c8a5620f813686ff969c26853.d41d8cd98f00b204e9800998ecf8427e'\n        )\n", "    def test_cache_key_with_path_information(self):\n        request = self.factory.get(f'{self.path}/subpath/')\n        template = engines['django'].from_string(\"This is a test\")\n        response = TemplateResponse(HttpRequest(), template)\n        # Expect None if no headers have been set yet.\n        self.assertIsNone(get_cache_key(request))\n        # Set headers to an empty list.\n        learn_cache_key(request, response)\n        # The path is taken into account\n        self.assertEqual(\n            get_cache_key(request),\n            'views.decorators.cache.cache_page.settingsprefix.GET.'\n            '1d338919b10346075280514ee2a9d991.d41d8cd98f00b204e9800998ecf8427e'\n        )\n", "    def test_cache_with_template_fragments(self):\n        request = self.factory.get(self.path)\n        template = engines['django'].from_string(\n            \"\"\"\n            {% load cache %}\n            {% cache 'fragment_key' %}\n            <h1>Fragment content</h1>\n            {% endcache %}\n            \"\"\"\n        )\n        response = TemplateResponse(request, template)\n        response.render()\n        key =  make_template_fragment_key('fragment_key')\n        self.assertIn(key, cache.get_all().keys())\n\n\n\n"], "sample_1060": ["def test_indexed_tensor():\n    x = IndexedBase('x')\n    expr = x[0, 1] + x[1, 0]\n    p = PythonCodePrinter()\n    assert p.doprint(expr) == 'x[0, 1] + x[1, 0]'\n", "def test_tensor_printing():\n    x = IndexedBase('x')\n    expr = x[0, :]\n    assert pycode(expr) == 'x[0, :]'\n    assert pycode(x[0, 1:3]) == 'x[0, 1:3]'\n    expr = sum(x[i, j] for i in range(3) for j in range(3))\n    assert pycode(expr) == 'sum([x[i, j] for i in range(3) for j in range(3)])'\n", "compilation error", "def test_printing_indexed_objects():\n    x = symbols('x')\n    a = IndexedBase('a')\n    b = IndexedBase('b')\n    expr1 = a[x]\n    expr2 = a[1] + b[x]\n    expr3 = a[x, y]\n    expr4 = a[x, y] + a[y, x]\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr1) == 'a[x]'\n    assert prntr.doprint(expr2) == 'a[1] + b[x]'\n    assert prntr.doprint(expr3) == 'a[x, y]'\n    assert prntr.doprint(expr4) == 'a[x, y] + a[y, x]'\n", "def test_indexed_arrays():\n    p = NumPyPrinter()\n    idx = IndexedBase('x')\n    expr = idx[0]\n    assert p.doprint(expr) == 'x[0]'\n    expr = idx[1:3]\n    assert p.doprint(expr) == 'x[1:3]'\n"], "sample_380": ["    def test_exists_with_filter_and_aggregate(self):\n        exists_qs = Author.objects.filter(age=100)\n        result = Book.objects.annotate(\n            exists=Exists(exists_qs),\n            count=Count('authors'),\n        ).values('exists')\n        self.assertSequenceEqual(result, [\n            {'exists': True},\n            {'exists': True},\n            {'exists': True},\n            {'exists': True},\n            {'exists': True},\n            {'exists': True},\n        ])\n\n", "    def test_aggregation_expression_with_subquery(self):\n        subquery = (\n            Subquery(Author.objects.filter(age__gt=50).values('id'))\n            .filter(id__in=Author.objects.filter(age__gt=50).values('id'))\n            .values('id')\n        )\n        result = Author.objects.annotate(\n            count=Count('book'),\n            subquery_count=Count(subquery),\n        ).values('id', 'count', 'subquery_count')\n        self.assertSequenceEqual(list(result), [\n            {'id': 1, 'count': 1, 'subquery_count': 1},\n            {'id': 2, 'count': 1, 'subquery_count': 1},\n            {'id': 3, 'count': 1, 'subquery_count': 0},\n            {'id': 4, 'count': 1, 'subquery_count': 0},\n            {'id': 5, 'count': 1, 'subquery_count': 0},\n            {'id': 6, 'count': 1, 'subquery_count': 0},\n        ])\n", "    def test_aggregation_default_custom_expression(self):\n        from django.db.models import ExpressionWrapper, F, Value\n            return F('pages') * Value(0.1)\n        result = Book.objects.filter(\n            rating__gt=3,\n        ).annotate(\n            value=Sum('price', default=my_default(F('price'))),\n        ).all()\n        self.assertSequenceEqual(\n            result.values_list('value'),\n            [Decimal('4.40'), Decimal('12.63'), Decimal('11.76'), Decimal('22.02'), Decimal('4.40'), Decimal('13.86')],\n        )\n", "    def test_exists_with_subquery_filter_and_aggregate(self):\n        authors = Author.objects.filter(age__gt=50)\n        qs = Book.objects.annotate(\n            count=Count('id'),\n            exists=Exists(authors.filter(id=OuterRef('author__id'))),\n        ).values('isbn', 'exists', 'count')\n        self.assertSequenceEqual(\n            qs,\n            [\n                {'isbn': '013235613', 'exists': True, 'count': 1},\n                {'isbn': '013790395', 'exists': True, 'count': 1},\n                {'isbn': '067232959', 'exists': False, 'count': 1},\n                {'isbn': '155860191', 'exists': True, 'count': 1},\n                {'isbn': '159059725', 'exists': True, 'count': 1},\n                {'isbn': '159059996', 'exists': True, 'count': 1},\n            ],\n        )\n\n", "    def test_aggregation_with_null_values(self):\n        with self.assertNumQueries(1):\n            # Ensure nullable fields are correctly aggregated with defaults.\n            result = Publisher.objects.annotate(\n                avg_rating=Avg('book__rating', default=None),\n            ).filter(avg_rating__isnull=False).values_list('name', 'avg_rating')\n\n            self.assertSequenceEqual(result, [\n                ('Apress', Decimal('4.25')),\n                ('Morgan Kaufmann', Decimal('4.50')),\n                ('Prentice Hall', Decimal('3.50')),\n                ('Sams', Decimal('3.00')),\n            ])\n        with self.assertNumQueries(1):\n            result = Publisher.objects.annotate(\n                avg_rating=Avg('book__rating', default=Decimal('100.00')),\n            ).filter(avg_rating__lt=Decimal('100.00')).values_list('name', 'avg_rating')\n            self.assertSequenceEqual(result, [\n                ('Apress', Decimal('4.25')),\n                ('Morgan Kaufmann', Decimal('4.50')),\n                ('Prentice Hall', Decimal('3.50')),\n                ('Sams', Decimal('3.00')),\n            ])\n\n\n\n"], "sample_21": ["def test_read_write_error_columns(tmp_path):\n    test_file = tmp_path / \"test.qdp\"\n    t1 = Table()\n    t1.add_column(Column(name=\"a\", data=[1, 2, 3]))\n    t1.add_column(MaskedColumn(data=[np.nan, 4.0, 5.0], name=\"b\", mask=[True, True, False]))\n    t1.add_column(Column(name=\"c\", data=[6, 7, 8]))\n    t1.write(test_file, format=\"ascii.qdp\", err_specs={'serr':[1]})\n    t2 = Table.read(test_file, table_id=0, format=\"ascii.qdp\", names=[\"a\", \"b\", \"c\"])\n\n    assert np.allclose(t2[\"a\"], t1[\"a\"])\n    assert np.allclose(t2[\"b\"], t1[\"b\"])\n    assert np.allclose(t2[\"c\"], t1[\"c\"])  \n\n\n\n", "def test_read_write_with_err_specs(tmp_path):\n    test_file = tmp_path / \"test.qdp\"\n    t1 = Table()\n    t1.add_column(Column(name='a', data=[1, 2, 3, 4]))\n    t1.add_column(MaskedColumn(data=[np.nan, 4.0, 3.0, 1.0], name='b', mask=[True, False, False, True]))\n    t1.write(test_file, format=\"ascii.qdp\", err_specs={'terr': [1], 'serr': [3]})\n    t2 = Table.read(test_file, format=\"ascii.qdp\", table_id=0, names=['a', 'b'], err_specs={'terr': [1], 'serr': [3]})\n    assert np.allclose(t2[\"a\"], t1[\"a\"])\n    assert np.allclose(t2[\"b_err\"], [np.nan, 0.0, 0.0, 0.0])\n", "def test_read_multiple_tables(tmp_path):\n    example_qdp = \"\"\"\n    ! Table 0\n    !a a(pos) a(neg) b\n    53000.5   0.25  -0.5   1\n    54000.5   1.25  -1.5   2\n    NO NO NO NO\n    ! Table 1\n    !a a(pos) a(neg) b \n    54000.5   2.25  -2.5   NO\n    55000.5   3.25  -3.5   4\n    NO NO NO NO\n    \"\"\"\n    path = tmp_path / \"test.qdp\"\n    with open(path, \"w\") as fp:\n        print(example_qdp, file=fp)\n\n    tables = ascii.read(path, format=\"ascii.qdp\")\n    assert len(tables) == 2\n    assert np.allclose(tables[0][\"a\"], [53000.5, 54000.5])\n    assert np.allclose(tables[1][\"a\"], [54000.5, 55000.5])\n", "def test_read_write_multiple_tables(tmp_path):\n    example_qdp = \"\"\"\n        ! First table\n        READ TERR 1\n        READ SERR 2\n        ! a b c\n        1 2 3\n        4 5 6\n        NO NO NO\n        ! Second table\n        ! a b c\n        7 8 9\n        10 11 12\n        NO NO NO\n        \"\"\"\n    test_file = tmp_path / \"test.qdp\"\n\n    with open(test_file, \"w\") as f:\n        print(example_qdp, file=f)\n    t1 = Table.read(test_file, format=\"ascii.qdp\", table_id=0)\n    t2 = Table.read(test_file, format=\"ascii.qdp\", table_id=1)\n    assert np.allclose(t1[\"a\"], [1, 4])\n    assert np.allclose(t2[\"a\"], [7, 10])\n\n\n", "compilation error"], "sample_376": ["    def test_not_finished(self):\n        storage = self.get_storage()\n        response = self.get_response()\n        messages = [Message(constants.INFO, 'test') for _ in range(10)]\n        storage.update(response, messages)\n        data = storage._decode(response.cookies['messages'].value)\n\n        # Check that the cookie contains messages and that the not_finished flag is not present.\n        self.assertEqual(len(data), 10)\n        self.assertNotIn(CookieStorage.not_finished, data)\n\n        # Remove a message and update the cookie again\n        storage.remove(data[0])\n        storage.update(response)\n        data = storage._decode(response.cookies['messages'].value)\n\n        # Check that the cookie now contains 9 messages and that the not_finished flag is present\n        self.assertEqual(len(data), 9)\n        self.assertIn(CookieStorage.not_finished, data)\n", "    def test_not_finished_sentinel(self):\n        storage = self.get_storage()\n        messages = [Message(constants.INFO, 'First'), Message(constants.INFO, 'Second')]\n        set_cookie_data(storage, messages[:1], encode_empty=True)\n        remaining_messages = list(storage)\n        self.assertEqual(len(remaining_messages), 1)\n        self.assertEqual(remaining_messages[0].message, 'Second')\n        self.assertEqual(stored_cookie_messages_count(storage, self.get_response()), 1)\n\n\n", "    def test_not_finished_sentinel(self):\n        storage = self.get_storage()\n        response = self.get_response()\n        messages = ['message 1', 'message 2', 'message 3']\n        storage._store(messages, response, remove_oldest=False)\n        storage._store(messages[1:], response, remove_oldest=False)\n        self.assertEqual(stored_cookie_messages_count(storage, response), 4)\n        self.assertEqual(list(storage), messages[:3])\n", "    def test_not_finished(self):\n        storage = self.get_storage()\n        response = self.get_response()\n\n        messages = [get_random_string(10) for _ in range(5)]\n        storage._store(messages, response, remove_oldest=False)\n        messages.append(CookieStorage.not_finished)\n        self.assertEqual(stored_cookie_messages_count(storage, response), 4)\n\n        # Now, retrieve the messages and ensure that the `not_finished` sentinel\n        # is removed and the next part of the data is available.\n        storage._get(response)\n        storage._store(messages[4:], response, remove_oldest=False)\n        self.assertEqual(stored_cookie_messages_count(storage, response), 4)\n", "    def test_not_finished(self):\n        storage = self.get_storage()\n        response = self.get_response()\n        messages = ['test'] * 15\n\n        # Fill the cookie with messages\n        set_cookie_data(storage, messages, encode_empty=True)\n\n        # Retrieve messages \n        messages, retrieved_all = storage._get()\n        self.assertEqual(len(messages), 14)\n        self.assertFalse(retrieved_all)\n        assert messages[-1] == storage.not_finished\n\n        # Retrieve again, should get all the remaining messages\n        messages, retrieved_all = storage._get()\n        self.assertEqual(len(messages), 1)\n        self.assertTrue(retrieved_all)\n        self.assertEqual(messages[0], 'test')\n\n        # Now the cookie should be empty\n        self.assertEqual(self.stored_messages_count(storage, response), 0)\n"], "sample_199": ["    def test_annotation_with_raw_query_against_related_model(self):\n        related_data = [\n            {'book_id': 1, 'created_at': datetime.date(2023, 10, 26), 'author_id': 1},\n            {'book_id': 1, 'created_at': datetime.date(2023, 10, 27), 'author_id': 2},\n            {'book_id': 2, 'created_at': datetime.date(2023, 10, 28), 'author_id': 1},\n        ]\n        BookRelation.objects.bulk_create([\n            BookRelation(book_id=book['book_id'], created_at=book['created_at'], author_id=book['author_id'])\n            for book in related_data\n        ])\n\n        books = Book.objects.annotate(\n            related_count=RawSQL('SELECT COUNT(*) FROM book_relations WHERE book_relations.book_id = books.id', [])\n        ).all()\n\n", "    def test_annotation_filter_with_subquery_on_m2m(self):\n        long_publisher_books_qs = Book.objects.filter(\n            publisher__name='Morgan Kaufmann',\n            pages__gt=400\n        ).values(\n            'publisher__name',\n            'publisher__id'\n        )\n        book_ids = Book.objects.filter(\n            publisher__name='Morgan Kaufmann',\n            pages__gt=400\n        ).values_list('pk', flat=True)\n        publisher_books_qs = Publisher.objects.filter(\n            books__id__in=book_ids,\n        ).annotate(\n            long_book_count=Count(books),\n        ).values('name', 'long_book_count')\n        self.assertCountEqual(\n            publisher_books_qs,\n            [{'name': 'Morgan Kaufmann', 'long_book_count': 2}],\n        )\n\n\n\n", "    def test_annotation_complex_ordering(self):\n        books = Book.objects.annotate(\n            rating_age=F('rating') * F('publisher__num_awards')\n        ).order_by('rating_age', 'publisher__name')\n        self.assertQuerysetEqual(\n            books, [\n                ('Practical Django Projects', 15),\n                ('Python Web Development with Django', 27),\n                ('Artificial Intelligence: A Modern Approach', 9),\n                ('Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp', 18),\n            ],\n            lambda b: (b.title, b.rating_age)\n        )\n\n\n\n", "    def test_nested_annotation_with_subquery(self):\n        long_books_qs = Book.objects.filter(\n            publisher=OuterRef('pk'),\n            pages__gt=400,\n        ).values('publisher').annotate(count=Count('pk')).values('count')\n\n        publisher_books_qs = Publisher.objects.annotate(\n            total_long_books=Subquery(long_books_qs, output_field=IntegerField()),\n            avg_long_book_count=Avg('total_long_books')\n        ).values('name', 'avg_long_book_count')\n        self.assertCountEqual(\n            publisher_books_qs, [{'name': 'Sams', 'avg_long_book_count': 0.5},\n            {'name': 'Morgan Kaufmann', 'avg_long_book_count': 0.6666666666666666}],\n        ) \n", "    def test_complex_annotate_ordering(self):\n        qs = (\n            DepartmentStore.objects\n            .annotate(\n                total_items=Count('books') + Count('employees'),\n                average_rating=Avg('books__rating'),\n            )\n            .order_by('-total_items', 'average_rating')\n        )\n        self.assertCountEqual(\n            qs,\n            [\n                # Angus & Robinson should be first due to most books and employees\n                {'total_items': 5, 'average_rating': 4.0},\n                {'total_items': 3, 'average_rating': 4.0},\n                {'total_items': 2, 'average_rating': 4.0},\n            ],\n        )\n"], "sample_973": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_534": ["def test_contour_negative_levels():\n    x = np.arange(-5, 5, 0.25)\n    y = np.arange(-5, 5, 0.25)\n    X, Y = np.meshgrid(x, y)\n    Z = np.sin(np.sqrt(X**2 + Y**2))\n    fig, ax = plt.subplots()\n    CS = ax.contour(X, Y, Z, levels=[-1.2, -0.8, -0.4, 0, 0.4, 0.8, 1.2],\n                    linestyles=['--', '-', '--', '-', '--', '-', '--'])\n    ax.clabel(CS, inline=True, fontsize=9)\n", "def test_contour_alpha():\n    fig, ax = plt.subplots()\n    x = np.arange(10)\n    y = np.arange(10)\n    X, Y = np.meshgrid(x, y)\n    Z = np.sin(X * Y)\n    cs = ax.contour(X, Y, Z, alpha=0.5)\n    assert cs.get_alpha() == 0.5\n", "def test_contour_positive_negative_colors():\n    delta = 0.025\n    x = np.arange(-3.0, 3.0, delta)\n    y = np.arange(-2.0, 2.0, delta)\n    X, Y = np.meshgrid(x, y)\n    Z1 = np.exp(-X**2 - Y**2)\n    Z2 = np.exp(-(X - 1)**2 - (Y - 1)**2)\n    Z = (Z1 - Z2) * 2\n\n    fig, ax = plt.subplots()\n    CS = ax.contour(X, Y, Z, 6,\n                    colors=['red', 'blue'],\n                    positive_colors=['green', 'purple'],\n                    negative_colors=['cyan', 'magenta'])\n    ax.clabel(CS, fontsize=9, inline=True)\n\n", "    def animate_contour(i):\n        Z = np.sin(np.sqrt(X**2 + Y**2) + i/100)\n        cs.set_array(Z.ravel())\n        return cs,\n", "def test_contour_with_invalid_levels():\n    x = np.arange(10)\n    y = np.arange(10)\n    X, Y = np.meshgrid(x, y)\n    Z = np.sin(X * Y)\n\n    with pytest.raises(ValueError,\n                       match=\"Levels must be a sequence of numbers\"):\n        plt.contour(X, Y, Z, levels=\"invalid\")\n    with pytest.raises(ValueError,\n                       match=\"Levels must be a sequence of numbers\"):\n        plt.contourf(X, Y, Z, levels=\"invalid\")\n\n"], "sample_166": ["    def test_get_random_string_length(self):\n        self.assertEqual(len(get_random_string(length=20)), 20)\n", "    def test_get_random_string_length(self):\n        self.assertEqual(len(get_random_string(16)), 16)\n        self.assertEqual(len(get_random_string(24)), 24)\n        self.assertEqual(len(get_random_string(length=100)), 100)\n", "    def test_get_random_string_length(self):\n        for length in range(1, 256):\n            random_string = get_random_string(length)\n            self.assertEqual(len(random_string), length)\n", "    def test_get_random_string_length(self):\n        self.assertEqual(len(get_random_string(length=20)), 20)\n        self.assertEqual(len(get_random_string(length=50)), 50)\n", "    def test_get_random_string_length_argument(self):\n        with self.subTest(length=16):\n            self.assertEqual(len(get_random_string(length=16)), 16)\n"], "sample_310": ["    def test_field_descriptions(self):\n        class Model(models.Model):\n            custom_field = CustomField(description=\"A custom field's description\")\n            \n        with self.settings(INSTALLED_APPS={'INSTALLED_APPS': [\n            'admin_docs_test'\n        ]}):\n            self.assertEqual(\n                views.get_readable_field_data_type(Model._meta.get_field('custom_field')),\n                'A custom field\\'s description'\n            )\n\n\n\n", "    def test_field_description_empty(self):\n        class NoDescriptionField(models.Field):\n            description = None\n\n        self.assertEqual(views.get_readable_field_data_type(NoDescriptionField()), 'Field of type: NoDescriptionField')\n", "    def test_field_description_is_used_if_available(self):\n        class MyModel(models.Model):\n            custom_field = CustomField()\n            name = models.CharField(max_length=100)\n\n        self.assertEqual(\n            views.get_readable_field_data_type(MyModel.custom_field),\n            'A custom field type'\n        )\n\n\n\n", "    def test_method_with_keyword_default_argument(self):\n        self.assertContains(self.response, \"<td>set_status</td>\")\n        self.assertContains(self.response, '<td>status=</td>')\n\n\n\n", "    def test_model_detail_template_rendering(self):\n        response = self.client.get(reverse('django-admindocs-models-detail', args=['admin_docs', 'Person']))\n        self.assertContains(response, '<h1>Person</h1>', html=True)\n"], "sample_558": ["def test_inset_axes_with_shared_limits():\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n    x = np.linspace(0, 10, 100)\n    y1 = np.sin(x)\n    y2 = np.cos(x)\n\n    ax1.plot(x, y1)\n    ax1.set_title('Main Axes')\n    ax2 = zoomed_inset_axes(ax1, zoom=2, loc='lower right')\n    ax2.plot(x, y2)\n    ax2.set_title('Zoomed Inset Axes')\n    ax1.set_xlim(0, 10)\n    ax1.set_ylim(-1.2, 1.2)\n    ax2.set_xlim(0, 10)\n    ax2.set_ylim(-1.2, 1.2)\n", "def test_anchored_locator_base_call_position():\n    fig = plt.figure(figsize=(3, 3))\n    fig1, fig2 = fig.subfigures(nrows=2, ncols=1)\n\n    ax = fig1.subplots()\n    ax.set(aspect=1, xlim=(-15, 15), ylim=(-20, 5))\n    ax.set(xticks=[], yticks=[])\n\n    Z = cbook.get_sample_data(\"axes_grid/bivariate_normal.npy\")\n    extent = (-3, 4, -4, 3)\n\n    axins = zoomed_inset_axes(ax, zoom=2, loc=\"upper left\")\n    axins.set(xticks=[], yticks=[])\n\n    axins.imshow(Z, extent=extent, origin=\"lower\")\n    \n\n\n\n\n", "def test_anchored_locator_zoom():\n    fig = plt.figure(figsize=(6, 3))\n    ax = fig.add_subplot(1, 2, 1)\n    ax.set(xlim=(-3, 3))\n    ax.set(ylim=(-3, 3))\n\n    axins = zoomed_inset_axes(ax, zoom=2, loc=\"upper left\")\n    axins.set(xlim=(-3, 3))\n    axins.set(ylim=(-3, 3))\n\n    ax.plot([0, 1], [0, 1])\n    axins.plot([0, 1], [0, 1])\n\n    plt.show()\n", "def test_imagegrid_shared_cbar():\n    arr = np.arange(16).reshape((4, 4))\n\n    fig = plt.figure(figsize=(10, 5))\n    grid = ImageGrid(fig, 111,\n                     nrows_ncols=(2, 2),\n                     cbar_location='right',\n                     cbar_mode='shared')\n    ax1, ax2, ax3, ax4 = grid\n\n    for ax in [ax1, ax2, ax3, ax4]:\n        ax.imshow(arr, cmap='nipy_spectral')\n\n    fig.canvas.draw()\n\n\n\n", "def test_anchored_locator_custom():\n    fig = plt.figure(figsize=(3, 3))\n    ax = fig.add_subplot(111)\n    \n        return np.arange(0, 10, 2)\n    \n    ax.xaxis.set_major_locator(my_locator)\n    ax.yaxis.set_major_locator(my_locator)\n\n    ax.set(aspect=1, xlim=(-15, 15), ylim=(-20, 5))\n    ax.set(xticks=[], yticks=[])\n\n\n"], "sample_67": ["    def test_model_form_handles_strict_assignment_during_save(self):\n        \"\"\"\n        Regression for #24706:  model forms should gracefully handle model\n        __setattr__ calls that raise ValidationErrors during the save process.\n\n        This test is similar to the previous one, but it explicitly tests\n        the save behavior and makes sure that the error message is propagated\n        correctly.\n        \"\"\"\n        form_class = modelform_factory(model=StrictAssignmentAll, fields=['title'])\n        form = form_class(data={'title': 'testing setattr'}, files=None)\n        # This line turns on the ValidationError; it avoids the model erroring\n        # when its own __init__() is called when creating form.instance.\n        form.instance._should_error = True\n\n        with self.assertRaises(ValidationError):\n            form.save()  # Should raise ValidationError\n\n        self.assertEqual(form.errors, {\n            '__all__': ['Cannot set attribute'],\n            'title': ['This field cannot be blank.']\n        })\n\n\n\n", "    def test_model_form_clean_error_messages(self):\n        class MyModel(models.Model):\n            name = models.CharField(max_length=100)\n\n                if self.name == 'invalid':\n                    raise forms.ValidationError('This name is invalid.')\n\n        class MyModelForm(forms.ModelForm):\n            class Meta:\n                model = MyModel\n                fields = ['name']\n\n        form = MyModelForm(data={'name': 'invalid'})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['name'], ['This name is invalid.'])\n", "    def test_model_form_raises_validation_error_on_custom_attr(self):\n        class CustomAttrModel(models.Model):\n            title = models.CharField(max_length=100)\n\n                if name != 'title':\n                    raise ValidationError(f\"Cannot set attribute '{name}' directly.\")\n                super().__setattr__(name, value)\n\n        form_class = modelform_factory(CustomAttrModel, fields=['title'])\n        form = form_class(data={'title': 'testing setattr'})\n        with self.assertRaises(ValidationError):\n            form.instance._should_error = True\n            form.save()\n", "    def test_model_form_clears_instance_on_invalid(self):\n        form_class = modelform_factory(model=StrictAssignmentFieldSpecific, fields=['title'])\n        form = form_class(data={'title': 'testing setattr'}, files=None)\n        # This line turns on the ValidationError; it avoids the model erroring\n        # when its own __init__() is called when creating form.instance.\n        form.instance._should_error = True\n        self.assertFalse(form.is_valid())\n        self.assertIsNone(form.instance)\n\n", "    def test_setattr_raises_validation_error_for_readonly_field(self):\n        \"\"\"\n        A model ValidationError on a readonly field\n        should be prevented from being set.\n        \"\"\"\n        form_class = modelform_factory(model=StrictAssignmentReadonly, fields=['title'])\n        form = form_class(data={'title': 'testing setattr'}, files=None)\n        # This line turns on the ValidationError; it avoids the model erroring\n        # when its own __init__() is called when creating form.instance.\n        form.instance._should_error = True\n\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors, {'title': ['This field cannot be blank.']})\n"], "sample_860": ["def test_check_is_fitted_multi_estimator():\n    from sklearn.base import BaseEstimator, clone\n    class MyEstimator(BaseEstimator):\n            pass\n\n    estimator1 = MyEstimator()\n    estimator2 = clone(estimator1)\n\n    # Check that all estimators in a list are un-fitted\n    multi_estimators = [estimator1, estimator2]\n    assert check_is_fitted(multi_estimators) is None\n\n    estimator1.fit(X=np.ones((1, 1)), y=np.ones((1)))\n\n    # Check that only the fitted estimator raises None and others\n    # raise NotFittedError\n    assert check_is_fitted(multi_estimators) is None\n    assert_raises(NotFittedError, check_is_fitted, estimator2)\n", "def test_check_sparse_matrix_same_format(sparse_format):\n    data = np.ones((5, 5))\n    X = sp.csr_matrix(data)\n    X_same_format = sp.convert_to_sparse_format(X, sparse_format)\n    assert _allclose_dense_sparse(X, X_same_format)\n", "def test_check_array_dtype_warnings():\n    X = np.array([1, 2, 3], dtype=np.float64)\n    with pytest.warns(UserWarning, match=\"Data with input dtype \"\n                      \"float64 were all converted to\"):\n        check_array(X, dtype=np.float32)\n    with pytest.warns(UserWarning, match=\"Data with input dtype \"\n                      \"float64 were all converted to\"):\n        check_array(X, dtype=np.int32, warn_on_dtype=True)\n    assert_raises(ValueError, check_array, X, dtype=np.int32,\n                  warn_on_dtype=False, allow_nd=False)\n", "def test_check_string_array():\n    # Regression test for https://github.com/scikit-learn/scikit-learn/issues/3144\n    A = np.array(['a', 'b', 'c'])\n    with pytest.raises(TypeError,\n                       match=\"expected an array-like of floats\"):\n        check_array(A, dtype=np.float64)\n", "def test_check_array_sparse_format_warning():\n    for sparse_format in ['csr', 'csc', 'coo', 'dok', 'lil', 'bsr']:\n        X = sp.random(10, 10, format=sparse_format)\n        with pytest.warns(UserWarning,\n                          match=\"The format of the sparse matrix\"):\n            check_array(X, accept_sparse=True)\n"], "sample_882": ["def test_mlp_sparse_input_dropout():\n    # Test sparse input with dropout\n    from sklearn.datasets import make_classification\n    from sklearn.preprocessing import sparse_to_dense\n\n    X, y = make_classification(n_samples=100, n_features=10, random_state=42)\n    X_sparse = csr_matrix(X)\n\n    mlp = MLPClassifier(\n        hidden_layer_sizes=(10,), activation=\"relu\", dropout=0.5, solver=\"adam\"\n    )\n    mlp.fit(X_sparse, y)\n    \n    pred = mlp.predict(X_sparse)\n    assert_array_equal(pred, mlp.predict(sparse_to_dense(X_sparse)))\n", "def test_mlp_partial_fit_empty_dataset():\n    # Test that partial_fit handles empty datasets gracefully\n    mlp = MLPClassifier(hidden_layer_sizes=(5,), random_state=42)\n    mlp.fit(X_digits[:50], y_digits[:50])\n\n    with pytest.raises(ValueError, match=\"Input X cannot be empty\"):\n        mlp.partial_fit([], [])\n\n    with pytest.raises(ValueError, match=\"Input X and y must have the same length\"):\n        mlp.partial_fit([[1], [2]], [])\n\n", "compilation error", "def test_mlp_with_categorical_features():\n    from sklearn.preprocessing import OneHotEncoder\n\n    X = [[3, 2], [1, 6], [0, 1]]\n    y = [1, 0, 1]\n    # Introduce a categorical feature\n    X_cat = [['A', 2], ['B', 6], ['A', 1]]\n\n    # Encode categorical features\n    encoder = OneHotEncoder(handle_unknown='ignore')\n    X_cat_encoded = encoder.fit_transform(X_cat).toarray()\n\n    # Combine numerical and categorical features\n    X = np.concatenate((X, X_cat_encoded), axis=1)\n\n\n    # Test with MLPClassifier\n    mlp = MLPClassifier(hidden_layer_sizes=(5, 3), random_state=1, max_iter=50)\n    mlp.fit(X, y)\n\n    # Test with MLPRegressor\n    mlp = MLPRegressor(hidden_layer_sizes=(5, 3), random_state=1, max_iter=50)\n    mlp.fit(X, y)\n", "def test_mlp_fit_with_sparse_data(MLPEstimator):\n    \"\"\"Test fitting with sparse data.\"\"\"\n    from sklearn.datasets import make_classification\n\n    X, y = make_classification(n_samples=100, n_features=50, random_state=0, sparse=True).data, np.argmax(X, axis=1)\n    mlp = MLPEstimator(solver=\"adam\", random_state=0)\n    mlp.fit(X, y)\n\n\n"], "sample_803": ["compilation error", "def test_label_ranking_loss_with_sparse_matrices():\n    # Test with sparse matrices\n    y_true = csr_matrix([[0, 1, 0], [1, 1, 0]])\n    y_score = csr_matrix([[0.1, 10., -3], [3, 1, 3]])\n    assert_almost_equal(label_ranking_loss(y_true, y_score),\n                        (0 + 2 / 2) / 2.)\n\n\n", "compilation error", "compilation error", "compilation error"], "sample_630": ["def test_get_annotation_assignattr(init_method, label):\n    \"\"\"AssignAttr\"\"\"\n    assign = rf\"\"\"\n        class A:\n            {init_method}\n    \"\"\"\n    node = astroid.extract_node(assign)\n    instance_attrs = node.instance_attrs\n    for _, assign_attrs in instance_attrs.items():\n        for assign_attr in assign_attrs:\n            got = get_annotation(assign_attr).name\n            assert isinstance(assign_attr, astroid.AssignAttr)\n            assert got == label, f\"got {got} instead of {label} for value {node}\"\n", "def test_infer_node_2(node_type, expected):\n    node = astroid.extract_node(f\"a.{node_type}\")\n    assert infer_node(node) == {expected}, f\"got {infer_node(node)} instead of {expected} for node {node_type}\"\n", "def test_get_annotation_multiple_assignattr(init_method, label):\n    \"\"\"AssignAttr\"\"\"\n    assign = rf\"\"\"\n        class A:\n            {init_method}\n    \"\"\"\n    node = astroid.extract_node(assign)\n    instance_attrs = node.instance_attrs\n    for _, assign_attrs in instance_attrs.items():\n        for assign_attr in assign_attrs:\n            got = \", \".join([get_annotation(a).name for a in assign_attr.targets])\n            assert isinstance(assign_attr, astroid.AssignAttr)\n            assert got == label, f\"got {got} instead of {label} for value {node}\"\n", "def test_get_annotation_assignattr_union(init_method, label):\n    \"\"\"AssignAttr\"\"\"\n    assign = rf\"\"\"\n        class A:\n            {init_method}\n    \"\"\"\n    node = astroid.extract_node(assign)\n    instance_attrs = node.instance_attrs\n    for _, assign_attrs in instance_attrs.items():\n        for assign_attr in assign_attrs:\n            got = get_annotation(assign_attr).name\n            assert isinstance(assign_attr, astroid.AssignAttr)\n            assert got == label, f\"got {got} instead of {label} for value {node}\"\n", "def test_get_annotation_assign_type(init_method, label):\n    \"\"\"AssignAttr for type\"\"\"\n    assign = rf\"\"\"\n        class A:\n            {init_method}\n    \"\"\"\n    node = astroid.extract_node(assign)\n    instance_attrs = node.instance_attrs\n    for _, assign_attrs in instance_attrs.items():\n        for assign_attr in assign_attrs:\n            got = get_annotation(assign_attr).name\n            assert isinstance(assign_attr, astroid.AssignAttr)\n            assert got == label, f\"got {got} instead of {label} for value {node}\"\n"], "sample_493": ["    def test_subquery_with_outer_ref_without_exists(self):\n        total_books_qs = (\n            Author.book_set.through.objects.values(\"author\")\n            .filter(author=OuterRef(\"pk\"))\n            .annotate(total=Count(\"book\"))\n        )\n        with self.assertNumQueries(1) as ctx:\n            aggregate = (\n                Author.objects.annotate(\n                    total_books=Subquery(total_books_qs.values(\"total\"))\n                )\n                .filter(total_books__gt=1)\n                .values(\"pk\", \"total_books\")\n                .aggregate(sum_total_books=Sum(\"total_books\"))\n            )\n        sql = ctx.captured_queries[0][\"sql\"].lower()\n        self.assertEqual(sql.count(\"select\"), 3, \"Subquery wrapping required\")\n        self.assertEqual(aggregate, {\"sum_total_books\": 6})\n\n\n\n", "    def test_aggregate_with_join_and_outerref(self):\n        with CaptureQueriesContext(connection) as ctx:\n            aggregate = (\n                Author.objects.join(\n                    Book.objects.values(\"rating\"),\n                    Book.objects.get_queryset(),\n                    \"author\",\n                ).annotate(avg_rating=Avg(F(\"rating\"))).values(\"pk\", \"avg_rating\")\n            )\n        sql = ctx.captured_queries[0][\"sql\"].lower()\n        self.assertEqual(sql.count(\"select\"), 1, \"Subquery wrapping required\")\n        self.assertIn(\"avg_rating\", sql)\n\n\n\n", "    def test_mixed_aggregate_and_non_aggregate_annotation(self):\n        with CaptureQueriesContext(connection) as ctx:\n            result = Book.objects.annotate(\n                author_count=Count(\"authors\"),\n                name_lower=Lower(\"name\"),\n            ).aggregate(Avg(\"pages\"))\n        self.assertEqual(len(ctx.captured_queries), 1, \"Single query expected\")\n        sql = ctx.captured_queries[0][\"sql\"].lower()\n        self.assertEqual(sql.count(\"select\"), 1, \"Subquery wrapping not needed\")\n        self.assertNotIn(\"name_lower\", sql)\n        self.assertIn(\"author_count\", sql)\n\n\n\n", "    def test_subquery_in_aggregate_with_outer_ref(self):\n        subquery = (\n            Book.objects.values(\"author__id\")\n            .filter(pages__gt=500)\n            .annotate(count=Count(\"id\"))\n        )\n        with self.assertNumQueries(1) as ctx:\n            aggregate = Author.objects.annotate(\n                count_long_books=Subquery(subquery),\n            ).values(\"pk\", \"count_long_books\").annotate(\n                total_books=Count(\"id\")\n            ).aggregate(\n                sum_long_books=Sum(\"count_long_books\"),\n            )\n        sql = ctx.captured_queries[0][\"sql\"].lower()\n        self.assertEqual(sql.count(\"select\"), 4, \"Subquery wrapping required\")\n        self.assertEqual(aggregate, {\"sum_long_books\": 1}) \n", "    def test_nested_subquery_joins_and_aggregates(self):\n        book_count_qs = Book.objects.annotate(\n            book_count=Count(\"id\")\n        ).values(\"name\")\n        publisher_count_qs = Publisher.objects.annotate(\n            publisher_book_count=Subquery(\n                book_count_qs.filter(publisher=OuterRef(\"book__publisher\")).values(\"book_count\")\n            )\n        )\n        with CaptureQueriesContext(connection) as ctx:\n            aggregate = publisher_count_qs.aggregate(\n                avg_books=Avg(\"publisher_book_count\")\n            )\n        self.assertEqual(aggregate[\"avg_books\"], 1.5)\n        sql = ctx.captured_queries[0][\"sql\"].lower()\n        self.assertEqual(sql.count(\"select\"), 3, \"Subquery wrapping required\")\n        self.assertEqual(sql.count(\"publisher\"), 2)\n"], "sample_563": ["def test_annotationbbox_arrowprops():\n    fig, ax = plt.subplots()\n\n    ab = AnnotationBbox(\n        DrawingArea(20, 20),\n        (0.5, 0.5),\n        xycoords='data',\n        arrowprops=dict(arrowstyle=\"->\",\n                       connectionstyle=\"arc3,rad=0.5\"),\n    )\n    ax.add_artist(ab)\n\n", "def test_offsetbox_rotation():\n    fig, ax = plt.subplots()\n\n    da = DrawingArea(50, 50)\n    patch = mpatches.Rectangle((0, 0), 50, 50,\n                               facecolor='lightblue',\n                               linewidth=1, edgecolor='black')\n    da.add_artist(patch)\n\n    ab = AnnotationBbox(da, (0.5, 0.5),  \n                        bbox_transform=ax.transAxes,\n                        xybox=(0.5, 0.5),\n                        pad=0.1, borderpad=0.1,\n                        arrowprops=dict(arrowstyle=\"->\"))\n    ax.add_artist(ab)\n\n    ab.set_rotation(45)\n\n    fig.canvas.draw()\n", "def test_offsetbox_patch_alignment():\n    fig, ax = plt.subplots()\n    \n    # Test the case where the child is aligned to its center\n    da = DrawingArea(5, 5)\n    da.add_artist(mpatches.Rectangle((0, 0), 5, 5, linewidth=0, facecolor='green', edgecolor='red'))\n    ab = AnnotationBbox(da, (0.5, 0.5), xybox=(0, 0),\n                        xycoords='data', boxcoords='axes fraction', \n                        box_alignment=(0.5, 0.5),\n                        arrowprops=dict(arrowstyle=\"->\"))\n    ax.add_artist(ab)\n\n    # Test the case where the child is aligned to its top-left\n    da = DrawingArea(5, 5)\n    da.add_artist(mpatches.Rectangle((0, 0), 5, 5, linewidth=0, facecolor='blue', edgecolor='black'))\n    ab = AnnotationBbox(da, (0.5, 0.5), xybox=(0, 0),\n                        xycoords='data', boxcoords='axes fraction', \n                        box_alignment=(0, 0),\n                        arrowprops=dict(arrowstyle=\"->\"))\n    ax.add_artist(ab)\n\n    fig.canvas.draw()\n", "def test_offsetbbox_bbox_inches():\n    fig, ax = plt.subplots()\n\n    da = DrawingArea(10, 10)\n    ab = AnnotationBbox(da, (0.5, 0.5), xybox=(0, 0),\n                        xycoords='data', boxcoords='axes fraction',\n                        pad=0, bbox_inches='tight')\n    ax.add_artist(ab)\n\n    fig.canvas.draw()\n", "def test_offsetbox_bbox_transform():\n    fig, ax = plt.subplots()\n    \n    # Create a simple drawing area\n    da = DrawingArea(100, 100)\n    \n    # Create an offsetbox with a transform\n    bbox_transform = ax.transAxes\n    ab = AnchoredOffsetbox(loc='lower right', child=da, bbox_transform=bbox_transform)\n\n    # Add the offsetbox to the axes\n    ax.add_artist(ab)\n    \n    fig.draw()   \n\n\n\n"], "sample_855": ["def test_dummy_classifier_with_unknown_strategy():\n    X = [[0]] * 5\n    y = [1, 2, 1, 2, 1]\n\n    with pytest.raises(ValueError):\n        clf = DummyClassifier(strategy='unknown')\n        clf.fit(X, y)\n\n", "def test_dummy_classifier_predict_proba_for_constant_strategy():\n    X = [[0], [0], [0], [0]]  # ignored\n    y = [2, 1, 2, 2]\n\n    clf = DummyClassifier(strategy=\"constant\", random_state=0, constant=1)\n    clf.fit(X, y)\n    proba = clf.predict_proba(X)\n\n    assert_array_almost_equal(proba, np.array([[1.0, 0.0],\n                                              [0.0, 1.0],\n                                              [1.0, 0.0],\n                                              [1.0, 0.0]]))\n", "def test_dummy_regressor_sparse_input():\n    X = sp.csc_matrix([[1], [0], [1]])\n    y = np.array([2, 3, 1])\n\n    reg = DummyRegressor(strategy=\"mean\")\n    reg.fit(X, y)\n    assert_array_almost_equal(reg.predict(X), np.array([2, 3, 1]))\n\n\n", "def test_dummy_regressor_multioutput_constatnt_strategy_with_different_constants():\n    X = [[0], [0], [0], [0]]  # ignored\n    y = np.array([[1, 2],\n                  [3, 4],\n                  [5, 6],\n                  [7, 8]])\n    constants = [1, 2]\n    clf = DummyRegressor(strategy=\"constant\", constant=constants)\n    clf.fit(X, y)\n    y_pred = clf.predict(X)\n    assert_array_almost_equal(y_pred, [[1, 2],\n                                       [1, 2],\n                                       [1, 2],\n                                       [1, 2]])\n", "def test_dummy_classifier_multioutput_stratified_strategy():\n    X = [[0]] * 5  # ignored\n    y = np.array([[1, 0],\n                  [1, 1],\n                  [0, 0],\n                  [1, 1],\n                  [0, 1]])\n    n_samples = len(X)\n    clf = DummyClassifier(strategy=\"stratified\", random_state=0)\n    clf.fit(X, y)\n\n    X = [[0]] * 500\n    y_pred = clf.predict(X)\n    assert sp.issparse(y_pred)\n    y_pred = y_pred.toarray()\n\n    for k in range(y.shape[1]):\n        p = np.bincount(y_pred[:, k]) / float(len(X))\n        assert_almost_equal(p[1], 3. / 5, decimal=1)\n        assert_almost_equal(p[0], 2. / 5, decimal=1)\n\n\n\n"], "sample_616": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_562": ["def test_line_joinstyles():\n    # Tolerance introduced after reordering of floating-point operations\n    # Remove when regenerating the images\n    fig, ax = plt.subplots()\n\n    ax.plot(range(10), linestyle='-', joinstyle='round', lw=5)\n    ax.plot(range(10), linestyle='-', joinstyle='miter', lw=5)\n    ax.plot(range(10), linestyle='-', joinstyle='bevel', lw=5)\n\n\n", "def test_set_markersize():\n    x = np.array([1, 2, 3])\n    y = np.array([4, 5, 6])\n    fig, ax = plt.subplots()\n    line = ax.plot(x, y)\n    line.set_markersize(10)\n\n    p = line.get_children()[0].get_transform(ax.transAxes)\n    marker = line.get_children()[0].get_markers()[0]\n\n    assert marker.get_size() == 10\n\n", "def test_dash_styles():\n    fig, ax = plt.subplots()\n\n    x = np.linspace(0, 1, 50)\n    y = np.sin(x * 10)\n\n    styles = ['solid', 'dashed', 'dotted', 'dashdot']\n    for i, style in enumerate(styles):\n        ax.plot(x, y + i * 0.2, linewidth=2, linestyle=style)\n", "def test_marker_offset():\n    fig, ax = plt.subplots()\n    # test marker offset for different markers\n    for marker in ['o', 's', '^', 'v', '<', '>']:\n        line, = ax.plot([0, 1], [0, 1], marker=marker, markersize=10, markerfacecolor=\"red\",\n                        markeredgecolor='black', markeredgewidth=1, markevery=0.5)\n        verts = line.get_children()[0].get_path().vertices\n        # Check if marker position aligns with the line\n        for i in range(len(verts)):\n            if i % 2 == 0:\n                x, y = verts[i]\n                marker_offset = line.get_marker_offset(i)\n                assert abs(x - marker_offset[0]) < 1e-6,\\\n                       f\"Marker offset mismatch for marker '{marker}' at index {i}\"\n                assert abs(y - marker_offset[1]) < 1e-6,\\\n                       f\"Marker offset mismatch for marker '{marker}' at index {i}\"\n\n\n\n", "def test_marker_size_scaling(fig_test, fig_ref):\n    fig_test.subplots().plot([0, 1, 2], [0, 1, 0], marker=\"o\", markersize=5,\n                             transform=fig_test.transFigure)\n    fig_ref.subplots().plot([0, 1, 2], [0, 1, 0], marker=\"o\",\n                             markersize=5)\n\n"], "sample_613": ["    def test_resample_time_coords_with_different_names(self):\n        times = pd.date_range(\"2000-01-01\", freq=\"6H\", periods=10)\n        ds = Dataset(\n            {\n                \"foo\": ([\"time\", \"x\"], np.random.randn(10, 5)),\n                \"time\": times,\n                \"time2\": times,  # Duplicate time coordinate with a different name\n            }\n        )\n\n        with pytest.raises(ValueError, match=r\"Cannot upresample with different time coordinates\"):\n            ds.resample(time=\"12H\").mean()\n", "    def test_resample_with_non_standard_time_resolution(self):\n        times = [pd.Timestamp(\"2023-04-20 10:00:00\"), pd.Timestamp(\"2023-04-20 12:00:00\"), pd.Timestamp(\"2023-04-20 14:00:00\")]\n        data = xr.DataArray([1, 2, 3], dims=('time'), coords={'time': times})\n\n        actual = data.resample('1H') \n        expected = xr.DataArray(\n            data=[data[0], data[1], data[2]], \n            dims=('time'), \n            coords={'time': pd.date_range('2023-04-20 10:00:00', '2023-04-20 14:00:00', freq='H')}\n        )\n        assert_allclose(actual, expected)\n", "    def test_resample_with_coords_in_dataset(\n        self, data_var, group_by_var, resample_freq", "    def test_resample_variable_name_conflict(self):\n        # Test that resampling works even when a variable name conflicts\n        # with the resampling target dimension.\n        times = pd.date_range(\"2000-01-01\", freq=\"6H\", periods=10)\n        ds = Dataset(\n            {\n                \"time\": times,\n                \"value\": ([\"time\"], np.random.randn(10)),\n                \"x\": np.arange(5),\n            }\n        )\n        with pytest.raises(ValueError, match=r\"Resampling dimension 'time' also exists as a variable name\"):\n            ds.resample(time=\"1D\").mean()\n", "    def test_resample_ds_with_time_as_index(self):\n        times = pd.date_range(\"2000-01-01\", freq=\"6H\", periods=10)\n        ds = xr.Dataset(\n            {\"foo\": (\"time\", np.random.randn(10)), \"time\": times.to_index()},\n            coords={\"time\": times},\n        )\n        resampled = ds.resample(time=\"1D\").mean()\n        assert isinstance(resampled[\"foo\"].index, pd.DatetimeIndex)\n"], "sample_886": ["def test__wrap_in_pandas_container_column_callable_with_input():\n    \"\"\"Check that `columns` callable can use input_features.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    X = np.asarray([[1, 0, 3], [0, 0, 1]])\n    input_features = [\"feat1\", \"feat2\", \"feat3\"]\n\n        if input_features is None:\n            return [\"col1\", \"col2\", \"col3\"]\n        return input_features\n\n    X_wrapped = _wrap_in_pandas_container(X, columns=get_columns, input_features=input_features)\n    assert_array_equal(X_wrapped.columns, input_features)\n\n", "def test_set_output_index():\n    \"\"\"Check that set_output with 'pandas' config preserves the passed index.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    X = np.asarray([[1, 0, 3], [0, 0, 1]])\n    est = EstimatorWithSetOutputIndex().fit(X)\n\n    # Transform should respect the original index\n    est.set_output(transform=\"pandas\")\n    X_trans_pd = est.transform(X)\n    assert_array_equal(X_trans_pd.index, [f\"s{i}\" for i in range(X.shape[0])])\n\n", "def test_set_output_index_preservation():\n    \"\"\"Check that set_output preserves index when `index` is provided.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    X = np.asarray([[1, 0, 3], [0, 0, 1]])\n    index = pd.Index([\"a\", \"b\"])\n    X_df = pd.DataFrame(X, index=index)\n\n    est = EstimatorWithSetOutputIndex().fit(X_df)\n    est.set_output(transform=\"pandas\")\n\n    X_trans_pd = est.transform(X_df)\n    assert_array_equal(X_trans_pd.index, index)\n\n\n\n", "def test_set_output_with_index():\n    \"\"\"Check that `set_output` respects the index.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    X = np.asarray([[1, 0, 3], [0, 0, 1]])\n    est = EstimatorWithSetOutputIndex().fit(X)\n    est.set_output(transform=\"pandas\")\n\n    X_trans_pd = est.transform(X)\n    assert isinstance(X_trans_pd.index, pd.Index)\n    assert all(i == f\"s{i}\" for i, _ in enumerate(X_trans_pd.index))\n\n\n\n", "def test__wrap_in_pandas_container_sparse_data():\n    \"\"\"Check that _wrap_in_pandas_container raises ValueError for sparse data.\"\"\"\n    from scipy.sparse import csc_matrix\n    \n    X_sparse = csc_matrix([[1, 0, 3], [0, 0, 1]])\n    with pytest.raises(ValueError, match=\"Pandas output does not support sparse data\"):\n        _wrap_in_pandas_container(X_sparse, columns=[\"a\", \"b\", \"c\"])\n\n"], "sample_192": ["    def test_non_valid(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [\n            {'choices-0-votes': ['This field is required.']},\n            {},\n        ]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n\n", "    def test_invalid(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',  # <-- this vote should be invalid 0\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [{'choices-0-votes': ['This field must be greater than 0.']}, {}]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n", "    def test_invalid(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': 'abc',  # Invalid votes input\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [{'votes': ['Enter a valid number.']}, {}]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n", "    def test_invalid(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': '',\n            'choices-1-votes': '',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [{'votes': ['This field is required.']}, {'choice': ['This field is required.'], 'votes': ['This field is required.']}]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n", "    def test_invalid(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': '',\n            'choices-1-votes': 'invalid',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [{'votes': ['Enter a valid integer.']}, {'choice': ['This field is required.']}]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n"], "sample_996": ["def test_issue_14356():\n    n = Symbol('n')\n    p = Product(n, (n, 1, oo))\n    assert p.is_convergent() is S.false \n", "def test_issue_14036_continued():\n    a, n = symbols('a n')\n    p = product(1 - a**2 / (n*pi)**2, [n, 1, oo])\n    assert p == (1/pi)**2 * (1/a) * (a*pi/2).polylog(2)\n\n\n\n", "compilation error", "def test_issue_15150():\n    x, y = symbols('x y')\n    p = Product(x**y, (y, 1, n))\n    q = p.rewrite(Function(\"f\"))\n    assert q.func == x**Function(\"f\", x)\n    assert q.variables == [x]\n\n\n\n", "def test_issue_14036():\n    a, n = symbols('a n')\n    assert product(1 - a**2 / (n*pi)**2, [n, 1, oo]).is_convergent()\n"], "sample_800": ["def test_check_estimator_with_deprecated_init():\n    \"\"\"Check that estimators with deprecated __init__ methods are handled.\"\"\"\n    class DeprecatedInitEstimator(BaseEstimator):\n        @deprecated(\"For testing purposes, this __init__ method is \"\n                    \"deprecated.\")\n            self.param1 = param1\n            self.param2 = param2\n\n    check_estimator(DeprecatedInitEstimator())\n\n\n\n", "def test_check_estimator_class_weight_balanced_linear_classifier():\n    # Check that check_class_weight_balanced_linear_classifier raises error for\n    # estimator with class_weight not set to 'auto' or 'balanced'\n    with ignore_warnings(category=(FutureWarning, DeprecationWarning)):\n        msg = (\"Estimator's class_weight parameter must be 'auto' or \"\n               \"'balanced', received 'some_value'.\")\n        assert_raises_regex(ValueError, msg,\n                            check_class_weight_balanced_linear_classifier,\n                            BadBalancedWeightsClassifier, class_weight='some_value')\n\n\n\n", "def test_check_estimator_handles_sparse_data_types():\n    # Test that estimators handle sparse data types correctly\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n\n    # With sparse data\n    X_sparse = sp.csc_matrix([[1, 2], [3, 4]])\n    y = np.array([0, 1])\n    \n    # Test Logistic Regression with sparse data\n    est = LogisticRegression()\n    est.fit(X_sparse, y)\n    assert_allclose(est.coef_, np.array([[0.25, 0.5]]))\n\n    # Test with scaler\n    scaler = StandardScaler()\n    X_sparse_scaled = scaler.fit_transform(X_sparse)\n    est = LogisticRegression().fit(X_sparse_scaled, y)\n    assert_allclose(est.coef_, np.array([[0.25, 0.5]]))\n\n    # Test with polynomial features\n    poly = PolynomialFeatures(degree=2)\n    X_sparse_poly = poly.fit_transform(X_sparse)\n    est = LogisticRegression().fit(X_sparse_poly, y)\n    assert_allclose(est.coef_.shape, (3, 1))\n\n    # Test with a dummy estimator that expects dense data\n    class DummyEstimator(BaseEstimator):\n            if not isinstance(X, np.ndarray):\n                raise ValueError(\"DummyEstimator expects dense data\")\n            return self\n\n    with assert_raises(ValueError):\n        DummyEstimator().fit(X_sparse, y)\n\n", "def test_check_estimator_with_deprecated_params():\n    class MyEstimator(BaseEstimator):\n        # Add deprecated parameters\n        @deprecated(\"This parameter is deprecated. Use 'new_param' instead.\")\n            self.old_param = old_param\n            self.new_param = new_param\n\n    check_estimator(MyEstimator)\n", "def test_check_estimator_sparse_matrix_handling():\n    # Tests that estimators handle sparse matrices correctly.\n\n    class TestSparseEstimator(BaseEstimator):\n            X = check_array(X)\n            if sp.issparse(X):\n                assert X.shape[0] == X.shape[1] \n            return self\n\n            X = check_array(X)\n            return X\n\n    check_estimator(TestSparseEstimator())\n"], "sample_745": ["def test_sparse_matrix_input():\n    X_sparse = sparse.csr_matrix([[1, 2], [3, 4]])\n\n    # Test that sparse matrix input is accepted if accept_sparse is True\n    trans = FunctionTransformer(\n        func=lambda X: X + 1, accept_sparse=True)\n    assert_allclose_dense_sparse(\n        trans.transform(X_sparse),\n        X_sparse + 1,\n    )\n\n    # Test that an exception is raised if accept_sparse is False\n    with pytest.raises(ValueError):\n        trans = FunctionTransformer(\n            func=lambda X: X + 1, accept_sparse=False)\n        trans.transform(X_sparse)\n\n", "def test_sparse_matrix_input():\n    X_sparse = sparse.csc_matrix([[1, 2], [3, 4]])\n    transformer = FunctionTransformer(accept_sparse=True)\n    transformed_sparse = transformer.transform(X_sparse)\n    assert isinstance(transformed_sparse, sparse.spmatrix)\n    assert_allclose_dense_sparse(\n        transformer.inverse_transform(transformed_sparse), X_sparse)\n\n    transformer = FunctionTransformer(accept_sparse=False)\n    with pytest.raises(ValueError):\n        transformer.transform(X_sparse) \n", "def test_sparse_input():\n    X_sparse = sparse.csc_matrix([[1, 2], [3, 4]])\n\n    # Test that sparse input is handled correctly if accept_sparse is True.\n    transformer = FunctionTransformer(lambda X: X * 2,\n                                     accept_sparse=True)\n    assert_allclose_dense_sparse(\n        transformer.transform(X_sparse),\n        X_sparse * 2\n    )\n\n\n\n", "    def test_sparse_matrix_input():\n        X_sparse = sparse.csr_matrix([[1, 2], [3, 4]])\n\n        # Test that sparse matrices are handled correctly\n        F = FunctionTransformer(np.sum, accept_sparse=True)\n        assert_allclose_dense_sparse(\n            F.transform(X_sparse),\n            np.sum(X_sparse, axis=1).reshape(-1, 1),\n        )\n", "def test_sparse_matrix_input():\n    X_sparse = sparse.csr_matrix(([1, 2, 3],\n                                (range(3), [0, 1, 0])),\n                                shape=(3, 1))\n    \n    F = FunctionTransformer(np.sum, accept_sparse=True)\n    \n    # Test that sparse matrix is accepted and result is correct\n    assert_allclose_dense_sparse(\n        F.transform(X_sparse),\n        np.array([1, 2, 3]).reshape(3, 1),\n    )\n\n"], "sample_1184": ["compilation error", "compilation error", "  def test_conjugate_gauss_beams_focal_length():\n        wavelen = 532e-9\n        waist_in = 1e-3\n        waist_out = 2e-3\n        f = 1e-2\n        s_in, s_out, f_ = conjugate_gauss_beams(wavelen, waist_in, waist_out, f=f)\n        assert streq(s_in, N(f*(1 - sqrt(1/((waist_in/waist_out)**2) - pi**2*(waist_in**4)/(f**2*(s_in**2))))) )\n        assert streq(s_out, gaussian_conj(s_in, waist2rayleigh(waist_in, wavelen), f)[0])\n        assert streq(f,f_)\n\n", "compilation error", "compilation error"], "sample_923": ["    def check_consistency(role, tag, expected):\n        for target in expected:\n            pattern = r'{role}-role:.*?'\n                   r'<(?P<tag>{tag}) .*? class=[\"\\']{}[\"\\'].*?>'\n                   r'.*'\n                   r'</(?P=tag)>'.format(role=role, tag=tag, target=target)\n            result = re.search(pattern, output)\n            expect = '''\\", "compilation error", "    def check_class_members(role, tag, members):\n        pattern = (r'{role}-role:.*?'\n                   r'<(?P<tag>{tag}) .*?'\n                   r'>(.*?)</(?P=tag)>').format(role=role, tag=tag)\n        result = re.search(pattern, output)\n        expect = '''\\", "    def check(role, tag, expected):\n        classes = set(classes(role, tag))\n        assert classes == expected, f\"Expected classes for role '{role}' with tag '{tag}' to be {expected!r}, but found {classes!r}\"", "def test_xref_template_param_qualified_name(app, status, warning):\n    app.builder.build_all()\n\n    test = 'template_param_qualified_name.html'\n    output = (app.outdir / test).read_text()\n\n    pattern = r'<li><p>T<a.*?>.*?<span.*?>T::U::paramWarn</span>.*?</a></p></li>'\n    result = re.search(pattern, output)\n    assert result\n"], "sample_684": ["def test_repr_locals_reprformat_locals() -> None:\n    r = ReprLocals(\n        [\n            \"x = 1\",\n            \"y = 2.5\",\n            \"z = [1, 2, 3]\",\n            \"w = {'a': 1, 'b': 2}\",\n        ]\n    )\n    expected = (\n        \"x = 1\\n\"\n        \"y = 2.5\\n\"\n        \"z = [1, 2, 3]\\n\"\n        \"w = {'a': 1, 'b': 2}\"\n    )\n    assert r.toterminal(None) == expected\n\n\n\n", "    def test_repr_locals(self) -> None:\n            x = 1\n            y = [1, 2, 3]\n            return\n        with pytest.raises(Exception, match=\"this will raise\"):\n            func()\n        excinfo = ExceptionInfo.from_current()\n        entry = excinfo.traceback[0]\n        locals = entry.locals\n        repr_locals = ReprLocals(locals)\n        assert repr_locals.lines[0] == \"x = 1\"\n        assert repr_locals.lines[1] == \"y = [1, 2, 3]\"\n", "    def test_reprexceptioninfo(self) -> None:\n        try:\n            raise NotImplementedError(\"Test exception for ReprExceptionInfo\")\n        except NotImplementedError:\n            excinfo = ExceptionInfo.from_current()\n            reprexci = ReprExceptionInfo(\n                reprtraceback=ReprTraceback(\n                    reprentries=[ReprEntry(lines=[\"  raise NotImplementedError('Test exception for ReprExceptionInfo')\"], style=\"short\")],\n                    extraline=None,\n                    style=\"long\",\n                ),\n                reprcrash=ReprFileLocation(path=\"test_reprcrash.py\", lineno=2, message=\"Test exception for ReprExceptionInfo\"),\n            )\n            reprexci.toterminal(TerminalWriter())\n\n", "    def test_repr_func_args_with_exception(self) -> None:\n        args = [(\"value\", ValueError(\"Custom Error\"))]\n\n        r = ReprFuncArgs(args)\n        with pytest.raises(ValueError) as e:\n            r.toterminal(tw_mock)\n        assert str(e.value) == \"Custom Error\"\n", "    def test_reprfuncargs_empty(self) -> None:\n        r = ReprFuncArgs([])\n        tw_mock = TerminalWriter.mock()\n        r.toterminal(tw_mock)\n        assert tw_mock.lines == []\n"], "sample_553": ["def test_embed_html_with_no_writer(tmpdir, writer):\n    with plt.rc_context({'animation.writer': writer}):\n        fig, ax = plt.subplots()\n        line, = ax.plot([], [])\n\n            line.set_data([0, 1], [0, i / 10])\n            return line,\n\n        anim = animation.FuncAnimation(fig, animate, frames=5)\n        html = anim.to_html5_video()\n        assert '<video' in html  # should contain video element\n    \n\n\n\n", "def test_save_count_override_warnings_generator(anim):\n        yield 1\n        yield 2\n        yield 3\n\n    save_count = 2\n    match_target = (\n        f'You passed in an explicit {save_count=} '\n        \"which is being ignored in favor of \"\n        \"generator's length (3).\"\n    )\n    with pytest.warns(UserWarning, match=re.escape(match_target)):\n        anim = animation.FuncAnimation(\n            **{**anim, 'frames': frames, 'save_count': save_count}\n        )\n\n    assert anim._save_count == save_count\n\n\n\n", "def test_save_animation_frames_per_frame(tmpdir, writer):\n    fig, ax = plt.subplots()\n    x = np.linspace(0, 2 * np.pi, 100)\n    line, = ax.plot([], [])\n\n        line.set_data(x, np.sin(x + i / 100))\n        return line,\n\n    frames_per_second = 10\n    anim = animation.FuncAnimation(\n        fig, animate, init_func=lambda: line,\n        frames=5, blit=True,\n        repeat=False,\n        interval=1000 // frames_per_second\n    )\n\n    with tmpdir.as_cwd():\n        anim.save(\n            'frames_per_frame.gif',\n            writer=writer,\n            fps=frames_per_second,\n        )\n\n        # Check the output:\n        # - Ensure the output file exists\n        assert Path('frames_per_frame.gif').exists()\n        # - Check the number of frames:\n        with imageio as im:\n            images = im.mimread('frames_per_frame.gif')\n        assert len(images) == 5\n\n\n\n", "def test_missing_save_count_warning(frames, tmpdir):\n    fig, ax = plt.subplots()\n        return []\n\n    anim = animation.FuncAnimation(fig, animate, frames=frames,\n                                   cache_frame_data=False,\n                                   save_count=None)\n    with tmpdir.as_cwd():\n        anim.save(\"test.gif\")  # should not raise error\n", "def test_repr_html_with_save_count(writer, html, want, anim):\n    # Test that the repr_html output is the same with and without save_count.\n    with plt.rc_context({'animation.writer': writer,\n                         'animation.html': html}):\n        anim = animation.FuncAnimation(**anim)\n    anim._save_count = 1\n    html1 = anim._repr_html_()\n    del anim\n    anim = animation.FuncAnimation(**anim)\n    html2 = anim._repr_html_()\n    assert html1 == html2\n\n"], "sample_450": ["    def test_get_admin_log_template_tag(self):\n        context = {}\n        context[\"log_entries\"] = LogEntry.objects.filter(\n            content_type__model__iexact=\"article\"\n        )\n        \n        # Test with a specific user\n        user = User.objects.create_user(username=\"testuser\", password=\"password\")\n        with self.subTest(user=user):\n            render_context = {\n                \"log_entries\": context[\"log_entries\"],\n                \"user\": user,\n            }\n            output = template.Template(\n                \"{% get_admin_log 10 as admin_log for_user user %}\"\n            ).render(render_context)\n            self.assertTrue(\n                \"admin_log\" in render_context,\n                \"Variable 'admin_log' should be in the context\",\n            )\n            self.assertEqual(\n                len(render_context[\"admin_log\"]), 10, \"Top 10 entries expected\"\n            )\n\n        # Test with user ID\n        with self.subTest(user_id=user.pk):\n            render_context = {\n                \"log_entries\": context[\"log_entries\"],\n                \"user\": user.pk,\n            }\n            output = template.Template(\n                \"{% get_admin_log 10 as admin_log for_user %}\" % user.pk\n            ).render(render_context)\n            self.assertTrue(\n                \"admin_log\" in render_context,\n                \"Variable 'admin_log' should be in the context\",\n            )\n            self.assertEqual(\n                len(render_context[\"admin_log\"]), 10, \"Top 10 entries expected\"\n            )\n\n", "    def test_get_admin_log_template_tag(self):\n        # Set up a context with log entries\n        entry1 = LogEntry.objects.log_action(\n            self.user.pk,\n            ContentType.objects.get_for_model(Article).pk,\n            self.a1.pk,\n            repr(self.a1),\n            CHANGE,\n            change_message=\"Changed something\",\n        )\n        entry2 = LogEntry.objects.log_action(\n            self.user.pk,\n            ContentType.objects.get_for_model(Article).pk,\n            self.a1.pk,\n            repr(self.a1),\n            DELETION,\n            change_message=\"Deleted something\",\n        )\n        context = {\n            \"log_entries\": LogEntry.objects.all(),\n        }\n        # Render the template tag\n        template = (\n            \"{% load admin_utils.tags %}\"\n            \"{% get_admin_log 10 as admin_log for_user 23 %}\"\n            \"{{ admin_log }}\"\n        )\n        rendered_content = template.render(context)\n        # Validate the output\n        self.assertIn(repr(self.a1), rendered_content)\n        self.assertNotIn(entry2.get_change_message(), rendered_content)\n\n\n", "    def test_get_admin_log_template_tag(self):\n        # Setup a template context\n        context = {\n            \"log_entries\": LogEntry.objects.all().order_by(\"-action_time\"),\n        }\n\n        # Test normal usage with user and limit\n        result = get_admin_log(\n            context,\n            \"10\",\n            \"admin_log\",\n            \"for_user\",\n            self.user,\n        )\n        self.assertEqual(len(result), 10)\n        self.assertEqual(context[\"admin_log\"], result[:10])\n\n        # Test usage without user\n        result = get_admin_log(\n            context,\n            \"5\",\n            \"other_log\",\n            \"\",\n        )\n        self.assertEqual(len(result), 5)\n        self.assertEqual(context[\"other_log\"], result[:5])\n\n\n\n", "    def test_get_admin_log_template_tag(self):\n        context = {'log_entries': LogEntry.objects.all()}\n        template = template.Template(\n            \"{% get_admin_log 10 as admin_log %}\"\n        )\n        result = template.render(context)\n        self.assertEqual(result, \"\")\n\n", "    def test_get_admin_log_tag(self):\n        with self.subTest(\"Empty log\"):\n            context = {\"log_entries\": []}\n            result = get_admin_log(\n                MockParser(),\n                \"Mock Tokens\"\n            ).render(context)\n            self.assertEqual(result, \"\")\n\n        with self.subTest(\"Limit and varname\"):\n            context = {\"log_entries\": LogEntry.objects.all()}\n            result = get_admin_log(\n                MockParser(),\n                \"Mock Tokens\"\n            ).render(context)\n            self.assertEqual(result, \"\")\n\n        with self.subTest(\"Filter by user\"):\n            context = {\"log_entries\": LogEntry.objects.all(), \"user\": User.objects.get(pk=1)}\n            result = get_admin_log(\n                MockParser(),\n                \"Mock Tokens\"\n            ).render(context)\n            self.assertEqual(result, \"\")\n\n\n"], "sample_1094": ["def test_rcall_commutative():\n    x, y = symbols('x y')\n    assert (x + y).rcall(1) == x + 1\n    assert (x*y).rcall(1) == x*1\n    assert (x**y).rcall(1) == x**1\n    assert (x/y).rcall(1) == x/1\n    assert ((x + y)/z).rcall(1) == (x + y)/1\n    assert (sin(x) + cos(x)).rcall(1) == sin(1) + cos(1)\n    assert (cos(x)**2).rcall(1) == cos(1)**2\n    assert ((x**2 + y**2)**(1/2)).rcall(1) == (1**2 + 1**2)**(1/2)\n", "compilation error", "def test_rewrite_with_undefined_functions():\n    from sympy.core.function import UndefinedFunction as UndefFunc\n    from sympy.functions import Exp\n\n    x = symbols('x')\n    f = UndefFunc('f')\n    assert f(x).rewrite(Exp) == f(x)\n    raises(TypeError, lambda: f(x).rewrite(cos))\n", "def test_rewrite_with_custom_rule():\n    x = symbols('x')\n    f = Function('f')\n        if isinstance(expr, f):\n            return x\n        return expr\n    assert f(x).rewrite(f, rule) == x \n    assert (f(x) + 1).rewrite(f, rule) == x + 1\n\n", "compilation error"], "sample_41": ["def test_parse_from_string_with_spaces():\n    \"\"\"\n    Regression test for #4113: Should handle spaces in unit strings correctly\n    \"\"\"\n    assert u.parse_from_string(\"  meters  \") == u.m\n    assert u.parse_from_string(\" km / s \") == u.km / u.s\n", "compilation error", "def test_composite_unit_repr():\n    unit = u.Unit('10 * cm / s / Hz')\n    assert repr(unit) == '10 * cm / (s * Hz)' \n", "compilation error", "def test_unit_summary_table():\n    from astropy.units import cds\n\n    with cds.enable():\n        from astropy.units.cds import cds_units\n        for unit in cds_units:\n            name = unit.name\n            if name == \"g_geo\" or name == \"M_geo\":\n                continue  # Skip these as they are handled differently\n            with cds.enable():\n                unit_summary = utils._get_unit_summary(unit)\n                assert unit_summary is not None\n                assert unit_summary[2] == unit.physical_type\n                assert unit_summary[3] == unit.scale\n\n"], "sample_486": ["    def test_inlineformset_factory_with_unique_fk(self):\n        \"\"\"\n        Models with a unique foreign key should limit the formset to a single item.\n        \"\"\"\n        FormSet = inlineformset_factory(\n            AutoPKParent, ChildWithEditablePK, fields=\"__all__\", max_num=1\n        )\n        formset = FormSet()\n        self.assertEqual(len(formset.forms), 1)\n\n\n\n", "    def test_inlineformset_factory_no_fk_field(self):\n        \"\"\"\n        #24377 - If no ForeignKey field is explicitly provided, it should\n        be inferred from the `parent_model` and `model` arguments\n        to `inlineformset_factory`.\n        \"\"\"\n        FormSet = inlineformset_factory(AutoPKParent, ChildWithEditablePK)\n        self.assertEqual(FormSet._meta.fields[\"parent\"].name, \"parent\")\n\n\n\n", "    def test_inlineformset_factory_empty_label_with_null_values(self):\n        \"\"\"\n        Tests that the empty_label is properly used with null values when\n        creating a new inline formset instance.\n        \"\"\"\n        class MyForm(forms.Form):\n            pk = forms.IntegerField(\n                widget=forms.HiddenInput(), required=False\n            )\n\n        FormSet = inlineformset_factory(\n            AutoPKParent, AutoPKChildOfUUIDPKParent, form=MyForm, empty_label=\"\"\n        )\n        formset = FormSet()\n        self.assertEqual(formset.forms[0].fields[\"pk\"].initial, None)\n        self.assertEqual(formset.forms[0].fields[\"pk\"].empty_label, \"\")\n\n", "    def test_inlineformset_factory_validates_pk_value(self):\n        FormSet = inlineformset_factory(\n            AutoPKParent, ChildWithEditablePK, fields=\"__all__\"\n        )\n        formset = FormSet()\n        form = formset.forms[0]\n        form.fields[\"parent\"] = 9999\n\n        with self.assertRaises(ValidationError) as e:\n            form.save()\n\n        self.assertEqual(\n            str(e.exception),\n            \"\u201c9999\u201d is not a valid value.\",\n        )\n", "    def test_inlineformset_factory_can_use_to_field_name(self):\n        FormSet = inlineformset_factory(\n            ChildWithEditablePK, AutoPKParent, fields=\"__all__\", to_field_name=\"name\"\n        )\n        formset = FormSet()\n        self.assertEqual(formset.forms[0].fields[\"parent\"].to_field_name, \"name\")\n"], "sample_585": ["def test_groupby_na_handling():\n    data = xr.DataArray([1, 2, np.nan, 4],\n                        dims=('x',),\n                        )\n    grouped = data.groupby('x').fillna(0)\n    expected = xr.DataArray([1, 2, 0, 4], dims=('x',))\n    assert_identical(grouped, expected)\n", "def test_groupby_no_groups():\n    array = xr.DataArray([1, 2, 3], dims='x')\n    with pytest.raises(ValueError):\n        array.groupby('nonexistent_dim')\n\n\n\n", "def test_da_groupby_skipna():\n\n    array = xr.DataArray([1, np.nan, 3], [('x', [1, 2, 3])])\n    expected = xr.DataArray([1, 3], [('x', [1, 2])])\n    actual = array.groupby('x').sum(skipna=True)\n    assert actual.equals(expected)\n\n", "def test_da_groupby_empty_after_apply():\n\n    empty_array = xr.DataArray([], dims='dim')\n\n    g = empty_array.groupby('dim')\n    actual = g.apply(lambda x: x)\n    assert actual.empty\n", "def test_da_groupby_empty_group():\n\n    array = xr.DataArray([1, 2, 3], [('x', [1, 1, 2])])\n    # Create an empty group\n    empty_group = array.groupby('x').get_group(None)\n\n    assert empty_group.size == 0\n"], "sample_1151": ["def test_issue_19277():\n    from sympy.functions import sign\n    x = symbols('x')\n    assert sign(x**3 - 3*x**2 + 2*x) == sign(x)\n", "def test_issue_19669():\n    from sympy import limit\n    x = symbols('x')\n    f = (x**2 - 1) / (x - 1)\n    assert limit(f, x, 1) == 2\n", "def test_issue_8677():\n    from sympy import sqrt\n    expr = sqrt(x**2)\n    assert expr.is_number is True\n    assert expr.is_Integer is False\n    assert expr.is_Rational is False\n    assert expr.is_Real is True\n    assert expr == abs(x)\n\n\n\n", "def test_issue_17130():\n    e = Add(b, -b, I, -I, evaluate=False)\n    assert e.is_zero is None # ideally this would be True\n", "def test_issue_18695():\n    x = symbols('x', imaginary=True)\n    assert (x + 1)**2.6 != (x**2 + 2*x + 1)**1.3\n"], "sample_747": ["def test_power_transformer_unknown_method():\n    pt = PowerTransformer(method='unknown')\n    X = np.abs(X_2d)\n    assert_raises(ValueError, pt.fit, X)\n", "def test_power_transformer_nan_handling():\n    X = np.abs(X_2d)\n    X[0, 1] = np.nan\n    X[1, 0] = np.nan\n\n    for standardize in [True, False]:\n        pt = PowerTransformer(method='box-cox', standardize=standardize)\n\n        # fit should handle nans gracefully\n        pt.fit(X)\n\n        # transform should handle nans gracefully\n        X_trans = pt.transform(X)\n        assert_array_almost_equal(X_trans,\n                                  np.where(np.isnan(X), np.nan,\n                                           pt.transform(X)))\n\n        # inverse should handle nans gracefully\n        X_inv = pt.inverse_transform(X_trans)\n        assert_array_almost_equal(X_inv,\n                                  np.where(np.isnan(X), np.nan,\n                                           pt.inverse_transform(X_trans)))\n\n", "compilation error", "def test_power_transformer_y_scale():\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    pt = PowerTransformer(method='box-cox', y_scale=True)\n\n    X_trans = pt.fit_transform(X)\n\n    # Assert that y_scale is applied to each feature independently\n    for i in range(X.shape[1]):\n        assert_almost_equal(X_trans[:, i], pt.transform(X[:, i],\n                                                        scale=True))\n\n    # Assert that inverse_transform correctly applies inverse y_scale\n    assert_equal(pt.inverse_transform(X_trans), X)\n", "def test_power_transformer_yule_simpson_fit_transform():\n    X = np.array(\n        [[1, 1], [10, 10], [100, 100]], dtype=np.float64\n    )\n\n    pt = PowerTransformer(method='yule-simpson')\n\n    X_trans = pt.fit_transform(X)\n\n    # Expect a warning for using yule-simpson on a 2D array\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"always\")\n        assert_warns(\n            UserWarning,\n            pt.fit_transform,\n            X,\n        )\n        assert_almost_equal(X_trans, X)\n\n\n    # Test inverse transformation\n    X_inv = pt.inverse_transform(X_trans)\n    assert_array_almost_equal(X_inv, X)\n"], "sample_739": ["compilation error", "compilation error", "def test_label_binarizer_fit_transform_with_empty_classes():\n    mlb = MultiLabelBinarizer()\n    with assert_raises(ValueError):\n        mlb.fit_transform([])\n\n    # test fit_transform with classes is empty\n    mlb = MultiLabelBinarizer()\n    assert_raises(ValueError, mlb.fit_transform,\n                  [('a',)], classes=[])\n\n    # test fit with classes is empty\n    mlb = MultiLabelBinarizer()\n    assert_raises(ValueError, mlb.fit, []).transform([])\n", "def test_label_binarize_empty_input():\n    for sparse_output in [True, False]:\n        # Empty input\n        assert_array_equal(label_binarize([], classes=[0, 1]),\n                           np.empty((0, 0), dtype=int))\n        assert_array_equal(label_binarize([], classes=[0, 1],\n                                           sparse_output=sparse_output),\n                           np.empty((0, 0), dtype=int))\n\n    assert_raises(ValueError, label_binarize, [], classes=[])\n\n\n", "def test_label_binarizer_sparse_output_dtype():\n    mlb = MultiLabelBinarizer(sparse_output=\"csr\")\n    y = [1, 2, 3]\n    mlb.fit_transform(y)\n    assert_equal(mlb.transform(y).dtype, np.int32)\n    assert_equal(mlb.inverse_transform(mlb.transform(y)).dtype, np.int32) \n\n    mlb = MultiLabelBinarizer(sparse_output=\"csc\")\n    y = [1, 2, 3]\n    mlb.fit_transform(y)\n    assert_equal(mlb.transform(y).dtype, np.int32)\n    assert_equal(mlb.inverse_transform(mlb.transform(y)).dtype, np.int32)\n\n    mlb = MultiLabelBinarizer(sparse_output=\"dok\")\n    y = [1, 2, 3]\n    mlb.fit_transform(y)\n    assert_equal(mlb.transform(y).dtype, np.int32)\n    assert_equal(mlb.inverse_transform(mlb.transform(y)).dtype, np.int32)\n\n\n\n"], "sample_966": ["def test_info_field_list_type_args(app):\n    text = (\".. py:module:: example\\n\"\n            \".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   :param age: blah blah\\n\"\n            \"   :type age: int[1, 10]\\n\")\n    doctree = restructuredtext.parse(app, text)\n\n    assert_node(doctree,\n                (nodes.target,\n                 addnodes.index,\n                 addnodes.index,\n                 [desc, ([desc_signature, ([desc_annotation, (\"class\", desc_sig_space)],\n                                           [desc_addname, \"example.\"],\n                                           [desc_name, \"Class\"])],\n                         [desc_content, nodes.field_list, nodes.field, (nodes.field_name,\n                                                        nodes.field_body)])]))\n    assert_node(doctree[3][1][0][0][1],\n                ([nodes.paragraph, ([addnodes.literal_strong, \"age\"],\n                                    \" (\",\n                                    [pending_xref, addnodes.literal_emphasis, \"int\"],\n                                    [addnodes.literal_emphasis, \"[\"],\n                                    [addnodes.literal_emphasis, \"1\"],\n                                    [addnodes.literal_emphasis, \", \"],\n                                    [addnodes.literal_emphasis, \"10\"],\n                                    [addnodes.literal_emphasis, \"]\"],\n                                    \")\",\n                                    \" -- \",\n                                    \"blah blah\")],))\n    assert_node(doctree[3][1][0][0][1][0][2], pending_xref,\n                refdomain=\"py\", reftype=\"class\", reftarget=\"int\",\n                **{\"py:module\": \"example\", \"py:class\": \"Class\"})\n\n\n\n", "compilation error", "def test_python_python_use_unqualified_type_names_enabled(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"n\"><a class=\"reference internal\" href=\"#Name\" title=\"Name\">'\n            '<span class=\"pre\">Name</span></a></span>' in content)\n    assert '<span class=\"n\"><span class=\"pre\">Age</span></span>' in content\n    assert ('<p><strong>name</strong> (<a class=\"reference internal\" href=\"#Name\" '\n            'title=\"Name\"><em>Name</em></a>) \u2013 blah blah</p>' in content)\n    assert '<p><strong>age</strong> (<em>Age</em>) \u2013 blah blah</p>' in content\n\n", "def test_python_python_use_unqualified_type_names_inheritance(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"n\"><a class=\"reference internal\" href=\"#foo.SubClass\" '\n            'title=\"foo.SubClass\"><span class=\"pre\">SubClass</span></a></span>' in content)\n    assert '<span class=\"n\"><span class=\"pre\">foo.Sub.Age</span></span>' in content\n    assert ('<p><strong>name</strong> (<a class=\"reference internal\" href=\"#foo.SubClass\" '\n            'title=\"foo.SubClass\"><em>SubClass</em></a>) \u2013 blah blah</p>' in content)\n    assert '<p><strong>age</strong> (<em>foo.Sub.Age</em>) \u2013 blah blah</p>' in content\n", "def test_python_python_use_unqualified_type_names_enabled(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"n\"><span class=\"pre\">Name</span></span>' in content)\n    assert '<span class=\"n\"><span class=\"pre\">Age</span></span>' in content\n    assert ('<p><strong>name</strong> (<span class=\"pre\">Name</span>) \u2013 blah blah</p>' in content)\n    assert '<p><strong>age</strong> (<em>Age</em>) \u2013 blah blah</p>' in content\n"], "sample_65": ["    def test_i18n_with_trans_domain(self):\n        with self.settings(LANGUAGE_CODE='en-us'), override('fr'):\n            response = self.client.get('/jsi18n_multi_domains/')\n            self.assertContains(response, 'this is from djangojs domain')\n            self.assertContains(response, 'this is from apps domain')\n\n\n", "    def test_i18n_redirect_with_invalid_language(self):\n        with self.settings(\n            LANGUAGE_CODE='en',\n            LANGUAGES=[('en', 'English'), ('de', 'German')],\n        ):\n            response = self.client.get('/i18n/setlang/?language=invalid')\n            self.assertRedirects(response, '/i18n/')\n            self.assertEqual(self.client.cookies[settings.LANGUAGE_COOKIE_NAME].value, 'en')\n\n\n\n", "    def test_i18n_plural_forms_fallback(self):\n        \"\"\"\n        Ensure plural forms fallback correctly when a language only has one form.\n\n        Test that if a language doesn't have plural forms,\n        it falls back to the default number of plural forms.\n        \"\"\"\n        with override('en-uk'):\n            response = self.client.get('/jsi18n/')\n            self.assertEqual(\n                response.context['catalog']['{count} plural3'],\n                ['{count} plural3', '{count} plural3s']\n            )\n            self.assertEqual(response.context['catalog']['{count} plural2'], ['{count} plural2', '{count} plural2s'])\n        with override('es'):\n            response = self.client.get('/jsi18n/')\n            self.assertEqual(\n                response.context['catalog']['{count} plural3'],\n                ['{count} plural3', '{count} plural3s']\n            )\n            self.assertEqual(response.context['catalog']['{count} plural2'], ['{count} plural2', '{count} plural2s'])\n\n\n\n", "    def test_i18n_with_domain_override(self):\n        with self.settings(LANGUAGE_CODE='en-us'), override('fr'):\n            response = self.client.get('/jsi18n_domain_override/')\n            self.assertContains(response, 'OverrideDomain string')\n\n\n", "    def test_i18n_with_custom_locale_path(self):\n        view = JavaScriptCatalog.as_view()\n        request = RequestFactory().get('/')\n\n        with override_settings(LOCALE_PATHS=['/tmp/locale']):\n            with self.settings(LANGUAGE_CODE='de'):\n\n                response = self.client.get('/jsi18n/')\n                self.assertContains(response, \"Entfernen\")\n"], "sample_499": ["def test_legend_empty_with_loc():\n    fig, ax = plt.subplots()\n    ax.set_xlim([0, 10])\n    ax.set_ylim([0, 10])\n    ax.legend(title=\"My Legend\", loc='upper right')\n    assert ax.legend('upper right') is not None\n", "def test_legend_fontsize_inheritance():\n    fig, axes = plt.subplots(2, 2)\n    axes[0, 0].plot(range(10))\n    axes[0, 0].set_title('default')\n    legend0 = axes[0, 0].legend(fontsize=12)\n    axes[0, 1].plot(range(10))\n    axes[0, 1].set_title('inherited')\n    legend1 = axes[0, 1].legend()\n    axes[1, 0].plot(range(10))\n    axes[1, 0].set_title('explicit')\n    legend2 = axes[1, 0].legend(fontsize=14)\n    axes[1, 1].plot(range(10))\n    axes[1, 1].set_title('explicit override')\n    legend3 = axes[1, 1].legend(fontsize=10)\n\n    assert legend0.get_fontsize() == mpl.rcParams['legend.fontsize']\n    assert legend1.get_fontsize() == mpl.rcParams['legend.fontsize']\n    assert legend2.get_fontsize() == 14\n    assert legend3.get_fontsize() == 10\n\n", "def test_legend_with_duplicate_entry():\n    fig, ax = plt.subplots()\n    line1, = ax.plot([1, 2], [3, 4], label='test')\n    line2, = ax.plot([1, 2], [3, 4], label='test')\n    leg = ax.legend()\n    assert len(leg.get_texts()) == 1\n    assert line1 in leg.legendHandles\n    assert line2 in leg.legendHandles\n", "compilation error", "compilation error"], "sample_150": ["    def test_database_checks_filtered_by_databases(self, mocked_check):\n        mocked_check.return_value = (\n            \"Warning\", \n            \"database_check\", \n            [\"message\"],\n            None\n        )\n\n        # Call the function with a list of databases\n        check_database_backends(databases=['default'])\n\n        # Assert that the mock function was called with the correct arguments\n        mocked_check.assert_called_once_with('default')\n\n        # Call it again to check with multiple databases\n        check_database_backends(databases=['default', 'other'])\n        mocked_check.assert_called_twice()\n\n        # Assert that the mock function was called with the correct arguments\n        mocked_check.assert_any_call('default')\n        mocked_check.assert_any_call('other')\n", "    def test_check_migrations_no_migrations(self, mock_objects):\n        from django.db.migrations.executor import MigrationExecutor\n        from django.apps import apps\n\n        with mock.patch('django.core.management.base.CommandParser', autospec=True) as mock_parser:\n            mock_parser.create_parser.return_value = mock.MagicMock()\n            mock_parser.parse_args.return_value = mock.MagicMock()\n\n            executor = MigrationExecutor(connection.connections['default'])\n            executor.migration_plan = mock.MagicMock(return_value=[])\n            command = BaseCommand()\n            command.check_migrations()\n\n            self.assertFalse(command.stdout.write.called)\n\n", "    def test_database_checks_filter_by_list(self):\n        check_database_backends(databases=['default'])\n        mocked_check = mock.patch('django.db.backends.base.validation.BaseDatabaseValidation.check')\n        with mock.patch('django.core.checks.check_database_backends') as mock_check_backends:\n            check_database_backends(databases=['default'])\n            self.assertTrue(mock_check_backends.called)\n            mock_check_backends.assert_called_once_with(databases=['default'])\n", "    def test_check_migrations_unapplied_migrations(self, mock_migration_plan):\n        mock_migration_plan.return_value = [\n            ('myapp', True)\n        ]\n        with mock.patch('django.core.management.base.Command.stdout') as stdout:\n            from django.core.management.base import BaseCommand\n            command = BaseCommand()\n            command.check_migrations()\n            stdout_output = stdout.getvalue()\n        self.assertIn(\n            'You have 1 unapplied migration(s). Your project may not work properly until you apply the migrations for app(s): myapp.',\n            stdout_output\n        )\n        self.assertIn(\"Run 'python manage.py migrate' to apply them.\", stdout_output)\n\n\n\n", "    def test_check_migrations(self, mock_get_apps):\n        mock_get_apps.return_value = mock.Mock(\n            get_app_config=mock.Mock(return_value=mock.Mock())\n        )\n        from django.core.management.commands.migrate import Command\n\n        command = Command()\n        command.check_migrations()\n"], "sample_962": ["def test_stringify_type_hints_complex(arg: Any, expected: str) -> None:\n    assert stringify(arg) == expected\n", "def test_stringify_type_hints_TypeAlias():\n    from typing import TypeAlias  # type: ignore\n    MyAlias = TypeAlias[\"str\"]  \n    assert stringify(MyAlias) == \"str\"\n\n\n\n\n", "def test_restify_type_hints_complex(value: Any, expected: str):\n    assert restify(value) == expected\n\n\n\n", "def test_stringify_type_hints_simple_types(arg: str):\n    assert stringify(arg) == arg\n\n\n\n", "def test_restify_basic_types(value: Any, expected: str) -> None:\n    assert restify(value) == expected\n"], "sample_857": ["def test_prune_tree_complex_structure():\n    # Check pruning works on complex trees with multiple splits at the same depth\n    X = np.random.RandomState(0).random_sample((100, 5))\n    y = np.random.RandomState(0).randint(2, size=100)\n    clf = DecisionTreeClassifier(max_depth=5, random_state=0)\n    clf.fit(X, y)\n\n    info = clf.cost_complexity_pruning_path(X, y)\n    pruning_path = info.ccp_alphas\n    for ccp_alpha in pruning_path:\n        new_clf = DecisionTreeClassifier(max_depth=5, ccp_alpha=ccp_alpha,\n                                        random_state=0)\n        new_clf.fit(X, y)\n        assert_is_subtree(clf.tree_, new_clf.tree_)\n\n\n", "compilation error", "def test_sparse_input_with_missing_values():\n    for tree_type in SPARSE_TREES:\n        X_sparse = csc_matrix(\n            [[1, 2, np.nan],\n             [4, np.nan, 6]], dtype=np.float32\n        )\n        y = [0, 1]\n        for regressor in [DecisionTreeRegressor, ExtraTreeRegressor]:\n            with pytest.raises(ValueError):\n                regressor(random_state=0).fit(X_sparse, y)\n", "compilation error", "def test_prune_tree_raises_invalid_ccp_alpha_type():\n    clf = DecisionTreeClassifier()\n    msg = \"ccp_alpha must be a non-negative number\"\n\n    with pytest.raises(TypeError, match=msg):\n        clf.set_params(ccp_alpha=\"abc\")\n        clf.fit(X, y)\n\n\n\n"], "sample_1127": ["compilation error", "def test_is_subgroup():\n    import itertools\n    G = SymmetricGroup(3)\n    H = G.subgroup([Permutation(1, 2)])\n    assert G.is_subgroup(H) is False\n\n    H = G.subgroup([Permutation(0, 1), Permutation(0, 2)])\n    assert G.is_subgroup(H) is True\n\n\n    G = AlternatingGroup(3)\n    H = G.subgroup([Permutation(0, 1, 2)])\n    assert G.is_subgroup(H) is False\n\n    H = G.subgroup([Permutation(1)])\n    assert G.is_subgroup(H) is True\n\n    G = DihedralGroup(4)\n    for i in itertools.product([Permutation(0,1,2,3), Permutation(0, 3)(1, 2)], repeat=2):\n        H = G.subgroup(i)\n        assert G.is_subgroup(H) is True\n", "compilation error", "compilation error", "compilation error"], "sample_374": ["    def test_nested_prefetch_related_with_lookup(self):\n        with self.assertNumQueries(3):\n            books = Book.objects.filter(title__contains='Big').prefetch_related(\n                Prefetch(\n                    'author',\n                    Author.objects.prefetch_related(\n                        Prefetch(\n                            'address',\n                            Address.objects.filter(street__startswith='4'),\n                        )\n                    )\n                )\n            )\n            book = books.first()\n\n        with self.assertNumQueries(0):\n            self.assertEqual(book.author.address.street, '456 Elm St')\n", "    def test_nested_prefetch_related(self):\n        with self.assertNumQueries(4):\n            authors = Author.objects.prefetch_related(\n                Prefetch(\n                    'books',\n                    queryset=Book.objects.prefetch_related(\n                        Prefetch('favorite_authors')\n                    ),\n                )\n            )\n\n        with self.assertNumQueries(0):\n            for author in authors:\n                book = author.books.first()\n                self.assertCountEqual(book.favorite_authors.all(), [author])\n\n\n\n\n", "    def test_nested_prefetches_with_empty_related_managers(self):\n        with self.assertNumQueries(3):\n            rooms = Room.objects.prefetch_related(\n                Prefetch(\n                    'house',\n                    queryset=House.objects.prefetch_related(\n                        Prefetch(\n                            'rooms',\n                            queryset=Room.objects.none(),\n                        ),\n                    ),\n                ),\n            )\n        with self.assertNumQueries(0):\n            for room in rooms:\n                self.assertFalse(room.house.rooms.exists())\n", "compilation error", "    def test_nested_prefetch_with_flattened_results(self):\n        with self.assertNumQueries(4):\n            rooms = Room.objects.prefetch_related(\n                Prefetch(\n                    'house',\n                    queryset=House.objects.prefetch_related(\n                        'rooms'\n                    )\n                )\n            )\n        with self.assertNumQueries(0):\n            for room in rooms:\n                self.assertEqual(len(room.house.rooms.all()), 1)\n"], "sample_267": ["    def test_serialization_of_datetime_fields(self):\n        from django.db import models\n        class TestModel(models.Model):\n            date_field = models.DateField()\n            timestamp_field = models.DateTimeField()\n        obj = TestModel.objects.create(date_field=now(), timestamp_field=now())\n        serialized = obj.to_dict()\n        self.assertIn('date_field', serialized)\n        self.assertIn('timestamp_field', serialized)\n        deserialized_obj = TestModel(**serialized)\n        self.assertEqual(obj.date_field, deserialized_obj.date_field)\n        self.assertEqual(obj.timestamp_field, deserialized_obj.timestamp_field)\n", "    def test_transactions_in_inner_blocks(self):\n        with transaction.atomic():\n            with connection.transaction_manager().outer_transaction():\n                with self.assertRaises(NotSupportedError):\n                    transaction.atomic()\n", "    def test_foreign_key_constraints_with_autoincrement(self):\n        class Parent(models.Model):\n            id = models.BigAutoField(primary_key=True)\n        class Child(models.Model):\n            parent = models.ForeignKey(Parent, on_delete=models.CASCADE)\n        with self.assertRaises(IntegrityError):\n            child_instance = Child.objects.create()\n            child_instance.parent_id = 999999\n            child_instance.save() \n", "    def test_savepoint_rollback_failure(self):\n        with connection.cursor() as cursor:\n            cursor.execute('BEGIN')\n            cursor.execute('INSERT INTO backends_object (name) VALUES (%s)', ['test'])\n            cursor.execute('SAVEPOINT my_savepoint')\n            cursor.execute('INSERT INTO backends_object (name) VALUES (%s)', ['test2'])\n            with self.assertRaises(NotSupportedError):\n                cursor.execute('ROLLBACK TO my_savepoint')\n            cursor.execute('ROLLBACK')\n\n\n\n", "    def test_sqlite_datetime_trunc(self):\n        with connection.cursor() as cursor:\n            cursor.execute('''CREATE TABLE test_trunc (\n                id INTEGER PRIMARY KEY,\n                date DATE,\n                time TIME\n            )''')\n            cursor.execute('''INSERT INTO test_trunc (date, time) VALUES ('2023-10-27', '14:30:00'),\n                                                           ('2023-10-28', '10:15:30')''')\n\n        with connection.cursor() as cursor:\n            cursor.execute('''SELECT strftime('%Y-%m-%d %H:%M:%S', date) as date_trunc FROM test_trunc''')\n            row = cursor.fetchone()\n            self.assertEqual(row[0], '2023-10-27 14:00:00')\n            cursor.execute('''SELECT strftime('%Y-%m-%d %H:%M:%S', time) as time_trunc FROM test_trunc''')\n            row = cursor.fetchone()\n            self.assertEqual(row[0], '2023-10-27 14:00:00')\n\n\n\n"], "sample_384": ["    def test_related_object_creation_during_bulk_update(self):\n        parent_instance = RelatedObject.objects.create(f=1)\n        child_objects = []\n        for i in range(5):\n            child = SingleObject()\n            child_objects.append((parent_instance, child))\n        updated_parent = parent_instance\n        updated_parent.single = child_objects[0][1]\n        updated_parent.f = 2\n        RelatedObject.objects.bulk_update(\n            [updated_parent], fields=[\"single\", \"f\"]\n        )\n        # Assert that the related object was created and associated correctly\n        self.assertEqual(updated_parent.single.pk, child_objects[0][1].pk)\n        self.assertEqual(updated_parent.f, 2)\n        # Assert that related object wasn't created for other parent instances\n        for _, child in child_objects[1:]:\n            self.assertIsNone(child.pk)\n", "    def test_bulk_create_after_bulk_update(self):\n        objects = [Note(note=str(i), misc=str(i)) for i in range(10)]\n        Note.objects.bulk_create(objects)\n        ids = [obj.id for obj in objects]\n        for obj in Note.objects.filter(id__in=ids):\n            obj.note = \"updated\"\n        Note.objects.bulk_update(objects, [\"note\"])\n        for i in range(10):\n            obj = Note.objects.get(pk=ids[i])\n            self.assertEqual(obj.note, \"updated\")\n\n", "    def test_json_field_nesting(self):\n        objects = [\n            JSONFieldNullable.objects.create(json_field={\"data\": {\"a\": i}}) for i in range(10)\n        ]\n        for obj in objects:\n            obj.json_field[\"data\"][\"a\"] = obj.json_field[\"data\"][\"a\"] + 1\n        JSONFieldNullable.objects.bulk_update(objects, [\"json_field\"])\n        self.assertCountEqual(\n            JSONFieldNullable.objects.filter(json_field__contains={\"data\": {\"a\": 1}}), objects\n        )\n", "    def test_bulk_update_in_transaction(self):\n        with transaction.atomic():\n            with self.assertNumQueries(2):\n                for note in Note.objects.all():\n                    note.note = \"test-%s\" % note.id\n                Note.objects.bulk_update(Note.objects.all(), [\"note\"])\n\n        with self.assertNumQueries(0):\n            for note in Note.objects.all():\n                self.assertEqual(note.note, \"test-%s\" % note.id)\n", "    def test_bulk_update_with_model_cache(self):\n        # Create some notes\n        notes = [Note.objects.create(note=str(i), misc=str(i)) for i in range(10)]\n        # Update some notes using bulk_update\n        Note.objects.bulk_update(notes[:5], [\"note\"], batch_size=1)\n        # Check the updated fields \n        for i in range(5):\n            self.assertEqual(notes[i].note, \"test-%s\" % i)  \n        # Ensure the cache is cleared\n        Note.objects.get(pk=notes[5].pk) # force a query to clear the cache\n        # Update the remaining notes\n        Note.objects.bulk_update(notes[5:], [\"note\"], batch_size=1)\n        # Check the updated fields\n        for i in range(5, 10):\n            self.assertEqual(notes[i].note, \"test-%s\" % i) \n"], "sample_387": ["    def test_clear_filter(self):\n        from selenium.webdriver.common.by import By\n\n        self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_widgets_event_add\")\n        )\n\n        # Select a band\n        self.selenium.find_element(By.ID, \"lookup_id_main_band\").click()\n        self.wait_for_and_switch_to_popup()\n        link = self.selenium.find_element(By.LINK_TEXT, \"Bogey Blues\")\n        self.assertIn(\"/band/42/\", link.get_attribute(\"href\"))\n        link.click()\n        self.selenium.switch_to.window(self)\n\n        # Find the clear button\n        clear_button = self.selenium.find_element(By.ID, \"clear_id_main_band\")\n\n        # Click the clear button\n        clear_button.click()\n\n        # Check that the field is cleared\n        self.assertEqual(\n            self.selenium.find_element(By.ID, \"id_main_band\").get_attribute(\"value\"), \"\"\n        )\n\n", "    def test_clear_field_button(self):\n        from selenium.webdriver.common.by import By\n\n        self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_widgets_event_add\")\n        )\n\n        # Open the popup window and click on a band\n        self.selenium.find_element(By.ID, \"lookup_id_main_band\").click()\n        self.wait_for_and_switch_to_popup()\n        link = self.selenium.find_element(By.LINK_TEXT, \"Bogey Blues\")\n        self.assertIn(\"/band/42/\", link.get_attribute(\"href\"))\n        link.click()\n\n        # The field now contains the selected band's id\n        self.selenium.switch_to.window(self.selenium.current_window_handle)\n        self.wait_for_value(\"#id_main_band\", \"42\")\n\n        # Click the clear button\n        self.selenium.find_element(By.ID, \"clear_id_main_band\").click()\n\n        # The field is now empty\n        self.assertEqual(\n            self.selenium.find_element(By.ID, \"id_main_band\").get_attribute(\"value\"), \"\"\n        )\n\n", "    def test_initial_values(self):\n        from selenium.webdriver.common.by import By\n\n        Event.objects.create(id=1, name=\"Concert\", band=Band.objects.get(id=42))\n        self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_widgets_event_change\", args=(1,))\n        )\n        main_window = self.selenium.current_window_handle\n        # Check that the field is populated with the correct value\n        self.wait_for_value(\"#id_main_band\", \"42\")\n\n        # Open the popup window\n        self.selenium.find_element(By.ID, \"lookup_id_main_band\").click()\n        self.wait_for_and_switch_to_popup()\n\n        # The selected band should be checked in the popup window\n        band_link = self.selenium.find_element(By.LINK_TEXT, \"Bogey Blues\")\n        self.assertEqual(band_link.get_attribute(\"class\"), \"selected\")\n\n\n\n", "    def test_removal_of_selected_value(self):\n        from selenium.webdriver.common.by import By\n\n        self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_widgets_event_add\")\n        )\n        main_window = self.selenium.current_window_handle\n\n        # Select a band\n        self.selenium.find_element(By.ID, \"lookup_id_main_band\").click()\n        self.wait_for_and_switch_to_popup()\n        link = self.selenium.find_element(By.LINK_TEXT, \"Bogey Blues\")\n        self.assertIn(\"/band/42/\", link.get_attribute(\"href\"))\n        link.click()\n\n        # The field now contains the selected band's id\n        self.selenium.switch_to.window(main_window)\n        self.wait_for_value(\"#id_main_band\", \"42\")\n\n        # Remove the selected band\n        self.selenium.find_element(By.ID, \"id_main_band\").click()\n        # There should be a \"X\" button next to each choice\n        self.selenium.find_element(By.CSS_SELECTOR, \"#id_main_band > .related-widget-item > .close\").click()\n\n        # The field should be empty\n        self.assertEqual(\n            self.selenium.find_element(By.ID, \"id_main_band\").get_attribute(\"value\"), \"\"\n        )\n\n", "    def test_filter_field(self):\n        from selenium.webdriver.common.by import By\n\n        self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_widgets_event_add\")\n        )\n\n        # Check that the filter field is available and works\n        filter_input = self.selenium.find_element(By.ID, \"id_main_band_filter\")\n        filter_input.send_keys(\"Boge\")\n        self.assertCountSeleniumElements(\n            \"id_main_band_filter_listbox > option\", 1\n        )\n        filter_input.send_keys(Keys.ARROW_DOWN)\n\n\n"], "sample_537": ["def test_psd_onesided_norm_detrend():\n    u = np.array([0.1, 0.2, 0.3, 0.4, 0.3, 0.2, 0.1])\n    dt = 1.0\n    Su = np.abs(np.fft.fft(u) * dt)**2 / (dt * u.size)\n    P, f = mlab.psd(u, NFFT=u.size, Fs=1/dt, window=mlab.window_none,\n                    detrend=mlab.detrend_linear, noverlap=0, pad_to=None,\n                    scale_by_freq=None,\n                    sides='onesided')\n    Su_1side = np.append([Su[0]], Su[1:4] + Su[4:][::-1])\n    assert_allclose(P, Su_1side, atol=1e-06)\n", "    def test_psd_onesided_norm_complex(self):\n        u = np.array([0, 1, 2, 3, 1, 2, 1]) + 1j * np.array([0, 1, 2, 3, 1, 2, 1])\n        dt = 1.0\n        Su = np.abs(np.fft.fft(u) * dt)**2 / (dt * u.size)\n        P, f = mlab.psd(u, NFFT=u.size, Fs=1/dt, window=mlab.window_none,\n                        detrend=mlab.detrend_none, noverlap=0, pad_to=None,\n                        scale_by_freq=None,\n                        sides='onesided')\n        Su_1side = np.append([Su[0]], Su[1:4] + Su[4:][::-1])\n        assert_allclose(P, Su_1side, atol=1e-06)\n", "    def test_psd_trigonometry_norm(self):\n        u = np.array([0, 1, 2, 3, 1, 2, 1])\n        dt = 1.0\n        Su = np.abs(np.fft.fft(u) * dt)**2 / (dt * u.size)\n        P, f = mlab.psd(u, NFFT=u.size, Fs=1/dt, window=mlab.window_none,\n                        detrend=mlab.detrend_none, noverlap=0, pad_to=None,\n                        scale_by_freq=True,\n                        sides='onesided')\n        Su_1side = np.append([Su[0]], Su[1:4] + Su[4:][::-1])\n        assert_allclose(P, Su_1side, atol=1e-06)\n", "    def test_csd_onesided_norm(self):\n        u = np.array([0, 1, 2, 3, 1, 2, 1])\n        v = np.array([1, 2, 3, 2, 1, 0, 0])\n        dt = 1.0\n        Suv = np.abs(np.fft.fft(u) * np.fft.fft(v).conj() * dt)**2 / (dt * u.size)\n        S, f = mlab.csd(u, v, NFFT=u.size, Fs=1/dt, window=mlab.window_none,\n                        detrend=mlab.detrend_none, noverlap=0, pad_to=None,\n                        scale_by_freq=None,\n                        sides='onesided')\n        Suv_1side = np.append([Suv[0]], Suv[1:4] + Suv[4:][::-1])\n        assert_allclose(S, Suv_1side, atol=1e-06)\n", "    def test_psd_onesided_norm_complex(self):\n        u = np.array([0, 1, 2, 3, 1, 2, 1]) + 1j * np.array([0, 0.5, 1, 0.5, 0, -0.5, -1])\n        dt = 1.0\n        Su = np.abs(np.fft.fft(u) * dt)**2 / (dt * u.size)\n        P, f = mlab.psd(u, NFFT=u.size, Fs=1/dt, window=mlab.window_none,\n                        detrend=mlab.detrend_none, noverlap=0, pad_to=None,\n                        scale_by_freq=None, sides='onesided')\n        Su_1side = np.append([Su[0]], Su[1:4] + Su[4:][::-1])\n        assert_allclose(P, Su_1side, atol=1e-06)\n"], "sample_922": ["def test_module_index_with_options(app):\n    text = (\".. py:module:: docutils\\n\"\n            \".. py:module:: sphinx\\n\"\n            \".. py:module:: sphinx\\n\"\n            \"   :deprecated:\\n\")\n    restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    assert index.generate() == (\n        [('d', [IndexEntry('docutils', 0, 'index', 'module-docutils', '', '', '')]),\n         ('s', [IndexEntry('sphinx', 1, 'index', 'module-sphinx', '', '', '')])],\n        False\n    )    \n", "def test_module_index_order(app):\n    text = (\".. py:module:: module3\\n\"\n            \".. py:module:: module1\\n\"\n            \".. py:module:: module2\\n\")\n    restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    assert index.generate() == (\n        [('m', [IndexEntry('module1', 1, 'index', 'module-module1', '', '', ''),\n                IndexEntry('module2', 2, 'index', 'module-module2', '', '', ''),\n                IndexEntry('module3', 0, 'index', 'module-module3', '', '', '')])],\n        False\n    )\n", "def test_module_index_inheritance(app):\n    text = \"\"\"", "def test_module_index_duplicates(app):\n    text = (\".. py:module:: docutils\\n\"\n            \".. py:module:: docutils\\n\"\n            \".. py:module:: sphinx\\n\"\n            \".. py:module:: sphinx\\n\")\n    restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    assert index.generate() == (\n        [('d', [IndexEntry('docutils', 0, 'index', 'module-docutils', '', '', '')]),\n         ('s', [IndexEntry('sphinx', 0, 'index', 'module-sphinx', '', '', '')])],\n        False\n    )\n\n", "def test_module_index_with_options(app):\n    text = (\".. py:module:: docutils\\n\"\n            \"   :members:\\n\"\n            \"   :inherited-members:\\n\"\n            \".. py:module:: sphinx\\n\")\n    restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    # Assertions to ensure the options are applied\n    assert index.generate() == (\n        ...,  # The actual index will vary, but it should include:\n        # - Entries for docutils module and its members\n        # - Entries for sphinx module and its inherited members\n    )  \n"], "sample_591": ["    def test_merge_with_different_dims(self):\n        ds1 = xr.Dataset({\"x\": ((\"y\", \"z\"), [1, 2])})\n        ds2 = xr.Dataset({\"y\": ((\"x\", \"z\"), [3, 4])})\n        with pytest.raises(ValueError):\n            ds1.merge(ds2)\n\n\n", "    def test_merge_compat_with_different_dims(self):\n        ds1 = xr.Dataset({\"a\": (\"x\", [1, 2])})\n        ds2 = xr.Dataset({\"b\": (\"y\", [3, 4])})\n\n        with pytest.raises(xr.MergeError):\n            ds1.merge(ds2, compat=\"equals\")\n\n        with raises_regex(ValueError, \"should be coordinates or not\"):\n            ds1.merge(ds2, compat=\"identical\")\n\n        with pytest.raises(xr.MergeError):\n            ds1.merge(ds2, compat=\"no_conflicts\")\n\n\n        # Test merge with broadcasting\n        ds1 = xr.Dataset({\"a\": (\"x\", [1, 2])})\n        ds2 = xr.Dataset({\"b\": (\"y\", [3, 4])})\n\n        merged = ds1.merge(ds2, compat=\"broadcast_equals\")\n        assert merged.dims == (\"x\", \"y\")\n        assert merged.data_vars == {\"a\": (\"x\", [1, 2]), \"b\": (\"y\", [3, 4])}\n\n", "    def test_merge_override(self):\n        ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]})\n        ds2 = xr.Dataset({\"a\": (\"x\", [2, 3]), \"x\": [1, 2]})\n        expected = xr.Dataset({\"a\": (\"x\", [2, 3]), \"x\": [0, 1, 2]})\n        assert expected.identical(ds1.merge(ds2, compat=\"override\"))\n        assert expected.identical(ds2.merge(ds1, compat=\"override\"))\n", "    def test_merge_complex_with_datetimelike_indexes(self):\n        # Test merging Datasets with datetimelike indexes\n        time1 = pd.to_datetime([\"2023-03-01\", \"2023-03-02\"])\n        time2 = pd.to_datetime([\"2023-03-02\", \"2023-03-03\"])\n\n        ds1 = xr.Dataset({\"var1\": ([\"time\"], [1, 2]), \"time\": time1})\n        ds2 = xr.Dataset({\"var2\": ([\"time\"], [3, 4]), \"time\": time2})\n\n        expected = xr.Dataset(\n            {\"var1\": ([\"time\"], [1, 2, np.nan]), \"var2\": ([\"time\"], [np.nan, 3, 4])},\n            {\"time\": pd.to_datetime([\"2023-03-01\", \"2023-03-02\", \"2023-03-03\"])},\n        )\n\n        assert expected.identical(ds1.merge(ds2))\n        assert expected.identical(ds2.merge(ds1))\n", "    def test_merge_coords(self):\n        ds1 = xr.Dataset({\"x\": ((\"y\",), [1, 2, 3]), \"y\": [4, 5, 6]})\n        ds2 = xr.Dataset({\"x\": ((\"y\",), [4, 5, 6]), \"y\": [7, 8, 9]})\n        expected = xr.Dataset({\"x\": ((\"y\",), [1, 2, 3, 4, 5, 6]), \"y\": [4, 5, 6, 7, 8, 9]})\n        assert expected.identical(ds1.merge(ds2))\n        assert expected.identical(ds2.merge(ds1))\n\n        with pytest.raises(ValueError):\n            ds1.merge(ds2, compat=\"no_conflicts\")\n\n        ds1 = xr.Dataset({\"x\": ((\"y\",), [1, 2, 3]), \"y\": [4, 5, 6]})\n        ds2 = xr.Dataset({\"x\": ((), [4, 5, 6]), \"y\": [7, 8, 9]})\n        with pytest.raises(ValueError):\n            ds1.merge(ds2)\n\n        ds1 = xr.Dataset({\"x\": ((\"y\",), [1, 2, 3]), \"y\": (\"x\", [4, 5, 6])})\n        ds2 = xr.Dataset({\"x\": ((\"y\",), [4, 5, 6]), \"y\": ((), [7, 8, 9])})\n        with pytest.raises(ValueError):\n            ds1.merge(ds2)\n"], "sample_173": ["    def test_sql_flush_with_tables(self):\n        author = Author.objects.create(name='Test Author')\n        book = Book.objects.create(title='Test Book', author=author)\n        with self.assertRaises(NotImplementedError) as context:\n            connection.ops.sql_flush(no_style(), [Author, Book])\n        self.assertRegex(str(context.exception), r'subclasses of BaseDatabaseOperations must provide a sql_flush() method')\n        Author.objects.all().delete()\n        Book.objects.all().delete() \n", "    def test_sql_flush_with_tables(self):\n        Author.objects.create(name='John Doe')\n        Book.objects.create(title='Django Book', author=Author.objects.first())\n        with self.assertRaises(NotImplementedError) as cm:\n            connection.ops.sql_flush(no_style(), [\n                (Author._meta.db_table, Author._meta.fields),\n                (Book._meta.db_table, Book._meta.fields)\n            ])\n        self.assertIn(\n            'subclasses of BaseDatabaseOperations must provide a sql_flush() method',\n            str(cm.exception)\n        )\n", "    def test_sql_flush_with_tables(self):\n        with transaction.atomic():\n            Author.objects.create(name='John Doe')\n            Book.objects.create(title='Example Book', author=Author.objects.first())\n        sql_list = connection.ops.sql_flush(no_style(), ['django_content_type', 'django_migrations', \n                                                          'auth_group', 'auth_group_permissions', 'auth_permission', \n                                                          'auth_user', 'auth_user_groups', 'auth_user_permissions', \n                                                          'session', 'contenttypes', 'auth', 'django_site'])\n        self.assertTrue(all(stmt.startswith('TRUNCATE TABLE') for stmt in sql_list))\n\n\n\n", "    def test_sql_flush_with_tablespace(self):\n        self.assertGreater(len(connection.ops.sql_flush(no_style(), [\n            Author,\n            Book,\n        ])), 0)\n", "    def test_sql_flush_with_tables(self):\n        Author.objects.create(name='John')\n        Book.objects.create(title='Test Book', author=Author.objects.get(name='John'))\n        with transaction.atomic():\n            connection.ops.sql_flush(no_style(), ['django_content_type', 'auth_group', 'auth_group_permissions', 'auth_permission', 'django_session', 'auth_user', 'auth_user_groups', 'auth_user_permissions', 'contenttypes', 'sessions', 'users', 'books', 'authors'])\n        self.assertEqual(Author.objects.count(), 0)\n        self.assertEqual(Book.objects.count(), 0)\n\n"], "sample_750": ["def test_omp_precompute_with_gram():\n    X_pre = X.copy()\n    Xy_pre = np.dot(X_pre.T, y)\n    omp_pre = orthogonal_mp(X_pre, y, n_nonzero_coefs=n_nonzero_coefs,\n                            precompute=True)\n    omp_no_pre = orthogonal_mp(X_pre, y, n_nonzero_coefs=n_nonzero_coefs)\n    assert_array_almost_equal(omp_pre, omp_no_pre)\n\n\n\n", "def test_omp_path_n_features():\n    # Test that the behavior of orthogonal_mp with varying n_features is as expected\n    n_features_list = [10, 20, 30]\n    for n_features in n_features_list:\n        X_new = X[:, :n_features]\n        y_new = y[:, :n_targets]\n        gamma = orthogonal_mp(X_new, y_new, n_nonzero_coefs=5, return_path=True)\n        assert_equal(gamma.shape, (n_features, n_targets, 5))\n", "def test_omp_path_gram_with_n_iter():\n    path, n_iter = orthogonal_mp_gram(G, Xy, n_nonzero_coefs=5,\n                                     return_n_iter=True, precompute=True)\n    assert_equal(path.shape, (n_features, n_targets, 5))\n    assert_equal(n_iter.shape, (n_targets,))\n", "def test_precompute_with_gram_and_gram_path():\n    idx, = gamma[:, 0].nonzero()\n    G_path, Xy_path = np.dot(X.T, X), np.dot(X.T, y)\n    gamma_gram_path = orthogonal_mp_gram(G, Xy, n_nonzero_coefs=5,\n                                        return_path=True, precompute=True)\n    gamma_gram_path_no_precompute = orthogonal_mp_gram(G_path, Xy_path,\n                                                      n_nonzero_coefs=5,\n                                                      return_path=True)\n    assert_array_equal(np.flatnonzero(gamma_gram_path[:, :, -1]), idx)\n    assert_array_equal(np.flatnonzero(gamma_gram_path_no_precompute[:, :, -1]), idx)\n", "def test_omp_path_gram_with_tol():\n    path = orthogonal_mp_gram(G, Xy, n_nonzero_coefs=5,\n                    tol=0.1, return_path=True)\n    last = orthogonal_mp_gram(G, Xy, n_nonzero_coefs=5, tol=0.1,\n                    return_path=False)\n    assert_equal(path.shape, (n_features, n_targets, 5))\n    assert_array_almost_equal(path[:, :, -1], last)\n"], "sample_1108": ["def test_prime_factorization():\n    assert prime_factorization(1) == [1]\n    assert prime_factorization(2) == [2]\n    assert prime_factorization(3) == [3]\n    assert prime_factorization(4) == [2, 2]\n    assert prime_factorization(5) == [5]\n    assert prime_factorization(6) == [2, 3]\n    assert prime_factorization(7) == [7]\n    assert prime_factorization(8) == [2, 2, 2]\n    assert prime_factorization(9) == [3, 3]\n    assert prime_factorization(10) == [2, 5]\n    assert prime_factorization(12) == [2, 2, 3]\n    assert prime_factorization(15) == [3, 5]\n    assert prime_factorization(20) == [2, 2, 5]\n    assert prime_factorization(21) == [3, 7]\n    assert prime_factorization(25) == [5, 5]\n    assert prime_factorization(30) == [2, 3, 5]\n    assert prime_factorization(36) == [2, 2, 3, 3]\n    assert prime_factorization(42) == [2, 3, 7]\n    assert prime_factorization(49) == [7, 7]\n    assert prime_factorization(50) == [2, 5, 5]\n    assert prime_factorization(60) == [2, 2, 3, 5]\n    assert prime_factorization(72) == [2, 2, 2, 3, 3]\n    assert prime_factorization(100) == [2, 2, 5, 5]\n    assert prime_factorization(101) == [101]\n\n\n\n", "compilation error", "compilation error", "compilation error", "def test_next_permutation():\n    perm = list(range(3))\n    next_perm = next_permutation(perm)\n    assert next_perm == [1, 2, 0]\n    perm = list(range(3))\n    next_perm = next_permutation(perm)\n    assert next_perm == [1, 2, 0]\n\n\n"], "sample_618": ["def test_polyval_invalid_inputs():\n    x = xr.DataArray([1, 2, 3], dims=\"x\")\n    coeffs = xr.DataArray([2, 3, 4], dims=\"degree\", coords={\"degree\": [0, 1, 2]})\n    with pytest.raises(TypeError):\n        xr.polyval(x, \"not an array\")\n    with pytest.raises(ValueError):\n        xr.polyval(x, coeffs.where(coeffs > 5))\n    with pytest.raises(ValueError):\n        xr.polyval(x, coeffs.fillna(0))\n    with pytest.raises(ValueError):\n        xr.polyval(x, coeffs.astype(\"string\"))\n\n    with pytest.raises(ValueError):\n        xr.polyval(coeffs, x)\n", "def test_polyval_scalar(use_dask: bool) -> None:\n    if use_dask:\n        if not has_dask:\n            pytest.skip(\"requires dask\")\n    x = xr.DataArray([1, 2, 3], dims=\"x\")\n    coeffs = 2\n    actual = xr.polyval(x, coeffs)\n    expected = xr.DataArray([2, 4, 6], dims=\"x\")\n    assert_identical(expected, actual)\n\n\n\n\n", "def test_polyval_empty_arrays():\n    x = xr.DataArray([], dims=\"x\")\n    coeffs = xr.DataArray([], dims=\"degree\")\n\n    actual = xr.polyval(x, coeffs)\n    assert actual.size == 0\n\n    coeffs = xr.DataArray([2, 3, 4], dims=\"degree\")\n\n    actual = xr.polyval(x, coeffs)\n    assert actual.size == 0 \n\n    x = xr.DataArray([1, 2, 3], dims=\"x\")\n    coeffs = xr.DataArray([], dims=\"degree\")\n    \n    actual = xr.polyval(x, coeffs)\n    assert actual.size == 3 and (actual.data == 0).all()\n", "def test_polyval_scalar_coeffs():\n    x = xr.DataArray([1, 2, 3], dims=\"x\")\n    coeffs = 2\n    actual = xr.polyval(x, coeffs)\n    expected = xr.DataArray([2, 4, 6], dims=\"x\")\n    assert_identical(expected, actual)\n", "def test_polyval_empty():\n    x = xr.DataArray([], dims=\"x\")\n    coeffs = xr.DataArray([], dims=\"degree\")\n    with raise_if_dask_computes():\n        actual = xr.polyval(coord=x, coeffs=coeffs)\n    assert actual.size == 0\n\n    x = xr.DataArray([1, 2, 3], dims=\"x\")\n    coeffs = xr.DataArray([], dims=\"degree\")\n    with raise_if_dask_computes():\n        actual = xr.polyval(coord=x, coeffs=coeffs)\n    assert actual.size == 0\n\n    x = xr.DataArray([], dims=\"x\")\n    coeffs = xr.DataArray([2, 3, 4], dims=\"degree\", coords={\"degree\": [0, 1, 2]})\n    with raise_if_dask_computes():\n        actual = xr.polyval(coord=x, coeffs=coeffs)\n    assert actual.size == 0\n\n"], "sample_308": ["    def test_datetime_timezone_ambiguities(self):\n        # Test for datetimes with ambiguities in the timezone.\n        dt = datetime(2015, 10, 25, 2, 30, 0, tzinfo=get_default_timezone())\n        \n        # Formatting may not reflect timezone information for ambiguous datetimes\n        self.assertEqual(format(dt, 'O'), '')\n        self.assertEqual(format(dt, 'T'), '')\n        self.assertEqual(format(dt, 'e'), '')\n        self.assertEqual(format(dt, 'Z'), '')\n", "    def test_invalid_date_format_specifiers(self):\n        dt = datetime(1984, 8, 7, 22, 00)\n\n        for specifier in ['a', 'A', 'f', 'g', 'G', 'h', 'H', 'i', 'P', 's', 'u', 't', 'w', 'W', 'y', 'Y', 'z']:\n            msg = (\n                \"The format for date objects may not contain time-related \"\n                \"format specifiers (found '%s').\" % specifier\n            )\n            with self.assertRaisesMessage(TypeError, msg):\n                dateformat.format(dt, specifier)\n", "    def test_ISO8601_with_timezone(self):\n\n        # Test ISO 8601 format with timezone information\n        my_birthday = datetime(1979, 7, 8, 22, 00, tzinfo=get_fixed_timezone(-5))\n        self.assertEqual(dateformat.format(my_birthday, 'c'), '1979-07-08T22:00:00-05:00')\n", "    def test_i_format_leading_zeros(self):\n        self.assertEqual(dateformat.format(datetime(2009, 5, 16, 1, 5, 30), 'i'), '05')\n        self.assertEqual(dateformat.format(datetime(2009, 5, 16, 5, 0, 30), 'i'), '00')\n", "    def test_f_format(self):\n        my_birthday = datetime(1979, 7, 8, 15, 30)\n        self.assertEqual(dateformat.format(my_birthday, 'f'), '03:30')\n"], "sample_135": ["    def test_format_non_existent_format_specifier(self):\n        d = datetime(2023, 10, 26, 10, 30, 0)\n        with self.assertRaisesMessage(ValueError, r\"Unknown format code 'X'\"):\n            dateformat.format(d, 'X')\n\n\n\n", "    def test_timezone_naive_datetime(self):\n        dt = datetime(2009, 5, 16, 5, 30, 30)\n        self.assertEqual(datetime.fromtimestamp(int(format(dt, 'U'))), dt)\n\n", "    def test_timezone_naive_datetime_format(self):\n        dt = datetime(2023, 10, 26, 10, 30)\n        self.assertEqual(dateformat.format(dt, 'O'), '')\n        self.assertEqual(dateformat.format(dt, 'T'), '')\n        self.assertEqual(dateformat.format(dt, 'e'), '')\n        self.assertEqual(dateformat.format(dt, 'Z'), '')\n", "    def test_iso8601_with_timezone(self):\n        tz = get_fixed_timezone(-510)\n        dt = make_aware(datetime(2009, 5, 16, 5, 30, 30), tz)\n        self.assertEqual(dateformat.format(dt, 'c'), '2009-05-16T05:30:30-05:30')\n", "    def test_iso8601_format(self):\n        dt = datetime(2023, 10, 26, 10, 30, 0, 123456)\n        self.assertEqual(dateformat.format(dt, 'c'), '2023-10-26T10:30:00.123456')\n"], "sample_1051": ["def test_custom_styles():\n    styles = [(Symbol, {'color': 'green', 'shape': 'diamond'}),\n              (Basic, {'color': 'blue', 'shape': 'ellipse'})]\n    text = dotprint(x + 2, styles=styles)\n    assert 'color=\"green\"' in text\n    assert 'shape=\"diamond\"' in text\n    assert 'color=\"blue\"' in text\n    assert 'shape=\"ellipse\"' in text\n", "def test_custom_styles():\n    styles = [(Basic, {'color': 'green', 'shape': 'diamond'}),\n              (Expr, {'color': 'red'})]\n    text = dotprint(x+2, styles=styles)\n    assert \"color=\\\"green\\\"\" in text\n    assert \"shape=\\\"diamond\\\"\" in text\n    assert \"color=\\\"red\\\"\" in text\n", "def test_custom_styles():\n    styles = [(Basic, {'shape': 'circle'}), (Symbol, {'color': 'green'})]\n    text = dotprint(x + 2, styles=styles)\n    assert '\"Symbol(\\'x\\')\" [\"color\"=\"green\", \"shape\"=\"circle\", \"label\"=\"x\"]' in text\n    assert '\"Add(Integer(2), Symbol(\\'x\\'))\" [\"color\"=\"black\", \"label\"=\"Add\", \"shape\"=\"circle\"]' in text \n", "def test_custom_styles():\n    styles = [(Basic, {'color': 'red'}), (Symbol, {'shape': 'circle'})]\n    text = dotprint(x + 2, styles=styles)\n    assert 'color=\"red\"' in text\n    assert 'shape=\"circle\"' in text\n", "def test_non_default_styles():\n    styles = [\n      (Basic, {'color': 'green', 'shape': 'circle'}),\n      (Expr, {'color': 'purple', 'shape': 'rectangle'})\n    ]\n    text = dotprint(x + 2, styles=styles)\n    assert 'color=\"green\"' in text\n    assert 'shape=\"circle\"' in text\n    assert 'color=\"purple\"' in text\n    assert 'shape=\"rectangle\"' in text\n"], "sample_125": ["    def test_path_argument(self):\n        response = HttpResponse()\n        response.delete_cookie('c', path='/admin')\n        cookie = response.cookies['c']\n        self.assertEqual(cookie['path'], '/admin')\n", "    def test_domain_and_path(self):\n        response = HttpResponse()\n        response.delete_cookie('c', path='/admin', domain='example.com')\n        cookie = response.cookies['c']\n        self.assertEqual(cookie['path'], '/admin')\n        self.assertEqual(cookie['domain'], 'example.com')\n", "    def test_path_and_domain(self):\n        response = HttpResponse()\n        response.set_cookie('c', value='old-value', path='/admin', domain='example.com')\n        response.delete_cookie('c', path='/admin', domain='example.com')\n        cookie = response.cookies.get('c')\n        self.assertIsNone(cookie)\n", "    def test_delete_cookie_with_path(self):\n        response = HttpResponse()\n        response.set_cookie('c', path='/path/')\n        response.delete_cookie('c', path='/path/')\n        cookie = response.cookies.get('c')\n        self.assertEqual(cookie['path'], '/path/')\n        self.assertEqual(cookie['expires'], 'Thu, 01 Jan 1970 00:00:00 GMT')\n        self.assertEqual(cookie['max-age'], 0)\n        self.assertEqual(cookie['secure'], '')\n        self.assertEqual(cookie['domain'], '')\n", "    def test_path_and_domain(self):\n        response = HttpResponse()\n        response.delete_cookie('c', path='/admin', domain='example.com')\n        cookie = response.cookies['c']\n        self.assertEqual(cookie['path'], '/admin')\n        self.assertEqual(cookie['domain'], 'example.com')\n"], "sample_1121": ["def test_issue_16134():\n    x, y = symbols('x y')\n    assert (x**-1).subs(x, 0) == oo\n    assert (x**-1).subs(x, 1) == 1\n    assert (x**-1).subs(x, -1) == -1\n", "def test_issue_15357():\n    a, b = symbols('a b')\n    assert Pow(a + 1, -1 / 2, evaluate=False).expand().is_Mul\n    assert Pow(a + 1, -1 / 2, evaluate=False).is_positive is None \n", "compilation error", "compilation error", "compilation error"], "sample_1019": ["compilation error", "def test_issue_8263_2():\n    F, G = symbols('F, G', commutative=False, cls=Function)\n    x, y = symbols('x, y')\n    expr, dummies, _ = _mask_nc(F(x)*G(y) - G(y)*F(x) + F(x)*F(y) - F(y)*F(x))\n    for v in dummies.values():\n        assert not v.is_commutative\n    assert not expr.is_zero\n", "def test_issue_8657():\n    from sympy.functions import exp, sin\n    x = symbols('x')\n    assert factor_nc(exp(x) - sin(x)).is_commutative is False\n    assert factor_nc(x*exp(x) - x*sin(x)).is_commutative is False\n\n\n", "compilation error", "compilation error"], "sample_1114": ["def test_issue_17858_continued():\n    assert Range(1, 10).is_subset(Range(0, 10))\n    assert Range(1, 10).intersection(Range(2, 9)) == Range(2, 9)\n    assert Range(1, 10).union(Range(11, 20)) == Range(1, 20)\n\n\n\n", "compilation error", "compilation error", "compilation error", "def test_issue_17858_b():\n    assert 1 in Range(-2, 3)\n    assert 0 not in Range(-2, 1)\n    assert 2 not in Range(-1, 0)\n"], "sample_663": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_505": ["def test_date2num_with_tzinfo():\n    dt = datetime.datetime(2020, 1, 1, 12, tzinfo=datetime.timezone.utc)\n    num = mdates.date2num(dt)\n    assert isinstance(num, float)\n    assert mdates.num2date(num, tz=datetime.timezone.utc) == dt\n\n\n\n", "def test_WeekdayLocator_with_tzinfo():\n    tz = dateutil.tz.gettz('Europe/London')\n    dt = datetime.datetime(2023, 10, 26, 10, 0, tzinfo=tz)\n    locator = mdates.WeekdayLocator(byweekday=mdates.TUESDAY, tzinfos=[tz])\n    locator.create_dummy_axis()\n    locator.axis.set_view_interval(mdates.date2num(dt - datetime.timedelta(days=10)),\n                                   mdates.date2num(dt + datetime.timedelta(days=10)))\n    expected_dates = [dt - datetime.timedelta(days=i) for i in range(10)]\n    assert list(map(str, mdates.num2date(locator(), tz=tz))) == [str(d) for d in expected_dates]\n", "compilation error", "def test_DayLocator_non_integer_intervals():\n    with pytest.raises(ValueError):\n        mdates.DayLocator(interval=1.5)\n", "compilation error"], "sample_1110": ["def test_jacv():\n    from sympy import besselj\n\n    expr = besselj(x, 0)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'scipy.special.jv(0, x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == '  # Not supported in Python with NumPy:\\n  # besselj\\nbesselj(x, 0)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == '  # Not supported in Python:\\n  # besselj\\nbesselj(x, 0)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr) == 'mpmath.jv(0, x)'\n", "def test_sympy_printing_matrix_operations():\n    from sympy import MatrixSymbol, Matrix, eye, zeros, ones, diag, BlockMatrix\n\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 3, 3)\n\n    prntr = SymPyPrinter()\n\n    # Matrix addition\n    expr = A + B\n    assert prntr.doprint(expr) == 'A + B'\n\n    # Matrix subtraction\n    expr = A - B\n    assert prntr.doprint(expr) == 'A - B'\n\n    # Matrix multiplication\n    expr = A * B\n    assert prntr.doprint(expr) == 'A * B'\n\n    # Transpose\n    expr = A.transpose()\n    assert prntr.doprint(expr) == 'A.T'\n\n    # Inverse\n    expr = A.inv()\n    assert prntr.doprint(expr) == 'A.inv()'\n\n    # Identity matrix\n    expr = eye(3)\n    assert prntr.doprint(expr) == 'Matrix([[1, 0, 0], [0, 1, 0], [0, 0, 1]])'\n\n    # Zero matrix\n    expr = zeros(2, 3)\n    assert prntr.doprint(expr) == 'Matrix([[0, 0, 0], [0, 0, 0]])'\n\n    # Ones matrix\n    expr = ones(2, 3)\n    assert prntr.doprint(expr) == 'Matrix([[1, 1, 1], [1, 1, 1]])'\n\n    # Diagonal matrix\n    expr = diag(A)\n    assert prntr.doprint(expr) == 'Matrix([[A[0, 0], 0, 0], [0, A[1, 1], 0], [0, 0, 0]])'\n\n    # Block matrix\n    expr = BlockMatrix([[A, B], [zeros(2, 2), C]])\n    assert prntr.doprint(expr) == 'BlockMatrix(((A, B), (Matrix([[0, 0], [0, 0", "def test_issue_17543():\n    from sympy import cos, sin, tan\n    from sympy import S\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(cos(S.Pi/2)) == 'math.cos(math.pi/2)'\n    assert prntr.doprint(sin(S.Pi/2)) == 'math.sin(math.pi/2)'\n    assert prntr.doprint(tan(S.Pi/4)) == 'math.tan(math.pi/4)'\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(cos(S.Pi/2)) == 'scipy.special.cos(math.pi/2)'\n    assert prntr.doprint(sin(S.Pi/2)) == 'scipy.special.sin(math.pi/2)'\n    assert prntr.doprint(tan(S.Pi/4)) == 'scipy.special.tan(math.pi/4)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(cos(S.Pi/2)) == 'numpy.cos(numpy.pi/2)'\n    assert prntr.doprint(sin(S.Pi/2)) == 'numpy.sin(numpy.pi/2)'\n    assert prntr.doprint(tan(S.Pi/4)) == 'numpy.tan(numpy.pi/4)'\n", "def test_polyroots():\n    from sympy import poly, roots\n\n    p = poly([1, -3, 2])\n    prntr = SciPyPrinter()\n    assert prntr.doprint(roots(p)) == 'np.roots(p)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(roots(p)) == 'numpy.roots(p)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(roots(p)) == 'numpy.roots(p)'\n", "compilation error"], "sample_172": ["    def test_clear_button(self):\n        self.admin_login(username='super', password='secret', login_url='/')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_event_add'))\n        main_window = self.selenium.current_window_handle\n\n        # Select a band\n        self.selenium.find_element_by_id('lookup_id_main_band').click()\n        self.wait_for_and_switch_to_popup()\n        link = self.selenium.find_element_by_link_text('Bogey Blues')\n        self.assertIn('/band/42/', link.get_attribute('href'))\n        link.click()\n\n        # The field should contain the selected band's id\n        self.selenium.switch_to.window(main_window)\n        self.wait_for_value('#id_main_band', '42')\n\n        # Click the clear button\n        self.selenium.find_element_by_id('clear_id_main_band').click()\n\n        # The field should be empty\n        self.assertEqual(self.selenium.find_element_by_id('id_main_band').get_attribute('value'), '')\n\n\n\n", "    def test_multiple_selections(self):\n        self.admin_login(username='super', password='secret', login_url='/')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_event_add'))\n\n        # Select multiple bands\n        self.selenium.find_element_by_id('lookup_id_supporting_bands').click()\n        self.wait_for_and_switch_to_popup()\n        self.selenium.find_element_by_link_text('Bogey Blues').click()\n        self.selenium.find_element_by_link_text('Green Potatoes').click()\n\n        self.selenium.switch_to.window(self.selenium.current_window_handle)\n        self.wait_for_value('#id_supporting_bands', '42,98')\n\n        # Save and check that the selected bands are stored in the database\n        with self.wait_page_loaded():\n            self.selenium.find_element_by_xpath('//input[@value=\"Save\"]').click()\n        event = Event.objects.last()\n        self.assertEqual(set([band.id for band in event.supporting_bands.all()]), {42, 98})\n", "    def test_empty_search(self):\n        self.admin_login(username='super', password='secret', login_url='/')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_event_add'))\n        main_window = self.selenium.current_window_handle\n\n        # No value has been selected yet\n        self.assertEqual(self.selenium.find_element_by_id('id_main_band').get_attribute('value'), '')\n\n        # Open the popup window and click on a band\n        self.selenium.find_element_by_id('lookup_id_main_band').click()\n        self.wait_for_and_switch_to_popup()\n        \n        # Empty search in popup should yield all results\n        self.selenium.find_element_by_id('id_search_popup_box').clear()\n        self.selenium.find_element_by_id('id_search_popup_box').send_keys('')\n        self.selenium.implicitly_wait(1)\n        self.assertEqual(len(self.selenium.find_elements_by_css_selector('.popup-option')), 2)\n\n        self.selenium.switch_to.window(main_window)\n\n", "    def test_filtering_with_empty_input(self):\n        \"\"\"\n        The filter dropdown should show all options when the input field is empty.\n        \"\"\"\n        self.admin_login(username='super', password='secret', login_url='/')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_event_add'))\n\n        from_box = self.selenium.find_element_by_id('lookup_id_main_band')\n        input_field = from_box.find_element_by_tag_name('input')\n        input_field.send_keys(' ')  # Clear input field\n\n        # Assert that all bands are visible\n        options = self.selenium.find_elements_by_css_selector('#lookup_id_main_band > option')\n        self.assertEqual(len(options), 2)\n\n\n", "    def test_multiple_lookups(self):\n        self.admin_login(username='super', password='secret', login_url='/')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_event_add'))\n        main_window = self.selenium.current_window_handle\n\n        # Select a band for 'main_band'\n        self.selenium.find_element_by_id('lookup_id_main_band').click()\n        self.wait_for_and_switch_to_popup()\n        link = self.selenium.find_element_by_link_text('Bogey Blues')\n        self.assertIn('/band/42/', link.get_attribute('href'))\n        link.click()\n        self.selenium.switch_to.window(main_window)\n        self.wait_for_value('#id_main_band', '42')\n\n        # Select a band for 'supporting_bands'\n        self.selenium.find_element_by_id('lookup_id_supporting_bands').click()\n        self.wait_for_and_switch_to_popup()\n        link = self.selenium.find_element_by_link_text('Green Potatoes')\n        self.assertIn('/band/98/', link.get_attribute('href'))\n        link.click()\n        self.selenium.switch_to.window(main_window)\n        self.wait_for_value('#id_supporting_bands', '98')\n\n\n\n"], "sample_1048": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_848": ["def test_multi_output_classification_partial_fit_different_sequences():\n    sgd_linear_clf = SGDClassifier(loss='log', random_state=1, max_iter=5)\n    multi_target_linear = MultiOutputClassifier(sgd_linear_clf)\n\n    X_train = [[1, 2], [3, 4], [5, 6]]\n    y_train = [[0, 1], [1, 0], [0, 1]]\n\n    # Train the multi_target_linear with different sequences\n    multi_target_linear.partial_fit(X_train[0:1], y_train[0:1], classes=None)\n    multi_target_linear.partial_fit(X_train[1:2], y_train[1:2], classes=None)\n    multi_target_linear.partial_fit(X_train[2:3], y_train[2:3], classes=None)\n\n    # Train the multi_target_linear sequentially\n    multi_target_linear_seq = MultiOutputClassifier(sgd_linear_clf)\n    multi_target_linear_seq.fit(X_train, y_train)\n\n    # Check that predictions are the same\n    predictions = multi_target_linear.predict(X_train)\n    predictions_seq = multi_target_linear_seq.predict(X_train)\n    assert_array_equal(predictions, predictions_seq)\n", "def test_multi_output_classification_cv_partial_fit():\n    # test if multi_target_linear initializes correctly with base estimator and fit\n    # assert predictions work as expected for predict, prodict_proba and score\n    sgd_linear_clf = SGDClassifier(loss='log', random_state=1, max_iter=5)\n    multi_target_linear = MultiOutputClassifier(sgd_linear_clf, cv=3)\n    multi_target_linear.partial_fit(X[:300], y[:300], classes=classes)\n    multi_target_linear.partial_fit(X[300:], y[300:], classes=classes)\n\n    predictions = multi_target_linear.predict(X)\n    assert (n_samples, n_outputs) == predictions.shape\n\n    predict_proba = multi_target_linear.predict_proba(X)\n\n    assert len(predict_proba) == n_outputs\n    for class_probabilities in predict_proba:\n        assert (n_samples, n_classes) == class_probabilities.shape\n\n    assert_array_equal(np.argmax(np.dstack(predict_proba), axis=1),\n                       predictions)\n\n\n", "def test_base_chain_partial_fit():\n    # Test if partial_fit works for both chain and base estimators\n    X, Y = generate_multilabel_dataset_with_correlations()\n    for chain in [ClassifierChain(LogisticRegression()),\n                  RegressorChain(Ridge())]:\n        X_train = X[:300, :]\n        Y_train = Y[:300, :]\n        chain.partial_fit(X_train, Y_train)\n        #  partial_fit should work even if data is reshaped with\n        #  reshaped data.\n        chain.partial_fit(X_train, Y_train, classes=list(np.unique(Y_train)))\n        Y_pred = chain.predict(X_train)\n        assert Y_pred.shape == Y_train.shape \n        assert isinstance(chain, ClassifierMixin) or isinstance(chain, RegressorMixin)\n", "def test_multi_output_classification_partial_fit_order():\n    # test if partial_fit with different orders\n    # results in different predictions.\n\n    sgd_linear_clf = SGDClassifier(loss='log', random_state=1, max_iter=5)\n    multi_target_linear = MultiOutputClassifier(sgd_linear_clf)\n    X = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n    y = [[1, 0], [0, 1], [1, 1]]\n\n    # fit with order 'auto'\n    multi_target_linear.fit(X, y)\n    predictions_auto = multi_target_linear.predict(X)\n\n    # fit with order 'random'\n    multi_target_linear_random = clone(multi_target_linear)\n    multi_target_linear_random.set_params(order='random')\n    multi_target_linear_random.fit(X, y)\n    predictions_random = multi_target_linear_random.predict(X)\n\n    # fit with order by indices\n    multi_target_linear_custom = clone(multi_target_linear)\n    multi_target_linear_custom.set_params(order=[1, 0])\n    multi_target_linear_custom.fit(X, y)\n    predictions_custom = multi_target_linear_custom.predict(X)\n\n    # assert that predictions are different for 'random' and 'auto' order\n    assert not np.array_equal(predictions_auto, predictions_random)\n\n    # assert that predictions are different for 'custom' and 'auto' order\n    assert not np.array_equal(predictions_auto, predictions_custom)\n\n", "def test_classifier_chain_partial_fit():\n    # Partially fit classifier chain and verify predict performance\n    X, Y = generate_multilabel_dataset_with_correlations()\n    X_train = X[:400, :]\n    X_test = X[400:, :]\n    Y_train = Y[:400, :]\n    Y_test = Y[400:, :]\n\n    chain = ClassifierChain(LogisticRegression(random_state=0))\n    chain.partial_fit(X_train[:200], Y_train[:200])\n    chain.partial_fit(X_train[200:], Y_train[200:])\n    Y_pred = chain.predict(X_test)\n\n    assert Y_pred.shape == Y_test.shape\n    assert_array_almost_equal(chain.predict(X_train), Y_train)\n\n"], "sample_755": ["def test_davies_bouldin_score():\n    assert_raises_on_only_one_label(davies_bouldin_score)\n\n    assert_raises_on_all_points_same_cluster(davies_bouldin_score)\n\n    # Assert the value is 0 when all points are equals\n    X = np.ones((10, 2))\n    labels = [0] * 10\n    assert 0.0 == davies_bouldin_score(X, labels)\n", "compilation error", "def test_davies_bouldin_index_with_empty_clusters():\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    labels = np.array([0, 0, 1, 1])\n\n    # Create clusters with one sample each\n    for i in range(X.shape[0]):\n        if i % 2 == 0:\n            labels[i] = 0\n        else:\n            labels[i] = 1\n\n    with pytest.raises(ValueError, match=\"Cluster has 0 samples\"):\n        davies_bouldin_score(X, labels)\n", "def test_davies_bouldin_score_empty_samples():\n    # Test with empty clusters,\n    # this should raise a zero division error\n    X = [[] for _ in range(10)]\n    labels = np.zeros(0)\n    with pytest.raises(ZeroDivisionError):\n        davies_bouldin_score(X, labels)\n\n\n\n", "def test_davies_bouldin_score():\n    assert_raises_on_only_one_label(davies_bouldin_score)\n\n    assert_raises_on_all_points_same_cluster(davies_bouldin_score)\n\n    # Test with empty datasets\n    assert davies_bouldin_score([], []) == 0.0\n    assert davies_bouldin_score(np.array([]), np.array([])) == 0.0\n\n    # Test with identical points in all clusters\n    X = np.array([[0, 0], [1, 1]] * 5)\n    labels = np.arange(10)\n    assert davies_bouldin_score(X, labels) == 0.0\n\n    # Test with a few simple cases\n    X = [[0, 0], [1, 1], [2, 2], [3, 3]]\n    labels = [0, 0, 1, 1]\n    d = davies_bouldin_score(X, labels)\n    assert d > 0\n    X = [[0, 0], [1, 1], [2, 2], [1.5, 1.5]]\n    labels = [0, 0, 1, 1]\n    d = davies_bouldin_score(X, labels)\n    assert d > 0\n\n"], "sample_72": ["    def test_serialize_complex_objects(self):\n        class ComplexObject:\n                self.value = value\n\n                return (\n                    'ComplexObject', [('value', self.value)], {}\n                )\n\n        string, imports = MigrationWriter.serialize(ComplexObject(123))\n        self.assertEqual(string, 'ComplexObject(123)')\n        self.assertEqual(imports, {'from migrations.test_writer import ComplexObject'})\n\n\n\n", "    def test_serialize_custom_field(self):\n        class CustomField(models.Field):\n                super().__init__(verbose_name=verbose_name, name=name, **options)\n\n                pass\n\n                pass\n\n                return 'CustomField'\n\n        field = CustomField(max_length=100)\n        string = MigrationWriter.serialize(field)[0]\n        self.assertEqual(string, f\"class CustomField(migrations.Field):\\n    pass\\n\")\n", "    def test_register_serializer_for_custom_object(self):\n        class MyCustomObject:\n                self.value = value\n        \n        class MySerializer(BaseSerializer):\n                return f'MyCustomObject({self.value})', {}\n\n        MigrationWriter._register_serializer(MyCustomObject, MySerializer)\n        string, imports = MigrationWriter.serialize(MyCustomObject(42))\n        self.assertEqual(string, 'MyCustomObject(42)')\n        self.assertEqual(imports, {'from migrations.test_writer import MyCustomObject'})\n", "    def test_serialize_custom_field(self):\n        class CustomField(models.Field):\n                super().__init__(*args, **kwargs)\n\n                return 'custom_type'\n\n                return models.CharField(**kwargs)\n\n                return ('CustomField', [], {})\n\n        field = CustomField(max_length=255)\n\n        string, imports = MigrationWriter.serialize(field)\n        self.assertEqual(string, \"CustomField(max_length=255)\")\n        self.assertEqual(imports, {'from migrations.test_writer import CustomField'})\n\n\n", "    def test_serialize_custom_field(self):\n        class CustomField(models.Field):\n                super().__init__(*args, **kwargs)\n                return 'MyCustomType'\n                return value\n\n        class CustomFieldSerializer(BaseSerializer):\n                return f\"custom_field({self.value})\", {}\n        \n        field = CustomField(max_length=255)\n        string, imports = MigrationWriter.serialize(field)\n        self.assertEqual(string, \"CustomField(max_length=255)\")\n        self.assertEqual(imports, {'from migrations.test_writer import CustomField'})\n\n        string, imports = MigrationWriter.serialize(CustomField(max_length=255), serializer=CustomFieldSerializer)\n        self.assertEqual(string, \"custom_field(CustomField(max_length=255))\")\n        self.assertEqual(imports, {'from migrations.test_writer import CustomField'})\n\n"], "sample_483": ["    def test_hidden_fields(self):\n        class AlbumAdmin(admin.ModelAdmin):\n            hidden_fields = (\"release_date\",)\n\n        errors = AlbumAdmin(Album, AdminSite()).check()\n        self.assertEqual(errors, [])\n\n\n\n\n", "    def test_check_field_name_is_unique_in_inline(self):\n        class DuplicateFieldInline(admin.TabularInline):\n            model = Song\n            fields = [\"title\", \"album\"]\n            extra = 1\n\n        class BookAdmin(admin.ModelAdmin):\n            inlines = [DuplicateFieldInline]\n        \n        errors = BookAdmin(Book, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The field 'title' is defined twice in 'DuplicateFieldInline'. \"\n                \"All fields in an inline must have unique names.\",\n                obj=DuplicateFieldInline,\n                id=\"admin.E010\",\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_hidden_fields_in_list_display(self):\n        class SongAdmin(admin.ModelAdmin):\n            list_display = [\"title\", \"original_release\", \"pk\"]\n            hidden_fields = (\"original_release\",)\n\n        errors = SongAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'list_display' refers to 'original_release', \"\n                \"which is hidden through 'hidden_fields'.\",\n                obj=SongAdmin,\n                id=\"admin.E029\",\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_check_empty_fieldsets(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            fieldsets = []\n\n        errors = MyModelAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Warning(\n                \"'fieldsets' is empty. This might lead to an unexpected admin \"\n                \"interface.\",\n                obj=MyModelAdmin,\n                id=\"admin.W001\",\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_check_list_display_fields_not_in_model(self):\n        class SongAdmin(admin.ModelAdmin):\n            list_display = [\"nonexistent_field\"]\n\n        errors = SongAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'list_display[0]' refers to 'nonexistent_field', which is \"\n                \"not a field of 'admin_checks.Song'.\",\n                obj=SongAdmin,\n                id=\"admin.E017\",\n            )\n        ]\n        self.assertEqual(errors, expected)\n\n"], "sample_958": ["def test_domain_cpp_parse_noindexentry_kwargs(app):\n    text = (\".. cpp:function:: void f(int a)\\n\"\n            \"   :noindexentry: 'param'\\n\")\n    doctree = restructuredtext.parse(app, text)\n\n", "compilation error", "compilation error", "compilation error", "def test_domain_cpp_parse_noindexentry_with_title(app):\n    text = (\".. cpp:function:: void f()\\n\"\n            \"   :noindexentry:\\n\"\n            \"   :title: my_title\\n\"\n            \".. cpp:function:: void g()\\n\"\n            \"   :noindexentry:\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc, addnodes.index, desc))\n    assert_node(doctree[0], addnodes.index, entries=[('single', 'f (C++ function)', '_CPPv41fv', '', None)])\n    assert_node(doctree[2], addnodes.index, entries=[('single', 'g (C++ function)', '_CPPv41gv', '', None)])\n\n"], "sample_540": ["def test_save_count_override_warnings_empty_frames(anim):\n    save_count = 5\n    frames = []\n    match_target = (\n        f'You passed in an explicit {save_count=} '\n        \"which is being ignored in favor of \"\n        f\"{len(frames)=}.\"\n    )\n\n    with pytest.warns(UserWarning, match=re.escape(match_target)):\n        anim = animation.FuncAnimation(\n            **{**anim, 'frames': frames, 'save_count': save_count}\n        )\n\n    assert anim._save_count == 0\n    anim._init_draw()\n", "def test_save_count_override_warnings_generator(anim):\n    save_count = 5\n        for i in range(10):\n            yield i\n    match_target = (\n        f'You passed in an explicit {save_count=} '\n        \"which is being ignored in favor of \"\n        f\"{next(gen(), None)=}.\"\n    )\n    with pytest.warns(UserWarning, match=re.escape(match_target)):\n        anim = animation.FuncAnimation(\n            **{**anim, 'frames': gen, 'save_count': save_count}\n        )\n\n    assert anim._save_count == save_count\n    anim._init_draw()\n", "def test_animation_blit_caching(tmpdir):\n    fig, ax = plt.subplots()\n    line, = ax.plot([], [])\n\n        line.set_data(np.sin(np.linspace(0, 2 * np.pi, 100) + i),\n                      np.cos(np.linspace(0, 2 * np.pi, 100) + i))\n        return line,\n\n    # Create an animation with blitting enabled and frame data caching enabled\n    anim = animation.FuncAnimation(fig, animate, blit=True,\n                                  cache_frame_data=True, frames=5)\n\n    with tmpdir.as_cwd():\n        anim.save(\"test.gif\", writer='imagemagick', dpi=100)\n", "def test_save_count_override_no_cache_warning(klass):\n    save_count = 5\n    frames = iter(range(2))\n    anim = klass(\n        fig=plt.figure(),\n        func=lambda i: None,\n        frames=frames,\n        save_count=save_count\n    )\n    assert anim._cache_frame_data\n    anim._init_draw() \n", "def test_save_count_override_warnings_no_length(anim):\n    save_count = 5\n    frames = iter(range(5))\n    match_target = (\n        \"You passed `save_count=5` which is being ignored in favor of \"\n        f\"the iterator for frames\"\n    ) \n    with pytest.warns(UserWarning, match=re.escape(match_target)):\n        anim = animation.FuncAnimation(\n            **{**anim, 'frames': frames, 'save_count': save_count}\n        )\n    assert anim._save_count == len(list(frames))\n    anim._init_draw()\n"], "sample_501": ["    def test_legend_fontsize_from_rcparam():\n        mpl.rcParams['legend.fontsize'] = 14\n        fig, ax = plt.subplots()\n        ax.plot([1, 2, 3], [4, 5, 6], label='test')\n        leg = ax.legend()\n        for text in leg.get_texts():\n            assert text.get_fontsize() == 14\n\n        mpl.rcParams['legend.fontsize'] = None\n        fig, ax = plt.subplots()\n        ax.plot([1, 2, 3], [4, 5, 6], label='test')\n        leg = ax.legend()\n        assert leg.get_texts()[0].get_fontsize() == mpl.rcParams['font.size']\n\n\n\n", "def test_legend_handler_line2d_kwargs():\n    # Test passing kwargs to Line2D in legend handler (#11357).\n    fig, ax = plt.subplots()\n    line, = ax.plot([0, 1], [0, 1])\n    handles = [mlines.Line2D([0], [0], marker=\"v\", color='red', linestyle='--')]\n    leg = ax.legend([line] + handles, [\"line\", \"marker\"], handler_map={\n        mlines.Line2D: mlines.LegendLine2D(\n            marker=handles[0].get_marker(),\n            color=handles[0].get_color(),\n            linestyle=handles[0].get_linestyle()\n        )\n    })\n    assert leg.legendHandles[0].get_marker() == handles[0].get_marker()\n    assert leg.legendHandles[0].get_color() == handles[0].get_color()\n    assert leg.legendHandles[0].get_linestyle() == handles[0].get_linestyle()\n", "def test_legend_bbox_to_anchor_with_tight_layout():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1])\n    ax.set_title('Test Title')\n    ax.set_xlabel('X-axis')\n    ax.set_ylabel('Y-axis')\n    fig.tight_layout()\n    legend = ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n    x, y = legend.get_window_extent().get_bounds()\n    assert x > 1.0\n    assert y > 0.0\n\n\n\n", "def test_legend_handle_duplicate_labels():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1], label=\"duplicate\")\n    ax.plot([1, 2], [1, 2], label=\"duplicate\")\n    with pytest.warns(UserWarning):\n        ax.legend()\n", "compilation error"], "sample_652": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_76": ["    def test_language_settings_consistent(self):\n        with self.settings(\n            LANGUAGE_CODE='en',\n            LANGUAGES=[('en', 'en'), ('fr', 'fr')],\n            LANGUAGES_BIDI=[\n                'en',\n                'fr',\n            ],\n        ):\n            self.assertEqual(check_language_settings_consistent(None), [])\n\n        with self.settings(\n            LANGUAGE_CODE='es',\n            LANGUAGES=[('en', 'en'), ('fr', 'fr')],\n            LANGUAGES_BIDI=[\n                'en',\n                'fr',\n            ],\n        ):\n            self.assertEqual(check_language_settings_consistent(None), [Error(E004.msg, id=E004.id)]) \n", "    def test_language_settings_consistent(self):\n        with self.settings(LANGUAGE_CODE='en', LANGUAGES=[('en', 'en'), ('es', 'es')]):\n            self.assertEqual(check_language_settings_consistent(None), [])\n\n        with self.settings(LANGUAGE_CODE='fr', LANGUAGES=[('en', 'en'), ('es', 'es')]):\n            self.assertEqual(check_language_settings_consistent(None), [E004])\n\n        with self.settings(LANGUAGE_CODE='en', LANGUAGES=[('en-us', 'en-us'), ('es-419', 'es-419')]):\n            self.assertEqual(check_language_settings_consistent(None), [])\n\n        with self.settings(LANGUAGE_CODE='es-419', LANGUAGES=[('en-us', 'en-us'), ('fr-CA', 'fr-CA')]):\n            self.assertEqual(check_language_settings_consistent(None), [E004])\n\n", "    def test_language_settings_consistent(self):\n        with self.subTest('valid case'):\n            with self.settings(\n                LANGUAGE_CODE='en',\n                LANGUAGES=[('en', 'en'), ('es', 'es')],\n                LANGUAGES_BIDI=[],  # Empty list for now\n            ):\n                self.assertEqual(check_language_settings_consistent(None), [])\n\n        with self.subTest('invalid LANGUAGE_CODE'):\n            with self.settings(\n                LANGUAGE_CODE='fr',  # Not in LANGUAGES\n                LANGUAGES=[('en', 'en'), ('es', 'es')],\n                LANGUAGES_BIDI=[],\n            ):\n                self.assertEqual(check_language_settings_consistent(None), [Error(E004)])\n\n", "    def test_language_settings_consistent(self):\n        with self.settings(\n            LANGUAGE_CODE='es-419',\n            LANGUAGES=[('en', 'en'), ('es', 'es-419')],\n            LANGUAGES_BIDI=['ar'],\n        ):\n            self.assertEqual(check_language_settings_consistent(None), [])\n\n        with self.settings(\n            LANGUAGE_CODE='fr-CA',\n            LANGUAGES=[('fr', 'fr'), ('en', 'en')],  # fr-CA is not here\n            LANGUAGES_BIDI=['ar'],\n        ):\n            self.assertEqual(check_language_settings_consistent(None), [Error(E004.msg, id=E004.id)])\n", "    def test_language_settings_consistent(self):\n        with self.settings(\n            LANGUAGE_CODE='fr-CA',\n            LANGUAGES=[('fr', 'French'), ('en', 'English')],\n            LANGUAGES_BIDI=[('en', 'English')],\n        ):\n            self.assertEqual(check_language_settings_consistent(None), [])\n        with self.settings(\n            LANGUAGE_CODE='fr-CA',\n            LANGUAGES=[('fr', 'French'), ('es', 'Spanish')],\n            LANGUAGES_BIDI=[('en', 'English')],\n        ):\n            self.assertEqual(check_language_settings_consistent(None), [E004])\n"], "sample_701": ["    def test_keyword_msg_arg(self, pytester: Pytester) -> None:\n        p = pytester.makepyfile(\n            \"\"\"\n            import pytest\n\n                pytest.approx(1, 2, abs=0.5)\n            \"\"\"\n        )\n        result = pytester.runpytest(p, '--strict')\n        result.stdout.fnmatch_lines(\n            [\n                \"*PytestDeprecationWarning: pytest.approx(msg=...) is now deprecated, \"\n                \"use pytest.approx(reason=...) instead\",\n                '*pytest.approx(1, 2, abs=0.5)*',\n            ]\n        )\n        result.assert_outcomes(passed=1, warnings=1)\n", "def test_argument_percent_default_deprecated(pytester: Pytester) -> None:\n    with pytest.warns(\n        pytest.PytestDeprecationWarning,\n        match=re.escape(\n            \"pytest now uses argparse. '%default' should be changed to '%(default)s'\"\n        ),\n    ):\n        with pytester.tmp_path() as tmp_path:\n            (tmp_path / \"test_argparse.py\").write_text(\n                \"\"\"\n                import argparse\n\n                parser = argparse.ArgumentParser()\n                parser.add_argument(\"--foo\", default=\"bar\", help=\"Foo bar\")\n                args = parser.parse_args()\n                print(args.foo)\n                \"\"\"\n            )\n            result = pytester.runpytest(tmp_path)\n            result.stdout.fnmatch_lines([\"bar\"])\n\n\n\n", "def test_argument_percent_default_is_deprecated(pytester: Pytester) -> None:\n    with pytest.warns(\n        pytest.PytestDeprecationWarning,\n        match=re.escape(\n            'pytest now uses argparse. \"%default\" should be changed to \"%(default)s\"',\n        ),\n    ):\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n                parser = argparse.ArgumentParser()\n                parser.add_argument(\"-a\", \"--arg\", type=str, default=\"%default\")\n                args = parser.parse_args()\n                assert args.arg == \"default value\"\n            \"\"\"\n        )\n        result = pytester.runpytest()\n        result.stdout.fnmatch_lines(\n            [\n                \"*'default' should be changed to '%(default)s'\",\n                \"*PytestDeprecationWarning: 'pytest now uses argparse. ...\"\n            ]\n        )\n        result.assert_outcomes(passed=1)\n", "def test_argument_percent_default(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import argparse\n\n            pass\n\n        parser = argparse.ArgumentParser()\n        parser.add_argument('--arg1', default=\"default1\", type=str)\n        parser.add_argument('--arg2', default=\"%default\", type=str)\n        args = parser.parse_args()\n\n        assert args.arg1 == \"default1\"\n        assert args.arg2 == \"default\"\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: pytest now uses argparse.%default*.\",\n            \"*\"\n            \"Should be changed to `%(default)s` .*\"\n            \"*\",\n        ]\n    )\n\n\n", "def test_argument_type_str_choice(pytester: Pytester) -> None:\n    with pytest.warns(\n        pytest.PytestDeprecationWarning,\n        match=re.escape(\n            \"For choices this is optional and can be omitted, but when supplied should be a type (for example `str` or `int`). (options: [\"\n        ),\n    ):\n        pytester.makefilecontent('test_arg.py', \"\"\"\n            import pytest\n\n            @pytest.mark.parametrize(\"val\", [\"str\", \"int\"])\n                assert isinstance(test_arg, val)\n            \"\"\", vars=locals())\n        result = pytester.runpytest('-vs', 'test_arg.py')\n        result.stdout.fnmatch_lines(\n            [\n                '*test_arg.py*:*PytestDeprecationWarning:*',\n            ]\n        )\n"], "sample_529": ["    def test_legend_loc_update():\n        fig, axes = plt.subplots(2, 2)\n        axes[0, 0].plot([1, 2, 3], [4, 5, 6], label='line1')\n        axes[0, 1].plot([1, 2, 3], [4, 5, 6], label='line2')\n        axes[1, 0].plot([1, 2, 3], [4, 5, 6], label='line3')\n        axes[1, 1].plot([1, 2, 3], [4, 5, 6], label='line4')\n\n        # Test legend position updates when called across multiple axes\n        for i, ax in enumerate(axes.flat):\n            if i % 2 == 0:\n                leg = ax.legend(loc='best')\n                leg.set_loc('upper right')  \n            else:\n                leg = ax.legend(loc='upper right')\n                leg.set_loc('lower left')\n\n        plt.tight_layout()\n        plt.show()\n", "def test_legend_title_fontsize_empty_legend():\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='test')\n    leg = ax.legend()\n    assert leg.get_title().get_text() == ''\n    leg.set_title(\"Aardvark\", fontsize=20)\n    assert leg.get_title().get_fontsize() == 20\n\n\n\n", "    def test_legend_handle_color_not_changed():\n        fig, axes = plt.subplots(2, 1)\n        axes[0].plot([1], [2], color='red', label='line1')\n        axes[1].plot([3], [4], color='blue', label='line2')\n\n        legend = fig.legend()\n        patches = legend.get_patches()\n        \n        assert patches[0].get_facecolor() == (1, 0, 0, 1) \n        assert patches[1].get_facecolor() == (0, 0, 1, 1) \n", "def test_legend_fontsize_prop():\n    fig, ax = plt.subplots()\n    ax.plot([0,1], [0, 1], label='test')\n    leg = ax.legend(fontsize=12)\n    assert leg.get_fontsize() == 12\n\n    # Test that fontsize is propagated to the text elements\n    texts = leg.get_texts()\n    for text in texts:\n        assert text.get_fontsize() == 12\n", "def test_legend_handles_update():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='line1')\n    leg = ax.legend()\n    ax.plot([2, 3], [4, 5], label='line2')\n    leg.update()\n    assert len(leg.get_lines()) == 2\n    assert leg.get_texts()[0].get_text() == 'line1'\n    assert leg.get_texts()[1].get_text() == 'line2'\n"], "sample_812": ["compilation error", "compilation error", "def test_nested_pipeline_with_gridsearch():\n    # render a nested pipeline inside a gridsearch\n    pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True)\n\n    pipeline = Pipeline([\n        ('reduce_dim', PCA(n_components=2)),\n        ('classify', SVC())\n    ])\n    param_grid = {\n        'reduce_dim__n_components': [1, 2, 3],\n        'classify__C': [1, 10, 100, 1000]\n    }\n    grid = GridSearchCV(pipeline, cv=3, n_jobs=1, param_grid=param_grid)\n    nested_search = GridSearchCV(grid, cv=5, param_grid={'reduce_dim__n_components': [2, 4]})\n    expected = \"\"\"", "def test_nested_pipeline():\n    from sklearn.pipeline import Pipeline\n    from sklearn.preprocessing import StandardScaler\n\n    # Render a nested pipeline with multiple steps\n    pipeline = Pipeline([\n        ('outer_scaler', StandardScaler()),\n        ('inner_pipeline', Pipeline([\n            ('scaler', StandardScaler()),\n            ('transform', lambda X: X ** 2)\n        ])),\n    ])\n    expected = \"\"\"", "def test_n_max_elements_to_show_with_n_attributes():\n    n_max_elements_to_show = 30\n    pp = _EstimatorPrettyPrinter(\n        compact=True, indent=1, indent_at_name=True,\n        n_max_elements_to_show=n_max_elements_to_show\n    )\n\n    class ModelWithManyAttributes(BaseEstimator):\n                     attr4={'key': 'value'}, attr5=True, attr6=None):\n            self.attr1 = attr1\n            self.attr2 = attr2\n            self.attr3 = attr3\n            self.attr4 = attr4\n            self.attr5 = attr5\n            self.attr6 = attr6\n\n    model = ModelWithManyAttributes(attr1=1, attr2='different', attr3=[1, 2, 3],\n                                   attr4={'key': 'value1'}, attr5=False,\n                                   attr6=None)\n    expected = (\n        \"\"\"ModelWithManyAttributes(attr1=1, attr2='different', attr3=[1, 2, 3],\n        attr4={'key': 'value1'}, attr5=False, attr6=None)\"\"\"\n    )\n    assert pp.pformat(model) == expected\n\n\n\n"], "sample_38": ["def test_to_fits_2():\n    \"\"\"\n    Test to_fits() with a header containing a FITS binary table extension.\n    \"\"\"\n    fits_name = get_pkg_data_filename('data/dist_table.fits')\n    w = wcs.WCS(fits_name)\n    wfits = w.to_fits()\n    assert isinstance(wfits, fits.HDUList)\n    assert isinstance(wfits[0], fits.PrimaryHDU)\n    assert isinstance(wfits[1], fits.BinTableHDU)\n", "def test_invalid_coordinate_string():\n    \"\"\"\n    Check that invalid coordinate strings raise a TypeError\n    \"\"\"\n    w = wcs.WCS()\n    with pytest.raises(TypeError):\n        w.wcs_world2pix(\"hello world\", 0)\n    with pytest.raises(TypeError):\n        w.wcs_pix2world([1, 2], \"hello world\", 0)\n", "def test_sip_with_missing_coeffs():\n    \"\"\"\n    Test handling SIP with missing coefficients.\n    \"\"\"\n    # Create a header with SIP but missing some coefficients\n    hdr = get_pkg_data_contents('data/sip-incomplete.hdr')\n    w = wcs.WCS(hdr)\n    assert w.sip is not None\n    assert w.sip.a_order == 4\n    assert w.sip.b_order == 4\n    assert w.sip.ap_order == 0\n    assert w.sip.bp_order == 0\n    # Check that the missing coefficients are handled gracefully\n    assert_allclose(w.all_pix2world([0, 100, 200], [0, -100, 200], 0),\n                    [None, None, None])\n    assert_allclose(w.all_world2pix([0, 100, 200], [0, -100, 200], 0),\n                    [None, None, None])\n\n\n", "def test_sip_ordering():\n    \"\"\"\n    Test that SIP coefficients are ordered correctly for different WCS inputs.\n    \"\"\"\n    # Test with a SIP header that specifies the coordinate system ordering\n    hdr = get_pkg_data_contents(os.path.join(\"data\", 'sip.fits'))\n    w = wcs.WCS(hdr)\n    assert w.sip.a_order_is_default == True\n    assert w.sip.b_order_is_default == True \n\n    # Test with a SIP header that specifies a non-default order\n    hdr = get_pkg_data_contents(os.path.join(\"data\", 'sip-reorder.fits')) \n    w = wcs.WCS(hdr)\n    assert w.sip.a_order_is_default == False\n    assert w.sip.b_order_is_default == False\n\n\n\n", "def test_wcs_from_invalid_header():\n    invalid_header = \"\"\"SIMPLE  =                   T / conforms to FITS standard"], "sample_735": ["compilation error", "def test_gaussian_mixture_predict_proba_with_invalid_covariance_type():\n    rng = np.random.RandomState(0)\n    rand_data = RandomData(rng)\n    X = rand_data.X['full']\n\n    gmm = GaussianMixture(n_components=rand_data.n_components,\n                          covariance_type='invalid', random_state=rng)\n    with pytest.raises(ValueError):\n        gmm.fit(X)\n\n", "def test_predict_proba_multilabel():\n    rng = np.random.RandomState(0)\n    n_samples, n_features, n_components = 50, 3, 2\n    X = rng.rand(n_samples, n_features)\n\n    # Create synthetic multi-label data\n    y_true = np.random.randint(0, 2, size=(n_samples, 2))\n    gmm = GaussianMixture(n_components=n_components, random_state=rng)\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", ConvergenceWarning)\n        gmm.fit(X)\n\n    y_pred_proba = gmm.predict_proba(X)\n    assert_array_almost_equal(y_pred_proba.sum(axis=1), np.ones(n_samples))\n", "def test_gaussian_mixture_fit_converged():\n    rng = np.random.RandomState(0)\n    rand_data = RandomData(rng)\n    n_components = rand_data.n_components\n\n    for covar_type in COVARIANCE_TYPE:\n        X = rand_data.X[covar_type]\n        gmm = GaussianMixture(n_components=n_components,\n                              covariance_type=covar_type, random_state=rng)\n        gmm.fit(X)\n        assert gmm.converged_\n        \n        # Test with a degenerate fit\n        X_degenerate = np.array([np.ones(rand_data.n_features),\n                                 np.zeros(rand_data.n_features)])\n        gmm = GaussianMixture(n_components=2,\n                              covariance_type=covar_type, random_state=rng)\n        gmm.fit(X_degenerate)\n        assert not gmm.converged_\n\n\n\n", "def test_means_changed_when_covars_changed():\n    rng = np.random.RandomState(0)\n    rand_data = RandomData(rng, scale=7)\n    n_components = rand_data.n_components\n\n    # We generate two sets of data with different means and covariances\n    # for the same components.\n\n    X1 = rand_data.X['full']\n    X2 = X1 + np.random.RandomState(42).randn(*X1.shape)\n    gmm1 = GaussianMixture(n_components=n_components, covariance_type='full',\n                          random_state=rng)\n    gmm1.fit(X1)\n\n    gmm2 = GaussianMixture(n_components=n_components, covariance_type='full',\n                          random_state=rng)\n    gmm2.fit(X2)\n\n    assert_array_almost_equal(gmm1.means_, gmm2.means_)\n"], "sample_887": ["def test_calibration_display_custom_ax(pyplot, iris_data):\n    X, y = iris_data\n    clf = LogisticRegression().fit(X, y)\n    viz = CalibrationDisplay.from_estimator(clf, X, y)\n\n    fig, ax = plt.subplots()\n    viz.plot(ax=ax)\n    assert ax is viz.ax_\n", "    def test_calibration_with_n_jobs(data):\n        X, y = data\n        for n_jobs in [-1, 0, 1, 2]:\n            clf = CalibratedClassifierCV(\n                estimator=LogisticRegression(), cv=StratifiedKFold(), n_jobs=n_jobs\n            )\n            clf.fit(X, y)\n\n", "def test_calibration_display_non_normalized_probs(pyplot, iris_data_binary):\n    X, y = iris_data_binary\n    clf = LogisticRegression()\n    clf.fit(X, y)\n    y_pred = clf.predict_proba(X)\n    # modify probabilities so they are not normalized\n    y_pred[:, 1] *= 2\n\n    viz = CalibrationDisplay.from_estimator(clf, X, y, n_bins=10, strategy=strategy)\n\n    expected_legend_labels = [clf.__class__.__name__, \"Perfectly calibrated\"]\n    legend_labels = viz.ax_.get_legend().get_texts()\n    assert len(legend_labels) == len(expected_legend_labels)\n    for labels in legend_labels:\n        assert labels.get_text() in expected_legend_labels\n\n\n\n", "def test_calibration_curve_n_bins_large(method, ensemble):\n    N_SAMPLES = 150\n    X, y = make_classification(n_samples=N_SAMPLES, random_state=42)\n    clf = LogisticRegression().fit(X, y)\n    prob_true, prob_pred = calibration_curve(y, clf.predict_proba(X)[:, 1], n_bins=100)\n    calibrated_clf = CalibratedClassifierCV(clf, method=method, ensemble=ensemble)\n    calibrated_ clf.fit(X, y)\n    prob_true_calibrated, prob_pred_calibrated = calibration_curve(\n        y, calibrated_clf.predict_proba(X)[:, 1], n_bins=100\n    )\n    assert_allclose(prob_true, prob_true_calibrated)\n    assert_allclose(prob_pred, prob_pred_calibrated)\n", "def test_calibration_with_non_int_cv(cv, data):\n    \"\"\"Check that `cv` parameter accepts a prefit estimator.\"\"\"\n    X, y = data\n    clf = LogisticRegression().fit(X, y)\n    calibrated_clf = CalibratedClassifierCV(estimator=clf, cv=cv)\n    with pytest.raises(TypeError):\n        calibrated_clf.fit(X, y)\n\n"], "sample_325": ["    def test_renderer_from_settings(self):\n        settings.TEMPLATES = [\n            {\n                'BACKEND': 'django.template.backends.django.DjangoTemplates',\n                'DIRS': [],\n                'APP_DIRS': True,\n                'OPTIONS': {\n                    'default_render_engine': 'some_custom_renderer'\n                }\n            }\n        ]\n\n        class CustomForm(Form):\n            pass\n\n        form = CustomForm()\n        self.assertEqual(form.renderer, get_template_engine('some_custom_renderer'))\n", "    def test_renderer_override(self):\n        custom = CustomRenderer()\n        form = Form(renderer=custom)\n        with self.subTest('`as_p`'):\n            self.assertEqual(form.as_p(), custom.render(form))\n        with self.subTest('`as_ul`'):\n            self.assertEqual(form.as_ul(), custom.render(form))\n        with self.subTest('`as_table`'):\n            self.assertEqual(form.as_table(), custom.render(form))\n\n", "    def test_renderer_attribute_overrides_kwarg(self):\n        custom_renderer = CustomRenderer()\n        form = Form(renderer=DjangoTemplates())\n        form.renderer = custom_renderer\n        self.assertEqual(form.renderer, custom_renderer)\n", "    def test_renderer_overrides(self):\n        class FormWithRenderer(Form):\n                return super().as_p().replace('<form', '<form class=\"my-form\"')\n\n        form = FormWithRenderer()\n        self.assertEqual(form.as_p(), '<form class=\"my-form\"><p>...</p></form>')\n\n", "    def test_renderer_overriding_in_instance(self):\n        custom_renderer = CustomRenderer()\n        form = Form(renderer=DjangoTemplates())\n        form.renderer = custom_renderer\n        self.assertEqual(form.renderer, custom_renderer)\n"], "sample_48": ["    def test_expression_on_aggregation_with_multiple_arguments(self):\n        class Greatest(Func):\n            function = 'GREATEST'\n\n                return super().as_sql(compiler, connection, function='MAX', extra={'expressions': ', '.join(self.get_source_expressions())})\n\n        books = Book.objects.annotate(\n            greatest_rating=Greatest('rating', F('price'))\n        ).values('rating', 'greatest_rating')\n        self.assertQuerysetEqual(\n            books, [\n                {'rating': 5, 'greatest_rating': 5},\n                {'rating': 4, 'greatest_rating': 4},\n                {'rating': 3, 'greatest_rating': 4},\n                {'rating': 4, 'greatest_rating': 4},\n                {'rating': 3, 'greatest_rating': 3}\n            ],\n            lambda x: (x['rating'], x['greatest_rating'])\n        )\n", "    def test_expression_on_aggregation_with_distinct(self):\n        class Greatest(Func):\n            function = 'GREATEST'\n\n                return f'DISTINCT ({super().as_sql(compiler, connection)})'\n\n        with self.assertRaises(NotImplementedError):\n            Book.objects.annotate(\n                max_rating=Greatest('rating', 'price')\n            ).values_list('rating', flat=True).distinct()\n        with self.assertRaises(NotImplementedError):\n            Book.objects.aggregate(\n                max_rating=Greatest('rating', 'price')\n            )\n\n\n\n", "    def test_expression_on_aggregation_with_filter(self):\n        with self.assertRaisesMessage(TypeError, \"Cannot compute GREATEST(id, id): 'id' is an aggregate\"):\n            Book.objects.filter(rating=4).annotate(\n                greatest_id=Greatest('id', 'id')\n            )\n\n        with self.assertRaisesMessage(TypeError, \"Cannot compute GREATEST(id, id): 'id' is an aggregate\"):\n            Book.objects.aggregate(greatest_id=Greatest('id', 'id'))\n\n        qs = Book.objects.filter(rating=4).annotate(\n            greatest_price=Greatest('price', F('rating') * 10)\n        )\n        self.assertEqual(qs.aggregate(greatest_price=Max('greatest_price'))['greatest_price'], 9999.98)\n\n\n", "    def test_expression_on_aggregation_complex(self):\n        class GreaterThan(Func):\n            function = 'GREATEST'\n\n                return super(GreaterThan, self).as_sql(compiler, connection, function='GREATEST', function_name='GREATEST_SIMPLE')\n\n        qs = Book.objects.annotate(\n            max_price_or_rating=GreaterThan('price', 'rating'))\n\n        b1 = qs.get(pk=self.b4.pk)\n        self.assertEqual(b1.max_price_or_rating, 99.99)\n", "    def test_multi_field_aggregate_over_annotate(self):\n        qs = Book.objects.annotate(\n            sum_ratings=Sum('rating')\n        ).annotate(\n            sum_ratings_and_pages=Sum('rating') + Sum('pages')\n        )\n        b1 = qs.get(pk=self.b4.pk)\n        self.assertEqual(b1.sum_ratings, 4)\n        self.assertEqual(b1.sum_ratings_and_pages, 4 + 209)\n        qs = Book.objects.annotate(\n            sum_ratings=Sum('rating')\n        ).annotate(\n            sum_ratings_and_pages=Sum('rating') + Sum('pages')\n        ).values('sum_ratings', 'sum_ratings_and_pages')\n        b1 = qs.first()\n        self.assertEqual(b1['sum_ratings'], 4)\n        self.assertEqual(b1['sum_ratings_and_pages'], 4 + 209)\n"], "sample_456": ["    def test_invalid(self):\n        data = {\n            \"choices-TOTAL_FORMS\": \"2\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"Zero\",\n            \"choices-0-votes\": \"\",\n            \"choices-1-choice\": \"One\",\n            \"choices-1-votes\": \"1\",\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [{\"votes\": [\"This field is required.\"] }, {}]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n", "    def test_invalid(self):\n        data = {\n            \"choices-TOTAL_FORMS\": \"2\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"Zero\",\n            \"choices-0-votes\": \"\",\n            \"choices-1-choice\": \"One\",\n            \"choices-1-votes\": \"1\",\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [\n            {\"votes\": [\"This field is required.\"] },\n            {},\n        ]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n", "    def test_with_validation_errors(self):\n        data = {\n            \"choices-TOTAL_FORMS\": \"2\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"Zero\",\n            \"choices-0-votes\": \"\",\n            \"choices-1-choice\": \"One\",\n            \"choices-1-votes\": \"invalid\",\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [\n            {\"votes\": [\"This field is required.\"] }, \n            {\"votes\": [\"Invalid input. This field requires a numeric value.\"] }\n        ]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors) \n", "    def test_invalid_data_mixed_in_valid_formsets(self):\n        data1 = {\n            \"choices-TOTAL_FORMS\": \"2\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"Zero\",\n            \"choices-0-votes\": \"0\",\n            \"choices-1-choice\": \"One\",\n            \"choices-1-votes\": \"1\",\n        }\n        data2 = {\n            \"choices-TOTAL_FORMS\": \"1\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"Two\",\n            \"choices-0-votes\": \"\",\n        }\n        formset1 = ChoiceFormSet(data1, auto_id=False, prefix=\"choices1\")\n        formset2 = ChoiceFormSet(data2, auto_id=False, prefix=\"choices2\")\n        self.assertIs(all_valid((formset1, formset2)), False)\n        self.assertEqual(formset1._errors, [{}, {}])\n        self.assertEqual(formset2._errors, [{}, {\"choices-0-votes\": [\"This field is required.\"] }])\n", "    def test_invalid(self):\n        data = {\n            \"choices-TOTAL_FORMS\": \"2\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"Zero\",\n            \"choices-0-votes\": \"\",\n            \"choices-1-choice\": \"One\",\n            \"choices-1-votes\": \"1\",\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [\n            {\"votes\": [u'This field is required.']},\n            {},\n        ]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n"], "sample_918": ["    def test_deprecated_module(app, text):\n        domain = app.env.get_domain('py')\n        restructuredtext.parse(app, text)\n        index = PythonModuleIndex(app.env.get_domain('py'))\n        assert index.generate() == (\n            [('s', [IndexEntry('sphinx', 1, '', '', 'deprecated', '', '')])],\n            False\n        )\n\n", "def test_module_index_with_type(app):\n    text = (\".. py:module:: docutils\\n\"\n            \".. py:module:: sphinx\\n\"\n            \".. py:module:: sphinx:type:: class\\n\")\n    restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    assert index.generate() == (\n        [('d', [IndexEntry('docutils', 0, 'index', 'module-docutils', '', '', '')]),\n         ('s', [IndexEntry('sphinx', 1, 'index', 'module-sphinx', '', '', ''),\n                IndexEntry('sphinx', 1, 'index', 'module-sphinx', 'class', '', '')])],\n        False\n    )\n", "    def test_pyfunction_signature_parameters_order(app):\n        text = \".. py:function:: compile(source, filename: str = None, symbol: str = None) -> ast object\\n\"\n        domain = app.env.get_domain('py')\n        doctree = restructuredtext.parse(app, text)\n        assert_node(doctree, (addnodes.index,\n                              [desc, ([desc_signature, ([desc_name, \"compile\"],\n                                                         [desc_parameterlist,\n                                                          ([desc_parameter, \"source\"],\n                                                           [desc_parameter,\n                                                            [desc_sig_name, \"filename\"],\n                                                            [desc_sig_operator, \"=\"],\n                                                            [desc_sig_name, \"str\"],\n                                                            [desc_optional,\n                                                             ([desc_parameter, \"symbol\"],\n                                                              [desc_sig_operator, \"=\"],\n                                                              [desc_sig_name, \"str\"])],\n                                                           ]),\n                                                          [desc_returns,\n                                                           pending_xref, \"ast object\"])],\n                                                         [desc_content, ()])]))\n\n", "    def test_pyfunction_signature_full_py38(app):\n        # case: separator at head\n        text = \".. py:function:: hello(*, a)\"\n        doctree = restructuredtext.parse(app, text)\n        assert_node(doctree[1][0][1],\n                    [desc_parameterlist, ([desc_parameter, nodes.inline, \"*\"],\n                                          [desc_parameter, desc_sig_name, \"a\"])])\n\n        # case: separator in the middle\n        text = \".. py:function:: hello(a, /, b, *, c)\"\n        doctree = restructuredtext.parse(app, text)\n        assert_node(doctree[1][0][1],\n                    [desc_parameterlist, ([desc_parameter, desc_sig_name, \"a\"],\n                                          [desc_parameter, desc_sig_operator, \"/\"],\n                                          [desc_parameter, desc_sig_name, \"b\"],\n                                          [desc_parameter, desc_sig_operator, \"*\"],\n                                          [desc_parameter, desc_sig_name, \"c\"])])\n\n        # case: separator in the middle (2)\n        text = \".. py:function:: hello(a, /)\"\n        doctree = restructuredtext.parse(app, text)\n        assert_node(doctree[1][0][1],\n                    [desc_parameterlist, ([desc_parameter, desc_sig_name, \"a\"],\n                                          [desc_parameter, desc_sig_operator, \"/\"])])\n\n        # case: separator at tail\n        text = \".. py:function:: hello(a, /)\"\n        doctree = restructuredtext.parse(app, text)\n        assert_node(doctree[1][0][1],\n                    [desc_parameterlist, ([desc_parameter, desc_sig_name, \"a\"],\n                                          [desc_parameter, desc_sig_operator, \"/\"])])\n\n", "def test_module_index_with_options(app):\n    text = (\".. py:module:: docutils\\n\"\n            \"   :platform: unix\\n\"\n            \".. py:module:: sphinx\\n\"\n            \"   :platform: all\\n\"\n            \"   :version: 1.0\\n\")\n    restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    assert index.generate() == (\n        [('d', [IndexEntry('docutils', 0, 'index', 'module-docutils', 'unix', '', '')]),\n         ('s', [IndexEntry('sphinx', 0, 'index', 'module-sphinx', 'all', '1.0', '')])],\n        False\n    )\n"], "sample_16": ["def test_dispatch_to_quantity_methods(self):\n        return x + y\n\n    with patch.object(u.Quantity, \"add\", mock_method):\n        result = (self.q_pv + self.q_pv) * 2\n    assert_array_equal(result, 2 * self.q_pv[\"p\"])\n", "    def test_function_helpers_exist(self, func):\n        assert hasattr(rfn, func)\n\n\n", "    def test_function_helpers_dispatch_correctly(self, func):\n        wrapped = FUNCTION_HELPERS[func]\n        assert isinstance(wrapped, type)\n        # TODO:  Implement a way to dispatch the wrapped function and\n        #  ensure that it behaves as expected. This may involve creating\n        #  sample input data, calling the wrapped function, asserting\n        #  results, and potentially comparing against the expected behavior of\n        #  the original NumPy function.\n", "    def test_unflattened_rec_append(self):\n        \"\"\"\n        Test that rec_append_fields works correctly\n        when the arrays are not flattened.\n        \"\"\"\n        q1 = self.q_pv\n        q2 = np.zeros_like(q1, dtype=q1.dtype)\n        q_app = rfn.rec_append_fields(\n            q1, np.array([f\"new_{str(i)}\" for i in range(3)], dtype=\"<U10\"),\n            q2\n        )\n        assert_array_equal(q_app[\"p\"], q1[\"p\"])\n        assert_array_equal(q_app[\"v\"], q1[\"v\"])\n        assert_array_equal(q_app[\"new_0\"], q2[\"new_0\"])\n        assert_array_equal(q_app[\"new_1\"], q2[\"new_1\"])\n        assert_array_equal(q_app[\"new_2\"], q2[\"new_2\"])\n", "    def test_function_helpers_apply(self, func):\n        with pytest.raises(TypeError, match=\"Unsupported Quantity type for\"):\n            func(u.Quantity(1) * u.one)\n"], "sample_612": ["    def test_resample_interpolate_fill_value(self):\n        times = pd.date_range(\"2000-01-01\", freq=\"6H\", periods=10)\n        ds = Dataset(\n            {\n                \"foo\": ([\"time\", \"x\", \"y\"], np.arange(10).reshape(10, 1, 1)),\n                \"time\": times,\n            }\n        )\n        \n        # Fill value when upsampling with interpolation\n        actual = ds.resample(time=\"1H\").interpolate(fill_value=-999)\n\n        expected_times = times.to_series().resample(\"1H\").asfreq().index\n        expected = Dataset(\n            {\n                \"foo\": (\n                    [\"time\", \"x\", \"y\"],\n                    np.full((len(expected_times), 1, 1), -999, dtype=float),\n                ),\n                \"time\": expected_times,\n            }\n        )\n        assert_identical(expected, actual)\n\n\n\n", "    def test_resample_with_multiple_coords(self):\n        times = pd.date_range(\"2000-01-01\", freq=\"6H\", periods=10)\n        lat = np.arange(3)\n        lon = np.arange(5)\n        ds = Dataset(\n            {\n                \"foo\": ([\"time\", \"lat\", \"lon\"], np.random.randn(10, 3, 5)),\n                \"time\": times,\n                \"lat\": lat,\n                \"lon\": lon,\n            }\n        )\n\n        actual = ds.resample(time=\"1D\").mean()\n        expected = ds.sel(time=ds.time.to_series().resample(\"1D\").index).mean()\n        assert_identical(actual, expected)\n", "    def test_resample_invalid_dim(self):\n        times = pd.date_range(\"2000-01-01\", freq=\"6H\", periods=10)\n        ds = Dataset(\n            {\"foo\": ([\"time\", \"x\", \"y\"], np.random.randn(10, 5, 3)), \"time\": times}\n        )\n\n        with pytest.raises(ValueError, match=r\"Invalid dimension\"):\n            ds.resample(\"time\", dim=\"x\").mean()\n\n\n\n", "    def test_resample_groupby_apply(self):\n        times = pd.date_range(\"2000-01-01\", freq=\"6H\", periods=10)\n        ds = Dataset(\n            {\n                \"foo\": ([\"time\", \"x\", \"y\"], np.random.randn(10, 5, 3)),\n                \"bar\": (\"time\", np.random.randn(10), {\"meta\": \"data\"}),\n                \"time\": times,\n            }\n        )\n            return {\"foo\": group.foo.sum().compute(), \"bar\": group.bar.mean()}\n        actual = ds.resample(time=\"1D\").groupby(\"time\").apply(my_func).compute()\n        expected = ds.groupby(\"time\").apply(my_func)\n        assert_identical(actual, expected)\n\n", "    def test_resample_by_last_with_time_reindexing(self):\n        times = pd.date_range(\"2000-01-01\", freq=\"6H\", periods=10)\n        ds = Dataset(\n            {\n                \"foo\": ([\"time\", \"x\", \"y\"], np.random.randn(10, 5, 3)),\n                \"bar\": (\"time\", np.random.randn(10), {\"meta\": \"data\"}),\n                \"time\": times,\n            }\n        )\n\n        # Inject nan values to test edge cases\n        ds[\"foo\"][-2:] = np.nan\n        actual = ds.resample(time=\"1D\").last()\n\n        # Reindex the time dimension to ensure values are taken correctly\n        expected_times = pd.date_range(\"2000-01-01\", freq=\"D\", periods=4)\n        ds = ds.reindex(time=expected_times)\n        expected = ds.sel(time=expected_times)\n        assert_identical(expected, actual)\n\n"], "sample_651": ["    def test_re_emit_match_none(self) -> None:\n        with pytest.warns():\n            with pytest.raises(pytest.fail.Exception) as excinfo:\n                with pytest.warns(DeprecationWarning, match=r\"must be \\d+$\"):\n                    warnings.warn(\"this is not here\", DeprecationWarning)\n\n        assert \"DID NOT WARN\" in str(excinfo.value) \n", "    def test_re_emit_match_multiple_explicit(self) -> None:\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"error\")  # if anything is re-emitted\n            with pytest.warns(UserWarning, match=\"user warning\"):\n                warnings.warn_explicit(\n                    \"user warning\", category=UserWarning, filename=\"test.py\", lineno=10\n                )\n                warnings.warn_explicit(\n                    \"another user warning\", category=UserWarning, filename=\"test.py\", lineno=11\n                )\n\n\n", "    def test_re_emit_none_match(self) -> None:\n        with pytest.warns(DeprecationWarning):\n            with pytest.warns(UserWarning):\n                warnings.warn(\"user warning\", UserWarning)\n                warnings.warn(\"some deprecation warning\", DeprecationWarning)\n        # Check that no warnings are printed again since we didn't specify a match\n        with pytest.warns(DeprecationWarning):\n            with pytest.warns(UserWarning):\n                warnings.warn(\"user warning\", UserWarning)\n                warnings.warn(\"some deprecation warning\", DeprecationWarning)\n", "    def test_re_emit_match_multiple_non_matching(self) -> None:\n        with pytest.warns(UserWarning, match=\"v2 warning\"):\n            with pytest.warns(UserWarning, match=\"v1 warning\"):\n                warnings.warn(\"v1 warning\", UserWarning)\n                warnings.warn(\"non-matching v2 warning\", UserWarning)\n                warnings.warn(\"v2 warning\", UserWarning)\n", "    def test_re_emit_match_multiple_complex(self) -> None:\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"error\")  # if anything is re-emitted\n            with pytest.warns(UserWarning, match=r\"user warning\"):\n                warnings.warn(\"user warning\", UserWarning)\n                warnings.warn(\"user warning with extra\", UserWarning)\n                warnings.warn(\"completely different warning\", RuntimeWarning)\n"], "sample_531": ["compilation error", "def test_add_subplot_gridspec_kwargs():\n    fig = plt.figure()\n    gs = fig.add_gridspec(2, 2)\n    ax1 = fig.add_subplot(gs[0, 0], projection='polar')\n    ax2 = fig.add_subplot(gs[0, 1], projection='polar')\n    ax3 = fig.add_subplot(gs[1, :], projection='polar')\n\n\n\n", "def test_unpickle_with_subplotspec():\n    fig = plt.figure()\n    gs = fig.add_gridspec(1, 2)\n    ax1 = fig.add_subplot(gs[0, 0])\n    ax2 = fig.add_subplot(gs[0, 1])\n\n    fig2 = pickle.loads(pickle.dumps(fig))\n    assert len(fig2.axes) == 2\n    assert isinstance(fig2.axes[0].get_position(), GridSpecBase.Position)\n", "compilation error", "compilation error"], "sample_752": ["def test_iforest_oob_score():\n    # This test case will be skipped since the \"OOB score\"\n    # functionality is not implemented in IsolationForest.\n    with pytest.raises(NotImplementedError):\n        clf = IsolationForest(random_state=0, oob_score=True).fit(iris.data)\n        clf.oob_score_\n", "def test_iforest_oob_score():\n    \"\"\"Test OOB score works as expected.\"\"\"\n    X = iris.data\n    y = iris.target\n\n    # Using OOB score requires setting bootstrap=True\n    clf = IsolationForest(random_state=0, bootstrap=True, n_estimators=10).fit(X, y)\n\n    # Check that oob_score_ is available\n    assert hasattr(clf, \"oob_score_\")\n\n    assert clf.oob_score_ is not None\n", "def test_iforest_predict_proba():\n    rng = check_random_state(0)\n    X_train, X_test, y_train, y_test = train_test_split(boston.data[:50],\n                boston.target[:50], random_state=rng)\n\n    clf = IsolationForest(n_estimators=10,\n                          contamination=0.1, random_state=rng)\n    clf.fit(X_train)\n\n    y_pred_proba = clf.predict_proba(X_test)\n    assert_array_almost_equal(y_pred_proba[:, 1], clf.decision_function(X_test),\n                             decimal=5)\n\n    assert_array_equal(y_pred_proba[:, 0] + y_pred_proba[:, 1],\n                       np.ones(len(X_test)))  # probabilities should sum to 1\n\n\n\n", "def test_iforest_oob_score():\n    \"\"\"Test that oob_score is not supported.\"\"\"\n    X = iris.data\n    with pytest.raises(NotImplementedError):\n        IsolationForest(random_state=0).fit(X)._set_oob_score(X, np.ones(len(X)))  \n\n\n\n", "def test_iforest_fit_sparse():\n    \"\"\"Check that IsolationForest can fit on sparse input.\"\"\"\n    rng = check_random_state(0)\n    X_train = csc_matrix(rng.rand(100, 20))\n    X_test = csc_matrix(rng.rand(20, 20))\n\n    clf = IsolationForest(n_estimators=10, random_state=0)\n    clf.fit(X_train)\n    clf.predict(X_test)\n\n\n\n"], "sample_407": ["    def test_reverse_foreign_key_bulk_creates(self):\n        # Regression for #17822: Ensure that bulk creation of related objects\n        # through a reverse ForeignKey works correctly.\n        parent = Parent.objects.create(name=\"Parent 1\")\n        ToFieldChild.objects.bulk_create([\n            ToFieldChild(parent=parent, name=\"Child 1\"),\n            ToFieldChild(parent=parent, name=\"Child 2\"),\n            ToFieldChild(parent=parent, name=\"Child 3\"),\n        ])\n        self.assertEqual(parent.to_field_children.count(), 3)\n", "    def test_reverse_foreign_key_instance_to_field_caching_with_delete(self):\n        parent = Parent.objects.create(name=\"a\")\n        child = ToFieldChild.objects.create(parent=parent)\n        with self.assertNumQueries(0):\n            self.assertIs(child.parent, parent)\n\n        parent.delete()\n        with self.assertNumQueries(1):\n            with self.assertRaises(ObjectDoesNotExist):\n                child.parent\n\n\n\n", "    def test_reverse_fk_to_null_with_null_allowed(self):\n        # Regression for #6886 -- test null values on reverse side of FK\n\n        parent = Parent.objects.create(name=\"parent\")\n        child = Child.objects.create(name=\"child\", parent=parent)\n        self.assertIsNot(child.parent, None)\n\n        child.parent = None\n        child.save()\n        self.assertIs(child.parent, None)\n        self.assertIsNot(Parent.objects.all().count(), 0)\n\n        # Ensure that the `parent` field on the child object remains `None`\n        # after fetching it from the database.\n        child = Child.objects.get(name=\"child\")\n        self.assertIs(child.parent, None)\n", "    def test_reverse_foreign_key_instance_to_field_caching_after_parent_update(self):\n        parent = Parent.objects.create(name=\"a\")\n        ToFieldChild.objects.create(parent=parent, name=\"child1\")\n        child = parent.to_field_children.get()\n        parent.name = \"b\"\n        parent.save()\n        with self.assertNumQueries(0):\n            self.assertEqual(child.parent, parent)\n", "    def test_cached_related_after_set_to_null(self):\n        parent = Parent.objects.create(name=\"a\")\n        child = ChildNullableParent.objects.create(parent=parent)\n        self.assertIs(child.parent, parent)\n\n        child.parent = None\n        child.save()  # Clear cache\n\n        self.assertIsNone(child.parent)\n        self.assertEqual(child.parent_id, None)\n"], "sample_186": ["    def test_check_fieldsets_for_duplicate_fieldsets(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            fieldsets = [\n                ('My First Fieldset', {'fields': ['title']}),\n                ('My First Fieldset', {'fields': ['album']}),\n            ]\n\n        errors = MyModelAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'fieldsets' contains duplicate fieldset(s) names.\",\n                obj=MyModelAdmin,\n                id='admin.E011'\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_check_fieldsets_for_duplicate_fields(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            fieldsets = [\n                (None, {\n                    'fields': ['title', 'author', 'genre']\n                }),\n                ('More info', {\n                    'fields': ['title', 'genre', 'release_date']\n                })\n            ]\n\n        errors = MyModelAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'fieldsets[1][1]['fields']' contains duplicate field(s).\",\n                obj=MyModelAdmin,\n                id='admin.E012'\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_check_inlines_for_duplicates(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            inlines = [\n                MyInline1,\n                MyInline2,\n            ]\n\n        class MyInline1(admin.TabularInline):\n            model = Song\n\n        class MyInline2(admin.TabularInline):\n            model = Song\n            \n        errors = MyModelAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"There are duplicate inline(s) for 'Song'.\",\n                obj=MyModelAdmin,\n                id='admin.E011',\n            )\n        ]\n        self.assertEqual(errors, expected)\n\n", "    def test_check_readonly_not_callable(self):\n        class SongAdmin(admin.ModelAdmin):\n            readonly_fields = ('title', 123)  \n\n        errors = SongAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"Each value in 'readonly_fields' must be a callable, an attribute \"\n                \"of 'SongAdmin', or an attribute of 'admin_checks.Song'.\",\n                obj=SongAdmin,\n                id='admin.E036',\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_check_fields_for_non_existent(self):\n        class SongAdmin(admin.ModelAdmin):\n            fields = ('does_not_exist', 'title')\n\n        errors = SongAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'fields[0]' is not an attribute of 'admin_checks.Song'.\",\n                obj=SongAdmin,\n                id='admin.E002',\n            )\n        ]\n        self.assertEqual(errors, expected)\n\n\n"], "sample_851": ["compilation error", "def test_mean_tweedie_deviance_power_errors(power):\n    y_true = [1] * 3\n    y_pred = [1] * 3\n\n    with pytest.raises(ValueError,\n                       match=\"is only defined for power<=0 and power>=1\"):\n        mean_tweedie_deviance(y_true, y_pred, power=power)\n\n", "def test_regression_multioutput_uniform_average():\n    y_true = [[1, 2], [2.5, -1], [4.5, 3], [5, 7]]\n    y_pred = [[1, 1], [2, -1], [5, 4], [5, 6.5]]\n\n    mse = mean_squared_error(y_true, y_pred, multioutput='uniform_average')\n    mae = mean_absolute_error(y_true, y_pred, multioutput='uniform_average')\n    r = r2_score(y_true, y_pred, multioutput='uniform_average')\n    evs = explained_variance_score(y_true, y_pred, multioutput='uniform_average')\n    \n    assert_almost_equal(mse,  0.4283, decimal=4)\n    assert_almost_equal(mae, 0.4750, decimal=4)\n    assert_almost_equal(r, 0.9224, decimal=4)\n    assert_almost_equal(evs, 0.9224, decimal=4)\n", "def test_regression_missing_values():\n    y_true = [1, 2, np.nan, 4]\n    y_pred = [1, 2, np.nan, 4]\n\n    with pytest.raises(ValueError):\n        mean_squared_error(y_true, y_pred)\n    with pytest.raises(ValueError):\n        mean_absolute_error(y_true, y_pred)\n    with pytest.raises(ValueError):\n        r2_score(y_true, y_pred)\n    with pytest.raises(ValueError):\n        explained_variance_score(y_true, y_pred)\n\n    # Tweedie deviance\n    with pytest.raises(ValueError):\n        mean_tweedie_deviance(y_true, y_pred, power=0)\n\n    # Check that mean_squared_log_error handles NaN values correctly\n    with pytest.raises(ValueError):\n        mean_squared_log_error(y_true, y_pred)\n\n\n\n\n", "def test_regression_multioutput_array_non_scalar_weights():\n    y_true = [[1, 2], [2.5, -1], [4.5, 3], [5, 7]]\n    y_pred = [[1, 1], [2, -1], [5, 4], [5, 6.5]]\n    weights = [[0.2, 0.8], [0.5, 0.5], [1, 0], [0, 1]]\n    mse = mean_squared_error(y_true, y_pred, multioutput=weights)\n    mae = mean_absolute_error(y_true, y_pred, multioutput=weights)\n    r = r2_score(y_true, y_pred, multioutput=weights)\n    evs = explained_variance_score(y_true, y_pred, multioutput=weights)\n    assert_array_almost_equal(mse, [0.125, 0.5625], decimal=2)\n    assert_array_almost_equal(mae, [0.25, 0.625], decimal=2)\n    assert_array_almost_equal(r, [0.95, 0.93], decimal=2)\n    assert_array_almost_equal(evs, [0.95, 0.93], decimal=2)\n"], "sample_271": ["    def test_snapshot_files_handles_unchanged_files(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n            snapshot1 = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot1)\n            snapshot2 = dict(self.reloader.snapshot_files())\n            self.assertEqual(snapshot1[self.existing_file], snapshot2[self.existing_file])\n", "    def test_snapshot_files_handles_different_file_types(self):\n        py_file = self.ensure_file(self.tempdir / 'file.py')\n        txt_file = self.ensure_file(self.tempdir / 'file.txt')\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[py_file, txt_file]):\n            snapshot = dict(self.reloader.snapshot_files())\n            self.assertIn(py_file, snapshot)\n            self.assertIn(txt_file, snapshot)\n", "    def test_snapshot_files_handles_new_files(self):\n        self.reloader.watched_files = mock.MagicMock(return_value=[self.existing_file, self.nonexistent_file])\n        snapshot1 = dict(self.reloader.snapshot_files())\n        self.assertIn(self.existing_file, snapshot1)\n        self.assertNotIn(self.nonexistent_file, snapshot1)\n        self.increment_mtime(self.nonexistent_file)\n        snapshot2 = dict(self.reloader.snapshot_files())\n        self.assertIn(self.nonexistent_file, snapshot2)\n", "    def test_snapshot_files_handles_new_files(self):\n        new_file = self.ensure_file(self.tempdir / 'new_file.py')\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file, new_file]):\n            snapshot1 = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot1)\n            self.assertIn(new_file, snapshot1)\n", "    def test_snapshot_files_updates_with_missing_files(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file, self.nonexistent_file]):\n            snapshot1 = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot1)\n            self.assertNotIn(self.nonexistent_file, snapshot1)\n            self.increment_mtime(self.existing_file)\n            snapshot2 = dict(self.reloader.snapshot_files())\n            self.assertNotEqual(snapshot1[self.existing_file], snapshot2[self.existing_file])\n"], "sample_894": ["def test_n_jobs_with_n_estimators(monkeypatch):\n    \"\"\"Test that n_jobs works as expected with n_estimators > 1.\"\"\"\n    n_estimators = 5\n    n_jobs = 2\n    \n    # Patch the Parallel class to return a dummy backend. We just need to make\n    # sure that the backend is invoked with the correct number of jobs.\n    dummy_backend = []\n        dummy_backend.append((\"dummy_backend\", n_jobs))\n        return dummy_backend\n\n    monkeypatch.setattr(joblib, \"Parallel\", dummy_parallel)\n\n    X = np.random.rand(100, 20)\n    y = np.random.randint(0, 2, size=100)\n    forest = RandomForestClassifier(n_estimators=n_estimators, n_jobs=n_jobs)\n    forest.fit(X, y)\n\n    assert len(dummy_backend) == 1\n    assert dummy_backend[0][1] == n_jobs \n\n", "def test_oob_score_with_sparse_data(random_state=42):\n    X = csr_matrix(np.random.rand(100, 10))\n    y = np.random.randint(0, 2, size=100)\n\n    clf = RandomForestClassifier(\n        n_estimators=10, n_jobs=-1, oob_score=True, random_state=random_state\n    )\n    clf.fit(X, y)\n    assert hasattr(clf, \"oob_score_\")\n    assert clf.oob_score_ is not None\n\n\n\n", "def test_sparse_matrix_data_type():\n    X = csr_matrix(np.random.rand(100, 10))\n    y = np.random.rand(100)\n    clf = RandomForestClassifier()\n    clf.fit(X, y)\n\n\n", "def test_warm_start_oob_with_custom_loss(name):\n    # Test warm start with different custom loss functions\n    X, y = hastie_X, hastie_y\n\n    # Define a custom loss function\n        return np.sum((y_true - y_pred) ** 2)\n\n    # Create a classifier with the custom loss\n    ForestClass = FOREST_CLASSIFIERS[name]\n    est = ForestClass(\n        n_estimators=10,\n        warm_start=True,\n        oob_score=True,\n        criterion=my_custom_loss,\n    )\n\n    # Fit the classifier and check oob score\n    est.fit(X, y)\n    assert hasattr(est, \"oob_score_\")\n    assert not math.isnan(est.oob_score_)\n\n    # Increase n_estimators and fit again\n    est.set_params(n_estimators=20)\n    est.fit(X, y)\n    assert hasattr(est, \"oob_score_\")\n    assert not math.isnan(est.oob_score_)\n", "def test_sparse_data_with_max_features(name):\n    X = csr_matrix(np.random.rand(100, 10))\n    y = np.random.rand(100)\n    est = FOREST_ESTIMATORS[name](max_features=\"sqrt\")\n    est.fit(X, y)\n"], "sample_1167": ["def test_commutative_printing():\n    x, y, z = symbols('x y z')\n    f, g, h = symbols('f g h', cls=Function)\n\n    assert latex(x*y, commutative=True) == r'xy'\n    assert latex(x*y, commutative=False) == r'x y'\n\n    assert latex(f(x)*g(y), commutative=True) == r'f{\\left(x\\right)} g{\\left(y\\right)}'\n    assert latex(f(x)*g(y), commutative=False) == r'f{\\left(x\\right)} \\cdot g{\\left(y\\right)}'\n\n    assert latex(x*y*z, commutative=True) == r'xyz'\n    assert latex(x*y*z, commutative=False) == r'x y z'\n\n    assert latex(f(x)*g(y)*h(z), commutative=True) == r'f{\\left(x\\right)} g{\\left(y\\right)} h{\\left(z\\right)}'\n    assert latex(f(x)*g(y)*h(z), commutative=False) == r'f{\\left(x\\right)} \\cdot g{\\left(y\\right)} \\cdot h{\\left(z\\right)}'\n", "def test_issue_15394():\n    from sympy import MatrixSymbol, Function\n\n    A = MatrixSymbol('A', 2, 2)\n    f = Function('f')(A)\n    assert latex(f) == r'f\\left(A\\right)'\n\n    A = MatrixSymbol('A', 3, 3)\n    f = Function('f')(A)\n    assert latex(f) == r'f\\left(A\\right)'\n\n    A = MatrixSymbol('A', 1, 1)\n    f = Function('f')(A)\n    assert latex(f) == r'f\\left(A\\right)'\n", "def test_latex_math_mode():\n    from sympy import sin, cos, exp, MatrixSymbol, symbols\n    A = MatrixSymbol(\"A\", 2, 2)\n    x, y = symbols('x y')\n\n    assert latex(sin(x), mode='math') == r\" \\sin{\\left(x \\right)}\"\n    assert latex(cos(y), mode='math') == r\" \\cos{\\left(y \\right)}\"\n    assert latex(exp(x + y), mode='math') == r\" e^{x + y}\"\n\n    assert latex(A, mode='math') == r\"\\mathbf{A}\"\n    assert latex(A**2, mode='math') == r\"\\mathbf{A}^{2}\"\n\n    assert latex(sin(x) + cos(y), mode='math') == r\"\\sin{\\left(x \\right)} + \\cos{\\left(y \\right)}\"\n", "def test_custom_latex_symbols():\n    from sympy.latex import LatexPrinter\n\n    class CustomPrinter(LatexPrinter):\n            if isinstance(e, Symbol):\n                return r\"\\mySymbol\"\n            return super()._print_Symbol(e)\n\n    p = CustomPrinter()\n    x = symbols('x')\n    assert latex(x, printer=p) == r\"\\mySymbol\"\n", "def test_unicode_symbols():\n    from sympy import Symbol, UnicodeSymbol\n\n    u = UnicodeSymbol(r'\\u03C0')\n    assert latex(u) == r'\\pi'\n\n\n    s = Symbol('s')\n    assert latex(u + s) == r'\\pi + s'\n\n\n\n"], "sample_574": ["    def test_label_formatter_custom(self, t):\n        s = Temporal().label(format)\n        a = PseudoAxis(s._setup(t, Coordinate())._matplotlib_scale)\n        a.set_view_interval(10, 1000)\n        label, = a.major.formatter.format_ticks([100])\n        assert label == t[1].strftime(format)\n", "    def test_label_with_custom_format(self, t):\n\n        format_str = \"%b %Y\"\n        s = Temporal().label(format_str)\n        a = PseudoAxis(s._setup(t, Coordinate())._matplotlib_scale)\n        a.set_view_interval(10, 1000)\n        label, = a.major.formatter.format_ticks([100])\n        assert label == \"Sep 1972\"\n", "    def test_label_formatter_custom(self, date_format, expected, t):\n        formatter = mpl.dates.DateFormatter(date_format)\n        s = Temporal().label(formatter)\n        a = PseudoAxis(s._setup(t, Coordinate())._matplotlib_scale)\n        a.set_view_interval(0, 365)\n        label, = a.major.formatter.format_ticks([t[0]])\n        assert label == expected\n", "    def test_tick_every(self, t, x):\n\n        s = Temporal().tick(every=100)._setup(t, Coordinate())\n        locator = s._matplotlib_scale.major.locator\n        assert_array_equal(locator(), np.round(x, 0).astype(int))\n\n", "    def test_label_formatter_with_date_style(self, t):\n\n        formatter = mpl.dates.DateFormatter('%b %Y')\n        s = Temporal().label(formatter)\n        a = PseudoAxis(s._setup(t, Coordinate())._matplotlib_scale)\n        a.set_view_interval(10, 1000)\n        label, = a.major.formatter.format_ticks([100])\n        assert label == 'Sep 1972'\n"], "sample_306": ["    def test_iso8601_duration(self):\n        test_values = (\n            ('P1D', timedelta(days=1)),\n            ('P1DT2H30M', timedelta(days=1, hours=2, minutes=30)),\n            ('P1W', timedelta(days=7)),\n            ('P1Y', timedelta(days=365)),\n            ('PT2H30M', timedelta(hours=2, minutes=30)),\n            ('PT2.5H', timedelta(hours=2, minutes=30)),\n            ('PT5S', timedelta(seconds=5)),\n            ('P-1D', timedelta(days=-1)),\n            ('P10Y2M3D7H15M30S', timedelta(days=3650, months=2, years=10, hours=7, minutes=15, seconds=30)),\n            ('P10Y2M3DT7H15M30S', timedelta(days=3650, months=2, years=10, hours=7, minutes=15, seconds=30)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n\n", "    def test_iso8601_duration(self):\n        test_values = (\n            ('P1D', timedelta(days=1)),\n            ('PT1H', timedelta(hours=1)),\n            ('PT1M', timedelta(minutes=1)),\n            ('PT1S', timedelta(seconds=1)),\n            ('P1DT1H', timedelta(days=1, hours=1)),\n            ('P1DT1H30M', timedelta(days=1, hours=1, minutes=30)),\n            ('P1DT1H30M10S', timedelta(days=1, hours=1, minutes=30, seconds=10)),\n            ('P1DT1H30M10.123S', timedelta(days=1, hours=1, minutes=30, seconds=10, microseconds=123000)),\n            ('PT1.123H', timedelta(hours=1, microseconds=1123000)),\n            ('PT1.123M', timedelta(minutes=1, microseconds=1123000)),\n            ('PT1.123S', timedelta(seconds=1, microseconds=1123000)),\n            ('P1D2H30M10S', timedelta(days=1, hours=2, minutes=30, seconds=10)),\n            ('P1D-2H30M10S', timedelta(days=1, hours=-2, minutes=-30, seconds=-10)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n", "    def test_iso8601(self):\n        test_values = (\n            ('P1D', timedelta(days=1)),\n            ('P1D2H30M', timedelta(days=1, hours=2, minutes=30)),\n            ('P1W', timedelta(days=7)),\n            ('P1Y', timedelta(days=365)),\n            ('PT1H30M', timedelta(hours=1, minutes=30)),\n            ('PT30S', timedelta(seconds=30)),\n            ('P1Y2M3DT10H', timedelta(days=365, microseconds=30)),\n            ('PT1.1S', timedelta(seconds=1, microseconds=1000000)),\n            ('PT10.001S', timedelta(seconds=10, microseconds=1)),\n            ('P1D-2H', timedelta(days=1, hours=-2)),\n            ('P1Y2M-3DT-10H', timedelta(days=365, microseconds=30, hours=-10)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n\n\n\n", "    def test_iso8601(self):\n        test_values = (\n            ('P1D', timedelta(days=1)),\n            ('P1DT1H', timedelta(days=1, hours=1)),\n            ('P1DT1H30M', timedelta(days=1, hours=1, minutes=30)),\n            ('P1DT1H30M15S', timedelta(days=1, hours=1, minutes=30, seconds=15)),\n            ('P1DT1H30M15.123S', timedelta(days=1, hours=1, minutes=30, seconds=15, microseconds=123000)),\n            ('PT1H30M15S', timedelta(hours=1, minutes=30, seconds=15)),\n            ('PT1H30M15.123S', timedelta(hours=1, minutes=30, seconds=15, microseconds=123000)),\n            ('P-1D', timedelta(days=-1)),\n            ('P-1DT-1H', timedelta(days=-1, hours=-1)),\n            ('P-1DT-1H-30M', timedelta(days=-1, hours=-1, minutes=-30)),\n            ('P-1DT-1H-30M-15S', timedelta(days=-1, hours=-1, minutes=-30, seconds=-15)),\n            ('P-1DT-1H-30M-15.123S', timedelta(days=-1, hours=-1, minutes=-30, seconds=-15, microseconds=-123000)),\n            ('PT-1H30M15S', timedelta(hours=-1, minutes=-30, seconds=-15)),\n            ('PT-1H30M15.123S', timedelta(hours=-1, minutes=-30, seconds=-15, microseconds=-123000)),\n            ('PT0S', timedelta(seconds=0)),\n            ('PT0H20M30S', timedelta(minutes=20, seconds=30)),\n            ('P0DT", "    def test_iso8601_duration(self):\n        test_values = (\n            ('P1D', timedelta(days=1)),\n            ('PT1H', timedelta(hours=1)),\n            ('PT1M', timedelta(minutes=1)),\n            ('PT1S', timedelta(seconds=1)),\n            ('P1DT1H', timedelta(days=1, hours=1)),\n            ('P1DT1H30M', timedelta(days=1, hours=1, minutes=30)),\n            ('P1D2H10M30S', timedelta(days=1, hours=2, minutes=10, seconds=30)),\n            ('P1DT1H30M10S.12345', timedelta(days=1, hours=1, minutes=30, seconds=10, microseconds=123450)),\n            ('-P1D', timedelta(days=-1)),\n            ('-PT1H30M', timedelta(hours=-1, minutes=-30)),\n            ('P1DT', timedelta(days=1)),\n            ('PT1H30', timedelta(hours=1, minutes=30)),\n            ('P1D2H10M30S.1234567890', timedelta(days=1, hours=2, minutes=10, seconds=30, microseconds=123456789)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n"], "sample_657": ["    def test_pytest_param_id_raises_for_non_string_types(s):\n        with pytest.raises(TypeError) as excinfo:\n            pytest.param(id=s)\n        msg, = excinfo.value.args\n        assert msg == \"Expected id to be a string, got <class 'int'>: 123\"\n\n\n\n", "    def test_pytest_param_id_raises_error_for_non_string_non_none(testdir):\n        with pytest.raises(TypeError) as excinfo:\n            pytest.param(id=123)\n        msg, = excinfo.value.args\n        assert msg == \"Expected id to be None or a string, got <class 'int'>: 123\" \n", "    def test_pytest_param_id_allows_string(s):\n        assert pytest.param(\"hello world\", id=s)\n\n", "    def test_pytest_param_id_allows_none_or_string_or_int(s):\n        assert pytest.param(id=s)\n", "    def test_pytest_param_id_allows_int(self, testdir):\n        \"\"\" #1042\"\"\"\n        p = testdir.makepyfile(\n            \"\"\"\n                assert request.node.id == \"test_param_id_int[123]\"\n            \"\"\"\n        )\n        result = testdir.runpytest(p)\n        result.stdout.fnmatch_lines(\n            [\n                \"collected 1 item\",\n                \"*= 1 passed in *\",\n            ]\n        )\n\n\n\n"], "sample_461": ["    def test_urlfield_protocol_validation(self):\n        f = URLField()\n        with self.assertRaisesMessage(ValidationError, \"'Enter a valid URL.'\"):\n            f.clean(\"ftp://example.com\")\n        with self.assertRaisesMessage(ValidationError, \"'Enter a valid URL.'\"):\n            f.clean(\"file:///path/to/example.txt\")\n        f = URLField(protocols=(\"http\", \"https\"))\n        with self.assertRaisesMessage(ValidationError, \"'Enter a valid URL.'\"):\n            f.clean(\"ftp://example.com\")\n        with self.assertRaisesMessage(ValidationError, \"'Enter a valid URL.'\"):\n            f.clean(\"file:///path/to/example.txt\")\n        self.assertEqual(f.clean(\"http://example.com\"), \"http://example.com\")\n        self.assertEqual(f.clean(\"https://example.com\"), \"https://example.com\")\n\n\n\n", "    def test_urlfield_clean_with_port(self):\n        f = URLField()\n\n        # Clean a URL with an explicit port number.\n        self.assertEqual(\n            f.clean(\"http://example.com:8080/\"), \"http://example.com:8080/\"\n        )\n\n        # Clean a URL with an implicit port number (using HTTPS).\n        self.assertEqual(\n            f.clean(\"https://example.com:443/\"), \"https://example.com:443/\"\n        )\n\n        # Ensure that invalid port numbers don't cause issues.\n        self.assertEqual(\n            f.clean(\"http://example.com:12345678/\"), \"http://example.com:12345678/\"\n        )\n\n        # Test case with a default port 80 for HTTP and 443 for HTTPS\n        self.assertEqual(\n            f.clean(\"http://example.com\"), \"http://example.com\"\n        )\n        self.assertEqual(\n            f.clean(\"https://example.com\"), \"https://example.com\"\n        )\n\n\n\n", "    def test_urlfield_clean_invalid_internationalized_domain(self):\n        f = URLField()\n        tests = [\n            # From RFC 5890 - Internationalized Domain Names in Applications\n            # (IDNA):\n            \"http://xn--example-kva.com\",\n            \"http://xn--example-kva.net\",\n            \"http://www.xn--xn--p1ai\",\n        ]\n        for url in tests:\n            with self.subTest(url=url):\n                self.assertEqual(f.clean(url), url)\n\n", "    def test_urlfield_clean_ipv6(self):\n        f = URLField()\n        self.assertEqual(f.clean(\"http://[2001:db8::1]/\"), \"http://[2001:db8::1]/\")\n        self.assertEqual(f.clean(\"https://[fe80::1]/\"), \"https://[fe80::1]/\")\n\n\n\n", "    def test_urlfield_clean_with_fragment(self):\n        f = URLField()\n        self.assertEqual(\n            f.clean(\"http://example.com#section1\"), \"http://example.com#section1\"\n        )\n        f = URLField(allow_fragment=False)\n        with self.assertRaisesMessage(ValidationError, \"'Invalid URL.'\"):\n            f.clean(\"http://example.com#section1\")\n"], "sample_181": ["    def test_filtered_aggregate_with_related_object_filter(self):\n        agg = Count('book', filter=Q(book__rating__gt=4.0))\n        self.assertEqual(Author.objects.annotate(book_count=agg).order_by('pk')[0].book_count, 2)\n", "    def test_filtered_aggregate_with_complex_filter(self):\n        agg = Sum('price', filter=Q(\n            pubdate__year=2008,\n        ) & Q(\n            rating__gte=4,\n        ) | Q(\n            rating__lt=3,\n            pubdate__year__lt=2008,\n        ))\n        self.assertEqual(Book.objects.aggregate(price=agg)['price'], 72.78)\n", "    def test_filtered_aggregate_with_ordering(self):\n        agg = Sum('age', filter=Q(name__startswith='test'))\n        qs = Author.objects.order_by('pk').annotate(total_age=agg)\n        self.assertEqual(qs.values_list('total_age', flat=True), [200])\n", "    def test_filtered_aggregate_with_exists(self):\n        subquery = Book.objects.filter(contact__name='test').exists()\n        agg = Sum('age', filter=Q(subquery))\n        self.assertEqual(Author.objects.aggregate(age=agg)['age'], 100)\n", "    def test_aggregate_with_related_fields_and_filter(self):\n        subquery = Book.objects.filter(\n            rating__gt=3,\n            author__name='test',\n        ).values('publisher__name')\n        agg = Avg('age', filter=Exists(subquery))\n        self.assertEqual(Author.objects.aggregate(avg_age=agg)['avg_age'], 40.0)\n\n"], "sample_1010": ["def test_issue_15251():\n    from sympy.tensor.tensor import Tensor\n\n    a, b, c, d = symbols('a b c d')\n    t = Tensor(a, b, c, d)\n    assert latex(t) == r\"a b c d\"\n", "def test_Tensor_printing():\n    from sympy.tensor.tensor import Tensor\n    T = Tensor(\"T\", indices=\"ij\")\n    assert latex(T) == r\"T_{ij}\"\n    T_ = Tensor(\"T_\", indices=(\"i\", \"j\"))\n    assert latex(T_) == r\"T_{ij}\"\n    T_ = Tensor(\"T\", indices=\"ijkl\")\n    assert latex(T_) == r\"T_{ijkl}\"\n\n\n", "def test_TensorProduct_printing():\n    from sympy.tensor.functions import TensorProduct\n    A = TensorProduct(MatrixSymbol(\"A\", 2, 2), MatrixSymbol(\"B\", 2, 2))\n    B = TensorProduct(MatrixSymbol(\"C\", 2, 2), MatrixSymbol(\"D\", 2, 2))\n    assert latex(A + B) == r\"A \\otimes B + C \\otimes D\"\n", "def test_transpose():\n    A = MatrixSymbol(\"A\", 2, 3)\n    B = MatrixSymbol(\"B\", 3, 2)\n    assert latex(A.transpose()) == r\"{A}^{T}\"\n    assert latex(B.transpose()) == r\"{B}^{T}\"\n\n\n\n", "def test_issue_15785():\n    from sympy.physics.quantum import Dagger\n    a = Symbol('a')\n    b = Symbol('b')\n    vector = Dagger(a + b)\n    assert latex(vector) == r\"^\\left(a + b\\right)\"\n"], "sample_472": ["    def test_paginating_queryset_with_extra_context_works(self):\n        paginator = Paginator(Article.objects.order_by(\"id\"), 5)\n        p = paginator.page(1)\n        extra_context = {\"test_key\": \"test_value\"}\n\n        self.assertEqual(p.object_list, self.articles[:5])\n        # Ensure that extra_context is accessible in the template.\n        with self.subTest('extra_context is accessible in the template'):\n            with self.assertListEqual(\n                p.get_context(extra_context),\n                [{\"test_key\": \"test_value\"}],\n            ):\n                pass \n        \n        # Check that extra_context can be accessed using the context processor.\n        with self.subTest(\n            'extra_context accessible using context processor'\n        ):\n            class ExtraContextProcessor(ContextProcessor):\n                    context[\"test_key\"] = \"test_value_2\"\n                    return context\n            paginator = Paginator(\n                Article.objects.order_by(\"id\"), 5, context_processors=[ExtraContextProcessor()]\n            )\n            p = paginator.page(1)\n            self.assertEqual(p.object_list, self.articles[:5])\n            self.assertEqual(p.get_context(), {\"test_key\": \"test_value_2\"})\n\n\n", "    def test_get_elided_page_range_empty_list(self):\n        paginator = Paginator([], 2)\n        page_range = paginator.get_elided_page_range(1)\n        self.assertEqual(list(page_range), [1])\n        self.assertEqual(paginator.num_pages, 1)\n\n\n\n", "    def test_using_django_queryset_order_by_in_pagination(self):\n        paginator = Paginator(\n            Article.objects.order_by(\"-pub_date\"), per_page=5\n        )\n        p = paginator.page(1)\n        self.assertEqual(p.object_list[0].headline, \"Article 9\") \n\n", "    def test_page_range(self):\n        paginator = Paginator(Article.objects.order_by(\"id\"), 5)\n        self.assertEqual(paginator.page_range, range(1, 2))\n        paginator = Paginator(Article.objects.order_by(\"id\"), 3)\n        self.assertEqual(paginator.page_range, range(1, 4))\n        paginator = Paginator(Article.objects.order_by(\"id\"), 10)\n        self.assertEqual(paginator.page_range, range(1, 2))\n\n\n", "    def test_pagination_empty_object_list_and_allow_empty_first_page_false(self):\n        paginator = Paginator(Article.objects.none(), 5, allow_empty_first_page=False)\n        with self.assertRaises(EmptyPage):\n            paginator.page(1)\n"], "sample_697": ["compilation error", "def test_tmp_path_factory_create_directory_with_safe_permissions(\n    tmp_path: Path, monkeypatch: MonkeyPatch", "def test_tmp_path_factory_handles_empty_basetemp(tmp_path_factory: TempPathFactory, monkeypatch: MonkeyPatch) -> None:\n    monkeypatch.setattr(tmp_path_factory, \"_given_basetemp\", None)\n    p = tmp_path_factory.getbasetemp()\n    assert p.exists()\n\n\n\n", "    def test_tmp_path_factory_create_directory_with_correct_owner(\n        tmp_path: Path, monkeypatch: MonkeyPatch", "    def test_tmpdir_creation_handles_existing_directories(\n        tmp_path_factory: TempPathFactory, monkeypatch: MonkeyPatch"], "sample_1017": ["def test_issue_15182():\n    assert (And(x > 0, x < 1)).canonical  == And(x > 0, x < 1)\n    assert (Or(x > 0, x < 1)).canonical == Or(x > 0, x < 1)\n    assert (x > 0).canonical == x > 0\n    assert (x < 0).canonical == x < 0\n", "def test_issue_13798():\n    assert ITE(Or(x, y), x, z).subs({x: 1, y: 1, z: 2}) == 1\n    assert ITE(Or(x, y), x, z).subs({x: 0, y: 0, z: 2}) == 2\n    assert ITE(Or(x, y), x, z).subs({x: 0, y: 1, z: 2}) == 1\n    assert ITE(Or(x, y), x, z).subs({x: 1, y: 0, z: 2}) == 1\n", "def test_issue_11040():\n    x, y = symbols('x y')\n    assert (Or(x < 1, y > 2)).simplify() == Or(x < 1, y > 2)\n    assert And(Or(x < 1, y > 2), x >= 0, y <= 5).simplify() == And(Or(0 <= x < 1, 2 < y <= 5))\n\n\n\n", "def test_issue_13077():\n    x = symbols('x', real=True)\n    assert (x > 0).as_expr() == x > 0\n    assert (x != 0).as_expr() == x != 0\n    assert (x <= 0).as_expr() == x <= 0\n    assert (x >= 0).as_expr() == x >= 0\n    assert (x < 0).as_expr() == x < 0\n", "def test_issue_10240():\n    assert Not(And(x > 2, x < 3)).as_set() == \\\n        Union(Interval(-oo,2),Interval(3,oo))\n"], "sample_884": ["def test_property_deprecation():\n    with pytest.warns(FutureWarning, match=\"n_features_ is deprecated\"):\n        mock = MockClass2()\n        val = mock.n_features_\n    assert val == 10\n", "compilation error", "def test_property_deprecation_message():\n    with pytest.warns(FutureWarning, match=\"n_features_ is deprecated\"):\n        instance = MockClass2()\n        value = instance.n_features_\n    assert value == 10\n", "def test_deprecated_property():\n    with pytest.warns(FutureWarning, match=\"n_features_ is deprecated\"):\n        instance = MockClass2()\n        value = instance.n_features_\n    assert value == 10\n", "def test_pickle_deprecated():\n    @deprecated(\"pickled_function\")\n        pass\n\n    pickled_function = pickle.dumps(mock_pickle_function)\n    unpickled_function = pickle.loads(pickled_function)\n\n    with pytest.warns(FutureWarning, match=\"pickled_function\"):\n        unpickled_function()\n"], "sample_492": ["    def test_serialize_custom_serializer(self):\n        class MyCustomSerializer(BaseSerializer):\n                return '{\"custom_key\": %r}' % value, {}\n\n        with mock.patch(\n            \"migrations.writer.MigrationWriter._serialize\", return_value=\"serialized_value\"\n        ):\n            result = MigrationWriter._serialize_field(\n                models.CharField(default=123), \"field_name\", MyCustomSerializer(), \"\"\n            )\n\n        self.assertEqual(result, '{\"custom_key\": 123}')\n", "    def test_serialize_nested_object(self):\n        class NestedObject:\n                self.value = value\n        obj = NestedObject(\"hello\")\n        serialized, imports = MigrationWriter.serialize(obj)\n        self.assertEqual(serialized, \"NestedObject(value='hello')\")\n        self.assertEqual(imports, {\"from migrations.test_writer import NestedObject\"})\n\n\n", "    def test_serialize_custom_serializer(self):\n        class CustomSerializer(BaseSerializer):\n                return f\"custom_serialized({obj!r})\", {}\n\n        with self.subTest(\"custom serializer\"):\n            self.assertSerializedResultEqual(\n                obj=\"hello\", serializer=CustomSerializer(),\n                (\"custom_serialized('hello')\", {}),\n            )", "    def test_serialize_complex_field_with_serializer(self):\n\n        class ComplexField(models.Field):\n                super().__init__()\n                self.serializer = serializer\n\n                return 'VARCHAR(255)'\n\n                return self.serializer.deserialize(value)\n\n                return self.serializer.serialize(value)\n\n        class ComplexSerializer(BaseSerializer):\n                return \"complex(%r)\" % self.value, {}\n\n                return value\n\n        complex_field = ComplexField(serializer=ComplexSerializer())\n\n        class MyModel(models.Model):\n            complex_field = complex_field\n\n        string, imports = MigrationWriter.serialize(MyModel)\n        self.assertIn('migrations.test_writer.ComplexSerializer', imports)\n        self.assertIn('ComplexField = ComplexField(serializer=ComplexSerializer)', string)\n\n", "    def test_complex_serializer(self):\n        self.assertSerializedResultEqual(\n            ComplexSerializer(\"hello\"), (\"complex('hello')\", {})\n        )\n\n\n"], "sample_674": ["def test_ischildnode_with_non_string_nodeid():\n    with pytest.raises(TypeError, match=\"nodeid must be a string\"):\n        nodes.ischildnode(\"foo\", 123)\n    with pytest.raises(TypeError, match=\"nodeid must be a string\"):\n        nodes.ischildnode(\"foo\", [])\n    with pytest.raises(TypeError, match=\"nodeid must be a string\"):\n        nodes.ischildnode(\"foo\", {})\n\n\n\n", "def test_NodeMeta():\n    class MyNode(nodes.Node, metaclass=nodes.NodeMeta):\n        pass\n\n    if not MyNode.is_node_class:\n        pytest.fail(\"MyNode is not correctly recognized\")\n", "compilation error", "def test_getfslineno_from_item_with_obj():\n    import inspect\n        pass\n    obj_instance = func()\n    item = nodes.Item(\"test\", fspath=py.path.local(__file__).dirpath())\n    item.obj = obj_instance\n    path, lineno = nodes.get_fslocation_from_item(item)\n    assert path == inspect.getfile(func)\n    assert lineno == inspect.getsourcelines(func)[1]\n\n\n\n", "def test_listnames(testdir):\n    items = testdir.getitems(\n        \"\"\"\n        class TestABC:\n                pass\n                pass\n        \"\"\"\n    )\n    names = [x.name for x in items.listnames()]\n    assert names == [\"TestABC\", \"test_a\", \"test_b\"]\n\n\n\n"], "sample_1072": ["def test_issue_9421():\n    x = Symbol('x')\n    assert floor(x/Rational(2, 3)).rewrite(floor) == (3/2)*(floor(x/Rational(3, 2)))\n\n", "def test_issue_12406():\n\n    from sympy.functions import ln, E\n\n    x = Symbol('x', real=True)\n\n    assert floor(ln(E)).evalf() == 1\n\n    assert floor(ln(E*x)).evalf() == 1 + floor(ln(x)).evalf()\n    assert floor(ln(E/x)).evalf() == 1 - floor(ln(x)).evalf()\n", "def test_issue_11207_continued():\n    assert (floor(floor(x)) == floor(x)).is_true\n    assert (ceiling(ceiling(x)) == ceiling(x)).is_true\n    assert (floor(ceiling(x)) == floor(x)).is_true\n    assert (ceiling(floor(x)) == floor(x)).is_true\n", "def test_issue_13259():\n    x = Symbol('x', real=True)\n    assert floor(x + 0.5).evalf(chop=True) == 0.5 if floor(x) == 0.5 else floor(x)\n    assert ceiling(x - 0.5).evalf(chop=True) == -0.5 if ceiling(x) == -0.5 else ceiling(x)\n", "def test_issue_11207_complex():\n    x = Symbol('x', complex=True)\n    assert floor(floor(x)) == floor(x)\n    assert floor(ceiling(x)) == floor(x)\n    assert ceiling(floor(x)) == floor(x)\n    assert ceiling(ceiling(x)) == ceiling(x)\n"], "sample_902": ["def test_pipeline_memory_with_non_transform_steps():\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    cachedir = mkdtemp()\n    try:\n        memory = Memory(cachedir=cachedir, verbose=10)\n        # Test with a pipeline containing a RandomForestClassifier and \n        # a NonTransf estimator\n        rf_clf = RandomForestClassifier(random_state=0)\n        non_transf = NoTransf()\n        pipe = Pipeline([('non_transf', non_transf), ('rf', rf_clf)])\n        cached_pipe = Pipeline([('non_transf', non_transf), ('rf', rf_clf)],\n                               memory=memory)\n\n        # Memoize the pipeline at the first fit\n        cached_pipe.fit(X, y)\n        pipe.fit(X, y)\n        # Check that cached_pipe and pipe yield identical results\n        assert_array_equal(pipe.predict(X), cached_pipe.predict(X))\n        assert_array_equal(pipe.score(X, y), cached_pipe.score(X, y))\n\n    finally:\n        shutil.rmtree(cachedir)\n", "def test_pipeline_parallel_fit_transform():\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n\n    n_jobs = 2\n\n    # Test with DummyTransf and DummyClassifier\n    transf = DummyTransf()\n    clf = DummyClassifier(strategy=\"most_frequent\")\n    pipe = Pipeline([('transf', transf), ('clf', clf)], n_jobs=n_jobs)\n    pipe.fit(X, y)\n    assert_array_equal(pipe.predict(X), clf.predict(transf.fit_transform(X)))\n\n    # Test with 2 different transformers and a classifier\n    transf1 = DummyTransf()\n    transf2 = DummyTransf()\n    clf = DummyClassifier(strategy=\"most_frequent\")\n    pipe = Pipeline([('transf1', transf1), ('transf2', transf2), ('clf', clf)],\n                    n_jobs=n_jobs)\n    pipe.fit(X, y)\n    assert_array_equal(pipe.predict(X), clf.predict(transf2.fit_transform(transf1.fit_transform(X))))\n\n    # Test with an estimator that doesn't support parallel fitting\n    class NoParallel(object):\n            return self\n            return np.array([1] * len(X))\n\n    pipe = Pipeline([('transf', DummyTransf()), ('no_parallel', NoParallel()),\n                    ('clf', DummyClassifier(strategy=\"most_frequent\"))],\n                   n_jobs=n_jobs)\n    pipe.fit(X, y)\n    assert_array_equal(pipe.predict(X), clf.predict(transf.fit_transform(X)))\n\n\n\n", "def test_pipeline_with_memory_on_fit_transform():\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    cachedir = mkdtemp()\n    try:\n        memory = Memory(cachedir=cachedir, verbose=10)\n        # Test with Transformer + SVC\n        clf = SVC(probability=True, random_state=0)\n        transf = DummyTransf()\n        pipe = Pipeline([('transf', clone(transf)), ('svc', clf)])\n        cached_pipe = Pipeline([('transf', transf), ('svc', clf)],\n                               memory=memory)\n        # Memoize the transformer at the first fit_transform\n        cached_pipe.fit_transform(X, y)\n        pipe.fit_transform(X, y)\n        # Get the time stamp of the transformer in the cached pipeline\n        ts = cached_pipe.named_steps['transf'].timestamp_\n        # Check that cached_pipe and pipe yield identical results\n        assert_array_equal(pipe.predict(X), cached_pipe.predict(X))\n        assert_array_equal(pipe.predict_proba(X), cached_pipe.predict_proba(X))\n        assert_array_equal(pipe.predict_log_proba(X),\n                           cached_pipe.predict_log_proba(X))\n        assert_array_equal(pipe.score(X, y), cached_pipe.score(X, y))\n        assert_array_equal(pipe.named_steps['transf'].means_,\n                           cached_pipe.named_steps['transf'].means_)\n        assert_false(hasattr(transf, 'means_'))\n        # Check that we are reading the cache while fitting\n        # a second time\n        cached_pipe.fit_transform(X, y)\n        # Check that cached_pipe and pipe yield identical results\n        assert_array_equal(pipe.predict(X), cached_pipe.predict(X))\n        assert_array_equal(pipe.predict_proba(X), cached_pipe.predict_proba(X))\n        assert_array_equal(pipe.predict_log_proba(X),\n                           cached_pipe.predict_log_proba(X))", "def test_pipeline_memory_partial_fit():\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    cachedir = mkdtemp()\n    memory = Memory(cachedir=cachedir, verbose=10)\n    clf = SVC(probability=True, random_state=0)\n    transf = DummyTransf()\n    pipe = Pipeline([('transf', transf), ('svc', clf)])\n    cached_pipe = Pipeline([('transf', transf), ('svc', clf)],\n                           memory=memory)\n\n    # Partial fit the cached pipeline\n    cached_pipe.partial_fit(X[:50], y[:50])\n    pipe.partial_fit(X[:50], y[:50])\n    assert_array_equal(pipe.predict(X[:50]), cached_pipe.predict(X[:50]))\n    assert_array_equal(pipe.predict_proba(X[:50]),\n                       cached_pipe.predict_proba(X[:50]))\n    assert_array_equal(pipe.predict_log_proba(X[:50]),\n                       cached_pipe.predict_log_proba(X[:50]))\n    assert_array_equal(pipe.score(X[:50], y[:50]),\n                       cached_pipe.score(X[:50], y[:50]))\n    assert_equal(transf.means_.shape, cached_pipe.named_steps['transf'].means_.shape)\n\n    # Check that partial fit works with cached estimators\n    cached_pipe.partial_fit(X[50:], y[50:])\n    pipe.partial_fit(X[50:], y[50:])\n    assert_array_equal(pipe.predict(X), cached_pipe.predict(X))\n    assert_array_equal(pipe.predict_proba(X), cached_pipe.predict_proba(X))\n    assert_array_equal(pipe.predict_log_proba(X),\n                       cached_pipe.predict_log_proba(X))\n    assert_array_equal(pipe.score(X, y), cached_pipe.score(X, y))\n    assert_equal(transf.means_.shape, cached", "def test_pipeline_memory_with_fit_transform():\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    cachedir = mkdtemp()\n    try:\n        memory = Memory(cachedir=cachedir, verbose=10)\n        clf = SVC(probability=True, random_state=0)\n        transf = DummyTransf()\n        pipe = Pipeline([('transf', transf), ('svc', clf)])\n        cached_pipe = Pipeline([('transf', transf), ('svc', clf)],\n                               memory=memory)\n\n        # Memoize the pipeline at the first fit\n        cached_pipe.fit_transform(X, y)\n        pipe.fit_transform(X, y)\n        # Check that cached_pipe and pipe yield identical results\n        assert_array_equal(pipe.predict(X), cached_pipe.predict(X))\n        assert_array_equal(pipe.predict_proba(X), cached_pipe.predict_proba(X))\n        assert_array_equal(pipe.predict_log_proba(X),\n                           cached_pipe.predict_log_proba(X))\n\n\n    finally:\n        shutil.rmtree(cachedir)\n\n\n\n\n"], "sample_288": ["    def test_nested_contains(self):\n        obj = NullableJSONModel.objects.create(\n            value={'d': [{'e': 'f', 'g': 'h'}, {'i': 'j', 'k': 'l'}]}\n        )\n        self.assertIs(\n            NullableJSONModel.objects.filter(value__d__0__contains={'e': 'f'}).exists(),\n            True,\n        )\n        self.assertIs(\n            NullableJSONModel.objects.filter(value__d__1__contains={'i': 'j'}).exists(),\n            True,\n        )\n        self.assertIs(\n            NullableJSONModel.objects.filter(value__d__0__contains={'g': 'h'}).exists(),\n            True,\n        )\n        self.assertIs(\n            NullableJSONModel.objects.filter(value__d__0__contains={'m': 'n'}).exists(),\n            False,\n        )\n", "    def test_key_transform_with_function(self):\n            return value + '!'\n\n        qs = NullableJSONModel.objects.filter(\n            value__bax__icontains=F('value__foo').transform(my_transform)\n        )\n        self.assertSequenceEqual(qs, [self.objs[7]])\n\n", "    def test_key_transform_with_subquery(self):\n        subquery_value = Subquery(NullableJSONModel.objects.filter(pk=OuterRef('pk')).values('value'))\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.annotate(\n                subquery_value=subquery_value,\n                key=KeyTransform('d', subquery_value),\n            ).filter(key__gt=14),\n            self.objs[4: ],\n        )\n", "    def test_contains_contained_by_with_key_transform_expressions(self):\n        tests = [\n            ('value__d__contains', F('value__f')),\n            ('value__baz__contains', F('value__a')),\n            ('value__contains', KeyTransform('bax', F('value__foo'))),\n            (\n                'value__contained_by',\n                KeyTransform(\n                    'x',\n                    RawSQL(self.raw_sql, ['{\"x\": {\"a\": F(\"value__a\"), \"c\": 1, \"d\": \"e\"}}']),\n                ),\n            ),\n        ]\n        for lookup, value in tests:\n            with self.subTest(lookup=lookup, value=value):\n                self.assertIs(NullableJSONModel.objects.filter(**{lookup: value}).exists(), True)\n\n\n\n", "    def test_contains_contained_by_with_key_transform_subquery(self):\n        obj = NullableJSONModel.objects.create(value={'baz': {'a': 'b', 'c': 'd'}})\n        qs = NullableJSONModel.objects.filter(\n            value__contains=Subquery(\n                NullableJSONModel.objects.filter(pk=OuterRef('pk')).values('value__baz'),\n            )\n        )\n        self.assertSequenceEqual(qs, [obj])\n\n"], "sample_279": ["    def test_opclasses_database_constraint(self):\n        opclasses_1 = ['text_pattern_ops', 'varchar_pattern_ops']\n        opclasses_2 = ['jsonb_path_ops', 'text_pattern_ops']\n        with self.subTest('same opclasses'):\n            UniqueConstraintProduct.objects.create(name='p1', color='red', opclasses=opclasses_1)\n            UniqueConstraintProduct.objects.create(name='p2', color='blue', opclasses=opclasses_1)\n            with self.assertRaises(IntegrityError):\n                UniqueConstraintProduct.objects.create(name='p3', color='red', opclasses=opclasses_1)\n\n        with self.subTest('different opclasses'):\n            UniqueConstraintProduct.objects.create(name='p1', color='red', opclasses=opclasses_1)\n            UniqueConstraintProduct.objects.create(name='p2', color='blue', opclasses=opclasses_2)\n            UniqueConstraintProduct.objects.create(name='p3', color='red', opclasses=opclasses_1)\n\n\n\n", "    def test_invalid_opclasses_length(self):\n        msg = 'UniqueConstraint.fields and UniqueConstraint.opclasses must have the same number of elements.'\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                fields=['field1', 'field2'],\n                name='uniq_opclasses',\n                opclasses=['opclass1'],\n            )\n", "    def test_opclasses_database_constraint(self):\n        with self.assertRaises(IntegrityError):\n            UniqueConstraintProduct.objects.create(name='p1', color='red', description='text pattern')\n", "    def test_opclasses_must_have_same_length_as_fields(self):\n        with self.assertRaisesMessage(ValueError, 'UniqueConstraint.fields and UniqueConstraint.opclasses must have the same number of elements.'):\n            models.UniqueConstraint(\n                fields=['field1', 'field2'],\n                name='uniq_opclasses',\n                opclasses=['opclass1'],\n            )\n\n", "    def test_opclasses_length(self):\n        msg = 'UniqueConstraint.fields and UniqueConstraint.opclasses must have the same number of elements.'\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                fields=['field1', 'field2'],\n                name='uniq_opclasses',\n                opclasses=['opclass1'],\n            )\n"], "sample_1202": ["compilation error", "def test_mod_division():\n    assert S(3) % S(2) == 1\n    assert S(3) % S.Half == 1\n    assert Float(3) % Float(2) == 1\n    assert Float(3) % Float(.5) == 1\n    assert (S(3) % S(0)).is_nan\n    assert (Float(3) % Float(0)).is_nan\n    assert (-3) % 2 == 1\n    assert (-3) % (-2) == 1\n    assert 3 % (-2) == 1\n    assert (-3) % (-2) == 1\n    \n", "compilation error", "compilation error", "def test_issue_10356():\n    assert S(1).is_number is True\n    assert S(1).is_integer is True\n    assert S(1).is_rational is True\n    assert S(1).is_real is True\n    assert S(1).is_complex is False\n    assert S(1j).is_number is True\n    assert S(1j).is_integer is False\n    assert S(1j).is_rational is False\n    assert S(1j).is_real is False\n    assert S(1j).is_complex is True\n"], "sample_1031": ["def test_scale_factor_consistency():\n    mass_units = [kg, milligram, gram, ton]\n    for unit in mass_units:\n        unit.set_dimension(mass)\n        unit.set_scale_factor(Random.randint(1, 1000))\n        assert unit.get_dimensional_expr() == 1 \n", "def test_quantity_dimensions_conversion():\n    mass = Quantity(\"mass\")\n    mass.set_dimension(mass)\n    mass.set_scale_factor(kg)\n\n    energy = Quantity(\"energy\")\n    energy.set_dimension(energy)\n    energy.set_scale_factor(joule)\n\n    time = Quantity(\"time\")\n    time.set_dimension(time)\n    time.set_scale_factor(second)\n\n    assert mass.dim == Dimension(mass)\n    assert energy.dim == Dimension(energy)\n    assert time.dim == Dimension(time)\n\n", "compilation error", "def test_quantity_addition():\n    m = Quantity(\"m\")\n    m.set_dimension(length)\n    m.set_scale_factor(1)\n    n = Quantity(\"n\")\n    n.set_dimension(length)\n    n.set_scale_factor(1)\n\n    result = m + n\n    assert result.name == \"m + n\"\n    assert result.dimension == length \n", "def test_derived_units():\n    ms = UnitSystem((m, s), (c,))\n    A = Quantity(\"A\")\n    A.set_dimension(current)\n    A.set_scale_factor(S.One)\n    Js = Quantity(\"Js\")\n    Js.set_dimension(action)\n    Js.set_scale_factor(1)\n    \n    ms.extend((A,), (Js,))\n    \n    assert ms.derived_dims == (velocity,)\n    assert ms.get_derived_unit(velocity) is not None\n"], "sample_1063": ["def test_issue_17047():\n    if not scipy:\n        skip(\"scipy not installed\")\n\n    from sympy import sin, cos\n    from sympy.physics.scphysics import airy\n\n    x = symbols('x')\n    f1 = airy(x)\n    F1 = lambdify(x, f1, modules='scipy')\n    assert abs(airy(1.5) - F1(1.5)) <= 1e-10\n\n    f2 = sin(x)\n    F2 = lambdify(x, f2, modules='scipy')\n    assert abs(sin(1.5) - F2(1.5)) <= 1e-10\n\n    f3 = cos(x)\n    F3 = lambdify(x, f3, modules='scipy')\n    assert abs(cos(1.5) - F3(1.5)) <= 1e-10\n", "compilation error", "compilation error", "def test_issue_17142():\n    if not scipy:\n        skip(\"scipy not installed\")\n\n    from sympy.special.gamma_functions import gamma\n    x = symbols('x')\n    f = gamma(x)\n    F = lambdify(x, f, modules='scipy')\n\n    assert abs(gamma(1.3) - F(1.3)) <= 1e-10\n\n\n", "def test_special_functions_scipy():\n    if not scipy:\n        skip(\"scipy not installed\")\n\n    f = lambdify(x, besselj(0, x))\n    assert abs(besselj(0, 1.3) - f(1.3)) <= 1e-10\n    f = lambdify(x, bessely(0, x))\n    assert abs(bessely(0, 1.3) - f(1.3)) <= 1e-10\n    f = lambdify(x, besseli(0, x))\n    assert abs(besseli(0, 1.3) - f(1.3)) <= 1e-10\n    f = lambdify(x, besselk(0, x))\n    assert abs(besselk(0, 1.3) - f(1.3)) <= 1e-10\n"], "sample_126": ["    def test_add_blank_textfield_and_charfield_with_default(self, mocked_ask_method):\n        \"\"\"\n        #23405 - Adding a blank `CharField` or `TextField` with a default\n        should not prompt for a default.\n        \"\"\"\n        changes = self.get_changes([self.author_empty], [self.author_with_biography_blank_default])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0)\n\n\n\n", "    def test_add_blank_textarea(self):\n        \"\"\"\n        #23405 - Adding a NOT NULL and blank `TextField`\n        should not prompt for a default.\n        \"\"\"\n        changes = self.get_changes([self.author_empty], [self.author_with_biography_textarea_blank])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0)\n", "    def test_add_field_after_removing_related_model(self):\n        \"\"\"\n        #23440 - Adding a field to a model after removing its related model\n        works correctly.\n        \"\"\"\n        changes = self.get_changes(\n            [self.author_with_publisher], [self.author_with_publisher_and_extra_field]\n        )\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['RemoveField', 'AddField'])\n        self.assertEqual(changes['testapp'][0].operations[0].field.name, 'publisher')\n        self.assertEqual(changes['testapp'][0].operations[1].field.name, 'extra_field')\n\n", "    def test_add_blank_with_default_textfield_and_charfield(self, mocked_ask_method):\n        \"\"\"\n        #23405 - Adding a NOT NULL and blank `CharField` or `TextField`\n        with a default should not prompt for a default.\n        \"\"\"\n        changes = self.get_changes([self.author_empty], [self.author_with_biography_blank_default])\n        self.assertEqual(mocked_ask_method.call_count, 0)\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, default=\"some string\")\n", "    def test_add_field_no_migration_when_auto_created(self):\n        author = ModelState('testapp', 'Author', [\n            ('id', models.AutoField(primary_key=True)),\n        ], options={'auto_created': True})\n        changes = self.get_changes([], [author])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 0)\n"], "sample_861": ["def test_refit_with_invalid_params():\n    X, y = make_classification(random_state=0)\n    clf = SVC()\n    gs = GridSearchCV(clf, {'C': [1, 2, 3]}, cv=3, refit=True,\n                      error_score=0.0)\n    gs.fit(X, y)\n    with pytest.raises(ValueError, match=\"refit=True\"):\n        gs._refit(None)\n\n    with pytest.raises(ValueError, match=\"refit=True\"):\n        gs._refit(random.choice(gs.cv_results_['params']))\n\n\n", "def test_param_grid_update_on_fit():\n    class UpdatingClassifier(BaseEstimator):\n            self.param = param\n            self.fitted = False\n\n            self.fitted = True\n            return self\n\n            return np.zeros(X.shape[0])\n\n    param_grid = {'param': [1, 2, 3]}\n    gs = GridSearchCV(UpdatingClassifier(param=0), param_grid, cv=2)\n    assert not gs.best_estimator_.fitted\n\n    gs.fit(X, y)\n    assert gs.best_estimator_.fitted\n\n\n", " def test_grid_search_cv_with_weighted_classes():\n    X, y = make_classification(n_samples=200, n_classes=2,\n                               weights=[0.1, 0.9], random_state=0)\n    \n    clf = LogisticRegression(random_state=0) \n    param_grid = {'C': [0.1, 1, 10]}\n\n    gs = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy',\n                      sample_weight=y.astype('float64'))\n    gs.fit(X, y)\n    \n    assert not np.isnan(gs.best_score_)\n\n", "def test_param_grid_string():\n    param_grid = {'max_depth': [\"1\", \"2\", \"3\"]}\n    with pytest.raises(ValueError,\n                       match=\"All values supplied should be of type object \"\n                             \"or can be passed by List\"):\n        GridSearchCV(SVC(), param_grid=param_grid, cv=2).fit(X, y)\n", "compilation error"], "sample_433": ["    def test_rename_field_in_model_with_index_together(self):\n        initial_author = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n                (\"age\", models.IntegerField()),\n            ],\n            {\n                \"indexes\": [\n                    models.Index(fields=[\"name\"], name=\"name_index\")\n                ],\n                \"index_together\": {(\"name\", \"age\")},\n            },\n        )\n        author_with_new_field = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"fullname\", models.CharField(max_length=200)),\n                (\"age\", models.IntegerField()),\n            ],\n            {\n                \"indexes\": [\n                    models.Index(fields=[\"fullname\"], name=\"fullname_index\")\n                ],\n                \"index_together\": {(\"fullname\", \"age\")},\n            },\n        )\n        changes = self.get_changes(\n            [initial_author],\n            [author_with_new_field],\n            MigrationQuestioner({\"ask_rename\": True}),\n        )\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(\n            changes,\n            \"testapp\",\n            0,\n            [\"RenameField\", \"AlterIndexTogether\"],\n        )\n", "    def test_suggest_name_with_custom_migration_name(self):\n        class Migration(migrations.Migration):\n            name = \"custom_migration_name\"\n            operations = [\n                migrations.CreateModel(\"Person\", fields=[]),\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"custom_migration_name\")\n", "    def test_combined_operations(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\"Person\", fields=[]),\n                migrations.AddField(\n                    \"Person\", \"address\", models.CharField(max_length=200),\n                ),\n                migrations.DeleteModel(\"Animal\"),\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"person_add_address_delete_animal\")\n", "    def test_rename_field_and_index_together_with_related_model(self):\n        changes = self.get_changes(\n            [AutodetectorTests.author_empty, self.book_index_together_3],\n            [\n                AutodetectorTests.author_empty,\n                ModelState(\n                    \"otherapp\",\n                    \"Book\",\n                    [\n                        (\"id\", models.AutoField(primary_key=True)),\n                        (\"author_id\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n                        (\"title\", models.CharField(max_length=200)),\n                    ],\n                    {\"index_together\": {(\"title\", \"author_id\")}},\n                ),\n            ],\n            MigrationQuestioner({\"ask_rename\": True}),\n        )\n        self.assertNumberMigrations(changes, \"otherapp\", 1)\n        self.assertOperationTypes(\n            changes,\n            \"otherapp\",\n            0,\n            [\"RenameField\", \"AlterIndexTogether\"],\n        )\n        self.assertOperationAttributes(\n            changes,\n            \"otherapp\",\n            0,\n            1,\n            name=\"book\",\n            index_together={(\"title\", \"author_id\")},\n        )\n\n", "    def test_operation_with_empty_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    name=\"\",\n                    fields=[models.CharField(max_length=200)],\n                )\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"empty_model_name\")\n"], "sample_503": ["def test_marker_size_scaling(fig_test, fig_ref, marker):\n    fig_test.subplots().plot([0, 1, 2], [1, 2, 3], marker=marker, markersize=1)\n    fig_ref.subplots().plot([0, 1, 2], [1, 2, 3], marker=marker, markersize=10)\n", "def test_get_bbox():\n    fig, ax = plt.subplots()\n    line, = ax.plot([1, 2, 3], [1, 2, 3])\n    bbox = line.get_bbox()\n    assert bbox.width > 0\n    assert bbox.height > 0\n", "def test_marker_placement_at_infinities(fig_test, fig_ref):\n    x = np.array([np.inf, 0, np.inf])\n    y = np.array([0, 1, 0])\n\n    fig_test.add_subplot().plot(x, y, marker='o')\n    fig_ref.add_subplot().plot(x, y, marker='o', markevery=[0, 1]) \n", "def test_line_marker_order(fig_test, fig_ref):\n    x = np.linspace(0, 1, 10)\n    y = np.sin(x)\n\n    fig_test.add_subplot().plot(x, y, marker='o', markersize=10,\n                                linestyle=\"-\", color='red', markerfacecolor='blue')\n    fig_ref.add_subplot().plot(x, y, marker='o', markersize=10,\n                                linestyle=\"-\", color='red', markerfacecolor='blue')\n", "def test_get_path_simplified(fig_test, fig_ref):\n    \"\"\"This test verifies the simplification logic in `_get_path`. \"\"\"\n    line = mlines.Line2D([0, 1, 2, 3, 4], [0, 1, 2, 1, 0], simplify=True)\n    fig_test.add_subplot().plot([0, 1, 2, 3, 4], [0, 1, 2, 1, 0], *[line._get_path()])\n    fig_ref.add_subplot().plot([0, 1, 2, 3, 4], [0, 1, 2, 1, 0], simplify=True)\n\n"], "sample_342": ["    def test_search_fields_with_related_model_pk(self):\n        class PKChildAdmin(admin.ModelAdmin):\n            search_fields = ['parent__name']\n\n        with model_admin(PKChild, PKChildAdmin):\n            request = self.factory.get(self.url, {'term': 'anna', 'field_name': 'parent'})\n            request.user = self.superuser\n            response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n            self.assertEqual(response.status_code, 200)\n            data = json.loads(response.content.decode('utf-8'))\n            self.assertEqual(data, {\n                'results': [{'id': str(c.pk), 'text': c.name} for c in PKChild.objects.filter(parent__name='Bertie')],\n                'pagination': {'more': False},\n            })\n", "    def test_no_related_model_error(self):\n        class QuestionAdmin(admin.ModelAdmin):\n            autocomplete_fields = ['no_related_field']\n\n        with model_admin(Question, QuestionAdmin):\n            with self.assertRaises(PermissionDenied):\n                self.selenium.get(self.live_server_url + reverse('autocomplete_admin', kwargs={'app_label': 'admin_views', 'model_name': 'question'}))\n\n", "    def test_to_field_name_with_custom_to_field(self):\n        class CustomToFieldAnswerAdmin(AnswerAdmin):\n            to_field_names = {'related_questions': 'related_questions'}\n\n        with model_admin(Answer, CustomToFieldAnswerAdmin):\n            q = Question.objects.create(question='Question with custom to_field', related_questions=None)\n            request = self.factory.get(self.url, {'term': 'quest', **self.opts, 'field_name': 'related_questions'})\n            request.user = self.superuser\n            response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n            self.assertEqual(response.status_code, 200)\n            data = json.loads(response.content.decode('utf-8'))\n            self.assertEqual(data, {\n                'results': [{'id': str(q.pk), 'text': q.related_questions.question}],\n                'pagination': {'more': False},\n            })\n", "    def test_search_with_no_results(self):\n        with self.subTest(search_term='does_not_exist'):\n            Question.objects.all().delete()\n            self.selenium.get(self.live_server_url + reverse('autocomplete_admin:admin_views_question_add'))\n            elem = self.selenium.find_element_by_css_selector('.select2-search__field')\n            elem.send_keys('does_not_exist')\n            results = self.selenium.find_element_by_css_selector('.select2-results')\n            self.assertTrue(results.is_displayed())\n            options = results.find_elements_by_css_selector('.select2-results__option')\n            self.assertEqual(len(options), 1)\n            self.assertEqual(options[0].text, 'No results found')\n\n", "    def test_non_utf8_text(self):\n        q = Question.objects.create(question='\u00bfQui\u00e9n soy?')\n        request = self.factory.get(self.url, {'term': 'quien', **self.opts})\n        request.user = self.superuser\n        response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n        self.assertEqual(response.status_code, 200)\n        data = json.loads(response.content.decode('utf-8'))\n        self.assertEqual(data['results'], [{'id': str(q.pk), 'text': '\u00bfQui\u00e9n soy?'}])\n"], "sample_248": ["    def test_stdin_read_empty_input(self, select):\n        with captured_stdin() as stdin, captured_stdout() as stdout:\n            stdin.write('')\n            stdin.seek(0)\n            call_command('shell')\n        self.assertEqual(stdout.getvalue().strip(), '')\n", "    def test_shell_with_bpython_not_installed(self, select):\n        select.return_value = ([], [], [])\n        with self.assertRaisesMessage(CommandError, \"Couldn't import bpython interface.\"):\n            call_command('shell', interface='bpython')\n", "    def test_python_shell_with_readline(self):\n        import readline\n        with self.assertLogs('test', 'INFO') as cm:\n            call_command('shell', interface='python')\n        self.assertEqual(cm.records[0].getMessage(), __version__)\n", "    def test_shell_with_bpython_not_installed(self, select):\n        select.return_value = ([], [], [])\n        with self.assertRaisesMessage(CommandError, \"Couldn't import bpython interface.\"):\n            call_command('shell', interface='bpython')\n", "    def test_stdin_read_with_no_startup(self, select):\n        with captured_stdin() as stdin, captured_stdout() as stdout:\n            stdin.write(self.script_globals)\n            stdin.seek(0)\n            call_command('shell', no_startup=True)\n        self.assertEqual(stdout.getvalue().strip(), 'True')\n"], "sample_280": ["    def test_aggregation_default_with_expression(self):\n        result = Publisher.objects.filter(\n            num_awards__gt=3\n        ).annotate(\n            avg_rating=Avg('book__rating'),\n            discount_ratio=Avg('book__discount', default=1.0),\n        ).values_list('avg_rating', 'discount_ratio').order_by('avg_rating')\n\n        expected = [\n            (4.5, 0.75),\n            (4.5, 0.5),\n            (4.4, 0.65),\n        ]\n        self.assertSequenceEqual(list(result), expected)\n", "    def test_aggregation_default_using_boolean_from_python(self):\n        result = Publisher.objects.filter(num_awards__lt=2).aggregate(\n            value=Count('book', default=False),\n        )\n        self.assertEqual(result['value'], False)\n", "    def test_aggregation_default_using_float_from_python(self):\n        result = Author.objects.filter(age__gt=100).aggregate(\n            value=Avg('age', default=float('nan')),\n        )\n        self.assertIs(result['value'], float('nan'))\n", "    def test_aggregation_default_with_filter(self):\n        with self.assertNumQueries(1):\n            result = Book.objects.filter(rating__gt=4.0).annotate(\n                rating_weighted=Avg('price', default=0, filter=Q(rating__gt=3.5)),\n            ).values_list('rating_weighted', flat=True)\n        self.assertSequenceEqual(result, [\n            25.0,\n            30.0,\n            40.0,\n            28.0,\n            27.0,\n            24.0,\n        ])\n\n", "    def test_aggregation_default_using_float_from_python(self):\n        result = Book.objects.filter(rating__lt=3.0).aggregate(\n            value=Sum('price', default=3.14),\n        )\n        self.assertAlmostEqual(result['value'], Decimal('3.14'), places=2)\n"], "sample_403": ["    def test_references_field_by_related_name(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ForeignKey(\"Other\", models.CASCADE, related_name=\"related_name_field\"),\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"related_name_field\", \"migrations\"), True\n        )\n        self.assertIs(operation.references_field(\"Other\", \"whatever\", \"migrations\"), False)\n        self.assertIs(\n            operation.references_field(\"Missing\", \"whatever\", \"migrations\"), False\n        )\n", "    def test_references_field_with_through_model(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ManyToManyField(\"Other\", through=\"ThroughModel\"),\n        )\n        self.assertIs(\n            operation.references_field(\"ThroughModel\", \"whatever\", \"migrations\"),\n            True,\n        )\n\n\n\n", "    def test_references_field_by_through_complex(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ManyToManyField(\n                \"Other\",\n                through=\"Through\",\n                related_name=\"through_field\",\n                symmetrical=False,\n            ),\n        )\n        self.assertIs(\n            operation.references_field(\"Through\", \"whatever\", \"migrations\"), True\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"whatever\", \"migrations\"), True\n        )\n        self.assertIs(\n            operation.references_field(\"Model\", \"whatever\", \"migrations\"), True\n        )\n", "    def test_references_field_by_related_name(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ForeignKey(\"Other\", models.CASCADE, related_name=\"my_related_name\"),\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"my_related_name\", \"migrations\"), True\n        )\n        self.assertIs(operation.references_field(\"Other\", \"whatever\", \"migrations\"), False)\n\n\n\n", "    def test_references_field_by_through_reverse_field(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ManyToManyField(\n                \"Other\", through=\"Through\", related_name=\"reverse_relationship\"\n            ),\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"reverse_relationship\", \"migrations\"),\n            True,\n        )\n        self.assertIs(\n            operation.references_field(\"Through\", \"reverse_relationship\", \"migrations\"),\n            True,\n        )\n"], "sample_821": ["def test_affinity_propagation_sparse_data():\n    # Test AffinityPropagation with sparse data\n    X = csr_matrix([[1, 0, 0], [0, 2, 0], [0, 0, 3]])\n    af = AffinityPropagation(affinity='precomputed')\n    labels = af.fit(X).labels_\n    assert_array_equal(labels, np.array([0, 1, 2]))\n", "def test_affinity_propagation_sparse_matrix():\n    # Test Affinity Propagation with sparse matrix input\n    X_sparse = csr_matrix(X)\n    af = AffinityPropagation(affinity=\"euclidean\", copy=False)\n    af.fit(X_sparse)\n    assert_array_equal(af.cluster_centers_,\n                       af.fit(X).cluster_centers_)\n    assert_array_equal(af.labels_, af.fit(X).labels_)\n", "def test_affinity_propagation_sparse_input():\n    # Test AffinityPropagation with sparse input data\n    n_samples, n_features = 100, 50\n    X = np.random.rand(n_samples, n_features)\n    X_sparse = csr_matrix(X)\n\n    af = AffinityPropagation()\n    af.fit(X_sparse)\n    labels = af.labels_\n\n    af2 = AffinityPropagation()\n    labels2 = af2.fit(X).labels_\n\n    assert_array_equal(labels, labels2)\n\n    # Test with precomputed affinity matrix\n    S = -euclidean_distances(X, squared=True).tocsr()\n    af = AffinityPropagation(affinity=\"precomputed\")\n    af.fit(S)\n    labels = af.labels_\n\n    af2 = AffinityPropagation(affinity=\"precomputed\")\n    labels2 = af2.fit(S).labels_\n\n    assert_array_equal(labels, labels2)\n", "def test_affinity_propagation_sparse_data():\n    X = csr_matrix(np.random.rand(100, 5))\n    af = AffinityPropagation()\n    af.fit(X)\n    assert_array_equal(af.cluster_centers_indices_, np.array([], dtype=int))\n    assert isinstance(af.affinity_matrix_, csr_matrix)\n\n\n\n", "def test_affinity_propagation_preference_impact():\n    # Test the impact of preference on the number of clusters\n    X = np.array([[1, 2], [1, 4], [1, 0],\n                 [4, 2], [4, 4], [4, 0]])\n\n    # Similarities\n    S = -euclidean_distances(X, squared=True)\n\n    # Small preference - should result in a single cluster\n    preference = np.min(S)\n    cluster_centers, labels = affinity_propagation(S, preference=preference)\n    assert_equal(len(cluster_centers), 1)\n\n    # Medium preference - should result in 2 clusters\n    preference = np.median(S)\n    cluster_centers, labels = affinity_propagation(S, preference=preference)\n    assert_equal(len(cluster_centers), 2)\n\n    # Large preference - should result in 3 clusters\n    preference = np.max(S)\n    cluster_centers, labels = affinity_propagation(S, preference=preference)\n    assert_equal(len(cluster_centers), 3)\n"], "sample_290": ["    def test_rename_field(self):\n        before = [\n            ModelState('app', 'Model', [\n                ('name', models.CharField(max_length=200)),\n            ]),\n        ]\n        after = [\n            ModelState('app', 'Model', [\n                ('title', models.CharField(max_length=200)),\n            ]),\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['RenameField'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='name', model_name='Model', to='title')\n", "    def test_non_ascii_model_name(self):\n        class Migration(migrations.Migration):\n            operations = [migrations.CreateModel('\u00c4\u00d6\u00dc\u00df', fields=[])]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(migration.suggest_name(), '\u00e4\u00f6\u00fc\u00df')\n\n", "    def test_rename_field_operation_suggestion(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.RenameField(\n                    model_name='Person',\n                    old_name='first_name',\n                    new_name='given_name',\n                ),\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'rename_first_name_to_given_name')\n\n\n\n", "    def test_operation_with_custom_sql(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.RunSQL(\n                    'CREATE EXTENSION IF NOT EXISTS IF EXIST \"some_extension\"')\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'create_extension_some_extension') \n", "    def test_suggest_name_with_empty_operations(self):\n        class Migration(migrations.Migration):\n            operations = []\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'empty') \n"], "sample_638": ["def test_project_name_is_set_correctly(mock_writer, capsys):\n    \"\"\"Test that the project name is set correctly.\"\"\"\n    main.Run([\"-p\", \"MyProject\", TEST_DATA_DIR])\n    assert mock_writer.DiagramWriter().project_name == \"MyProject\"\n", "def test_project_name_handling():\n    with fix_import_path([TEST_DATA_DIR]):\n        with mock.patch(\"pylint.pyreverse.main.Linker\") as mock_linker:\n            with mock.patch(\"pylint.pyreverse.main.DiadefsHandler\") as mock_handler:\n                main.Run([\"--project\", \"MyProject\", TEST_DATA_DIR])\n                mock_linker.assert_called_once_with(project, tag=True)\n                mock_handler.assert_called_once_with(mock.ANY)\n\n\n", "def test_run_with_no_arguments(mock_writer, capsys):\n    \"\"\"Test that pyreverse exits with an error message if no arguments are given.\"\"\"\n    with pytest.raises(SystemExit) as wrapped_sysexit:\n        main.Run([])\n    assert wrapped_sysexit.value.code != 0\n    assert \"usage: pyreverse [options] <packages>\" in capsys.readouterr().out\n\n\n\n", "def test_ignored_files_and_directories(mock_writer, capsys):\n    \"\"\"Test that ignored files and directories are correctly skipped.\"\"\"\n    main.Run([\"-i\", \"CVS\", TEST_DATA_DIR])\n    # Assert that pyreverse didn't try to process any files from the ignored list.\n    # You'd need to inspect the mocks here to verify this assertion.\n    # Example: assert mock_writer.DiagramWriter().write.call_count == 0 \n", "def test_output_format_option(mock_writer, capsys):\n    \"\"\"Test that the output format option is passed correctly to the writer.\"\"\"\n    main.Run([\"-o\", \"dot\", TEST_DATA_DIR])\n    mock_writer.DiagramWriter().write.assert_called_once_with(format=\"dot\")\n    assert (\n        f\"Format: dot\"\n        in capsys.readouterr().out\n    ) \n"], "sample_225": ["    def test_register_unregister(self):\n        class MyModel(models.Model):\n            pass\n\n        # Register the model\n        self.site.register(MyModel)\n        self.assertTrue(isinstance(self.site._registry[MyModel], admin.ModelAdmin))\n        # Unregister the model\n        self.site.unregister(MyModel)\n        self.assertFalse(MyModel in self.site._registry)\n\n        with self.assertRaises(admin.NotRegistered):\n            self.site._registry[MyModel]\n", "    def test_register_unregister(self):\n        class ModelOne(object):\n            pass\n        self.site.register(ModelOne)\n        self.assertIn(ModelOne, self.site._registry)\n        self.site.unregister(ModelOne)\n        self.assertNotIn(ModelOne, self.site._registry)\n\n        with self.assertRaises(AlreadyRegistered):\n            self.site.register(ModelOne)\n", "    def test_unregister(self):\n        class MyAdmin(admin.ModelAdmin):\n            pass\n        self.site.register(Article, MyAdmin)\n        self.assertIn(Article, self.site._registry)\n        self.site.unregister(Article)\n        self.assertNotIn(Article, self.site._registry)\n", "    def test_unregister_model(self):\n        class DummyAdmin(admin.ModelAdmin):\n            pass\n\n        model = type('DummyModel', (object,), {})\n        reg_admin = DummyAdmin()\n        self.site.register(model, reg_admin)\n        self.assertIn(model, self.site._registry)\n        self.site.unregister(model)\n        self.assertNotIn(model, self.site._registry)\n", "    def test_unregister(self):\n        self.site.register(Article)\n        self.assertIn(Article, self.site._registry)\n\n        self.site.unregister(Article)\n        self.assertNotIn(Article, self.site._registry)\n\n\n"], "sample_743": ["def test_sparse_metric_callable_non_sparse_input():\n        assert_raises(ValueError, 'Input must be sparse matrices',\n                      sparse_matrix_to_dense, x)\n\n    X = np.random.rand(10, 10)\n    classifier = neighbors.KNeighborsClassifier(n_neighbors=1, metric=sparse_metric)\n    assert_raises(ValueError, classifier.fit, X, np.ones(10))\n\n", "compilation error", "compilation error", "compilation error", "def test_sparse_matrix_params():\n    X = csr_matrix([[1, 2], [3, 4]])\n    y = np.array([0, 1])\n\n    nbrs1 = neighbors.KNeighborsClassifier(n_neighbors=1, algorithm='brute')\n    nbrs1.fit(X, y)\n\n    assert_array_almost_equal(nbrs1.kneighbors(X)[0],\n                              nbrs1.kneighbors(X.toarray())[0])\n\n    nbrs2 = neighbors.KNeighborsClassifier(n_neighbors=1, algorithm='ball_tree')\n    nbrs2.fit(X, y)\n\n    assert_array_almost_equal(nbrs2.kneighbors(X)[0],\n                              nbrs2.kneighbors(X.toarray())[0])\n\n\n"], "sample_546": ["def test_interactive_colorbar_axes_limits():\n    fig, ax = plt.subplots()\n    data = np.arange(12).reshape((4, 3))\n    coll = ax.imshow(data, cmap='viridis', interpolation='nearest')\n    cbar = fig.colorbar(coll, ax=ax)\n\n    # Set initial axes limits\n    ax.set_xlim([0, 3])\n    ax.set_ylim([0, 3])\n\n    # Zoom in on the colorbar\n    # Mouse from 0.2 to 0.8 (data coordinates, \"d\").\n    vmin, vmax = 0.2, 0.8\n    # Convert to screen coordinates (\"s\").\n    s0 = cbar.ax.transData.transform((vmin, 0.5)).astype(int)\n    s1 = cbar.ax.transData.transform((vmax, 0.5)).astype(int)\n\n    start_event = MouseEvent(\n        \"button_press_event\", fig.canvas, *s0, MouseButton.LEFT)\n    stop_event = MouseEvent(\n        \"button_release_event\", fig.canvas, *s1, MouseButton.LEFT)\n\n    tb = NavigationToolbar2(fig.canvas)\n    tb.zoom()\n    tb.press_zoom(start_event)\n    tb.drag_zoom(stop_event)\n    tb.release_zoom(stop_event)\n\n    # Check that the colorbar's limits have updated\n    assert (cbar.vmin, cbar.vmax) == pytest.approx((vmin, vmax))\n\n    # Check that the axes limits have also updated\n    assert ax.get_xlim() == pytest.approx([0.2, 0.8])\n    assert ax.get_ylim() == pytest.approx([0, 3])\n\n\n\n", "def test_toolmanager_prioritize():\n    with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n        plt.rcParams['toolbar'] = 'toolmanager'\n    fig = plt.gcf()\n    fig.canvas.manager.toolmanager.add_tool('zoom')\n    fig.canvas.manager.toolmanager.add_tool('pan')\n    assert fig.canvas.manager.toolmanager.active_tool == 'zoom'  # Default\n    fig.canvas.manager.toolmanager.activate_tool('pan')\n    assert fig.canvas.manager.toolmanager.active_tool == 'pan'\n\n\n", "def test_figure_tight_layout_subplots_adjust():\n    fig = plt.figure()\n    ax1 = fig.add_subplot(1, 2, 1)\n    ax2 = fig.add_subplot(1, 2, 2)\n\n    ax1.set_title(\"Axis 1\")\n    ax2.set_title(\"Axis 2\")\n\n    fig.tight_layout()\n\n    # Check if the subplot parameters have been adjusted\n    assert ax1.get_position()[0] > 0\n    assert ax2.get_position()[0] > 0 + ax1.get_position()[2]\n    assert ax1.get_position()[1] > 0\n    assert ax2.get_position()[1] > 0\n\n    # Test with specific parameters for subplots_adjust\n    fig.subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.9)\n    fig.tight_layout()\n\n    # Check the adjustments\n    assert ax1.get_position()[0] > 0.1\n    assert ax2.get_position()[0] > 0.1 + ax1.get_position()[2]\n    assert ax1.get_position()[1] > 0.1\n    assert ax2.get_position()[1] > 0.1\n\n", "def test_toolmanager_toggle():\n    with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n        plt.rcParams['toolbar'] = 'toolmanager'\n    fig = plt.gcf()\n    assert 'zoom' in fig.canvas.manager.toolmanager.tools\n    fig.canvas.manager.toolmanager.toggle_tool('zoom')\n    assert 'zoom' not in fig.canvas.manager.toolmanager.tools\n    fig.canvas.manager.toolmanager.toggle_tool('zoom')\n    assert 'zoom' in fig.canvas.manager.toolmanager.tools\n", "def test_tight_layout_preserve_spacing():\n    fig, ax = plt.subplots(2, 2)\n    ax[0, 0].set_title(\"Top Left\")\n    ax[0, 1].set_title(\"Top Right\")\n    ax[1, 0].set_title(\"Bottom Left\")\n    ax[1, 1].set_title(\"Bottom Right\")\n    plt.tight_layout()\n    # Check that the spacing between subplots is preserved\n\n    # Get the bounding boxes of the titles\n    bbox_top_left = ax[0, 0].get_title_position()\n    bbox_top_right = ax[0, 1].get_title_position()\n    bbox_bottom_left = ax[1, 0].get_title_position()\n    bbox_bottom_right = ax[1, 1].get_title_position()\n\n    # Assert that the titles don't overlap\n    assert bbox_top_left[0] < bbox_top_right[0]\n    assert bbox_bottom_left[0] < bbox_bottom_right[0]\n    assert bbox_top_left[1] > bbox_bottom_left[1]\n\n\n"], "sample_428": ["    def test_empty_string(self):\n        self.assertEqual(nformat(\"\"), \"\")\n        self.assertEqual(nformat(\"\"), \".\", decimal_pos=2)\n        self.assertEqual(nformat(\"\"), \".\", grouping=2, thousand_sep=\",\")\n\n\n\n", "    def test_empty_number(self):\n        self.assertEqual(nformat(\"\"), \"\")\n        self.assertEqual(nformat(None), \"\")\n", "    def test_empty_string_number(self):\n        self.assertEqual(nformat(\"\", \".\"), \"\")\n        self.assertEqual(nformat(\"\", \".\", decimal_pos=2), \".00\")\n        self.assertEqual(nformat(\"\", \".\", grouping=2, thousand_sep=\",\"), \"\")\n        self.assertEqual(nformat(\"\", \".\", grouping=2, thousand_sep=\",\", force_grouping=True), \"\")\n        self.assertEqual(nformat(\"\", \".\", decimal_pos=1), \".0\")\n\n\n\n", "    def test_non_numeric_input(self):\n        self.assertEqual(nformat(\"abc\", \".\"), \"abc\")\n        self.assertEqual(nformat(None, \".\"), \"None\")\n        self.assertEqual(nformat(True, \".\"), \"True\")\n        self.assertEqual(nformat(False, \".\"), \"False\")\n", "    def test_grouping_with_empty_string(self):\n        self.assertEqual(nformat(1234, \".\", grouping=\"\", thousand_sep=\",\"), \"1234\")\n"], "sample_587": ["    def test_merge_explicit_coords(self):\n        ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]}, coords={\"x\": [0, 1, 2]})\n        ds2 = xr.Dataset({\"b\": (\"x\", [3, 4]), \"x\": [1, 2]})\n        expected = xr.Dataset(\n            {\"a\": (\"x\", [1, 2, np.nan]), \"b\": (\"x\", [np.nan, 3, 4])}, {\"x\": [0, 1, 2]}\n        )\n        actual = ds1.merge(ds2, explicit_coords=(\"x\",))\n        assert expected.identical(actual)\n\n        actual = ds2.merge(ds1, explicit_coords=(\"x\",))\n        assert expected.identical(actual)\n", "    def test_merge_overwrite_vars(self):\n        data = create_test_data()\n        ds1 = data[[\"var1\", \"var2\"]]\n        ds2 = data[[\"var3\", \"var2\"]]\n        expected = data[[\"var1\", \"var2\", \"var3\"]]\n        with raises_regex(ValueError, \"cannot be None or an empty string\"):\n            ds1.merge(ds2, overwrite_vars=None)\n        with raises_regex(ValueError, \"cannot be None or an empty string\"):\n            ds1.merge(ds2, overwrite_vars=\"\")\n\n        actual = ds1.merge(ds2, overwrite_vars=[\"var2\"])\n        assert expected.identical(actual)\n\n        actual = ds1.merge(ds2, overwrite_vars=[\"var1\", \"var2\"])\n        assert expected.identical(actual)\n\n        actual = ds1.merge(ds2, overwrite_vars=list(ds2))\n        assert expected.identical(actual)\n", "    def test_merge_exact_join(self):\n        ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]})\n        ds2 = xr.Dataset({\"b\": (\"x\", [3, 4]), \"x\": [1, 2]})\n        with raises_regex(ValueError, \"indexes .* not equal\"):\n            ds1.merge(ds2, join=\"exact\")\n", "    def test_merge_with_explicit_coords(self):\n        data = create_test_data()\n        data_with_coords = data.copy(deep=True)\n        data_with_coords[\"new_coords\"] = ((\"y\",), [0, 1, 2])\n        data_with_coords_mismatch = data.copy(deep=True)\n        # mismatch in explicit coords\n        data_with_coords_mismatch[\"new_coords\"] = ((\"x\",), [0, 1, 2])\n        \n        with pytest.raises(ValueError):\n            data_with_coords.merge(data_with_coords_mismatch)\n\n        with pytest.raises(ValueError):\n            data_with_coords_mismatch.merge(data_with_coords)\n        \n        actual = data_with_coords.merge(data)\n        expected = data_with_coords.merge(data.reset_coords(drop=True))\n        assert expected.identical(actual)\n\n\n\n", "    def test_merge_explicit_coords(self):\n        data = create_test_data()\n\n        ds1 = data\n        ds2 = data.rename({\"var1\": \"var2\"})\n\n        expected = xr.Dataset(\n            {\"var1\": data.var1, \"var2\": data.var2, \"dim1\": data.dim1},\n            {\"dim1\": data.dim1},\n        )\n        actual = ds1.merge(ds2, explicit_coords=[\"var1\", \"var2\"])\n        assert expected.identical(actual)\n\n        actual = ds2.merge(ds1, explicit_coords=[\"var1\", \"var2\"])\n        assert expected.identical(actual)\n\n        with pytest.raises(ValueError):\n            ds1.merge(ds2, explicit_coords=[\"var3\"])\n        \n        with pytest.raises(ValueError):\n            ds1.merge(ds2, explicit_coords=[\"var1\", \"dim1\"])\n\n\n\n"], "sample_1137": ["def test_issue_17915():\n    from sympy.physics.units import (Quantity, kilogram, meter, second)\n    q1 = Quantity('q1')\n    q2 = Quantity('q2',  dimension=meter**2/second**2)\n    q1.set_global_relative_scale_factor(1, kilogram)\n    q2.set_global_relative_scale_factor(1, kilogram*meter**2/second**2)\n    assert q1 + q2 == Quantity('q1 + q2', dimension=meter**2/second**2 + kilogram)\n    assert q1 + q2 == Quantity('q1 + q2', dimension=meter**2/second**2 + kilogram)\n", "def test_issue_14932():\n    assert (log(inch) - log(2)).simplify() == log(inch/2)\n    assert (log(inch) - log(foot)).simplify() == -log(12)\n    p = symbols('p', positive=True)\n    assert (log(inch) - log(p)).simplify() == log(inch/p)\n", "def test_issue_14548():\n    from sympy import Symbol\n    from sympy.physics.units import joule, second\n    t = Symbol('t')\n    assert (joule/second).expand(complex=True) == joule/second\n    assert (joule/second).expand(function=True) == joule/second\n    assert (joule/second).expand(log=True) == joule/second\n    assert (joule/second).expand(power_base=True) == joule/second\n    assert (joule/second).expand(rational=True) == joule/second\n    assert (joule/second).expand(trig=True) == joule/second\n    assert (joule/second).expand(assumptions=True).subs(t, 0) == joule/second\n\n\n\n", "def test_issue_15216():\n    from sympy.physics.units import (\n        amount_of_substance,\n        convert_to,\n        meter,\n        second,\n    )\n    \n    mol = amount_of_substance\n    assert convert_to(sec*meter, mol) == 1\n", "def test_issue_17349():\n\n    u1 = Quantity('u1', 'meter', 1)\n    u2 = Quantity('u2', 'meter', 2)\n    u3 = Quantity('u3', 'second', 1)\n\n    v1 = u1 * u3\n    v2 = u2 * u3\n    assert v1 * v2 == u1 * u2 * u3**2\n    v3 = u3 / u1\n    assert v3 == 1/(u1*u3)\n\n    expr = v1.subs(u1, 5)\n\n    assert expr == 5 * u3\n\n\n\n"], "sample_780": ["def test_lda_subsampling():\n    # Test LDA perplexity with sub-sampling\n    # perplexity should be lower after each iteration with sub-sampling\n    n_components, X = _build_sparse_mtx()\n    lda_1 = LatentDirichletAllocation(n_components=n_components,\n                                      max_iter=1, learning_method='batch',\n                                      total_samples=100, random_state=0)\n    lda_2 = LatentDirichletAllocation(n_components=n_components,\n                                      max_iter=10, learning_method='batch',\n                                      total_samples=100, random_state=0)\n    lda_1.fit(X)\n    perp_1 = lda_1.perplexity(X, sub_sampling=True)\n\n    lda_2.fit(X)\n    perp_2 = lda_2.perplexity(X, sub_sampling=True)\n    assert_greater_equal(perp_1, perp_2)\n\n\n", "def test_lda_n_components_changed_in_partial_fit():\n    # Test LDA partial_fit with increasing n_components\n    rng = np.random.RandomState(0)\n    X = rng.randint(4, size=(20, 10))\n    n_components_1 = 2\n    n_components_2 = 5\n    lda = LatentDirichletAllocation(n_components=n_components_1,\n                                    total_samples=20, random_state=rng)\n    lda.partial_fit(X)\n\n    # test with a new n_components\n    with pytest.raises(ValueError, match=r\"Cannot change n_components\"):\n        lda = LatentDirichletAllocation(n_components=n_components_2,\n                    total_samples=20, random_state=rng)\n        lda.partial_fit(X)\n", "def test_lda_partial_fit_reset():\n    # test `partial_fit` method resets state if called again with different data\n    rng = np.random.RandomState(0)\n    n_components = rng.randint(3, 6)\n    n_col = rng.randint(6, 10)\n    X_1 = np.random.randint(4, size=(10, n_col))\n    X_2 = np.random.randint(4, size=(10, n_col + 1))\n    lda = LatentDirichletAllocation(n_components=n_components,\n                                    learning_offset=5., total_samples=20,\n                                    random_state=rng)\n    lda.partial_fit(X_1)\n    lda.partial_fit(X_2)\n    # check that perplexity after fit with X_2 will be different\n    perp_after_fit = lda.perplexity(X_2)\n    perp_after_fit_with_X1 = lda.perplexity(X_1)\n    assert_less(perp_after_fit, perp_after_fit_with_X1)\n", "def test_lda_partial_fit_empty_batches():\n    # Test LDA partial_fit with empty batches.\n    rng = np.random.RandomState(0)\n    n_components = rng.randint(3, 6)\n    n_col = rng.randint(6, 10)\n    X_1 = np.random.randint(4, size=(10, n_col))\n    X_empty = np.empty((0, n_col))\n\n    lda = LatentDirichletAllocation(n_components=n_components,\n                                    learning_offset=5., total_samples=20,\n                                    random_state=rng)\n\n    lda.partial_fit(X_1)\n    lda.partial_fit(X_empty)\n    assert_raises_regexp(ValueError, r'The provided data has 0 non-zero elements',\n                         lda.partial_fit, X_empty)\n\n\n\n", "def test_lda_partial_fit_empty_iter():\n    # Test LDA partial_fit with empty iteration\n    rng = np.random.RandomState(0)\n    n_components, X = _build_sparse_mtx()\n    lda = LatentDirichletAllocation(n_components=n_components,\n                                    learning_offset=5., total_samples=100,\n                                    random_state=rng)\n    lda.partial_fit(X[:5])\n    assert_raises_regexp(ValueError, r'empty iteration',\n                         lda.partial_fit, X[:0])\n\n\n\n"], "sample_609": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_475": ["    def test_actions_not_callable(self):\n        class BandAdmin(ModelAdmin):\n            actions = \"hello\"\n\n        self.assertIsInvalid(\n            BandAdmin,\n            Band,\n            \"The value of 'actions' must be a list of callable objects.\",\n            id=\"admin.E131\",\n        )\n", "    def test_actions_non_function(self):\n        class BandAdmin(ModelAdmin):\n            actions = (\"not_a_function\",)\n\n        self.assertIsInvalid(\n            BandAdmin,\n            Band,\n            \"'not_a_function' must be a callable (function).\"\n            \" admin.E131\",\n        )\n", "    def test_actions_with_permissions_not_unique(self):\n        @admin.action(permissions=(\"custom1\",))\n            pass\n\n        @admin.action(permissions=(\"custom2\",))\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (action1, action2)\n\n        self.assertIsInvalid(\n            BandAdmin,\n            Band,\n            \"BandAdmin must define a has_custom1_permission() and \"\n            \"has_custom2_permission() method for the actions \"\n            \"action1 and action2.\",\n            id=\"admin.E129\",\n        )\n\n\n\n", "    def test_actions_permission_mismatch(self):\n        @admin.action(permissions=[\"add_band\"])\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (action,)\n\n        self.assertIsInvalid(\n            BandAdmin,\n            Band,\n            \"Action 'action' requires 'add_band' permission\",\n            id=\"admin.E132\",\n        )\n\n", "    def test_actions_require_method_name(self):\n        class BandAdmin(ModelAdmin):\n            actions = ({\"name\": lambda modeladmin, request, queryset: None},)\n\n        self.assertIsInvalid(\n            BandAdmin,\n            Band,\n            \"Actions defined in BandAdmin must have a name attribute. This action \"\n            \"is missing a name.\",\n            id=\"admin.E131\",\n        )\n"], "sample_1107": ["def test_count_permutations():\n    assert count_permutations(list(range(3))) == 6\n    assert count_permutations([1, 2, 3]) == 6\n    assert count_permutations([1, 2, 3], 0) == 6\n    assert count_permutations([1, 2, 3], 1) == 3\n    assert count_permutations([1, 2, 3], 2) == 1\n    assert count_permutations([1, 2, 3], 3) == 0\n    assert count_permutations(list(range(10)), 3) == 120\n    assert count_permutations('abc', 0) == 6\n    assert count_permutations('abc', 1) == 3\n    assert count_permutations('abc', 2) == 1 \n    assert count_permutations('abc', 3) == 0   \n\n\n", "def test_derangements_multi():\n    assert list(derangements_multi(3)) == [[1, 2, 0], [2, 0, 1], [0, 1, 2]]\n    assert list(derangements_multi([2, 3, 1])) ==\\\n        [[2, 3, 1], [3, 1, 2], [1, 2, 3]]\n    assert list(derangements_multi([1, 1, 2, 3])) ==\\\n        [[2, 3, 1, 1], [3, 1, 2, 1], [1, 2, 3, 1], [1, 3, 2, 1]]\n\n\n", "def test_is_ordered_permutation():\n    assert is_ordered_permutation(list(range(3))) is True\n    assert is_ordered_permutation(list(range(3))[::-1]) is False\n    assert is_ordered_permutation([1, 2, 1]) is False\n    assert is_ordered_permutation([1, 2, 1, 2]) is True\n    assert is_ordered_permutation([1, 2, 1, 3]) is True\n    assert is_ordered_permutation(list(range(10))) is True\n    assert is_ordered_permutation([1, 1, 2, 3]) is False\n    assert is_ordered_permutation((1, 2, 1, 3)) is False\n    assert is_ordered_permutation((1, 2, 1, 3), directed=True) is True\n", "def test_combinations_with_replacement():\n    assert list(combinations_with_replacement('abc', 2)) == [\n        ('a', 'a'), ('a', 'b'), ('a', 'c'), ('b', 'a'), ('b', 'b'),\n        ('b', 'c'), ('c', 'a'), ('c', 'b'), ('c', 'c')]\n    assert list(combinations_with_replacement('abc', 3)) == [\n        ('a', 'a', 'a'), ('a', 'a', 'b'), ('a', 'a', 'c'),\n        ('a', 'b', 'a'), ('a', 'b', 'b'), ('a', 'b', 'c'),\n        ('a', 'c', 'a'), ('a', 'c', 'b'), ('a', 'c', 'c'),\n        ('b', 'a', 'a'), ('b', 'a', 'b'), ('b', 'a', 'c'),\n        ('b', 'b', 'a'), ('b', 'b', 'b'), ('b', 'b', 'c'),\n        ('b', 'c', 'a'), ('b', 'c', 'b'), ('b', 'c', 'c'),\n        ('c', 'a', 'a'), ('c', 'a', 'b'), ('c', 'a', 'c'),\n        ('c', 'b', 'a'), ('c', 'b', 'b'), ('c', 'b', 'c'),\n        ('c', 'c', 'a'), ('c', 'c', 'b'), ('c', 'c', 'c')]\n\n\n\n", "compilation error"], "sample_564": ["def test_collection_properties(fig_test, fig_ref):\n    ax = fig_ref.add_subplot(projection='3d')\n    verts = [(0, 0, 0), (1, 0, 0), (1, 1, 0), (0, 1, 0), (0, 0, 1)]\n    faces = [(0, 1, 2, 3), (3, 0, 4, 3)]\n    col = art3d.Poly3DCollection(\n        [verts, verts],\n        facecolors='red',\n        edgecolors='blue'\n    )\n    ax.add_collection3d(col)\n\n    ax = fig_test.add_subplot(projection='3d')\n\n    verts = [(0, 0, 0), (1, 0, 0), (1, 1, 0), (0, 1, 0), (0, 0, 1)]\n    faces = [(0, 1, 2, 3), (3, 0, 4, 3)]\n    col = art3d.Poly3DCollection(\n        [verts, verts],\n        facecolors='red',\n        edgecolors='blue',\n        linewidths=2,\n        linestyles='--'\n    )\n    ax.add_collection3d(col)\n", "def test_text3d_vertical_alignment(fig_test, fig_ref):\n    ax = fig_test.add_subplot(projection=\"3d\")\n    txt = Text3D(0.5, 0.5, 0.5, r'Foo', fontsize=10, ha='center', va='center')\n\n    ax.add_artist(txt)\n    ax2 = fig_ref.add_subplot(projection=\"3d\")\n\n    # ensure no ValueError from a bad va\n    text_kwargs = dict(fontsize=10, ha='center', va='center')\n    text = ax2.text(0.5, 0.5, 0.5, r'Foo', **text_kwargs)\n\n", "def test_text3d_rotation(fig_test, fig_ref):\n    ax = fig_ref.add_subplot(projection='3d')\n    txt = art3d.Text3D((0.5, 0.5, 0.5), text='Text', zdir='x')\n    ax.add_artist(txt)\n    ax.view_init(elev=30, azim=45)\n\n    ax = fig_test.add_subplot(projection='3d')\n    txt = art3d.Text3D((0.5, 0.5, 0.5), text='Text', zdir='x')\n    ax.add_artist(txt)\n    ax.view_init(elev=30, azim=45) \n", "def test_artist_transforms():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    t = art3d.transformation_3d\n\n    for func in (t.rotate_x, t.rotate_y, t.rotate_z):\n        x = np.arange(0, 10, 0.1)\n        y = np.arange(0, 10, 0.1)\n        X, Y = np.meshgrid(x, y)\n        Z = np.sin(X) * np.cos(Y)\n        surf = ax.plot_surface(X, Y, Z)\n\n        func(surf, 45, 'z')\n        fig.canvas.draw()\n\n\n\n", "def test_pathpatch_invalid_input_verts(verts, expected_type):\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    with pytest.raises(expected_type):\n        art3d.PathPatch3D(verts=verts, zs=None)\n\n    with pytest.raises(expected_type):\n        art3d.pathpatch_2d_to_3d(None, z=None, zdir='z')\n"], "sample_567": ["def test_text_backgroundcolor_with_bbox_props():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, 'test',\n                   backgroundcolor='red',\n                   bbox={'facecolor': 'green', 'alpha': 0.5})\n    fig.canvas.draw()\n    assert text.get_window_extent().get_color() == 'red'\n\n\n\n\n", "    def test_bbox_clip_path_transparency():\n        fig, ax = plt.subplots()\n        patch = plt.Rectangle((0.1, 0.1), 0.8, 0.8, facecolor='none', edgecolor='r',\n                              linewidth=2, alpha=0.5)\n        ax.add_patch(patch)\n        text = ax.text(0.5, 0.5, 'text', clip_path=patch)\n\n        fig.canvas.draw()\n\n\n\n", "def test_get_bbox_with_rotation():\n    fig = plt.figure()\n    text = fig.text(0.5, 0.5, 'test', rotation=45)\n    bbox = text.get_bbox()\n    assert bbox is not None\n    fig.canvas.draw()\n", "def test_text_clipping_on_off():\n    fig, ax = plt.subplots()\n    fig.set_size_inches(1, 1)\n    text = ax.text(0.5, 0.5, 'Very long text exceeding boundaries',\n                   clip_on=True)\n\n    text2 = ax.text(0.5, 0.5, 'Very long text exceeding boundaries',\n                    clip_on=False)\n\n    fig.canvas.draw()\n    \n    # Assert that text1 is clipped and text2 is not\n    assert text.get_window_extent().width < 1 \n    assert text2.get_window_extent().width > 1\n\n    ", "def test_text_bbox_clip():\n    fig, ax = plt.subplots()\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n\n    text = ax.text(0.5, 0.5, 'Foo', bbox_inches='tight', clip_on=True)\n    fig.canvas.draw()\n    bbox = text.get_window_extent()\n    assert 0 <= bbox.xmin < bbox.xmax <= 1\n    assert 0 <= bbox.ymin < bbox.ymax <= 1\n\n    text = ax.text(0.5, 0.5, 'Foo', bbox_inches='tight', clip_on=False)\n    fig.canvas.draw()\n    bbox = text.get_window_extent()\n    assert 0 <= bbox.xmin < bbox.xmax\n    assert 0 <= bbox.ymin < bbox.ymax\n\n\n\n"], "sample_647": ["        def test_example():\n            pass", "def test_warn_explicit_for() -> None:\n    @pytest.mark.parametrize(\n        \"message, expected_message\",\n        [\n            (\n                pytest.PytestWarning(\"hello\"),\n                \"hello\",\n            ),\n            (\n                pytest.PytestWarning(\"hello {arg}\"),\n                \"hello {arg}\",\n            ),\n        ],\n    )\n            pass  \n\n        with pytest.warns(message.__class__, match=expected_message) as warnings:\n            warn_explicit_for(test_function, message)\n        assert len(warnings) == 1\n\n\n\n", "compilation error", "def test_warn_explicit_for() -> None:\n    @attr.s(auto_attribs=True)\n    class MyTestFunction:\n            self.message = message\n\n        warnings.warn(PytestWarning(\"Warning from my_function\"))\n\n    my_function = MyTestFunction(message=\"Test Message\")\n    warn_explicit_for(my_function.my_function, PytestWarning(\"Warning from warn_explicit_for\"))\n    warn_explicit_for(\n        my_function.my_function, PytestWarning(\"test_warn_explicit_for warning\")\n    )\n\n    assert my_function.message == \"Test Message\"\n", "    def test_warn_explicit_for(pytester: Pytester) -> None:\n            pass\n\n        with pytest.warns(PytestWarning) as warnings:\n            warn_explicit_for(my_function, PytestWarning(\"test warning\"))\n\n        assert len(warnings) == 1\n        warning = warnings[0]\n        assert warning.message == \"test warning\"\n        assert warning.filename == pytester.getpyfile().strpath\n        assert warning.lineno == my_function.__code__.co_firstlineno\n"], "sample_594": ["    def test_summarize_attr(self):\n        attrs = {\"attr_1\": 1, \"attr_2\": \"hello world\", \"attr_3\": np.array([1, 2, 3]), \"attr_4\": None}\n        short = formatting.summarize_attr(\"attr_1\", 1)\n        long = formatting.summarize_attr(\"attr_2\", \"hello world\")\n        array = formatting.summarize_attr(\"attr_3\", np.array([1, 2, 3]))\n        none = formatting.summarize_attr(\"attr_4\", None)\n        assert short == \"    attr_1: 1\"\n        assert len(long) <= 80\n        assert long.endswith(\"...\")\n        assert array == \"    attr_3: [1 2 3]\"\n        assert none == \"    attr_4: None\"\n\n\n\n", "    def test_summarize_attr(self):\n        cases = [\n            (\"key\", None, \"key: None\"),\n            (\"key\", True, \"key: True\"),\n            (\"key\", 1, \"key: 1\"),\n            (\"key\", \"value\", \"key: value\"),\n            (\"key\", 1.5, \"key: 1.5\"),\n            (\"key\", \" \", \"key: \"),\n            (\"key\", [\"a\", \"b\", \"c\"], \"key: ['a', 'b', 'c']\"),\n        ]\n        for key, value, expected in cases:\n            actual = formatting.summarize_attr(key, value)\n            assert actual == expected\n\n\n\n", "    def test_unindexed_dims_repr(self):\n        ds = xr.Dataset(\n            data_vars={\"var\": ((\"x\", \"y\"), np.arange(12).reshape(3, 4))},\n            coords={\"x\": [1, 2, 3], \"y\": [4, 5, 6, 7]},\n        )\n        ds = ds.unindexed(\"y\")\n        actual = formatting.unindexed_dims_repr(ds)\n        expected = \"Unindexed dimensions:\\n    y\"\n        assert actual == expected\n\n\n        ds = xr.Dataset(\n            data_vars={\"var\": ((\"x\", \"y\"), np.arange(12).reshape(3, 4))},\n            coords={\"x\": [1, 2, 3], \"y\": [4, 5, 6, 7]},\n        )\n        ds = ds.unindexed([\"y\", \"z\"])\n        actual = formatting.unindexed_dims_repr(ds)\n        expected = \"Unindexed dimensions:\\n    y\\n    z\"\n        assert actual == expected\n", "    def test_summarize_attr_empty(self):\n        assert formatting.summarize_attr(\"key\", \"\") == \"    key: \"\n\n\n\n", "    def test_short_data_repr_array_like(self):\n        class ArrayLike:\n                return ArrayLike(np.array(func(self)))\n\n                return 10\n\n                return i\n\n        array_like = ArrayLike(np.arange(10))\n        actual = formatting.short_data_repr(array_like)\n        expected = \"array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\"\n        assert actual == expected\n"], "sample_1026": ["def test_issue_16777():\n    x = symbols('x')\n    assert lambdify(x, sin(x)**2 + cos(x)**2)(1) == 1\n\n\n", "def test_lambdify_with_Piecewise():\n    f = Piecewise((x**2, x > 0), (0, True))\n    g = lambdify(x, f)\n    assert g(2) == 4\n    assert g(-1) == 0\n", "def test_issue_16474():\n    x = Symbol('x')\n    f = lambdify(x, tan(x))\n    assert abs(f(0) - 0) < 1e-15\n    assert abs(f(pi/4) - 1) < 1e-15\n", "def test_lambdify_issue_17070():\n    # Issue #17070:  lambdify with lambdify within a function\n        return lambdify(y, x + y, 'numpy')(x + 1)\n    assert myfunc(2) == 3 \n", "def test_complex_functions():\n    if not numpy:\n        skip(\"numpy not installed.\")\n\n    f = lambdify(z, sin(z), modules=\"numpy\")\n    assert abs(f(1j) - numpy.sin(1j)).n() < 1e-15\n    f = lambdify(z, cos(z), modules=\"numpy\")\n    assert abs(f(1j) - numpy.cos(1j)).n() < 1e-15\n"], "sample_467": ["    def test_invalid_value_is_converted_to_empty_string(self):\n        widget = SelectDateWidget(years=(\"2010\",))\n        self.assertEqual(\n            widget.format_value(\"invalid_date\"), {\"day\": None, \"month\": None, \"year\": None}\n        )\n        self.assertEqual(widget.format_value(\"\"), {\"day\": None, \"month\": None, \"year\": None})\n", "    def test_empty_values_render(self):\n        widget = SelectDateWidget(years=(\"2014\",))\n        html = self.widget.render(\"mydate\", \"\")\n        self.assertInHTML('<option selected value=\"\">---</option>', html)\n        self.assertInHTML('<option value=\"\">---</option>', html)\n        \n        widget = SelectDateWidget(years=(\"2014\",), empty_label=\"empty_year\")\n        html = self.widget.render(\"mydate\", \"\")\n        self.assertInHTML('<option selected value=\"\">empty_year</option>', html)\n", "compilation error", "    def test_invalid_year_format(self):\n        widget = SelectDateWidget(years=(\"2010\",))\n\n        with self.assertRaisesRegex(ValueError, \"Invalid year format\"):\n            widget.value_from_datadict({\"field_year\": \"abc\"}, {}, \"field\")\n\n        with self.assertRaisesRegex(ValueError, \"Invalid year format\"):\n            widget.value_from_datadict({\"field_year\": \"2010A\"}, {}, \"field\")\n", "    def test_sanitize_strftime_format(self):\n        widget = SelectDateWidget()\n        self.assertEqual(widget.sanitize_strftime_format(\"%Y%m%d\"), \"%Y-%m-%d\")\n        self.assertEqual(widget.sanitize_strftime_format(\"%d-%m-%Y\"), \"%d-%m-%Y\")\n        self.assertEqual(widget.sanitize_strftime_format(\"%Y/%m/%d\"), \"%Y/%m/%d\")\n        self.assertEqual(widget.sanitize_strftime_format(\"%Y.%m.%d\"), \"%Y.%m.%d\")\n"], "sample_146": ["    def test_consistent_language_settings(self):\n        with self.subTest('consistent'), self.settings(LANGUAGE_CODE='en'):\n            self.assertEqual(check_language_settings_consistent(None), [])\n", "    def test_consistent_language_settings(self):\n        with self.subTest('en'):\n            with self.settings(LANGUAGE_CODE='en'):\n                self.assertEqual(\n                    check_language_settings_consistent(None), []\n                )\n\n        with self.subTest('fr'):\n            with self.settings(LANGUAGE_CODE='fr'):\n                self.assertEqual(\n                    check_language_settings_consistent(None), []\n                )\n\n", "    def test_consistent_language_settings(self):\n        with self.subTest('en'), self.settings(LANGUAGE_CODE='en'):\n            self.assertEqual(check_language_settings_consistent(None), [])\n", "    def test_consistent_language_settings(self):\n        msg = (\n            'You have provided a value for the LANGUAGE_CODE setting that is '\n            'not in the LANGUAGES setting.'\n        )\n        with self.subTest('en'):\n            self.assertEqual(check_language_settings_consistent(None), [])\n", "    def test_consistent_language_settings(self):\n        for tag in ['en', 'fr']:\n            with self.subTest(tag), self.settings(LANGUAGE_CODE=tag):\n                self.assertEqual(check_language_settings_consistent(None), [])\n"], "sample_1201": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_299": ["    def test_relative_path(self):\n        with self.settings(CACHES={\n            'default': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': 'cache',\n            },\n        }):\n            self.assertEqual(check_file_based_cache_is_absolute(None), [\n                Warning(\n                    f\"Your 'default' cache LOCATION path is relative. Use an \"\n                    f\"absolute path instead.\", id='caches.W003',\n                ),\n            ])\n", "    def test_relative_path(self):\n        with self.settings(CACHES={\n            'default': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': 'cache',\n            },\n        }):\n            self.assertEqual(check_file_based_cache_is_absolute(None), [\n                Warning(\n                    f\"Your 'default' cache LOCATION path is relative. Use an \"\n                    f\"absolute path instead.\",\n                    id='caches.W003',\n                ),\n            ])\n", "    def test_relative_path(self):\n        with self.settings(CACHES={\n            'default': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': 'cache',\n            },\n        }):\n            self.assertEqual(\n                check_file_based_cache_is_absolute(None),\n                [Warning(\n                    f\"Your 'default' cache LOCATION path is relative. Use an \"\n                    f\"absolute path instead.\",\n                    id='caches.W003',\n                )],\n            )\n", "    def test_relative_path(self):\n        with self.settings(CACHES={\n            'default': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': 'cache',\n            },\n        }):\n            self.assertEqual(check_file_based_cache_is_absolute(None), [\n                Warning(\n                    f\"Your 'default' cache LOCATION path is relative. Use an \"\n                    f\"absolute path instead.\",\n                    id='caches.W003',\n                ),\n            ])\n", "    def test_relative_path(self):\n        with self.settings(CACHES={\n            'default': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': 'cache',\n            },\n        }):\n            self.assertEqual(check_file_based_cache_is_absolute(None), [\n                Warning(\n                    f\"Your 'default' cache LOCATION path is relative. Use an \"\n                    f\"absolute path instead.\",\n                    id='caches.W003',\n                ),\n            ])\n"], "sample_1080": ["compilation error", "compilation error", "        def _eval_refine(self, assumptions):\n            return False", "        def __init__(self, base, exponent):\n            super(MyPow, self).__init__(base, exponent)\n", "compilation error"], "sample_252": ["    def test_key_transform_with_complex_values(self):\n        obj = NullableJSONModel.objects.create(value={'foo': {'bar': {'baz': 'qux'}}})\n        self.assertEqual(NullableJSONModel.objects.filter(value__foo__bar__baz='qux').get(), obj)\n\n\n", "    def test_key_transform_with_isnull(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__baz__has_key='a', value__baz__isnull=False\n            ),\n            [self.objs[7]],\n        )\n", "    def test_key_transform_ordering(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__d__0__isnull=False).order_by(\n                KeyTransform('d', 'value')\n            ).values('value__d__0'),\n            [{'value__d__0': 'e'}, {'value__d__0': {'f': 'g'}}],\n        )\n\n\n", "    def test_key_transform_with_subquery(self):\n        subquery_value = KeyTransform('baz', KeyTransform('a', 'value'))\n        self.assertIs(\n            NullableJSONModel.objects.filter(\n                id__in=NullableJSONModel.objects.filter(value__baz__contains=subquery_value)\n            ).exists(),\n            True,\n        )\n", "    def test_key_transform_with_nested_lookups(self):\n        tests = (\n            ('value__baz__f__contains', 'g'),\n            ('value__baz__0__gt', 1),\n        )\n        for lookup, value in tests:\n            with self.subTest(lookup=lookup):\n                self.assertIs(NullableJSONModel.objects.filter(\n                    **{lookup: KeyTransform('baz', KeyTransform('1', 'value'))},\n                ).exists(), True)\n"], "sample_983": ["compilation error", "compilation error", "compilation error", "compilation error", "def test_sparse_svd():\n    A = SparseMatrix(((2, 1, 0), (1, 2, 0), (0, 0, 1)))\n    U, S, V = A.svd()\n    assert U.T * A * V == S * V\n    assert U.T * U == SparseMatrix(eye(3))\n    assert V.T * V == SparseMatrix(eye(3))\n\n\n"], "sample_931": ["def test_pyfunction_signature_with_varargs(app):\n    text = (\".. py:function:: func\\n\"\n            \"   :varargs: args\\n\"\n            \"   :keywords: kwargs\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"func\"],\n                                                    desc_parameterlist,\n                                                    [desc_parameter, desc_sig_name, \"args\"],\n                                                    [desc_parameter, desc_sig_name, \"kwargs\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n", "def test_pyfunction_signature_with_varargs(app):\n    text = (\n        \".. py:function:: func\\n\"\n        \"   :param name: The name\\n\"\n        \"   :param args: More arguments...\\n\"\n        \"  \"\n    )\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"func\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"None\"])],\n                                  desc_content)]))\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist,\n                 [desc_parameter, [desc_sig_name, \"name\"],\n                [desc_sig_punctuation, \": \"],\n                [desc_sig_name, \"The name\"]],\n                 [desc_parameter, [desc_sig_name, \"args\"],\n                [desc_sig_punctuation, \": \"],\n                [desc_sig_name, \"More arguments...\"]]\n                ])\n\n\n\n", "def test_module_index_no_explicit_module_name(app):\n    text = \".. py:function:: hello\\n\"\n    restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    assert index.generate() == (\n        [('h', [IndexEntry('hello', 0, 'index', 'function-hello', '', '', '')])],\n        True\n    ) \n", "def test_module_index_duplicate_entries(app):\n    text = (\".. py:module:: docutils\\n\"\n            \".. py:module:: docutils\\n\"\n            \".. py:module:: sphinx\\n\")\n    restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    assert index.generate() == (\n        [('d', [IndexEntry('docutils', 0, 'index', 'module-docutils', '', '', '')]),  \n         ('s', [IndexEntry('sphinx', 0, 'index', 'module-sphinx', '', '', '')])],\n        False\n    )\n", "def test_module_index_with_parameters(app):\n    text = (\".. py:module:: docutils\\n\"\n            \".. py:module:: sphinx\\n\"\n            \".. py:module:: sphinx:class:: Builder\\n\"\n            \".. py:attribute:: Sphinx:config:: BUILD_DIRS\\n\")\n    restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    assert index.generate() == (\n        [('d', [IndexEntry('docutils', 0, 'index', 'module-docutils', '', '', '')]),\n         ('s', [IndexEntry('sphinx', 0, 'index', 'module-sphinx', '', '', ''),\n                IndexEntry('sphinx.Builder', 2, 'index', 'module-sphinx.Builder', '', '', '')]),  # NOQA\n         ('S', [IndexEntry('Sphinx', 0, 'index', 'module-Sphinx', '', '', '')])],  # NOQA\n        False\n    )\n\n"], "sample_719": ["def test_tfidf_vectorizer_ngram_range():\n    data = [\"This is a test sentence\", \"This sentence is another test\"]\n    vect = TfidfVectorizer(ngram_range=(1, 2))\n    X = vect.fit_transform(data)\n    assert_array_equal(\n        X.toarray(),\n        [[0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0],\n         [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]]\n    )\n", "def test_stop_words_removal_from_custom_stop_words():\n    custom_stop_words = ['hello', 'world']\n    vect = CountVectorizer(stop_words=custom_stop_words)\n    X = vect.fit_transform(JUNK_FOOD_DOCS)\n\n    assert_true(len(vect.get_feature_names()) < len(set(\n        word for doc in JUNK_FOOD_DOCS for word in doc.split())))\n    for word in custom_stop_words:\n        assert_equal(\n            np.sum(X[:, vect.vocabulary_[word]]), 0)\n\n\n\n", "compilation error", "    def test_vectorizer_with_stop_words_none():\n        # Test that stop_words='' is equivalent to None\n        for vec in (CountVectorizer, TfidfVectorizer):\n            vec1 = vec()\n            vec2 = vec(stop_words='')\n            assert_equal(vec1.stop_words_, None)\n            assert_equal(vec2.stop_words_, None)\n\n\n\n            \n", "    def test_tfidfvectorizer_with_custom_stopwords():\n        custom_stopwords = ['the', 'a', 'an']\n        vect = TfidfVectorizer(stop_words=custom_stopwords)\n        X = vect.fit_transform(JUNK_FOOD_DOCS)\n        assert_true(all(word not in vect.vocabulary_ for word in custom_stopwords))\n        assert_not_equal(X.shape[1], len(vect.vocabulary_))\n"], "sample_790": ["def test_kernel_pca_sparse_inverse_transform():\n    rng = np.random.RandomState(0)\n    X_fit = sp.csr_matrix(rng.random_sample((5, 4)))\n    X_pred = sp.csr_matrix(rng.random_sample((2, 4)))\n\n    kpca = KernelPCA(4, kernel='rbf', fit_inverse_transform=True)\n    X_fit_transformed = kpca.fit_transform(X_fit)\n    X_pred_transformed = kpca.transform(X_pred)\n    X_pred_inverse = kpca.inverse_transform(X_pred_transformed)\n    assert_allclose(X_pred_inverse.toarray(),\n                     X_pred.toarray())\n", "def test_kernel_pca_with_zero_eigenvalues():\n    rng = np.random.RandomState(0)\n    X = rng.rand(10, 10)\n    X[:, 0] = 0  # Ensures some eigenvalues will be zero\n    kpca = KernelPCA(n_components=5, remove_zero_eig=True)\n    Xt = kpca.fit_transform(X)\n    assert_equal(Xt.shape, (10, 5))\n", "def test_kernel_pca_fit_inverse_transform():\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    X_pred = rng.random_sample((2, 4))\n\n    kpca = KernelPCA(kernel=\"rbf\", n_components=2, fit_inverse_transform=True)\n    X_transformed = kpca.fit_transform(X)\n    X_pred_transformed = kpca.transform(X_pred)\n\n    X_pred_back = kpca.inverse_transform(X_pred_transformed)\n\n    assert_array_almost_equal(\n        X_pred_back, X_pred,\n        decimal=3,\n    )\n", "def test_kernel_pca_feature_scaling():\n    rng = np.random.RandomState(0)\n    X = rng.rand(100, 10)\n    X_scaled = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n\n    kpca1 = KernelPCA(n_components=3, kernel='rbf').fit(X)\n    kpca2 = KernelPCA(n_components=3, kernel='rbf').fit(X_scaled)\n\n    # Check that the transformed data have similar variance\n    assert_less(np.var(kpca1.transform(X)), np.var(kpca2.transform(X_scaled)))\n    assert_less(np.var(kpca1.transform(X_scaled)), np.var(kpca2.transform(X_scaled)))\n\n\n", "def test_kernel_pca_fit_inverse_transform_precomputed():\n    rng = np.random.RandomState(0)\n    X_fit = rng.random_sample((5, 4))\n    X_pred = rng.random_sample((2, 4))\n    kernel = rbf_kernel(X_fit, gamma=0.1)\n    kpca = KernelPCA(kernel='precomputed', n_components=2,\n                     fit_inverse_transform=True)\n\n    # Fit with a precomputed kernel\n    kpca.fit(kernel)\n\n    # Transform and Inverse Transform\n    X_transformed = kpca.transform(X_pred)\n    X_inverse_transformed = kpca.inverse_transform(X_transformed)\n    assert_array_almost_equal(X_inverse_transformed, X_pred)\n"], "sample_210": ["    def test_template_params_filtering_with_kwargs(self):\n        class ArtistView(TemplateView):\n            template_name = 'generic_views/about.html'\n\n                context = super().get_context_data(**kwargs)\n                artist = Artist.objects.get(name=artist_name)\n                context['artist'] = artist\n                return context\n\n        artist = Artist.objects.create(name='Rene Magritte')\n        response = ArtistView.as_view()(self.rf.get('/'), artist_name=artist.name, extra_info='Extra Info')\n        self.assertEqual(response.context_data['artist'], artist)\n        self.assertEqual(response.context_data['extra_info'], 'Extra Info')\n\n", "    def test_template_params_filtering_with_kwargs(self):\n        class ArtistView(TemplateView):\n            template_name = 'generic_views/about.html'\n\n                context = super().get_context_data(**kwargs)\n                artist = Artist.objects.get(name=artist_name)\n                context['artist'] = artist\n                context['extra_info'] = kwargs['extra_info']\n                return context\n\n        artist = Artist.objects.create(name='Rene Magritte')\n        response = ArtistView.as_view()(self.rf.get('/'), artist_name=artist.name, extra_info='extra detail')\n        self.assertEqual(response.context_data['artist'], artist)\n        self.assertEqual(response.context_data['extra_info'], 'extra detail')\n", "    def test_template_params_rendering(self):\n        class ArtistView(TemplateView):\n            template_name = 'generic_views/about.html'\n\n                context = super().get_context_data(**kwargs)\n                artist = Artist.objects.get(name=artist_name)\n                return {**context, 'artist': artist}\n\n        artist = Artist.objects.create(name='Rene Magritte', bio='Surrealist painter')\n        response = ArtistView.as_view()(self.rf.get('/'), artist_name=artist.name)\n        self.assertContains(response, 'Rene Magritte')\n        self.assertContains(response, 'Surrealist painter')\n", "    def test_redirect_with_kwargs_in_url(self):\n        view = RedirectView.as_view(url='/new-profile/{username}/', permanent=True)\n        response = view(self.rf.get('/redirect/john-doe/'))\n        self.assertEqual(response.status_code, 301)\n        self.assertEqual(response.url, '/new-profile/john-doe/')\n", "    def test_template_params_with_kwargs(self):\n        \"\"\"A template view can receive arbitrary kwargs and pass them to the context.\"\"\"\n        response = self.client.get('/template/kwargs/john/doe/')\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.context['first_name'], 'john')\n        self.assertEqual(response.context['last_name'], 'doe')\n\n        response = self.client.get('/template/kwargs/jane/smith/')\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.context['first_name'], 'jane')\n        self.assertEqual(response.context['last_name'], 'smith')\n\n        response = self.client.get('/template/kwargs/missing/')\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.context['first_name'], None)\n        self.assertEqual(response.context['last_name'], None)\n\n"], "sample_549": ["compilation error", "def test_get_cycle_label():\n    assert cbook._get_cycle_label(0) == '0'\n    assert cbook._get_cycle_label(1) == '1'\n    assert cbook._get_cycle_label(1023) == '1023'\n    assert cbook._get_cycle_label(1024) == '1'\n    assert cbook._get_cycle_label(-1) == '-0'\n    assert cbook._get_cycle_label(-1024) == '-1'\n", "def test_safe_first_finite_with_nan():\n    datetime_lst = [date.today() + timedelta(days=i) for i in range(10)]\n    datetime_lst[0] = np.nan\n    actual = cbook._safe_first_finite(datetime_lst)\n    assert actual is not None and actual == datetime_lst[1]\n", "def test_safe_first_element_with_inf():\n    datetime_lst = [date.today() + timedelta(days=i) for i in range(10)]\n    datetime_lst[0] = np.inf\n    actual = cbook._safe_first_finite(datetime_lst)\n    assert actual is not None and actual == datetime_lst[1]\n", "compilation error"], "sample_1139": ["def test_issue_17858_continued():\n    assert 0 in Range(0, oo)\n    assert 0 not in Range(-oo, 0)\n    assert oo in Range(-oo, oo, 1)\n    assert -oo in Range(-oo, oo, 1)\n    assert -oo not in Range(0, oo)\n\n\n", "def test_issue_17858_continued():\n    assert 1 not in Range(-oo, 1, 2)\n    assert -1 in Range(-oo, 1, -2)\n    assert oo not in Range(-oo, oo, 2)\n    assert -oo not in Range(-oo, oo, 2)\n", "def test_issue_12776():\n    assert ImageSet(Lambda(x, x**2), S.RiemannSphere) == S.RiemannSphere\n    assert ImageSet(Lambda(x, x**2), S.Complexes) == \\\n        S.Complexes\n", "def test_issue_18291():\n    assert ImageSet(Lambda(x, x + 1), S.integers).intersection(S.integers) == \\\n        ImageSet(Lambda(x, x + 1), S.integers)\n", "def test_issue_18247():\n    a = Symbol('a')\n    b = Symbol('b')\n    assert ImageSet(Lambda(n, n*a + b), S.Integers).is_subset(S.Reals)\n    assert ImageSet(Lambda(n, n*a + b), S.Naturals).is_subset(S.Reals)\n"], "sample_547": ["def test_offsetbox_zorder():\n    fig, ax = plt.subplots()\n\n    da1 = DrawingArea(10, 10)\n    da1.add_artist(mpatches.Rectangle((0, 0), 10, 10, linewidth=0,\n                                      edgecolor='red', facecolor='none'))\n    ab1 = AnnotationBbox(da1, (0.5, 0.5),  zorder=1)\n\n    da2 = DrawingArea(10, 10)\n    da2.add_artist(mpatches.Rectangle((0, 0), 10, 10, linewidth=0,\n                                      edgecolor='blue', facecolor='none'))\n    ab2 = AnnotationBbox(da2, (0.5, 0.5), zorder=2)\n\n    ax.add_artist(ab1)\n    ax.add_artist(ab2)\n\n    fig.canvas.draw()\n", "def test_packer_child_alignment(direction, packing_type):\n    fig = plt.figure(dpi=72)\n    renderer = fig.canvas.get_renderer()\n\n    if direction == 'horizontal':\n        x1, y1 = 10, 30\n        x2, y2 = 20, 60\n        r1 = DrawingArea(x1, y1)\n        r2 = DrawingArea(x2, y2)\n\n        if packing_type == 'HPacker':\n            packer = HPacker(children=[r1, r2], align='center')\n        else:\n            packer = VPacker(children=[r1, r2], align='center')\n        \n        packer.draw(renderer)\n        bbox = packer.get_bbox(renderer)\n        px, py = packer.get_offset(bbox, renderer)\n\n        assert_allclose(bbox.bounds, (0, 0, x1 + x2, max(y1, y2)))\n        assert_allclose([child.get_offset() for child in packer.get_children()],\n                        [(px + x1/2, py + y2/2), (px + x2/2, py + y2/2)])\n\n    elif direction == 'vertical':\n        x1, y1 = 10, 30\n        x2, y2 = 20, 60\n        r1 = DrawingArea(x1, y1)\n        r2 = DrawingArea(x2, y2)\n\n        if packing_type == 'HPacker':\n            packer = HPacker(children=[r1, r2], align='center')\n        else:\n            packer = VPacker(children=[r1, r2], align='center')\n\n        packer.draw(renderer)\n        bbox = packer.get_bbox(renderer)\n        px, py = packer.get_offset(bbox, renderer)\n\n        assert_allclose(bbox.bounds, (0, -max(y1, y2), max(x1, x2),", "def test_text_align(align):\n    fig, ax = plt.subplots()\n    text = AnchoredText(\"Test\", loc=\"center left\", pad=0.1, prop={'ha': align})\n    ax.add_artist(text)\n\n    # Check that the text is aligned as expected\n    bbox = text.get_tightbbox(fig.canvas.get_renderer())\n    if align == 'center':\n        assert bbox.centerx == 0.5\n    elif align == 'left':\n        assert bbox.left == 0\n    elif align == 'right':\n        assert bbox.right == 1\n\n\n\n", "def test_padding_box(pad, width):\n    fig, ax = plt.subplots()\n\n    da = DrawingArea(width[0], width[1], 0, 0, clip=True)\n    pb = PaddedBox(da, pad=pad)\n    ax.add_artist(pb)\n    \n    bbox = pb.get_bbox(ax.figure.canvas.get_renderer())\n    assert bbox.width == width[0] + pad[0] * 2  \n    assert bbox.height == width[1] + pad[1] * 2 \n", "def test_offsetbox_padding():\n    fig, ax = plt.subplots()\n    text = AnchoredText('Test text', loc='upper left')\n    box = OffsetBox(text,  bbox=dict(\n        pad=10,\n        boxstyle='round',\n        facecolor='lightblue',\n    ))\n    ax.add_artist(box)\n\n\n"], "sample_394": ["    def test_url_language_code_preserved(self):\n        superuser = User.objects.create_user(\n            username=\"staff\",\n            password=\"secret\",\n            email=\"staff@example.com\",\n            is_staff=True,\n        )\n        self.client.force_login(superuser)\n        known_url = reverse(\"admin:admin_views_article_changelist\")\n        response = self.client.get(known_url + \"?lang=es\")\n        self.assertContains(response, \"lang=es\", count=1)\n", "    def test_final_catch_all_view_disabled_unknown_url(self):\n        admin_site = AdminSite(name='admin10')\n        admin_site.final_catch_all_view = None\n        request = HttpRequest()\n        response = admin_site._handle_request(request)\n        self.assertEqual(response.status_code, 404)\n\n", "    def test_disabled_final_catch_all_view(self):\n        # Override the final_catch_all_view to None\n        admin = AdminSite(name=\"test_admin10\")\n        admin.final_catch_all_view = None\n        from django.contrib import admin\n        admin.site = admin\n        unknown_url = \"/test_admin10/unknown/\"\n        response = self.client.get(unknown_url)\n        self.assertEqual(response.status_code, 404)\n\n", "    def test_missing_slash_append_slash_true_final_catch_all_view_disabled(self):\n        superuser = User.objects.create_user(\n            username=\"staff\",\n            password=\"secret\",\n            email=\"staff@example.com\",\n            is_staff=True,\n        )\n        self.client.force_login(superuser)\n        known_url = reverse(\"admin:admin_views_article_changelist\")\n        response = self.client.get(known_url[:-1])\n        self.assertEqual(response.status_code, 404)\n", "    def test_script_name_with_final_catch_all_view_disabled(self):\n        superuser = User.objects.create_user(\n            username=\"staff\",\n            password=\"secret\",\n            email=\"staff@example.com\",\n            is_staff=True,\n        )\n        self.client.force_login(superuser)\n        known_url = reverse(\"admin:admin_views_article_changelist\")\n        response = self.client.get(known_url[:-1], SCRIPT_NAME=\"/prefix/\")\n        self.assertEqual(response.status_code, 404)\n"], "sample_1132": ["compilation error", "compilation error", "def test_multiset_combinations_without_replacement():\n    assert list(multiset_combinations_without_replacement('mississippi', 3)) == [\n        ['m', 'i', 's'], ['m', 'i', 'p'], ['m', 's', 'i'], ['m', 's', 'p'],\n        ['m', 'p', 'i'], ['m', 'p', 's'], ['i', 'm', 's'], ['i', 'm', 'p'],\n        ['i', 's', 'm'], ['i', 's', 'p'], ['i', 'p', 'm'], ['i', 'p', 's'],\n        ['s', 'm', 'i'], ['s', 'm', 'p'], ['s', 'i', 'm'], ['s', 'i', 'p'],\n        ['s', 'p', 'm'], ['s', 'p', 'i'], ['p', 'm', 'i'], ['p', 'm', 's'],\n        ['p', 'i', 'm'], ['p', 'i', 's'], ['p', 's', 'm'], ['p', 's', 'i']]\n    assert len(list(multiset_combinations_without_replacement('mississippi', 3))) == 100\n    assert list(multiset_combinations_without_replacement('aaabbb', 3)) == \\\n        [('a', 'a', 'b'), ('a', 'a', 'b'), ('a', 'a', 'b'), ('a', 'a', 'b'),\n         ('a', 'b', 'a'), ('a', 'b', 'a'), ('a', 'b', 'a'), ('a', 'b', 'a'),\n         ('a', 'b', 'a'), ('a', 'b', 'a'), ('a', 'b', 'b'), ('a', 'b', 'b'),\n         ('a', 'b', 'b'), ('b', 'a', 'a'), ('b', 'a', 'a'), ('b', 'a', 'a'),\n         ('b', 'a', 'a'), ('b', 'a', 'a'), ('b', 'a', 'a'), ('b', 'a', 'a'),\n         ('b', '", "compilation error", "compilation error"], "sample_1000": ["def test_matrix_element_access():\n    A = Matrix([[1, 2, 3], [4, 5, 6]])\n    assert mcode(A[0, 1]) == \"A(1, 2)\"\n    assert mcode(A[1, 0] * A[0, 2]) == \"A(2, 1) * A(1, 3)\"\n    assert mcode(A[:, 1]) == \"A(:, 2)\"\n    assert mcode(A[0, :]) == \"A(1, :)\"\n", "def test_custom_functions():\n    custom_functions = {\n        \"f\": \"existing_octave_fcn\",\n        \"g\": [(lambda x: x.is_Matrix, \"my_mat_fcn\"),\n              (lambda x: not x.is_Matrix, \"my_fcn\")]\n    }\n    assert mcode(f(x) + g(x) + g(mat)) == \\\n        \"existing_octave_fcn(x) + my_fcn(x) + my_mat_fcn([1])\"\n", "def test_symbols_in_matrices():\n    x = Symbol('x')\n    y = Symbol('y')\n    A = Matrix([[x, y], [x**2, y**2]])\n    assert mcode(A) == \"[x y; x.^2 y.^2]\"\n", "def test_matrix_powers():\n    A = MatrixSymbol('A', 2, 2)\n    assert mcode(A**k) == \"A.^k\" for k in range(1, 5)\n\n\n\n", "def test_matrix_pow():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    assert mcode(A**3) == \"A^3\"\n    assert mcode(A**Rational(1, 2)) == \"A^(1/2)\"\n    assert mcode(A**(B + 2)) == \"A^(B + 2)\"\n    assert mcode((A*B)**2) == \"(A*B).^2\"\n    assert mcode(A**-1) == \"inv(A)\"\n    assert mcode(A**-2) == \"inv(A).^2\"\n\n"], "sample_451": ["    def test_replace_metacharacters(self):\n        pattern = r\"^(\\(\\?P<a>\\\\w+)\\)\"\n        result = replace_metacharacters(pattern)\n        expected = r\"^(\\?P<a>\\w+)\"\n        self.assertEqual(result, expected)\n\n", "    def test_replace_metacharacters(self):\n        pattern = r\"^(\\w+)(?P<a>[\\w\\.]+)(.*)\"\n        modified_pattern = replace_metacharacters(pattern)\n        self.assertEqual(modified_pattern, r\"^(\\w+)\\\\\\?P<a>[\\w\\\\.]+(.*)\")\n\n\n\n", "    def test_parse_rst_with_escaped_metacharacters(self):\n        source = r\"^\\(?P<a>(\\\\w+)/b/(?P<c>\\\\w+)$\"\n        rendered = (\n            \"<p>^(\\\\?P<a>(\\\\w+)/b/(?P<c>\\\\w+))$</p>\\n\"\n        )\n        self.assertHTMLEqual(parse_rst(source, \"view\"), rendered)\n", "    def test_parse_rst_with_metadata(self):\n        docstring = \"\"\"\n        Display an individual :model:`myapp.MyModel`.\n\n        **Context**\n\n        ``RequestContext``\n\n        ``mymodel``\n            An instance of :model:`myapp.MyModel`.\n\n        **Template:**\n\n        :template:`myapp/my_template.html` (DESCRIPTION)\n\n        some_metadata: some data\n        author: John Doe\n        \"\"\"\n        title, description, metadata = parse_docstring(docstring)\n        parsed_description = parse_rst(description, \"model\", \"model:admindocs\")\n        self.assertIn('<strong>Author:</strong> John Doe', parsed_description)\n\n\n\n", "    def test_replace_metacharacters(self):\n        pattern = r\"^(?P<a>\\w+)/b/([\\w*+?]+)$\"\n        expected = \"^<a>/b/<var>$\"\n        self.assertEqual(replace_metacharacters(pattern), expected)\n\n\n"], "sample_496": ["    def test_suggestions_with_unknown_subcommand(self):\n        args = ['nonexistentcommand', '--settings=test_project.settings']\n        out, err = self.run_django_admin(args)\n        self.assertNoOutput(out)\n        self.assertOutput(err, \"Unknown command: 'nonexistentcommand'.\")\n\n\n", "    def test_suggestions_with_spaces(self):\n        args = ['run  server', '--settings=test_project.settings']\n        out, err = self.run_django_admin(args)\n        self.assertNoOutput(out)\n        self.assertOutput(err, \"Unknown command: 'run  server'. Did you mean runserver?\")\n", "    def test_suggestions_with_known_command(self):\n        args = ['runserver', '--settings=test_project.settings']\n        out, err = self.run_django_admin(args)\n        self.assertNoOutput(err)\n        self.assertOutput(out, \"Starting development server at http://127.0.0.1:8000/\")\n\n", "    def test_suggestions_with_extra_args(self):\n        args = ['rnserver', '--settings=test_project.settings', '--help']\n        out, err = self.run_django_admin(args)\n        self.assertNoOutput(out)\n        self.assertOutput(err, \"Unknown command: 'rnserver'. Did you mean runserver?\")\n        self.assertIn(\"runserver\", err)\n\n\n\n", "    def test_suggestions_with_args(self):\n        args = ['rnserver', '--settings=test_project.settings', '--port=8001']\n        out, err = self.run_django_admin(args)\n        self.assertNoOutput(out)\n        self.assertOutput(err, \"Unknown command: 'rnserver'. Did you mean runserver?\")\n"], "sample_188": ["    def test_transform_with_complex_expression(self):\n        expr = ExpressionWrapper(\n            F('field1') + F('field2') * Value(2),\n            output_field=IntegerField(),\n        )\n        self.assertEqual(expr.get_group_by_cols(alias=None), [])\n\n", "    def test_expression_wrapper_repr(self):\n        self.assertEqual(\n            repr(ExpressionWrapper(F('name'), output_field=CharField(max_length=100))),\n            \"ExpressionWrapper(F(name))\"\n        )\n", "    def test_datetime_output_field(self):\n        from django.db.models import DateTimeField\n        expr = ExpressionWrapper(F('start') + datetime.timedelta(days=1), output_field=DateTimeField())\n        self.assertEqual(expr.get_group_by_cols(alias=None), [])\n        self.assertEqual(expr.get_update_columns(), [\"start\"])", "    def test_output_field_with_deconstruct(self):\n        expr = ExpressionWrapper(F('field'), output_field=CharField())\n        path, args, kwargs = expr.deconstruct()\n        self.assertEqual(path, 'django.db.models.expressions.ExpressionWrapper')\n        self.assertEqual(args, (F('field'),))\n        self.assertEqual(kwargs['output_field'].deconstruct(), CharField().deconstruct())\n\n\n\n", "    def test_output_field_with_different_type(self):\n        with self.assertRaises(TypeError):\n            ExpressionWrapper(F('id'), output_field=CharField())\n"], "sample_1157": ["def test_issue_13247():\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n    assert parse_expr('x + y + z') == x + y + z\n    assert parse_expr('x + y + \\\n        (z)') == x + y + z\n    assert parse_expr('x + y + \\\n        (z + x)') == x + y + z + x\n\n\n", "compilation error", "def test_issue_15174():\n    from sympy import symbols\n    x, y = symbols('x y')\n    assert parse_expr('x**y + y**x', evaluate=False) == x**y + y**x\n", "def test_exponential_function():\n    t = standard_transformations + (function_exponentiation,)\n    x = Symbol('x')\n    y = Symbol('y')\n    f = Function('f')\n    assert parse_expr('exp(x)**y', transformations=t) == (exp(x))**y\n    assert parse_expr('E(x)**y', transformations=t) == exp(x)**y\n    assert parse_expr('f(x)**y', transformations=t) == (f(x))**y\n", "compilation error"], "sample_145": ["    def test_actions_callable(self):\n        action = 'not callable'\n\n        class BandAdmin(ModelAdmin):\n            actions = (action,)\n\n        self.assertIsInvalid(\n            BandAdmin, Band,\n            \"Action names in 'actions' must be callable objects.\",\n            id='admin.E131',\n        )\n", "    def test_actions_invalid_type(self):\n        class BandAdmin(ModelAdmin):\n            actions = 1\n\n        self.assertIsInvalid(\n            BandAdmin, Band,\n            'The value of \"actions\" must be a list or tuple.',\n            id='admin.E128',\n        )\n\n", "    def test_actions_have_correct_signature(self):\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (action,)\n\n                return True\n\n        self.assertIsInvalid(\n            BandAdmin, Band,\n            \"The action 'action' must accept three arguments: \"\n            \"'modeladmin', 'request', and 'queryset'.\",\n            id='admin.E131',\n        )\n\n", "    def test_actions_can_not_be_a_class_without_call(self):\n        class DummyAction:\n                pass\n\n        class BandAdmin(ModelAdmin):\n            actions = [DummyAction()]\n\n        self.assertIsInvalid(\n            BandAdmin, Band,\n            \"'actions' must be callable.\",\n            id='admin.E131',\n        )\n\n", "    def test_actions_invalid_signature(self):\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (action,)\n\n        self.assertIsInvalid(\n            BandAdmin, Band,\n            'The value of \"actions[0]\" must be a callable with signature '\n            '(modeladmin, request, queryset).',\n            id='admin.E131',\n        )\n\n\n\n"], "sample_205": ["    def test_update_error_dict(self):\n        error_dict = {}\n        exception = ValidationError(error_dict)\n        self.assertEqual(error_dict, {})\n\n        exception.update_error_dict({'field1': ['E1', 'E2']})\n        self.assertEqual(error_dict, {'field1': ['E1', 'E2']})\n\n        exception.update_error_dict({'field2': ['E3', 'E4']})\n        self.assertEqual(error_dict, {'field1': ['E1', 'E2'], 'field2': ['E3', 'E4']})\n\n        exception = ValidationError({'field1': ['E1']})\n        error_dict = {}\n        exception.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['E1']})\n\n        exception = ValidationError({'field1': ['E1'], 'field2': ['E2']})\n        error_dict = {'field3': ['E3']}\n        exception.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['E1'], 'field2': ['E2'], 'field3': ['E3']})\n\n", "    def test_update_error_dict(self):\n        error_dict = {}\n        exception = ValidationError(error_dict)\n        self.assertEqual(error_dict, {})\n        error_dict = {'field1': 'E1'}\n        exception = ValidationError(error_dict)\n        self.assertEqual(error_dict, {'field1': 'E1'})\n        exception.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': 'E1'})\n        error_dict2 = {'field2': 'E2'}\n        exception = ValidationError(error_dict2)\n        self.assertEqual(error_dict2, {'field2': 'E2'})\n        exception.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': 'E1', 'field2': 'E2'})\n        error_dict3 = {'field1': ['E3']}\n        exception = ValidationError(error_dict3)\n        self.assertEqual(error_dict3, {'field1': ['E3']})\n        exception.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['E3', 'E1'], 'field2': 'E2'})\n", "    def test_update_error_dict(self):\n        error_dict = {}\n        exception = ValidationError(error_dict)\n        self.assertEqual(error_dict, {})\n\n        exception.update_error_dict({'field1': ['E1', 'E2']})\n        self.assertEqual(error_dict, {'field1': ['E1', 'E2']})\n\n        exception.update_error_dict({'field2': ['E3', 'E4']})\n        self.assertEqual(error_dict, {'field1': ['E1', 'E2'], 'field2': ['E3', 'E4']})\n\n        exception.update_error_dict({'field1': 'ERROR'})\n        self.assertEqual(error_dict, {'field1': ['E1', 'E2', 'ERROR'], 'field2': ['E3', 'E4']})\n\n        exception = ValidationError({'field1': ['E1', 'E2']})\n        error_dict = {}\n        exception.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['E1', 'E2']})\n\n\n", "    def test_update_error_dict(self):\n        error_dict = {}\n        exception = ValidationError(error_dict)\n        self.assertEqual(error_dict, {})\n        error_dict = exception.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {})\n\n        error_dict = {'field1': ['E1', 'E2']}\n        exception = ValidationError(error_dict)\n        self.assertEqual(exception.update_error_dict(error_dict), error_dict)\n\n        error_dict = {'field2': ['E3', 'E4']}\n        exception = ValidationError(error_dict)\n        self.assertEqual(exception.update_error_dict(error_dict), {'field1': ['E1', 'E2'], 'field2': ['E3', 'E4']})\n\n        error_dict = {}\n        exception = ValidationError({'field1': ['E1', 'E2']})\n        self.assertEqual(exception.update_error_dict(error_dict), {'field1': ['E1', 'E2']})\n\n        error_dict = {'field1': ['E1', 'E2'], 'field2': ['E3', 'E4']}\n        exception = ValidationError({'field3': ['E5', 'E6']})\n        self.assertEqual(exception.update_error_dict(error_dict), {'field1': ['E1', 'E2'], 'field2': ['E3', 'E4'], 'field3': ['E5', 'E6']})\n\n        error_dict = {'field1': ['E1', 'E2']}\n        exception = ValidationError({'field1': ['E3', 'E4']})\n        self.assertEqual(exception.update_error_dict(error_dict), {'field1': ['E1', 'E2', 'E3', 'E4']})\n", "    def test_update_error_dict(self):\n        error_dict = {}\n        error1 = ValidationError('E1')\n        error1.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'__all__': [error1]})\n\n        error_dict = {}\n        error2 = ValidationError({'field1': 'E1'})\n        error2.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': [error2]})\n\n        error_dict = {'field1': ['E1']}\n        error1 = ValidationError('E2')\n        error1.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['E1', error1]})\n\n        nested_error = ValidationError({'field1': 'E1', 'field2': 'E2'})\n        error_dict = {}\n        nested_error.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': [nested_error], 'field2': [nested_error]})\n"], "sample_494": ["    def test_serialize_custom_field(self):\n        class MyCustomField(models.Field):\n            description = \"My custom field\"\n\n                return \"VARCHAR(255)\"\n\n                return value\n\n                return \"CharField\"\n\n        class ModelWithCustomField(models.Model):\n            my_field = MyCustomField()\n\n        string, imports = MigrationWriter.serialize(ModelWithCustomField)\n        self.assertIn(\"MyCustomField\", string)\n        self.assertIn(\"MyCustomField\", imports)\n\n", "    def test_serialize_deconstructible_object(self):\n        class MyDeconstructible(deconstructible):\n                self.value = value\n\n                return (\"migrations.test_writer.MyDeconstructible\", (self.value,))\n        \n        string, imports = MigrationWriter.serialize(MyDeconstructible(42))\n        self.assertEqual(string, \"migrations.test_writer.MyDeconstructible(42)\")\n        self.assertEqual(imports, {\"from migrations.test_writer import MyDeconstructible\"})    \n", "    def test_serialize_deconstructible_class(self):\n        class DeconstructibleClass:\n                self.value = value\n\n                return (\n                    \"migrations.test_writer.DeconstructibleClass\",\n                    [(\"value\", self.value)],\n                )\n\n        string, imports = MigrationWriter.serialize(DeconstructibleClass(42))\n        self.assertEqual(string, \"migrations.test_writer.DeconstructibleClass(value=42)\")\n        self.assertEqual(imports, {\"from migrations.test_writer import DeconstructibleClass\"})\n\n", "    def test_serialize_custom_types(self):\n        class CustomType:\n                self.value = value\n\n                return str(self.value)\n\n                return f\"CustomType({self.value})\"\n\n                return repr(self), {}\n        string, imports = MigrationWriter.serialize(\n            CustomType(\"value\")\n        )\n        self.assertEqual(string, \"CustomType('value')\")\n        self.assertEqual(imports, {})\n\n", "    def test_serialize_custom_class_with_deconstruct(self):\n        class MyCustomClass(object):\n                self.value = value\n\n                return self.value == other.value\n\n                return (\n                    \"my_custom_class\",\n                    [],\n                    {\"value\": self.value},\n                    None,\n                )\n\n        string, imports = MigrationWriter.serialize(MyCustomClass(42))\n        self.assertEqual(string, \"my_custom_class(**{'value': 42})\")\n        self.assertEqual(imports, {\"from django.db.models import models\"})"], "sample_509": ["def test_date_ticker_factory_with_tz():\n    locator, _ = mdates.date_ticker_factory(1, tz=dateutil.tz.gettz('EST'))\n    assert isinstance(locator, mdates.HourLocator)\n    assert locator.tz == dateutil.tz.gettz('EST')\n", "def test_date_range_creation():\n    start = datetime.datetime(2023, 10, 26)\n    end = datetime.datetime(2023, 11, 1)\n    expected_dates = [\n        datetime.datetime(2023, 10, 26),\n        datetime.datetime(2023, 10, 27),\n        datetime.datetime(2023, 10, 28),\n        datetime.datetime(2023, 10, 29),\n        datetime.datetime(2023, 10, 30),\n        datetime.datetime(2023, 10, 31),\n        datetime.datetime(2023, 11, 1),\n    ]\n\n    dates = mdates.date_range(start, end, freq='D')\n\n    assert dates.tolist() == expected_dates\n", "    def test_date_ticker_factory_date_range():\n        locator, _ = mdates.date_ticker_factory(span=(datetime.datetime(2010, 1, 1), datetime.datetime(2012, 1, 1)))\n        assert isinstance(locator, mdates.YearLocator)\n", "    def test_rrulewrapper_with_timezone():\n        tz = dateutil.tz.gettz('America/Los_Angeles')\n\n        dtstart = tz.localize(datetime.datetime(2023, 1, 1, 0))\n        dtend = tz.localize(datetime.datetime(2023, 1, 15, 0))\n\n        rule = mdates.rrulewrapper(freq=dateutil.rrule.DAILY, dtstart=dtstart)\n        act = rule.between(dtstart, dtend)\n        exp = [\n            tz.localize(datetime.datetime(2023, 1, 1, 13, 0)),\n            tz.localize(datetime.datetime(2023, 1, 2, 14, 0)),\n            tz.localize(datetime.datetime(2023, 1, 3, 15, 0)),\n            tz.localize(datetime.datetime(2023, 1, 4, 16, 0)),\n            tz.localize(datetime.datetime(2023, 1, 5, 17, 0)),\n            tz.localize(datetime.datetime(2023, 1, 6, 18, 0)),\n            tz.localize(datetime.datetime(2023, 1, 7, 19, 0)),\n            tz.localize(datetime.datetime(2023, 1, 8, 20, 0)),\n            tz.localize(datetime.datetime(2023, 1, 9, 21, 0)),\n            tz.localize(datetime.datetime(2023, 1, 10, 22, 0)),\n            tz.localize(datetime.datetime(2023, 1, 11, 23, 0)),\n            tz.localize(datetime.datetime(2023, 1, 12, 24, 0)),\n            tz.localize(datetime.datetime(2023, 1, 13, 25, 0)),\n            tz.localize(datetime.datetime", "    def test_rrulewrapper_with_tz():\n        tz = dateutil.tz.gettz('America/New_York')\n        dtstart = tz.localize(datetime.datetime(2017, 4, 1, 0))\n        dtend = tz.localize(datetime.datetime(2017, 4, 4, 0))\n\n        rule = mdates.rrulewrapper(freq=dateutil.rrule.DAILY,\n                                  dtstart=dtstart,\n                                  tzinfo=tz)\n\n        act = rule.between(dtstart, dtend)\n        exp = [tz.localize(datetime.datetime(2017, 4, 1, 13, 0)),\n               tz.localize(datetime.datetime(2017, 4, 2, 14, 0))]\n        assert act == exp\n\n"], "sample_766": ["def test_sparse_coder_partial_fit():\n    n_components = 12\n    rng = np.random.RandomState(0)\n    V = rng.randn(n_components, n_features)  # random init\n    V /= np.sum(V ** 2, axis=1)[:, np.newaxis]\n    sparse_coder = SparseCoder(dictionary=V, transform_algorithm='lars',\n                              transform_alpha=0.001)\n    sparse_coder.partial_fit(X[0:10, :])\n    code = sparse_coder.transform(X[0:10, :])\n    assert_array_almost_equal(code, sparse_encode(X[0:10, :], V,\n                                                 algorithm='lars',\n                                                 alpha=0.001))\n", "def test_dict_learning_n_jobs(transform_algorithm, n_jobs):\n    n_components = 5\n    dico = DictionaryLearning(n_components, transform_algorithm=transform_algorithm,\n                              random_state=0, n_jobs=n_jobs).fit(X)\n    assert dico.components_.shape == (n_components, n_features) \n", "def test_sparse_coder_fit_transform():\n    n_components = 12\n    rng = np.random.RandomState(0)\n    V = rng.randn(n_components, n_features)  # random init\n    V /= np.sum(V ** 2, axis=1)[:, np.newaxis]\n    coder = SparseCoder(dictionary=V, transform_algorithm='lasso_lars',\n                       transform_alpha=0.001)\n    coder.fit(X)\n    code = coder.transform(X)\n    assert not np.all(code == 0)\n    assert_less(np.sqrt(np.sum((np.dot(code, V) - X) ** 2)), 0.1)\n", "def test_dict_learning_online_n_nonzero_coefs(transform_algorithm):\n    n_components = 8\n    dico = MiniBatchDictionaryLearning(n_components,\n                                      transform_algorithm=transform_algorithm,\n                                      n_jobs=4,\n                                      transform_n_nonzero_coefs=3, random_state=0).fit(X)\n    code = dico.transform(X[np.newaxis, 1])\n    assert_equal(len(np.flatnonzero(code)), 3)\n\n\n\n", "def test_sparse_coder_fit_transform():\n    n_components = 12\n    rng = np.random.RandomState(0)\n    V = rng.randn(n_components, n_features)  # random init\n    V /= np.sum(V ** 2, axis=1)[:, np.newaxis]\n    coder = SparseCoder(dictionary=V, transform_algorithm='lasso_lars',\n                       transform_alpha=0.001)\n    code = coder.fit_transform(X)\n    assert not np.all(code == 0)\n    assert_less(np.sqrt(np.sum((np.dot(code, V) - X) ** 2)), 0.1)\n"], "sample_220": ["    def test_delete_cookie_with_path(self):\n        response = HttpResponse()\n        response.set_cookie('c', path='/admin')\n        response.delete_cookie('c', path='/')\n        self.assertEqual(response.cookies['c']['path'], '/')\n", "    def test_delete_cookie_path(self):\n        response = HttpResponse()\n        response.set_cookie('c', path='/test/')\n        response.delete_cookie('c', path='/test')\n        cookie = response.cookies['c']\n        self.assertEqual(cookie['path'], '/test')\n        response.delete_cookie('c', path='/')\n        cookie = response.cookies['c']\n        self.assertEqual(cookie['path'], '/')\n\n", "    def test_delete_cookie_path(self):\n        response = HttpResponse()\n        response.set_cookie('c', path='/admin')\n        response.delete_cookie('c', path='/')\n        self.assertEqual(response.cookies['c']['path'], '/')\n\n\n\n", "    def test_delete_cookies_with_path(self):\n        response = HttpResponse()\n        response.set_cookie('c', path='/admin')\n        response.delete_cookie('c', path='/admin')\n        cookie = response.cookies.get('c')\n        self.assertIsNone(cookie)\n\n        response.set_cookie('d', path='/')\n        response.delete_cookie('d', path='/admin')\n        cookie = response.cookies.get('d')\n        self.assertIsNotNone(cookie)\n", "    def test_delete_cookie_path(self):\n        response = HttpResponse()\n        response.set_cookie('c', path='/admin')\n        response.delete_cookie('c', path='/')\n        self.assertEqual(response.cookies['c']['path'], '/')\n\n\n"], "sample_826": ["def test_one_hot_encoder_sparse_output():\n    X = np.array([['abc', 2, 55], ['def', 1, 55]])\n\n    ohe = OneHotEncoder(sparse=True)\n    trans = ohe.fit_transform(X)\n    assert isinstance(trans, sparse.csr_matrix)\n\n    # also test that sparse output works as expected with other methods\n    trans = ohe.transform(X)\n    assert isinstance(trans, sparse.csr_matrix)\n\n", "def test_one_hot_encoder_sparse_output():\n    X = [['abc', 2, 55], ['def', 1, 55]]\n    enc = OneHotEncoder(sparse=True)\n    enc.fit(X)\n    assert isinstance(enc.transform(X), sparse.csr_matrix)\n    assert_array_equal(enc.inverse_transform(enc.transform(X)).toarray(),\n                       X)\n\n    enc = OneHotEncoder(sparse=True, handle_unknown='ignore')\n    X2 = [['abc', 2, 56], ['def', 1, 55]]\n    assert isinstance(enc.transform(X2), sparse.csr_matrix)\n    # When sparse=True, handle_unknown='ignore' should still return a sparse matrix\n    assert isinstance(enc.transform(X2).toarray(), np.ndarray)\n\n\n\n", "def test_handle_unknown_categorical_encoder():\n    categories_data = [['a', 'b', 'c'], [1, 2, 3]]  # Two features\n    X = np.array([['a', 1], ['d', 4], ['c', 2]]).T\n\n    enc = OneHotEncoder(handle_unknown='ignore', categories=categories_data)\n    x_transformed = enc.fit_transform(X)\n    assert x_transformed.shape == (3, 6)  # Three samples, six features\n\n    # Check if unknown values are ignored\n    assert np.all(x_transformed[:, 2] == 0)  # 'd' is unknown in category1\n    assert np.all(x_transformed[:, 5] == 0)  # 4 is unknown in category2\n\n    enc = OneHotEncoder(handle_unknown='error')\n    with pytest.raises(ValueError, match=\"Found unknown categories\"):\n        enc.fit(X)\n\n", "def test_one_hot_encoder_handle_unknown_invalid_value():\n    with pytest.raises(ValueError, match=\"Invalid value for 'handle_unknown'\"):\n        OneHotEncoder(handle_unknown='invalid')\n\n", "def test_one_hot_encoder_invalid_categories_dtype():\n    enc = OneHotEncoder()\n    with pytest.raises(ValueError, match=\"`categories` should be a list of\"):\n        enc.fit([[1, 2], [3, 4]], categories=[\n            np.array([1, 2, 3]), np.array([1, 2, 3])\n        ])\n\n\n"], "sample_413": ["    def test_template_tags_with_same_name_in_list(self):\n        self.assertEqual(\n            check_for_template_tags_with_the_same_name(None),\n            [self.error_same_tags],\n        )\n", "    def test_template_tags_same_library_name_and_different_module_path(self):\n        with self.settings(\n            TEMPLATES=[\n                self.get_settings(\n                    \"same_tags\", \"same_tags_app_1.templatetags.same_tags\"\n                ),\n                self.get_settings(\n                    \"same_tags\", \"check_framework.template_test_apps.same_tags_app_2.templatetags.different_tags\"\n                ),\n            ]\n        ):\n            self.assertEqual(\n                check_for_template_tags_with_the_same_name(None),\n                [],\n            )\n", "    def test_template_tags_with_same_name_from_different_apps(self):\n        with self.settings(\n            TEMPLATES=[\n                self.get_settings(\n                    \"same_tags\", \"check_framework.template_test_apps.other_app.templatetags.same_tags\"\n                ),\n                self.get_settings(\n                    \"same_tags\", \"check_framework.template_test_apps.other_app_2.templatetags.same_tags\"\n                ),\n            ]\n        ):\n            self.assertEqual(check_for_template_tags_with_the_same_name(None), [self.error_same_tags])\n", "    def test_template_tags_with_same_library_name_and_different_module_names(self):\n        with self.settings(\n            TEMPLATES=[\n                self.get_settings(\n                    \"same_tags\", \"same_tags_app_1.templatetags.same_tags\"\n                ),\n                self.get_settings(\n                    \"same_tags\", \"same_tags_app_2.templatetags.different_tags\"\n                ),\n            ]\n        ):\n            self.assertEqual(\n                check_for_template_tags_with_the_same_name(None),\n                [\n                    Error(\n                        E003.msg.format(\n                            \"'same_tags'\",\n                            \"'check_framework.template_test_apps.same_tags_app_1.\"\n                            \"templatetags.same_tags', \"\n                            \"'check_framework.template_test_apps.same_tags_app_2.\"\n                            \"templatetags.different_tags'\",\n                        ),\n                        id=E003.id,\n                    )\n                ],\n            )\n", "    def test_template_tags_with_same_library_name_and_same_module_name(self):\n        with self.settings(\n            TEMPLATES=[\n                self.get_settings(\n                    \"same_tags\", \"same_tags_app_1.templatetags.same_tags\"\n                ),\n                self.get_settings(\n                    \"same_tags\", \"same_tags_app_2.templatetags.same_tags\"\n                ),\n            ]\n        ):\n            self.assertEqual(\n                check_for_template_tags_with_the_same_name(None),\n                [self.error_same_tags],\n            )\n"], "sample_137": ["    def test_parse_rst_nested_groups(self):\n        pattern = r'\\(\\?P<a>\\w+)/b/(.+)\\s+(?P<c>\\w+)\\s*$'\n        replaced_pattern = replace_named_groups(pattern)\n        self.assertEqual(replaced_pattern, r'<a>/b/<var> <c>')\n", "    def test_replace_named_groups(self):\n        self.assertEqual(replace_named_groups(r'^(?P<name>\\w+)/b/(?P<id>\\d+)$'), r'^<name>/b/<id>$')\n        self.assertEqual(replace_named_groups(r'^(?P<a>\\w+)/b/(?P<b>\\w+)/c/(?P<c>\\w+)$'), r'^<a>/b/<b>/c/<c>$')\n        self.assertEqual(replace_named_groups(r'^(?P<name>\\w+)/b/(\\w+)$'), r'^<name>/b/<var>$')\n        self.assertEqual(replace_named_groups(r'^(?P<a>\\w+)/b/((x|y)\\w+)$'), r'^(?P<a>\\w+)/b/<var>$')\n        self.assertEqual(replace_named_groups(r'^(?P<name>\\w+)/b/\\w+$'), r'^<name>/b/<var>$')\n        self.assertEqual(replace_named_groups(r'^(?P<a>\\w+)/b/(?P<c>\\w+)$'), r'^<a>/b/<c>$')\n        self.assertEqual(replace_named_groups(r'^(?:(?P<a>\\w+)/b/(?P<c>\\w+))'), r'^<a>/b/<c>$')\n", "    def test_replace_named_groups(self):\n        pattern = r'^(?P<name>\\w+)/b/(?P<id>\\d+)$'\n        expected_pattern = r'^<name>/b/<id>$'\n        self.assertEqual(replace_named_groups(pattern), expected_pattern)\n\n        pattern = r'^(?P<name>\\w+)/b/(?P<id>\\d+)/(?P<value>\\w+)$'\n        expected_pattern = r'^<name>/b/<id>/<value>$'\n        self.assertEqual(replace_named_groups(pattern), expected_pattern)\n\n        pattern = r'^(?P<name>\\w+)/b/(?P<id>\\w+)$'\n        expected_pattern = r'^<name>/b/<id>$'\n        self.assertEqual(replace_named_groups(pattern), expected_pattern)\n\n\n", "    def test_replace_named_groups(self):\n        self.assertEqual(replace_named_groups('^(?P<a>\\w+)/b/(\\w+)$'), '^<a>/b/<var>$')\n        self.assertEqual(replace_named_groups('^(?P<a>\\w+)/b/(?P<c>\\w+)/$'), '^<a>/b/<c>$')\n        self.assertEqual(replace_named_groups('^(?P<a>\\w+)/b/(\\w+)'), '^<a>/b/<var>')\n        self.assertEqual(replace_named_groups('^(?P<a>\\w+)/b/(?P<c>\\w+)'), '^<a>/b/<var>')\n        self.assertEqual(replace_named_groups('^(?P<a>\\w+)'), '<a>')\n        self.assertEqual(replace_named_groups('^((?P<a>\\w+))'), '<var>')\n", "    def test_parse_rst_with_docstring_leading_line_feed(self):\n        title, body, _ = parse_docstring('\\nfirstline\\n\\n    second line')\n        with captured_stderr() as stderr:\n            self.assertEqual(parse_rst(title, ''), '<p>firstline</p>\\n')\n            self.assertEqual(parse_rst(body, ''), '<p>second line</p>\\n')\n        self.assertEqual(stderr.getvalue(), '') \n"], "sample_1168": ["compilation error", "def test_partition_by_key():\n    from collections import defaultdict\n    d = defaultdict(list)\n    for i in range(10):\n        d[i % 3].append(i)\n    assert list(partition_by_key(d, lambda x: x % 3)) == [\n        [0, 3, 6, 9], [1, 4, 7], [2, 5, 8]]\n    d = defaultdict(list)\n    d['a'].append(1)\n    d['b'].append(2)\n    d['a'].append(3)\n    assert list(partition_by_key(d, key=lambda x: x)) == [\n        ['a', 'a'], ['b'],]\n", "compilation error", "compilation error", "compilation error"], "sample_1083": ["compilation error", "compilation error", "def test_issue_11711():\n    x = Symbol('x')\n    assert sech(x).rewrite('tanh') == 1 / (sqrt(1 - tanh(x)**2))\n\n\n", "def test_asinh_acosh_atanh_with_complex_values():\n\n\n    #Test values\n    z = symbols('z', complex=True)\n    assert asinh(z).is_complex is True\n    assert acosh(z).is_complex is True\n    assert atanh(z).is_complex is True\n", "    def test_acoth_complex():\n        a, b = symbols('a,b', real=True)\n        z = a + b*I\n        for func in [acoth]:\n            assert func(z).conjugate() == func(a - b*I)\n\n\n\n"], "sample_890": ["def test_kfold_cv(direction):\n    \"\"\"Check that KFold CV is used when no other cv is specified\n    and the estimator is a classifier.\"\"\"\n\n    X, y = make_classification(n_samples=50, n_features=5, random_state=0)\n    knn = KNeighborsClassifier()\n    sfs = SequentialFeatureSelector(\n        knn,\n        n_features_to_select=\"auto\",\n        direction=direction,\n    )\n    sfs.fit(X, y)\n    assert isinstance(sfs.cv, KFold)\n\n", "def test_n_features_to_select_integer_out_of_range(direction):\n    \"\"\"Test that an error is raised when `n_features_to_select` is set to a\n    value integer out of range.\n    \"\"\"\n    n_features = 5\n    X, y = make_regression(n_features=n_features, random_state=0)\n\n    for invalid_n_features in [-1, n_features + 1]:\n        with pytest.raises(ValueError, match=r\"n_features_to_select must be\"):\n            SequentialFeatureSelector(\n                LinearRegression(),\n                n_features_to_select=invalid_n_features,\n                direction=direction,\n                cv=2,\n            ).fit(X, y)\n\n", "def test_scoring_parameter(direction):\n    X, y = make_classification(n_features=10)\n    sfs = SequentialFeatureSelector(\n        KNeighborsClassifier(),\n        n_features_to_select=\"auto\",\n        direction=direction,\n        scoring=\"f1\",\n        cv=2,\n    )\n    sfs.fit(X, y)\n    assert sfs.get_score()\n", "def test_scoring_param():\n    X, y = make_classification(n_features=10, random_state=0)\n    scoring = \"roc_auc\"\n\n    sfs = SequentialFeatureSelector(\n        KNeighborsClassifier(),\n        n_features_to_select=\"auto\",\n        scoring=scoring,\n        cv=2,\n    )\n    sfs.fit(X, y)\n    assert sfs.get_support(indices=True).shape[0] > 0\n\n    # Check that the scoring parameter used is indeed the one we passed\n    assert sfs._estimator.scoring == scoring\n\n", "def test_n_features_to_select_fraction_with_many_features(direction):\n    # Test passing a fraction  as n_features_to_select with many features\n    # (50 features to pick 20%)\n    X, y = make_regression(n_features=50)\n    sfs = SequentialFeatureSelector(\n        LinearRegression(),\n        n_features_to_select=0.2,\n        direction=direction,\n        cv=2,\n    )\n    sfs.fit(X, y)\n    assert sfs.get_support(indices=True).shape[0] == int(0.2 * 50)\n\n\n\n"], "sample_569": ["    def test_lmplot_transform(self):\n\n        df = self.df.copy()\n        df[\"z\"] = np.sqrt(df[\"z\"])\n        g = lm.lmplot(x=\"x\", y=\"z\", data=df)\n        ax = g.axes[0, 0]\n        assert ax.lines[0].get_xdata() == df[\"x\"].unique()\n        assert ax.lines[0].get_ydata() == df.groupby(\"x\")[\"z\"].transform('mean')  \n\n", "    def test_lmplot_facet_grid(self):\n\n        g = lm.lmplot(data=self.df, x=\"x\", y=\"y\", hue=\"z\", col=\"g\",\n                      row=\"h\",  facet_kws={\"gridspec_kw\": {\"height_ratios\": [1, 2]}})\n        assert g.axes.shape == (3, 2, 2)\n", "    def test_lmplot_multiple_y(self):\n\n        df = pd.DataFrame({\"x\": self.df.x,\n                           \"y1\": self.df.y,\n                           \"y2\": self.df.y + self.rs.randn(len(self.df))})\n\n        g = lm.lmplot(x=\"x\", y=[\"y1\", \"y2\"], data=df)\n        assert len(g.axes[0, 0].lines) == 2\n", "    def test_regplot_with_units(self):\n\n        p = lm._RegressionPlotter(\"x\", \"y\", data=self.df, units=\"s\")\n        f, ax = plt.subplots()\n        lm.regplot(x=\"x\", y=\"y\", data=self.df, ax=ax)\n        label = ax.lines[0].get_label()\n        assert label == \"x vs y (s)\"\n\n\n\n", "    def test_lmplot_data_column_missing(self):\n\n        df = self.df.copy()\n        df['missing_col'] = None\n        with pytest.raises(ValueError):\n            lm.lmplot(x=\"x\", y=\"y\", data=df, hue=\"missing_col\")\n"], "sample_938": ["def test_man_show_urls(app, status, warning):\n    app.config['man_show_urls'] = True\n    app.builder.build_all()\n    content = (app.outdir / 'sphinxtests.1').read_text()\n    assert r'http://example.com' in content\n", "def test_custom_man_pages_setting(app, status, warning):\n    app.build()\n    assert (app.outdir / '1' / 'example.1').exists()\n", "def test_unicode_characters(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'unicode.1').read_text()\n    assert '\u00f6\u00e4\u00fc' in content\n", "def test_unknown_docname(app, status, warning):\n    app.config.man_pages = [('unknown', 'sphinx', 'unknown', [], 1)]\n    with pytest.raises(sphinx.errors.NoUri):\n        app.builder.write()\n", "def test_man_show_urls(app, status, warning):\n    app.config.man_show_urls = True\n    app.build()\n    content = (app.outdir / 'python.1').read_text()\n    assert '<a href=\"https://www.example.com\">Example URL</a>' in content\n"], "sample_1122": ["compilation error", "compilation error", "def test_issue_14692_polar_lift():\n    from sympy import polar_lift, exp_polar\n    x = Symbol('x')\n    assert polar_lift(exp_polar(pi*I)*x) == exp_polar(I*pi)*x\n    assert polar_lift(exp_polar(pi*I)) == exp_polar(pi*I)\n    assert polar_lift(x + I*x) == (x*sqrt(2))*exp_polar(pi/4)\n\n\n\n", "def test_issue_14995():\n    from sympy import cos, sin, pi\n    x = Symbol('x')\n    y = Symbol('y')\n    assert cos(x + I*y) == cos(x)*cosh(y) - I*sin(x)*sinh(y)\n    assert sin(x + I*y) == sin(x)*cosh(y) + I*cos(x)*sinh(y)\n    assert cos(x - I*y) == cos(x)*cosh(y) + I*sin(x)*sinh(y)\n    assert sin(x - I*y) == sin(x)*cosh(y) - I*cos(x)*sinh(y)\n    assert cos(I*x) == cosh(x)\n    assert sin(I*x) == I*sinh(x)\n    assert cos(x + I*pi) == cos(x)*(-1) - I*sin(x)*0\n    assert sin(x + I*pi) == sin(x)*(-1) + I*cos(x)*0\n", "compilation error"], "sample_710": ["    def test_do_cleanups_on_teardown_failure_in_setup_failure_order(pytester: Pytester) -> None:\n        testpath = pytester.makepyfile(\n            \"\"\"\n            import unittest\n            class MyTestCase(unittest.TestCase):\n                values = []\n                        self.values.append(1)\n                    self.addCleanup(cleanup)\n                    assert False\n                    assert False\n                    pass\n                    pass\n                assert MyTestCase.values == [1, 1]\n            \"\"\"\n        )\n        reprec = pytester.inline_run(testpath)\n        passed, skipped, failed = reprec.countoutcomes()\n        assert failed == 2\n        assert passed == 1\n", "def test_do_cleanups_on_multiple_failed_tests(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n        class MyTestCase(unittest.TestCase):\n            values = []\n                    self.values.append(1)\n                self.addCleanup(cleanup)\n                assert False\n                assert False\n                pass\n            assert MyTestCase.values == [1, 1]\n    \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    passed, skipped, failed = reprec.countoutcomes()\n    assert failed == 2\n    assert passed == 1\n", "    def test_do_cleanups_when_test_is_skipped(pytester: Pytester) -> None:\n        testpath = pytester.makepyfile(\n            \"\"\"\n            import unittest\n            class MyTestCase(unittest.TestCase):\n                values = []\n                @unittest.skip(\"Test should be skipped\")\n                    pass\n                        self.values.append(1)\n                    self.addCleanup(cleanup)\n                    pass\n                assert MyTestCase.values == [1]\n            \"\"\"\n        )\n        reprec = pytester.inline_run(testpath)\n        passed, skipped, failed = reprec.countoutcomes()\n        assert skipped == 1\n        assert passed == 1\n", "    def test_do_cleanups_are_not_called_if_test_fails_before_cleanup(pytester: Pytester) -> None:\n        testpath = pytester.makepyfile(\n            \"\"\"\n            import unittest\n            class MyTestCase(unittest.TestCase):\n                values = []\n                        self.values.append(1)\n                    self.addCleanup(cleanup)\n\n                    assert False\n                    pass\n                assert MyTestCase.values == [1]\n            \"\"\"\n        )\n        reprec = pytester.inline_run(testpath)\n        passed, skipped, failed = reprec.countoutcomes()\n        assert failed == 1\n        assert passed == 1\n", "    def test_do_cleanups_after_teardown_failure(pytester: Pytester) -> None:\n        testpath = pytester.makepyfile(\n            \"\"\"\n            import unittest\n            class MyTestCase(unittest.TestCase):\n                values = []\n                        self.values.append(1)\n                    self.addCleanup(cleanup)\n                    assert False\n                    self.values.append(2) \n                    pass\n                    pass\n                assert MyTestCase.values == [1, 2, 2]\n            \"\"\"\n        )\n        reprec = pytester.inline_run(testpath)\n        passed, skipped, failed = reprec.countoutcomes()\n        assert failed == 2\n        assert passed == 1\n\n"], "sample_720": ["def test_power_transformer_non_numeric_data():\n    pt = PowerTransformer(method='box-cox')\n\n    # Test raising an error for non-numeric data\n    X = np.array([['a', 2, 'c'], ['b', 4, 'd']])\n    assert_raises(TypeError, pt.fit_transform, X)\n", "def test_power_transformer_robust_scale():\n    X = np.abs(X_2d)\n    X_centered = X - np.mean(X, axis=0)\n\n    pt_robust = PowerTransformer(method='box-cox', standardize=False,\n                                robust=True)\n    pt_default = PowerTransformer(method='box-cox', standardize=False)\n\n    # Test that robust scaling works\n    pt_robust.fit(X_centered)\n    pt_default.fit(X_centered)\n\n    X_robust_trans = pt_robust.transform(X_centered)\n    X_default_trans = pt_default.transform(X_centered)\n    assert_array_almost_equal(X_robust_trans, X_default_trans)\n", "def test_power_transformer_nan_values():\n    X = np.abs(X_2d)\n    X[0, 1] = np.nan\n\n    for standardize in [True, False]:\n        pt = PowerTransformer(method='box-cox', standardize=standardize)\n        \n        with assert_raises(ValueError):\n            pt.fit(X)\n\n        with assert_raises(ValueError):\n            pt.transform(X)\n\n\n\n\n", "def test_power_transformer_with_nan():\n    X = np.array([[1, 2, np.nan], [4, 5, 6], [7, 8, 9]])\n    pt = PowerTransformer(method='box-cox')\n\n    # Test handling of NaNs during fitting\n    with np.errstate(invalid='ignore'):\n        pt.fit(X)\n    assert np.isnan(pt.lambdas_).any()  # Check that NaN values are handled\n\n    # Test transformation with NaNs\n    X_transformed = pt.transform(X)\n    assert np.isnan(X_transformed).any()  # Check that NaNs are preserved\n\n    # Test inverse transformation with NaNs (should raise)\n    assert_raises(RuntimeWarning, pt.inverse_transform, X_transformed)\n\n\n\n", "compilation error"], "sample_565": ["def test_grid_axes_locator():\n    fig = plt.figure()\n    grid = Grid(fig, 111, (2, 2), direction=\"row\")\n    for i in range(4):\n        locator = grid[i].get_axes_locator()\n        assert locator.args[0] == i\n    for i in range(2):\n        locator = grid[i].get_axes_locator()\n        assert locator.args[1] == i\n\n\n", "def test_inset_axes_aspect():\n    fig, ax = plt.subplots()\n    ax_inset = zoomed_inset_axes(ax, zoom=2, loc='upper right')\n    ax_inset.set(aspect='equal')\n    ax.set_xlim(-1, 1)\n    ax.set_ylim(-1, 1)\n    ax.plot([0.5, 0.8], [0.5, 0.2])\n    ax_inset.plot([0.2, 0.3], [0.2, 0.4])\n", "def test_inset_transform_coords():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], [4, 5, 6])\n    inset_ax = zoomed_inset_axes(ax, zoom=2, loc='upper left')\n    inset_ax.plot([1, 2, 3], [4, 5, 6])\n    \n    # Test transforming coordinates from ax to inset_ax\n    x, y = 1.5, 5.5\n    inset_x, inset_y = ax.transAxes.transform_point((x, y))\n    inset_x, inset_y = inset_ax.transData.inverted().transform((inset_x, inset_y))\n    assert inset_x == pytest.approx(1.5)\n    assert inset_y == pytest.approx(5.5)\n\n\n", "def test_inset_locator_position():\n    fig, ax = plt.subplots(figsize=(3, 3))\n    ax_ins = plt.axes([0, 0, 1, 1])\n    ip = InsetPosition(ax, [0.3, 0.25, 0.4, 0.3])  # New position\n    ax_ins.set_axes_locator(ip)\n    ax_ins.plot([0, 1], [0, 1])\n\n\n\n", "def test_text_flow_wrap():\n    fig, ax = plt.subplots()\n    ax.text(0.5, 0.5, \"This is a long text string that should wrap to multiple lines. This is a long text string that should wrap to multiple lines.\",\n            ha=\"center\", va=\"center\",\n            wrap=True,)\n"], "sample_292": ["    def test_bare_secret_accepted_and_replaced_with_session_based(self):\n        \"\"\"\n        The csrf token is reset from a bare secret when using session-based\n        CSRF tokens.\n        \"\"\"\n        req = self._get_POST_bare_secret_csrf_cookie_request_with_token()\n        mw = CsrfViewMiddleware(token_view)\n        mw.process_request(req)\n        resp = mw.process_view(req, token_view, (), {})\n        self.assertIsNone(resp)\n        csrf_cookie = req.session[CSRF_SESSION_KEY]\n        self._check_token_present(resp, csrf_id=csrf_cookie)\n", "    def test_process_request_with_same_origin_referer(self):\n        \"\"\"\n        A POST request from the same origin as the current host is always\n        accepted.\n        \"\"\"\n        req = self._get_POST_request_with_token()\n        req._is_secure_override = True\n        req.META['HTTP_ORIGIN'] = 'https://www.example.com'\n        req.META['HTTP_HOST'] = 'www.example.com'\n        req.META['HTTP_REFERER'] = 'https://www.example.com'\n        mw = CsrfViewMiddleware(post_form_view)\n        mw.process_request(req)\n        resp = mw.process_view(req, post_form_view, (), {})\n        self.assertIsNone(resp)\n\n", "    def test_csrf_cookie_age_with_session_cookie_domain(self):\n        req = self._get_GET_no_csrf_cookie_request()\n        with self.settings(\n            CSRF_COOKIE_NAME='csrfcookie',\n            CSRF_COOKIE_DOMAIN='.example.com',\n            SESSION_COOKIE_DOMAIN='.example.com',\n            CSRF_COOKIE_AGE=123,\n            CSRF_COOKIE_PATH='/test/',\n            CSRF_COOKIE_SECURE=True,\n            CSRF_COOKIE_HTTPONLY=True,\n        ):\n            # token_view calls get_token() indirectly\n            mw = CsrfViewMiddleware(token_view)\n            mw.process_view(req, token_view, (), {})\n            resp = mw(req)\n            csrf_cookie = resp.cookies.get('csrfcookie')\n            self.assertEqual(csrf_cookie.get('max-age'), '123')\n", "    def test_cookie_secure_setting_respected(self):\n        \"\"\"\n        CSRF cookie is set with 'secure' flag based on settings.\n        \"\"\"\n        req = self._get_GET_no_csrf_cookie_request()\n        mw = CsrfViewMiddleware(ensure_csrf_cookie_view)\n        with self.settings(CSRF_COOKIE_SECURE=True):\n            mw.process_view(req, ensure_csrf_cookie_view, (), {})\n            resp = mw(req)\n            self.assertTrue('Secure' in resp.cookies[settings.CSRF_COOKIE_NAME].get_raw_value())\n\n        with self.settings(CSRF_COOKIE_SECURE=False):\n            mw.process_view(req, ensure_csrf_cookie_view, (), {})\n            resp = mw(req)\n            self.assertFalse('Secure' in resp.cookies[settings.CSRF_COOKIE_NAME].get_raw_value())\n", "    def test_https_good_referer_mismatched_cookie_domain(self):\n        \"\"\"\n        A POST HTTPS request with a good referer from a subdomain that is not\n        allowed by CSRF_COOKIE_DOMAIN and is allowed by SESSION_COOKIE_DOMAIN\n        should be accepted.\n        \"\"\"\n        self._test_https_good_referer_mismatched_cookie_domain()\n"], "sample_614": ["    def test_array_repr_with_complex_dtypes(self) -> None:\n        cases = [\n            np.complex64(1 + 2j),\n            np.complex128(1 + 2j),\n            np.array([1 + 2j, 3 + 4j], dtype=np.complex64),\n            np.array([[1 + 2j, 3 + 4j], [5 + 6j, 7 + 8j]], dtype=np.complex128),\n        ]\n        for array in cases:\n            actual = formatting.array_repr(array)\n            # Check if complex components are represented\n            assert \"j\" in actual\n", "    def test_diff_array_repr_with_attrs(self) -> None:\n        da_a = xr.DataArray(np.array([[1, 2, 3], [4, 5, 6]], dtype=\"int64\"),\n        dims=(\"x\", \"y\"),\n        coords={\n            \"x\": np.array([\"a\", \"b\"], dtype=\"U1\"),\n            \"y\": np.array([1, 2, 3], dtype=\"int64\"),\n        },\n        attrs={\"units\": \"m\", \"description\": \"desc\"})\n\n        da_b = xr.DataArray(\n            np.array([1, 2], dtype=\"int64\"),\n            dims=\"x\",\n            coords={\n                \"x\": np.array([\"a\", \"c\"], dtype=\"U1\"),\n                \"label\": (\"x\", np.array([1, 2], dtype=\"int64\")),\n            },\n            attrs={\"units\": \"kg\", \"note\": \"this is a note\"}\n        )\n\n        expected = dedent(\n            \"\"\"\\\n        Left and right DataArray objects are not identical\n        Differing dimensions:\n            (x: 2, y: 3) != (x: 2)\n        Differing values:\n        L\n            array([[1, 2, 3],\n                   [4, 5, 6]], dtype=int64)\n        R\n            array([1, 2], dtype=int64)\n        Differing coordinates:\n        L * x        (x) %cU1 'a' 'b'\n        R * x        (x) %cU1 'a' 'c'\n        Coordinates only on the left object:\n          * y        (y) int64 1 2 3\n        Coordinates only on the right object:\n            label    (x) int64 1 2\n        Differing attributes:\n        L   units: m\n        R   units: kg\n        L   description: desc\n        R   note: this is a note\n        Attributes only on the left object:\n            description: desc\"\"\"\n        ).strip()\n        \n        byteorder = \"<\" if sys.byteorder == \"little\" else \">\"\n\n        ", "    def test_summarize_long_array_repr(self) -> None:\n        da = xr.DataArray(np.random.randn(1000, 1000), dims=(\"x\", \"y\"))\n        with xr.set_options(display_expand_data=False):\n            repr_summary = formatting.array_repr(da)\n            assert \"...\" in repr_summary\n            assert \"array\" in repr_summary\n        with xr.set_options(display_expand_data=True):\n            repr_summary = formatting.array_repr(da)\n            assert \"array\" in repr_summary\n            assert f\"{da.shape[0]}x{da.shape[1]}\" in repr_summary\n", "compilation error", "    def test_data_vars_repr_with_empty_data(self) -> None:\n        ds = xr.Dataset(data_vars={\"var1\": xr.DataArray([], dims=\"x\")})\n        actual = formatting.data_vars_repr(ds.data_vars)\n        expected = \"Data variables:\\nvar1    (x: 0)\\n    <xarray.DataArray (0)>\\n\"\n        assert actual == expected\n\n"], "sample_905": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_200": ["    def test_send_messages_broken_connection(self):\n        \"\"\"\n        send_messages() should handle a broken connection gracefully.\n        \"\"\"\n        backend = smtp.EmailBackend()\n        backend.connection = True  \n        with self.assertRaises(SMTPException):\n            backend.send_messages([email])\n", "    def test_server_stopped_and_reopened(self):\n        \"\"\"\n        When the server is stopped, reopen() should return False and\n        restarting the server can be done using start()\n        \"\"\"\n        self.backend.close()\n        self.assertIs(self.backend.open(), False)\n        self.server.start()\n", "    def test_invalid_host_throws_error(self):\n        with self.assertRaises(ValueError):\n            smtp.EmailBackend(host='invalid_host')\n", "    def test_connection_closed_correctly(self):\n        \"\"\"\n        The connection is closed properly, even if an exception occurs during\n        sending messages.\n        \"\"\"\n        with self.assertRaises(SMTPException):\n            with self.backend:\n                self.backend.send_messages([EmailMessage('Subject', 'Content', 'from@example.com', ['to@example.com'])])\n\n\n", "    def test_connection_closed_after_send_messages(self):\n        with self.assertRaises(ConnectionResetError):\n            self.backend.send_messages([\n                EmailMessage('Subject', 'Content', 'from@example.com', ['to@example.com'])\n            ])\n"], "sample_275": ["    def test_related_object_deletion_with_m2m(self):\n        \"\"\"\n        Tests deletion of related objects with many-to-many relationship.\n        Related objects should be deleted when the parent object is deleted.\n        \"\"\"\n        person = Person.objects.create(name='Alice')\n        group = OrgUnit.objects.create(name='Group A')\n        person.orgunits.add(group)\n        group.members.add(person) # Person added to OrgUnit's group member m2m field\n\n\n        # Delete Person\n        person.delete()\n\n        # Check if related objects are deleted\n        self.assertFalse(Person.objects.filter(pk=person.pk).exists())\n        self.assertFalse(OrgUnit.objects.filter(pk=group.pk).exists()) \n", "    def test_related_object_deletion(self):\n        \"\"\"\n        Tests deletion cascades through related objects with a foreign key\n        relationship, including deletion of related objects with\n        non-nullable fields.\n\n        Refs #20107.\n        \"\"\"\n        p = Person.objects.create(name=\"Alice\")\n        c = Contact.objects.create(label=\"Work\", person=p)\n\n        c.delete()\n        \n        self.assertFalse(Contact.objects.filter(person=p).exists())\n        self.assertEqual(Person.objects.get(name=\"Alice\").contacts.count(), 0) \n", "    def test_delete_related_objects_with_m2m_through(self):\n        \"\"\"\n        Deletion cascades correctly through a model with an M2M relationship\n        using a through model.\n\n        This test case ensures that when deleting a model instance that has an\n        M2M relationship with another model through a through model, the\n        related objects are also deleted correctly.\n        \"\"\"\n        person = Person.objects.create(name='Alice')\n        group = Group.objects.create(name='Writers')\n        person_group = PersonGroup.objects.create(person=person, group=group)  # The through model\n\n        with self.assertNumQueries(1):\n            person.delete()\n\n        self.assertFalse(Person.objects.filter(name='Alice').exists())\n        self.assertFalse(Group.objects.filter(name='Writers').exists())\n        self.assertFalse(PersonGroup.objects.filter(person=person).exists())\n", "    def test_delete_related_objects_on_reverse_relation(self):\n        \"\"\"\n        Deleting an object that has a reverse relation to another object (not\n        the primary key) should delete all related objects.\n        \"\"\"\n        p = Person.objects.create(name=\"Alice\")\n        # Create a reverse relation to Person in AwardNote\n        a = AwardNote.objects.create(note=\"Awarded for good work\", person=p)\n        award = Award.objects.create(name=\"Best Employee\", content_object=p)\n\n        # Delete the related Person object\n        p.delete()\n        self.assertFalse(Person.objects.filter(name=\"Alice\").exists())\n        self.assertFalse(AwardNote.objects.filter(note=\"Awarded for good work\").exists())\n        self.assertFalse(Award.objects.filter(name=\"Best Employee\").exists())\n\n\n\n", "    def test_delete_cascades_with_complex_m2m_through(self):\n        \"\"\"\n        Ensure that cascade deletes work correctly with M2M relationships through\n        complex models involving another foreign key.\n\n        \"\"\"\n        \n        # create items, locations and links\n        item1 = Item.objects.create(name=\"Item 1\")\n        item2 = Item.objects.create(name=\"Item 2\")\n        location1 = Location.objects.create(name=\"Location 1\")\n        location2 = Location.objects.create(name=\"Location 2\")\n        \n        # link items to locations through the complex M2M through model\n        link1 = Location.objects.create(name=\"Link 1\", item=item1)\n        link2 = Location.objects.create(name=\"Link 2\", item=item2)\n        \n        # cascade deletes should work\n        location1.delete()\n        self.assertEqual(Location.objects.count(), 1)\n        self.assertEqual(Item.objects.all().count(), 2)\n        self.assertTrue(Item.objects.get(name=\"Item 1\").location_set.count() == 0)\n        self.assertTrue(Item.objects.get(name=\"Item 2\").location_set.count() == 1)\n\n\n\n        \n\n\n\n"], "sample_223": ["    def test_ticket_24909(self):\n        c1 = Category.objects.create(name='c1')\n        c2 = Category.objects.create(name='c2')\n        item1 = Item.objects.create(category=c1)\n        item2 = Item.objects.create(category=c2)\n        subquery_result = (\n            Item.objects.filter(category__name='c1').values('pk')\n        )\n        qs = Item.objects.filter(\n            Q(category__name='c1') | Q(category__pk__in=subquery_result)\n        )\n        self.assertSequenceEqual(qs, [item1, item2])\n\n\n", "    def test_ticket_24605_complex_join(self):\n        \"\"\"\n        Subquery table names should be quoted even in complex joins.\n        \"\"\"\n        i1 = Individual.objects.create(alive=True, name='i1')\n        i2 = Individual.objects.create(alive=False, name='i2')\n        i3 = Individual.objects.create(alive=True, name='i3')\n\n        related_i1 = RelatedIndividual.objects.create(related=i1)\n        related_i2 = RelatedIndividual.objects.create(related=i2)\n\n        Parent.objects.create(child=i1)\n        Parent.objects.create(child=i3)\n\n        qs = (\n            Individual.objects.filter(\n                Q(related_individual__isnull=False),\n                Q(name__startswith='i')\n            )\n        )\n        self.assertQuerysetEqual(qs, [i1, i3])\n", "    def test_ticket_24605_double_filter(self):\n        # Ensure that double filtering with a nested Q object works correctly.\n        i1 = Individual.objects.create(alive=True)\n        RelatedIndividual.objects.create(related=i1)\n        i2 = Individual.objects.create(alive=False)\n        RelatedIndividual.objects.create(related=i2)\n        i3 = Individual.objects.create(alive=True)\n        i4 = Individual.objects.create(alive=False)\n\n        query = Individual.objects.filter(\n            Q(alive=True, related_individual__isnull=False) |\n            Q(alive=False, related_individual__exists=False)\n        )\n        self.assertSequenceEqual(list(query), [i1, i4])\n", "    def test_ticket_24952(self):\n        Category.objects.create(name='category1')\n        Category.objects.create(name='category2')\n        Subcategory.objects.create(name='subcategory1', category=Category.objects.get(name='category1'))\n        Subcategory.objects.create(name='subcategory2', category=Category.objects.get(name='category1'))\n        qs = Subcategory.objects.filter(category__name='category1').distinct()\n        self.assertSequenceEqual(qs, [Subcategory.objects.get(name='subcategory1'), Subcategory.objects.get(name='subcategory2')])\n", "    def test_ticket_24605_subquery_with_related_lookup(self):\n        \"\"\"\n        Subqueries with related lookups should use the correct table names.\n        \"\"\"\n        i1 = Individual.objects.create(alive=True)\n        RelatedIndividual.objects.create(related=i1)\n        i2 = Individual.objects.create(alive=False)\n        RelatedIndividual2.objects.create(related=i1)\n        i3 = Individual.objects.create(alive=True)\n\n        qs = Individual.objects.filter(\n            Q(alive=True) &\n            Q(related_individual__isnull=False) &\n            Q(related_individual2__isnull=True)\n        )\n        self.assertSequenceEqual(qs, [i1])\n"], "sample_731": ["compilation error", "    def test_feature_names(data):\n        assert data.feature_names == [\n            \"MedInc\", \"HouseAge\", \"AveRooms\", \"AveBedrms\", \"Population\",\n            \"AveOccup\", \"Latitude\", \"Longitude\"]\n", "compilation error", "compilation error", "compilation error"], "sample_550": ["def test_toolmanager_event_handling():\n    with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n        plt.rcParams['toolbar'] = 'toolmanager'\n    fig = plt.gcf()\n    initial_tools = fig.canvas.manager.toolmanager.tools\n    \n    # Simple tool with a custom event handler\n    class CustomTool(NavigationToolbar2.Tool):\n        name = 'custom_tool'\n            pass  # Do nothing for this test\n            pass  # Do nothing for this test\n            if isinstance(event, MouseEvent):\n                self.last_mouse_event = event\n    fig.canvas.manager.toolmanager.add_tool(CustomTool())\n    \n    # Trigger mouse event\n    start_event = MouseEvent(\n        \"button_press_event\", fig.canvas, 0, 0, MouseButton.LEFT,\n    )\n    fig.canvas.callbacks.process(\"mouse_press_event\", start_event)\n    \n    # Check if the tool handled the event\n    assert hasattr(fig.canvas.manager.toolmanager.get_tool('custom_tool'),\n                  'last_mouse_event')\n    assert start_event == fig.canvas.manager.toolmanager.get_tool('custom_tool').last_mouse_event\n    \n    # Remove the custom tool and restore initial state\n    fig.canvas.manager.toolmanager.remove_tool('custom_tool')\n    assert set(fig.canvas.manager.toolmanager.tools) == set(initial_tools)\n", "def test_toolmanager_set_enabled():\n    with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n        plt.rcParams['toolbar'] = 'toolmanager'\n    fig = plt.gcf()\n    toolmanager = fig.canvas.manager.toolmanager\n    assert toolmanager.get_tool('pan').enabled is True\n    toolmanager.set_enabled('pan', False)\n    assert toolmanager.get_tool('pan').enabled is False\n    toolmanager.set_enabled('pan', True)\n    assert toolmanager.get_tool('pan').enabled is True\n\n    with pytest.warns(UserWarning, match=\"ToolManager does not control tool 'foo'\"):\n        toolmanager.set_enabled('foo', False)\n    with pytest.warns(UserWarning, match=\"ToolManager does not control tool 'foo'\"):\n        assert toolmanager.get_tool('foo').enabled is True\n    toolmanager.set_enabled('foo', True)\n    with pytest.warns(UserWarning, match=\"ToolManager does not control tool 'foo'\"):\n        assert toolmanager.get_tool('foo').enabled is True\n\n    # test setting multiple tools at once\n    toolmanager.set_enabled(('pan', 'zoom'), False)\n    assert not toolmanager.get_tool('pan').enabled\n    assert not toolmanager.get_tool('zoom').enabled\n", "def test_toolbar_buttons():\n    with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n        plt.rcParams['toolbar'] = 'toolmanager'\n\n    fig = plt.gcf()\n    tb = NavigationToolbar2(fig.canvas)\n\n    # Check that tool buttons exist\n    assert hasattr(tb, 'zoom')\n    assert hasattr(tb, 'pan')\n    assert hasattr(tb, 'home')\n    assert hasattr(tb, 'save')\n\n    # Test that functions are triggered\n    tb.zoom()\n    assert fig.canvas.manager.toolmanager.active_tool.name == 'zoom'\n\n    tb.pan()\n    assert fig.canvas.manager.toolmanager.active_tool.name == 'pan'\n\n    tb.home()\n    assert fig.canvas.manager.toolmanager.active_tool.name == 'home'\n\n    tb.save()\n    assert fig.canvas.manager.toolmanager.active_tool.name == 'save'\n\n\n\n", "def test_toolbar_home_restores_zoom():\n    fig, ax = plt.subplots()\n    ax.plot(range(11), range(11))\n\n    tb = NavigationToolbar2(fig.canvas)\n    tb.zoom()\n\n    # Zoom in from (x, y) = (2, 2) to (5, 5).\n    start, stop = ax.transData.transform([(2, 2), (5, 5)])\n    MouseEvent(\"button_press_event\", fig.canvas, *start, MouseButton.LEFT)._process()\n    MouseEvent(\"button_release_event\", fig.canvas, *stop, MouseButton.LEFT)._process()\n    # Go back to home.\n    KeyEvent(\"key_press_event\", fig.canvas, \"h\")._process()\n\n    assert ax.get_xlim() == ax.get_ylim() == (0, 10)\n    assert ax.get_xaxis().get_scale() == 'linear'\n    assert ax.get_yaxis().get_scale() == 'linear'\n", "compilation error"], "sample_957": ["def test_stringify_type_hints_TypedDict():\n    from typing import TypedDict  # type: ignore\n\n    class MyDict(TypedDict):\n        name: str\n        age: int\n        \n\n    assert stringify(MyDict) == \"tests.test_util_typing.MyDict\"\n    assert stringify(TypedDict) == \"TypedDict\"\n\n\n\n", "def test_stringify_type_hints_Union_Empty():\n    assert stringify(Union[]) == \"Union\"  # type: ignore \n    assert stringify(Union[None]) == \"Optional[Union]\"  # type: ignore \n", "def test_restify_type_hints_complex_list(annotation, expected):\n    assert restify(annotation) == expected\n\n", "def test_stringify_type_hints_union_with_typing_ForwardRef():\n    from typing import ForwardRef  # type: ignore\n    from types import GenericAlias\n\n    class_forward_ref = ForwardRef(\"MyClass\")\n    assert isinstance(class_forward_ref, GenericAlias)\n    assert stringify(Union[class_forward_ref, int]) == \"Union[MyClass, int]\" \n", "def test_stringify_type_hints_type_aliases_forward_ref():\n    from typing import Tuple, ForwardRef  # type: ignore\n    from typing import TypeAlias as TypeAlias  \n\n    MyTuple = Tuple[str, int]\n    AliasType = TypeAlias(\"MyAlias\", MyTuple)\n\n    assert stringify(MyTuple) == \"Tuple[str, int]\"\n    assert stringify(AliasType) == \"MyAlias\"\n"], "sample_1172": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_700": ["compilation error", "compilation error", "compilation error", "def test_importorskip_warning() -> None:\n    import warnings\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"always\")\n        pytest.importorskip(\"warnings\")\n\n\n\n", "        def test_importorskip_existing():\n            pytest.importorskip('os')"], "sample_249": ["    def test_object_reference_with_foreign_key(self):\n        # serialize_db_to_string() and deserialize_db_from_string() handles\n        # circular references with foreign keys.\n        obj = Object.objects.create(name='X')\n        obj_ref = ObjectReference.objects.create(obj=obj)\n\n        data = connection.creation.serialize_db_to_string()\n        Object.objects.all().delete()\n        ObjectReference.objects.all().delete()\n        connection.creation.deserialize_db_from_string(data)\n        obj = Object.objects.get(name='X')\n        obj_ref = ObjectReference.objects.get()\n        self.assertEqual(obj_ref.obj, obj)\n\n", "    def test_multiple_object_references(self):\n        # serialize_db_to_string() and deserialize_db_from_string() handles multiple\n        # object references.\n        obj_1 = Object.objects.create(name='A')\n        obj_2 = Object.objects.create(name='B')\n        obj_ref_1 = ObjectReference.objects.create(obj=obj_1)\n        obj_ref_2 = ObjectReference.objects.create(obj=obj_2)\n        obj_1.object_references.add(obj_ref_1)\n        obj_1.object_references.add(obj_ref_2)\n        obj_2.object_references.add(obj_ref_1)\n        obj_2.object_references.add(obj_ref_2)\n        obj_ref_1.name = 'X'\n        obj_ref_1.save()\n        obj_ref_2.name = 'Y'\n        obj_ref_2.save()\n        # Serialize objects.\n        with mock.patch('django.db.migrations.loader.MigrationLoader') as loader:\n            # serialize_db_to_string() serializes only migrated apps, so mark\n            # the backends app as migrated.\n            loader_instance = loader.return_value\n            loader_instance.migrated_apps = {'backends'}\n            data = connection.creation.serialize_db_to_string()\n        Object.objects.all().delete()\n        ObjectReference.objects.all().delete()\n        # Deserialize objects.\n        connection.creation.deserialize_db_from_string(data)\n        obj_1 = Object.objects.get(name='A')\n        obj_2 = Object.objects.get(name='B')\n        obj_ref_1 = ObjectReference.objects.get(name='X')\n        obj_ref_2 = ObjectReference.objects.get(name='Y')\n        self.assertEqual(obj_1.object_references.count(), 2)\n        self.assertEqual(obj_2.object_references.count(), 2)\n        self.assertEqual(obj_1.object_references.first(), obj_ref_1)\n        self.assertEqual(obj", "    def test_schools_classes_with_objects(self):\n        # Tests serialization and deserialization of models with FKs to\n        # other models (including self-referential relationships)\n        obj = Object.objects.create(key='X')\n        school_class = SchoolClass.objects.create(name='First Class', obj=obj)\n        obj_2 = Object.objects.create(key='Y')\n        school_class_2 = SchoolClass.objects.create(name='Second Class', obj=obj_2)\n        school_class.students.add(school_class_2)\n        # Serialize objects\n        with mock.patch('django.db.migrations.loader.MigrationLoader') as loader:\n            loader_instance = loader.return_value\n            loader_instance.migrated_apps = {'backends'}\n            data = connection.creation.serialize_db_to_string()\n        SchoolClass.objects.all().delete()\n        Object.objects.all().delete()\n        # Deserialize objects\n        connection.creation.deserialize_db_from_string(data)\n        obj_1 = Object.objects.get(key='X')\n        obj_2 = Object.objects.get(key='Y')\n        school_class_1 = SchoolClass.objects.get(name='First Class')\n        school_class_2 = SchoolClass.objects.get(name='Second Class')\n        self.assertEqual(obj_1, school_class_1.obj)\n        self.assertEqual(obj_2, school_class_2.obj)\n        self.assertIn(school_class_2, school_class_1.students.all())", "    def test_object_reference_with_deferred_foreignkey(self):\n        # serialize_db_to_string() and deserialize_db_from_string() handles object references with deferred foreign keys.\n        obj1 = Object.objects.create(pk=1, field_a='X')\n        obj2 = ObjectReference.objects.create(obj=obj1)\n        \n        # Serialize objects.\n        with mock.patch('django.db.migrations.loader.MigrationLoader') as loader:\n            # serialize_db_to_string() serializes only migrated apps, so mark\n            # the backends app as migrated.\n            loader_instance = loader.return_value\n            loader_instance.migrated_apps = {'backends'}\n            data = connection.creation.serialize_db_to_string()\n\n        Object.objects.all().delete()\n        ObjectReference.objects.all().delete()\n\n        # Deserialize objects.\n        connection.creation.deserialize_db_from_string(data)\n        obj1 = Object.objects.get(pk=1)\n        obj2 = ObjectReference.objects.get()\n        self.assertEqual(obj2.obj, obj1)\n\n\n\n", "    def test_handle_object_references(self):\n        # serialize_db_to_string() and deserialize_db_from_string()\n        # correctly handle models with Object references.\n        obj_1 = Object.objects.create(key='X')\n        obj_2 = Object.objects.create(key='Y')\n        obj_ref = ObjectReference.objects.create(obj=obj_1)\n        obj_ref_2 = ObjectReference.objects.create(obj=obj_2)\n        # Serialize objects.\n        with mock.patch('django.db.migrations.loader.MigrationLoader') as loader:\n            # serialize_db_to_string() serializes only migrated apps, so mark\n            # the backends app as migrated.\n            loader_instance = loader.return_value\n            loader_instance.migrated_apps = {'backends'}\n            data = connection.creation.serialize_db_to_string()\n        Object.objects.all().delete()\n        ObjectReference.objects.all().delete()\n        # Deserialize objects.\n        connection.creation.deserialize_db_from_string(data)\n\n        # Assert that the object references are correctly linked.\n        obj_1 = Object.objects.get(key='X')\n        obj_2 = Object.objects.get(key='Y')\n        obj_ref = ObjectReference.objects.get()\n        self.assertEqual(obj_ref.obj, obj_1)\n\n\n\n"], "sample_101": ["    def test_get_script_name(self):\n        \"\"\"\n        Test get_script_name with different server configurations.\n        \"\"\"\n        environ = self.request_factory._base_environ(\n            SCRIPT_NAME='/myapp/',\n            PATH_INFO='/path/to/page/',\n            REQUEST_METHOD='GET'\n        )\n        self.assertEqual(get_script_name(environ), '/myapp/')\n\n        environ = self.request_factory._base_environ(\n            SCRIPT_URL='/myapp/',\n            PATH_INFO='/path/to/page/',\n            REQUEST_METHOD='GET'\n        )\n        self.assertEqual(get_script_name(environ), '/myapp/')\n\n        environ = self.request_factory._base_environ(\n            PATH_INFO='/path/to/page/',\n            REQUEST_METHOD='GET'\n        )\n        self.assertEqual(get_script_name(environ), '')\n", "    def test_request_class(self):\n        application = get_wsgi_application()\n        environ = self.request_factory._base_environ(\n            PATH_INFO=\"/\",\n            CONTENT_TYPE=\"text/html; charset=utf-8\",\n            REQUEST_METHOD=\"GET\"\n        )\n        request = application(environ, lambda *args: None)[0]\n        self.assertIsInstance(request, WSGIRequest)\n", "    def test_limited_stream(self):\n        environ = self.request_factory._base_environ(\n            PATH_INFO=\"/\",\n            CONTENT_TYPE=\"text/html; charset=utf-8\",\n            REQUEST_METHOD=\"POST\",\n            CONTENT_LENGTH=\"10\"\n        )\n        request = WSGIRequest(environ)\n        self.assertEqual(request._stream.remaining, 10)\n        self.assertEqual(request._stream.read(5), b\"xxxxx\")\n        self.assertEqual(request._stream.remaining, 5)\n        self.assertEqual(request._stream.read(5), b\"xxxxx\")\n        self.assertEqual(request._stream.remaining, 0)\n        with self.assertRaises(ValueError):\n            request._stream.read(1)\n\n\n\n", "    def test_limited_stream(self):\n        environ = self.request_factory._base_environ(\n            PATH_INFO=\"/\",\n            CONTENT_TYPE=\"text/html; charset=utf-8\",\n            REQUEST_METHOD=\"POST\",\n            CONTENT_LENGTH=\"10\"\n        )\n        request = WSGIRequest(environ)\n        self.assertEqual(len(request._stream.buffer), 0)\n        self.assertEqual(request._stream.remaining, 10)\n        data = b'Hello World'\n        # Simulate reading data with a limited stream.\n        request._stream._read_limited(len(data))\n        self.assertEqual(len(request._stream.buffer), 0)\n        self.assertEqual(request._stream.remaining, 0)\n", "    def test_get_script_name(self):\n        \"\"\"\n        Test get_script_name for various setups\n        \"\"\"\n        test_envs = [\n            {'SCRIPT_URL': b'/path/to/app'},\n            {'SCRIPT_NAME': b'/path/to/app'},\n            {'SCRIPT_NAME': b'/path/to/app/', 'PATH_INFO': b'/'},\n            {'PATH_INFO': b'/another/path'},\n        ]\n        for env in test_envs:\n            with self.subTest(env=env):\n                environ = self.request_factory.get_environ(**env)\n                script_name = get_script_name(environ)\n                if 'SCRIPT_URL' in env:\n                    self.assertEqual(script_name, '/path/to/app')\n                elif 'SCRIPT_NAME' in env:\n                    self.assertEqual(script_name, '/path/to/app')\n                else:\n                    self.assertEqual(script_name, '')\n"], "sample_862": ["compilation error", "def test_callable_analyzer_non_empty_list(Estimator):\n    if issubclass(Estimator, HashingVectorizer):\n        pytest.xfail('HashingVectorizer is not supported on PyPy')\n    data = ['this is text, not file or filename']\n    analyzer = lambda doc: ['a', 'b', 'c']\n    vec = Estimator(analyzer=analyzer).fit_transform(data)\n    assert vec.shape == (1, 3)\n", "compilation error", "compilation error", "def test_analyzer_callable_accept_array_like(Estimator):\n    if issubclass(Estimator, HashingVectorizer):\n        pytest.xfail('HashingVectorizer is not supported on PyPy')\n    data = [\n        ['this', 'is', 'text'],\n        ['this', 'is', 'again']\n    ]\n    vec = Estimator(analyzer=lambda doc: doc)\n    with pytest.raises(ValueError, match='Input must be a string'):\n        vec.fit_transform(data)\n"], "sample_85": ["    def test_fast_delete_related_models_with_deletion_ordering(self):\n        s = S.objects.create(r=R.objects.create())\n        for i in range(10):\n            T.objects.create(s=s)\n        # 1 for the delete, 1 for each related model delete.\n        self.assertNumQueries(11, s.delete)\n\n        models.signals.pre_delete.connect(self.log_pre_delete, sender=S)\n\n            self.assertEqual(S._meta.name, kwargs['instance'].__class__.__name__)\n\n        s_ = S.objects.get(id=s.id)\n        self.assertNumQueries(1, s_.delete)\n\n        for i in range(10):\n            t = get_child_objects(10, T)\n            self.assertEqual(get_parent_objects(10, S).first().id, s_.id)\n\n        models.signals.pre_delete.disconnect(log_pre_delete, sender=S)\n", "    def test_fast_delete_with_on_delete_set_null(self):\n        u = User.objects.create(avatar=Avatar.objects.create())\n        a = Avatar.objects.get(pk=u.avatar_id)\n        connection.ops.set_default_sequence(u._meta.db_table)\n        # 1 to delete u, then another for a\n        self.assertNumQueries(2, a.delete)\n        self.assertFalse(User.objects.exists())\n        self.assertFalse(Avatar.objects.exists())\n", "    def test_fast_delete_with_m2m_cascade(self):\n        m = M.objects.create()\n        m2m = M2MFrom.objects.create()\n        m.m2m.add(m2m)\n        # One query for M.objects.filter(pk=m.pk).delete()\n        # One query for deleting related M2MFrom objects\n        self.assertNumQueries(2, m.delete)\n        self.assertFalse(M.objects.exists())\n        self.assertFalse(M2MFrom.objects.exists())\n\n\n", "    def test_fast_delete_with_related_object_deletion(self):\n        m2m_from = M2MFrom.objects.create()\n        m2m_to = M2MTo.objects.create()\n        m2m_from.m2m.add(m2m_to)\n        m2m_from.delete()\n\n        self.assertFalse(M2MTo.objects.filter(pk=m2m_to.pk).exists())\n", "    def test_fast_delete_related_to_null_field(self):\n        a = create_a('test_fast_delete_related_to_null_field')\n        a.setnull.delete()\n        a = A.objects.get(pk=a.pk)\n        self.assertIsNone(a.setnull)\n\n\n"], "sample_340": ["    def test_circular_dependency(self):\n        \"\"\"\n        Raising a circular dependency error when the migrations are circular.\n        \"\"\"\n        with self.assertRaises(CircularDependencyError):\n            MigrationLoader(connection)\n", "    def test_loading_frozen_environment(self):\n        \"\"\"\n        MigrationLoader loads migrations from a frozen environment.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        loader.load_disk()\n        migrations = [\n            name for app, name in loader.disk_migrations if app == 'migrations'\n        ]\n        self.assertCountEqual(migrations, ['0001_initial', '0002_second']) \n", "    def test_file_ordering_respected(self):\n        loader = MigrationLoader(connection)\n        loader.load_disk()\n        self.assertEqual(\n            sorted([name for app, name in loader.disk_migrations if app == 'migrations']),\n            ['0001_initial', '0003_third', '0002_second']\n        )\n", "    def test_loading_squashed_complex_multi_apps_mixed_order(self):\n        loader = MigrationLoader(connection)\n        recorder = MigrationRecorder(connection)\n        self.record_applied(recorder, 'app1', '2_auto')\n        self.record_applied(recorder, 'app1', '1_auto')\n        loader.build_graph()\n\n        plan = set(loader.graph.forwards_plan(('app1', '4_auto')))\n        plan = plan - loader.applied_migrations.keys()\n        expected_plan = {\n            ('app1', '3_auto'),\n            ('app1', '4_auto'),\n        }\n\n        self.assertEqual(plan, expected_plan)\n", "    def test_empty_migration_directory(self):\n        loader = MigrationLoader(connection)\n        loader.load_disk()\n        self.assertEqual(loader.disk_migrations, []) \n"], "sample_84": ["    def test_parsing_invalid_http_date(self):\n        with self.assertRaises(ValueError):\n            parse_http_date('Invalid HTTP date')\n", "    def test_parsing_invalid_format(self):\n        with self.assertRaises(ValueError):\n            parse_http_date('Invalid Date')\n", "    def test_parsing_errors(self):\n        for date_string in (\n            'Sun, 06 Nov 1994 08:49:37 GM',  # Missing T\n            'Sun, 06 Nov 1994 08:49:37 GMTZ',  # Invalid timezone\n            'Sun Nov  6 08:49:37 1994Z',  # Missing timezone offset\n            'Sun Nov  6 08:49:37 0000',  # Year format error\n            'invalid',\n        ):\n            with self.assertRaises(ValueError):\n                parse_http_date(date_string)\n", "    def test_parsing_with_invalid_year(self):\n        with self.assertRaises(ValueError):\n            parse_http_date('Sun Nov  6 08:49:37 19')  \n\n\n", "    def test_parsing_with_timezone(self):\n        parsed = parse_http_date('Sun, 06 Nov 1994 08:49:37 PST')\n        self.assertEqual(datetime.utcfromtimestamp(parsed), datetime(1994, 11, 6, 16, 49, 37))\n"], "sample_694": ["def test_argument_type_str_choice_deprecated(pytester: Pytester) -> None:\n    p = pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            pass\n\n        @pytest.fixture\n            pass\n\n        @pytest.fixture\n            pass\n        \"\"\"\n    )\n\n    result = pytester.runpytest(\n        p,\n        \"-Wdefault::pytest.PytestRemovedIn8Warning\",\n        \"--filter=test_func\",\n    )\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestRemovedIn8Warning:  `type` argument to addoption() is the string 'str'.\",\n            \"For choices this is optional and can be omitted, \",\n            \"but when supplied should be a type (for example `str` or `int`).\"\n            \" (options: ['arg'])*\",\n        ]\n    )\n\n    result = pytester.runpytest(p, \"-Wdefault::pytest.PytestRemovedIn8Warning\", \"--filter=test_func\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestRemovedIn8Warning:  `type` argument to addoption() is the string 'int | str'.\",\n            \"For choices this is optional and can be omitted, \",\n            \"but when supplied should be a type (for example `str` or `int`).\"\n            \" (options: ['arg'])*\",\n        ]\n    )\n\n    result = pytester.runpytest(\n        p,\n        \"-Wdefault::pytest.PytestRemovedIn8Warning\",\n        \"--filter=test_func\",\n    )\n    result.assert_outcomes(passed=1)\n", "def test_private_is_deprecated_with_kwargs(pytester: Pytester) -> None:\n    class PrivateInit:\n            deprecated.check_ispytest(_ispytest)\n\n    with pytest.warns(\n        pytest.PytestDeprecationWarning, match=\"private pytest class or function\"\n    ):\n        PrivateInit(10, bar=\"hello\")  # Using kwargs should not trigger a warning\n\n", "def test_fspath_deprecated_for_config_files(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n            # Deprecated\n            config.rootdir = py.path.local(\"bla\")  \n            # Use this instead\n            config.rootdir = Path(\"bla\") \n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestRemovedIn8Warning: The (fspath: py.path.local) argument to File is deprecated.*\"\n        ]\n    )\n\n\n", "def test_deprecated_argument_choices_types(pytester: Pytester) -> None:\n        deprecated.check_ispytest(_ispytest)\n\n    pytester.makepyfile(\n        \"\"\"\n            pass\n        \"\"\"\n    )\n    with pytest.warns(\n        PytestDeprecationWarning,\n        match=re.escape(\n            \"`type` argument to addoption() is the string {typ!r}, \"\n            \"but when supplied should be a type (for example `str` or `int`). \"\n            \"(options: {names})\"\n        ),\n    ):\n        # type: ignore[call-overload]\n        pytester.runpytest(\"-Wdefault::pytest.PytestDeprecationWarning\")\n\n\n", "def test_deprecated_argument_type_choice(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            pass\n\n        @pytest.mark.parametrize(\"arg_type\", [str, int, float])\n            pass\n\n        @pytest.mark.parametrize(\"arg_type\", [\n            str,\n            int,\n            float\n        ])\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest(\n        \"-Wdefault::pytest.PytestRemovedIn8Warning\",\n        extra_ini=[\"addopts\", \"test_with_choices\", \"t.args=arg\", \"t.args=arg_type\"],\n    )\n    result.stdout.fnmatch_lines(\n        [\n            r\"*PytestRemovedIn8Warning: The `type` argument to addoption() is the string.*\",\n            r\"*PytestRemovedIn8Warning: The `type` argument to addoption() is the string.*\",\n        ]\n    )\n\n"], "sample_345": ["    def test_snapshot_files_handles_new_files(self):\n        new_file = self.ensure_file(self.tempdir / 'new_file.py')\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file, new_file]):\n            snapshot1 = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot1)\n            self.assertIn(new_file, snapshot1)\n", "    def test_snapshot_files_handles_new_files(self):\n        new_file = self.ensure_file(self.tempdir / 'new_file.py')\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file, new_file]):\n            snapshot1 = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot1)\n            self.assertIn(new_file, snapshot1)\n", "    def test_snapshot_files_handles_concurrent_modifications(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n            snapshot1 = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot1)\n            with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n                future1 = executor.submit(self.increment_mtime, self.existing_file)\n                future2 = executor.submit(self.increment_mtime, self.existing_file)\n                future1.result()\n                future2.result()\n            snapshot2 = dict(self.reloader.snapshot_files())\n            self.assertNotEqual(snapshot1[self.existing_file], snapshot2[self.existing_file])\n", "    def test_snapshot_files_handles_multiple_changes(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[\n            self.existing_file, self.nonexistent_file\n        ]):\n            snapshot1 = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot1)\n            self.assertNotIn(self.nonexistent_file, snapshot1)\n            self.increment_mtime(self.existing_file)\n        \n            snapshot2 = dict(self.reloader.snapshot_files())\n            self.assertNotEqual(snapshot1[self.existing_file], snapshot2[self.existing_file])\n            self.assertNotIn(self.nonexistent_file, snapshot2)\n", "    def test_snapshot_files_handles_new_files(self):\n        new_file = self.ensure_file(self.tempdir / 'new_file.py')\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file, new_file]):\n            snapshot1 = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot1)\n            self.assertIn(new_file, snapshot1)\n"], "sample_928": ["def test_heading():\n    env = Environment(undefined='strict')\n    env.language = 'en' \n\n    assert heading(env, 'Hello world!', 1) == 'Hello world!\\n===='\n    assert heading(env, 'Longer Header', 2) == 'Longer Header\\n---'\n    assert heading(env, 'This is a heading', 3) == 'This is a heading\\n~~'\n\n", "def test_heading():\n    env = Environment(undefined=jinja2.StrictUndefined)\n    env.language = 'en'\n    with env.from_string(\"This is heading text\"):\n        heading_text = heading(env, \"This is heading text\", level=2)\n    assert heading_text == \"This is heading text\\n--\\n\"\n", "def test_heading():\n    env = Environment()\n    assert heading(env, 'Hello', 1) == 'Hello\\n=\\n'\n    assert heading(env, '##Hello', 2) == '##Hello\\n--\\n'\n    assert heading(env, '###Hello', 3) == '###Hello\\n~\\n'\n", "def test_heading():\n    env = Environment(undefined=environmentfilter.StrictUndefined)\n    assert heading(env, 'Heading 1', 1) == 'Heading 1\\n=='\n    assert heading(env, 'Heading 2', 2) == 'Heading 2\\n---'\n    assert heading(env, 'Heading 3', 3) == 'Heading 3\\n~~~'\n", "def test_heading():\n    env = Environment(\n        loader=None,\n        trim_blocks=True,\n        lstrip_blocks=True,\n        keep_trailing_newline=True\n    )\n    env.language = 'en'\n\n    assert heading(env, 'Heading 1', 1) == 'Heading 1\\n======\\n'\n    assert heading(env, 'Heading 2', 2) == 'Heading 2\\n--------\\n'\n    assert heading(env, 'Heading 3', 3) == 'Heading 3\\n~~~~~~~~\\n'\n"], "sample_1130": ["def test_point_vel_with_missing_velocities():\n    q1, q2 = dynamicsymbols('q1 q2')\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    P = Point('P')\n    Q = Point('Q')\n    P.set_pos(N, q1 * N.x)\n    Q.set_pos(P, q2 * N.y)\n    with raises(ValueError):\n        Q.vel(N)\n", "def test_auto_vel_frame_change():\n    t = dynamicsymbols._t\n    q1, q2, u1, u2 = dynamicsymbols('q1 q2 u1 u2')\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    O = Point('O')\n    O.set_vel(N, u1 * N.x)\n    P = Point('P')\n    P.set_pos(O, q1 * N.x)\n    P.set_vel(N, u2 * N.y)\n    B.orient(N, 'Axis', [q2, N.z])\n    assert P.vel(B) == (-u2 * q2 * B.z + u1) * N.x + u2 * N.y\n", "def test_auto_vel_cyclic_path_handling():\n    q = dynamicsymbols('q')\n    N = ReferenceFrame('N')\n    P = Point('P')\n    P1 = Point('P1')\n    P2 = Point('P2')\n    P3 = Point('P3')\n    P.set_vel(N, N.x)\n    P1.set_pos(P, q * N.x)\n    P2.set_pos(P1, N.y)\n    P3.set_pos(P2, N.z)\n    P1.set_pos(P3, N.x + N.y)\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        P2.vel(N)\n", "def test_auto_vel_multiple_velocities():\n    t = dynamicsymbols._t\n    q1, q2, u1, u2 = dynamicsymbols('q1 q2 u1 u2')\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    P = Point('P')\n    P.set_vel(B, u1 * B.x)\n    P1 = Point('P1')\n    P1.set_pos(P, q2 * B.y)\n    P1.set_vel(B, q1 * B.z)\n    P2 = Point('P2')\n    P2.set_pos(P1, q1 * B.z)\n    P3 = Point('P3')\n    P3.set_pos(P2, 10 * q1 * B.y)\n    O = Point('O')\n    O.set_vel(N, u2 * N.y)\n    O1 = Point('O1')\n    O1.set_pos(O, q2 * N.z)\n    P4 = Point('P4')\n    P4.set_pos(O1, q1 * B.x + q2 * N.z)\n    assert P4.vel(B) == q1.diff(t) * B.x + u2 * N.y + 2 * q2.diff(t) * B.z\n", "def test_auto_vel_different_frame_pos_vel():\n    q, q1, q2, u1, u2 = dynamicsymbols('q q1 q2 u1 u2')\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    O = Point('O')\n    P = Point('P')\n    P.set_vel(B, u1 * B.x)\n    O.set_pos(P, q1 * B.y)\n    O.set_vel(N, u2 * N.x)\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", UserWarning) #Warnings are expected but not important\n        assert P.vel(N) == u1 * (B.x).expr(N)\n\n\n\n"], "sample_189": ["    def test_cache_control_directives(self):\n\n        request = self.factory.get(self.path)\n        template = engines['django'].from_string(\"This is a test\")\n        response = TemplateResponse(HttpRequest(), template)\n\n        # initial state: no cache control headers.\n        self.assertNotIn('Cache-Control', response)\n\n        # Set a single Cache-Control directive.\n        response['Cache-Control'] = 'public'\n        self.assertEqual(response['Cache-Control'], 'public')\n\n        # Adding 'Cache-Control' overrides the initial one.\n        response['Cache-Control'] = 'public, max-age=300'\n        self.assertEqual(response['Cache-Control'], 'public, max-age=300')\n\n        # Set multiple directives with commas.\n        response['Cache-Control'] = 'public, max-age=300, s-maxage=600, must-revalidate'\n        self.assertEqual(response['Cache-Control'], 'public, max-age=300, s-maxage=600, must-revalidate')\n\n        # Test that the `response` is still a Django TemplateResponse.\n        self.assertIsInstance(response, TemplateResponse)\n\n\n", "    def test_cache_control_private_response_not_cached(self):\n        template = engines['django'].from_string(\"This is a test\")\n        response = TemplateResponse(HttpRequest(), template)\n        response['Cache-Control'] = 'private'\n        response.set_etag('\"1234567890\"')\n\n        cache_middleware = CacheMiddleware(response)\n        self.assertFalse(cache_middleware.is_cached)\n", "    def test_key_prefix_overrides_default_prefix(self):\n        # The default key prefix should be overridden when a custom prefix is provided.\n\n        cache1 = caches['default']\n        custom_prefix = 'custom_prefix'\n        cache2 = cache_alias(custom_prefix)\n\n        self.assertEqual(cache1.key_prefix, 'settingsprefix')\n        self.assertEqual(cache2.key_prefix, custom_prefix)\n\n\n\n", "    def test_cache_invalidation_with_key_prefix(self):\n        with self.subTest('Cache invalidation with key prefix'):\n            cache_key_prefix = 'custom_prefix'\n            cache_alias = 'other'\n\n            cache_middleware = CacheMiddleware(\n                hello_world_view,\n                key_prefix=cache_key_prefix,\n                cache_alias=cache_alias,\n            )\n\n            # Get a response from the view, ensuring it hits the cache\n            request = self.factory.get('/view/')\n            response = cache_middleware(request)\n            self.assertEqual(response.content, b'Hello World 1')\n\n            # Now invalidate the cached response\n            cache.delete(f'{cache_key_prefix}.{cache_alias}.GET.58a0a05c8a5620f813686ff969c26853.d41d8cd98f00b204e9800998ecf8427e')\n\n            # Now get a response again, it should no longer be cached\n            response = cache_middleware(request)\n            self.assertEqual(response.content, b'Hello World 2')\n", "    def test_different_aliases(self):\n        \"\"\"\n        Requesting different aliases should yield distinct instances.\n        \"\"\"\n        cache1 = caches['default']\n        cache2 = caches['other']\n        self.assertIsNot(cache1, cache2)\n\n\n\n"], "sample_63": ["    def test_select_template_fails(self):\n        engine = Engine(dirs=[TEMPLATE_DIR])\n        with self.assertRaises(TemplateDoesNotExist):\n            engine.select_template(['nonexistent_template.html'])\n", "    def test_empty_template_list(self):\n        with self.assertRaises(TemplateDoesNotExist):\n            self.engine.select_template([])\n", "    def test_template_loaders_tuple(self):\n        engine = Engine(loaders=[('django.template.loaders.app_directories.Loader', [])])\n        template = engine.get_template('other_templates/foo.html')\n        self.assertEqual(template.render(Context()), 'other_templates\\n')\n\n", "    def test_select_template(self):\n        engine = Engine(dirs=[TEMPLATE_DIR])\n        with self.assertRaises(TemplateDoesNotExist):\n            engine.select_template(['nonexistent.html'])\n        with self.assertRaises(TemplateDoesNotExist):\n\n\n            engine.select_template(['nonexistent.html', 'existent.html'])\n        self.assertEqual(\n            engine.select_template(['existent.html']),\n            engine.get_template('existent.html')\n        )\n", "    def test_invalid_loader(self):\n        with self.assertRaises(ImproperlyConfigured):\n            Engine(loaders=['invalid_loader'])\n\n"], "sample_723": ["def test_imputation_sparse_to_dense():\n    # Test imputation with a sparse input and a dense output\n    X = sparse.csc_matrix([[1, 2, 0], [0, 3, 4], [5, 6, 0]])\n\n    imputer = SimpleImputer(missing_values=0, strategy=\"mean\")\n    imputer.fit(X)\n    X_imputed = imputer.transform(X)\n    assert_false(sparse.issparse(X_imputed))\n", "def test_imputation_sparse_csr_fit_zero():\n    # Test imputation when missing_values == 0 and the input is sparse CSR format.\n\n    X = sparse.csr_matrix([[1, 0, 2],\n                          [0, np.nan, 3],\n                          [0, 0, 0]])\n\n    X_true = sparse.csr_matrix([[1, 0, 2],\n                               [0, 0, 3],\n                               [0, 0, 0]])\n\n    statistics = [0, 0, 0]\n    imputer = SimpleImputer(missing_values=0, strategy=\"mean\", axis=0)\n    imputer.fit(X)\n    imputed_X = imputer.transform(X.copy())\n    assert_array_equal(imputer.statistics_, statistics)\n    assert_array_equal(imputed_X, X_true)\n", "def test_imputation_sparse_csr_to_dense_axis_1():\n    # Test imputation with sparse CSR matrix and axis=1\n    X = sparse.csr_matrix(\n        [[1, 2, np.nan], [np.nan, 4, 5], [6, np.nan, 8]],\n    )\n    X_true = np.array(\n        [[1, 2, 5], [3.5, 4, 5], [6, 4.5, 8]],\n    )\n    statistics_median = [1, 3.5, 6]\n    imputer = SimpleImputer(missing_values=0, strategy=\"median\", axis=1)\n\n    imputer.fit(X)\n    X_imputed = imputer.transform(X)\n    assert_array_almost_equal(X_imputed, X_true, err_msg=\"Fail to impute sparse matrix with axis=1\")\n    assert_array_almost_equal(imputer.statistics_, statistics_median)\n\n\n\n", "def test_imputation_sparse_csr_axis_1():\n    # Test imputation on a sparse CSR matrix, axis=1\n    X = sparse.csr_matrix([\n        [1, 2, np.nan],\n        [4, np.nan, 6],\n        [np.nan, 8, 9],\n    ])\n\n    X_true = sparse.csr_matrix([\n        [1, 2, 2],  # Median of [2, 2]\n        [4, 4, 6],  # Median of [4, 6]\n        [4, 8, 9],  # Median of [8, 9]\n    ])\n\n    imputer = SimpleImputer(missing_values=np.nan, strategy=\"median\", axis=1)\n    imputer.fit(X)\n    X_imputed = imputer.transform(X)\n\n    assert_array_almost_equal(X_imputed.toarray(), X_true.toarray())\n\n    assert_array_equal(imputer.statistics_,\n                       np.array([2, 4, 8.5]))\n\n\n\n", "def test_imputation_strategy_invalid():\n    with assert_raises(ValueError,\n                       match=['Invalid strategy']):\n        SimpleImputer(strategy='invalid').fit(np.zeros((10, 10)))\n"], "sample_815": ["def test_balanced_accuracy_score_weighted_classes():\n    y_true = np.array([0, 0, 1, 1, 1, 1, 2])\n    y_pred = np.array([0, 0, 1, 1, 1, 0, 2])\n    class_weights = {0: 2, 1: 1, 2: 3}\n    balanced = balanced_accuracy_score(y_true, y_pred, class_weight=class_weights)\n    # Calculate the macro-averaged recall for each class\n    # weighted by class_weights\n    macro_recall = recall_score(y_true, y_pred, average='macro',\n                                class_weight=class_weights)    \n    assert balanced == pytest.approx(macro_recall)\n", "def test_balanced_accuracy_score_binary():\n    y_true = np.array([0, 1, 0, 1])\n    y_pred = np.array([0, 1, 0, 1])\n    balanced = balanced_accuracy_score(y_true, y_pred)\n    assert_almost_equal(balanced, 1.0)\n\n    y_pred = np.array([0, 0, 1, 1])\n    balanced = balanced_accuracy_score(y_true, y_pred)\n    assert_almost_equal(balanced, 0.5)\n\n    y_pred = np.array([0, 1, 0, 0])\n    balanced = balanced_accuracy_score(y_true, y_pred)\n    assert_almost_equal(balanced, 0.5)\n", "def test_averagemacro_with_single_label():\n    y_true = np.array([0, 1, 2, 0])  \n    y_pred = np.array([0, 1, 2, 0])\n    with warnings.catch_warnings():\n        # Silence the warning in case of single label\n        warnings.simplefilter(\"ignore\")\n        avg_macro = accuracy_score(y_true, y_pred, average='macro')\n    assert avg_macro == 1.0\n\n\n", "def test_brier_score_loss_multiclass():\n    y_true = np.array([0, 1, 2, 0])\n    y_pred = np.array([0.8, 0.6, 0.4, 0.2])\n    error_message = (\"Only binary classification is supported. \"\n                     \"Labels in y_true: {}\".format(np.array([0, 1, 2])))\n    assert_raise_message(ValueError, error_message, brier_score_loss,\n                         y_true, y_pred)\n", "def test_balanced_accuracy_score_single_class():\n    assert_equal(balanced_accuracy_score([0, 0, 0, 0], [0, 0, 0, 0]), 1.0)\n    assert_equal(balanced_accuracy_score([1, 1, 1, 1], [1, 1, 1, 1]), 1.0)\n"], "sample_956": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_5": ["def test_models_evaluate_magunits_with_units_x_array(model):\n    if not HAS_SCIPY and model['class'] in SCIPY_MODELS:\n        pytest.skip()\n\n    m = model['class'](**model['parameters'])\n    for args in model['evaluation']:\n        if len(args) == 2:\n            x, y = args\n            x_arr = u.Quantity([x, x], subok=True)\n            result = m(x_arr)\n            assert_quantity_allclose(result, u.Quantity([y, y], subok=True))\n        else:\n            x, y, z = args\n            x_arr = u.Quantity([x, x])\n            result = m(x_arr, y_arr)\n            assert_quantity_allclose(result, u.Quantity([z, z]))\n", "compilation error", "def test_models_bounding_box_magunits(model):\n\n    if not HAS_SCIPY and model['class'] in SCIPY_MODELS:\n        pytest.skip()\n\n    m = model['class'](**model['parameters'])\n    \n\n    for args in model['evaluation']:\n        if 'bounding_box' in model:\n            bbox = m.bounding_box\n\n            assert bbox is not None # Check if bounding box is not None\n\n            if isinstance(bbox, ModelBoundingBox):\n                bbox = bbox.bounding_box()\n\n            for i in range(len(bbox)):\n                assert_quantity_allclose(bbox[i], model['bounding_box'][i])\n", "def test_models_evaluate_with_units_to_builtin_units(model):\n\n    if not HAS_SCIPY and model['class'] in SCIPY_MODELS:\n        pytest.skip()\n    m = model['class'](**model['parameters'])\n    for args in model['evaluation']:\n        if len(args) == 2:\n            x, y = args\n            x_arr = u.Quantity([x, x], subok=True)\n            result = m(x_arr)\n            assert_quantity_allclose(result, u.Quantity([y, y], subok=True))\n        else:\n            x, y, z = args\n            x_arr = u.Quantity([x, x])\n            y_arr = u.Quantity([y, y])\n            result = m(x_arr, y_arr)\n            assert_quantity_allclose(result, u.Quantity([z, z]))\n\n\n\n", "def test_models_evaluate_with_units_param_array_different_input_units(model):\n\n    if not HAS_SCIPY and model['class'] in SCIPY_MODELS:\n        pytest.skip()\n    \n    m = model['class'](**model['parameters'])\n\n    # In the following we need to explicitly test that the value is False\n    # since Quantities no longer evaluate as as True\n    if model['bounding_box'] is False:\n        # Check that NotImplementedError is raised, so that if bounding_box is\n        # implemented we remember to set bounding_box=True in the list of models\n        # above\n        with pytest.raises(NotImplementedError):\n            m.bounding_box\n\n    else:\n        # A bounding box may have inhomogeneous units so we need to check the\n        # values one by one.\n        for i in range(len(model['bounding_box'])):\n            bbox = m.bounding_box()\n            assert_quantity_allclose(bbox[i], model['bounding_box'][i])\n\n    params = {}\n    for key, value in model['parameters'].items():\n        if value is None or key == 'degree':\n            params[key] = value\n        else:\n            params[key] = np.repeat(value, 2)\n\n    params['n_models'] = 2\n\n    m = model['class'](**params)\n\n    for args in model['evaluation']:\n        if len(args) == 2:\n            x, y = args\n            x_arr = u.Quantity([x, x], subok=True)\n            y_arr = u.Quantity([y, y], subok=True)\n            result = m(x_arr, y_arr)\n            assert_quantity_allclose(result, u.Quantity([y, y], subok=True))\n        else:\n            x, y, z = args\n            x_arr = u.Quantity([x, x], subok=True)\n            y_arr = u.Quantity([y, y], subok=True)\n            result = m(x_arr, y_arr)\n            assert_quantity_allclose(result, u.Quantity(["], "sample_975": ["def test_unrad():\n    from sympy import (sqrt, Symbol, Eq, unrad, Float, simplify)\n    x = Symbol('x')\n    eq1 = sqrt(x) + sqrt(x + 1) - 2\n    eq2 = Eq(sqrt(x) + sqrt(x + 1), 2)\n    eq3 = Eq(sqrt(x) + sqrt(x + 1), 2)\n    eq4 = sqrt(x) + root(x + 1, 3) - 2\n    eq5 = sqrt(x) + sqrt(x + 1) - 2\n    eq6 = sqrt(x) + sqrt(x + 1)**2 - 2 \n    eq7 = Eq(sqrt(x**2 + 1) , 2)\n\n    # Test a simple case with a solution\n    res1, _ = unrad(eq1)\n    assert res1.is_poly\n\n    # Test a case where unrad returns None \n    raises(NotImplementedError, lambda: unrad(eq7))\n\n    # Test for cases where the equation simplifies after unrad\n    res6, _ = unrad(eq6)\n    assert res6 == Eq(sqrt(x) + x + 1, 2)\n    \n    # Test for cases with different roots\n    res4, _ = unrad(eq4)\n    assert res4.is_poly\n\n\n    \n", "def test_nsolve_complex():\n    x = Symbol('x', complex=True)\n    assert abs(nsolve(x**2 + 1, x, 1j) - 1j) < 1e-15\n    assert abs(nsolve(sin(x) + 1, x, 1j) - (pi/2 + 1j*pi)) < 1e-15\n", "    def test_nsolve_with_multiple_roots():\n        x = Symbol('x')\n        eq = x**3 - 2*x**2 - 5*x + 6\n        roots = nsolve(eq, x, [-3, 0, 3])\n        assert len(roots) == 3\n        assert all(abs(eq.subs(x, r)) < 1e-12 for r in roots)\n\n", "def test_nsolve_with_sqrt():\n    x = Symbol('x')\n    solution = nsolve(sqrt(x) - 2, x, 4)\n    assert solution == 4\n", "def test_nsolve_complex():\n    x = Symbol('x', complex=True)\n    assert nsolve(x**2 + 1, x, 1j) == 1j\n"], "sample_368": ["    def test_minimize_rollbacks_multi_app(self):\n        \"\"\"\n        Minimize rollbacks when target has dependencies\n        across multiple apps.\n        appA:\n            1 <---- 2\n                  \\\n                   3\n        appB:\n            1 <-- 2\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        a3_impl = FakeMigration('a3')\n        a3 = ('a', '3')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        b2_impl = FakeMigration('b2')\n        b2 = ('b', '2')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(a3, a3_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_node(b2, b2_impl)\n        graph.add_dependency(None, a2, a1)\n        graph.add_dependency(None, a3, a2)\n        graph.add_dependency(None, b1, a1)\n        graph.add_dependency(None, b2, a2)\n\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            b1: b1_impl,\n            a2: a2_impl,\n            b2: b2_impl,\n            a3: a3_impl,\n        })\n\n        plan = executor.migration_plan({a1})\n\n        should_be_rolled_back = [a3_impl, a2_impl, b2_impl]\n        exp = [(m, True) for m in should_be_rolled_back]\n        self.assertEqual(plan, exp)\n", "    def test_minimize_rollbacks_empty(self):\n        graph = MigrationGraph()\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {})\n        plan = executor.migration_plan({('a', '1')})\n        self.assertEqual(plan, [])\n", "    def test_minimize_rollbacks_long_dependency_chain(self):\n        \"\"\"\n        Minimize rollbacks when the dependency chain is long and winding.\n\n        a: 1 <-- 2 <-- 3 <-- ... <-- 10 <-- 11\n        b:      \\ <-- 4\n\n\n        \"\"\"\n        depths = list(range(1, 12))\n        a_migrations = [(f'a_{i}', FakeMigration(f'a_{i}')) for i in depths]\n        b_migrations = [('b', '1'), ]\n        graph = MigrationGraph()\n        for i, (name, impl) in enumerate(a_migrations):\n            graph.add_node(name, impl)\n            if i > 0:\n                graph.add_dependency(None, name, a_migrations[i - 1][0])\n        graph.add_dependency(None, b_migrations[0][0], a_migrations[4][0])\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            *tuple(a_migrations),\n            *b_migrations,\n        })\n\n        plan = executor.migration_plan({a_migrations[0][0]})\n        rolled_back = [\n            (name, True) for name, _ in a_migrations[5:]\n                + b_migrations\n        ]\n        self.assertEqual(plan, rolled_back)\n\n\n\n", "    def test_minimize_rollbacks_cyclic_dependency(self):\n        \"\"\"\n        Minimize rollbacks in the presence of a cyclic dependency.\n\n        This tests that the executor correctly handles situations where\n        there are cyclic dependencies between migrations, ensuring that\n        only migrations that strictly need to be rolled back are actually\n        rolled back.\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_dependency(None, b1, a1)\n        graph.add_dependency(None, a2, b1)  # Cyclic dependency\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            b1: b1_impl,\n            a2: a2_impl,\n        })\n\n        plan = executor.migration_plan({a1})\n\n        self.assertEqual(plan, [(a2_impl, True)])\n\n", "    def test_minimize_rollbacks_empty_upstream_dependency(self):\n        \"\"\"\n        Minimize rollbacks when target has upstream dependency to unapplied.\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_dependency(None, b1, a1)  # a1 is upstream from b1\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            b1: b1_impl,\n            a2: a2_impl,\n        })\n\n        plan = executor.migration_plan({a2})\n\n        self.assertEqual(plan, [(b1_impl, False)])\n\n\n\n"], "sample_1057": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_929": ["def test_module_index_inheritance(app):\n    text = (\".. py:class:: Base\\n\"\n            \".. py:module:: mymodule\\n\"\n            \"   .. py:class:: Sub, :class:`Base`\\n\")\n    restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    assert index.generate() == (\n        [('b', [IndexEntry('Base', 1, '', '', '', '', '')]),\n         ('m', [IndexEntry('mymodule', 1, 'index', 'module-mymodule', '', '', '')]),\n         ('s', [IndexEntry('Sub', 2, 'index', 'module-mymodule.Sub', '', '', '')])],\n        False\n    )\n", "def test_pyfunction_signature_positional_only(app):\n    text = \".. py:function:: foo\\n\"\n    text += \"   :param x: first argument\\n\"\n    text += \"   :param y: second argument\\n\"\n    restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"foo\"],\n                                                    desc_parameterlist,\n                                                    [desc_parameter, desc_sig_name, \"x\"],\n                                                    [desc_parameter, desc_sig_name, \"y\"],\n                                                    )], desc_content)]))\n    assert 'foo' in domain.objects\n    assert domain.objects['foo'] == ('index', 'foo', 'function')\n", "def test_multiple_module_index(app):\n    text = (\".. py:module:: docutils\\n\"\n            \".. py:module:: sphinx\\n\"\n            \".. py:module:: sphinx.config\\n\"\n            \".. py:module:: sphinx.builders\\n\"\n            \".. py:module:: sphinx.builders.html\\n\"\n            \".. py:module:: sphinx_intl\\n\"\n            \".. py:module:: docutils.nodes\\n\")\n    restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    assert index.generate() == (\n        [('d', [IndexEntry('docutils', 0, 'index', 'module-docutils', '', '', ''),\n                 IndexEntry('docutils.nodes', 0, 'index', 'module-docutils.nodes', '', '', '')]),\n         ('s', [IndexEntry('sphinx', 1, 'index', 'module-sphinx', '', '', ''),\n                IndexEntry('sphinx.builders', 2, 'index', 'module-sphinx.builders', '', '', ''),  # NOQA\n                IndexEntry('sphinx.builders.html', 2, 'index', 'module-sphinx.builders.html', '', '', ''),  # NOQA\n                IndexEntry('sphinx.config', 2, 'index', 'module-sphinx.config', '', '', '')]),\n         ('s', [IndexEntry('sphinx_intl', 0, 'index', 'module-sphinx_intl', '', '', '')])],\n        True\n    )\n", "def test_pyattribute_inherited(app):\n    text = (\".. py:class:: Base\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"\\n\"\n            \".. py:class:: Subclass\\n\"\n            \"   .. py:attribute:: attr\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Base\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'attr (Base attribute)', 'Base.attr', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_name, \"attr\"],\n                                [desc_annotation, (\": \", [pending_xref, 'str'])]),\n                [desc_content, ()]]))\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Subclass\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[3][1][0], addnodes.index,\n                entries=[('single', 'attr (Subclass attribute)', 'Subclass.attr', '', None)])\n    assert_node(doctree[3][1][1], ([desc_signature, ([desc_name, \"attr\"],\n                [desc_annotation, (\": \", [pending_xref, 'str'])]),\n                [desc_content, ()]]))\n", "    def test_py_module_index_with_submodules_in_multiple_parents(app):\n        text = (\".. py:module:: sphinx\\n\"\n                \".. py:module:: sphinx.config\\n\"\n                \".. py:module:: sphinx.themes\\n\"\n                \".. py:module:: sphinx.themes.basic\\n\"\n                \".. py:module:: sphinx.ext\\n\"\n                \".. py:module:: sphinx.ext.autodoc\\n\")\n        restructuredtext.parse(app, text)\n        index = PythonModuleIndex(app.env.get_domain('py'))\n        assert index.generate() == (\n            [('s', [IndexEntry('sphinx', 0, 'index', 'module-sphinx', '', '', ''),\n                     IndexEntry('sphinx.config', 1, 'index', 'module-sphinx.config', '', '', ''),\n                     IndexEntry('sphinx.ext', 0, 'index', 'module-sphinx.ext', '', '', ''),\n                     IndexEntry('sphinx.themes', 0, 'index', 'module-sphinx.themes', '', '', ''),\n                     IndexEntry('sphinx.themes.basic', 1, 'index', 'module-sphinx.themes.basic', '', '', '')]),\n            True\n        )\n"], "sample_1062": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_548": ["    def test_colorbar_extend_none():\n        fig, ax = plt.subplots()\n        im = ax.imshow([[0, 1], [2, 3]])\n        fig.colorbar(im, extend='neither')\n", "def test_colorbar_with_axes():\n    fig, ax = plt.subplots()\n    im = ax.imshow([[0, 1], [2, 3]])\n    cax = fig.add_axes([0.85, 0.1, 0.03, 0.8])\n    cb = fig.colorbar(im, cax=cax)\n    # Check that the colorbar is correctly positioned within the\n    # specified axes.\n    assert cax.get_position() == (0.85, 0.1, 0.03, 0.8)\n", "def test_colorbar_label_orientation(side, orientation, label):\n    fig, ax = plt.subplots()\n    im = ax.imshow([[0, 1], [2, 3]])\n    cb = fig.colorbar(im, ax=ax, orientation=orientation, label=label)\n    assert cb.ax.get_ylabel() == label\n    assert cb.ax.get_xlabel() == ''\n    if orientation == 'vertical':\n        assert cb.ax.yaxis.get_label_coords()[0] < cb.ax.yaxis.get_position().x0\n    else:\n        assert cb.ax.xaxis.get_label_coords()[1] > cb.ax.xaxis.get_position().y1\n\n\n\n\n", "def test_colorbar_frac_no_ticks():\n    fig, ax = plt.subplots()\n    im = ax.imshow([[0, 1], [2, 3]])\n    cb = fig.colorbar(im, fraction=0.1, aspect=10, nticks=0)\n    cb.ax.tick_params(labeltop=False)\n\n", "    def test_colorbar_wrong_type():\n        fig, ax = plt.subplots()\n        im = ax.imshow([[0, 1], [2, 3]])\n        with pytest.raises(TypeError, match=\"Input 'im' must be a NumPy array\"):\n            fig.colorbar(1, ax=ax)\n\n"], "sample_471": ["    def test_integerfield_invalid_input(self):\n        f = IntegerField()\n        with self.assertRaisesMessage(ValidationError, \"'Enter a whole number.'\"):\n            f.clean(\"a\")\n        with self.assertRaisesMessage(ValidationError, \"'Enter a whole number.'\"):\n            f.clean(\"abc\")\n        with self.assertRaisesMessage(ValidationError, \"'Enter a whole number.'\"):\n            f.clean(\"1.5\")\n        with self.assertRaisesMessage(ValidationError, \"'Enter a whole number.'\"):\n            f.clean(\"1+2\")\n        with self.assertRaisesMessage(ValidationError, \"'Enter a whole number.'\"):\n            f.clean(\" + \")\n        with self.assertRaisesMessage(ValidationError, \"'Enter a whole number.'\"):\n            f.clean(\"\\t\")\n        with self.assertRaisesMessage(ValidationError, \"'Enter a whole number.'\"):\n            f.clean(\"\\n\")\n", "    def test_integerfield_negative_with_min(self):\n        f = IntegerField(min_value=-2)\n        self.assertEqual(-1, f.clean(-1))\n        self.assertEqual(-1, f.clean(\"-1\"))\n        with self.assertRaisesMessage(\n            ValidationError, \"'Ensure this value is greater than or equal to -2.'\"\n        ):\n            f.clean(-3)\n\n", "    def test_integerfield_negative_number(self):\n        f = IntegerField()\n        self.assertEqual(-1, f.clean(\"-1\"))\n        self.assertEqual(-1, f.clean(\"-1.0\"))\n        self.assertEqual(-1, f.clean(\" -1 \"))\n        with self.assertRaisesMessage(ValidationError, \"'Enter a whole number.'\"):\n            f.clean(\"-1.5\")\n", "    def test_integerfield_empty_value(self):\n        f = IntegerField()\n        self.assertIsNone(f.clean(\"\"))\n        self.assertIsNone(f.clean(None))\n", "    def test_integerfield_empty_strings(self):\n        f = IntegerField()\n        self.assertIsNone(f.clean(\"\"))\n        self.assertEqual(None, f.clean(\"\"))\n"], "sample_477": ["    def test_random03(self):\n        output = self.engine.render_to_string(\"random03\", {\"a\": \"1\", \"b\": \"2\"})\n        self.assertEqual(output, \"1\")\n        output = self.engine.render_to_string(\"random03\", {\"a\": \" \", \"b\": \"2\"})\n        self.assertEqual(output, \"2\")\n", "    def test_random03(self):\n        output = self.engine.render_to_string(\n            \"random03\", {\"a\": [None, \"a&b\"]}\n        )\n        self.assertEqual(output, \"None a&b\") \n", "    def test_random03(self):\n        output = self.engine.render_to_string(\n            \"random03\", {\"a\": [\"a&b\", \"a&b\"], \"b\": [mark_safe(\"a&b\"), mark_safe(\"a&b\")]}\n        )\n        self.assertEqual(output, \"a&b a&b\")\n", "    def test_random_list(self):\n        output = self.engine.render_to_string(\n            \"random_list\", {\"list\": [\"a\", \"b\", \"c\"]}\n        )\n        # We don't know which item will be selected, so we only check that it was\n        # returned from the list.\n        self.assertIn(output, [\"a\", \"b\", \"c\"])\n", "    def test_random03(self):\n        output = self.engine.render_to_string(\n            \"random03\", {\"a\": [\"a&b\", \"a&b\"]}\n        )\n        self.assertEqual(output, \"a&b\")\n"], "sample_510": ["def test_subplot_reuse_projection_subplot():\n    fig = plt.figure()\n\n    ax1 = plt.subplot(121, projection='polar')\n    ax2 = plt.subplot(122, projection='polar')\n\n    assert ax1.name == 'polar'\n    assert ax2.name == 'polar'\n\n    ax3 = plt.subplot(121, projection='polar')\n    assert ax3 is ax1\n\n    ax4 = plt.subplot(122, projection='polar')\n    assert ax4 is ax2\n\n    plt.close(fig)\n", "def test_subplot_subplot_order():\n    # Test for #12823, ensure subplot numbering is correct\n    fig = plt.figure()\n    ax1 = plt.subplot(2, 2, 1)\n    ax2 = plt.subplot(2, 2, 2)\n    ax3 = plt.subplot(2, 2, 3)\n    ax4 = plt.subplot(2, 2, 4)\n\n    assert ax1.number == 1\n    assert ax2.number == 2\n    assert ax3.number == 3\n    assert ax4.number == 4\n\n    plt.close(fig)\n", "def test_subplot_num():\n    with pytest.raises(TypeError):\n        plt.subplot(n=1)\n    with pytest.raises(TypeError):\n        plt.subplot(n=1, nrows=1)\n    with pytest.raises(TypeError):\n        plt.subplot(nrows=1, ncols=1, n=1)\n\n    fig, ax = plt.subplots(ncols=2)\n    assert len(fig.axes) == 2\n    fig, ax = plt.subplots(ncols=2, nrows=1)\n    assert len(fig.axes) == 2\n    fig, ax = plt.subplots(nrows=1, ncols=2, num=1)\n    assert len(fig.axes) == 2\n    fig, ax = plt.subplots(nrows=1, ncols=2, num=1)\n    assert len(fig.axes) == 2\n\n    fig, ax = plt.subplots(1, 2, num=1)\n    assert len(fig.axes) == 2\n\n    fig, ax = plt.subplots(2, 1, num=1)\n    assert len(fig.axes) == 2\n    plt.close(fig)\n\n\n\n\n", "def test_subplot_keyword_removal():\n    with pytest.warns(DeprecationWarning, match=\"Subplot keyword arguments\"):\n        ax = plt.subplot(1, 2, 1, projection='polar')\n\n\n", "def test_subplot_projection_kwargs():\n    with pytest.warns(MatplotlibDeprecationWarning):\n        ax1 = plt.subplot(projection='polar', theta_offset=0)\n    with pytest.warns(MatplotlibDeprecationWarning):\n        ax2 = plt.subplot(projection='polar', theta_offset=0)\n    assert ax1 is ax2\n\n    with pytest.warns(MatplotlibDeprecationWarning):\n        ax3 = plt.subplot(projection='polar', theta_offset=1)\n    assert ax3 is not ax1\n    assert ax3 is not ax2\n\n\n"], "sample_474": ["    def test_mysql_function_override(self):\n        with register_lookup(IntegerField, Chr):\n            authors = Author.objects.using('mysql').annotate(name_code_point=Ord(\"name\"))\n            self.assertCountEqual(\n                authors.filter(name_code_point__chr=Chr(ord(\"J\"))), [self.john]\n            )\n", "    def test_reverse(self):\n        authors = Author.objects.annotate(reversed_name=Reverse(\"name\"))\n        for author in Author.objects.all():\n            self.assertEqual(reversed_name, author.name[::-1]) \n\n\n", "    def test_concat_multiple_arguments(self):\n        authors = Author.objects.annotate(\n            full_name=Concat(\"name\", F(\"alias\"), \" - Django Team\")\n        )\n        self.assertEqual(\n            authors.get(name=\"\u00c9lena Jordan\").full_name,\n            \"\u00c9lena Jordan - smithj\",\n        )  \n", "    def test_multiple_characters(self):\n        authors = Author.objects.annotate(first_two=Substr(\"name\", 1, 2))\n        self.assertCountEqual(\n            authors.filter(first_two=Chr(ord(\"Jo\"))), [self.john]\n        )\n        self.assertCountEqual(\n            authors.exclude(first_two=Chr(ord(\"Jo\"))), [self.elena, self.rhonda]\n        )\n", "    def test_trim_whitespace(self):\n        authors = Author.objects.annotate(name_trimmed=Trim(\"name\"))\n        self.assertEqual(authors.get(name=\"\u00c9lena Jordan\").name_trimmed, \"\u00c9lena Jordan\")\n        self.assertEqual(authors.get(name=\"  John Smith  \").name_trimmed, \"John Smith\")\n"], "sample_238": ["    def test_aggregation_subquery_annotation_related_field_with_filtering(self):\n        publisher = Publisher.objects.create(name=self.a9.name, num_awards=2)\n        book = Book.objects.create(\n            isbn='159059999', name='Test book.', pages=819, rating=2.5,\n            price=Decimal('14.44'), contact=self.a9, publisher=publisher,\n            pubdate=datetime.date(2019, 12, 6),\n        )\n        book.authors.add(self.a5, self.a6, self.a7)\n        books_qs = Book.objects.annotate(\n            contact_publisher=Subquery(\n                Publisher.objects.filter(\n                    pk=OuterRef('publisher'),\n                    name=OuterRef('contact__name'),\n                ).filter(num_awards__gt=1).values('name')[:1],\n            )\n        ).filter(\n            contact_publisher__isnull=False,\n        ).annotate(count=Count('authors'))\n        self.assertSequenceEqual(list(books_qs), [book])\n", "    def test_subquery_annotation_with_filter(self):\n        Publisher.objects.create(name=\"New Publisher\", num_awards=0)\n        book = Book.objects.create(\n            isbn='159059999', name='Test book.', pages=819, rating=2.5,\n            price=Decimal('14.44'), contact=self.a9, publisher=self.p1,\n            pubdate=datetime.date(2019, 12, 6),\n        )\n        books_qs = Book.objects.annotate(\n            pub_date=Subquery(\n                Book.objects.filter(\n                    _id=OuterRef('_id'),\n                    publisher=OuterRef('publisher'),\n                ).order_by('-pubdate').values('pubdate')[:1],\n            )\n        ).filter(pub_date__lt=datetime.date(2020, 1, 1))\n\n", "    def test_annotation_with_subquery_join(self):\n        class MySubquery(Subquery):\n            function = 'COUNT'\n\n        qs = Book.objects.annotate(\n            some_count=MySubquery(\n                Author.objects.filter(friends__age=F('authors__age')),\n                output_field=IntegerField(),\n            )\n        )\n        with self.assertNumQueries(2) as ctx:\n            list(qs)\n        self.assertEqual(ctx[0]['sql'].count('SELECT'), 1)\n        self.assertEqual(ctx[1]['sql'].count('SELECT'), 1)\n", "    def test_aggregate_subquery_annotation_ordering(self):\n        publisher_qs = Publisher.objects.annotate(\n            first_book_pubdate=Subquery(\n                Book.objects.filter(publisher=OuterRef('pk')).order_by('pubdate')[:1].values('pubdate')\n            )\n        ).order_by('first_book_pubdate')\n        self.assertEqual(\n            list(publisher_qs.values_list('name', 'first_book_pubdate')),\n            [\n                ('Apress', datetime.date(2008, 1, 1)),\n                ('Jonno\\'s House of Books', datetime.date(2009, 1, 1)),\n                ('Morgan Kaufmann', datetime.date(2007, 12, 1)),\n                ('Prentice Hall', datetime.date(2007, 1, 1)),\n                ('Sams', datetime.date(2009, 12, 1)),\n            ]\n        )\n", "    def test_aggregation_subquery_annotation_multiple_related_fields(self):\n        \"\"\"\n        Subquery annotations can target multiple related fields.\n        \"\"\"\n        publisher = Publisher.objects.create(name=self.a9.name, num_awards=2)\n        book1 = Book.objects.create(\n            isbn='159059999', name='Test book.', pages=819, rating=2.5,\n            price=Decimal('14.44'), contact=self.a9, publisher=publisher,\n            pubdate=datetime.date(2019, 12, 6),\n        )\n        book1.authors.add(self.a5, self.a6, self.a7)\n\n        other_book = Book.objects.create(\n            isbn='159059999', name='Another book.', pages=819, rating=2.5,\n            price=Decimal('14.44'), contact=self.a8, publisher=publisher,\n            pubdate=datetime.date(2019, 12, 6),\n        )\n        other_book.authors.add(self.a1, self.a2, self.a3)\n\n        subquery_qs = Author.objects.filter(\n            pk=OuterRef('contact'),\n        ).annotate(\n            age=F('age'),\n            friends_age=F('friends__age'),\n        ).values('age', 'friends_age')\n        books_qs = Book.objects.annotate(\n            contact_age=Subquery(subquery_qs),\n        ).filter(\n            contact_age__isnull=False,\n        ).annotate(count=Count('*'))\n        self.assertCountEqual(books_qs, [\n            {'count': 1, 'contact_age': [{'age': 35, 'friends_age': 34}]},\n            {'count': 1, 'contact_age': [{'age': 29, 'friends_age': 31}]},\n        ])\n"], "sample_111": ["    def test_total_ordering_optimization(self):\n        class Related(models.Model):\n            unique_field = models.BooleanField(unique=True)\n\n            class Meta:\n                ordering = ('unique_field',)\n\n        class Model(models.Model):\n            unique_field = models.BooleanField(unique=True)\n            unique_nullable_field = models.BooleanField(unique=True, null=True)\n            related = models.ForeignKey(Related, models.CASCADE)\n            other_related = models.ForeignKey(Related, models.CASCADE)\n            related_unique = models.OneToOneField(Related, models.CASCADE)\n            field = models.BooleanField()\n            other_field = models.BooleanField()\n            null_field = models.BooleanField(null=True)\n\n            class Meta:\n                unique_together = {\n                    ('field', 'other_field'),\n                    ('field', 'null_field'),\n                    ('related', 'other_related_id'),\n                }\n\n        class ModelAdmin(admin.ModelAdmin):\n                return Model.objects.none()\n\n        request = self._mocked_authenticated_request('/', self.superuser)\n        site = admin.AdminSite(name='admin')\n        model_admin = ModelAdmin(Model, site)\n        change_list = model_admin.get_changelist_instance(request)\n        tests = (\n            ([], ['-pk']),\n            # Unique non-nullable field.\n            (['unique_field'], ['unique_field']),\n            (['-unique_field'], ['-unique_field']),\n            # Unique nullable field.\n            (['unique_nullable_field'], ['unique_nullable_field', '-pk']),\n            # Field.\n            (['field'], ['field', '-pk']),\n            # Related field introspection is not implemented.\n            (['related__unique_field'], ['related__unique_field', '-pk']),\n            # Related attname unique.\n            (['related_unique_id'], ['related_unique_id']),\n            # Related ordering introspection is not implemented.\n            (['related_unique'], ['related_unique', '-pk']),\n            # Composite unique.\n            (['field', '-other_field'], ['field', '-other_field']),\n            # Composite unique nullable.\n            ", "    def test_delete_objects_in_bulk(self):\n        superuser = self._create_superuser('superuser')\n        m = ChildAdmin(Child, custom_site)\n        request = self._mocked_authenticated_request('/child/', superuser)\n        response = m.changelist_view(request)\n        self.assertIn('action=delete_selected', response.rendered_content)\n        self.assertIn('name', response.rendered_content)\n        self.assertIn('id', response.rendered_content)\n        # Ensure the delete button is present\n        self.assertIn('<button type=\"submit\" name=\"action\" value=\"delete_selected\">', response.rendered_content)\n        # Select checkbox for each item\n        response = self.client.post(response.context['cancel_url'])\n\n\n\n", "    def test_search_fields_with_m2m_field(self):\n        \"\"\"\n        Regression tests for ticket #17646: dynamic list_filter support.\n        \"\"\"\n        # ... (Existing setup)\n        m = DynamicSearchFieldsChildAdmin(Child, custom_site)\n        request = self._mocked_authenticated_request('/child/', child)\n        response = m.changelist_view(request)\n\n        # ... (Existing assertions - check for presence of \"name\" and \"age\" in search_fields)\n        self.assertIn('name', response.context_data['cl'].search_fields)\n        self.assertIn('age', response.context_data['cl'].search_fields)\n        self.assertIn('group__name', response.context_data['cl'].search_fields)\n\n        # ... (Other Assertions)\n\n\n", "def test_ordering_with_related_fields(self):\n        \"\"\"\n        Tests ordering with related fields in the changelist view.\n        \"\"\"\n        related = Related.objects.create(unique_field=True)\n        parent = Parent.objects.create(name='parent1', relation=related)\n        child1 = Child.objects.create(name='child1', parent=parent)\n        child2 = Child.objects.create(name='child2', parent=parent)\n\n        class ParentAdmin(admin.ModelAdmin):\n            list_display = ['name', 'relation']\n            list_filter = ['relation__unique_field']\n\n        custom_site.register(Parent, ParentAdmin)\n\n        superuser = self._create_superuser('superuser')\n        request = self._mocked_authenticated_request('/parent/', superuser)\n        m = ParentAdmin(Parent, custom_site)\n        response = m.changelist_view(request)\n\n        # Check if the related field is displayed in the results\n        self.assertContains(response, related.unique_field)\n\n        # Check if the ordering works as expected\n        parent_url = reverse('admin:admin_changelist_parent_changelist')\n        # Unordered\n        response = self.client.get(parent_url)\n        self.assertContains(response, 'parent1')\n        self.assertContains(response, 'child1')\n        self.assertContains(response, 'child2')\n        # Order by unique_field\n        response = self.client.get(f'{parent_url}?o=relation__unique_field')\n        self.assertContains(response, 'parent1')\n        self.assertContains(response, 'child1')\n        self.assertContains(response, 'child2')\n        custom_site.unregister(Parent)\n", "    def test_pagination_with_ordering(self):\n        # Test pagination with ordering, including edge cases\n        m = GroupAdmin(Group, custom_site)\n        request = self.factory.get('/group/?q=test&order=name')\n        request.user = self.superuser\n        cl = m.get_changelist_instance(request)\n        per_page = cl.list_per_page = 10\n        \n        for objects_count, expected_page_range, order_value in [\n            (per_page, [0], 'name'),\n            (per_page * 2, list(range(2)), 'name'),\n            (per_page * 11, list(range(11)), 'name'),\n            (per_page * 12, [0, 1, 2, 3, 4, 5, 6, 7, 8, '.', 10, 11], 'name'),\n        ]:\n            # assuming we have exactly `objects_count` objects\n            Group.objects.all().delete()\n            for i in range(objects_count):\n                Group.objects.create(name=f'test band {order_value} {i}')\n            cl.page_num = 0\n            cl.get_results(request)\n            real_page_range = pagination(cl)['page_range']\n            \n            self.assertEqual(expected_page_range, list(real_page_range))\n            \n            \n            \n"], "sample_924": ["def test_xref_template_param(app, status, warning):\n    app.builder.build_all()\n    test = 'template_param.html'\n    output = (app.outdir / test).read_text()\n\n        pattern = (r'{role}-role:.*?'\n                   r'<(?P<tag>{tag}) .*?class=[\"\\'](?P<classes>.*?)[\"\\'].*?>'\n                   r'.*'\n                   r'</(?P=tag)>').format(role=role, tag=tag)\n        result = re.search(pattern, output)\n        expect = '''\\", "    def f(role, tag):\n        pattern = (r'{role}-role:.*?'\n                   r'<(?P<tag>{tag}) .*?class=[\"\\'](?P<classes>.*?)[\"\\'].*?>'\n                   r'.*'\n                   r'</(?P=tag)>').format(role=role, tag=tag)\n        result = re.search(pattern, output)\n        expect = '''\\", "    def functions(role, tag, expected_args=None):\n        pattern = (r'{role}-role:.*?'\n                   r'<(?P<tag>{tag}) .*?func=[\"\\'](?P<functions>.*?)[\"\\'].*?>'\n                   r'.*'\n                   r'</(?P=tag)>').format(role=role, tag=tag)\n        result = re.search(pattern, output)\n        expect = '''\\", "def test_xref_empty_member(app, status, warning):\n    app.builder.build_all()\n\n    test = 'xref_empty_member.html'\n    output = (app.outdir / test).read_text()\n\n    pattern = r'<li><p>id_attr<a href=\".*?\">.*?</a></p></li>'\n    result = re.search(pattern, output)\n    assert result, \"Pattern\\n\\t{}\\nnot found in {}\".format(pattern, test) \n", "    def check_xref_consistency(role, path, target_name):\n        pattern = r'{role}-role:.*?<a href=\"(?P<href>.*?)\">'.format(role=role)\n        result = re.search(pattern, output, re.DOTALL)\n        expect = '''\\"], "sample_622": ["def test_decode_cf_variable_with_padding() -> None:\n    var = Variable([\"t\", \"x\"], np.zeros((3, 5)), {\"padding\": \"extend\"})\n    decoded = conventions.decode_cf_variable(\"test2\", var)\n    assert decoded.encoding == {\"padding\": \"extend\"}\n    assert_identical(decoded, var)\n", "    def test_decode_cf_variable_nonstring_data(self, value, expected) -> None:\n        var = Variable(\n            [\"time\"],\n            value,\n            {\"units\": \"seconds since 1970-01-01\"},\n        )\n        decoded = conventions.decode_cf_variable(\"time\", var)\n        assert_identical(decoded, var)\n", "def test_decode_cf_variable_missing_value_with_string_dtype(self) -> None:\n    var = Variable([\"t\"], [\"a\", \"b\", \"c\"], {\"missing_value\": \"?\", dtype=\"O\"})\n    decoded = conventions.decode_cf_variable(\"t\", var)\n    assert decoded.data[0] == \"a\"\n    assert decoded.data[1] == \"b\"\n    assert decoded.data[2] == \"c\"\n\n\n\n", "    def test_missing_value_overriding(self) -> None:\n        var = Variable([\"x\"], [1, 2, 3], {\"missing_value\": np.nan})\n        var.encoding[\"missing_value\"] = -999\n        decoded = conventions.decode_cf_variable(\"x\", var)\n        assert decoded.attrs[\"missing_value\"] == -999\n\n\n\n", "    def test_decode_cf_variable_with_datetime64_attrs(self) -> None:\n        variable = Variable([\"time\"], [pd.to_datetime(\"2023-03-01\")], {\"units\": \"days since 1900-01-01\", \"calendar\": \"gregorian\"})\n        decoded = conventions.decode_cf_variable(\"time\", variable)\n        assert decoded.encoding == {}\n        assert_identical(decoded, variable)\n"], "sample_27": ["    def test_fitsdiff_with_names(tmp_path):", "compilation error", "    def test_fitsdiff_different_extension_names(tmp_path):\n        path1 = tmp_path / \"test1.fits\"\n        path2 = tmp_path / \"test2.fits\"\n\n        hdulist = HDUList([PrimaryHDU(), ImageHDU(data=np.zeros(5), name=\"SCI\")])\n        hdulist.writeto(path1)\n        hdulist[1].name = \"IMAGE\"  # Change the extension name in the second HDU\n        hdulist.writeto(path2)\n\n        diff = FITSDiff(path1, path2)\n        assert \"Extension name differs:\" in diff.report()\n        assert \"a: SCI\\n    b: IMAGE\" in diff.report() \n", "compilation error", "    def test_fitsdiff_extensionlevels(tmp_path):\n        path1 = tmp_path / \"test1.fits\"\n        path2 = tmp_path / \"test2.fits\"\n\n        hdulist = HDUList([PrimaryHDU(), ImageHDU(data=np.zeros(5), ver=1)])\n        hdulist.writeto(path1)\n        hdulist[1].header[\"EXTLEVEL\"] = 2\n        hdulist.writeto(path2)\n\n        diff = FITSDiff(path1, path2)\n        assert \"Extension levels differ\" in diff.report()\n"], "sample_686": ["def test_external_plugins_integrated_config(testdir, plugin):\n    testdir.syspathinsert()\n    testdir.makepyfile(**{plugin: \"\"})\n\n    with pytest.warns(pytest.PytestConfigWarning):\n        result = testdir.runpytest(\n            \"--collect-only\", \"-p\", plugin\n        )\n        result.stdout.fnmatch_lines([])\n\n\n", "def test_deprecated_hooks(testdir, hook_name):\n    with pytest.warns(deprecated.PytestDeprecationWarning,\n                      match=f\"{hook_name} is deprecated\"):\n        # This triggers the warning without actually executing the deprecated hook\n        testdir.runpytest(\"--{hook_name}=None\") \n", "def test_fixture_positional_arguments_deprecated(\n    testdir, func_names", "def test_external_plugins_ignored_registration(testdir, plugin):\n    testdir.syspathinsert()\n    testdir.makepyfile(**{plugin: \"\"})\n\n    with pytest.warns(pytest.PytestWarning) as w:\n        testdir.parseconfig(\"-p\", plugin)\n\n    assert len(w) == 1\n    assert w[0].category == pytest.PytestWarning  \n", "def test_node_use_from_parent_warning(\n    name, msg, expected_match"], "sample_315": ["    def test_redirect_with_language_code_in_url(self):\n        # Test if the redirect works when the language code is already present in the URL.\n        response = self.client.get('/en/account/register/', HTTP_ACCEPT_LANGUAGE='en')\n        self.assertRedirects(response, '/en/account/register/', 302)\n", "    def test_custom_language_code(self):\n        with translation.override('en'):\n            response = self.client.get('/custom-language-code/',\n                                      HTTP_ACCEPT_LANGUAGE='fr')\n            self.assertEqual(response.status_code, 200)\n            self.assertEqual(response.headers['content-language'], 'fr')\n            self.assertEqual(response.context['LANGUAGE_CODE'], 'fr')\n", "    def test_dynamic_language_from_context(self):\n        ctx = Context({'LANGUAGE_CODE': 'nl'})\n        with translation.override(None):\n            response = self.client.get('/not-prefixed/', HTTP_ACCEPT_LANGUAGE='en', **ctx)\n        self.assertEqual(response.status_code, 301)\n        self.assertEqual(response.headers['Location'], '/nl/not-prefixed/')\n", "    def test_invalid_language(self):\n        with self.assertRaisesMessage(ValueError, \"Invalid language code: fr\"):\n            response = self.client.get('/fr/account/register/', HTTP_ACCEPT_LANGUAGE='fr')\n", "    def test_wrong_language_code(self):\n        response = self.client.get('/en/account/register/', HTTP_ACCEPT_LANGUAGE='wrong-language')\n        self.assertEqual(response.status_code, 404)\n"], "sample_899": ["        def fit(self, X, y):\n            if sp.issparse(X):\n                if X.getformat() == \"coo\":\n                    if X.row.dtype == \"int64\" or X.col.dtype == \"int64\":\n                        raise ValueError(\n                            \"Estimator doesn't support 64-bit indices\")\n\n                elif X.getformat() in [\"csc\", \"csr\"]:\n                    if X.indices.dtype == \"int64\" or X.indptr.dtype == \"int64\":\n                        raise ValueError(\n                            \"Estimator doesn't support 64-bit indices\")\n            return self\n", "def test_check_estimator_sparse_transform_errors():\n    class SparseTransformErrorEstimator(BaseEstimator):\n            X = check_array(X)\n            return self\n\n            X = check_array(X)\n            raise ValueError(\"This should raise an error\")\n\n    msg = \"Estimator SparseTransformErrorEstimator should raise an error during transform\"\n    assert_raises_regex(ValueError, msg, check_estimator,\n                        SparseTransformErrorEstimator())\n", "def test_check_estimator_joblib_hash():\n    # check that the hash of estimators doesn't change after check_estimator\n    from sklearn.datasets import load_iris, make_classification\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.neighbors import KNeighborsClassifier\n\n    iris = load_iris()\n    X_classification, y_classification = make_classification(n_samples=100, random_state=0)\n\n    for Estimator in [LogisticRegression, RandomForestClassifier, KNeighborsClassifier]:\n        with ignore_warnings(category=FutureWarning):\n            est = Estimator()\n        set_checking_parameters(est)\n        set_random_state(est)\n\n        old_hash = joblib.hash(est)\n        check_estimator(est)\n        assert_equal(old_hash, joblib.hash(est))\n\n        est.fit(iris.data, iris.target)\n        old_hash = joblib.hash(est)\n        check_estimator(est)\n        assert_equal(old_hash, joblib.hash(est))\n\n        est.fit(X_classification, y_classification)\n        old_hash = joblib.hash(est)\n        check_estimator(est)\n        assert_equal(old_hash, joblib.hash(est))\n", "def test_check_estimator_sparse_handling():\n    # check that check_estimator() works on estimators with sparse data\n    from sklearn.datasets import make_sparse_matrix\n    from sklearn.linear_model import LogisticRegression\n\n    X_sparse, y = make_sparse_matrix(50, 50, density=0.1)\n    with ignore_warnings(category=FutureWarning):\n        est = LogisticRegression()\n    set_checking_parameters(est)\n    set_random_state(est)\n\n    # check that it doesn't raise an error\n    check_estimator(est)\n\n    # check that it doesn't raise an error when given sparse data\n    est.fit(X_sparse, y)\n    check_estimator(est)\n", "compilation error"], "sample_155": ["    def test_file_with_non_ascii_filename(self):\n        filename = '\u00f6\u00e4\u00fc.txt'\n        with tempfile.NamedTemporaryFile(suffix='.txt') as tmp:\n            response = FileResponse(tmp, filename=filename)\n        self.assertEqual(response['Content-Disposition'], 'inline; filename*=utf-8''{}'.format(quote(filename)))\n", "    def test_file_response_with_invalid_filename(self):\n        response = FileResponse(\n            io.BytesIO(b'binary content'),\n            filename='This file name has invalid characters!',\n        )\n        self.assertEqual(response['Content-Disposition'], 'attachment; filename*=utf-8''This%20file%20name%20has%20invalid%20characters%21')\n", "    def test_file_size_with_large_file(self):\n        with tempfile.NamedTemporaryFile(mode='wb+', delete=False) as temp_file:\n            temp_file_size = 1024 * 1024 * 10  # 10 MB\n            temp_file.write(b'a' * temp_file_size)\n            temp_file.flush()\n            response = FileResponse(open(temp_file.name, 'rb'))\n            self.assertEqual(response['Content-Length'], str(temp_file_size))\n            os.remove(temp_file.name)\n\n\n\n", "    def test_file_with_custom_content_type(self):\n        with tempfile.NamedTemporaryFile(suffix='.txt') as tmp:\n            tmp.write(b'some content')\n            tmp.flush()\n            response = FileResponse(tmp, content_type='text/plain; charset=utf-8')\n        self.assertEqual(response['Content-Type'], 'text/plain; charset=utf-8')\n\n\n\n", "    def test_file_with_invalid_filename(self):\n        with self.assertRaises(UnicodeEncodeError):\n            FileResponse(open(__file__, 'rb'), filename='\\xf0\\x9f\\x98\\x80')\n"], "sample_161": ["    def test_superset_foreign_object_with_foreign_key(self):\n        class Parent(models.Model):\n            a = models.PositiveIntegerField()\n            b = models.PositiveIntegerField()\n            c = models.PositiveIntegerField()\n\n            class Meta:\n                unique_together = (('a', 'b'),)\n\n        class Child(models.Model):\n            a = models.PositiveIntegerField()\n            b = models.PositiveIntegerField()\n            c = models.PositiveIntegerField()\n            value = models.CharField(max_length=255)\n            parent = models.ForeignObject(\n                Parent,\n                on_delete=models.SET_NULL,\n                from_fields=('a', 'b', 'c'),\n                to_fields=('a', 'b'),\n                related_name='children',\n            )\n\n        field = Child._meta.get_field('parent')\n        self.assertEqual(field.check(from_model=Child), [\n            Error(\n                \"No subset of the fields 'a', 'b', 'c' on model 'Parent' is unique.\",\n                hint=(\n                    'Mark a single field as unique=True or add a set of '\n                    'fields to a unique constraint (via unique_together or a '\n                    'UniqueConstraint (without condition) in the model '\n                    'Meta.constraints).'\n                ),\n                obj=field,\n                id='fields.E310',\n            ),\n        ])\n\n\n\n", "    def test_invalid_foreign_object_ordering(self):\n        class Parent(models.Model):\n            a = models.PositiveIntegerField()\n            b = models.PositiveIntegerField()\n            c = models.PositiveIntegerField()\n\n        class Child(models.Model):\n            a = models.PositiveIntegerField()\n            b = models.PositiveIntegerField()\n            value = models.CharField(max_length=255)\n            parent = models.ForeignObject(\n                Parent,\n                on_delete=models.SET_NULL,\n                from_fields=('b', 'a'),\n                to_fields=('a', 'b'),\n                related_name='children',\n            )\n\n        field = Child._meta.get_field('parent')\n        self.assertEqual(field.check(from_model=Child), [\n            Error(\n                \"The order of fields in 'from_fields' and 'to_fields' \"\n                \"must match exactly.\",\n                hint=(\n                    \"Both 'from_fields' and 'to_fields' should \"\n                    \"specify the same fields in the same order.\"\n                ),\n                obj=field,\n                id='fields.E313',\n            ),\n        ])\n\n\n\n", "    def test_circular_m2m_through_relations(self):\n        class A(models.Model):\n            related_b = models.ManyToManyField('B', through='C')\n\n        class B(models.Model):\n            related_a = models.ManyToManyField('A', through='C')\n\n        class C(models.Model):\n            a = models.ForeignKey(A, on_delete=models.CASCADE)\n            b = models.ForeignKey(B, on_delete=models.CASCADE)\n\n        with self.assertRaises(ValueError) as context:\n            A._meta.get_field('related_b').check()\n        self.assertIn(\n            'Circular dependency detected in ManyToManyField through relationship.',\n            str(context.exception),\n        )\n\n", "    def test_reverse_query_name_clash_with_related_to_fields(self):\n        class Model(models.Model):\n            related_to = models.ForeignKey('RelatedTo', models.CASCADE, related_name='models')\n\n        class RelatedTo(models.Model):\n            clash = models.CharField(max_length=10)\n\n        self.assertEqual(Model.check(), [\n            Error(\n                'Reverse query name for \"Model.related_to\" clashes with field name \"RelatedTo.models\".',\n                hint=(\n                    \"Rename field 'RelatedTo.models', or add/change a related_name \"\n                    \"argument to the definition for field 'Model.related_to'.\"\n                ),\n                obj=Model._meta.get_field('related_to'),\n                id='fields.E303',\n            ),\n        ])\n", "    def test_auto_join_field(self):\n        class Parent(models.Model):\n            a = models.CharField(max_length=255)\n            b = models.CharField(max_length=255)\n            c = models.CharField(max_length=255)\n            class Meta:\n                unique_together = (('a', 'b'),)\n\n        class Child(models.Model):\n            a = models.CharField(max_length=255)\n            b = models.CharField(max_length=255)\n            c = models.CharField(max_length=255)\n            parent = models.ForeignObject(\n                Parent,\n                on_delete=models.SET_NULL,\n                from_fields=('a', 'b'),\n                to_fields=('a', 'b'),\n                related_name='children',\n            )\n\n        field = Child._meta.get_field('parent')\n        self.assertEqual(field.check(from_model=Child), [])\n"], "sample_1146": ["def test_unevaluated_expr_latex():\n    from sympy import UnevaluatedExpr\n    x = symbols('x')\n    expr = UnevaluatedExpr(x**2 + 2*x)\n    assert latex(expr) == r'x^{2} + 2 x'\n", "def test_special_functions_with_symbols():\n    from sympy import sin, cos, exp, log, sqrt, erfc, I, pi\n    x, y, z = symbols('x y z')\n\n    assert latex(sin(x*y)) == r'\\sin{\\left(x y \\right)}'\n    assert latex(cos(x**2 + y)) == r'\\cos{\\left(x^{2} + y \\right)}'\n    assert latex(exp(x*I*y)) == r'\\exp{\\left(i x y \\right)}'\n    assert latex(log(x*y)) == r'\\log{\\left(x y \\right)}'\n    assert latex(sqrt(x**2 + y**2)) == r'\\sqrt{x^{2} + y^{2}}'\n    assert latex(erfc(x)) == r'\\operatorname{erfc}{\\left(x \\right)}'\n    assert latex(pi) == r'\\pi'\n\n    assert latex(sin(pi*x)) == r'\\sin{\\left(\\pi x \\right)}'\n    assert latex(cos(2*pi*x)) == r'\\cos{\\left(2 \\pi x \\right)}'\n    assert latex(exp(I*pi*x)) == r'\\exp{\\left(i \\pi x \\right)}'\n\n    assert latex(log(z**2)) == r'2 \\log{\\left(z \\right)}'\n\n    assert latex(log(x/y) ) == r'\\log{\\left(\\frac{x}{y} \\right)}'\n\n\n\n\n", "def test_function_limits():\n    x = Symbol('x')\n    f = sin(x)\n    assert latex(limit(f, x, 0)) == r'\\lim_{x \\to 0} \\sin{\\left(x \\right)}'\n    assert latex(limit(f, x, pi/2)) == r'\\lim_{x \\to \\frac{\\pi}{2}} \\sin{\\left(x \\right)}'\n    assert latex(limit(f, x, oo)) == r'\\lim_{x \\to \\infty} \\sin{\\left(x \\right)}'\n    assert latex(limit(f, x, -oo)) == r'\\lim_{x \\to - \\infty} \\sin{\\left(x \\right)}'\n    assert latex(limit(f, x, 0, dir='right')) == r'\\lim_{x \\to 0^{+}} \\sin{\\left(x \\right)}'\n    assert latex(limit(f, x, 0, dir='left')) == r'\\lim_{x \\to 0^{-}} \\sin{\\left(x \\right)}'\n\n", "def test_unicode_symbols():\n    from sympy import Symbol, unicode\n    x = Symbol('x')\n    y = Symbol('y')\n\n    # Test basic Unicode characters\n    assert latex(unicode('\u03b1')) == r'\\alpha'\n    assert latex(unicode('\u03b2')) == r'\\beta'\n    assert latex(unicode('\u03b3')) == r'\\gamma'\n    assert latex(unicode('\u03b5')) == r'\\epsilon'\n\n    # Test Unicode symbols with a greek letter\n    assert latex(unicode('\u03c0x')) == r'\\pi x'\n    assert latex(unicode('\u03b3 + \u03b2')) == r'\\gamma + \\beta'\n    assert latex(unicode('x + \u03b1')) == r'x + \\alpha'\n\n    # Test Unicode characters within an expression\n    assert latex(x*unicode('\u2211')*y) == r'x \\sum y'\n\n", "    def test_pretty_print():\n        from sympy import MatrixSymbol, symbols, pprint\n        import inspect\n\n        # check that pretty_print uses the same LaTeX representation as latex\n        A = MatrixSymbol('A', 2, 2)\n        x, y = symbols('x y')\n\n        pprint(A)\n        latex_str = latex(A)\n        assert latex_str == pprint(A)\n\n        pprint(x + y)\n        latex_str = latex(x + y)\n        assert latex_str == pprint(x + y)\n\n\n"], "sample_1020": ["def test_Sum():\n    assert mcode(Sum(x**2, (x, 0, 5))) == \"Hold[Sum[x^2, {x, 0, 5}]]\"\n    assert mcode(Sum(x**2, (x, 0, oo))) == \"Hold[Sum[x^2, {x, 0, Infinity}]]\"\n", "def test_Sum():\n    assert mcode(Sum(x**n, (n, 0, 10))) == \"Hold[Sum[x^n, {n, 0, 10}]]\"\n    assert mcode(Sum(x**n/factorial(n), (n, 0, oo))) == \\\n        \"Hold[Sum[x^n/Factorial[n], {n, 0, Infinity}]]\"\n", "def test_Sum():\n    assert mcode(Sum(x**2, (x, 1, 5))) == \"Hold[Sum[x^2, {x, 1, 5}]]\"\n", "def test_Sum():\n    assert mcode(Sum(x**n, (n, 0, 10))) == \"Hold[Sum[x^n, {n, 0, 10}]]\"\n", "def test_Sum():\n    assert mcode(Sum(x**n, (n, 0, 10))) == \"Hold[Sum[x^n, {n, 0, 10}]]\"\n    assert mcode(Sum((x + y)**n, (n, 1, 5))) == \"Hold[Sum[(x + y)^n, {n, 1, 5}]]\"\n"], "sample_742": ["def test_predict_proba_validation():\n    X, y = make_classification(n_samples=100, n_features=20, random_state=0)\n\n    lr = LogisticRegression(multi_class='multinomial', solver='lbfgs',\n                            random_state=0)\n\n    y_pred = lr.fit(X, y).predict(X)\n    y_proba = lr.predict_proba(X)\n\n    # Check that probabilities are predicted correctly\n    for i in range(X.shape[0]):\n        assert_equal(y[i], np.argmax(y_proba[i]))\n\n    # Check that probabilities sum to 1\n    for i in range(y_proba.shape[0]):\n        assert_almost_equal(np.sum(y_proba[i]), 1.0)\n\n\n", "def test_intercept_scaling_l1():\n    # Test that intercept_scaling works with l1 penalty\n    rng = np.random.RandomState(0)\n    n_samples = 100\n    X = rng.randn(n_samples, 10)\n    y = (X[:, 0] > 0).astype(int)\n\n    for intercept_scaling in [1.0, 0.5, 0.1]:\n        lr = LogisticRegression(penalty=\"l1\", C=1.0, solver='saga',\n                                intercept_scaling=intercept_scaling,\n                                random_state=0)\n        lr.fit(X, y)\n        assert_almost_equal(lr.intercept_,\n                            lr.intercept_ / intercept_scaling)\n\n\n\n", "    def test_fit_intercept_with_constant_features():\n        # Test that fit_intercept works correctly with constant features\n        X = np.ones((100, 5))\n        y = np.ones(100)\n\n        # fit_intercept=True\n        lr = LogisticRegression(fit_intercept=True)\n        lr.fit(X, y)\n        assert_equal(lr.intercept_, 0.0)\n\n        # fit_intercept=False\n        lr = LogisticRegression(fit_intercept=False)\n        lr.fit(X, y)\n        assert_equal(lr.intercept_, 0.0)\n", "compilation error", "def test_logistic_regression_validation():\n    X, y = make_classification(n_samples=100, n_features=20, random_state=0)\n    y_bin = y.copy()\n    y_bin[y_bin == 2] = 0\n\n    # Test with a simple CV strategy\n    clf = LogisticRegression(penalty='l2', C=1., random_state=42, solver='lbfgs')\n    with ignore_warnings(category=ConvergenceWarning):\n        clf.fit(X, y_bin)\n    \n    # Test with a custom CV strategy\n    from sklearn.model_selection import RepeatedKFold\n    \n    repeated_kfold = RepeatedKFold(n_splits=3, n_repeats=2, random_state=42)\n    clf_cv = LogisticRegression(penalty='l2', C=1., random_state=42, solver='lbfgs')\n    \n    # Test with 'fit_intercept=False' to ensure consistent comparison\n    clf_cv_fit_intercept_false = LogisticRegression(penalty='l2', C=1.,\n                                                    random_state=42, solver='lbfgs',\n                                                    fit_intercept=False)\n    \n    scores = cross_val_score(clf, X, y_bin, cv=repeated_kfold,\n                             scoring=\"accuracy\")\n    assert_almost_equal(np.mean(scores), clf.score(X, y_bin))\n    \n    scores_cv = cross_val_score(clf_cv, X, y_bin, cv=repeated_kfold,\n                             scoring=\"accuracy\")\n    assert_almost_equal(np.mean(scores_cv), clf_cv.score(X, y_bin))\n\n    scores_cv_fit_intercept_false = cross_val_score(clf_cv_fit_intercept_false, X, y_bin, cv=repeated_kfold,\n                             scoring=\"accuracy\")\n    assert_almost_equal(np.mean(scores_cv_fit_intercept_false), clf_cv_fit_intercept_false.score(X, y_bin))\n"], "sample_624": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_410": ["    def test_group_permissions(self):\n        g = Group(name=\"Test Group\")\n        g.save()\n        perm = Permission.objects.create(name=\"Test Permission\", content_type=ContentType.objects.get_for_model(Group), codename=\"test_permission\")\n        g.permissions.add(perm)\n        self.assertIn(perm, g.permissions.all())\n        g.permissions.remove(perm)\n        self.assertNotIn(perm, g.permissions.all())\n", "    def test_add_user_to_group(self):\n        group1 = Group.objects.create(name=\"group1\")\n        user1 = User.objects.create(username=\"user1\")\n        group1.user_set.add(user1)\n        self.assertTrue(user1 in group1.user_set.all())\n", "    def test_permissions_str(self):\n        g = Group(name=\"My Group\")\n        with self.settings(AUTH_USER_MODEL=\"auth.User\"):\n            permission = Permission.objects.create(\n                content_type=ContentType.objects.get_for_model(User),\n                codename=\"can_post\",\n                name=\"Can post\",\n            )\n            g.permissions.add(permission)\n        self.assertEqual(str(g), \"My Group (Can post)\")\n", "    def test_add_user(self):\n        g = Group(name=\"Users\")\n        u = User.objects.create_user(\"testuser\")\n        g.save()\n        g.user_set.add(u)\n        self.assertIn(u, g.user_set.all())\n\n\n\n", "    def test_get_groups(self):\n        user1 = User.objects.create_user(\"user1\")\n        group1 = Group.objects.create(name=\"group1\")\n        group2 = Group.objects.create(name=\"group2\")\n        group1.user_set.add(user1)\n        self.assertCountEqual(user1.groups.all(), [group1])\n        self.assertCountEqual(user1.groups.get() , [group1])\n\n"], "sample_544": ["def test_imshow_array_with_alpha_output(fig_test, fig_ref, ext):\n    data = np.random.rand(10, 10)\n    alpha = np.random.rand(10, 10) * 0.5 + 0.5 \n    rgba = np.dstack((data, data, data, alpha))\n\n    fig = fig_test\n    ax = fig.add_subplot()\n    im = ax.imshow(rgba, interpolation='nearest')\n\n    fig.savefig(f'temp_{ext}', format=ext)\n    if ext == 'png':\n        # Ensure alpha values are present in PNG output.\n        with io.open(f'temp_{ext}', 'rb') as f:\n            png_data = f.read()\n            assert b'\\x00\\x00\\x00' in png_data\n", "def test_imshow_large_data_with_cmap(fig_test, fig_ref):\n    # Test that imshow handles large arrays correctly, especially when\n    # combined with a cmap\n    N = 10000\n    data = np.random.rand(N, N)\n\n    cmap = mpl.cm.get_cmap(\"viridis\")\n    ax_test = fig_test.subplots()\n    ax_test.imshow(data, cmap=cmap)\n    ax_ref = fig_ref.subplots()\n    ax_ref.imshow(data, cmap=cmap)\n", "def test_imshow_with_masked_data(fig_test, fig_ref):\n    data = np.arange(10).reshape((2, 5))\n    mask = np.zeros_like(data, dtype=bool)\n    mask[0, 2:] = True\n    masked_data = np.ma.masked_array(data, mask=mask)\n    ax_test = fig_test.subplots()\n    ax_ref = fig_ref.subplots()\n\n    ax_test.imshow(masked_data, interpolation='nearest')\n    ax_ref.imshow(masked_data, interpolation='nearest')\n    ax_test.set_axis_off()\n    ax_ref.set_axis_off()\n", "def test_imshow_masked_data(fig_test, fig_ref):\n    data = np.zeros((10, 10))\n    mask = np.zeros_like(data, dtype=bool)\n    mask[2:8, 2:8] = True\n    data[2:8, 2:8] = 1\n\n    ax_test = fig_test.subplots()\n    img = ax_test.imshow(data, mask=mask, cmap='viridis',\n                         interpolation='nearest')\n    ax_ref = fig_ref.subplots()\n    ax_ref.imshow(data, mask=mask, cmap='viridis',\n                  interpolation='nearest')\n\n    fig_test.colorbar(img)\n    fig_ref.colorbar(fig_ref.images[0])\n\n\n\n", "def test_masked_imshow_data_limits():\n    data = np.ma.masked_array(np.arange(10), mask=[True, False, False, True,\n                                                  False, False, False, True,\n                                                  False, False])\n    fig, ax = plt.subplots()\n\n    # Test setting data limits\n    ax.imshow(data, vmin=2, vmax=7, interpolation='nearest')\n    ax.set_xlim(0, 9)\n    ax.set_ylim(0, 9)\n\n    # Test data limits with masked values\n    ax.imshow(data, vmin=1, vmax=8, interpolation='nearest')\n    ax.set_xlim(0, 9)\n    ax.set_ylim(0, 9)\n\n\n\n"], "sample_1024": ["compilation error", "def test_issue_10998():\n    assert sqrt(2).rewrite(sin) is None\n    assert cos(pi/4).rewrite(sqrt) == sqrt(2)/2\n    assert sin(pi/6).rewrite(tan) is None\n", "compilation error", "compilation error", "compilation error"], "sample_802": ["def test_pipeline_memory_kwargs():\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    cachedir = mkdtemp()\n    try:\n        memory = Memory(cachedir=cachedir, verbose=0,\n                         compress=True,\n                         ttl=3600)\n        pipe = Pipeline([('transf', DummyTransf()), ('clf', SVC())],\n                        memory=memory)\n        pipe.fit(X, y)\n        # Check that parameters passed to Memory are preserved in the pipeline instance\n        assert isinstance(pipe.memory, Memory)\n        assert pipe.memory.cachedir == cachedir\n        assert pipe.memory.verbose == 0\n        assert pipe.memory.compress is True\n        assert pipe.memory.ttl == 3600\n    finally:\n        shutil.rmtree(cachedir)\n", "def test_pipeline_with_invalid_memory_path():\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    invalid_path = '/this/path/does/not/exist'\n    # Test with a non-existent cache directory\n    try:\n        memory = Memory(location=invalid_path, verbose=10)\n        pipe = Pipeline([('transf', DummyTransf()), ('svc', SVC())],\n                        memory=memory)\n        assert_raises(FileNotFoundError, pipe.fit, X, y)\n    except FileNotFoundError:\n        pass\n", "def test_pipeline_memory_invalid_use():\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    cachedir = mkdtemp()\n    memory = Memory(location=cachedir, verbose=10)\n    pipe = Pipeline([('transf', Transf()), ('clf', Mult())], memory=memory)\n    # Test with a None memory\n    pipe = Pipeline([('transf', Transf()), ('clf', Mult())], memory=None)\n    pipe.fit(X, y)\n    assert_array_equal(pipe.predict(X), Mult().fit_transform(X))\n\n    # Test with a wrong type of memory (string)\n    pipe = Pipeline([('transf', Transf()), ('clf', Mult())], memory=cachedir)\n    assert_raises(TypeError, pipe.fit, X, y)\n", "def test_pipeline_memory_error_handling():\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    cachedir = mkdtemp()\n    try:\n        memory = Memory(location=cachedir, verbose=10)\n        # Test with invalid memory paths\n        invalid_paths = [\n            \"non_existing_path\",\n            cachedir + \"/no_permissions\",\n            cachedir + \"\",\n            \"file:// \"\n        ]\n        for path in invalid_paths:\n            with assert_raises(FileNotFoundError) as e:\n                Pipeline([('transf', DummyTransf()), ('svc', SVC())],\n                         memory=Memory(location=path)).fit(X, y)\n            assert \"Cannot create directory\" in str(e.exception)\n\n        with assert_raises(OSError) as e:\n            Pipeline([('transf', DummyTransf()), ('svc', SVC())],\n                     memory=Memory(location=cachedir)).fit(X, y)\n            assert \"Permission denied\" in str(e.exception)\n\n    finally:\n        shutil.rmtree(cachedir)\n", "def test_pipeline_memory_with_fit_transform():\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    cachedir = mkdtemp()\n    try:\n        memory = Memory(cachedir=cachedir, verbose=10)\n        # Test with Transformer + SVC\n        clf = SVC(gamma='scale', probability=True, random_state=0)\n        transf = DummyTransf()\n        pipe = Pipeline([('transf', clone(transf)), ('svc', clf)])\n        cached_pipe = Pipeline([('transf', transf), ('svc', clf)],\n                               memory=memory)\n\n        # Memoize the transformer at the first fit_transform\n        cached_pipe.fit_transform(X, y)\n        pipe.fit_transform(X, y)\n        # Check that cached_pipe and pipe yield identical results\n        assert_array_equal(pipe.predict(X), cached_pipe.predict(X))\n        assert_array_equal(pipe.predict_proba(X), cached_pipe.predict_proba(X))\n        assert_array_equal(pipe.predict_log_proba(X),\n                           cached_pipe.predict_log_proba(X))\n        assert_array_equal(pipe.score(X, y), cached_pipe.score(X, y))\n        assert_array_equal(pipe.named_steps['transf'].means_,\n                           cached_pipe.named_steps['transf'].means_)\n        assert_equal(len(pipe.steps), len(cached_pipe.steps))  \n    finally:\n        shutil.rmtree(cachedir) \n"], "sample_268": ["    def test_snapshot_files_keeps_unchanged_files(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n            snapshot1 = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot1)\n            snapshot2 = dict(self.reloader.snapshot_files())\n            self.assertEqual(snapshot1[self.existing_file], snapshot2[self.existing_file])\n", "    def test_snapshot_files_with_existing_files_returns_mtime(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n            self.reloader.snapshot_files()\n            mtime = os.path.getmtime(str(self.existing_file))\n            self.assertEqual(self.reloader.snapshot_files()[self.existing_file], mtime)\n\n\n", "    def test_snapshot_files_updates_with_missing_file(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file, self.nonexistent_file]):\n            snapshot1 = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot1)\n            self.assertNotIn(self.nonexistent_file, snapshot1)\n            self.increment_mtime(self.existing_file)\n            snapshot2 = dict(self.reloader.snapshot_files())\n            self.assertNotEqual(snapshot1[self.existing_file], snapshot2[self.existing_file])\n            self.assertNotIn(self.nonexistent_file, snapshot2)\n", "    def test_snapshot_files_handles_stat_errors(self, mock_stat):\n        mock_stat.side_effect = OSError()\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n            snapshot = dict(self.reloader.snapshot_files())\n            self.assertNotIn(self.existing_file, snapshot)\n", "    def test_snapshot_files_adds_new_files(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file, self.nonexistent_file]):\n            snapshot1 = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot1)\n            self.assertNotIn(self.nonexistent_file, snapshot1)\n            self.increment_mtime(self.nonexistent_file)\n            snapshot2 = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot2)\n            self.assertIn(self.nonexistent_file, snapshot2)\n"], "sample_511": ["def test_replot():\n    \"\"\"Test that replot works correctly.\"\"\"\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3])\n    fig.canvas.draw()\n\n    # Replot the same data\n    ax.plot([1, 2, 3])\n    fig.canvas.draw()\n    assert ax.get_lines()[-1].get_xdata() == [1, 2, 3]\n\n    # Replot with different data\n    ax.cla()  \n    ax.plot([4, 5, 6])\n    fig.canvas.draw()\n    assert ax.get_lines()[-1].get_xdata() == [4, 5, 6]\n\n\n\n", "def test_subplot_non_unique_spec():\n    with pytest.raises(ValueError):\n        plt.subplot(121)\n        plt.subplot(121)  \n\n    plt.close()\n", "def test_stackplot_with_labels(): \n    x = [1, 2, 3]\n    y1 = [1, 2, 3]\n    y2 = [4, 5, 6]\n    y3 = [7, 8, 9]\n\n    fig, ax = plt.subplots()\n    \n    labels = ['bar 1', 'bar 2', 'bar 3']\n\n    plt.stackplot(x, y1, y2, y3, labels=labels)\n    plt.legend(loc='best')\n    \n    assert ax.get_legend().get_texts()[0].get_text() == 'bar 1' \n    assert ax.get_legend().get_texts()[1].get_text() == 'bar 2' \n    assert ax.get_legend().get_texts()[2].get_text() == 'bar 3' \n", "def test_stackplot_kwargs():\n    data  = [\n        [1, 2, 3, 4],\n        [5, 6, 7, 8],\n    ]\n    kwargs = {'colors': ['red', 'blue'],\n              'labels': ['A', 'B'],\n              'alpha': 0.5}\n    with plt.rc_context({'figure.figsize': (4, 3)}):\n        fig, ax = plt.subplots()\n        plt.stackplot(ax=ax, data=data, **kwargs)\n    plt.close()\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    plt.stackplot(ax=ax, data=data, palette='viridis')\n    plt.close()\n", "def test_subplot_projection_reuse_with_kwarg():\n    # check that subplot reuse works with projection kwarg\n    ax1 = plt.subplot(projection='polar')\n    ax2 = plt.subplot(projection='polar')\n    assert ax1 is ax2\n    ax1.remove()\n    ax3 = plt.subplot(projection='polar')\n    assert ax3 is not ax1\n    assert ax3 is ax2\n\n    # check that subplot reuse works with additional kwarg\n    ax4 = plt.subplot(projection='polar', theta_offset=0)\n    ax5 = plt.subplot(projection='polar', theta_offset=0)\n    assert ax4 is ax5\n"], "sample_1058": ["def test_printing_special_functions():\n    p = SciPyPrinter()\n    assert p.doprint(scipy.special.erf(x)) == 'scipy.special.erf(x)'\n    assert p.doprint(scipy.special.erfc(x)) == 'scipy.special.erfc(x)'\n    assert p.doprint(scipy.special.jv(2, x)) == 'scipy.special.jv(2, x)'\n", "compilation error", "def test_codegen_array_operations():\n    from sympy.codegen.array_utils import CodegenArrayTensorProduct, CodegenArrayContraction, CodegenArrayDiagonal, CodegenArrayPermuteDims, CodegenArrayElementwiseAdd\n    prntr = NumPyPrinter()\n\n    # Test tensor product\n    t1 = CodegenArrayTensorProduct(\n        [MatrixSymbol('a', 2, 2), MatrixSymbol('b', 2, 1)], \n        [('a', 0, 'b', 0), ('a', 1, 'b', 0)]\n    )\n    assert prntr.doprint(t1) == 'numpy.einsum(\\'ab,bc->ac\\', a, b)'\n\n    # Test contraction\n    t2 = CodegenArrayContraction(\n        expr=MatrixSymbol('c', 2, 3), \n        contraction_indices=[0, 0]\n    )\n    assert prntr.doprint(t2) == 'numpy.sum(c, axis=0)'\n\n    # Test diagonal\n    t3 = CodegenArrayDiagonal(expr=MatrixSymbol('d', 3, 3), diagonal_indices=[(0, 0)])\n    assert prntr.doprint(t3) == 'numpy.diagonal(d, 0, axis1=0, axis2=0)'\n\n    # Test permutation\n    t4 = CodegenArrayPermuteDims(expr=MatrixSymbol('e', 3, 2), permutation=[1, 0])\n    assert prntr.doprint(t4) == 'numpy.transpose(e, [1, 0])'\n\n    # Test elementwise addition\n    t5 = CodegenArrayElementwiseAdd(args=[MatrixSymbol('f', 2, 2), MatrixSymbol('g', 2, 2)])\n    assert prntr.doprint(t5) == 'numpy.add(f, g)'\n", "def test_issue_14283():\n    prntr = PythonCodePrinter()\n\n    assert prntr.doprint(oo) == \"float('inf')\"\n\n", "def test_sympy_functions():\n    prntr = SymPyPrinter()\n    assert prntr.doprint(sympy.erf(x)) == 'sympy.erf(x)'\n    assert prntr.doprint(sympy.erfc(x)) == 'sympy.erfc(x)'\n    assert prntr.doprint(sympy.factorial(x)) == 'sympy.factorial(x)'\n    assert prntr.doprint(sympy.gamma(x)) == 'sympy.gamma(x)'\n    assert prntr.doprint(sympy.loggamma(x)) == 'sympy.loggamma(x)'\n    assert prntr.doprint(sympy.digamma(x)) == 'sympy.digamma(x)'\n    assert prntr.doprint(sympy.RisingFactorial(x, 2)) == 'sympy.poch(x, 2)'\n\n\n"], "sample_259": ["    def test_prefetch_object_with_related_lookups(self):\n        book1 = Book.objects.get(id=self.book1.id)\n        with self.assertNumQueries(1):\n            prefetch_related_objects([book1], Prefetch(\n                'authors',\n                to_attr='the_authors',\n                queryset=Author.objects.select_related('first_book'),\n            ))\n\n        with self.assertNumQueries(0):\n            self.assertCountEqual(book1.the_authors, [self.author1, self.author2, self.author3])\n            self.assertEqual(book1.the_authors[0].first_book.title, 'Poems')\n\n", "    def test_prefetch_related_with_nested_lookups(self):\n        reader1 = Reader.objects.get(id=self.reader1.id)\n        with self.assertNumQueries(2): \n            prefetch_related_objects(\n                [reader1], \n                Prefetch('books_read__authors',  \n                         to_attr='read_books_authors')\n            )\n\n        with self.assertNumQueries(0):\n            self.assertCountEqual(reader1.read_books_authors, \n                                 [\n                                     set([self.author1, self.author2, self.author3]),\n                                     set([self.author4])\n                                 ])\n", "    def test_prefetch_related_with_nested_lookups(self):\n        authors = Author.objects.all()\n        with self.assertNumQueries(2):\n            prefetch_related_objects(\n                authors,\n                Prefetch(\n                    'books',\n                    queryset=Book.objects.filter(title__startswith='W'),\n                    to_attr='matching_books',\n                ),\n            )\n        with self.assertNumQueries(0):\n            self.assertEqual(\n                [a.matching_books for a in authors],\n                [[self.book3]],\n            )\n\n", "    def test_prefetch_related_object_with_nested_relationships(self):\n        readers = list(Reader.objects.all())\n        with self.assertNumQueries(1):\n            prefetch_related_objects(readers, Prefetch('books_read__authors'))\n\n        with self.assertNumQueries(0):\n            for reader in readers:\n                self.assertEqual(\n                    set([a.name for b in reader.books_read.all() for a in b.authors.all()]),\n                    set(['Charlotte', 'Anne', 'Emily', 'Jane'])\n                )\n", "    def test_prefetch_object_with_cache_key(self):\n        book1 = Book.objects.get(id=self.book1.id)\n        with self.assertNumQueries(1):\n            prefetch_related_objects([book1], Prefetch('authors', cache_key='authors_cache'))\n\n        with self.assertNumQueries(0):\n            prefetch_related_objects([book1], Prefetch('authors', cache_key='authors_cache'))\n\n\n"], "sample_1117": ["def test_factorization():\n    X = MatrixSymbol('X', 4, 4)\n    assert ask(Q.diagonal(X), Q.diagonal(LofLU(X)))\n    assert ask(Q.invertible(X), Q.invertible(LofLU(X)))\n    assert ask(Q.singular(X), Q.singular(LofLU(X)))\n\n\n", "def test_non_commutative():\n    X = MatrixSymbol('X', 2, 2)\n    Y = MatrixSymbol('Y', 2, 2)\n    assert not ask(Q.commutative(X*Y), Q.commutative(X) & Q.commutative(Y))\n    assert not ask(Q.commutative(X*Y), Q.commutative(X)*Q.commutative(Y))\n    assert not ask(Q.commutative(X*Y), Q.commutative(X))\n    assert not ask(Q.commutative(X*Y), Q.commutative(Y))\n    assert not ask(Q.commutative(X*Y), Q.commutative(MatrixSymbol('Z', 2, 2)))\n\n\n\n", "def test_nonzero_elements():\n    X = MatrixSymbol('X', 4, 4)\n    Y = MatrixSymbol('Y', 4, 4)\n    assert ask(Q.nonzero_elements(X), Q.invertible(X)) is None\n    assert ask(Q.nonzero_elements(X), Q.fullrank(X)) is True\n    assert ask(Q.nonzero_elements(X + Y), Q.nonzero_elements(X) & Q.nonzero_elements(Y)) is True\n    assert ask(Q.nonzero_elements(X*Y), Q.nonzero_elements(X) & Q.nonzero_elements(Y)) is True\n    assert ask(Q.nonzero_elements(DiagonalMatrix(X)), Q.nonzero_elements(X)) is True\n    assert ask(Q.nonzero_elements(ZeroMatrix(4, 4))) is False\n    assert ask(Q.nonzero_elements(OneMatrix(4, 4))) is True\n", "compilation error", "def test_matrix_element_logic():\n    X = MatrixSymbol('X', 4, 4)\n    Y = MatrixSymbol('Y', 4, 4)\n    assert ask(Q.positive(X[0, 0]) & Q.positive(X[1, 1]),\n               Q.positive_definite(X))\n    assert ask(Q.nonzero(X), Q.invertible(X))\n    assert ask(Q.nonzero(X), Q.square(X))\n    assert ask(Q.allclose(X+Y, X) & Q.allclose(X, Y), Q.equal(X, Y))\n    assert ask(Q.allclose(X*Y, Y*X), Q.commutative(X, Y))\n    assert ask(Q.diag_entries(X), Q.diagonal(X))\n    assert ask(Q.trace(X), Q.sum(diag_entries(X)))\n    assert ask(~Q.invertible(X), Q.singular(X))\n\n\n"], "sample_1091": ["compilation error", "compilation error", "def test_issue_17263():\n    x, y = symbols('x y')\n    f = Eq(sin(x) + sin(y), 2*sin(x/2)*cos(y/2))\n    assert simplify(f) == Eq(2*sin(x/2)*cos(y/2), sin(x) + sin(y))\n\n\n", "def test_issue_17923():\n    x, y = symbols('x y')\n    expr = Eq(x**2 + y**2, 1)\n    assert expr.as_domain() == And(x**2 + y**2 == 1)\n", "compilation error"], "sample_910": ["def test_prefix_warnings(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    with logging.prefixed_warnings(\"PREFIX:\"):\n        logger.warning('message1')\n        logger.warning('message2')\n\n    assert 'PREFIX: WARNING: message1' in warning.getvalue()\n    assert 'PREFIX: WARNING: message2' in warning.getvalue()\n\n    logger.warning('message3')\n    assert 'WARNING: message3' in warning.getvalue()\n\n\n\n", "def test_prefixed_warnings(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    with logging.prefixed_warnings(\"TEST:\"):\n        logger.warning('message')\n\n    assert 'TEST: WARNING: message' in warning.getvalue()\n\n    with logging.prefixed_warnings(\"TEST: \") as prefix:\n        logger.warning('message')\n\n    assert 'TEST:  WARNING: message' in warning.getvalue()\n\n    with logging.prefixed_warnings(\"TEST *\"):\n        logger.warning('message')\n\n    assert 'TEST: WARNING: message' in warning.getvalue()\n\n    with logging.prefixed_warnings(\"TEST.*\"):\n        logger.warning('message.sub')\n\n    assert 'TEST:.*: WARNING: message.sub' in warning.getvalue()\n\n    with logging.prefixed_warnings(\"TEST.sub\"):\n        logger.error('message.sub')\n\n    assert 'TEST.sub: WARNING: message.sub' in warning.getvalue()\n\n\n\n", "def test_prefixed_warnings(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    with prefixed_warnings(\"TEST:\"):\n        logger.warning('message1')\n        logger.warning('message2')\n\n    assert 'TEST: WARNING: message1' in warning.getvalue()\n    assert 'TEST: WARNING: message2' in warning.getvalue()\n", "def test_prefix_warnings(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    with prefixed_warnings(\"PREFIX: \"):\n        logger.warning('message1')\n        logger.warning('message2')\n\n    assert 'PREFIX: WARNING: message1' in warning.getvalue()\n    assert 'PREFIX: WARNING: message2' in warning.getvalue()\n", "def test_prefixed_warnings(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    with prefixed_warnings(\"TEST:\"):\n        logger.warning('message1')\n        logger.warning('message2')\n\n    assert 'TEST: WARNING: message1' in warning.getvalue()\n    assert 'TEST: WARNING: message2' in warning.getvalue()\n\n"], "sample_320": ["    def test_references_field_by_through_reverse(self):\n        operation = FieldOperation(\n            \"Model\", \"field\", models.ManyToManyField(\"Other\", through=\"Through\")\n        )\n        self.assertIs(\n            operation.references_field(\"Through\", \"other_field\", \"migrations\"),\n            True,\n        )\n        self.assertIs(\n            operation.references_field(\"Missing\", \"whatever\", \"migrations\"),\n            False,\n        )\n", "    def test_references_field_by_manytomany_through_model(self):\n        operation = FieldOperation(\n            \"Model\", \"field\", models.ManyToManyField(\"Other\", through=\"Through\")\n        )\n        self.assertIs(\n            operation.references_field(\n                \"Through\", \"many_to_many_field\", \"migrations\"\n            ),\n            True,\n        )\n        self.assertIs(\n            operation.references_field(\"Model\", \"field\", \"migrations\"),\n            True,\n        )\n", "    def test_references_field_by_through_related_name(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ManyToManyField(\"Other\", through=\"Through\", related_name=\"through_relationship\"),\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"through_relationship\", \"migrations\"),\n            True,\n        )\n        self.assertIs(\n            operation.references_field(\"Through\", \"whatever\", \"migrations\"),\n            False,\n        )\n\n\n\n", "    def test_rename_field_in_manytomany(self):\n        operation = migrations.RemoveField(\n            \"Model\", \"old_field\"\n        )\n        # Model, field, and app labels\n        operation.name = \"RenameField\"\n        operation.model_name = \"Model\"\n        operation.field_name = \"old_field\"\n        operation.app_label = \"migrations\"\n        self.assertEqual(\n            operation.describe(),\n            'RenameField of Model.old_field in migrations',\n        )\n\n\n\n", "    def test_references_field_by_reverse_related(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.OneToOneField(\"Other\", models.CASCADE, related_name=\"related_field\"),\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"related_field\", \"migrations\"), True\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"whatever\", \"migrations\"), False\n        )\n"], "sample_202": ["    def test_not_finished_sentinel(self):\n        storage = self.storage_class(self.get_request())\n        example_messages = ['test', 'me']\n        set_cookie_data(storage, example_messages)\n        # The message contains what's expected.\n        self.assertEqual(list(storage), example_messages)\n        # Add more messages.\n        storage.add(constants.INFO, 'more_test')\n        storage.add(constants.WARNING, 'more_me')\n\n        response = self.get_response()\n        unstored_messages = storage.update(response)\n        # Ensure that the message storage size is as expected\n        self.assertEqual(stored_cookie_messages_count(storage, response), len(example_messages))\n\n        # Set the cookie data to contain the not_finished sentinel\n        set_cookie_data(storage, example_messages + ['test', 'me']) \n        self.assertEqual(storage._decode(storage.request.COOKIES[storage.cookie_name]), example_messages + ['test', 'me'])\n\n\n\n", "    def test_not_finished_sentinel(self):\n        storage = self.get_storage()\n        response = self.get_response()\n\n        # Create a list of messages that will exceed the max cookie size.\n        messages = ['message 1' for _ in range(int(CookieStorage.max_cookie_size / 3))]\n        storage.add_message(constants.INFO, messages[-1], extra_tags='important')\n\n        # Store the messages, which will truncated due to max cookie size\n        storage.update(response)\n\n        # Check that the cookie was updated\n        set_cookie_data(storage, messages)\n\n        # Retrieve the messages\n        self.assertEqual(list(storage), messages[:-1])  # Check for truncated messages\n\n        # Check that the sentinel value was added to the cookie\n        self.assertTrue(storage._decode(storage.request.COOKIES[CookieStorage.cookie_name])[-1] == CookieStorage.not_finished)\n\n\n\n", "    def test_not_finished(self):\n        storage = self.get_storage()\n        response = self.get_response()\n\n        storage.add(constants.INFO, 'test')\n        storage.add(constants.INFO, 'another test')\n        unstored_messages = storage.update(response, remove_oldest=False)\n\n        self.assertEqual(len(unstored_messages), 0)\n        cookie_data = response.cookies.get(storage.cookie_name)\n        self.assertEqual(cookie_data.value.endswith(storage.not_finished))\n\n        storage.add(constants.INFO, 'last message')\n        unstored_messages = storage.update(response, remove_oldest=False)\n\n        self.assertEqual(len(unstored_messages), 0)\n        cookie_data = response.cookies.get(storage.cookie_name)\n        self.assertEqual(cookie_data.value.endswith(storage.not_finished))\n\n        # Simulate retrieving only part of the data\n        storage.request.COOKIES = {storage.cookie_name: cookie_data.value[:-len(storage.not_finished)]}\n        messages, _ = storage._get()\n        self.assertEqual(len(messages), 3)\n        self.assertEqual(messages[-1], 'last message')\n\n        # Now retrieve the remaining messages\n        messages, all_retrieved = storage._get()\n        self.assertEqual(len(messages), 1)\n        self.assertEqual(messages[0], storage.not_finished)\n        self.assertFalse(all_retrieved)\n\n\n", "    def test_not_finished_sentinel(self):\n        storage = self.get_storage()\n        response = self.get_response()\n        messages = [\n            Message(constants.INFO, 'message 1'),\n            Message(constants.INFO, 'message 2'),\n            Message(constants.INFO, 'message 3'),\n        ]\n        storage._store(messages, response, remove_oldest=False)\n        storage._store(messages, response, remove_oldest=False)\n        self.assertEqual(self.stored_messages_count(storage, response), 6)\n", "    def test_not_finished_sentinel(self):\n        storage = self.storage_class(self.get_request())\n        messages = ['test', 'me']\n        set_cookie_data(storage, messages, encode_empty=True)\n        # Get messages from cookie\n        messages, all_retrieved = storage._get()\n        self.assertEqual(messages, [])\n        self.assertFalse(all_retrieved)\n\n        # Store next batch of messages\n        messages = ['more_test', 'more_me']\n        set_cookie_data(storage, messages)\n        messages, all_retrieved = storage._get()\n        self.assertEqual(messages, ['more_test', 'more_me'])\n        self.assertTrue(all_retrieved)\n"], "sample_453": ["    def test_cell_count(self):\n        inline_admin_formset = self.create_inline_admin_formset()\n        cell_count = cell_count(inline_admin_formset)\n        self.assertEqual(cell_count, 4)\n\n", "    def test_override_admin_modify_and_admin_list_template_tags_for_article(self):\n        article = Article.objects.all()[0]\n        request = self.factory.get(\n            reverse(\"admin:admin_views_article_change\", args=[article.pk])\n        )\n        request.user = self.superuser\n        admin = ArticleAdmin(Article, site)\n        extra_context = {\"show_publish\": True, \"extra\": True}\n        response = admin.change_view(\n            request, str(article.pk), extra_context=extra_context\n        )\n        response = admin.changelist_view(request)\n        response.render()\n        self.assertContains(response, 'name=\"_save\"') \n        self.assertContains(response, 'name=\"_publish\"') \n        self.assertContains(response, \"override-actions\")  \n        self.assertContains(response, \"override-change_list_object_tools\")  \n        self.assertContains(response, \"override-change_list_results\")\n", "    def test_inline_admin_form_cell_count(self):\n        article = Article.objects.all()[0]\n        request = self.request_factory.get(\n            reverse(\"admin:admin_views_article_change\", args=[article.pk])\n        )\n        request.user = self.superuser\n        admin = ArticleAdmin(Article, site)\n        response = admin.change_view(request, str(article.pk))\n        template_context = response.context_data\n        inline_admin_formset = template_context[\"inlines\"][0] # Assuming there is one inline formset\n        cell_count = cell_count(inline_admin_formset)\n        self.assertGreater(cell_count, 1)  \n        self.assertIn(\"id\", [field.name for field in inline_admin_formset.formset])\n", "    def test_date_hierarchy_empty_view(self):\n        modeladmin = ModelAdmin(Question, site)\n        modeladmin.date_hierarchy = \"posted\"\n        request = self.factory.get(\"/\", {\"posted__year\": \"2018\"})\n        request.user = self.superuser\n        changelist = modeladmin.get_changelist_instance(request)\n        spec = date_hierarchy(changelist)\n        self.assertEqual(spec[\"choices\"], [])\n", "    def test_prepopulated_fields_js(self):\n        article = Article.objects.all()[0]\n        request = self.request_factory.get(\n            reverse(\"admin:admin_views_article_change\", args=[article.pk])\n        )\n        request.user = self.superuser\n        admin = ArticleAdmin(Article, site)\n        response = admin.change_view(request, str(article.pk))\n        response.render()\n        self.assertContains(response, \"prepopulated_fields\")\n        self.assertContains(response, \"prepopulated_fields_json\")\n"], "sample_903": ["def test_negative_perplexity():\n    with assert_raises(ValueError):\n        tsne = TSNE(perplexity=-1)\n", "def test_non_square_embeddings():\n    # Test that tsne handles non-square embeddings gracefully\n    random_state = check_random_state(0)\n    X = random_state.randn(5, 3)\n    tsne = TSNE(n_components=2)\n    tsne.fit_transform(X)\n\n\n\n\n", "    def test_sparse_input():\n        # Test handling of sparse input matrices\n        from scipy.sparse import csr_matrix\n        random_state = check_random_state(0)\n        X_dense = random_state.randn(100, 2)\n        X_sparse = csr_matrix(X_dense)\n\n        tsne = TSNE(n_components=2, perplexity=30, random_state=0,\n                    method='barnes_hut')\n        tsne.fit_transform(X_sparse)\n\n        # Check that the output is a dense array\n        assert tsne.embedding_.shape == (100, 2)\n        assert not isinstance(tsne.embedding_, sp.csr_matrix)\n\n\n\n", "def test_n_components_too_large():\n    # Test that an error is raised when n_components > n_samples\n    random_state = check_random_state(0)\n    X = random_state.randn(100, 2)\n    for n_components in range(101, 1000):\n        with pytest.raises(ValueError):\n            tsne = TSNE(n_components=n_components, random_state=0)\n            tsne.fit(X)\n\n\n\n", "def test_n_iter_without_progress_with_early_exaggeration():\n    # Test if n_iter_without_progress works correctly when early_exaggeration is active\n    random_state = check_random_state(0)\n    X = random_state.randn(100, 2)\n\n    tsne = TSNE(n_iter_without_progress=-1, verbose=2,\n                learning_rate=1e8, random_state=0, method='barnes_hut',\n                n_iter=351, init=\"random\", early_exaggeration=10.0)\n\n    old_stdout = sys.stdout\n    sys.stdout = StringIO()\n    try:\n        tsne.fit_transform(X)\n    finally:\n        out = sys.stdout.getvalue()\n        sys.stdout.close()\n        sys.stdout = old_stdout\n\n    assert_in(\"did not make any progress during the \"\n              \"last -1 episodes. Finished.\", out)\n\n"], "sample_322": ["    def test_minimize_rollbacks_complex(self):\n        \"\"\"\n        Minimize rollbacks when target has a complex dependency graph.\n\n        a: 1 <---- 2 <---- 3\n          \\     /   \\\n           \\- 4 --- 5\n            \\     /\n             \\   /\n              \\- 6\n              \n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        a3_impl = FakeMigration('a3')\n        a3 = ('a', '3')\n        a4_impl = FakeMigration('a4')\n        a4 = ('a', '4')\n        a5_impl = FakeMigration('a5')\n        a5 = ('a', '5')\n        a6_impl = FakeMigration('a6')\n        a6 = ('a', '6')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(a3, a3_impl)\n        graph.add_node(a4, a4_impl)\n        graph.add_node(a5, a5_impl)\n        graph.add_node(a6, a6_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_dependency(None, a2, a1)\n        graph.add_dependency(None, a3, a2)\n        graph.add_dependency(None, a4, a3)\n        graph.add_dependency(None, a5, a4)\n        graph.add_dependency(None, a6, a5)\n        graph.add_dependency(None, b1, a6)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            b1: b1_impl,\n            a2: a2", "    def test_deferred_sql_does_not_block_other_migrations(self):\n        \"\"\"Deferred SQL shouldn't block other migrations from applying.\"\"\"\n        class DeferredApply:\n                self.raises = raises\n\n                if self.raises:\n                    raise RuntimeError(\"Deferred SQL failed.\")\n                # Otherwise, just do something that doesn't impact the tables\n\n        class Migration(migrations.Migration):\n            atomic = False\n\n                deferred = DeferredApply()\n                schema_editor.deferred_sql.append(deferred())\n\n        executor = MigrationExecutor(None)\n        graph = MigrationGraph()\n        graph.add_node(('a', '1'), Migration('a1'))\n        graph.add_node(('b', '1'), Migration('b1'))\n        graph.add_dependency(None, ('b', '1'), ('a', '1'))\n\n        executor.loader = FakeLoader(graph, {})\n\n        with self.assertRaises(RuntimeError):\n            executor.apply_migration(\n                ProjectState(),\n                Migration('a1'),\n            )\n\n\n\n        self.assertObjectInState(\n            executor.loader.applied_migrations,\n            ('b', '1'),\n        )\n", "    def test_migration_plan_with_dependencies(self):\n        \"\"\"\n        Migration plan takes app dependencies into account.\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        c1_impl = FakeMigration('c1')\n        c1 = ('c', '1')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_node(c1, c1_impl)\n        graph.add_dependency(None, b1, a1)\n        graph.add_dependency(None, c1, a2)\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {})\n\n        plan = executor.migration_plan({'a': '1', 'b': '1'})\n        self.assertEqual(plan, [\n            (a1_impl, True),\n            (b1_impl, True),\n            (a2_impl, True),\n            (c1_impl, True)\n            ])\n", "    def test_minimize_rollbacks_complex_dependencies(self):\n        r\"\"\"Minimize rollbacks in complex dependency graphs.\"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        a3_impl = FakeMigration('a3')\n        a3 = ('a', '3')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        b2_impl = FakeMigration('b2')\n        b2 = ('b', '2')\n        c1_impl = FakeMigration('c1')\n        c1 = ('c', '1')\n        c2_impl = FakeMigration('c2')\n        c2 = ('c', '2')\n        d1_impl = FakeMigration('d1')\n        d1 = ('d', '1')\n\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(a3, a3_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_node(b2, b2_impl)\n        graph.add_node(c1, c1_impl)\n        graph.add_node(c2, c2_impl)\n        graph.add_node(d1, d1_impl)\n\n        graph.add_dependency(None, b1, a1)\n        graph.add_dependency(None, a2, a1)\n        graph.add_dependency(None, b2, a2)\n        graph.add_dependency(None, c1, a3)\n        graph.add_dependency(None, c2, a3)\n        graph.add_dependency(None, d1, c2)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            b1: b1_impl,\n            a2: a2_impl,\n            b2: b2_", "    def test_minimize_rollbacks_no_dependencies(self):\n        \"\"\"Minimize rollbacks when there are no dependencies.\"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            a2: a2_impl,\n        })\n\n        plan = executor.migration_plan({a1})\n\n        self.assertEqual(plan, [(a2_impl, True)])\n"], "sample_434": ["    def test_redirect_view_works(self):\n        factory = RequestFactory()\n        view = RedirectView.as_view(url=\"/about/\")\n        response = view(factory.get(\"/\"))\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response[\"Location\"], \"/about/\")\n", "    def test_redirect_views_work(self):\n        rf = RequestFactory()\n        view = RedirectView()\n\n        view.url = 'https://www.example.com'\n        response = view.get(rf.get('/'))\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response['Location'], 'https://www.example.com')\n\n        view.pattern_name = 'hello'\n        response = view.get(rf.get('/'))\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response['Location'], reverse('hello'))\n", "    def test_redirect_view_with_url(self):\n        class RedirectViewWithUrl(RedirectView):\n            url = \"/another-view/\"\n\n        rf = RequestFactory()\n        view = RedirectViewWithUrl()\n        response = view.get(rf.get('/'))\n\n        self.assertRedirects(response, '/another-view/')\n", "    def test_template_response_mixin(self):\n        class TemplateView(TemplateResponseMixin, ContextMixin, View):\n            template_name = \"test/template.html\"\n\n                context = self.get_context_data(**kwargs)\n                return self.render_to_response(context)\n\n        view = TemplateView()\n        response = view.get(RequestFactory().get('/'))\n        self.assertIsInstance(response, HttpResponse)\n        self.assertEqual(response.template_name, \"test/template.html\")\n", "    def test_redirect_view_behavior(self):\n        redirect_view = RedirectView.as_view(url='http://google.com')\n        request = RequestFactory().get('/')\n        response = redirect_view(request)\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response['Location'], 'http://google.com')\n\n        permanent_redirect_view = RedirectView.as_view(url='http://google.com', permanent=True)\n        response = permanent_redirect_view(request)\n        self.assertEqual(response.status_code, 301)\n        self.assertEqual(response['Location'], 'http://google.com')\n\n\n\n"], "sample_347": ["    def test_localdate_zoneinfo_ambiguous(self):\n        ambiguous = datetime.datetime(2015, 10, 25, 2, 30, tzinfo=PARIS_ZI)\n        self.assertEqual(timezone.localdate(ambiguous), datetime.date(2015, 10, 25)) \n", "    def test_localtime_ambiguous(self):\n        ambiguous = datetime.datetime(2015, 10, 25, 2, 30)\n        naive = timezone.localtime(ambiguous)\n        self.assertEqual(naive.hour, 4)\n        self.assertEqual(naive.minute, 30)\n\n        with self.assertRaises(pytz.AmbiguousTimeError):\n            timezone.localtime(ambiguous, timezone=CET)\n\n\n\n\n", "    def test_localtime_with_dst(self):\n        naive = datetime.datetime(2015, 3, 29, 2, 30, tzinfo=None)\n        with mock.patch('django.utils.timezone.now', return_value=naive):\n            with self.assertRaisesMessage(ValueError, 'localtime() cannot be applied to a naive datetime'):\n                timezone.localtime(naive)\n            with self.assertRaisesMessage(ValueError, 'localtime() cannot be applied to a naive datetime'):\n                timezone.localtime(naive, timezone=EAT)\n\n        aware = timezone.make_aware(naive, timezone=CET)\n\n        with timezone.override(EAT):\n            self.assertEqual(timezone.localtime(aware), datetime.datetime(2015, 3, 29, 2, 30, tzinfo=EAT))\n            self.assertEqual(timezone.localtime(aware, timezone=EAT), datetime.datetime(2015, 3, 29, 2, 30, tzinfo=EAT))\n\n        with timezone.override(CET):\n            self.assertEqual(timezone.localtime(aware), datetime.datetime(2015, 3, 29, 2, 30, tzinfo=CET))\n            self.assertEqual(timezone.localtime(aware, timezone=CET), datetime.datetime(2015, 3, 29, 2, 30, tzinfo=CET)) \n\n\n", "    def test_localtime_ambiguous(self):\n        ambiguous = datetime.datetime(2015, 10, 25, 2, 30, tzinfo=CET)\n        with self.subTest('ambiguous'):\n            self.assertEqual(timezone.localtime(ambiguous), ambiguous.astimezone(CET))\n\n        std = timezone.localtime(ambiguous.replace(fold=1), timezone=CET)\n        dst = timezone.localtime(ambiguous, timezone=CET)\n        with self.subTest('ambiguous dst'):\n            self.assertEqual(std - dst, datetime.timedelta(hours=1))\n\n        with mock.patch('django.utils.timezone.now', return_value=ambiguous):\n            with self.assertRaises(pytz.AmbiguousTimeError):\n                localtime_ambigous()\n\n\n\n", "    def test_make_aware_ambiguous_zoneinfo(self):\n        # 2:30 happens twice, once before DST ends and once after\n        ambiguous = datetime.datetime(2015, 10, 25, 2, 30)\n\n        std = timezone.make_aware(ambiguous, timezone=PARIS_ZI)\n        dst = timezone.make_aware(ambiguous, timezone=PARIS_ZI)\n\n        self.assertEqual(\n            std.astimezone(UTC) - dst.astimezone(UTC),\n            datetime.timedelta(hours=1)\n        )\n        self.assertEqual(std.utcoffset(), datetime.timedelta(hours=1))\n        self.assertEqual(dst.utcoffset(), datetime.timedelta(hours=2))\n"], "sample_1054": ["def test_issue_12061():\n    cr1 = ComplexRegion(FiniteSet(1 + 2*I, 1 - 2*I))\n    cr2 = ComplexRegion(FiniteSet(1 + 3*I, 1 - 3*I))\n    assert cr1.union(cr2) == ComplexRegion(FiniteSet(1 + 2*I, 1 - 2*I, 1 + 3*I, 1 - 3*I))\n", "    def test_issue_12045():\n        cr = ComplexRegion(FiniteSet(1, 2, 3) * FiniteSet(1, 2))\n        assert simplify(cr) == ComplexRegion(FiniteSet(1 + 1*I, 1 + 2*I, 2 + 1*I, 2 + 2*I, 3 + 1*I, 3 + 2*I))\n\n", "compilation error", "compilation error", "def test_issue_12098():\n    from sympy.abc import z\n    c = ComplexRegion(Interval(0, 1)*Interval(0, pi))\n    assert union(c, ComplexRegion(Interval(1, 2)*Interval(0, pi))) == \\\n        ComplexRegion(Interval(0, 2)*Interval(0, pi))\n    assert intersection(c, ComplexRegion(Interval(0, 1)*Interval(0, pi))) == c\n    assert intersection(c, ComplexRegion(Interval(1, 2)*Interval(0, pi))) == \\\n        FiniteSet(1 + 0.5*I)\n    assert intersection(c, ComplexRegion(Interval(0, 1)*Interval(pi, 2*pi))) == \\\n        FiniteSet(1 + 0.5*I)\n\n    c1 = ComplexRegion(Interval(1, 2)*Interval(0, pi))\n    c2 = ComplexRegion(FiniteSet(1 + I))\n    c3 = ComplexRegion(Interval(0, 1)*Interval(0, pi))\n    assert union(c1, c2) != c3\n    assert intersection(c1, c2) == FiniteSet(1 + I)\n    assert union(c1, c3) == c1\n\n\n\n    \n\n\n\n\n"], "sample_124": ["    def test_renderer_overrides(self):\n        class MyForm(Form):\n            my_field = CharField()\n\n            return f'<input type=\"text\" name=\"{field.name}\" value=\"{field.value}\" class=\"custom-class\">'\n\n        form = MyForm()\n        form.renderer = my_renderer_method  # override default renderer\n        self.assertEqual(form.as_p(), '<p><label for=\"id_my_field\">My field:</label>'\n                         '<input type=\"text\" name=\"my_field\" value=\"\" class=\"custom-class\"></p>')\n\n\n\n", "    def test_renderer_inheritance(self):\n        class CustomForm(Form):\n            default_renderer = CustomRenderer\n\n        form = CustomForm()\n        self.assertTrue(isinstance(form.renderer, CustomRenderer))\n        \n        class SubForm(CustomForm):\n            pass\n        \n        sub_form = SubForm()\n        self.assertTrue(isinstance(sub_form.renderer, CustomRenderer))\n", "    def test_renderer_attribute_overrides_kwargs(self):\n        custom_renderer = CustomRenderer()\n        form = Form(renderer=DjangoTemplates())  \n        form = Form(renderer=custom_renderer)\n\n        self.assertEqual(form.renderer, custom_renderer)\n", "    def test_custom_renderer_instance(self):\n        custom = CustomRenderer()\n        form = Form(renderer=custom)\n        self.assertEqual(form.renderer, custom)\n", "    def test_renderer_priority(self):\n        class FormA(Form):\n            default_renderer = CustomRenderer\n\n        class FormB(FormA):\n            default_renderer = DjangoTemplates()\n\n        form_a = FormA()\n        form_b = FormB()\n        self.assertTrue(isinstance(form_a.renderer, CustomRenderer))\n        self.assertEqual(form_b.renderer, DjangoTemplates())\n"], "sample_992": ["def test_matrix_operations():\n    p = NumPyPrinter()\n    A = p.doprint(symbols('A B C')[:, 1])\n    B = p.doprint(symbols('D E F')[:, 2])\n    C = p.doprint(symbols('G H I')[:, 3])\n    assert A == 'numpy.array([[0], [0], [0]])'\n    assert B == 'numpy.array([[0], [0], [0]])'\n    assert C == 'numpy.array([[0], [0], [0]])'\n    dot_product = p.doprint(A.dot(B))\n    assert dot_product == '(numpy.array([[0], [0], [0]])).dot(numpy.array([[0], [0], [0]]))'\n    mat_mul = p.doprint(A.dot(B).dot(C))\n    assert mat_mul == '(numpy.array([[0], [0], [0]])).dot(numpy.array([[0], [0], [0]])).dot(numpy.array([[0], [0], [0]]))'\n", "    def test_user_functions():\n        p = PythonCodePrinter(user_functions={'myfunc': lambda x: x**2})\n        assert p.doprint(p.known_functions['myfunc'](x)) == 'myfunc(x)'\n        p = PythonCodePrinter(user_constants={'myconst': 5})\n        assert p.doprint(p.known_constants['myconst']) == 'myconst'\n\n\n\n", "compilation error", "def test_NumPyPrinter_matrix_multiplication():\n    p = NumPyPrinter()\n    a = [[1, 2], [3, 4]]\n    b = [[5, 6], [7, 8]]\n    expr = p.printer(a @ b)\n    assert expr == 'numpy.dot(numpy.array([[1, 2], [3, 4]]), numpy.array([[5, 6], [7, 8]]))'\n\n", "    def test_CustomPrintedObject():\n        print_obj = CustomPrintedObject()\n        assert pycode(print_obj, printer=NumPyPrinter()).strip() == 'numpy'\n        assert pycode(print_obj, printer=MpmathPrinter()).strip() == 'mpmath'\n"], "sample_108": ["    def test_reverse_value_error_raises_exception(self):\n        @DynamicConverter.register_to_url\n            raise ValueError()\n        with self.assertRaises(ValueError):\n            reverse('dynamic-value', kwargs={'dynamic': '1'})\n", "    def test_resolve_to_url_error_means_no_match(self):\n        @DynamicConverter.register_to_url\n            raise ValueError()\n        with self.assertRaises(Resolver404):\n            resolve('/dynamic/abc/')\n\n\n\n", "    def test_resolve_value_error_includes_original_path(self):\n        @DynamicConverter.register_to_python\n            raise ValueError('Something went wrong')\n        with self.assertRaises(Resolver404) as context:\n            resolve('/dynamic/abc/')\n        self.assertIn('Something went wrong', str(context.exception))\n", "    def test_reverse_value_error_makes_reverse_fail(self):\n        @DynamicConverter.register_to_url\n            raise ValueError()\n        with self.assertRaises(ValueError):\n            reverse('dynamic', kwargs={'dynamic': 'abc'})\n", "    def test_reverse_value_error_raises_exception(self):\n        @DynamicConverter.register_to_url\n            raise ValueError()\n        with self.assertRaises(ImproperlyConfigured):\n            reverse('dynamic-converter', kwargs={'some_key': 'abc'})\n\n\n\n"], "sample_335": ["    def test_decimalfield_support_thousand_separator(self):\n        f = DecimalField(localize=True)\n        self.assertEqual(f.clean('1 001,10'), decimal.Decimal(\"1001.10\"))\n        self.assertEqual(f.clean('1001,10'), decimal.Decimal(\"1001.10\"))\n", "    def test_decimalfield_empty_string(self):\n        f = DecimalField(required=False)\n        self.assertIsNone(f.clean(''))\n\n\n\n", "    def test_decimalfield_empty_strings(self):\n        f = DecimalField(max_digits=4, decimal_places=2)\n        self.assertIsNone(f.clean(\"\"))\n        self.assertIsNone(f.clean(\"   \"))\n        self.assertIsNone(f.clean(\"\\t\"))\n", "    def test_decimalfield_invalid_input_types(self):\n        f = DecimalField()\n        with self.subTest(value=True):\n            with self.assertRaises(ValidationError):\n                f.clean(True)\n        with self.subTest(value=False):\n            with self.assertRaises(ValidationError):\n                f.clean(False)\n        with self.subTest(value='abc'):\n            with self.assertRaises(ValidationError):\n                f.clean('abc')\n        with self.subTest(value=None):\n            self.assertIsNone(f.clean(None))\n\n\n", "    def test_decimalfield_empty_string_and_none(self):\n        f = DecimalField()\n        self.assertIsNone(f.clean(''))\n        self.assertIsNone(f.clean(None))\n\n\n\n"], "sample_51": ["    def test_iso8601_duration(self):\n        test_values = (\n            ('P1D', timedelta(days=1)),\n            ('P1DT2H', timedelta(days=1, hours=2)),\n            ('P1DT2H30M', timedelta(days=1, hours=2, minutes=30)),\n            ('P1DT2H30M15S', timedelta(days=1, hours=2, minutes=30, seconds=15)),\n            ('P1DT2H30M15S.123456', timedelta(days=1, hours=2, minutes=30, seconds=15, microseconds=123456)),\n            ('P1D-2H', timedelta(days=1, hours=-2)),\n            ('P1D-2H30M', timedelta(days=1, hours=-2, minutes=-30)),\n            ('PT2H30M15S', timedelta(hours=2, minutes=30, seconds=15)),\n            ('PT2H30M15S.123456', timedelta(hours=2, minutes=30, seconds=15, microseconds=123456)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n", "    def test_iso8601_duration(self):\n        test_values = (\n            ('P1D', timedelta(days=1)),\n            ('P24H', timedelta(hours=24)),\n            ('PT2H30M', timedelta(hours=2, minutes=30)),\n            ('PT10S', timedelta(seconds=10)),\n            ('P1DT2H30M', timedelta(days=1, hours=2, minutes=30)),\n            ('P1D23H30M10S', timedelta(days=1, hours=23, minutes=30, seconds=10)),\n            ('P1DT2H30M10S.123456789', timedelta(days=1, hours=2, minutes=30, seconds=10, microseconds=123456789)),\n            ('PT-2H30M', timedelta(hours=-2, minutes=-30)),\n            ('P-1D', timedelta(days=-1)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n\n", "    def test_iso8601_duration_format(self):\n        test_values = (\n            ('PT1H', timedelta(hours=1)),\n            ('PT1D', timedelta(days=1)),\n            ('PT1H30M', timedelta(hours=1, minutes=30)),\n            ('PT1H30M15S', timedelta(hours=1, minutes=30, seconds=15)),\n            ('PT1H30M15S.123456', timedelta(hours=1, minutes=30, seconds=15, microseconds=123456)),\n            ('P-1D', timedelta(days=-1)),\n            ('PT1H-30M', timedelta(hours=1, minutes=-30)),\n            ('PT1H30M-15S', timedelta(hours=1, minutes=30, seconds=-15)),\n            ('P1D2H30M15S', timedelta(days=1, hours=2, minutes=30, seconds=15)),\n            ('P1Y', timedelta(days=365)),\n            ('P1Y2M3D4H5M6S', timedelta(days=365 + 60 * 2 + 3, hours=4, minutes=5, seconds=6)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n", "    def test_iso8601_format(self):\n        test_values = (\n            ('P10H', timedelta(hours=10)),\n            ('P10DT15M', timedelta(days=1, minutes=15)),\n            ('P10DT15H30M', timedelta(days=10, minutes=15, seconds=30)),\n            ('PT10H15M30S', timedelta(hours=10, minutes=15, seconds=30)),\n            ('P10DT15H30M2.5S', timedelta(days=10, minutes=15, seconds=30, microseconds=500000)),\n            ('P-10H', timedelta(hours=-10)),\n            ('p10d', timedelta(days=10)),\n            ('PT10H15M30.123456S', timedelta(hours=10, minutes=15, seconds=30, microseconds=123456)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n", "    def test_iso8601_duration(self):\n        test_values = (\n            ('P1D', timedelta(days=1)),\n            ('P1D2H3M4S', timedelta(days=1, hours=2, minutes=3, seconds=4)),\n            ('PT1H30M', timedelta(hours=1, minutes=30)),\n            ('PT20S', timedelta(seconds=20)),\n            ('P1Y', timedelta(days=365)),\n            ('P1Y2M3D', timedelta(days=365, days=2, days=3)),\n            ('P1Y2M3DT2H3M4S', timedelta(days=365, days=2, days=3, hours=2, minutes=3, seconds=4)),\n            \n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n"], "sample_979": ["def test_matrix_symbol_subs():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    C = MatrixSymbol('C', m, l)\n\n    expr = A*B + C\n    assert expr.subs(B, C) == A*C + C\n\n    expr = (A*B).transpose()\n    assert expr.subs(B, C).doit() == (A*C).transpose()\n", "def test_matrix_sub():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    C = MatrixSymbol('C', n, n)\n    assert (A - A).shape == A.shape\n    assert (A - B).shape == A.shape\n    assert (A - B).is_Matrix\n\n    with raises(ShapeError):\n        A - C\n    with raises(TypeError):\n        A - 5\n\n\n    D = MatrixSymbol('D', 2, 2)\n    assert (D - D).doit() == ZeroMatrix(2, 2)\n", "def test_sparse_matrix():\n    A = MatrixSymbol('A', 3, 4)\n    spA = SparseMatrix({(0, 0): 2, (1, 2): 5})\n    assert isinstance(spA + A, SparseMatrix)\n    assert isinstance(spA*A, SparseMatrix)\n    assert isinstance(spA*2, SparseMatrix)\n    assert spA.shape == (3, 4)\n    assert spA[0,0] == 2\n    assert spA[1,2] == 5\n", "def test_transpose_commutativity():\n    A = MatrixSymbol('A', n, m)\n    assert (Transpose(A)).T == A\n    assert (A.T).T == A\n\n\n\n", "def test_matrix_subs():\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    C = MatrixSymbol('C', 3, 3)\n\n    M = MatAdd(A, MatPow(B, 2))\n\n    assert M.subs(B, C) == MatAdd(A, MatPow(C, 2))\n    assert M.subs(A, Identity(3)) == MatAdd(Identity(3), MatPow(B, 2))\n"], "sample_353": ["    def test_create_permissions_existing_content_types(self):\n        # Simulate ContentTypes already created for the auth app.\n        permission_content_type = ContentType.objects.get_or_create(\n            app_label='auth', model='permission')[0]\n        with self.assertNumQueries(0):\n            create_permissions(self.app_config, verbosity=0)\n\n        self.assertEqual(Permission.objects.count(),\n                         len(Permission._meta.permissions) + len(Permission._meta.default_permissions))\n\n\n\n", "    def test_custom_permissions(self):\n        permission_content_type = ContentType.objects.get_by_natural_key('auth', 'permission')\n        # Create custom permissions in app configuration.\n        self.app_config.permissions = [\n            ('my_custom_permission', 'Some custom permission')\n        ]\n        create_permissions(self.app_config, verbosity=0)\n        # Assert that custom permissions are created.\n        self.assertEqual(Permission.objects.filter(\n            content_type=permission_content_type,\n            codename='my_custom_permission',\n        ).count(), 1)\n\n\n\n", "    def test_permissions_creation_with_existing_permissions(self):\n        # Simulate existing permissions for testing\n        existing_permissions = [\n            Permission(\n                content_type=ContentType.objects.get_for_model(User),\n                codename='view_user',\n                name='Can view user',\n            ),\n            Permission(\n                content_type=ContentType.objects.get_for_model(Group),\n                codename='add_group',\n                name='Can add group',\n            ),\n        ]\n        Permission.objects.bulk_create(existing_permissions)\n\n        # Call create_permissions again\n        create_permissions(self.app_config, verbosity=0)\n\n        # Check that existing permissions remain unchanged\n        self.assertEqual(Permission.objects.count(), len(existing_permissions) + 5)\n\n        # New permissions should be created\n        self.assertTrue(Permission.objects.filter(codename='my_custom_permission').exists())\n", "    def test_custom_permissions(self):\n        custom_permissions = [\n            ('auth_custom_permission', 'Custom Permission'),\n            ('auth_another_custom_permission', 'Another Custom Permission'),\n        ]\n        Permission._meta.permissions = custom_permissions\n        create_permissions(self.app_config)\n        self.assertEqual(Permission.objects.count(), len(custom_permissions))\n        for permission in custom_permissions:\n            self.assertEqual(Permission.objects.filter(\n                codename=permission[0],\n                name=permission[1],\n            ).count(), 1)\n\n        self.assertNumQueries(1)  # One query to count all permissions\n\n\n\n", "    def test_create_permissions_handles_existing_permissions(self):\n        permission_content_type = ContentType.objects.get_by_natural_key('auth', 'permission')\n        Permission._meta.permissions = [\n            ('my_custom_permission', 'Some permission'),\n            ('another_custom_permission', 'Another permission'),\n        ]\n        create_permissions(self.app_config, verbosity=0)\n\n        existing_permissions = Permission.objects.filter(\n            content_type=permission_content_type,\n        )\n        self.assertEqual(len(existing_permissions), 5)  # 3 default + 2 custom\n\n        create_permissions(self.app_config, verbosity=0)\n        # Existing permissions shouldn't be re-created\n        self.assertEqual(len(Permission.objects.all()), 5)\n\n\n"], "sample_532": ["def test_contour_levels_with_filled():\n    x, y = np.meshgrid(np.linspace(0, 10, 100), np.linspace(0, 10, 100))\n    z = np.sin(x * y)\n    fig, ax = plt.subplots()\n    cs = ax.contourf(x, y, z, levels=np.linspace(0, 1, 5))\n    assert len(cs.levels) == 5\n\n\n\n\n", "def test_contour_fill_negative_contours():\n    from matplotlib import patches\n\n    x = np.arange(0, 10, 0.1)\n    y = np.arange(0, 10, 0.1)\n    X, Y = np.meshgrid(x, y)\n\n    z = -np.sin(np.sqrt(X**2 + Y**2))\n\n    fig, ax = plt.subplots()\n    CS = ax.contourf(X, Y, z, levels=[-1, 0, 1],\n                     cmap='viridis', extend='both')\n\n    ax.clabel(CS, inline=True)\n\n    patches = CS.collections[:2]  # First two are for negative contours\n    assert [patch.get_facecolor() for patch in patches] == [\n        (0.3, 0.4, 0.5, 1),\n        (0.8, 0.9, 0.6, 1)\n    ]\n", "def test_contour_invalid_input():\n    with pytest.raises(ValueError, match=\"Input data is not 2-dimensional\"):\n        plt.contour([1, 2, 3])\n    \n    with pytest.raises(ValueError, match=\"Input data is not 2-dimensional\"):\n        plt.contour(np.arange(9).reshape((3, 3)), np.arange(9).reshape((3, 3))) \n", "def test_contour_zero_data():\n    fig, ax = plt.subplots()\n    cs = ax.contour(np.zeros((10, 10)))\n    assert cs.levels == [0]\n    assert len(cs.collections) == 1\n\n\n\n", "def test_contour_with_invalid_levels():\n    z = np.array([[1, 2], [3, 4]])\n    with pytest.raises(ValueError, match=r\"Levels must be a non-empty sequence\"):\n        plt.contour(z)\n"], "sample_2": ["def test_wcs_keyword_removal_for_invalid_wcs_header():\n    \"\"\"\n    Test that when a header with an invalid WCS is provided,\n    no WCS keywords remain after processing.\n    \"\"\"\n    # Construct a header with an invalid WCS\n    invalid_wcs_header = fits.Header()\n    invalid_wcs_header['SIMPLE'] = 'TRUE'\n    invalid_wcs_header['BITPIX'] = 16\n    invalid_wcs_header['NAXIS'] = 2\n    invalid_wcs_header['NAXIS1'] = 1024\n    invalid_wcs_header['NAXIS2'] = 1024\n    invalid_wcs_header['CDELT1'] = 0.1\n    invalid_wcs_header['CDELT2'] = 0.1\n\n    # Attempt to read the header as a WCS object and check for errors\n    with pytest.raises(astropy.wcs.exceptions.WCSParseError):\n        WCS(invalid_wcs_header)\n\n\n", "def test_read_with_invalid_wcs_header():\n    from astropy.nddata import CCDData\n    from astropy.wcs import WCS\n\n    # Create a header with a malformed WCS\n    header = fits.Header()\n    header['CTYPE1'] = 'RA---TAN'\n    header['CTYPE2'] = 'DEC---TAN'\n    header.update({'CDELT1': 0.1, 'CDELT2': 0.1})\n    header.update({'CRVAL1': 0, 'CRVAL2': 0})\n    header.update({'NAXIS': 2, 'NAXIS1': 100, 'NAXIS2': 100})\n\n    # Simulate a FITS file with this header and some data\n    hdu = fits.PrimaryHDU(np.random.rand(100, 100), header=header)\n    \n    # Try to read the FITS file with CCDData\n    with pytest.raises(astropy.wcs.exceptions.WCSInputError) as exc:\n        ccd = CCDData.read(hdu)\n    assert 'Could not parse WCS keywords' in str(exc.value)\n", "def test_writedata_with_header_keywords(tmpdir):\n    ccd_data = create_ccd_data()\n    header_keywords = {'COMMENT': 'This is a comment',\n                       'OBSERVER': 'Bob'}\n    filename = tmpdir.join('afile.fits').strpath\n    ccd_data.write(filename, header_keywords=header_keywords)\n    hdul = fits.open(filename)\n    assert hdul[0].header['COMMENT'] == 'This is a comment'\n    assert hdul[0].header['OBSERVER'] == 'Bob'\n\n", "def test_ccddata_from_fits_with_invalid_wcs(tmpdir):\n    from astropy.nddata import CCDData\n    from astropy.io import fits\n    import numpy as np\n\n    # Create a FITS file with deliberately invalid WCS information.\n    with fits.open('invalid_wcs.fits', mode='w') as hdul:\n        hdul.append(fits.PrimaryHDU(np.zeros((10, 10))))\n\n        hdul[0].header['SIMPLE'] = 'FALSE'  # Make it non-standard\n        hdul[0].header['CTYPE1'] = 'BAD_CTYPE'\n\n\n    filename = tmpdir.join('invalid_wcs.fits').strpath\n\n    # Try reading the FITS file with CCDData.\n    with pytest.raises(AstropyWarning, match=r\"Invalid WCS information found\"):\n        CCDData.read(filename)\n", "def test_read_with_invalid_header_keyword(tmpdir):\n    # Test that a ValueError is raised if a file has an invalid header keyword\n    # astropy/ccddata#667\n    \n    hdr_txt = textwrap.dedent('''\n    SIMPLE  =                    T / Fits standard\n    BITPIX   =                   16 / Bits per pixel\n    NAXIS    =                    2 / Number of axes\n    NAXIS1   =                 1104 / Axis length\n    NAXIS2   =                 4241 / Axis length\n    CRVAL1   =         164.98110962 / Physical value of the reference pixel X\n    CRVAL2   =          44.34089279 / Physical value of the reference pixel Y\n    CRPIX1   =                -34.0 / Reference pixel in X (pixel)\n    CRPIX2   =               2041.0 / Reference pixel in Y (pixel)\n    CDELT1   =           0.10380000 / X Scale projected on detector (#/pix)\n    CDELT2   =           0.10380000 / Y Scale projected on detector (#/pix)\n    CTYPE1   = 'RA---TAN'           / Pixel coordinate system\n    CTYPE2   = 'WAVELENGTH'         / Pixel coordinate system\n    CUNIT1   = 'degree  '           / Units used in both CRVAL1 and CDELT1\n    CUNIT2   = 'nm      '           / Units used in both CRVAL2 and CDELT2\n    BAD_KEY = 'hello' \n    ''')\n\n    data = np.ones((4241, 1104))\n    hdul = fits.HDUList([fits.PrimaryHDU(data, header=hdr_txt)])\n    filename = tmpdir.join('afile.fits').strpath\n    hdul.writeto(filename)\n    with pytest.raises(ValueError):\n        CCDData.read(filename, unit='adu')\n\n\n"], "sample_718": ["compilation error", "compilation error", "def test_check_estimators_data_not_an_array():\n    # test that estimators raise an error when provided non-array data\n\n    # Check that estimator doesn't allow non-array data for X and y\n\n        from sklearn.datasets import load_iris\n        iris = load_iris()\n        X, y = iris.data, iris.target\n        estimator = type(estimator_name)()\n        set_random_state(estimator)\n\n        # Test with invalid data types\n        for data_type in [str, list, dict, bool, np.bool_]:\n            X_invalid = data_type(X)\n            y_invalid = data_type(y)\n            with ignore_warnings(category=DeprecationWarning):\n                with pytest.raises(TypeError) as e:\n                    estimator.fit(X_invalid, y_invalid)\n            assert_true(\n                \"Input X must be a numpy array\" in str(e.value)\n                or \"Input y must be a numpy array\" in str(e.value)\n            )\n\n    # test that 'predict' with non-array data also raises an error\n        from sklearn.datasets import load_iris\n        iris = load_iris()\n        estimator = type(estimator_name)()\n        estimator.fit(iris.data, iris.target)\n        X_invalid = np.array(iris.data, dtype=str)\n        with ignore_warnings(category=DeprecationWarning):\n            with pytest.raises(TypeError) as e:\n                estimator.predict(X_invalid)\n        assert_true(\n            \"Input X must be a numpy array\" in str(e.value)\n        )\n\n    test_data_not_an_array(LinearRegression)\n    test_data_not_an_array(SGDClassifier)\n    test_data_not_an_array(RandomForestClassifier)\n    test_predict_data_not_an_array(LinearRegression)\n    test_predict_data_not_an_array(SGDClassifier)\n    test_predict_data_not_an", "        def get_params(self, deep=True):\n            return {'sample_weight': None}\n", "def test_check_estimator_sparse_matrix():\n    from sklearn.datasets import make_sparse_toy\n    X, y = make_sparse_toy(n_samples=10, n_features=5, random_state=0)\n    for Estimator in [LinearRegression,\n                      SGDClassifier]:\n        with ignore_warnings(category=FutureWarning):\n            est = Estimator()\n        set_random_state(est)\n        est.fit(X, y)\n        assert_true(sp.issparse(est.coef_))\n\n\n\n"], "sample_570": ["    def test_empty_errorbars(self):\n\n        agg = EstimateAggregator(\"mean\", \"sd\")\n        out = agg(pd.DataFrame(), \"x\")\n        assert pd.isna(out[\"x\"])\n        assert pd.isna(out[\"xmin\"])\n        assert pd.isna(out[\"xmax\"])\n", "    def test_errorbar_kwargs(self, long_df):\n        agg = EstimateAggregator(\"mean\", \"se\", error_kwargs=dict(level=0.99))\n        out = agg(long_df, \"y\")\n\n        se = long_df[\"y\"].sem()\n        crit_val = 2.576  # z-score for 99% confidence\n        assert out[\"ymin\"] == (long_df[\"y\"].mean() - crit_val * se)\n        assert out[\"ymax\"] == (long_df[\"y\"].mean() + crit_val * se)\n", "    def test_bivariate_estimator(self, long_df):\n        agg = EstimateAggregator(lambda x, y: x.mean(), errorbar=\"se\")\n        out = agg(long_df, \"x\", \"y\")\n        assert out[\"x\"] == long_df[\"x\"].mean()\n        assert out[\"ymin\"] == (long_df[\"x\"].mean() - long_df[\"x\"].sem())\n        assert out[\"ymax\"] == (long_df[\"x\"].mean() + long_df[\"x\"].sem())\n\n", "    def test_errorbar_kwargs(self, long_df):\n        agg = EstimateAggregator(\"mean\", \"se\")\n        out = agg(long_df, \"x\", level=99)\n        assert out[\"x\"] == long_df[\"x\"].mean()\n        assert out[\"xmin\"] == (long_df[\"x\"].mean() - long_df[\"x\"].sem() * 2.576)\n        assert out[\"xmax\"] == (long_df[\"x\"].mean() + long_df[\"x\"].sem() * 2.576)\n", "    def test_bootstrap_ci(self, long_df):\n        agg = EstimateAggregator(\"mean\", \"ci\", n_boot=1000)\n        out = agg(long_df, \"y\")\n\n        # Check that the bootstrapped confidence intervals are within \n        # expected ranges\n        assert (out[\"ymin\"] < out[\"y\"]) & (out[\"ymax\"] > out[\"y\"]) \n\n        # Check that the bootstrap confidence intervals are consistent\n        # across multiple runs\n        out2 = agg(long_df, \"y\")\n        assert (out[\"ymin\"] < out2[\"y\"]) & (out[\"ymax\"] > out2[\"y\"])\n\n\n\n"], "sample_11": ["compilation error", "def test_pixel_to_world_values_large_array():\n    wcs = WCS_SPECTRAL_CUBE\n    sll = SlicedLowLevelWCS(wcs, np.s_[30:40, 50:60, 80:90])\n    large_pixel_array = np.arange(1000)\n    with pytest.raises(ValueError, match='Input array is larger than the output array'):\n        sll.pixel_to_world_values(large_pixel_array)\n\n", "def test_slicing_with_invalid_slice():\n    wcs = WCS_SPECTRAL_CUBE\n    with pytest.raises(ValueError, match=\"Slice object must be one of:\"):\n        SlicedLowLevelWCS(wcs, np.s_[:, 1, :])\n", "def test_pixel_to_world_values_with_nan():\n    wcs = SlicedLowLevelWCS(WCS_SPECTRAL_CUBE, np.s_[:, 0, :])\n    with pytest.raises(ValueError, match='Encountered NaN value in pixel array'):\n        wcs.pixel_to_world_values([0, np.nan, 1])\n", "def test_world_to_pixel_values_with_None_type_handling():\n    wcs = WCS_SPECTRAL_CUBE_NONE_TYPES\n\n    # Test case where world values are None for the spectral dimension\n    world_values = [10, None, 25]\n    with pytest.raises(ValueError, match=\"Could not convert to pixel values: \"\n                        \"One or more world values are None for a dimension where \"\n                        \"a valid pixel value is expected\"):\n        wcs.world_to_pixel_values(world_values)\n\n    # Test case where world values are None for the celestial dimensions\n    world_values = [None, 20, 25]\n    with pytest.raises(ValueError, match=\"Could not convert to pixel values: \"\n                        \"One or more world values are None for a dimension where \"\n                        \"a valid pixel value is expected\"):\n        wcs.world_to_pixel_values(world_values)\n"], "sample_631": ["    def test_ignored_argument_names_no_message_local_scope(self):\n        node = astroid.parse(\n            \"\"\"\n                pass\n            inner(1)\n        \"\"\"\n        )\n        with self.assertNoMessages():\n            self.walk(node)\n\n\n", "    def test_ignore_args_flag(self):\n        node = astroid.parse(\n            \"\"\"\n            pass\n        \"\"\"\n        )\n        with self.assertNoMessages():\n            self.walk(node)\n", "    def test_unused_variable_in_lambda(self):\n        node = astroid.parse(\n            \"\"\"\n            lambda x: x + 1\n        \"\"\"\n        )\n        with self.assertNoMessages() :\n            self.walk(node)\n", "    def test_cell_var_from_loop_in_generator(self):\n        \"\"\"Make sure cell variables are checked correctly within generators.\"\"\"\n        node = astroid.parse(\n            \"\"\"\n            x = 1\n            yield lambda: x\n        for i in generator():\n            print(i())\n        \"\"\"\n        )\n        with self.assertAddsMessages(\n            Message(\"cell-var-from-loop\", node=node.body[1].value.attr, args=\"x\")\n        ):\n            self.walk(node)\n\n", "    def test_nonlocal_in_lambda(self):\n        \"\"\"Ensure nonlocal usage in lambda functions is handled correctly.\"\"\"\n        node = astroid.parse(\n            \"\"\"\n            x = 1\n            lambda: nonlocal x\n        \"\"\"\n        )\n        with self.assertNoMessages():\n            self.walk(node)\n"], "sample_1011": ["def test_limits_and_integrals():\n    assert mcode(integrate(sin(x), (x, 0, pi))) == \"integral(sin(x), x, 0, pi)\"\n    assert mcode(integrate(x**2, (x, 1, 2), evaluate=False)) == (\n        \"integral(x.^2, x, 1, 2)\"\n    ).replace(\" \", \"\")\n\n\n\n", "def test_trigonometric_with_symbols():\n    assert mcode(sin(x + y)) == 'sin(x + y)'\n    assert mcode(cos(x*y)) == 'cos(x.*y)'\n    assert mcode(tan(x**2 + y)) == 'tan(x.^2 + y)'\n    assert mcode(atan2(y, x)) == 'atan2(y, x)'\n\n", "def test_symbols_in_matrices():\n    x = symbols('x')\n    y = symbols('y')\n    A = Matrix([[x, 2, x*y], [y**2, 3, x]])\n    expected = \"[x 2 x.*y; y^2 3 x]\"\n    assert mcode(A) == expected\n", "def test_rising_factorial():\n    assert mcode(RisingFactorial(x, 3)) == \"gamma(x + 3)/gamma(x)\"\n\n\n", "def test_polygamma():\n    assert mcode(polygamma(1, x)) == 'psi(x)'\n    assert mcode(polygamma(2, x)) == \"polygamma(2, x)\" \n    assert mcode(polygamma(3, x)) == \"polygamma(3, x)\" \n    assert mcode(polygamma(n, x)) == \"polygamma(n, x)\" \n"], "sample_37": ["def test_distortion_coefficients():\n    \"\"\"\n    Test for reading distortion coefficients.\n    \"\"\"\n    hdr = get_pkg_data_contents('data/sip.hdr')\n    w = wcs.WCS(hdr)\n    assert isinstance(w.distortion, wcs.Distortion)\n    assert isinstance(w.distortion, wcs.Sip)\n    assert w.distortion.coefficients.shape == (4, 4)\n\n    \n", "compilation error", "def test_wcs_wcslib_warn():\n    \"\"\"\n    Test for #5553.\n\n    This test ensures that when a WCS object is created with a header\n    that has conflicting SIP parameters (e.g., both A_ORDER and\n    B_ORDER are set), a warning is issued and the parameters\n    are adjusted according to the SIP specifications.\n\n    \"\"\"\n    header = get_pkg_data_contents(\"data/sip_conflict.hdr\")\n    with pytest.warns(UserWarning) as warning:\n        w = wcs.WCS(header)\n    assert \"conflicting SIP parameters detected\" in str(warning[0].message)\n    assert w.sip.a_order == 4\n    assert w.sip.b_order == 4\n\n", "compilation error", "def test_wcs_with_missing_crpix():\n    \"\"\"\n    Test WCS creation with missing CRPIX values.\n    \"\"\"\n    hdr = get_pkg_data_contents(\"data/missing_crpix.hdr\")\n    w = wcs.WCS(hdr)\n    assert w.wcs.crpix[0] is None\n    assert w.wcs.crpix[1] is None\n    ra, dec = w.wcs_pix2world([0, 0], 0)\n    assert np.isnan(ra)\n    assert np.isnan(dec)\n"], "sample_838": ["compilation error", "def test_column_transformer_sparse_transformer_with_remainder():\n    X_array = np.array([[0, 1, 2], [2, 4, 6], [8, 6, 4]]).astype('float').T\n\n    ct = ColumnTransformer([('trans1', StandardScaler(), [0])],\n                           remainder=SparseMatrixTrans(),\n                           sparse_threshold=0.5)\n\n    X_trans = ct.fit_transform(X_array)\n\n    assert sparse.issparse(X_trans)\n    assert X_trans.shape == (3, 4)  # 3 features from trans1 + 1 from remainder\n\n    # Check if the sparse matrix has the correct structure\n    assert (X_trans.toarray()[:, 0] == ct.transformers_[0][1].mean_)\n    assert (X_trans.toarray()[:, 1] == ct.transformers_[0][1].scale_)\n    assert (X_trans.toarray()[:, 2] == X_array[:, 1]).all()\n    assert (X_trans.toarray()[:, 3] == X_array[:, 2]).all()\n\n\n\n", "def test_column_transformer_sparse_remainder_transformer_with_remainder_array():\n    X_array = np.array([[0, 1, 2],\n                        [2, 4, 6],\n                        [8, 6, 4]]).T\n\n    ct = ColumnTransformer([('trans1', Trans(), [0])],\n                           remainder=SparseMatrixTrans(),\n                           remainder_array=[1, 0])\n\n    X_trans = ct.fit_transform(X_array)\n    assert sparse.issparse(X_trans)\n\n    # 'trans1' is applied to the first column only, \n    # remainder_array [1, 0] indicates that only the \n    # second column should be transformed.\n    #  SparseMatrixTrans creates 3 features for each column\n    assert X_trans.shape == (3, 3)\n    assert_array_equal(X_trans.toarray(), np.eye(3))\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == 'remainder'\n    assert isinstance(ct.transformers_[-1][1], SparseMatrixTrans)\n    assert_array_equal(ct.transformers_[-1][2], [1])\n", "compilation error", "compilation error"], "sample_1183": ["def test_Domain_round():\n    I = S.ImaginaryUnit\n    a, b = [ZZ_I.convert(x) for x in (3 + 2*I, 5 - 4*I)]\n    assert ZZ_I.round(a) == ZZ_I(3, 2)\n    assert ZZ_I.round(b) == ZZ_I(5, -4)\n    assert ZZ_I.round(a/2) == ZZ_I(1, 1)\n    assert ZZ_I.round(b/2) == ZZ_I(2, -2)\n\n\n\n", "def test_Domain_mod():\n    I = S.ImaginaryUnit\n    a, b = [ZZ_I.convert(x) for x in (2 + 3*I, 5 - I)]\n    c = ZZ_I(7, 2)\n\n    assert ZZ_I.mod(a, c) == ZZ_I(2, 2)\n    assert ZZ_I.mod(b, c) == ZZ_I(5, -1)\n    assert ZZ_I.mod(c, c) == ZZ_I(0, 0)\n\n    raises(ZeroDivisionError, lambda: ZZ_I.mod(a, ZZ_I(0, 0)))\n    raises(TypeError, lambda: ZZ_I.mod(a, 'abc'))\n", "def test_Domain_gcd():\n    assert ZZ.gcd(12, 8) == 4\n    assert QQ.gcd(12/3, 8/4) == 4/4\n    assert RR.gcd(3.14, 2.71) == 1\n    assert CC.gcd(2 + 3*I, 4 - I) == 1 + 2*I\n    assert ZZ_I.gcd(1 + 2*I, 3 - I) == 1\n\n    assert ZZ[x].gcd(x**2, x + 1) == ZZ([1])\n    assert ZZ_I[x].gcd(x**2 + 1, x + I) == ZZ_I([1])\n\n\n\n", "    def test_Domain_to_sympy():\n        ZZ_convert = [ZZ(x) for x in (2, -2, 0, 1j, -1j)]\n        for x in ZZ_convert:\n            assert ZZ.to_sympy(x) == x\n        QQ_convert = [QQ(x) for x in (2/3, -2/3, 0, 1j, -1j)]\n        for x in QQ_convert:\n            assert QQ.to_sympy(x) == x\n        RR_convert = [RR(x) for x in (2.5, -2.5, 0, 1j, -1j)]\n        for x in RR_convert:\n            assert RR.to_sympy(x) == x\n        CC_convert = [CC(x) for x in (2.5 + 3j, -2.5 - 3j, 0, 1j, -1j)]\n        for x in CC_convert:\n            assert CC.to_sympy(x) == x\n        for dom in (ZZ, QQ, RR, CC):\n            assert dom.to_sympy(dom(S.One)) == S.One\n            assert dom.to_sympy(dom(-S.One)) == -S.One\n            assert dom.to_sympy(dom(S(2))) == S(2)\n            assert dom.to_sympy(dom(-S(2))) == -S(2)\n\n        ZZ_I_convert = [ZZ_I(x) for x in (2 + 3j, -2 - 3j, 0, 1j, -1j)]\n        for x in ZZ_I_convert:\n            assert ZZ_I.to_sympy(x) == x\n        QQ_I_convert = [QQ_I(x) for x in (2/3 + 3j/4, -2/3 - 3j/4, 0, 1j, -1j)]\n        for x in QQ_I_convert:\n            assert QQ_I.to_sympy(x) == x\n\n\n\n", "compilation error"], "sample_177": ["    def test_multiple_abstract_bases(self):\n        A = self.create_model(\"A\", abstract=True)\n        B = self.create_model(\"B\", abstract=True)\n        C = self.create_model(\"C\", bases=(A, B,))\n        self.assertRelated(A, [C])\n        self.assertRelated(B, [C])\n        self.assertRelated(C, [])\n", "    def test_proxy_fk_through(self):\n        A = self.create_model(\"A\", foreign_keys=[models.ForeignKey('B', models.CASCADE)])\n        B = self.create_model(\"B\", proxy=True)\n        C = self.create_model(\"C\", foreign_keys=[models.ForeignKey('A', models.CASCADE)])\n        self.assertRelated(A, [B, C])\n        self.assertRelated(B, [])\n        self.assertRelated(C, [A])\n\n", "    def test_proxy_multi_inheritance(self):\n        A = self.create_model(\"A\")\n        B = self.create_model(\"B\", bases=(A,))\n        C = self.create_model(\"C\", bases=(A,))\n        D = self.create_model(\"D\", bases=(B, C, ))\n        self.assertRelated(A, [B, C, D])\n        self.assertRelated(B, [D])\n        self.assertRelated(C, [D])\n        self.assertRelated(D, [])\n\n", "    def test_unique_together_and_check_constraints(self):\n        class ModelWithConstraints(models.Model):\n            field1 = models.CharField(max_length=50)\n            field2 = models.CharField(max_length=50)\n            field3 = models.IntegerField()\n\n            class Meta:\n                unique_together = [(\"field1\", \"field2\")]\n                constraints = [\n                    models.CheckConstraint(check=models.Q(field3__gt=0), name='field3_gt_0'),\n                ]\n\n        state = ModelState.from_model(ModelWithConstraints)\n        model_constraints = ModelWithConstraints._meta.constraints\n        state_constraints = state.options['constraints']\n        self.assertEqual(model_constraints, state_constraints)\n        self.assertEqual(len(state.options['unique_together']), 1)\n        self.assertEqual(state.options['unique_together'][0], (\"field1\", \"field2\"))\n\n\n\n", "    def test_abstract_proxy(self):\n        A = self.create_model(\"A\", abstract=True)\n        B = self.create_model(\"B\", bases=(A,), proxy=True)\n        self.assertRelated(A, [B])\n        self.assertRelated(B, [])\n"], "sample_881": ["def test_label_ranking_average_precision_score_with_zero_scores():\n    y_true = np.array([[1, 0, 0], [0, 0, 1]])\n    y_score = np.array([[0, 0, 0], [0, 0, 0]])\n    result = label_ranking_average_precision_score(y_true, y_score)\n    assert result == pytest.approx(0)\n\n\n", "def test_label_ranking_average_precision_score_ties_handling():\n    # Handling ties in y_true\n    y_true = np.array([\n        [1, 0, 1, 0],\n        [1, 1, 0, 0],\n        [0, 1, 1, 0],\n        [0, 0, 1, 1],\n    ])\n    y_score = np.array([\n        [0.8, 0.2, 0.7, 0.1],\n        [0.7, 0.3, 0.6, 0.4],\n        [0.6, 0.5, 0.4, 0.5],\n        [0.4, 0.6, 0.5, 0.7],\n    ])\n    result = label_ranking_average_precision_score(y_true, y_score)\n    assert result == pytest.approx(0.5)  \n", "def test_label_ranking_average_precision_score_empty_sets():\n    # Test cases with empty labels\n    # Test case for empty y_true and y_score\n    assert label_ranking_average_precision_score([], []) == 0.0\n\n    # Test case with empty y_true\n    y_score = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    assert label_ranking_average_precision_score([], y_score) == 0.0\n\n    # Test case with empty y_score\n    y_true = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    assert label_ranking_average_precision_score(y_true, []) == 0.0\n\n", "def test_label_ranking_average_precision_score_ties():\n    # Test handling of ties in labels\n    y_true = csr_matrix([[1, 0, 0], [1, 0, 1]])\n    y_score = np.array([[0.5, 0.9, 0.9], [0.6, 0.3, 0.9]])\n    result = label_ranking_average_precision_score(y_true, y_score)\n    assert result == pytest.approx(0.6667)  # Adjusted for ties\n\n", "def test_label_ranking_average_precision_score_with_no_samples():\n    # Test that it doesn't raise an error when y_true has no samples\n    y_true = csr_matrix([])\n    y_score = np.array([])\n    result = label_ranking_average_precision_score(y_true, y_score)\n    assert result == 0\n\n\n"], "sample_107": ["    def test_cleanse_setting_nested_dict(self):\n        self.assertEqual(\n            cleanse_setting('FOOBAR', {'SECRET_KEY': 'top_secret'}),\n            {'FOOBAR': CLEANSED_SUBSTITUTE}\n        )\n\n        self.assertEqual(\n            cleanse_setting('FOOBAR.NESTED', {'FOOBAR': {'SECRET_KEY': 'top_secret'}}),\n            {'FOOBAR': {'NESTED': CLEANSED_SUBSTITUTE}}\n        )\n", "    def test_cleanse_setting_nested_dictionaries(self):\n        sensitive_data = {\n            'user_data': {\n                'password': 'secret',\n                'email': 'user@example.com',\n            },\n            'config': {\n                'API_KEY': 'abc-123',\n                'SECRET_KEY': 'some_other_secret',\n            }\n        }\n        cleansed_data = cleanse_settings(sensitive_data)\n        self.assertEqual(cleansed_data['user_data']['password'], CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleansed_data['config']['API_KEY'], CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleansed_data['config']['SECRET_KEY'], CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleansed_data['user_data']['email'], 'user@example.com')\n", "    def test_cleanse_setting_nested_dict(self):\n        self.assertEqual(cleanse_setting('my_settings.sensitive', {'key': 'super_secret'}), {'key': CLEANSED_SUBSTITUTE})\n        self.assertEqual(cleanse_setting('my_settings.not_sensitive', {'key': 'super_secret'}), {'key': 'super_secret'})\n\n\n\n", "    def test_cleanse_setting_multiple_words(self):\n        self.assertEqual(cleanse_setting('SECRET_KEY', 'my_secret_key'), CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleanse_setting('API_TOKEN', 'my_awesome_token_123'), CLEANSED_SUBSTITUTE)\n", "    def test_cleanse_setting_nested_dict(self):\n        complex_dict = {'sensitive': 'secret', 'other': 123, 'nested': {'password': 'super_secret'}}\n        cleansed_dict = cleanse_setting_recursive(complex_dict)\n        self.assertEqual(cleansed_dict['sensitive'], CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleansed_dict['other'], 123)\n        self.assertEqual(cleansed_dict['nested']['password'], CLEANSED_SUBSTITUTE)\n"], "sample_897": ["def test_partial_dependence_display_kind_list_centered(\n    pyplot, kind, centered, clf_diabetes, diabetes", "def test_partial_dependence_kind_subsample_interaction(\n    pyplot, clf_diabetes, diabetes", "def test_partial_dependence_display_kind_contour_levels(\n    pyplot, clf_diabetes, diabetes", "def test_partial_dependence_display_grid_resolution(\n    pyplot, clf_diabetes, diabetes", "def test_partial_dependence_display_kind_subsample_error(\n    pyplot, clf_diabetes, diabetes"], "sample_285": ["    def test_nonexistent_dir_in_staticfiles_dirs(self):\n        with self.settings(STATICFILES_DIRS=['/path/that/does/not/exist/']):\n            self.assertEqual(check_finders(None), [\n                Warning(\n                    f\"The directory '/path/that/does/not/exist/' in the \"\n                    f\"STATICFILES_DIRS setting does not exist.\",\n                    id='staticfiles.W004',\n                ),\n            ])\n", "    def test_non_existing_dirs_in_staticfiles_dirs(self):\n        self.assertEqual(check_finders(None), [\n            Warning(\n                f\"The directory '{Path(TEST_ROOT) / 'project' / 'does_not_exist'}\"\n                f\" in the STATICFILES_DIRS setting does not exist.\",\n                id='staticfiles.W004',\n            )\n        ])\n", "    def test_dirs_contains_nonexistent_directory(self):\n        with self.assertRaises(Warning):\n            check_finders(None)\n", "    def test_app_directories_finder_finds_files(self):\n        with mock.patch('django.contrib.staticfiles.finders.get_finders') as get_finders_mock:\n            get_finders_mock.return_value = [\n                get_finder('django.contrib.staticfiles.finders.AppDirectoriesFinder')()\n            ]\n            self.assertEqual(find('app1/css/style.css'), [\n                os.path.join(TEST_ROOT, 'project', 'static', 'app1', 'css', 'style.css')\n            ])\n            self.assertEqual(find('app2/js/script.js', all=True), [\n                os.path.join(TEST_ROOT, 'project', 'static', 'app2', 'js', 'script.js')\n            ])\n\n\n\n", "    def test_dirs_non_existent(self):\n        with self.settings(STATICFILES_DIRS=[TEST_ROOT]):\n            self.assertEqual(check_finders(None), [\n                Warning(\n                    f\"The directory '{TEST_ROOT}' in the STATICFILES_DIRS setting \"\n                    f\"does not exist.\",\n                    id='staticfiles.W004',\n                )\n            ])\n"], "sample_859": ["    def mock_decision_path(self, X, *args, **kwargs):\n        nonlocal calls\n        calls += 1\n        return self.coef_\n", "    def test_lasso_selection(self):\n\n        X, y, _, _ = build_dataset()\n        coef = None\n        for selection in ['cyclic', 'random']:\n            clf = Lasso(selection=selection, max_iter=100)\n            clf.fit(X, y)\n            if coef is None:\n                coef = clf.coef_\n            else:\n                assert_array_almost_equal(clf.coef_, coef, decimal=5)\n\n\n", "    def test_lasso_with_precompute_not_override(monkeypatch, precompute,\n                inner_precompute):\n        lasso = Lasso(n_alphas=1, max_iter=1, precompute=precompute)\n        lasso_cv = LassoCV(precompute=precompute)\n        lasso_cv.fit(X, y)\n        assert lasso_cv._inner_estimator.precompute == inner_precompute\n\n\n\n", "    def _test_precompute_setter(self, X, y):\n        self.precompute = precompute\n        super().fit(X, y)\n        assert self.precompute == precompute\n", "    def _deprecated_fit(self, X, y, precompute=None):\n        if precompute is not None:\n            warnings.warn(\"The `precompute` parameter is deprecated.\"\n                          \"  Use `precompute_Gram` instead.\",\n                          DeprecationWarning)\n        return self.fit(X, y, precompute_Gram=precompute)\n"], "sample_1171": ["def test_issue_17858_bounds():\n    assert Range(-oo, 10).contains(10) == True\n    assert Range(-oo, 10).contains(9) == True\n    assert Range(-oo, 10).contains(8) == True\n    assert Range(-oo, 10).contains(0) == True\n    assert Range(-oo, 10).contains(-oo) == False\n    assert Range(10, oo).contains(10) == False\n    assert Range(10, oo).contains(11) == True\n    assert Range(10, oo).contains(oo) == True\n    assert Range(10, oo).contains(-oo) == False\n    assert Range(10, 10).contains(10) == False\n    assert Range(10, 10).contains(11) == False\n    assert Range(-oo, 10).bounds() == (MinusInfinity, 10)\n    assert Range(10, oo).bounds() == (10, Infinity)\n", "def test_issue_17858b():\n    assert Range(oo, -oo, -1) == Range(-oo, oo, 1)\n    assert Range(-oo, oo, 2) == Range(-oo, oo, 2)\n", "def test_issue_17858b():\n    assert (0 not in Range(-oo, 0, 1))\n    assert (oo not in Range(-oo, 0, 1))\n    assert (-oo in Range(-oo, 0, -1))\n    assert (0 in Range(0, oo, 1))\n    assert (oo in Range(0, oo, 1)) \n", "compilation error", "def test_issue_18080():\n    assert ImageSet(Lambda(x, x**2),\n            Interval(-1, 0)).intersection(S.Integers) == FiniteSet()\n    assert ImageSet(Lambda(x, x**2), Interval(0, 1)).intersection(S.Integers) == FiniteSet(0)\n    assert ImageSet(Lambda(x, x**2),\n            Interval(-oo, 0)).intersection(S.Integers) == FiniteSet()\n    assert ImageSet(Lambda(x, x**2), Interval(0, oo)).intersection(S.Integers) == S.Integers\n    \n"], "sample_1106": ["def test_matmul_broadcast():\n    A = Matrix([[1, 2], [3, 4]])\n    B = Matrix([[5, 6], [7, 8]])\n    C = Matrix([[9, 10]])\n    D = Matrix([[11, 12], [13, 14]])\n    assert MatMul(A, B, C, D).args == (A, B, C, D)\n    assert MatMul(A, B, C, D).doit() == ImmutableMatrix([[ \n        [58, 64], \n        [139, 154]\n    ]])\n", "def test_compatibility_with_sympy_functions():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    assert adjoint(MatMul(A, B)) == MatMul(adjoint(B), adjoint(A))\n    assert transpose(MatMul(A, B)) == MatMul(transpose(B), transpose(A))\n    assert det(MatMul(A, B)) == det(B)*det(A) \n", "def test_matmul_with_empty_matrices():\n    A = Matrix([])\n    B = Matrix([])\n    assert MatMul(A, B).doit() == ZeroMatrix(0, 0)\n\n    A = Matrix([[1, 2], [3, 4]])\n    B = Matrix([])\n    assert MatMul(A, B).doit() == ZeroMatrix(2, 0)\n\n    A = Matrix([])\n    B = Matrix([[1, 2]])\n    assert MatMul(A, B).doit() == ZeroMatrix(0, 1)\n", "def test_matmul_with_zero_matrices():\n    A = Matrix(2, 2, [1, 2, 3, 4])\n    B = ZeroMatrix(2, 2)\n    assert MatMul(A, B).doit() == ZeroMatrix(2, 2)\n    assert MatMul(B, A).doit() == ZeroMatrix(2, 2)\n    assert MatMul(A, B, C=Matrix(2, 2, [5, 6, 7, 8])).doit() == ZeroMatrix(2, 2)\n", "def test_matmul_with_zero_matrix():\n    assert MatMul(C, ZeroMatrix(n, k)).doit() == ZeroMatrix(n, k)\n    assert MatMul(ZeroMatrix(n, k), C).doit() == ZeroMatrix(n, k)\n    assert MatMul(ZeroMatrix(n, k), ZeroMatrix(n, k)).doit() == ZeroMatrix(n, k)\n\n    # Test with MatMul(ZeroMatrix(n, k), MatMul(A, B))\n    assert MatMul(ZeroMatrix(n, k), MatMul(A, B)).doit() == ZeroMatrix(n, k)\n"], "sample_730": ["compilation error", "def test_enet_path_sparse():\n    # Test that dense and sparse input give the same input for descent paths.\n    X, y, _, _ = build_dataset(n_samples=50, n_features=20)\n    csr = sparse.csr_matrix(X)\n    for path in [enet_path, lasso_path]:\n        _, coefs, _ = path(X, y, fit_intercept=False)\n        _, sparse_coefs, _ = path(csr, y, fit_intercept=False)\n        assert_array_almost_equal(coefs, sparse_coefs)\n", "def test_enet_multioutput_with_sparse_data():\n    # Test with sparse data and multi-output elastic net\n    n_samples = 100\n    n_features = 5000\n    n_targets = 3\n    X = sparse.csr_matrix((np.random.rand(n_samples * n_features),\n                         (np.random.randint(n_features, size=n_samples * n_features),\n                          np.random.randint(n_samples, size=n_samples * n_features))))\n    y = np.random.rand(n_samples, n_targets)\n\n    enet = MultiTaskElasticNet(alpha=0.5, max_iter=100)\n    enet.fit(X, y)\n\n", "def test_enet_cv_multi_target():\n    X, y, X_test, y_test = build_dataset(n_samples=50, n_features=10, n_targets=3)\n    \n    enet_cv = ElasticNetCV(n_alphas=10, eps=1e-3, max_iter=100, \n                           l1_ratio=0.5, random_state=42)\n    enet_cv.fit(X, y) \n    assert_equal(enet_cv.coef_.shape, (3, X.shape[1])) \n    assert_equal(enet_cv.intercept_.shape, (3,))\n\n\n", "def test_enet_l1_ratio_multitask():\n    # Test that l1_ratio works as expected with multitask elastic net\n    X = np.array([[1, 2, 4, 5, 8],\n                  [3, 5, 7, 7, 8]]).T\n    y = np.array([[12, 10, 11, 21, 5],\n                  [20, 15, 16, 26, 6]]).T\n\n    for l1_ratio in [0.0, 0.1, 0.5, 0.9, 1.0]:\n        clf = MultiTaskElasticNet(l1_ratio=l1_ratio)\n        clf.fit(X, y)\n        assert_almost_equal(clf.l1_ratio_, l1_ratio)\n\n\n"], "sample_140": ["    def test_sensitive_variables_decorator_error_handling(self):\n        with self.settings(DEBUG=True):\n            @sensitive_variables\n                1 / 0  \n\n            with self.assertRaises(ZeroDivisionError):\n                test_func(\"password\") \n\n", "    def test_sensitive_variables_decorator_on_class_methods(self):\n        class MyClass:\n            @sensitive_variables\n                return f\"Password: {password}, Sauce: {sauce}\"\n\n        instance = MyClass()\n        with self.settings(DEBUG=True):\n            response = self.client.post('/my_view/', data={'password': 'secret', 'sauce': 'worcestershire'})\n            self.assertContains(response, 'Password: censored, Sauce: censored', status_code=500)\n        with self.settings(DEBUG=False):\n            response = self.client.post('/my_view/', data={'password': 'secret', 'sauce': 'worcestershire'})\n            self.assertContains(response, 'Password: censored, Sauce: censored', status_code=500)\n", "    def test_sensitive_variables_decorator_with_multiple_args(self):\n        @sensitive_variables\n            return password, secret_key, username\n\n        with self.settings(DEBUG=True):\n            result = test_func('password', 'secret_key', 'username')\n            self.assertEqual(result, (CLEANSED_SUBSTITUTE, CLEANSED_SUBSTITUTE, 'username'))\n", "    def test_sensitive_get_parameters_not_called(self):\n        msg = (\n            'sensitive_get_parameters() must be called to use it as a '\n            'decorator, e.g., use @sensitive_get_parameters(), not '\n            '@sensitive_get_parameters.'\n        )\n        with self.assertRaisesMessage(TypeError, msg):\n            @sensitive_get_parameters\n                return index_page(request)\n", "    def test_sensitive_variables_on_method(self):\n        class MyClass:\n            @sensitive_variables\n                return password\n\n        instance = MyClass()\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(lambda request: instance.my_method('secret'))\n            self.verify_unsafe_email(lambda request: instance.my_method('secret'))\n\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(lambda request: instance.my_method('secret'))\n            self.verify_safe_email(lambda request: instance.my_method('secret'))\n"], "sample_575": ["    def test_label_format(self, t):\n\n        s = Temporal().label(format=\"%m-%d-%Y\")\n        a = PseudoAxis(s._setup(t, Coordinate())._matplotlib_scale)\n        a.set_view_interval(10, 1000)\n        label, = a.major.formatter.format_ticks([100])\n        assert label == \"09-27-1972\"\n", "    def test_label_formatter_custom(self, t):\n\n        formatter = lambda x: mpl.dates.datestr(x, \"%d-%m-%Y\")\n        s = Temporal().label(formatter)\n        a = PseudoAxis(s._setup(t, Coordinate())._matplotlib_scale)\n        a.set_view_interval(10, 1000)\n        label, = a.major.formatter.format_ticks([100])\n        assert label == \"15-03-2000\"\n\n", "    def test_tick_every(self, t, x):\n\n        s = Temporal().tick(every=100)\n        a = PseudoAxis(s._setup(t, Coordinate())._matplotlib_scale)\n        a.set_view_interval(0, 365)\n        ticks = a.major.locator()\n        assert np.allclose(\n            mpl.dates.date2num(pd.to_datetime([t.iloc[7], t.iloc[13]]))\n            [:2], ticks[:2]\n        )\n        assert len(ticks) == 4 \n\n\n", "    def test_tick_count(self, t, x):\n\n        n = 5\n        s = Temporal().tick(count=n)\n        a = PseudoAxis(s._setup(t, Coordinate())._matplotlib_scale)\n        a.set_view_interval(0, 365)\n        assert len(a.major.locator()) == n\n\n\n\n\n", "    def test_label_datetime64(self, t):\n\n        s = Temporal()._setup(t, Coordinate())\n        label, = s(t)  \n        assert isinstance(label, Timestamp) \n"], "sample_333": ["    def test_renderer_in_form_class(self):\n        class CustomForm(Form):\n            renderer = CustomRenderer\n\n        form = CustomForm()\n        self.assertIsInstance(form.renderer, CustomRenderer)\n", "    def test_renderer_overrides_default(self):\n        class CustomForm(Form):\n            default_renderer = DjangoTemplates()\n\n        form = CustomForm(renderer=CustomRenderer())\n        self.assertNotEqual(form.renderer, CustomForm.default_renderer)\n        self.assertIsInstance(form.renderer, CustomRenderer)\n", "    def test_renderer_attribute_overrides_kwargs(self):\n        custom = CustomRenderer()\n        form = Form(renderer=DjangoTemplates())\n        self.assertEqual(form.renderer, DjangoTemplates())\n", "    def test_field_error_messages_inheritance(self):\n        class CustomCharField(CharField):\n                kwargs['error_messages'] = {'invalid': 'Form custom error message.'}\n                super().__init__(**kwargs)\n\n        class CustomForm(Form):\n            field = CustomCharField()\n\n        form = CustomForm()\n        self.assertEqual('Form custom error message.', form.fields['field'].error_messages['invalid'])\n\n", "    def test_renderer_in_form_class(self):\n        class CustomForm(Form):\n                super().__init__(*args, **kwargs)\n                self.renderer = CustomRenderer()\n\n        form = CustomForm()\n        self.assertIsInstance(form.renderer, CustomRenderer)\n"], "sample_671": ["compilation error", "compilation error", "def test_importorskip_with_module_already_imported():\n    import os\n    os_available = True\n    with pytest.raises(pytest.skip.Exception, match=\"already imported\"):\n        pytest.importorskip(\"os\", already_imported=os_available)\n\n\n\n", "compilation error", "def test_skip_in_setup_function():\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.skip\n            assert 0\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*1 skipped*\"])\n"], "sample_379": ["    def test_safe_string_inheritance(self):\n        class MySafeString(SafeString):\n            pass\n\n        s = MySafeString('a&b')\n        self.assertRenderEqual('{{ s }}', 'a&b', s=s)\n", "    def test_mark_safe_retains_type(self):\n        s = mark_safe(123)\n        self.assertIsInstance(s, SafeString)\n        self.assertEqual(str(s), '123')\n", "    def test_mark_safe_with_unescaped_html(self):\n        \"\"\"\n        Ensure that mark_safe() correctly handles strings with unescaped HTML\n        elements.\n\n        This test checks that mark_safe() escapes potentially dangerous\n        characters like '<' and '>' when necessary, preventing XSS vulnerabilities.\n        \"\"\"\n        unsafe_string = \"<img src=x onerror='alert(document.cookie)'>\"\n        safe_string = mark_safe(unsafe_string)\n        self.assertRenderEqual('{{ safe_string }}', '&lt;img src=x onerror=&quot;alert(document.cookie)&quot;&gt;', safe_string=safe_string)\n", "    def test_mark_safe_decorator_with_arguments(self):\n            return '<' + arg1 + '>' + arg2\n\n        @mark_safe\n            return my_function(arg1, arg2)\n\n        self.assertRenderEqual('{{ wrapped_function(\"hello\", \"world\") }}', '&lt;hello&gt;world', arg1=\"hello\", arg2=\"world\")\n", "    def test_mark_safe_with_invalid_input(self):\n        with self.assertRaises(TypeError):\n            mark_safe(123)\n\n        with self.assertRaises(TypeError):\n            mark_safe([1, 2, 3])\n\n        with self.assertRaises(TypeError):\n            mark_safe(True)\n"], "sample_758": ["    def __len__(self):\n        return len(self.data)\n", "        def __len__(self):\n            return 3\n", "        def __len__(self):\n            return 4\n", "        def __len__(self):\n            return 1\n", "        def __len__(self):\n            return 1\n"], "sample_584": ["    def test_auto_combine_with_mismatched_dimensions(self):\n        with pytest.raises(ValueError, match=\"Dimension names 'x' and 'y'\"\n                         \" are not aligned\"):\n            auto_combine([Dataset({'x': [0]}), Dataset({'y': [1]})])\n\n", "    def test_auto_combine_no_coords_raises_error(self):\n        objs = [Dataset({'x': [0]}, {'y': 0}),\n                Dataset({'x': 1}, {'y': 1, 'z': 1})]\n        with pytest.raises(ValueError, match=\"Every dimension needs a coordinate\"):\n            auto_combine(objs) \n\n", "    def test_auto_combine_with_mismatched_coords(self):\n        with pytest.raises(ValueError, match=\"Coordinates\"):\n            auto_combine([Dataset({'x': [0], 'y': [0]}), Dataset({'x': [1]})])\n", "    def test_auto_combine_with_existing_data_on_new_coords(self):\n        objs = [Dataset({'a': ('x', [0])}, coords={'x': ('x', [0])}),\n                Dataset({'b': ('x', [1]), 'c': (('x'), [2])}),\n                Dataset({'c': (('x'), [3])})]\n        with pytest.warns(FutureWarning, match=\"auto_combine\"):\n            auto_combine(objs)\n", "    def test_auto_combine_with_multiple_coords_same_dim(self):\n        objs = [Dataset({'x': (('a', 'b'), [0])}, coords={'a': ('a', [0]), 'b': ('b', [0])}),\n                Dataset({'x': (('a', 'b'), [1])}, coords={'a': ('a', [1]), 'b': ('b', [1])})]\n        with pytest.warns(FutureWarning, match=\"`auto_combine`\"):\n            auto_combine(objs) \n"], "sample_1028": ["compilation error", "def test_issue_14986():\n    a = symbols('a', commutative=False)\n    b = symbols('b', commutative=False)\n    assert (a*b*a).is_commutative is False\n    assert (a**2*b).is_commutative is False\n    assert (b*a**2).is_commutative is False\n    assert (a*b**2).is_commutative is False\n", "  def test_issue_14392_2():\n    assert (cos(zoo)**2).as_real_imag() == (nan, nan)\n", "def test_Add_commutative_nan():\n    x = Dummy(finite=False)\n    y = Dummy(finite=False)\n    assert Add(x, y).is_commutative is None\n    assert Add(y, x).is_commutative is None\n    assert Add(x, x).is_commutative is None\n    assert Add(x, -x).is_commutative is None\n\n\n", "def test_issue_15018():\n    a, b = symbols('a b', real=True)\n    assert (a**2 + b**2).is_positive is None\n    assert (a**2 + b**2).is_nonnegative\n    assert (a**2 + b**2).is_nonpositive is False\n    assert (-a**2 - b**2).is_nonpositive\n    assert (-a**2 - b**2).is_positive is False\n\n\n\n"], "sample_605": ["def test_groupby_datetime_corner_cases():\n    times = pd.date_range(\"2000-01-01\", periods=4, freq=\"D\")\n    data = np.random.rand(4)\n    da = xr.DataArray(data, coords={\"time\": times}, dims=\"time\")\n\n    # Test with NaT\n    da[\"time\"][-1] = pd.NaT\n    g = da.groupby(\"time.day\")\n    with pytest.raises(ValueError):\n        g.mean()\n\n    # Test with duplicate time labels, but different datetimes\n    da = xr.DataArray(data, coords={\"time\": [times[0], times[1], times[0], times[2]]}, dims=\"time\")\n    g = da.groupby(\"time.hour\")\n    with pytest.raises(ValueError):\n        g.mean()\n\n    # Test with multiple groupby on datetime\n    g = da.groupby([\"time.year\", \"time.month\"])\n    g = g.mean()\n    assert_identical(g.time.dt.year, g.time.dt.year)\n    assert_identical(g.time.dt.month, g.time.dt.month)\n\n    # Test with datetime accessor on grouped data\n    g = da.groupby(\"time.day\")\n    assert isinstance(g.time.dt.hour, xr.DataArray)\n\n    # Test with multiple datetime attributes\n    times = pd.to_datetime([\"2023-01-01 00:00:00\", \"2023-01-01 01:00:00\", \"2023-01-01 02:00:00\"])\n    data = np.arange(3)\n    da = xr.DataArray(data, coords={\"time\": times}, dims=\"time\")\n    g = da.groupby(\"time.hour\")\n\n    # Test with multiple datetime attributes\n    g = da.groupby([\"time.hour\", \"time.minute\"])\n    g = g.mean()\n\n", "def test_groupby_bins_with_missing_data():\n    ds = xr.Dataset()\n    ds[\"time\"] = xr.DataArray(\n        pd.date_range(\"2010-08-01\", \"2010-08-15\", freq=\"15min\"), dims=\"time\"\n    )\n    ds[\"val\"] = xr.DataArray(np.ones(*ds[\"time\"].shape), dims=\"time\")\n    ds[\"val\"][::2] = np.nan\n    time_bins = pd.date_range(start=\"2010-08-01\", end=\"2010-08-15\", freq=\"24H\")\n    actual = ds.groupby_bins(\"time\", time_bins).sum()\n    expected = xr.DataArray(\n        96 * np.ones((14,)),\n        dims=[\"time_bins\"],\n        coords={\"time_bins\": pd.cut(time_bins, time_bins).categories},\n    ).to_dataset(name=\"val\")\n    assert_identical(actual, expected)\n\n\n\n", "    def test_groupby_empty_groups():\n        array = xr.DataArray([1, 2, 3], [(\"x\", [1, 2, 2])])\n        with pytest.raises(ValueError):\n            array.groupby(\"x\").first()\n        with pytest.raises(ValueError):\n            array.groupby(\"x\").last()\n\n    ", "def test_groupby_datetime_categorical_index():\n    times = pd.date_range(start=\"2020-01-01\", end=\"2020-01-07\", freq=\"D\")\n    data = np.random.randn(len(times))\n    da = xr.DataArray(data, coords={\"time\": times})\n    da = da.set_index(\"time\")\n\n    grouped = da.groupby(\"time.month\")\n    expected = da.groupby(da.index.month)\n    assert_identical(grouped, expected)\n\n\n", "    def test_gb_reduce_over_multiple_coords(array):\n        grouped = array.groupby([\"x\", \"y\"])\n        with pytest.raises(ValueError):\n            grouped.mean(\"z\")\n"], "sample_1055": ["def test_bg_public_key():\n    assert 23*31 == bg_public_key(23, 31)\n    assert 13*17 == bg_public_key(13, 17)\n    raises(ValueError, lambda: bg_private_key(8, 16))\n    raises(ValueError, lambda: bg_private_key(8, 8))\n    raises(ValueError, lambda: bg_private_key(13, 17))\n", "def test_bg_public_key():\n    assert 1783 == bg_public_key(7, 73)\n    assert 256 == bg_public_key(11, 23)\n", "def test_bg_public_key():\n    assert bg_public_key(23, 31) == 713 \n    assert bg_public_key(13, 17) == 221\n    raises(ValueError, lambda: bg_public_key(13, 16))\n", "def test_bg_public_key():\n    assert 1871 == bg_public_key(23, 31)\n    assert 416 == bg_public_key(7, 19)\n    raises(ValueError, lambda: bg_public_key(13, 17))\n\n\n", "def test_bg_public_key():\n    assert 1837 == bg_public_key(23, 31)\n    assert 86 == bg_public_key(7, 71)\n    assert 10561 == bg_public_key(103, 11) \n    raises(ValueError, lambda: bg_public_key(15, 19))\n"], "sample_106": ["    def test_different_aliases(self):\n        cache1 = caches['default']\n        cache2 = caches['other']\n\n        self.assertNotIs(cache1, cache2)\n", "    def test_cache_key_creation_with_Vary_header(self):\n        request = self.factory.get(self.path)\n        template = engines['django'].from_string(\"This is a test\")\n        response = TemplateResponse(HttpRequest(), template)\n        response['Vary'] = ['Accept-Encoding']\n        learn_cache_key(request, response)\n        self.assertEqual(\n            get_cache_key(request),\n            'views.decorators.cache.cache_page.settingsprefix.GET.'\n            '58a0a05c8a5620f813686ff969c26853.d41d8cd98f00b204e9800998ecf8427e'\n        )\n", "    def test_different_aliases(self):\n        \"\"\"\n        Requesting different aliases should yield distinct instances.\n        \"\"\"\n        cache1 = caches['default']\n        cache2 = caches['other']\n\n        self.assertIsNot(cache1, cache2)\n\n", "    def test_different_aliases_different_instances(self):\n        cache1 = caches['default']\n        cache2 = caches['other']\n        self.assertIsNot(cache1, cache2)\n", "    def test_different_aliases(self):\n        \"\"\"\n        Requesting different aliases should yield different instances.\n        \"\"\"\n        cache1 = caches['default']\n        cache2 = caches['other']\n\n        self.assertIsNot(cache1, cache2)\n"], "sample_344": ["    def test_exclude_fields(self):\n        A = self.create_model(\"A\", foreign_keys=[models.ForeignKey('B', models.CASCADE)])\n        B = self.create_model(\"B\")\n\n        # Define a M2M model with an explicitly excluded field\n        T = self.create_model(\"T\",\n                             foreign_keys=[\n                                models.ForeignKey('A', models.CASCADE),\n                                models.ForeignKey('B', models.CASCADE),\n                             ],\n                             exclude=['id'])\n        self.assertRelated(A, [B, T])\n        self.assertRelated(B, [A, T])\n        self.assertRelated(T, [A, B])\n", "    def test_proxy_to_abstract_base(self):\n        A = self.create_model(\"A\", abstract=True)\n        B = self.create_model(\"B\", bases=(A,), proxy=True)\n        self.assertRelated(A, [B])\n        self.assertRelated(B, [])\n", "    def test_custom_managed_related_fields(self):\n        class CustomRelatedManager(RelatedManager):\n                return super().get_queryset().filter(name='custom')\n\n        class A(models.Model):\n            name = models.CharField(max_length=50)\n\n            related_objects = models.ManyToManyField(\n                'B',\n                related_query_name='related_objects',\n                through='RelatedThrough',\n                managers=[CustomRelatedManager()]\n            )\n\n        class B(models.Model):\n            name = models.CharField(max_length=50)\n\n        class RelatedThrough(models.Model):\n            a = models.ForeignKey(A, models.CASCADE)\n            b = models.ForeignKey(B, models.CASCADE)\n\n        A.objects.create(name='test')\n        B.objects.create(name='custom')\n        B.objects.create(name='other')\n        a = A.objects.get(name='test')\n        related_objects = a.related_objects.all()\n        self.assertEqual(len(related_objects), 1)\n        self.assertEqual(related_objects.get().name, 'custom')\n", "    def test_ordering_with_respect_to_nested_bases(self):\n        A = self.create_model(\"A\")\n        B = self.create_model(\"B\", bases=(A,))\n        C = self.create_model(\"C\", bases=(B,))\n        D = self.create_model(\"D\", bases=(C,))\n        E = self.create_model(\"E\", bases=(C,))\n\n        D.order_with_respect_to = \"E\"\n\n        self.assertRelated(A, [B, C, D, E])\n        self.assertRelated(B, [A, C, D, E])\n        self.assertRelated(C, [A, B, D, E])\n        self.assertRelated(D, [A, B, C, E])\n        self.assertRelated(E, [A, B, C, D])\n\n        \n", "    def test_multiple_proxies(self):\n        A = self.create_model(\"A\")\n        B = self.create_model(\"B\", bases=(A,), proxy=True)\n        C = self.create_model(\"C\", bases=(A,), proxy=True)\n        D = self.create_model(\"D\", bases=(B, C,))\n        self.assertRelated(A, [B, C, D])\n        self.assertRelated(B, [D])\n        self.assertRelated(C, [D])\n        self.assertRelated(D, [])\n\n"], "sample_656": ["    def test_capture_with_live_logging_multi_threads(testdir, capture_fixture):\n        testdir.makepyfile(\n            \"\"\"\n            import logging\n            import sys\n            import threading\n            import time\n\n            logger = logging.getLogger(__name__)\n\n                print(msg)\n                logging.info(msg)\n\n                print_and_log(f\"Thread {threading.get_ident()} starting\")\n                time.sleep(sleep_time)\n                print_and_log(f\"Thread {threading.get_ident()} finishing\")\n                capture.readouterr()\n\n                capture = {0}\n                threads = [\n                    threading.Thread(target=worker_thread, args=(capture, 0.5)),\n                    threading.Thread(target=worker_thread, args=(capture, 1.0)),\n                    threading.Thread(target=worker_thread, args=(capture, 0.25)),\n                ]\n                for t in threads:\n                    t.start()\n                for t in threads:\n                    t.join()\n            \"\"\".format(capture_fixture)\n        )\n\n        result = testdir.runpytest_subprocess(\"--log-cli-level=INFO\")\n        assert result.ret == 0\n", "def test_capture_with_subprocess(testdir):\n    # Issue 4054: capture should work with subprocesses\n\n    p = testdir.makepyfile(\n        \"\"\"\n        import sys\n            import subprocess\n            p = subprocess.Popen(['echo', 'hello'], stdout=subprocess.PIPE)\n            out, err = p.communicate()\n            assert out.decode('utf-8') == 'hello\\n'\n    \"\"\"\n    )\n    result = testdir.runpytest_subprocess()\n    result.stdout.fnmatch_lines(\n        \"\"\"\n        *test_subprocess*\n        *hello*\n    \"\"\"\n    )\n\n\n\n\n", "    def test_capture_with_live_logging_and_nested_files(testdir, capture_fixture):\n        testdir.makepyfile(\n            \"\"\"\n        import logging\n        import sys\n        from _pytest import capture\n\n        logger = logging.getLogger(__name__)\n\n            print(\"hello\")\n            sys.stderr.write(\"world\\\\n\")\n\n            captured = {0}.readouterr()\n            assert captured.out == \"hello\\\\n\"\n            assert captured.err == \"world\\\\n\"\n\n            with open(\"subfile.txt\", \"w\") as f:\n                f.write(\"subfile content\")\n\n            logging.info(\"something\")\n            print(\"next\")\n\n            captured = {0}.readouterr()\n            assert captured.out == \"next\\\\n\"\n        \"\"\".format(  capture_fixture\n            )\n        )\n        result = testdir.runpytest_subprocess(\"--log-cli-level=INFO\")\n        assert result.ret == 0\n", "compilation error", "def test_capture_in_a_context_manager(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import contextlib\n        from _pytest.capture import StdCapture\n        with contextlib.suppress(AttributeError):\n            class CustomCapture(contextlib.ContextDecorator):\n                    self.captured = None\n\n                    self.captured = StdCapture()\n                    return self.captured\n\n                    self.captured.stop_capturing()\n        \n            with CustomCapture() as cap:\n                print(\"hello\")\n                sys.stderr.write(\"world\\\\n\")\n            assert cap.readouterr().out == \"hello\\\\n\"\n            assert cap.readouterr().err == \"world\\\\n\"\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=1)\n"], "sample_398": ["    def test_user_change_password_empty_new_password(self):\n        url = reverse(\"auth_test_admin:auth_user_password_change\", args=(self.admin.pk,))\n        response = self.client.post(\n            url, {\"password1\": \"\", \"password2\": \"\"}\n        )\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, \"This field is required.\")\n        self.assertContains(response, \"Enter a new password.\")\n", "    def test_user_change_password_reset_by_admin(self):\n        u = User.objects.get(email=\"staffmember@example.com\")\n        response = self.client.post(\n            reverse(\"auth_test_admin:auth_user_password_change\", args=(u.pk,)),\n            {\n                \"password1\": \"password1\",\n                \"password2\": \"password1\",\n            },\n        )\n        self.assertRedirects(\n            response, reverse(\"auth_test_admin:auth_user_change\", args=(u.pk,))\n        )\n        row = LogEntry.objects.latest(\"id\")\n        self.assertEqual(row.user_id, self.admin.pk)\n        self.assertEqual(row.object_id, str(u.pk))\n        self.assertEqual(row.get_change_message(), \"Changed password.\")\n\n\n", "    def test_user_change_password_reset_old_password_field(self):\n        user = User.objects.get(username=\"testclient\")\n        user.is_superuser = False\n        user.save()\n        self.login()\n\n        password_change_url = reverse(\n            \"auth_test_admin:auth_user_password_change\", args=(user.pk,)\n        )\n        response = self.client.get(password_change_url)\n        self.assertContains(response, \"Old Password\")\n\n        response = self.client.post(\n            password_change_url,\n            {\n                \"password1\": \"password1\",\n                \"password2\": \"password1\",\n                \"old_password\": \"secret\",  # Provide an incorrect old password\n            },\n        )\n        self.assertContains(response, \"Invalid old password.\")\n", "    def test_permission_denied_user_change_view(self):\n        u = User.objects.get(email=\"staffmember@example.com\")\n        # Remove the 'change_user' permission from the test user.\n        self.admin.user_permissions.filter(content_type__app_label=\"auth\",codename='change_user').delete()\n        response = self.client.get(\n            reverse(\"auth_test_admin:auth_user_change\", args=(u.pk,)),\n        )\n        self.assertEqual(response.status_code, 403)\n", "    def test_user_cannot_change_password_without_permission(self):\n        u = User.objects.get(email=\"staffmember@example.com\")\n        u.is_staff = False\n        u.save()\n        response = self.client.post(\n            reverse(\"auth_test_admin:auth_user_password_change\", args=(u.pk,)),\n            {\n                \"password1\": \"password1\",\n                \"password2\": \"password1\",\n            },\n        )\n        self.assertEqual(response.status_code, 403)\n"], "sample_968": ["def test_python_python_use_unqualified_type_names_enabled(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"n\"><a class=\"reference internal\" href=\"#foo.Name\" title=\"foo.Name\">'\n            '<span class=\"pre\">Name</span></a></span>' in content)\n    assert ('<p><strong>name</strong> (<a class=\"reference internal\" href=\"#foo.Name\" '\n            'title=\"foo.Name\"><em>Name</em></a>) \u2013 blah blah</p>' in content)\n", "def test_python_python_use_unqualified_type_names_with_inheritance(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"n\"><a class=\"reference internal\" href=\"#foo.SubName\" '\n            'title=\"foo.SubName\"><span class=\"pre\">SubName</span></a></span>' in content)\n    assert '<span class=\"n\"><span class=\"pre\">foo.SubAge</span></span>' in content\n    assert ('<p><strong>name</strong> (<a class=\"reference internal\" href=\"#foo.SubName\" '\n            'title=\"foo.SubName\"><em>SubName</em></a>) \u2013 blah blah</p>' in content)\n    assert '<p><strong>age</strong> (<em>foo.SubAge</em>) \u2013 blah blah</p>' in content\n", "    def test_warn_on_circular_imports(app, warning):\n        text = (\".. py:module:: A\\n\"\n                \"   :import_as: a\\n\"\n                \"   \\n\"\n                \"   .. py:data:: x\\n\"\n                \"       :import: B.y\\n\"\n                \"\\n\"\n                \".. py:module:: B\\n\"\n                \"   :import_as: b\\n\"\n                \"   \\n\"\n                \"   .. py:data:: y\\n\"\n                \"       :import: A.x\\n\")\n        restructuredtext.parse(app, text)\n        assert warning.getvalue() == \"\"\n\n\n\n", "def test_python_python_use_unqualified_type_names_enabled(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"n\"><span class=\"pre\">Name</span></span>' in content)\n    assert ('<span class=\"n\"><span class=\"pre\">Age</span></span>' in content)\n    assert ('<p><strong>name</strong> (<span class=\"n\"><span class=\"pre\">Name</span></span>) \u2013 blah blah</p>' in content)\n    assert ('<p><strong>age</strong> (<span class=\"n\"><span class=\"pre\">Age</span></span>) \u2013 blah blah</p>' in content)\n\n", "def test_python_python_use_unqualified_type_names_enabled(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"n\"><a class=\"reference internal\" href=\"#foo.Name\" title=\"foo.Name\">'\n            '<span class=\"pre\">Name</span></a></span>' in content)\n    assert ('<p><strong>name</strong> (<a class=\"reference internal\" href=\"#foo.Name\" '\n            'title=\"foo.Name\"><em>Name</em></a>) \u2013 blah blah</p>' in content)\n"], "sample_302": ["    def test_runshell_handles_signal_interrupt(self):\n        with mock.patch('django.db.backends.base.client.BaseDatabaseClient.runshell') as mock_runshell:\n            client = DatabaseClient(connection=connection)\n            client.runshell(['--command', 'SELECT 1'])\n            mock_runshell.assert_called_once_with(['--command', 'SELECT 1'])\n", "compilation error", "    def test_runshell_handles_sigint(self):\n        with mock.patch('signal.getsignal') as mock_getsignal:\n            mock_getsignal.return_value = lambda sig: 42  \n            with mock.patch('__main__.DatabaseClient.db_create_connection') as mock_db_create:\n                self.client.runshell([])\n                mock_getsignal.assert_called_once_with(signal.SIGINT) \n                self.assertEqual(signal.signal, mock.call(signal.SIGINT, signal.SIG_IGN))\n                mock_db_create.assert_called_once_with(self.client) \n", "    def test_runshell_ignores_sigint(self):\n        with mock.patch('builtins.input') as mock_input:\n            # Set up a mock function to capture the input\n            mock_input.side_effect = ['\\n'] * 3\n\n            # Run the psql shell (which internally calls runshell)\n            self.client.runshell([])\n\n            # Verify that the SIGINT handler was restored\n            self.assertEqual(signal.getsignal(signal.SIGINT), signal.SIG_DFL)\n", "    def test_runshell_handles_sigint(self):\n        with mock.patch('django.db.backends.base.client.BaseDatabaseClient.runshell') as mock_runshell:\n            client = DatabaseClient(connection=connection)\n            client.runshell([])\n            mock_runshell.assert_called_once()\n\n\n\n"], "sample_664": ["def test_deprecation_warnings_are_raised(testdir, warning):\n    testdir.makepyfile(\n        \"\"\"\n            pytest.fixture(params=[\"a\", \"b\"])\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    if isinstance(warning, PytestDeprecationWarning):\n        assert warning.message in result.stdout.str()\n    elif isinstance(warning, UnformattedWarning):\n        assert str(warning) in result.stdout.str()\n", "compilation error", "def test_deprecated_options_in_pytest_help(testdir, option, warning):\n    result = testdir.runpytest(\"--help\")\n    result.stdout.fnmatch_lines([f\"*{warning.message}*\"])\n", "def test_deprecation_warnings(testdir, warning):\n    testdir.makepyfile(\n        \"\"\"\n            pass\n\n        @pytest.fixture\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    if isinstance(warning, deprecated.PytestDeprecationWarning):\n        result.stdout.fnmatch_lines([*warning.args])\n", "compilation error"], "sample_7": ["def test_masked_column_copy_deep_masked_array():\n    original_data = np.array([1, 2, 3])\n    original_mask = np.array([False, True, False])\n    mc = table.MaskedColumn(original_data, mask=original_mask)\n\n    mc_copy_shallow = mc.copy()\n    mc_copy_deep = mc.view(table.MaskedColumn).copy()\n\n    mc_copy_shallow.mask[0] = True\n\n    assert np.all(mc.mask == original_mask)\n    assert np.all(mc_copy_shallow.mask != original_mask)\n    assert np.all(mc_copy_deep.mask == original_mask)\n\n\n", "def test_masked_column_no_mask_after_unmasking():\n    mc = table.MaskedColumn([1., 2., 3.], mask=[True, False, True])\n    mc_unmasked = mc.filled(0)\n    assert np.all(mc_unmasked == [1., 2., 3.])\n    assert np.all(mc_unmasked.mask == False)\n", "def test_masked_column_serialization_modes():\n    mc = table.MaskedColumn([1., 2., np.ma.masked, 4.], mask=[False, False, True, False])\n\n    for method in ['data_mask', 'null_value']:\n        mc.info.serialize_method['ecsv'] = method\n        with pytest.raises(TypeError) as e:\n            mc.to_ecsv('test.ecsv', format='ecsv')\n        assert 'MaskedArray serialization' in str(e)\n        assert 'method' in str(e)\n\n\n\n    with pytest.raises(ValueError) as e:\n        mc.to_ecsv('test.ecsv', format='ecsv', serialize_method='foobar')\n    assert 'Invalid serialize_method' in str(e)\n\n\n\n", "def test_masked_column_scalar_masking():\n    mc = table.MaskedColumn([1, 2, 3])\n    mc[:] = np.ma.masked\n\n    assert mc.mask[0]\n    assert mc.mask[1]\n    assert mc.mask[2]\n\n    mc[0] = np.ma.masked\n    assert mc.mask[0]\n    assert not mc.mask[1]\n    assert mc.mask[2]\n\n    mc[1:2] = np.ma.masked\n    assert mc[:].mask == [True, True, False]\n", "def test_masked_column_invalid_mask():\n    \"\"\"\n    Ensure that creating a MaskedColumn with an invalid mask raises an error.\n    \"\"\"\n\n    with pytest.raises(ValueError):\n        table.MaskedColumn([1, 2, 3], mask=['a', 'b', 'c'])\n\n    with pytest.raises(ValueError):\n        table.MaskedColumn([1, 2, 3], mask=[1.5, True, False])\n\n    with pytest.raises(ValueError):\n        table.MaskedColumn([1, 2, 3], mask=[True, {'a': True}])\n\n    with pytest.raises(ValueError):\n        table.MaskedColumn([1, 2, 3], mask=[True, False, True, 'a'])\n"], "sample_80": ["    def test_related_object_filtering(self):\n        query = Query(Author)\n        where = query.build_where(Q(objectc__name__icontains='test'))\n        lookup = where.children[0]\n        self.assertIsInstance(lookup, Q)\n        self.assertEqual(lookup.children[0].connector, 'AND')\n        self.assertEqual(lookup.children[0].children[0].rhs, 'test')\n        self.assertEqual(lookup.children[0].children[0].lhs.target, ObjectC._meta.get_field('name'))\n\n\n\n", "    def test_related_isnull(self):\n        query = Query(Item)\n        where = query.build_where(Q(creator__isnull=True))\n        lookup = where.children[0]\n        self.assertIsInstance(lookup, RelatedIsNull)\n        self.assertEqual(lookup.lhs.target, Item._meta.get_field('creator'))\n\n", "    def test_related_lookup_f(self):\n        query = Query(Item)\n        where = query.build_where(Q(creator__name__iendswith=F('name')))\n        lookup = where.children[0]\n        self.assertIsInstance(lookup, Exact)\n        self.assertTrue(isinstance(lookup.lhs, RelatedIsNull))\n        self.assertEqual(lookup.lhs.rel, Item._meta.get_field('creator').related_model._meta.get_field('name'))\n        self.assertIsInstance(lookup.rhs, SimpleCol)\n        self.assertEqual(lookup.rhs.target, Item._meta.get_field('name')) \n", "    def test_related_isnull(self):\n        query = Query(Author)\n        where = query.build_where(Q(related_author__isnull=False) & Q(name__contains=\"test\"))\n        related_isnull_lookup = where.children[0]\n        self.assertIsInstance(related_isnull_lookup, IsNull)\n        self.assertIsInstance(related_isnull_lookup.lhs, RelatedIsNull)\n        self.assertEqual(related_isnull_lookup.lhs.field_name, 'related_author')\n\n        contains_lookup = where.children[1]\n        self.assertIsInstance(contains_lookup, Exact)\n        self.assertIsInstance(contains_lookup.lhs, CharField)\n        self.assertEqual(contains_lookup.lhs.name, 'name')\n        self.assertEqual(contains_lookup.rhs, \"test\")\n\n", "    def test_related_isnull(self):\n        query = Query(Author)\n        where = query.build_where(Q(author_item__isnull=True))\n        lookup = where.children[0]\n        self.assertIsInstance(lookup, RelatedIsNull)\n        self.assertEqual(lookup.lhs.target, Author._meta.get_field('author_item'))\n\n\n\n"], "sample_397": ["    def test_template_loaders_explicit(self):\n        engine = DjangoTemplates(\n            {\n                \"DIRS\": [],\n                \"APP_DIRS\": False,\n                \"NAME\": \"django\",\n                \"OPTIONS\": {\n                    \"loaders\": [\n                        (\"django.template.loaders.filesystem.Loader\", []),\n                        (\"django.template.loaders.cached.Loader\", []),\n                    ]\n                },\n            }\n        )\n\n        self.assertEqual(\n            engine.engine.template_loaders,\n            [\n                \"django.template.loaders.filesystem.Loader\",\n                \"django.template.loaders.cached.Loader\",\n            ],\n        )\n", "    def test_loaders_with_custom_app_dirs(self):\n        \"\"\"\n        Custom app directories should be appended to the default loaders.\n        \"\"\"\n        custom_app_dirs = ['my_app', 'another_app']\n        engine = DjangoTemplates(\n            {\"DIRS\": [], \"APP_DIRS\": custom_app_dirs, \"NAME\": \"django\", \"OPTIONS\": {}}\n        )\n        self.assertEqual(\n            engine.engine.loaders,\n            [\n                (\n                    \"django.template.loaders.cached.Loader\",\n                    [\n                        \"django.template.loaders.filesystem.Loader\",\n                        *custom_app_dirs,\n                    ],\n                ),\n            ],\n        )\n", "    def test_template_loaders_with_custom_loader(self):\n        \"\"\"\n        Custom loader configured in settings is used.\n        \"\"\"\n        custom_loader_class = type(\"CustomLoader\", (object,), {})\n        engine = DjangoTemplates(\n            {\"DIRS\": [], \"APP_DIRS\": False, \"NAME\": \"django\", \"OPTIONS\": {\n                \"loaders\": [(\"CustomLoader\", [])\n            ]}\n        )\n        self.assertEqual(engine.engine.loaders, [(\"CustomLoader\", [])])\n", "    def test_find_template_raises_on_invalid_loader(self):\n        engine = DjangoTemplates(\n            {\"DIRS\": [], \"APP_DIRS\": False, \"NAME\": \"django\", \"OPTIONS\": {}}\n        )\n        with self.assertRaises(ImproperlyConfigured):\n            engine.find_template(\"template.html\", loaders=[(\"invalid\",)])\n", "    def test_template_loader_from_non_string_config(self):\n        engine = DjangoTemplates(\n            {\"DIRS\": [], \"APP_DIRS\": False, \"NAME\": \"django\", \"OPTIONS\": {\"loaders\": [((\"django.template.loaders.filesystem.Loader\",), 1), (\"cached\",)]}}\n        )\n        loaders = engine.engine.template_loaders\n        self.assertEqual(len(loaders), 2)\n        self.assertEqual(loaders[0].loader.__class__.__name__, 'FileSystemLoader')\n        self.assertEqual(loaders[1].loader.__class__.__name__, 'CachedLoader')\n"], "sample_1148": ["def test_matrix_element_subs():\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    C = MatrixSymbol('C', 3, 3)\n    expr = (A + B)*C\n    assert expr.subs({A: 2*Identity(3), B: C}).doit() == 3*C\n    expr = (A*B).subs({A: 2*Identity(3), B: C})\n    assert expr.doit() == 2*C\n    expr = (A*B).subs({B: 2*Identity(3), A: C})\n    assert expr.doit() == 2*C\n    expr = (A + B).subs({A: 2*C, B: C})\n    assert expr.doit() == 3*C\n\n\n", "def test_matrix_element_subs():\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n    x = symbols('x')\n    D = Matrix([[1, 2*x], [3, 4*x]])\n\n    assert A[0, 0].subs(A, D) == D[0, 0]\n    assert A[1, 1].subs(A, D) == D[1, 1]\n\n    assert (A[0, 0] + A[1, 1]).subs(A, D) == D[0, 0] + D[1, 1]\n\n    assert (A[0, 1] + A[1, 0]).subs(A, D) == D[0, 1] + D[1, 0]\n\n    assert A[0, 0].subs(A, B) == B[0, 0]\n    assert A[0, 0].subs(A, C) == C[0, 0]\n\n    assert A[1, 1].subs(A, B) == B[1, 1]\n    assert A[1, 1].subs(A, C) == C[1, 1]\n\n    assert (A[0, 0] + A[1, 1]).subs(A, B) == B[0, 0] + B[1, 1]\n\n\n\n", "def test_matrix_contraction_symbols():\n    A = MatrixSymbol('A', 2, 3)\n    B = MatrixSymbol('B', 3, 4)\n    C = MatrixSymbol('C', 4, 5)\n    expr = MatMul(MatMul(A, B), C)\n    result = expr.contract([A.columns, B.rows])\n    assert isinstance(result, Matrix)\n    assert result.shape == (2, 5)\n\n    expr2 = MatMul(A, B)\n    result2 = expr2.contract([A.rows, B.cols])\n    assert isinstance(result2, Matrix)\n    assert result2.shape == (3, 4)\n", "def test_matrix_symbol_subs():\n    A = MatrixSymbol(\"A\", 2, 2)\n    B = MatrixSymbol(\"B\", 2, 2)\n    C = MatrixSymbol(\"C\", 3, 3)\n    expr = A + B*C\n\n    # Test substitution of a single MatrixSymbol\n    expr_sub = expr.subs({A: ImmutableMatrix([[1, 2], [3, 4]])})\n    assert isinstance(expr_sub, MatAdd)\n\n    # Test substitution with multiple MatrixSymbols\n    expr_sub = expr.subs({A: ImmutableMatrix([[1, 2], [3, 4]]), B: ImmutableMatrix([[5, 6], [7, 8]])})\n    assert isinstance(expr_sub, MatAdd)\n    \n    # Test substitution with a different MatrixSymbol\n    expr_sub = expr.subs({A: C})\n    assert isinstance(expr_sub, MatAdd)\n\n    # Test substitution with a scalar\n    expr_sub = expr.subs(B, 2)\n    assert isinstance(expr_sub, MatMul) \n\n    # Test substitution with a matrix\n    expr_sub = expr.subs(C, ImmutableMatrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]]))\n    assert isinstance(expr_sub, MatAdd)\n\n\n\n\n", "def test_matrix_element_ordering():\n    A = MatrixSymbol('A', 3, 3)\n    assert A[0,0] == A[0, 0]\n    assert A[1, 2] == A[1, 2]\n    assert A[2, 1] == A[2, 1]\n\n    assert A[0, 0].subs(A, [[1, 2, 3], [4, 5, 6], [7, 8, 9]]) == 1\n\n    assert A[1, 2].subs(A, [[1, 2, 3], [4, 5, 6], [7, 8, 9]]) == 6\n\n    assert A[2, 1].subs(A, [[1, 2, 3], [4, 5, 6], [7, 8, 9]]) == 8\n"], "sample_964": ["def test_python_python_use_unqualified_type_names_enabled(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"n\"><span class=\"pre\">Name</span></span>' in content)\n    assert ('<span class=\"n\"><span class=\"pre\">Age</span></span>' in content)\n    assert ('<p><strong>name</strong> (<span class=\"n\"><span class=\"pre\">Name</span></span>) \u2013 blah blah</p>' in content)\n    assert '<p><strong>age</strong> (<span class=\"n\"><span class=\"pre\">Age</span></span>) \u2013 blah blah</p>' in content\n\n", "def test_python_python_use_unqualified_type_names_enabled(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"n\"><a class=\"reference internal\" href=\"#foo.Name\" title=\"foo.Name\">'\n            '<span class=\"pre\">Name</span></a></span>' in content)\n    assert '<span class=\"n\"><span class=\"pre\">foo.Age</span></span>' in content\n    assert ('<p><strong>name</strong> (<a class=\"reference internal\" href=\"#foo.Name\" '\n            'title=\"foo.Name\"><em>Name</em></a>) \u2013 blah blah</p>' in content)\n    assert '<p><strong>age</strong> (<em>foo.Age</em>) \u2013 blah blah</p>' in content\n", "def test_python_python_use_unqualified_type_names_enabled(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"n\"><a class=\"reference internal\" href=\"#Name\" title=\"Name\">'\n            '<span class=\"pre\">Name</span></a></span>' in content)\n    assert '<span class=\"n\"><span class=\"pre\">Age</span></span>' in content\n    assert ('<p><strong>name</strong> (<a class=\"reference internal\" href=\"#Name\" '\n            'title=\"Name\"><em>Name</em></a>) \u2013 blah blah</p>' in content)\n    assert '<p><strong>age</strong> (<em>Age</em>) \u2013 blah blah</p>' in content\n", "def test_info_field_list_inherited(app):\n    text = (\".. py:class:: Base\\n\"\n            \"   :param str name: blah blah\\n\"\n            \"   :type name: str\\n\"\n            \"\\n\"\n            \".. py:class:: Subclass\\n\"\n            \"   :inherit_doc:\\n\"\n            \"   :param int age: blah blah\\n\"\n            \"   :type age: int\\n\")\n    doctree = restructuredtext.parse(app, text)\n\n    assert_node(doctree, (addnodes.index,\n                          [desc, (desc_signature,\n                                  [desc_content, nodes.field_list, nodes.field])]))\n    assert_node(doctree[1][1][0][0], ([nodes.field_name, \"Parameters\"],\n                [nodes.field_body, nodes.bullet_list, ([nodes.list_item, nodes.paragraph],\n                [nodes.list_item, nodes.paragraph])]))\n\n    # :param str name:\n    assert_node(doctree[1][1][0][0][1][0][0],\n                ([addnodes.literal_strong, \"name\"],\n                 \" (\",\n                 [pending_xref, addnodes.literal_emphasis, \"str\"],\n                 \")\",\n                 \" -- \",\n                 \"blah blah\"))\n    assert_node(doctree[1][1][0][0][1][0][0][2], pending_xref,\n                refdomain=\"py\", reftype=\"class\", reftarget=\"str\",\n                **{\"py:module\": \"example\", \"py:class\": \"Base\"})\n\n    # :param int age:\n    assert_node(doctree[1][1][0][0][1][1][0],\n                ([addnodes.literal_strong, \"age\"],\n                 \" (\",\n                 [pending_xref, addnodes.literal_emphasis, \"int\"],\n                 \")\",\n                 \" -- \",\n                 \"blah blah\"))\n\n\n\n    assert_node(doctree[1][1][0][0][1][1][0][2], pending_xref,\n                refdomain=\"py\", reftype=\"class\", reft", "def test_python_python_use_unqualified_type_names_enabled(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"n\"><span class=\"pre\">Name</span></span>' in content)\n    assert ('<span class=\"n\"><span class=\"pre\">Age</span></span>' in content)\n    assert ('<p><strong>name</strong> (<span class=\"pre\">Name</span>) \u2013 blah blah</p>' in content)\n    assert ('<p><strong>age</strong> (<em>Age</em>) \u2013 blah blah</p>' in content)\n"], "sample_1175": ["def test_pretty_matrices_with_symbols():\n    from sympy.matrices import Matrix\n\n    A = Matrix([['x', 'y'], [1, 2]])\n    assert pretty(A) == \"\u23a1x  y\u23a4\\n\u23a31  2\u23a6\"\n    assert upretty(A) == \"\u23a1x  y\u23a4\\n\u23a31  2\u23a6\"\n\n    B = Matrix([[Symbol('a'), Symbol('b')], [Symbol('c'), Symbol('d')]])\n    assert pretty(B) == \"\u23a1a  b\u23a4\\n\u23a3c  d\u23a6\"\n    assert upretty(B) == \"\u23a1a  b\u23a4\\n\u23a3c  d\u23a6\"\n\n    C = Matrix([[A, Matrix([Symbol('e'), Symbol('f')])],\n                [Matrix([Symbol('g'), Symbol('h')]), Matrix([Symbol('i'), Symbol('j')])]])\n    assert pretty(C) == \"\u23a1\u23a1x  y\u23a4  \u23a1e  f\u23a4\u23a4\\n\u23a3\u23a11  2\u23a6  \u23a3i  j\u23a6\u23a6\"\n    assert upretty(C) == \"\u23a1\u23a1x   y\u23a4  \u23a1e   f\u23a4\u23a4\\n\u23a3\u23a11   2\u23a6  \u23a3i   j\u23a6\u23a6\"\n\n    D = Matrix([[1, 2], [3, 4]])\n    assert pretty(D) == \"\u23a11  2\u23a4\\n\u23a33  4\u23a6\"\n    assert upretty(D) == \"\u23a11  2\u23a4\\n\u23a33  4\u23a6\"\n\n\n\n", "compilation error", "def test_issue_18458():\n    from sympy.abc import x, y\n    assert pretty(x**y, use_unicode=True) == 'x^{y}'\n    assert upretty(x**y, use_unicode=True) == 'x\u02b8'\n", "compilation error", "def test_issue_18569():\n    from sympy.physics.quantum.states import TensorProduct\n    from sympy.physics.quantum.dagger import Dagger\n    a = QuantumOperator(\"a\")\n    b = QuantumOperator(\"b\")\n    state = TensorProduct(a, b)\n    assert pretty(Dagger(state)) == 'a\u2020 \u2297 b\u2020'\n    assert upretty(Dagger(state)) == 'a\u2020 \u2297 b\u2020' \n"], "sample_1064": ["def test_tensorflow_code_printing_of_functions():\n    if not tf:\n        skip(\"TensorFlow not installed\")\n    g = Function(\"g\", [x], x**2)\n    f = lambdify(x, g(x), 'tensorflow')\n    code = tensorflow_code(g(x))\n    assert code == \"tensorflow.math.square(x)\"\n\n\n", "compilation error", "def test_tensorflow_derivatives():\n    if not tf:\n        skip(\"TensorFlow not installed\")\n\n    x, y, z = symbols(\"x y z\")\n    expr = x**2 + y*z\n    f = lambdify((x, y, z), expr, 'tensorflow')\n    \n    grad_x = tf.gradients(expr, x)\n    grad_y = tf.gradients(expr, y)\n    grad_z = tf.gradients(expr, z)\n    \n    with tf.compat.v1.Session() as session:\n        r_x, r_y, r_z = session.run([grad_x, grad_y, grad_z], feed_dict={x: 1, y: 2, z: 3})\n        assert (r_x[0] == 2).all()\n        assert (r_y[0] == 3).all()\n        assert (r_z[0] == 2).all()\n\n\n\n", "compilation error", "compilation error"], "sample_219": ["    def test_resolve_output_field_errors(self):\n        with self.assertRaises(FieldError):\n            ExpressionWrapper(Value(3), output_field=CharField())\n\n        with self.assertRaises(FieldError):\n            ExpressionWrapper(F('field'), output_field=DateTimeField())\n\n\n\n", "    def test_empty_group_by_with_alias(self):\n        expr = ExpressionWrapper(Value(3), output_field=IntegerField(), alias='my_alias')\n        self.assertEqual(expr.get_group_by_cols(alias=None), [('my_alias',)])\n", "    def test_output_field_is_respected(self):\n        expr = ExpressionWrapper(F('age'), output_field=CharField())\n        self.assertEqual(expr.output_field, CharField())\n\n\n", "    def test_group_by_with_expressions(self):\n        expr = ExpressionWrapper(F('count') + 1, output_field=IntegerField())\n        self.assertEqual(expr.get_group_by_cols(alias=None), ['count'])\n", "    def test_output_field_overrides_expression(self):\n        expr = ExpressionWrapper(\n            Expression(Col('name')),\n            output_field=IntegerField(),\n        )\n        self.assertIsInstance(expr.output_field, IntegerField)\n"], "sample_506": ["def test_spines_circular():\n    fig, ax = plt.subplots()\n    ax.spines['top'].set_patch_circle((0.5, 0.5), 0.2)\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n", "def test_spines_circle():\n    fig, ax = plt.subplots()\n    ax.spines['top'].set_patch_circle((0.5, 0.5), 0.2)\n    ax.spines['bottom'].set_patch_circle((0.5, 0.5), 0.2)\n    ax.spines['left'].set_patch_circle((0.5, 0.5), 0.2)\n    ax.spines['right'].set_patch_circle((0.5, 0.5), 0.2)\n", "def test_circular_spine():\n    fig, ax = plt.subplots()\n    spine = SpinesProxy(\n        {'left': Spines.circular_spine(ax, (0.5, 0.5), 0.3)})\n    spine.set_color('red')\n    ax.plot([0.1, 0.9], [0.1, 0.9])\n", "def test_spines_set_bounds():\n    fig, ax = plt.subplots()\n    ax.hlines(y=0.5, xmin=0, xmax=2, linewidth=2, color='red')\n    ax.vlines(x=0.5, ymin=-1, ymax=2, linewidth=2, color='red')\n\n    ax.spines['top'].set_bounds(low=1, high=2)  # Extends top spine above the existing tick\n\n    ax.set_xlim(0, 2)\n    ax.set_ylim(-1, 2)\n", "def test_circle_spine():\n    fig, ax = plt.subplots()\n    spine = SpinesProxy(\n        {\n            'top': Spines.circular_spine(ax, (0.5, 0.5), 0.2, )\n        }\n    )\n    spine.top.set_facecolor('none')\n    spine.top.set_linewidth(3)\n\n    ax.set_xlim([0, 1])\n    ax.set_ylim([0, 1])\n\n\n\n"], "sample_258": ["    def test_receiver_decorator_kwargs(self):\n        @receiver(a_signal, sender=self)\n            self.state.append((val, kwargs))\n        self.state = []\n        a_signal.send(sender=self, val='test', extra='data')\n        self.assertEqual(self.state, [('test', {'extra': 'data'})])\n", "    def test_receiver_dispatch_uid(self):\n        @receiver(a_signal, dispatch_uid=\"unique_uid\")\n            self.state.append(val)\n\n        @receiver(a_signal, dispatch_uid=\"unique_uid\")\n            self.state.append(val)\n        self.state = []\n        a_signal.send(sender=self, val='test')\n        self.assertEqual(self.state, ['test', 'test'])\n        a_signal.disconnect(dispatch_uid=\"unique_uid\")\n        a_signal.send(sender=self, val='test')\n        self.assertEqual(self.state, ['test', 'test', 'test'])\n\n", "    def test_receiver_dispatch_uid(self):\n        @receiver(a_signal, dispatch_uid=\"unique_uid\")\n            self.state.append(val)\n        self.state = []\n        a_signal.send(sender=self, val=True)\n        self.assertEqual(self.state, [True])\n        @receiver(a_signal, dispatch_uid=\"unique_uid\")\n            self.state.append(val)\n        a_signal.send(sender=self, val=False)\n        self.assertEqual(self.state, [True, False])\n\n\n", "    def test_receiver_weak_reference(self):\n        class Receiver:\n                self.called = False\n\n                self.called = True\n\n        receiver = Receiver()\n        a_signal.connect(receiver)\n        del receiver\n        garbage_collect()\n\n        a_signal.send(sender=self, val='test')\n        self.assertFalse(receiver.called)\n        self.assertTestIsClean(a_signal)\n\n\n\n", "    def test_receiver_dispatch_uid(self):\n        @receiver(a_signal, dispatch_uid=\"uid\")\n            self.state.append(val)\n        @receiver(a_signal, dispatch_uid=\"uid\")\n            self.state.append(val)\n        \n        self.state = []\n        a_signal.send(sender=self, val=True)\n        self.assertEqual(self.state, ['True'])\n"], "sample_1133": ["def test_deviation_complex_medium():\n    from sympy.physics.optics.medium import Medium as ComplexMedium\n\n    n1 = ComplexMedium('m1', n=1, permittivity=e0)\n    n2 = ComplexMedium('m2', n=1.33, permittivity=e0)\n    r1 = Ray3D(Point3D(-1, -1, 1), Point3D(0, 0, 0))\n\n    assert isinstance(deviation(r1, n1, n2, plane=None), complex)\n    assert ae(deviation(r1, n1, n2, plane=None).real, 0.119, 5)  \n    assert ae(deviation(r1, n1, n2, plane=None).imag, 0.000, 5)\n", "compilation error", "compilation error", "compilation error", "    def test_transmission_reflection_coefficients():\n        n1, n2 = symbols('n1, n2')\n        i = Matrix([1, 1, 1])\n        assert all(ae(i, j, 5) for i, j in zip(fresnel_coefficients(0.5, 1, 1.33),\n                                              [0.11163, -0.17138, 0.83581, 0.82862]))\n        ans1 = fresnel_coefficients(0.5, 1, 1.33, \n                                    'trans')\n        ans2 = fresnel_coefficients(0.5, 1, 1.33, 'refl')\n        assert all(ae(i, j, 5) for i, j in zip(ans1, \n                                              [0.11163, -0.17138]))\n        assert all(ae(i, j, 5) for i, j in zip(ans2, \n                                              [0.83581, 0.82862]))\n"], "sample_94": ["    def test_create_permissions_with_custom_permissions(self):\n        \"\"\"\n        Test `create_permissions` with custom permissions.\n        \"\"\"\n        custom_permissions = [\n            ('my_custom_permission', 'Custom Permission Description'),\n            ('another_custom_permission', 'Another Custom Permission Description'),\n        ]\n        Permission._meta.permissions = custom_permissions\n        content_type = ContentType.objects.get_by_natural_key('auth', 'permission')\n        create_permissions(self.app_config, verbosity=0)\n\n        self.assertEqual(Permission.objects.filter(\n            content_type=content_type,\n        ).count(), len(custom_permissions))\n        for permission_name, permission_description in custom_permissions:\n            self.assertIn(\n                permission_name,\n                set(perm.name for perm in Permission.objects.all()),\n            )\n            self.assertEqual(\n                Permission.objects.get(name=permission_name).content_type,\n                content_type,\n            )\n            self.assertEqual(\n                Permission.objects.get(name=permission_name).codename,\n                permission_name,\n            )\n            self.assertEqual(\n                Permission.objects.get(name=permission_name).description,\n                permission_description,\n            )\n", "    def test_permissions_with_custom_app(self):\n        \"\"\"\n        Permissions are created for content types of a custom app\n        \"\"\"\n        from django.db import models\n\n        class MyModel(models.Model):\n            pass\n\n        # Register the custom app\n        self.app_config = apps.get_app_config('my_app')\n\n        from django.db import migrations\n\n            migrations.Migration(\n                id=1,\n                name=name,\n            )\n\n        with patch('django.core.management.version') as version:\n            version.get_migration_version.return_value = 0\n            with patch('django.apps.AppConfig.apps') as patched_apps:\n                patched_apps.return_value = apps\n                with patch('django.db.migrations.Migration.objects.create') as mock_create:\n                    mock_create.side_effect = _create_migration\n                    create_permissions(self.app_config, verbosity=0)\n                    self.assertEqual(len(mock_create.mock_calls), 1)\n\n        # Assert that permissions exist for custom model in the default app.\n        permission_content_type = ContentType.objects.get_by_natural_key('my_app', 'mymodel')\n        self.assertEqual(Permission.objects.filter(content_type=permission_content_type).count(), 4)\n\n\n\n", "    def test_create_permissions_handles_app_config_absence(self):\n        with self.assertNumQueries(0):\n            create_permissions(None, interactive=False, verbosity=0)\n", "    def test_create_permissions_with_custom_content_types(self):\n        original_content_types = ContentType.objects.all()\n\n        # Create a custom content type\n        custom_app_label = 'my_app'\n        custom_model_name = 'MyCustomModel'\n        custom_content_type = ContentType.objects.create(\n            app_label=custom_app_label, model=custom_model_name,\n        )\n\n        try:\n            # Patch apps to include our custom app\n            app_configs = [\n                apps.get_app_config(app) for app in [\n                    'contenttypes',\n                    'auth',\n                    custom_app_label\n                ]\n            ]\n\n            # Call create_permissions\n            create_permissions(\n                app_configs[1],\n                interactive=False,\n                verbosity=0,\n                apps=app_configs,\n            )\n\n            # Assert that permissions were created for the custom content type\n            self.assertEqual(\n                Permission.objects.filter(\n                    content_type=custom_content_type\n                ).count(),\n                5,\n            )\n\n        finally:\n            # Clean up the custom content type\n            custom_content_type.delete()\n            ContentType.objects.all().update(models=original_content_types)\n\n\n\n", "    def test_create_permissions_with_custom_permissions(self):\n        custom_permissions = [\n            ('my_custom_permission', 'My custom permission'),\n            ('another_custom_permission', 'Another custom permission'),\n        ]\n        Permission._meta.permissions = custom_permissions\n        create_permissions(self.app_config, verbosity=0)\n        self.assertEqual(Permission.objects.count(), len(custom_permissions))\n\n        permission_content_type = ContentType.objects.get_by_natural_key('auth', 'permission')\n        for permission_name, permission_label in custom_permissions:\n            self.assertTrue(Permission.objects.filter(\n                content_type=permission_content_type,\n                codename=permission_name,\n                name=permission_label,\n            ).exists())\n"], "sample_253": ["    def test_snapshot_files_handles_large_number_of_files(self):\n        num_files = 100\n        files = [self.ensure_file(self.tempdir / f\"file{i}.py\") for i in range(num_files)]\n        with mock.patch.object(self.reloader, 'watched_files', return_value=files):\n            snapshot = dict(self.reloader.snapshot_files())\n            for file in files:\n                self.assertIn(file, snapshot)\n", "    def test_snapshot_files_handles_files_with_same_mtime(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]) as mock_watched_files:\n            self.reloader.snapshot_files()\n            mock_watched_files.return_value = [self.existing_file, self.existing_file]  # Duplicate file\n            snapshot1 = dict(self.reloader.snapshot_files())  \n            snapshot2 = dict(self.reloader.snapshot_files())  \n            self.assertEqual(snapshot1, snapshot2)\n\n\n", "    def test_snapshot_files_handles_missing_files(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file, Path(\"/abs/path/to/nonexistent/file.py\")]):\n            snapshot = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot)\n            self.assertNotIn(\"/abs/path/to/nonexistent/file.py\", snapshot)  \n\n", "    def test_tick_ignores_non_existing_files(self, mock_notify_file_changed):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.nonexistent_file]):\n            ticker = self.reloader.tick()\n            next(ticker)\n        self.assertEqual(mock_notify_file_changed.call_count, 0)\n\n\n", "    def test_snapshot_files_with_duplicate_files(self):\n        file1 = self.ensure_file(self.tempdir / 'test1.py')\n        file2 = self.ensure_file(self.tempdir / 'test2.py')\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[file1, file2]):\n            snapshot = dict(self.reloader.snapshot_files())\n            self.assertIn(file1, snapshot)\n            self.assertIn(file2, snapshot)\n\n\n"], "sample_59": ["    def test_unique_together_validation(self):\n        \"\"\"\n        Test that unique_together constraints are enforced.\n\n        \"\"\"\n        Party.objects.create(name=\"Party A\", date=datetime.date(2023, 10, 26))\n        with self.assertRaises(ValidationError):\n            Party.objects.create(name=\"Party A\", date=datetime.date(2023, 10, 26))\n", "    def test_fk_validation(self):\n        \"\"\"\n        Regression test for #10404: Foreign key validation on object creation.\n        \"\"\"\n        d = Department.objects.create(name=\"Marketing\")\n        w = Worker(department=d, name=\"Full-time\")\n        w.full_clean()\n\n\n", "    def test_check_unique_together_constraint(self):\n        \"\"\"Test that unique_together constraint works as expected.\"\"\"\n        with self.assertRaises(ValidationError):\n            NonAutoPK.objects.create(name=\"unique_name\", department__name=\"unique_dept\")\n            NonAutoPK.objects.create(name=\"unique_name\", department__name=\"unique_dept\") \n\n\n", "    def test_related_object_validation(self):\n        d = Department.objects.create(name=\"IT\")\n        with self.assertRaises(ValidationError):\n            Worker(name=\"John Doe\", department=d, salary=-100)\n", "    def test_max_length_validation(self):\n        with self.assertRaises(ValidationError):\n            NonAutoPK.objects.create(name=\"a\" * 256)\n"], "sample_621": ["    def test_getitem_getitem_slice(self, indexes) -> None:\n        assert indexes[\"x\"][1:3].equals(indexes.get_unique()[\"x\"][1:3])\n        assert indexes[\"z\"][1:3].equals(indexes.get_unique()[\"z\"][1:3])\n        with pytest.raises(IndexError):\n            indexes[\"x\"][1:5]\n        with pytest.raises(IndexError):\n            indexes[\"z\"][5:6]\n        with pytest.raises(KeyError):\n            indexes[\"no_index\"]\n", "    def test_isel(self, indexes) -> None:\n        with pytest.raises(ValueError, match=\".*cannot perform isel on multi-index.*\"):\n            indexes.isel({\"z\": [0, 1, 2]})\n        actual = indexes.isel({\"x\": [0, 2]})\n        assert actual[\"x\"].equals(indexes[\"x\"][[0, 2]])\n        assert actual[\"y\"].equals(indexes[\"y\"])\n\n        with pytest.raises(ValueError, match=r\"cannot perform isel on multi-index.*\"):\n            indexes.isel({\"one\": [0, 1]})\n \n        actual = indexes.isel({\"one\": {\"a\": [0, 1], \"b\": [1, 2]}})\n        assert actual[\"z\"].equals(indexes[\"z\"][[0, 1, 2, 3]])\n", "    def test_isel(self, indexes) -> None:\n        assert indexes.isel({\"x\": 1, \"z\": slice(1, 3)}) == Indexes(\n            {\"x\": PandasIndex(pd.Index([2], name=\"x\"), \"x\"), \"y\": PandasIndex(pd.Index([4, 5, 6], name=\"y\"), \"y\"), \"z\": PandasMultiIndex(pd.MultiIndex.from_product([[\"a\", \"b\"], [2]], names=(\"one\", \"two\")), \"z\")},\n            indexes.variables\n        )\n        assert indexes.isel({\"x\": slice(1, 3), \"y\": 4}) == Indexes(\n            {\"x\": PandasIndex(pd.Index([2], name=\"x\"), \"x\"), \"y\": PandasIndex(pd.Index([4], name=\"y\"), \"y\"), \"z\": indexes.variables[\"z\"]},\n            indexes.variables\n        )\n", "    def test_join_multi_index(self, indexes) -> None:\n        midx = PandasMultiIndex.from_arrays(\n            [\n                [\"a\", \"b\"],\n                [1, 2],\n            ],\n            names=[\"one\", \"two\"],\n        )\n        midx2 = PandasMultiIndex.from_arrays(\n            [\n                [\"a\", \"c\"],\n                [1, 3],\n            ],\n            names=[\"one\", \"two\"],\n        )\n\n        # test that joining multi-index with itself works\n        result = indexes[\"z\"].join(midx)\n        expected = PandasMultiIndex(\n            pd.MultiIndex.from_product(\n                [[\"a\", \"b\", \"c\"], [1, 2, 3]], names=[\"one\", \"two\"]\n            ),\n            \"z\",\n        )\n        assert result.equals(expected)\n        assert result.level_coords_dtype == {\"one\": \"<U1\", \"two\": \"i\"}\n        assert result.dim == \"z\"\n\n        # test that joining with a different dimensions works\n        with pytest.raises(ValueError, match=r\".*conflicting dimensions.*\"):\n            indexes[\"x\"].join(midx)\n\n\n\n\n", "    def test_to_xr_indexes(self, indexes) -> None:\n        xr_indexes = indexes.to_xr_indexes()\n        assert isinstance(xr_indexes, Indexes)\n        assert all([isinstance(idx, PandasIndex) for idx in xr_indexes.values()])\n        assert indexes.variables == xr_indexes.variables\n"], "sample_449": ["    def test_close_connection(self):\n            start_response(\"200 OK\", [])\n            return [\"Hello World\"]\n\n        rfile = BytesIO(b\"GET / HTTP/1.0\\r\\n\")\n        rfile.seek(0)\n\n        wfile = UnclosableBytesIO()\n\n            if mode == \"rb\":\n                return rfile\n            elif mode == \"wb\":\n                return wfile\n\n        request = Stub(makefile=makefile)\n        server = Stub(base_environ={}, get_app=lambda: test_app)\n\n        # Prevent logging from appearing in test output.\n        with self.assertLogs(\"django.server\", \"INFO\"):\n            handler = WSGIRequestHandler(request, \"192.168.0.2\", server)\n            handler.close_connection = True\n            handler.handle()\n\n        wfile.seek(0)\n        lines = list(wfile.readlines())\n        body = lines[-1]\n        self.assertEqual(body, b\"Hello World\")\n\n", "    def test_connection_close(self):\n            start_response(\"200 OK\", [(\"Content-Length\", \"10\")])\n            yield b\"Hello World\"\n\n        rfile = BytesIO(b\"GET / HTTP/1.0\\r\\n\")\n        rfile.seek(0)\n\n        wfile = UnclosableBytesIO()\n\n            if mode == \"rb\":\n                return rfile\n            elif mode == \"wb\":\n                return wfile\n\n        request = Stub(makefile=makefile)\n        server = Stub(base_environ={}, get_app=lambda: test_app)\n        handler = WSGIRequestHandler(request, \"192.168.0.2\", server)\n\n        # Explicitly set Connection: close in the request\n        request.environ[\"HTTP_CONNECTION\"] = \"close\"\n\n        with self.assertLogs(\"django.server\", \"INFO\"):\n            handler.handle()\n\n        wfile.seek(0)\n        body = list(wfile.readlines())[-1]\n        self.assertEqual(body, b\"Hello World\")\n        self.assertIn(b\"Connection: close\\r\\n\", wfile.getvalue())\n\n\n\n", "    def test_content_length_header_sent_for_post_requests(self):\n        hello_world_body = b\"Hello World\"\n        content_length = len(hello_world_body)\n\n            \"\"\"A WSGI app that reflects the content length.\"\"\"\n            start_response(\"200 OK\", [(\"Content-Length\", str(content_length))])\n            return [hello_world_body]\n\n        rfile = BytesIO(\n            b\"POST / HTTP/1.0\\r\\n\"\n            b\"Content-Length: %d\\r\\n\" % content_length\n            b\"\\r\\n\"\n            b\"Hello World\"\n        )\n        rfile.seek(0)\n\n        wfile = UnclosableBytesIO()\n\n            if mode == \"rb\":\n                return rfile\n            elif mode == \"wb\":\n                return wfile\n\n        request = Stub(makefile=makefile)\n        server = Stub(base_environ={}, get_app=lambda: test_app)\n\n        # Prevent logging from appearing in test output.\n        with self.assertLogs(\"django.server\", \"INFO\"):\n            WSGIRequestHandler(request, \"192.168.0.2\", server)\n\n        wfile.seek(0)\n        lines = list(wfile.readlines())\n        body = lines[-1]\n        # The body should be \"Hello World\"\n        self.assertEqual(body, hello_world_body)\n        self.assertIn(f\"Content-Length: {content_length}\\r\\n\".encode(), lines)\n", "    def test_close_connection(self):\n        hello_world_body = b\"<!DOCTYPE html><html><body>Hello World</body></html>\"\n        content_length = len(hello_world_body)\n\n            \"\"\"A WSGI app that returns a hello world.\"\"\"\n            start_response(\"200 OK\", [])\n            yield hello_world_body\n\n        rfile = BytesIO(b\"GET / HTTP/1.0\\r\\n\")\n        rfile.seek(0)\n\n        wfile = UnclosableBytesIO()\n\n            if mode == \"rb\":\n                return rfile\n            elif mode == \"wb\":\n                return wfile\n\n        request = Stub(makefile=makefile)\n        server = Stub(base_environ={}, get_app=lambda: test_app)\n\n        with self.assertLogs(\"django.server\", \"INFO\"):\n            # Instantiating a handler runs the request as side effect.\n            handler = WSGIRequestHandler(request, \"192.168.0.2\", server)\n            handler.close_connection = True\n        wfile.seek(0)\n        lines = list(wfile.readlines())\n        \n        # 200 OK response is sent\n        self.assertEqual(lines[0].strip(), b\"HTTP/1.0 200 OK\\r\\n\")\n        \n        # Content-Length header is present in the response\n        self.assertIn(f\"Content-Length: {content_length}\\r\\n\".encode(), lines)  \n        \n        # Connection header indicates closure\n        self.assertIn(b\"Connection: close\\r\\n\", lines) \n\n\n        \n", "    def test_close_connection_header(self):\n            start_response(\"200 OK\", [])\n            yield b\"some content\"\n\n        rfile = BytesIO(b\"GET / HTTP/1.0\\r\\n\")\n        rfile.seek(0)\n        wfile = UnclosableBytesIO()\n\n            if mode == \"rb\":\n                return rfile\n            elif mode == \"wb\":\n                return wfile\n\n        request = Stub(makefile=makefile)\n        server = Stub(base_environ={}, get_app=lambda: test_app)\n\n        with self.assertLogs(\"django.server\", \"INFO\"):\n            # Instantiating a handler runs the request as side effect.\n            WSGIRequestHandler(request, \"192.168.0.2\", server)\n\n        wfile.seek(0)\n        lines = list(wfile.readlines())\n\n        # The connection should not be closed if the Content-Length header is set\n        self.assertNotIn(b\"Connection: close\\r\\n\", lines)\n"], "sample_246": ["    def test_custom_template_loader(self):\n        from django.template import Template\n\n        class MockTemplateLoader(Template.loader.Loader):\n                return Template(\"MOCK\")\n        with override_settings(TEMPLATES=[{'BACKEND': 'django.template.backends.dummy.Dummy',\n                                          'OPTIONS': {'loaders': [MockTemplateLoader()]}}]):\n            management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n            self.assertMsgId(\"MOCK\", self.PO_FILE)\n\n\n\n", "    def test_app_locale_paths(self):\n        \"\"\"\n        * Translations for an app containing a locale folder are stored in that folder\n        * translations outside of that app are in LOCALE_PATHS[0]\n        \"\"\"\n        with override_settings(LOCALE_PATHS=[os.path.join(self.test_dir, 'project_locale')]):\n            management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n            app_de_locale = os.path.join(\n                self.test_dir, 'app_with_locale', 'locale', 'de', 'LC_MESSAGES', 'django.po')\n            project_de_locale = os.path.join(\n                self.test_dir, 'project_locale', 'de', 'LC_MESSAGES', 'django.po')\n            self.assertTrue(os.path.exists(app_de_locale))\n            self.assertFalse(os.path.exists(project_de_locale))\n            with open(app_de_locale) as fp:\n                po_contents = fp.read()\n                self.assertMsgId('This app has a locale directory', po_contents)\n\n\n", "    def test_custom_encoding(self):\n        with open(os.path.join(self.test_dir, 'app_with_locale', '__init__.py'), 'w', encoding='utf-16') as f:\n            f.write('This is a test string.\\n')\n\n        with override_settings(LOCALE_PATHS=[os.path.join(self.test_dir, 'project_locale')]):\n            management.call_command('makemessages', locale=[LOCALE], verbosity=0, encoding='utf-16')\n            \n        project_de_locale = os.path.join(\n            self.test_dir, 'project_locale', 'de', 'LC_MESSAGES', 'django.po')\n        self.assertTrue(os.path.exists(project_de_locale))\n        with open(project_de_locale, encoding='utf-16') as fp:\n            po_contents = fp.read()\n            self.assertMsgId(\"This is a test string.\", po_contents)\n", "    def test_custom_locale_name(self):\n        with override_settings(LOCALE_NAME='django_locale'):\n            management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n            self.assertTrue(os.path.exists(os.path.join('django_locale', LOCALE, 'LC_MESSAGES', 'django.po')))\n", "    def test_custom_layout_with_template_dirs(self):\n        with override_settings(\n            LOCALE_PATHS=[\n                os.path.join(self.test_dir, 'project_locale'),\n                os.path.join(self.test_dir, 'app_with_locale', 'locale'),\n            ],\n            TEMPLATES=[\n                {'BACKEND': 'django.template.backends.django.DjangoTemplates',\n                 'DIRS': [os.path.join(self.test_dir, 'app_with_locale', 'templates'),],\n                 'OPTIONS': {}}\n            ]\n        ):\n            management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n            project_de_locale = os.path.join(\n                self.test_dir, 'project_locale', 'de', 'LC_MESSAGES', 'django.po')\n            app_de_locale = os.path.join(\n                self.test_dir, 'app_with_locale', 'locale', 'de', 'LC_MESSAGES', 'django.po')\n            self.assertTrue(os.path.exists(project_de_locale))\n            self.assertTrue(os.path.exists(app_de_locale))\n"], "sample_151": ["    def test_add_blank_foreignkey_without_default(self, mocked_ask_method):\n        \"\"\"\n        #23405 - Adding a NOT NULL and blank `ForeignKey` without\n        a default should not prompt for a default.\n        \"\"\"\n        changes = self.get_changes([self.author_empty], [self.author_with_publisher_blank])\n        self.assertEqual(mocked_ask_method.call_count, 0)\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0)\n\n", "    def test_alter_non_blank_textfield_and_charfield(self, mocked_ask_method):\n        \"\"\"\n        #23405 - Altering a NOT NULL and non-blank `CharField` or `TextField`\n        without default should prompt for a default.\n        \"\"\"\n        changes = self.get_changes([self.author_with_biography_non_blank], [self.author_with_biography_non_blank_blank])\n        self.assertEqual(mocked_ask_method.call_count, 2)\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0)\n", "    def test_add_field_with_default_value(self):\n        \"\"\"\n        #23405 - Adding a field with a default value should not prompt\n        for a default.\n        \"\"\"\n        changes = self.get_changes([self.author_empty],\n                                  [self.author_with_biography_default])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0,\n                                      default='Default Biography')\n", "    def test_add_blank_textfield_and_charfield_with_default(self):\n        \"\"\"\n        #23405 - Adding a NOT NULL and blank `CharField` or `TextField`\n        with a default should not prompt for a default.\n        \"\"\"\n        changes = self.get_changes([self.author_empty], [self.author_with_biography_blank_default])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0)\n", "    def test_add_field_to_field_with_default_value(self):\n        \"\"\"\n        Adding a field to a model with a field already having a default value\n        should work correctly.\n        \"\"\"\n        changes = self.get_changes([self.author_empty], [self.author_with_biography_and_email_default])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\", \"AddField\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"biography\") \n        self.assertOperationAttributes(changes, 'testapp', 0, 1, name=\"email\", default='test@example.com')\n\n"], "sample_1199": ["def test_tensor_product_trace():\n    assert Tr(TP(A, B)) == Tr(A)*Tr(B)\n    assert Tr(TP(A, TP(B, C))) == Tr(A*B*C)\n    assert Tr(TP(Dagger(A), B)) == Tr(Dagger(A)*B)\n    assert Tr(TP(A, Dagger(B))) == Tr(A*Dagger(B))\n", "def test_tensor_product_trace():\n    assert Tr(TP(A, B)) == Tr(A) * Tr(B)\n    assert Tr(TP(A, TP(B, C))) == Tr(A * B * C)\n    assert Tr(TP(OuterProduct(A, B), OuterProduct(C, D))) == \\\n        Tr(A*C * B*D)\n\n\n", "def test_tensor_product_trace():\n    assert Tr(TensorProduct(A, B)) == Tr(A)*Tr(B)\n    assert Tr(TensorProduct(A, Dagger(B))) == Tr(A)*Tr(Dagger(B))\n    assert Tr(TensorProduct(A, TensorProduct(B, C))) == \\\n        Tr(TensorProduct(A*B, C))\n    assert Tr(TensorProduct(A, B) * TensorProduct(C, D)) == \\\n        Tr(A*C)*Tr(B*D)\n\n\n\n", "def test_tensor_product_trace():\n    assert Tr(TP(A, B)) == Tr(A) * Tr(B)\n    assert Tr(TP(A, Dagger(B))) \\\n        == Tr(Dagger(A)) * Tr(B)\n    assert Tr(TP(Dagger(A), B)) == Tr(A) * Tr(B)\n    assert Tr(TP(Dagger(A), Dagger(B))) \\\n        == Tr(Dagger(A)) * Tr(Dagger(B))\n    assert Tr(TP(A, TP(B, C))) == Tr(TP(TP(A, B), C))\n", "def test_tensor_product_trace():\n    assert Tr(TensorProduct(A, B)) == Tr(A) * Tr(B)\n    assert Tr(TensorProduct(TP(A, B), TP(C,D))) == \\\n        Tr(TP(A, C)) * Tr(TP(B, D))\n"], "sample_204": ["    def test_loading_namespace_not_in_disk_migrations(self):\n        \"\"\"Migration directories without an __init__.py file are ignored.\"\"\"\n        loader = MigrationLoader(connection)\n        loader.load_disk()\n        migrations = [name for app, name in loader.disk_migrations if app == 'migrations']\n        self.assertEqual(migrations, [])\n", "    def test_circular_dependence(self):\n        \"\"\"\n        MigrationLoader should handle circular dependencies correctly\n        \"\"\"\n        with self.assertRaises(AmbiguityError) as context:\n            MigrationLoader(connection)\n        self.assertIn(\n            \"Migration 'migrations.0001_initial' depends on 'migrations.0002_second', \"\n            \"which in turn depends on 'migrations.0001_initial'\",\n            str(context.exception),\n        )\n", "    def test_missing_state(self):\n        \"\"\" MigrationLoader should handle migrations missing state information. \"\"\"\n        loader = MigrationLoader(connection)\n        with self.assertRaises(ImportError):\n            loader.build_graph()\n", "    def test_invalid_migration(self):\n        \"\"\"\n        MigrationLoader should raise an exception when a migration is invalid.\n\n        Django will attempt to load invalid migrations and should raise an\n        exception. The exception should be caught and re-raised with a more\n        helpful message.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        with self.assertRaises(Exception) as context:\n            loader.load_disk()\n        self.assertIn(\"Invalid migration\", str(context.exception))\n", "    def test_missing_migrations(self):\n        \"\"\"Tests that missing migrations don't crash the loader.\"\"\"\n        with self.assertRaises(FileNotFoundError):\n            MigrationLoader(connection)\n"], "sample_541": ["def test_polygon_selector_reset(fig_test, fig_ref, draw_bounding_box):\n    verts = [(0.1, 0.4), (0.5, 0.9), (0.3, 0.2)]\n    ax_test = fig_test.add_subplot()\n\n    tool_test = widgets.PolygonSelector(\n        ax_test, onselect=noop, draw_bounding_box=draw_bounding_box)\n    tool_test.verts = verts\n    assert tool_test.verts == verts\n    tool_test.reset()\n    assert tool_test.verts == []\n\n    ax_ref = fig_ref.add_subplot()\n    tool_ref = widgets.PolygonSelector(\n        ax_ref, onselect=noop, draw_bounding_box=draw_bounding_box)\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n        ('on_key_press', dict(key='escape')),\n    ]\n    for (etype, event_args) in event_sequence:\n        do_event(tool_ref, etype, **event_args)\n    assert tool_ref.verts == []\n", "def test_polygon_selector_empty_polygon(ax, draw_bounding_box):\n    tool = widgets.PolygonSelector(ax, onselect=noop,\n                                   draw_bounding_box=draw_bounding_box)\n    # Check that empty polygon is returned\n    assert tool.verts == []\n\n", "def test_rectangle_selector_remove_from_canvas(ax):\n    # Create a RectangleSelector\n    tool = widgets.RectangleSelector(ax, onselect=noop)\n    # Manually set initial start and end points\n    tool._start = (0, 0)\n    tool._end = (1, 1)\n\n    # Add a rectangle to the canvas\n    tool._draw()\n    \n    # Simulate removing the rectangle from the canvas\n    tool._handles_artists = []\n    tool._rectangle_artist = None\n    \n\n    # Make sure the rectangle is gone\n    assert len(ax.get_children()) == ax.get_children().index(ax.legend()) + 1 \n", "def test_polygon_selector_boundary_select(draw_bounding_box):\n    verts = [(20, 0), (40, 20), (60, 0), (40, -20)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[3]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    tool = widgets.PolygonSelector(ax, onselect=noop, draw_bounding_box=draw_bounding_box)\n\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    # In order to trigger the correct callbacks, trigger events on the canvas\n    # instead of the individual tools\n    t = ax.transData\n    canvas = ax.figure.canvas\n\n    # Select a point on the bounding box boundary.\n    MouseEvent(\n        \"button_press_event\", canvas, *t.transform((40, -20)), 1)._process()\n    MouseEvent(\n        \"button_release_event\", canvas, *t.transform((40, -20)), 1)._process()\n    assert tool.verts == verts\n\n", "def test_polygon_selector_empty_polygon(ax, draw_bounding_box):\n    event_sequence = [polygon_place_vertex(50, 50)]\n    \n    tool = widgets.PolygonSelector(ax, onselect=noop,\n                                   draw_bounding_box=draw_bounding_box)\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    \n    assert tool.verts == []\n\n\n"], "sample_256": ["    def test_password_validation(self):\n        user = User.objects.get(username='testclient')\n        data = {\n            'password1': 'too_short',\n            'password2': 'too_short',\n        }\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertIn(\n            'Your password must be at least 8 characters long.',\n            form.errors['password1'][0],\n        )\n\n        data = {\n            'password1': 'password',\n            'password2': 'password',\n        }\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertIn(\n            'Your password must have at least one lowercase letter, one uppercase letter, one digit, and one symbol.',\n            form.errors['password1'][0],\n        )\n", "    def test_password_validation(self):\n        user = User.objects.get(username='testclient')\n        data = {'password1': 'short', 'password2': 'short'}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertIn(\n            \"This password is too short. It must contain at least 8 characters.\",\n            form.errors['password1'],\n        )\n        self.assertIn(\n            \"This password is too short. It must contain at least 8 characters.\",\n            form.errors['password2'],\n        )\n", "    def test_password_length_validation(self):\n        user = User.objects.get(username='testclient')\n        data = {'password1': 'p', 'password2': 'p'}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['password1'], [form.error_messages['password_too_short']])\n\n", "    def test_password_too_short(self):\n        user = User.objects.get(username='testclient')\n        data = {'password1': 'too', 'password2': 'too'}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertIn('This password is too short. It must contain at least 8 characters.', form.errors['password1'])\n\n\n\n", "    def test_password_minimum_length(self):\n        user = User.objects.get(username='testclient')\n        data = {'password1': 'too short', 'password2': 'too short'}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertIn('This password is too short. It must contain at least 8 characters.', form.errors['password1'])\n        self.assertIn('This password is too short. It must contain at least 8 characters.', form.errors['password2'])\n\n"], "sample_1098": ["def test_appellf1_limits():\n    a, b1, b2, c, x, y = symbols('a b1 b2 c x y')\n    assert limit(appellf1(a, b1, b2, c, x, 0), x, S.Infinity) == 0\n    assert limit(appellf1(a, b1, b2, c, 0, y), y, S.Infinity) == 0\n    assert limit(appellf1(a, b1, b2, c, S.Zero, S.Zero), a, 0) == S.One\n", "compilation error", "def test_meijerg_as_hyper():\n    from sympy import meijerg, hyper\n    a, b, c, d, z = symbols('a b c d z')\n\n    assert meijerg([], [1], [0], [], z).as_hyper() == \\\n        hyper([], [1], z)\n    assert meijerg([1], [], [], [0], z).as_hyper() == \\\n        hyper([1], [], z)\n    assert meijerg([], [], [0], [], z).as_hyper() == \\\n        hyper([], [], z)\n    assert meijerg([1, 1], [], [1], [], z).as_hyper() == \\\n        hyper([1, 1], [1], z)\n\n", "def test_appellf1_eval():\n    a, b1, b2, c, x, y = symbols('a b1 b2 c x y')\n    assert appellf1(a, b1, b2, c, x, y).as_poly(x) == \\\n           appellf1(a, b1, b2, c, x, y).as_poly(y)\n    assert appellf1(0, 1, 1, 2, x, y) == 1\n    assert appellf1(1, 1, 1, 2, x, y) == 1/(1 - x*y)\n\n    # Test with special values\n    assert appellf1(1/2, 1/2, 1/2, 3/2, 1/2, 1/2) == 1/sqrt(2)\n\n\n\n\n\n", "def test_appellf1_expr_small():\n    a, b1, b2, c, x, y = symbols('a b1 b2 c x y')\n    from sympy.abc import z\n    assert appellf1(a, b1, b2, c, x, y).expr_small(z).func is appellf1\n    assert appellf1(a, b1, b2, c, x, y).expr_small(z).args == (a, b1, b2, c, x, y)\n    assert appellf1(a, b1, b2, c, S.Zero, S.Zero).expr_small(z) == S.One\n"], "sample_807": ["    def predict_proba(self, X):\n        return np.array([[0.5, 0.5], [0.8, 0.2], [0.3, 0.7]])\n", "    def predict_proba(self, X):\n        return np.random.random((X.shape[0], len(self.classes_)))\n", "    def predict(self, X):\n        return np.argmax(X, axis=-1)\n", "def test_calibration_with_predefined_classes(method):\n    # Test calibration with predefined classes.\n    X, y = make_classification(n_samples=100, n_features=6,\n                               random_state=42)\n    classes = [0, 2, 4]  \n    clf = MultinomialNB().fit(X, y)\n    cal_clf = CalibratedClassifierCV(clf, method=method, cv=2)\n    cal_clf.fit(X, y, classes=classes)\n    assert_array_equal(cal_clf.classes_, classes)\n    prob_pos_clf = cal_clf.predict_proba(X)[:, 1]\n    assert_array_almost_equal(prob_pos_clf,\n                             cal_clf.predict_proba(X)[:, 1])\n\n\n", "    def predict(self, X):\n        return np.ones(len(X)) * 2\n    "], "sample_349": ["    def test_i18n_selection(self):\n        with translation.override('fr'):\n            form = AlbumForm()\n            attrs = form['band'].field.widget.build_attrs({})\n            self.assertEqual(attrs['lang'], 'fr')\n\n", "    def test_i18n_attributes(self):\n        with translation.override('fr'):\n            form = AlbumForm()\n            attrs = form['band'].field.widget.get_context(name='my_field', value=None, attrs={})['widget']['attrs']\n        self.assertEqual(attrs['lang'], 'fr')\n\n", "    def test_render_options_with_disabled_selection(self):\n        form = AlbumForm()\n        output = form.as_table()\n        self.assertIn('<option value=\"\" disabled>-- Please select a band --</option>', output)\n", "    def test_autocomplete_url_with_language(self):\n        translation.activate('fr')\n        with self.subTest('Translation language'):\n            form = AlbumForm()\n            attrs = form['band'].field.widget.get_context(name='my_field', value=None, attrs={})['widget']['attrs']\n            self.assertEqual(attrs['lang'], 'fr')\n        translation.activate('en')\n\n\n", "    def test_render_options_foreign_key_with_null_value(self):\n        form = AlbumForm(initial={'band': None})\n        output = form.as_table()\n        self.assertIn(self.empty_option, output)\n\n\n"], "sample_898": ["compilation error", "    def _check_weighted_multilabel_average(metric, y_true, y_pred, sample_weight):\n        n_samples, n_classes = y_true.shape\n        # No averaging\n        label_measure = metric(y_true, y_pred, sample_weight= None)\n        assert_array_almost_equal(label_measure,\n                                  [metric(y_true_binarize[:, i],\n                                        y_pred_binarize[:, i],\n                                        sample_weight=None)\n                                   for i in range(n_classes)])\n\n        # weighted average\n        weighted_measure = metric(y_true, y_pred, sample_weight=sample_weight)\n        \n        assert_almost_equal(weighted_measure,\n                            np.average(label_measure,\n                                       weights=sample_weight))\n", "compilation error", "    def test_multilabel_sample_weight():\n       _, y_true = make_multilabel_classification(n_features=1, n_classes=5,\n               random_state=0, n_samples=100, allow_unlabeled=False)\n       _, y_pred = make_multilabel_classification(n_features=1, n_classes=5,\n               random_state=1, n_samples=100, allow_unlabeled=False)\n       sample_weight = np.random.randint(1, 10, size=len(y_true))\n       for name in MULTILABELS_METRICS + THRESHOLDED_MULTILABEL_METRICS:\n           metric = ALL_METRICS[name]\n           measure_with_weight = metric(y_true, y_pred, sample_weight=sample_weight)\n           measure_without_weight = metric(y_true, y_pred)\n           assert_almost_equal(measure_with_weight, measure_without_weight,\n                               err_msg=\"Metric '{}' failed sample weight test.\".format(name))\n", "compilation error"], "sample_114": ["    def test_add_field_with_choices(self, mocked_ask_method):\n        \"\"\"\n        #23405 - Adding a field with choices should prompt for choices.\n        \"\"\"\n        changes = self.get_changes([self.author_empty], [self.author_with_book_genre])\n        self.assertEqual(mocked_ask_method.call_count, 1)\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"book_genre\")\n\n", "    def test_alter_field_to_null_to_non_null_textfields(self, mocked_ask_method):\n        \"\"\"\n        #23405 - Adding a NOT NULL and non-blank `CharField` or `TextField`\n        without default should prompt for a default.\n        \"\"\"\n        changes = self.get_changes([self.author_with_biography_blank], [self.author_with_biography_non_blank])\n        self.assertEqual(mocked_ask_method.call_count, 2)\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"]) \n        self.assertOperationAttributes(changes, 'testapp', 0, 0)\n", "    def test_add_blank_textfield_and_charfield_with_default(self):\n        \"\"\"\n        #23405 - Adding a NOT NULL and blank `CharField` or `TextField`\n        with default should not prompt for a default.\n        \"\"\"\n        changes = self.get_changes([self.author_empty], [self.author_with_biography_blank_default])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0)\n\n\n", "    def test_add_nullable_non_blank_textfield_and_charfield(self, mocked_ask_method):\n        changes = self.get_changes([self.author_empty], [self.author_with_biography_nullable_non_blank])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\", \"AddField\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0)\n", "    def test_add_blank_foreignkey(self, mocked_ask_method):\n        \"\"\"\n        #23405 - Adding a NOT NULL and blank `ForeignKey` without\n        default should not prompt for a default.\n        \"\"\"\n        changes = self.get_changes([self.other_pony], [self.other_pony_with_blank_fk])\n        self.assertNumberMigrations(changes, 'otherapp', 1)\n        self.assertOperationTypes(changes, 'otherapp', 0, ['AddField'])\n        self.assertOperationAttributes(changes, 'otherapp', 0, 0)\n\n"], "sample_1131": ["def test_issue_17744():\n    from sympy import cosm1\n    \n    prntr = SciPyPrinter()\n    assert prntr.doprint(cosm1(x)) == 'scipy.special.cosm1(x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(cosm1(x)) == '  # Not supported in Python with NumPy:\\n  # cosm1\\ncosm1(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(cosm1(x)) == '  # Not supported in Python:\\n  # cosm1\\ncosm1(x)'\n", "def test_lambertw():\n    from sympy import lambertw\n\n    expr = lambertw(x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'scipy.special.lambertw(x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == '  # Not supported in Python with NumPy:\\n  # lambertw\\nlambertw(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == '  # Not supported in Python:\\n  # lambertw\\nlambertw(x)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr) == 'mpmath.lambertw(x)'\n\n\n\n", "def test_matrix_operations():\n    from sympy import MatrixSymbol, Matrix, eye, zeros, ones, diag, diagmatrix,\n        transpose, inv, det, sin, cos, exp, log, log10, sqrt\n\n\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 1, 5)\n    D = MatrixSymbol('D', 3, 4)\n    \n\n    p = NumPyPrinter()\n    assert p.doprint(eye(3)) == 'numpy.eye(3)'\n    assert p.doprint(zeros(2, 3)) == 'numpy.zeros((2, 3))'\n    assert p.doprint(ones(3, 2)) == 'numpy.ones((3, 2))'\n    assert p.doprint(diag(C)) == 'numpy.diag(C)'\n    assert p.doprint(diagmatrix(D)) == 'numpy.diagflat(D)'\n\n    assert p.doprint(transpose(A)) == 'numpy.transpose(A)'\n    assert p.doprint(inv(A)) == 'numpy.linalg.inv(A)'\n    assert p.doprint(det(A)) == 'numpy.linalg.det(A)'\n    assert p.doprint(sin(A)) == 'numpy.sin(A)'\n    assert p.doprint(cos(A)) == 'numpy.cos(A)'\n    assert p.doprint(exp(A)) == 'numpy.exp(A)'\n    assert p.doprint(log(A)) == 'numpy.log(A)'\n    assert p.doprint(log10(A)) == 'numpy.log10(A)'\n    assert p.doprint(sqrt(A)) == 'numpy.sqrt(A)'\n\n\n    \n\n\n\n\n", "def test_issue_15376():\n    from sympy import besselj, bessely, besseli, besselk\n\n    x = symbols('x')\n    n = symbols('n', integer=True)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(besselj(n, x)) == 'scipy.special.jv(n, x)'\n    assert prntr.doprint(bessely(n, x)) == 'scipy.special.yv(n, x)'\n    assert prntr.doprint(besseli(n, x)) == 'scipy.special.iv(n, x)'\n    assert prntr.doprint(besselk(n, x)) == 'scipy.special.kv(n, x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(besselj(n, x)) == 'numpy.jv(n, x)'\n    assert prntr.doprint(bessely(n, x)) == 'numpy.yv(n, x)'\n    assert prntr.doprint(besseli(n, x)) == 'numpy.iv(n, x)'\n    assert prntr.doprint(besselk(n, x)) == 'numpy.kv(n, x)'\n", "def test_SymPyPrinter_print_trig():\n    from sympy import sin, cos, tan, cot, sec, csc\n\n    prntr = SymPyPrinter()\n\n    assert prntr.doprint(sin(x)) == 'sympy.sin(x)'\n    assert prntr.doprint(cos(x)) == 'sympy.cos(x)'\n    assert prntr.doprint(tan(x)) == 'sympy.tan(x)'\n    assert prntr.doprint(cot(x)) == 'sympy.cot(x)'\n    assert prntr.doprint(sec(x)) == 'sympy.sec(x)'\n    assert prntr.doprint(csc(x)) == 'sympy.csc(x)'\n\n\n"], "sample_914": ["compilation error", "    def test_unparse_keyword_arguments():\n        source = \"def func(a, *, b=1, c=2): pass\"\n        module = ast.parse(source)\n        expected = \"def func(a, *, b=1, c=2): pass\"\n        assert ast.unparse(module.body[0]) == expected\n\n\n\n\n", "    def test_unparse_functiondef(source, expected):\n        module = ast.parse(source)\n        assert ast.unparse(module.body[0]) == expected\n", "    def test_unparse_keywords():\n        source = \"def my_function(arg1=1, arg2, *args, **kwargs):\\n    pass\"\n        module = ast.parse(source)\n        assert ast.unparse(module.body[0]) == \"\"\"def my_function(arg1=1, arg2, *args, **kwargs):", "    def test_unparse_assignment(source, expected):\n        module = ast.parse(source)\n        # There could be assignments in ast.body. We check the first one\n        assert ast.unparse(module.body[0].value) == expected\n"], "sample_179": ["    def test_deferrable_unique_constraint_required_db_features(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                required_db_features = {'supports_deferrable_unique_constraints'}\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['age'],\n                        name='unique_age_deferrable',\n                        deferrable=models.Deferrable.DEFERRED,\n                    ),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [])\n", "    def test_deferrable_unique_constraint_required_db_features(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                required_db_features = {'supports_deferrable_unique_constraints'}\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['age'],\n                        name='unique_age_deferrable',\n                        deferrable=models.Deferrable.DEFERRED,\n                    ),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [])\n\n", "    def test_deferrable_unique_constraint_required_db_features(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                required_db_features = {'supports_deferrable_unique_constraints'}\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['age'],\n                        name='unique_age_deferrable',\n                        deferrable=models.Deferrable.DEFERRED,\n                    ),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [])\n", "    def test_deferrable_unique_constraint_required_db_features(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                required_db_features = {'supports_deferrable_unique_constraints'}\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['age'],\n                        name='unique_age_deferrable',\n                        deferrable=models.Deferrable.DEFERRED,\n                    ),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [])\n", "    def test_deferrable_unique_constraint_required_db_features(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                required_db_features = {'supports_deferrable_unique_constraints'}\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['age'],\n                        name='unique_age_deferrable',\n                        deferrable=models.Deferrable.DEFERRED,\n                    ),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [])\n\n\n"], "sample_643": ["def test_get_color_style_default(linter):\n    output = StringIO()\n    linter.reporter.out = output\n    linter.open()\n    linter.add_message(\"C0301\", line=1, args=(1, 2))\n\n    msg = linter.reporter.format_message.format_message(linter.messages[0])\n    assert msg[0] == \"\\033[0m\"\n    assert msg[-1] == \"\\033[0m\"\n", "def test_output_error_message(linter: PyLinter) -> None:\n    \"\"\"Test that error messages are correctly handled in the output.\"\"\"\n    output = StringIO()\n    linter.reporter.out = output\n    linter.config.msg_template = \"{path}:{line}:{msg_id}: {msg}\"\n    linter.open()\n    linter.set_current_module(\"my_mod\")\n    linter.add_message(\n        \"line-too-long\", line=1, args=(1, 2), severity=Message.ERROR\n    )\n    linter.generate_reports()\n    assert (\n        output.getvalue()\n        == \"my_mod:1:line-too-long: Line too long (1/2)\\n\"\n    )\n", "def test_template_option_with_custom_symbols(linter: PyLinter) -> None:\n    output = StringIO()\n    linter.reporter.out = output\n    linter.config.msg_template = \"{path}:{line}:{symbol} ({msg_id}): {msg}\"\n    linter.open()\n    linter.set_current_module(\"my_mod\")\n\n    linter.add_message(\n        \"line-too-long\", line=1, args=(1, 2), symbol=\"my_symbol\"\n    )\n    out_lines = output.getvalue().split(\"\\n\")\n    assert out_lines[1] == \"my_mod:1:my_symbol (line-too-long): Line too long (1/2)\"\n\n\n", "def test_template_option_with_custom_key(linter) -> None:\n    output = StringIO()\n    linter.reporter.out = output\n    template = \"{path}:{line}:{category} ({symbol}): {msg} ({msg_id})\"\n    linter.config.msg_template = template\n    linter.open()\n    linter.set_current_module(\"my_mod\")\n    linter.add_message(\"C0301\", line=1, args=(1, 2))\n    assert output.getvalue() == \"my_mod:1:convention (C0301): Line too long (1/2) (line-too-long)\\n\"\n\n\n", "def test_handle_message_with_non_string_message(linter: PyLinter) -> None:\n    reporter = TextReporter()\n    linter.reporter = reporter\n\n    with pytest.raises(TypeError) as exc:\n        linter.add_message(\n            \"C0301\", line=1, args=(1, 2), msg=123\n        )\n    assert \"must be a string\" in str(exc)\n\n\n\n"], "sample_243": ["    def test_nested_filters_with_related_lookups_in_OR(self):\n        query = Query(Author)\n        where = query.build_where(\n            Q(items__creator__name='foo') | Q(items__content='bar')\n        )\n        self.assertEqual(len(where.children), 2)\n\n        first_lookup = where.children[0]\n        self.assertIsInstance(first_lookup, Q)\n        self.assertEqual(first_lookup.connector, AND)\n        self.assertIsInstance(first_lookup.children[0], RelatedThrough)\n        self.assertEqual(first_lookup.children[0].lhs.target, Author._meta.get_field('items'))\n        self.assertIsInstance(first_lookup.children[0].rhs, Exact)\n        self.assertEqual(first_lookup.children[0].rhs.target, 'foo')\n\n        second_lookup = where.children[1]\n        self.assertIsInstance(second_lookup, Q)\n        self.assertEqual(second_lookup.connector, AND)\n        self.assertIsInstance(second_lookup.children[0], RelatedThrough)\n        self.assertEqual(second_lookup.children[0].lhs.target, Author._meta.get_field('items'))\n        self.assertIsInstance(second_lookup.children[0].rhs, Exact)\n        self.assertEqual(second_lookup.children[0].rhs.target, 'bar')\n", "    def test_join_promotion(self):\n        query = Query(Author)\n        query.extra_joins = [\n            'JOIN Item ON Author.id = Item.creator_id',\n        ]\n        where = query.build_where(Q(num__gt=2) & Q(Item__name__icontains='foo'))\n        join_promoter = JoinPromoter(query.connector, len(where.children), False)\n        for join in query.extra_joins:\n            join_promoter.add_votes([join])\n        join_promoter.update_join_types(query)\n        \n\n", "    def test_ordering_with_f_expression(self):\n        query = Query(Item)\n        order = query.build_order_by([('name', 'ASC'), (F('created'), 'DESC')])\n\n        self.assertEqual(order, [('name', 'ASC'), ('created', 'DESC')])\n\n\n", "    def test_multiple_filters_with_nested_joins(self):\n        query = Query(Item)\n        where = query.build_where(\n            Q(creator__author__name='John') & Q(modified__gt=F('created'))\n        )\n        self.assertEqual(len(where.children), 2)\n        creator_lookup = where.children[0]\n        self.assertIsInstance(creator_lookup, Q)\n        self.assertEqual(len(creator_lookup.children), 1)\n        author_lookup = creator_lookup.children[0]\n        self.assertIsInstance(author_lookup, Exact)\n        self.assertEqual(author_lookup.rhs, 'John')\n        self.assertEqual(author_lookup.lhs.target, Author._meta.get_field('name'))\n        modified_lookup = where.children[1]\n        self.assertIsInstance(modified_lookup, GreaterThan)\n        self.assertIsInstance(modified_lookup.rhs, Col)\n        self.assertIsNone(modified_lookup.rhs.alias)\n        self.assertEqual(modified_lookup.lhs.target, Item._meta.get_field('modified'))\n", "    def test_join_promotion(self):\n        query = Query(Item)\n        query.add_join(Author, 'creator', 'on', 'id=creator_id')\n        query.set_select_fields([Col('id'), F('creator__name')])\n        promote_joins = JoinPromoter(connector=OR, num_children=2, negated=False)\n        promote_joins.add_votes({'author': 2})\n        promote_joins.update_join_types(query)\n        self.assertEqual(query.alias_map['author'].join_type, \"LEFT OUTER\")\n\n"], "sample_120": ["    def test_serialize_custom_field(self):\n        class CustomField(models.Field):\n                super().__init__(*args, **kwargs)\n\n                return 'CustomField'\n\n                if value is None:\n                    return None\n                return value\n\n        field = CustomField()\n        string, imports = MigrationWriter.serialize(field)\n        self.assertEqual(string, \"models.Field\")\n        self.assertEqual(imports, {'from django.db import models'})\n\n", "    def test_serialize_custom_field(self):\n        class MyCustomField(models.Field):\n                super().__init__(*args, **kwargs)\n\n                return ('MyCustomField', [], {})\n        \n        field = MyCustomField()\n        string = MigrationWriter.serialize(field)[0]\n        self.assertEqual(string, \"MyCustomField()\")\n", "    def test_serialize_function_with_kwargs(self):\n            return a, kwargs\n\n        serialized_function, _ = MigrationWriter.serialize(my_function)\n        self.assertEqual(serialized_function, 'lambda a, **kwargs: (a, kwargs)')\n        \n\n\n", "    def test_serialize_custom_field(self):\n        class CustomField(models.Field):\n                super().__init__(**kwargs)\n\n                return 'my_custom_type'\n\n                return forms.CharField(**kwargs)\n\n                return value\n\n        class MyModel(models.Model):\n            myfield = CustomField()\n\n        migration = type(\"Migration\", (migrations.Migration,), {\n            \"operations\": [\n                migrations.CreateModel(\"MyModel\", (), {}, (MyModel,)),\n            ],\n            \"dependencies\": []\n        })\n        writer = MigrationWriter(migration)\n        output = writer.as_string()\n        self.assertIn(\"class CustomField(models.Field):\", output)\n        self.assertIn(\"db_type('my_custom_type')\", output)\n        self.assertIn('myfield = CustomField()', output)\n\n\n\n\n", "    def test_serialize_custom_field(self):\n        class MyCustomField(models.Field):\n                super().__init__(**kwargs)\n\n                return 'MyCustomType'\n\n                return 'MyCustomType'\n\n                super().contribute_to_class(cls, name)\n                cls._meta.get_field_by_name(name).custom_field_type = self\n\n        class MyModel(models.Model):\n            my_field = MyCustomField()\n\n        migration = type(\"Migration\", (migrations.Migration,), {\n            \"operations\": [\n                migrations.CreateModel(\"MyModel\", ((\"my_field\", MyCustomField()),), {}, (models.Model,))\n            ],\n            \"dependencies\": []\n        })\n        writer = MigrationWriter(migration)\n        output = writer.as_string()\n        self.assertIn(\"custom_field_type\", output)\n        self.assertIn(\"MyCustomType\", output)\n"], "sample_940": ["def test_is_builtin_class_method_with_closure():\n        x = 10\n            return x\n\n        return inner_func\n\n    class MyClass(type):\n            attrs['method'] = closure_func()\n            return super().__new__(cls, clsname, bases, attrs)\n\n    MyClass.__name__ = 'MyClass'\n    \n    MyClass.__module__ = 'test_util_inspect'\n    MyClassMeta = MyClass('MyClass', (), {})\n    \n    class MyInstance(metaclass=MyClassMeta):\n        pass\n\n    assert inspect.is_builtin_class_method(MyInstance.method) is False\n    assert inspect.is_builtin_class_method(MyInstanceMeta.method) is False\n", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_536": ["def test_polygon_selector_tool_click_on_existing_point(ax, draw_bounding_box):\n    verts = [(50, 50), (150, 50), (50, 150)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    tool = widgets.PolygonSelector(ax, onselect=noop, draw_bounding_box=draw_bounding_box)\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    # Click on an existing point\n    event = ('button_press_event', dict(xdata=150, ydata=50))\n    do_event(tool, event[0], **event[1])\n\n    # Assert that no new vertice is added\n    assert len(tool.verts) == 3\n\n\n", "compilation error", "def test_polygon_selector_empty_polygon(draw_bounding_box):\n    verts = []\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    tool = widgets.PolygonSelector(ax, onselect=noop, draw_bounding_box=draw_bounding_box)\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n    assert tool.verts == []\n\n\n\n\n", "def test_polygon_selector_click_outside(ax, draw_bounding_box):\n    verts = [(50, 50), (150, 50), (50, 150)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    tool = widgets.PolygonSelector(ax, onselect=noop, draw_bounding_box=draw_bounding_box)\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n    # Click outside the polygon\n    t = ax.transData\n    canvas = ax.figure.canvas\n\n    MouseEvent(\"button_press_event\", canvas, *t.transform((200, 200)), 1)._process()\n    assert len(tool.verts) == 3\n    assert tool.onselect.call_count == 0\n\n\n\n", "def test_polygon_selector_clear(ax, draw_bounding_box):\n    verts = [(50, 50), (150, 50), (50, 150)]\n    tool = widgets.PolygonSelector(ax, onselect=noop, draw_bounding_box=draw_bounding_box)\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n    tool.clear()\n    assert len(tool.verts) == 0\n\n\n"], "sample_523": ["def test_legend_handler_for_patch():\n    fig, ax = plt.subplots()\n    patch = patches.Rectangle((0, 0), 1, 1, color='red', label='patch')\n    ax.add_patch(patch)\n    legend = ax.legend()\n    assert isinstance(legend.legendHandles[0], patches.Rectangle) \n\n\n", "def test_legend_handles_with_custom_patches():\n    patch = plt.Rectangle((0, 0), 1, 1,\n                          facecolor='red', edgecolor='black')\n    handles = [patch]\n    labels = ['Custom Patch']\n    fig, ax = plt.subplots()\n    leg = ax.legend(handles, labels)\n    assert isinstance(leg.legendHandles[0], mpl.patches.Rectangle)\n\n\n\n", "def test_legend_handles_inheritance():\n    class MyLine2D(mlines.Line2D):\n        pass\n    line = MyLine2D([0], [0], color='red', label='test')\n    legend = fig.legend([line])\n    assert isinstance(legend.legendHandles[0], MyLine2D)\n", "def test_set_legend_title_using_dict():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1], label='line')\n    ax.legend(title={'label': 'My Legend'})\n    assert ax.get_legend().get_title().get_text() == 'My Legend'\n\n", "def test_legend_fontsize_inheritance():\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='test')\n\n    mpl.rcParams['legend.fontsize'] = 12\n\n    leg = ax.legend(fontsize=10)\n    assert leg.get_fontsize() == 10\n\n    leg2 = ax.legend(fontsize=None)\n    assert leg2.get_fontsize() == 12\n"], "sample_561": ["def test_marker_transformed_invalid():\n    marker = markers.MarkerStyle(\"o\")\n    with pytest.raises(TypeError):\n        new_marker = marker.transformed(1)\n    with pytest.raises(TypeError):\n        new_marker = marker.transformed(marker=Affine2D().translate(1, 1))\n", "def test_marker_transformed_with_user_transform():\n    marker = markers.MarkerStyle(\"o\", transform=Affine2D().rotate_deg(45))\n    t = Affine2D().translate(1, 1)\n    new_marker = marker.transformed(t)\n    assert new_marker is not marker\n    assert new_marker.get_user_transform() == marker.get_user_transform() * t\n    assert marker._user_transform is not new_marker._user_transform\n", "def test_marker_path_creation(marker):\n    marker_style = markers.MarkerStyle(marker)\n    # Make sure the marker path has the expected number of vertices\n    path = marker_style.get_path()\n    assert len(path.vertices) > 0\n\n\n", "def test_marker_path(marker, path, expected_path):\n    new_marker = marker.copy()\n    new_marker._path = path  # Override the path\n    new_path = new_marker._get_path()\n    assert new_path == expected_path\n\n\n", "def test_marker_path_validation(marker):\n    marker_style = markers.MarkerStyle(marker)\n    try:\n        marker_path = marker_style.get_path()\n        assert isinstance(marker_path, Path)\n    except AttributeError:\n        assert marker_style._path is None, \"Path should not be cached for unsupported markers\"\n\n\n\n"], "sample_911": ["    def check_target(role, tag, target, expected_classes):\n        pattern = (r'{role}-role:.*?'\n                   r'<(?P<tag>{tag}) .*?class=[\"\\'](?P<classes>.*?)[\"\\'].*?>'\n                   r'.*'\n                   r'</(?P=tag)>').format(role=role, tag=tag)\n        result = re.search(pattern, output)\n        assert result, expect\n        classes_found = set(result.group('classes').split())\n        assert classes_found == expected_classes, \\\n            f\"Classes for `role: {role}, tag: {tag}, target: {target}`\"\n\n", "    def concepts(role, tag):\n        pattern = (r'{role}-role:.*?'\n                   r'<(?P<tag>{tag}) .*?class=[\"\\'](?P<classes>.*?)[\"\\'].*?>'\n                   r'.*'\n                   r'</(?P=tag)>').format(role=role, tag=tag)\n        result = re.search(pattern, output)\n        expect = '''\\", "    def check_type_references():\n        pattern = r'\\bstd::vector\\s*<int>\\b'\n        result = re.search(pattern, output)\n        assert result, '''\\", "def test_build_domain_cpp_qualified_alias(app, status, warning):\n    app.builder.build_all()\n\n    test = 'qualified_alias.html'\n    output = (app.outdir / test).read_text()\n\n    assert 'QualifiedAlias::sub' in output\n    assert 'QualifiedAlias::sub::sub' in output\n\n\n\n", "    def check_classes(classes_set):\n        pattern = r'class{classes_set}.*?' + r'<a.*?>.*?</a>'\n        result = re.search(pattern, output)\n        expect = '''\\"], "sample_1165": ["compilation error", "compilation error", "def test_quaternion_integration():\n    q1 = Quaternion(1, 2, 3, 4)\n    assert integrate(q1, x) == Quaternion(x, 2*x, 3*x, 4*x)\n    assert integrate(q1, (x, 0, pi)) == Quaternion(pi, 2*pi, 3*pi, 4*pi)\n", "def test_quaternion_integration():\n   \n   q = Quaternion(1, 2, 3, 4)\n\n   integrated_q = integrate(q,x)\n\n   assert integrated_q == Quaternion(x, 2*x, 3*x, 4*x) \n\n", "def test_quaternion_trigsimp():\n    q1 = Quaternion(sqrt(2), 0, 0, sqrt(3))\n    assert trigsimp(q1.pow_cos_sin(2)) == Quaternion(30, 60*sqrt(29)/29, 90*sqrt(29)/29, 120*sqrt(29)/29)\n    q2 = Quaternion(cos(pi/8), 0, 0, sin(pi/8))\n    assert trigsimp(q2.to_axis_angle()) == ( (0, 0, sin(pi/8)/Abs(sin(pi/8))),  pi/4)\n    assert trigsimp(q2.to_rotation_matrix()) == Matrix([\n                [cos(pi/8), -sin(pi/8), 0],\n                [sin(pi/8),  cos(pi/8), 0],\n                [0,           0,          1]])\n\n\n\n"], "sample_478": ["    def test_actions_permissions_invalid_type(self):\n        @admin.action(permissions=[\"invalid_permissions\"])\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (action,)\n\n        self.assertIsInvalid(\n            BandAdmin,\n            Band,\n            \"permissions must be a list or tuple of strings, not 'invalid_permissions'.\",\n            id=\"admin.E125\",\n        ) \n", "    def test_actions_invalid_type(self):\n        class BandAdmin(ModelAdmin):\n            actions = 10\n\n        self.assertIsInvalid(\n            BandAdmin,\n            Band,\n            \"The value of 'actions' must be a list or tuple.\",\n            id=\"admin.E128\",\n        )\n\n\n", "    def test_actions_not_callable(self):\n        class BandAdmin(ModelAdmin):\n            actions = (\"not_callable\",)\n\n        self.assertIsInvalid(\n            BandAdmin,\n            Band,\n            \"The value of 'actions[0]' must be a function.\",\n            id=\"admin.E131\",\n        )\n", "    def test_actions_with_permissions_require_has_method(self):\n        @admin.action(permissions=[\"custom_permission\"])\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (custom_permission_action,)\n\n        self.assertIsInvalid(\n            BandAdmin,\n            Band,\n            \"BandAdmin must define a has_custom_permission() method for the \"\n            \"custom_permission_action action.\",\n            id=\"admin.E129\",\n        )\n\n\n", "    def test_actions_not_callable(self):\n        class BandAdmin(ModelAdmin):\n            actions = [\"not_callable\"]\n\n        self.assertIsInvalid(\n            BandAdmin,\n            Band,\n            \"The value of 'actions[0]' must be a callable.\",\n            id=\"admin.E131\",\n        )\n\n"], "sample_896": ["    def test_nmf_batch_size_validation(Estimator):", "def test_nmf_custom_init_with_n_components_auto():\n    # Check that n_components is correctly inferred when init=\"custom\"\n    # and n_components=\"auto\".\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((6, 5))\n    W_init = rng.random_sample((6, 3))\n    H_init = rng.random_sample((3, 5))\n    W, H, _ = non_negative_factorization(\n        X,\n        W=W_init,\n        H=H_init,\n        init=\"custom\",\n        n_components=\"auto\",\n    )\n    assert W.shape == (X.shape[0], H_init.shape[0])\n    assert H.shape == (H_init.shape[0], X.shape[1])\n\n\n\n", "def test_nmf_init_from_existing_components():\n    rng = np.random.RandomState(0)\n    X = rng.rand(10, 5)\n    W = rng.rand(10, 3)\n    H = rng.rand(3, 5)\n    nmf = NMF(n_components=3, init=\"custom\")\n    W_init, H_init = nmf._initialize_nmf(\n        X, n_components=3, init=\"custom\", random_state=0\n    )\n    \n    # Test with existing valid components\n    with pytest.warns(UserWarning, match=\"The existing W\"):\n        nmf = NMF(n_components=3, init=(W, H))\n        nmf.fit(X)\n    \n    # Test with existing components of wrong shape\n    with pytest.raises(ValueError, match=\"Shape mismatch in provided W\"):\n        nmf = NMF(n_components=3, init=(W, rng.rand(2, 5)))\n        nmf.fit(X)\n\n\n\n", "compilation error", "def test_nmf_n_iter_max_no_improvement(solver):\n    # Test that max_no_improvement works correctly by checking the number of iterations\n    rng = np.random.mtrand.RandomState(42)\n    X = rng.randn(20, 15)\n    nmf = NMF(\n        n_components=5,\n        tol=1e-3,\n        max_iter=50,\n        max_no_improvement=3,\n        solver=solver,\n        random_state=0,\n    )\n    with warnings.catch_warnings(record=True) as warnings_list:\n        warnings.simplefilter(\"always\")\n        W = nmf.fit_transform(X)\n\n    assert len(warnings_list) == 1\n    assert \"max_no_improvement\" in str(warnings_list[0].message)\n\n\n\n"], "sample_776": ["def test_lars_path_with_nan():\n    # Test handling of NaN values in the input data\n    X = np.array([[1, 2, np.nan],\n                  [4, 5, 6],\n                  [7, 8, 9]])\n    y = np.array([10, 11, 12])\n    with pytest.raises(ValueError):\n        linear_model.lars_path(X, y)\n", "def test_lars_handle_sparse_data():\n    # Ensure that Lars objects handle sparse input data correctly.\n    import sparse\n\n    n_samples, n_features = 100, 500\n    # Generate a sparse matrix\n    data = np.random.rand(n_samples, n_features)\n    row_indices = np.arange(n_samples)\n    col_indices = np.random.randint(n_features, size=n_samples * n_features)\n    X_sparse = sparse.csr_matrix((data.flatten(), (row_indices, col_indices)),\n                                 shape=(n_samples, n_features))\n\n    y = np.random.rand(n_samples)\n\n    # Test LassoLars\n    lars = linear_model.Lars(fit_intercept=False)\n    lars.fit(X_sparse, y)\n    assert lars.coef_.shape == (n_features,)\n\n    # Test LassoLarsCV\n    lars_cv = linear_model.LassoLarsCV(cv=5, fit_intercept=False)\n    lars_cv.fit(X_sparse, y)\n\n    # Test LassoLarsIC\n    lars_ic = linear_model.LassoLarsIC(model_selection='bic', fit_intercept=False)\n    lars_ic.fit(X_sparse, y)\n", "compilation error", "def test_sparse_data():\n    # Test that the estimator works correctly when provided with sparse\n    # input data.\n    n_samples = 100\n    n_features = 1000\n    X = sp.random.rand(n_samples, n_features)\n    y = np.random.rand(n_samples)\n    lars = linear_model.LassoLars(alpha=0.1)\n    lars.fit(X, y)\n    assert isinstance(lars.coef_, sp.csr_matrix)\n    assert np.all(lars.coef_ == lars.coef_)\n\n\n\n", "def test_lars_path_with_precompute():\n    # Tests both Lars and LassoLars with precompute. Lars, unlike LassoLars,\n    # should only be able to compute the path if fit_intercept is False\n    #  \n    # Note: precompute may not be used very often as it is deprecated\n    # in favor of the more powerful fit_path method\n\n    X = 3 * diabetes.data\n    y = diabetes.target\n\n    # Test Lars with precompute\n    lars = linear_model.Lars(fit_intercept=False)\n    with assert_warns(DeprecationWarning, match=\"precompute\" ):\n        lars_path = lars.fit_path(X, y, precompute=True)\n    assert lars_path.shape[1] == 1\n    # Test LassoLars with precompute\n    lasso_lars = linear_model.LassoLars(fit_intercept=False)\n    with assert_warns(DeprecationWarning, match=\"precompute\" ):\n        lasso_lars_path = lasso_lars.fit_path(X, y, precompute=True)\n    assert lasso_lars_path.shape[1] == 1   \n    lasso_lars = linear_model.LassoLars(fit_intercept=True) # Should work\n    lasso_lars_path = lasso_lars.fit_path(X, y, precompute=True) \n    assert lasso_lars_path.shape[1] > 1\n\n\n\n"], "sample_484": ["    def test_negative_length(self):\n        with self.assertRaisesMessage(ValueError, \"'length' must be greater than 0\"):\n            Author.objects.annotate(raises=Right(\"name\", -5))\n", "    def test_negative_index(self):\n        with self.assertRaisesMessage(ValueError, \"'pos' must be greater than 0\"):\n            Author.objects.annotate(\n                name_part=Right(\"name\", -10)\n            )\n\n\n", "    def test_negative_length(self):\n        with self.assertRaisesMessage(ValueError, \"'length' must be greater than 0\"):\n            Author.objects.annotate(raises=Right(\"name\", -5))\n", "    def test_negative_length(self):\n        with self.assertRaisesMessage(ValueError, \"'length' must be greater than 0\"):\n            Author.objects.annotate(raises=Right(\"name\", -1))\n", "    def test_null_handling(self):\n        Author.objects.create(name=None, alias=\"tom\")\n        authors = Author.objects.annotate(name_part=Right(\"name\", 5))\n        self.assertQuerySetEqual(\n            authors.order_by(\"id\"),\n            [\n                None,\n                \"smith\",\n            ],\n            lambda a: a.name_part,\n        )\n"], "sample_187": ["    def test_format_lazy(self):\n        text = lazystr('This is {var} a test.')\n        self.assertEqual(text.format(var='lazy'), 'This is lazy a test.')\n        self.assertEqual(format_lazy(text, var='lazy'), 'This is lazy a test.')\n        \n        with self.subTest(var=None):\n            with self.assertRaises(TypeError):\n                text.format(var=None)\n        with self.subTest(var=1):\n            self.assertEqual(text.format(var=1), 'This is 1 a test.')\n        with self.subTest(var=1):\n            self.assertEqual(format_lazy(text, var=1), 'This is 1 a test.') \n", "    def test_camel_case_to_spaces(self):\n        items = [\n            ('fooBarBaz', 'foo bar baz'),\n            ('fooBar2baz', 'foo bar2 baz'),\n            ('camelCaseString', 'camel case string'),\n            ('dontSplitNumbers', 'dont split numbers'),\n            ('this_is_a_test', 'this is a test'),\n            ('AllCaps', 'All Caps'),\n            ('', ''),\n            (None, None),\n        ]\n        for value, output in items:\n            self.assertEqual(text.camel_case_to_spaces(value), output)\n            self.assertEqual(text.camel_case_to_spaces(lazystr(value)), output)\n\n\n", "    def test_format_lazy(self):\n        self.assertEqual(format_lazy('the {0} is {1}', 'quick', 'brown fox'), 'the quick is brown fox')\n        self.assertEqual(format_lazy('the {0} is {1}', 'quick', lazystr('brown fox')), 'the quick is brown fox')\n        self.assertEqual(format_lazy('the {quick} is {brown fox}', 'quick', 'brown fox'), 'the quick is brown fox')\n        self.assertEqual(format_lazy('the {quick} is {brown fox}', lazystr('quick'), 'brown fox'), 'the quick is brown fox')\n", "    def test_smart_split_with_unicode(self):\n        testdata = [\n            ('This is \"a person\u2019s\" test.',\n                ['This', 'is', '\"a person\\u2019s\"', 'test.']),\n            (\"It's a beautiful \u00f6\u00e4\u00fc world.\",\n                ['It', \"s\", 'a', 'beautiful', '\u00f6\u00e4\u00fc', 'world.']),\n            (\"This is 'a string \u00f6\u00e4\u00fc'.\",\n                ['This', 'is', \"'a string \u00f6\u00e4\u00fc'\"]),\n        ]\n        for test, expected in testdata:\n            self.assertEqual(list(text.smart_split(test)), expected)\n\n", "    def test_smart_split_with_unicode(self):\n        testdata = [\n            ('It\\'s a \"difficult\" time.',\n                ['It', \"'s\", '\"difficult\"', 'time.']),\n            (\"It's a \\\"difficult\\\" time.\",\n                ['It', \"'s\", '\"difficult\"', 'time.']),\n            ('url search_page words=hello \\u0438 \\u0442\\u043e',\n                ['url', 'search_page', 'words=hello', ' \\u0438', ' \\u0442\\u043e']),\n        ]\n        for test, expected in testdata:\n            self.assertEqual(list(text.smart_split(test)), expected)\n"], "sample_367": ["    def test_cache_control_decorator_headers(self):\n        @cache_control(no_cache=True, max_age=3600, public=True)\n            return HttpResponse()\n\n        response = a_view(HttpRequest())\n        self.assertEqual(response.headers['Cache-Control'], 'no-cache, max-age=3600, public')\n", "    def test_cache_control_decorator_public(self):\n        @cache_control(public=True)\n            return HttpResponse()\n\n        response = a_view(HttpRequest())\n        self.assertEqual(response.headers['Cache-Control'], 'public')\n", "    def test_cache_control_decorator_http_request_proxy(self):\n        class MyClass:\n            @method_decorator(cache_control(a='b'))\n                return HttpResponse()\n\n        request = HttpRequest()\n        response = MyClass().a_view(HttpRequestProxy(request))\n        self.assertIn('Cache-Control', response.headers)\n        self.assertEqual(response.headers['Cache-Control'], 'a=\"b\"')\n\n\n", "    def test_cache_control_decorator(self):\n        @cache_control(private=True, max_age=3600)\n            return HttpResponse()\n\n        response = a_view(HttpRequest())\n        self.assertEqual(response.headers['Cache-Control'], 'private, max-age=3600')\n", "    def test_cache_control_decorator_kwargs(self):\n        @cache_control(must_revalidate=True, max_age=3600, proxy_revalidate=False)\n            return HttpResponse()\n        response = a_view(HttpRequest())\n        self.assertEqual(response.headers['Cache-Control'], 'must-revalidate, max-age=3600')\n\n\n\n"], "sample_539": ["def test_polygon_selector_cancel(ax, draw_bounding_box):\n    event_sequence = [\n        *polygon_place_vertex(50, 50),\n        *polygon_place_vertex(150, 50),\n        ('on_key_press', dict(key='escape')),\n        ('on_key_release', dict(key='escape')),\n    ]\n    tool = widgets.PolygonSelector(ax, onselect=noop,\n                                   draw_bounding_box=draw_bounding_box)\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n    assert tool.verts == []\n", "def test_polygon_selector_extents(ax, draw_bounding_box):\n    # Test setting and getting extents\n    verts = [(50, 50), (150, 50), (50, 150)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    tool = widgets.PolygonSelector(ax, onselect=noop,\n                                   draw_bounding_box=draw_bounding_box)\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    # Get and assert extents\n    initial_extents = tool._box.extents\n    assert initial_extents[0] == min([v[0] for v in verts])\n    assert initial_extents[1] == max([v[0] for v in verts])\n    assert initial_extents[2] == min([v[1] for v in verts])\n    assert initial_extents[3] == max([v[1] for v in verts])\n\n    # Set extents manually\n    tool._box.extents = (10, 10, 100, 100)\n    assert tool._box.extents == (10, 10, 100, 100)\n    # Check that the polygon vertices are updated accordingly\n    for i, v in enumerate(tool.verts):\n        assert v[0] in range(10, 100)\n        assert v[1] in range(10, 100)\n\n\n\n", "def test_polygon_selector_empty_polygon(draw_bounding_box):\n    verts = []\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n    check_polygon_selector(event_sequence, verts, 0,\n                           draw_bounding_box=draw_bounding_box)\n\n\n", "def test_polygon_selector_empty(ax, draw_bounding_box):\n    tool = widgets.PolygonSelector(ax, onselect=noop,\n                                   draw_bounding_box=draw_bounding_box)\n    assert tool.verts == []\n    assert tool._selection_artist is None\n\n    # Add a point, remove it, and verify the states\n    event_sequence = [\n        *polygon_place_vertex(50, 50),\n        *polygon_remove_vertex(50, 50),\n    ]\n\n\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    assert tool.verts == []\n    assert tool._selection_artist is None\n\n\n\n", "def test_polygon_selector_reset(ax, draw_bounding_box):\n    verts = [(50, 50), (150, 50), (50, 150)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n        ('on_key_press', dict(key='escape')),\n    ]\n\n    tool = widgets.PolygonSelector(ax, onselect=noop,\n                                   draw_bounding_box=draw_bounding_box)\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n    assert tool.verts == []\n\n\n"], "sample_81": ["    def test_match_kwargs(self):\n        pattern = RoutePattern(_('translated/<int:pk>'))\n        match = pattern.match('/translated/123')\n        self.assertIsNotNone(match)\n        self.assertEqual(match.groups(), ('123',))\n        self.assertEqual(match.kwargs, {'pk': '123'}) \n", "    def test_no_named_groups(self):\n        pattern = RoutePattern(_('translated/<int:pk>'))\n        match = pattern.match('/translated/123')\n        self.assertTrue(match)\n        self.assertEqual(match.groupdict(), {'pk': '123'})\n\n", "    def test_match_non_named_groups(self):\n        pattern = RoutePattern('translated/<int:pk>')\n        match = pattern.match('/translated/123')\n        self.assertIsNotNone(match)\n        self.assertEqual(match.args, ())\n        self.assertEqual(match.kwargs, {'pk': '123'})\n", "    def test_match(self):\n        pattern = RoutePattern(_('translated/<int:pk>'))\n        match = pattern.match('/translated/123')\n        self.assertIsNotNone(match)\n        self.assertEqual(match.args, ())\n        self.assertEqual(match.kwargs, {'pk': '123'})\n", "    def test_match_with_converter(self):\n        pattern = RoutePattern(_('translated/<int:pk>'))\n        match = pattern.match('/translated/123')\n        self.assertIsNotNone(match)\n        self.assertEqual(match.kwargs['pk'], 123)\n\n        match = pattern.match('/translated/abc')\n        self.assertIsNone(match)\n"], "sample_281": ["    def test_invalid_remote_model(self):\n        with self.subTest(invalid_remote_model=True):\n            invalid_model = 'ThisDoesNotExist'\n            request = self.factory.get(self.url, {'term': 'is', **self.opts, 'field_name': 'question_with_to_field'})\n            request.user = self.superuser\n            with self.assertRaises(PermissionDenied):\n                AutocompleteJsonView.as_view(**self.as_view_args)(request)\n", "    def test_search_field_not_on_related_model(self):\n        class EmptySearchAdminRelated(QuestionAdmin):\n            search_fields = []\n\n        with model_admin(Question, EmptySearchAdminRelated):\n            msg = \"EmptySearchAdminRelated must have search_fields for the autocomplete_view.\"\n            with self.assertRaisesMessage(Http404, msg):\n                site.autocomplete_view(self.factory.get(self.url, {'term': '', **self.opts}))\n", "    def test_ordering_with_complex_filter(self):\n        with model_admin(Question, QuestionAdmin, admin_site=site):\n            q1 = Question.objects.create(question='question 1', related_questions=Question.objects.create(question='related question 1'))\n            q2 = Question.objects.create(question='question 2', related_questions=Question.objects.create(question='related question 2'))\n            q3 = Question.objects.create(question='question 3', related_questions=Question.objects.create(question='related question 3'))\n            q4 = Question.objects.create(question='question 4')\n        request = self.factory.get(self.url, {'term': 'question', **self.opts})\n        request.user = self.superuser\n\n        class ComplexOrderingQuestionAdmin(QuestionAdmin):\n            ordering = ['related_questions__question', 'question']\n\n        with model_admin(Question, ComplexOrderingQuestionAdmin):\n            response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n        self.assertEqual(response.status_code, 200)\n        data = json.loads(response.content.decode('utf-8'))\n        self.assertEqual(sorted(\n            [q.question for q in data['results']],\n            ['related question 1', 'related question 2', 'related question 3', 'question 1', 'question 2', 'question 3', 'question 4']\n        ), \n        [q.question for q in data['results']])\n\n\n\n", "    def test_search_results_with_multiple_fields(self):\n        class QuestionAdminWithMultipleFields(QuestionAdmin):\n            search_fields = ['question', 'related_questions__question']\n\n        with model_admin(Question, QuestionAdminWithMultipleFields):\n            q1 = Question.objects.create(question='Question with related', related_questions=None)\n            q2 = Question.objects.create(question='Question 1')\n            q3 = Question.objects.create(question='Question with related', related_questions=q2)\n            q4 = Question.objects.create(question='Question 2')\n            request = self.factory.get(self.url, {'term': 'question', **self.opts})\n            request.user = self.superuser\n            response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n            self.assertEqual(response.status_code, 200)\n            data = json.loads(response.content.decode('utf-8'))\n            self.assertEqual(len(data['results']), 4)\n\n\n", "    def test_custom_permissions_for_related_model(self):\n        class RestrictedAdmin(AuthorshipInline.base_admin_class):\n                return False\n\n        class AllowRestrictedAuthorshipAdmin(BookAdmin):\n            inlines = [RestrictedAdmin]\n\n        with model_admin(Authorship, RestrictedAdmin, admin_site=site):\n            with self.subTest(permission='add'):\n                self.admin_login(username='super', password='secret', login_url=reverse('autocomplete_admin:index'))\n                self.selenium.get(self.live_server_url + reverse('autocomplete_admin:admin_views_book_add'))\n                elem = self.selenium.find_element_by_css_selector('.add-authorship')\n                elem.click()\n                row = self.selenium.find_element_by_css_selector('.dynamic-authorship_set .form-group')\n                self.assertTrue(row.find_element_by_tag_name('button').text == 'Add another Authorship')\n\n            with self.subTest(permission='change'):\n                msg = 'Permission denied.'\n                with self.assertRaisesMessage(AssertionError, msg):\n                    self.admin_login(username='user', password='secret', login_url=reverse('autocomplete_admin:index'))\n                    self.selenium.get(self.live_server_url + reverse('autocomplete_admin:admin_views_book_add'))\n                    elem = self.selenium.find_element_by_css_selector('.add-authorship')\n                    elem.click()\n                    row = self.selenium.find_element_by_css_selector('.dynamic-authorship_set .form-group')\n\n"], "sample_234": ["    def test_union_with_different_fields(self):\n        Number.objects.bulk_create(\n            Number(num=i, other_num=10 - i) for i in range(10)\n        )\n        qs1 = Number.objects.values('num')\n        qs2 = Number.objects.values('other_num')\n        with self.assertRaises(DatabaseError):\n            list(qs1.union(qs2))\n\n\n\n", "    def test_union_subquery(self):\n        qs1 = Number.objects.filter(num__lte=1)\n        qs2 = Number.objects.filter(num__gte=2, num__lte=3).values('num')\n        self.assertNumbersEqual(qs1.union(qs2.subquery()).order_by('num'), [0, 1, 2, 3])\n\n", "    def test_ordering_with_aggregated_fields(self):\n        qs1 = Number.objects.values('num').annotate(count=Count('id'))\n        qs2 = Number.objects.values('other_num').annotate(count=Count('id'))\n        self.assertNumbersEqual(\n            qs1.union(qs2).order_by('count').order_by('num'),\n            [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1)],\n        )\n", "    def test_union_with_multiple_related_objects(self):\n        class Author(models.Model):\n            name = models.CharField(max_length=100)\n\n        class Book(models.Model):\n            title = models.CharField(max_length=100)\n            author = models.ForeignKey(Author, on_delete=models.CASCADE)\n\n        Author.objects.create(name='Author A')\n        Author.objects.create(name='Author B')\n\n        book1 = Book.objects.create(title='Book 1', author=Author.objects.get(name='Author A'))\n        book2 = Book.objects.create(title='Book 2', author=Author.objects.get(name='Author A'))\n\n        book3 = Book.objects.create(title='Book 3', author=Author.objects.get(name='Author B'))\n\n        related_queryset = (\n            Book.objects.filter(author=book1.author.id).values('title')\n            .union(\n                Book.objects.filter(author=book3.author.id).values('title')\n            )\n        )\n\n        self.assertCountEqual(list(related_queryset), ['Book 1', 'Book 2', 'Book 3'])\n", "    def test_distinct_after_union(self):\n        qs1 = Number.objects.filter(num=1)\n        qs2 = Number.objects.filter(num=1)\n        self.assertEqual(list(qs1.union(qs2).distinct()), [1])\n"], "sample_190": ["    def test_complex_date_time_lookups(self):\n        now = datetime.now()\n        for i in range(10):\n            Article.objects.create(\n                pub_date=now - timedelta(days=i),\n                headline=f'Article {i}',\n            )\n        self.assertQuerysetEqual(\n            Article.objects.filter(\n                pub_date__day__range=(5, 9),\n                pub_date__month__gt=datetime.now().month - 1\n            ),\n            [\n                '<Article: Article 4>',\n                '<Article: Article 5>',\n                '<Article: Article 6>',\n                '<Article: Article 7>',\n                '<Article: Article 8>',\n                '<Article: Article 9>',\n            ]\n        )\n        self.assertQuerysetEqual(\n            Article.objects.filter(pub_date__week=now.isocalendar().week),\n            [\n                '<Article: Article 0>',\n                '<Article: Article 1>',\n                '<Article: Article 2>',\n                '<Article: Article 3>',\n                '<Article: Article 4>',\n                '<Article: Article 5>',\n                '<Article: Article 6>',\n                '<Article: Article 7>',\n                '<Article: Article 8>',\n                '<Article: Article 9>',\n            ]\n        )\n        self.assertQuerysetEqual(\n            Article.objects.filter(\n                pub_date__year=now.year,\n                pub_date__month=now.month - 1\n            ),\n            [\n                '<Article: Article 2>',\n                '<Article: Article 3>',\n                '<Article: Article 4>',\n                '<Article: Article 5>',\n                '<Article: Article 6>',\n                '<Article: Article 7>',\n                '<Article: Article 8>',\n                '<Article: Article 9>',\n            ]\n        )\n\n\n\n", "    def test_related_object_update_using_filter_with_related_instance(self):\n        season = Season.objects.create(year=2016, name='2016 Season')\n        game1 = Game.objects.create(\n            home=\"Chicago Cubs\", away=\"New York Mets\", season=season,\n        )\n        game2 = Game.objects.create(\n            home=\"Los Angeles Dodgers\", away=\"Arizona Diamondbacks\", season=season,\n        )\n        game1.home_runs = 3\n        game1.save()\n        game_data = {\n            'home_runs': 5,\n            'season__name': '2016 Season',\n            'away': 'New York Mets',\n        }\n        game = Game.objects.filter(\n            *get_filter_parts(game_data)\n        ).first()\n        game.home_runs = 6\n        game.save()\n\n        self.assertEqual(game.home_runs, 6)\n        self.assertEqual(Game.objects.filter(home=\"Chicago Cubs\", away=\"New York Mets\").first().home_runs, 6)\n\n", "    def test_exact_query_rhs_with_selected_columns_multiple(self):\n        newest_author1 = Author.objects.create(name='Author 1')\n        newest_author2 = Author.objects.create(name='Author 2')\n        authors_max_ids = Author.objects.filter(\n            name__in=['Author 1', 'Author 2'],\n        ).values(\n            'name',\n        ).annotate(\n            max_id=Max('id'),\n        ).values('max_id')\n        authors = Author.objects.filter(id__in=authors_max_ids)\n        self.assertCountEqual(\n            authors, [newest_author1, newest_author2],\n        )\n\n", "    def test_subquery_in_exclude(self):\n        Tag.objects.create(name='tag1')\n        Tag.objects.create(name='tag2')\n\n        article1 = Article.objects.create(headline='Article 1', tags=['tag1'])\n        article2 = Article.objects.create(headline='Article 2', tags=['tag1', 'tag2'])\n        article3 = Article.objects.create(headline='Article 3', tags=['tag2'])\n        article4 = Article.objects.create(headline='Article 4')\n\n        self.assertCountEqual(Article.objects.exclude(tags__name__in=Tag.objects.filter(name='tag1')).all(), [article4])\n", "    def test_negated_lookups_with_slice(self):\n        Article.objects.bulk_create([\n            Article(headline='a', pub_date=datetime(2005, 11, 20)),\n            Article(headline='b', pub_date=datetime(2005, 11, 21)),\n            Article(headline='c', pub_date=datetime(2005, 11, 22)),\n        ])\n        self.assertQuerysetEqual(\n            Article.objects.filter(headline__icontains='b').exclude(headline__startswith='a'),\n            ['<Article: b>']\n        )\n        self.assertQuerysetEqual(\n            Article.objects.filter(pub_date__range=(datetime(2005, 11, 21), datetime(2005, 11, 23))).exclude(\n                headline__startswith='a'),\n            ['<Article: b>', '<Article: c>']\n        )\n\n"], "sample_852": ["def test_make_classification_n_redundant():\n    n_redundant = [0, 1, 2, 3, 4]\n    for n_redundant in n_redundant:\n        X, y = make_classification(n_redundant=n_redundant, n_samples=100,\n                                   n_features=10, n_informative=5, random_state=0)\n        assert X.shape == (100, 10), \"X shape mismatch\"\n        assert y.shape == (100,), \"y shape mismatch\"\n        assert sum(np.unique(y)) == 3, \"Expected 3 classes\"\n\n", "compilation error", "def test_make_classification_categorical_features():\n    X, y = make_classification(n_samples=100, n_features=10, n_informative=3,\n                              n_redundant=1,\n                              n_clusters_per_class=1, random_state=0,\n                              cat_features=[0, 2], )\n    assert X.shape == (100, 10), \"X shape mismatch\"\n    assert y.shape == (100,), \"y shape mismatch\"\n    assert X[:, 0].dtype == 'object'\n    assert X[:, 2].dtype == 'object' \n\n\n", "def test_make_classification_return_sparse_invalid_types():\n    with pytest.raises(ValueError, match=\"The 'return_sparse' parameter\"):\n        make_classification(return_sparse='abc')\n    with pytest.raises(ValueError, match=\"The 'return_sparse' parameter\"):\n        make_classification(return_sparse=123)\n\n\n\n", "    def test_make_moons_noise():\n        X, y = make_moons(n_samples=100, noise=0.1, shuffle=False)\n        for x, label in zip(X, y):\n            center = [0.0, 0.0] if label == 0 else [1.0, 0.5]\n            dist_sqr = ((x - center) ** 2).sum()\n            assert_almost_equal(dist_sqr, 1.0,\n                                err_msg=\"Point is not on expected unit circle\")\n            assert_almost_equal(dist_sqr, 1.0 + 0.1*np.random.randn(),\n                                err_msg=\"Point should have variance due to noise\")\n"], "sample_122": ["    def test_non_existing_alias(self):\n        \"\"\"\n        Requesting a non-existent alias should raise an exception.\n        \"\"\"\n        with self.assertRaises(CacheKeyError):\n            caches['nonexistent']\n", "    def test_different_caches(self):\n        \"\"\"\n        Requesting different aliases should yield different instances.\n        \"\"\"\n        cache1 = caches['default']\n        cache2 = caches['other']\n\n        self.assertIsNot(cache1, cache2)\n\n\n", "    def test_vary_headers_from_response(self):\n        request = self.factory.get(self.path)\n        template = engines['django'].from_string(\"This is a test\")\n        response = TemplateResponse(HttpRequest(), template)\n        response['Vary'] = 'Accept-Encoding, Cookie'\n        learn_cache_key(request, response)\n        key = get_cache_key(request)\n        self.assertEqual(key, 'views.decorators.cache.cache_page.settingsprefix.GET.64a85617b80f0057b3ea6b5796678b2c.d41d8cd98f00b204e9800998ecf8427e')\n", "    def test_caching_error_response(self):\n        class MyView(View):\n                raise PermissionDenied\n        \n        view = MyView.as_view()\n        request = self.factory.get(self.path)\n        response = cache_page(3)(view)(request)\n        self.assertEqual(response.status_code, 403)  \n        self.assertEqual(response['Cache-Control'], 'no-cache, no-store, must-revalidate, private')\n        self.assertEqual(response['Content-Type'], 'text/html; charset=utf-8')\n        self.assertIn(\"Permission denied\", response.content.decode())\n", "    def test_middleware_with_cache_key_prefix(self):\n        request = self.factory.get('/view/')\n        response = HttpResponse(\"This is a response.\")\n\n        # Define a middleware function that modifies the cache key prefix\n                request._cache_update_cache = True  \n                request.META['HTTP_X_CACHE_PREFIX'] = 'prefix123'\n                response = get_response(request, *args, **kwargs)\n                return response\n            return middleware\n\n        # Apply the middleware to the view\n        view = cache_page(1)(hello_world_view)\n        middleware = modify_prefix_middleware(view)\n        response = middleware(request)\n\n        # Assert that the cache key includes the modified prefix\n        key = get_cache_key(request)\n        self.assertIn('prefix123', key)\n\n\n\n"], "sample_1053": ["compilation error", "def test_issue_12188():\n    assert S.pi.is_finite is False\n    assert S.Exp1.is_finite is False\n    assert S.sin(S(1)).is_finite is True\n    assert S.cos(S(1)).is_finite is True\n    assert S.tan(S(1)).is_finite is True\n    assert S.log(S(2)).is_finite is True\n    assert S(2).is_finite is True\n\n\n\n", "def test_issue_10586():\n    a = Float('1e-100', precision=100)\n    b = Float('1e-99', precision=100)\n    assert a != b\n    assert same_and_same_prec(a + b, Float('1e-99'))\n    assert same_and_same_prec(a - b, Float('1e-100'))\n    assert same_and_same_prec(a * b, Float('1e-199'))\n    assert same_and_same_prec(a / b, Float('1e-1'))\n\n\n", "def test_complex_issue_10184():\n    assert (3 + 4j)**-1 == complex(-3/25, 4/25)\n    assert (3 - 4j)**-1 == complex(3/25, -4/25)\n\n\n\n", "def test_issue_8608():\n    assert (sqrt(2) * Complex(1, 1)).evalf(5) == Complex(2.41421, 1.41421) \n"], "sample_716": ["def test_sparse_matrix_input_to_fit():\n    rng = np.random.RandomState(0)\n    n_samples, n_features = 100, 5\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples)\n\n    for sparse_type in (sp.csr_matrix, sp.csc_matrix, sp.coo_matrix):\n        X_sparse = sparse_type(X)\n        ridge = Ridge()\n        ridge.fit(X_sparse, y)\n        assert_array_almost_equal(ridge.coef_, ridge.predict(X_sparse))\n\n", "def test_ridge_sparse_svd_with_sample_weight():\n    n_samples = 100\n    n_features = 50\n    rng = np.random.RandomState(42)\n\n    X = sp.csc_matrix((rng.rand(n_samples, n_features),\n                    (rng.randint(n_features, size=n_samples),\n                     rng.randint(n_samples, size=n_samples))))\n    y = rng.randn(n_samples)\n    sample_weight = rng.rand(n_samples)\n\n    ridge = Ridge(solver='svd', fit_intercept=False)\n    ridge.fit(X, y, sample_weight=sample_weight)\n    assert_equal(ridge.coef_.shape, (n_features,))\n\n\n\n", "def test_ridge_sparse_solver_switch():\n    for solver in ['svd', 'cholesky']:\n        sparse_ridge = Ridge(alpha=1., tol=1e-15, solver=solver, fit_intercept=True)\n        dense_ridge = Ridge(alpha=1., tol=1e-15, solver=solver, fit_intercept=True)\n        X, y = make_regression(n_samples=1000, n_features=2, n_informative=2,\n                               bias=10., random_state=42)\n        X_sparse = sp.csr_matrix(X)\n        sparse_ridge.fit(X_sparse, y)\n        dense_ridge.fit(X, y)\n        assert_almost_equal(sparse_ridge.intercept_, dense_ridge.intercept_)\n        assert_array_almost_equal(sparse_ridge.coef_, dense_ridge.coef_)\n\n    for solver in ['lsqr', 'sag', 'saga']:\n        sparse_ridge = Ridge(alpha=1., tol=1e-15, solver=solver, fit_intercept=True)\n        dense_ridge = Ridge(alpha=1., tol=1e-15, solver=solver, fit_intercept=True)\n        X, y = make_regression(n_samples=1000, n_features=2, n_informative=2,\n                               bias=10., random_state=42)\n        X_sparse = sp.csr_matrix(X)\n\n        sparse_ridge.fit(X_sparse, y)\n        dense_ridge.fit(X, y)\n        assert_almost_equal(sparse_ridge.intercept_, dense_ridge.intercept_)\n        assert_array_almost_equal(sparse_ridge.coef_, dense_ridge.coef_)\n\n\n\n\n", "def test_ridge_sparse_fit_intercept():\n    X = sp.csr_matrix(X_diabetes)\n    y = y_diabetes\n\n    # Test with fit_intercept=True\n    ridge = Ridge(fit_intercept=True, solver='sag')\n    ridge.fit(X, y)\n    assert_almost_equal(ridge.intercept_, ridge.intercept_)\n\n    # Test with fit_intercept=False\n    ridge = Ridge(fit_intercept=False, solver='sag')\n    ridge.fit(X, y)\n    assert_equal(ridge.intercept_, 0.0)\n", "def test_sparse_matrix_creation():\n    rng = check_random_state(42)\n    n_samples, n_features = 10, 5\n    X = rng.randn(n_samples, n_features)\n\n    ridge = Ridge(fit_intercept=False)\n    ridge.fit(X, y_diabetes)\n    assert_array_equal(ridge.coef_.shape, (n_features,))\n\n    X_sparse = sp.csr_matrix(X)\n    ridge_sparse = Ridge(fit_intercept=False)\n    ridge_sparse.fit(X_sparse, y_diabetes)\n    assert_array_equal(ridge_sparse.coef_.shape, (n_features,))\n\n    assert_array_almost_equal(ridge.coef_, ridge_sparse.coef_)\n\n\n\n"], "sample_1173": ["compilation error", "compilation error", "def test_complex_mixed_numbers():\n    transformations = standard_transformations + (rationalize,)\n    assert parse_expr(\"1 + 2.5j * 3\") == 3 + 7.5*I", "def test_nested_quantification():\n    assert parse_expr('Q.even(x) & Q.odd(y)') == And(Q.even(x), Q.odd(y))\n", "compilation error"], "sample_128": ["    def test_covering_index_with_condition_and_multiple_includes(self):\n        index = Index(\n            name='covering_partial_headline_idx',\n            fields=['headline'],\n            include=['pub_date', 'published', 'slug'],\n            condition=Q(pub_date__gt=datetime.datetime(year=2021, month=1, day=1, tzinfo=timezone.get_current_timezone())),\n        )\n        with connection.schema_editor() as editor:\n            self.assertIn(\n                '(%s) INCLUDE (%s, %s, %s) WHERE %s' % (\n                    editor.quote_name('headline'),\n                    editor.quote_name('pub_date'),\n                    editor.quote_name('published'),\n                    editor.quote_name('slug'),\n                    editor.quote_name('pub_date'),\n                ),\n                str(index.create_sql(Article, editor)),\n            )\n            editor.add_index(Article, index)\n            with connection.cursor() as cursor:\n                constraints = connection.introspection.get_constraints(\n                    cursor=cursor, table_name=Article._meta.db_table,\n                )\n                self.assertIn(index.name, constraints)\n                self.assertEqual(\n                    constraints[index.name]['columns'],\n                    ['headline', 'pub_date', 'published', 'slug'],\n                )\n            editor.remove_index(Article, index)\n            with connection.cursor() as cursor:\n                self.assertNotIn(index.name, connection.introspection.get_constraints(\n                    cursor=cursor, table_name=Article._meta.db_table,\n                ))\n", "    def test_index_condition_for_fields_in_include(self):\n        with connection.schema_editor() as editor:\n            index = Index(\n                name='covering_headline_idx',\n                fields=['headline', 'pub_date'],\n                include=['pub_date', 'published'],\n                condition=Q(headline__startswith='A'),\n            )\n            self.assertIn(\n                '(%s, %s) INCLUDE (%s, %s) WHERE %s' % (\n                    editor.quote_name('headline'),\n                    editor.quote_name('pub_date'),\n                    editor.quote_name('pub_date'),\n                    editor.quote_name('published'),\n                    editor.quote_name('headline'),\n                ),\n                str(index.create_sql(Article, editor)),\n            )\n            editor.add_index(Article, index)\n            with connection.cursor() as cursor:\n                constraints = connection.introspection.get_constraints(\n                    cursor=cursor, table_name=Article._meta.db_table,\n                )\n                self.assertIn(index.name, constraints)\n                self.assertEqual(\n                    constraints[index.name]['columns'],\n                    ['headline', 'pub_date', 'published'],\n                )\n            editor.remove_index(Article, index)\n            with connection.cursor() as cursor:\n                self.assertNotIn(index.name, connection.introspection.get_constraints(\n                    cursor=cursor, table_name=Article._meta.db_table,\n                ))\n\n\n\n\n", "    def test_covering_index_with_ordering(self):\n        index = Index(\n            name='covering_headline_idx_ordered',\n            fields=['headline'],\n            include=['pub_date', 'published'],\n            order=['headline', 'pub_date'],\n        )\n        with connection.schema_editor() as editor:\n            self.assertIn(\n                '(%s) INCLUDE (%s, %s) ORDER BY (%s, %s)' % (\n                    editor.quote_name('headline'),\n                    editor.quote_name('pub_date'),\n                    editor.quote_name('published'),\n                    editor.quote_name('headline'),\n                    editor.quote_name('pub_date'),\n                ),\n                str(index.create_sql(Article, editor)),\n            )\n            editor.add_index(Article, index)\n            with connection.cursor() as cursor:\n                constraints = connection.introspection.get_constraints(\n                    cursor=cursor, table_name=Article._meta.db_table,\n                )\n                self.assertIn(index.name, constraints)\n                self.assertEqual(\n                    constraints[index.name]['columns'],\n                    ['headline', 'pub_date', 'published'],\n                )\n            editor.remove_index(Article, index)\n            with connection.cursor() as cursor:\n                self.assertNotIn(index.name, connection.introspection.get_constraints(\n                    cursor=cursor, table_name=Article._meta.db_table,\n                )) \n", "    def test_covering_index_with_opsclass(self):\n        index = Index(\n            name='covering_headline_idx_with_opsclass',\n            fields=['headline'],\n            include=['pub_date', 'published'],\n            opclasses=['varchar_pattern_ops'],\n        )\n        with connection.schema_editor() as editor:\n            self.assertIn(\n                '(%s varchar_pattern_ops) INCLUDE (%s, %s)' % (\n                    editor.quote_name('headline'),\n                    editor.quote_name('pub_date'),\n                    editor.quote_name('published'),\n                ),\n                str(index.create_sql(Article, editor)),\n            )\n            editor.add_index(Article, index)\n            with connection.cursor() as cursor:\n                constraints = connection.introspection.get_constraints(\n                    cursor=cursor, table_name=Article._meta.db_table,\n                )\n                self.assertIn(index.name, constraints)\n                self.assertEqual(\n                    constraints[index.name]['columns'],\n                    ['headline', 'pub_date', 'published'],\n                )\n            editor.remove_index(Article, index)\n            with connection.cursor() as cursor:\n                self.assertNotIn(index.name, connection.introspection.get_constraints(\n                    cursor=cursor, table_name=Article._meta.db_table,\n                ))\n\n\n\n\n", "    def test_condition_not_included_in_covering_index(self):\n        index = Index(\n            name='covering_headline_idx',\n            fields=['headline'],\n            include=['pub_date', 'published'],\n            condition=Q(pub_date__gt=datetime.date(2023, 1, 1)),\n        )\n        with connection.schema_editor() as editor:\n            self.assertNotIn(\n                'WHERE %s' % editor.quote_name('pub_date'),\n                str(index.create_sql(Article, editor)),\n            )\n            editor.add_index(Article, index)\n            with connection.cursor() as cursor:\n                constraints = connection.introspection.get_constraints(\n                    cursor=cursor, table_name=Article._meta.db_table,\n                )\n                self.assertIn(index.name, constraints)\n                self.assertEqual(\n                    constraints[index.name]['columns'],\n                    ['headline', 'pub_date', 'published'],\n                )\n            editor.remove_index(Article, index)\n            with connection.cursor() as cursor:\n                self.assertNotIn(index.name, connection.introspection.get_constraints(\n                    cursor=cursor, table_name=Article._meta.db_table,\n                ))\n"], "sample_4": ["    def test_readwrite_html_table_multiple_cosmologies(self, cosmo_cls, cosmo1, cosmo2, read, write, tmp_path, add_cu):\n        \"\"\"Test reading and writing multiple cosmologies in the same file.\"\"\"\n        fp = tmp_path / \"test_readwrite_html_multiple_cosmologies.html\"\n\n        # concatenate tables with metadata\n        table = vstack([cosmo1.to_format(\"astropy.table\"), cosmo2.to_format(\"astropy.table\")],\n                       metadata_conflicts='silent')\n        \n        # write tables with different cosmo names\n        write(fp, format=\"ascii.html\")\n        \n        # read and check\n        read_cosmo1 = cosmo_cls.read(fp, index=0, format=\"ascii.html\")\n        read_cosmo2 = cosmo_cls.read(fp, index=1, format=\"ascii.html\")\n\n        assert read_cosmo1 == cosmo1\n        assert read_cosmo2 == cosmo2\n", "    def test_readwrite_html_table_metadata(self, cosmo, read, write, tmp_path):\n        \"\"\"Test metadata in HTML table.\"\"\"\n        fp = tmp_path / \"test_readwrite_html_table_metadata.html\"\n\n        write(fp, format=\"ascii.html\", **cosmo.__dict__)\n\n        # read with the same class that wrote fills in the missing info with\n        # the default value\n        got = read(fp, format=\"ascii.html\")\n        assert got.meta == cosmo.meta\n\n\n\n", "    def test_readwrite_html_table_metadata(self, cosmo, read, write, tmp_path, add_cu):\n        \"\"\"Test metadata handling in html format.\"\"\"\n        fp = tmp_path / \"test_readwrite_html_table_metadata.html\"\n        cosmo_with_metadata = cosmo.clone(meta={'key': 10})\n\n        write(fp, format=\"ascii.html\", cosmology=cosmo_with_metadata)\n        read_cosmo = read(fp, format=\"ascii.html\")\n\n        assert read_cosmo.meta == cosmo_with_metadata.meta\n", "    def test_readwrite_html_table_metadata(self, cosmo_cls, cosmo, read, write, tmp_path, add_cu):\n        \"\"\"Test metadata handling during read/write.\"\"\"\n        fp = tmp_path / \"test_readwrite_html_table_metadata.html\"\n\n        # Define some metadata to store\n        metadata = {\"key\": 10, \"another_key\": \"value\"}\n        cosmo_with_metadata = cosmo.clone(meta=metadata)\n\n        write(fp, format=\"ascii.html\", cosmology=cosmo_with_metadata)\n\n        # read back the cosmology and assert metadata is preserved\n        got = read(fp, format=\"ascii.html\")\n        assert got.meta == metadata\n\n        # Test for conflict resolution when new metadata is provided\n        new_metadata = {\"key\": 42}\n        got2 = read(fp, format=\"ascii.html\", move_to_meta=new_metadata)\n        assert got2.meta == {\"key\": 42, \"another_key\": \"value\"}\n", "    def test_readwrite_html_table_index_error(self, cosmo, read, write, tmp_path, add_cu):\n        \"\"\"Test if index error is raised when there is no index.\"\"\"\n        fp = tmp_path / \"test_readwrite_html_table_index_error.html\"\n\n        # Write a table with no index column\n        write(fp, format=\"ascii.html\", index=None)\n\n        # Attempt to read the table using an index\n        with pytest.raises(ValueError, match=\"Cosmology table does not have an index\"):\n            read(fp, index=0, format=\"ascii.html\")\n\n\n\n"], "sample_782": ["def test_column_transformer_callable_specifier_invalid():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n        raise ValueError(\"Invalid function\")\n\n    with pytest.raises(ValueError):\n        ct = ColumnTransformer([('trans', Trans(), func)], remainder='drop')\n        ct.fit_transform(X_array)\n\n\n\n\n", "compilation error", "compilation error", "def test_column_transformer_sparse_output():\n    X_array = np.array([[0, 1, 0], [2, 4, 6], [8, 6, 4]]).astype('float')\n\n    # test sparse transformer\n    ct = ColumnTransformer([('trans1', StandardScaler(), [0]),\n                            ('trans2', SparseMatrixTrans(), [1, 2])],\n                           sparse_output=True)\n\n    X_trans = ct.fit_transform(X_array)\n    assert sparse.issparse(X_trans)\n    assert X_trans.shape == (3, 3)\n\n    ct = ColumnTransformer([('trans1', StandardScaler(), [0]),\n                            ('trans2', StandardScaler(), [1, 2])],\n                           sparse_output=True)\n    X_trans = ct.fit_transform(X_array)\n    assert not sparse.issparse(X_trans)\n    assert X_trans.shape == (3, 3)\n\n    ct = ColumnTransformer([('trans1', StandardScaler(), [0]),\n                            ('trans2',  lambda X: X, [1, 2])],\n                           sparse_output=True)\n    X_trans = ct.fit_transform(X_array)\n    assert sparse.issparse(X_trans)\n    assert X_trans.shape == (3, 3)\n", "def test_column_transformer_callable_specifier_error():\n    X_array = np.array([[0, 1, 2],\n                        [2, 4, 6]]).T\n\n        raise ValueError\n\n    ct = ColumnTransformer([('trans', Trans(), func)])\n    assert_raise_message(ValueError, \"callable\", ct.fit_transform, X_array)\n"], "sample_455": ["    def test_expressions_with_include(self):\n        msg = (\n            \"UniqueConstraint.include cannot be used with expressions. Use \"\n            \"django.contrib.postgres.indexes.GinIndex() instead.\"\n        )\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                Lower(\"field\"),\n                name=\"test_func_include\",\n                include=[\"other_field\"],\n            )\n", "    def test_opclass_with_custom_function(self):\n        msg = (\n            \"UniqueConstraint.opclasses cannot be used with custom functions. \"\n            \"Use django.contrib.postgres.indexes.OpClass() instead.\"\n        )\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                Lower(\"field\"),\n                F(\"field\"),\n                name=\"test_func_opclass\",\n                opclasses=[\"jsonb_path_ops\"],\n            )\n\n\n", "    def test_expressions_with_include(self):\n        msg = (\n            \"UniqueConstraint.include cannot be used with expressions. \"\n            \"Use django.db.models.Model.objects.filter() to restrict \"\n            \"rows instead.\"\n        )\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                Lower(\"field\"), name=\"test_func_include\", include=[\"other_field\"]\n            ) \n", "    def test_expressions_with_condition(self):\n        msg = \"UniqueConstraint.condition cannot be used with expressions.\"\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                Lower(\"field\"),\n                name=\"test_func_condition\",\n                condition=models.Q(id__lt=10),\n            )\n\n", "    def test_expressions_with_include(self):\n        msg = \"UniqueConstraint.include cannot be used with expressions.\"\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                Lower(\"field\"),\n                name=\"test_func_include\",\n                include=[\"other_field\"],\n            )\n"], "sample_391": ["    def test_create_model_add_field_order_reversal(self):\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    \"Foo\", [(\"name\", models.CharField(max_length=255))]\n                ),\n                migrations.AddField(\"Foo\", \"age\", models.IntegerField()),\n                migrations.CreateModel(\n                    \"Bar\", [(\"name\", models.CharField(max_length=255))]\n                ),\n                migrations.AddField(\n                    \"Bar\", \"foo\", models.ForeignKey(\"migrations.Foo\", models.CASCADE)\n                ),\n            ],\n            [\n                migrations.CreateModel(\n                    \"Bar\",\n                    [\n                        (\"name\", models.CharField(max_length=255)),\n                        (\n                            \"foo\",\n                            models.ForeignKey(\"migrations.Foo\", models.CASCADE),\n                        ),\n                    ],\n                ),\n                migrations.CreateModel(\n                    \"Foo\", [(\"name\", models.CharField(max_length=255)), (\"age\", models.IntegerField())],\n                ),\n            ],\n        )\n", "    def test_create_model_add_field_with_options(self):\n        \"\"\"\n        AddField with options should optimize into CreateModel.\n        \"\"\"\n        managers = [(\"objects\", EmptyManager())]\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[\n                        (\"name\", models.CharField(max_length=255)),\n                    ],\n                    options={\n                        \"verbose_name\": \"Foo\",\n                        \"db_table\": \"my_foo_table\",\n                    },\n                    bases=(UnicodeModel,),\n                    managers=managers,\n                ),\n                migrations.AddField(\n                    \"Foo\", \"age\",\n                    models.IntegerField(),\n                    options={\"default\": 0},\n                ),\n            ],\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[\n                        (\"name\", models.CharField(max_length=255)),\n                        (\"age\", models.IntegerField(default=0)),\n                    ],\n                    options={\n                        \"verbose_name\": \"Foo\",\n                        \"db_table\": \"my_foo_table\",\n                    },\n                    bases=(UnicodeModel,),\n                    managers=managers,\n                ),\n            ],\n        )\n\n", "    def test_models_with_same_name_but_different_fields(self):\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    \"Foo\", [(\"name\", models.CharField(max_length=255))]\n                ),\n                migrations.CreateModel(\n                    \"Foo\", [(\"value\", models.IntegerField())]\n                ),\n            ],\n            [\n                migrations.CreateModel(\n                    \"Foo\",\n                    [(\"name\", models.CharField(max_length=255)), (\"value\", models.IntegerField())],\n                ),\n            ],\n        )\n\n", "    def test_create_alter_fk_field(self):\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    \"Foo\", [(\"name\", models.CharField(max_length=255))]\n                ),\n                migrations.CreateModel(\"Bar\", [(\"name\", models.CharField(max_length=255))]),\n                migrations.AddField(\n                    \"Foo\", \"bar\", models.ForeignKey(\"migrations.Bar\", models.CASCADE)\n                ),\n                migrations.AlterField(\n                    \"Foo\", \"bar\", models.ForeignKey(\"migrations.Bar\", models.PROTECT)\n                ),\n            ],\n            [\n                migrations.CreateModel(\n                    \"Foo\",\n                    [\n                        (\"name\", models.CharField(max_length=255)),\n                        (\n                            \"bar\",\n                            models.ForeignKey(\"migrations.Bar\", models.PROTECT),\n                        ),\n                    ],\n                ),\n            ],\n        )\n", "    def test_create_alter_unique_field_with_fk_conflict(self):\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    \"Foo\", [(\"name\", models.CharField(max_length=255))]\n                ),\n                migrations.CreateModel(\"Bar\", [(\"id\", models.AutoField(primary_key=True))]),\n                migrations.AddField(\n                    \"Foo\", \"bar\", models.ForeignKey(\"migrations.Bar\", on_delete=models.CASCADE)\n                ),\n                migrations.AlterUniqueTogether(\n                    \"Foo\",\n                    [(\"name\", \"bar__id\")],\n                ),\n            ],\n            [\n                migrations.CreateModel(\n                    \"Foo\",\n                    [\n                        (\"name\", models.CharField(max_length=255)),\n                        (\"bar\", models.ForeignKey(\"migrations.Bar\", on_delete=models.CASCADE)),\n                    ],\n                ),\n            ],\n        )\n\n\n\n"], "sample_420": ["    def test_model_form_instance_error_propagates(self):\n        form_class = modelform_factory(\n            model=StrictAssignmentFieldSpecific, fields=[\"title\"]\n        )\n        form = form_class(data={\"title\": \"testing setattr\"}, files=None)\n        form.instance._should_error = True\n        with self.assertRaises(ValidationError):\n            form.validate()\n", "    def test_invalid_model_field_with_custom_error_messages(self):\n        \"\"\"\n        Ensure that custom error messages from model fields persist\n        when used with model forms\n        \"\"\"\n        class CustomErrorField(models.CharField):\n                if value == \"invalid\":\n                    raise ValidationError(\"This value is invalid.\", code=\"custom\")\n                return value\n\n        class CustomErrorModel(models.Model):\n            field = CustomErrorField(max_length=10)\n\n        class CustomForm(forms.ModelForm):\n            class Meta:\n                model = CustomErrorModel\n                fields = [\"field\"]\n\n        form = CustomForm(data={\"field\": \"invalid\"})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors[\"field\"], [\"This value is invalid.\"])\n        self.assertEqual(form.errors[\"field\"][0].code, \"custom\")\n\n", "    def test_model_form_with_custom_clean_method(self):\n        class CustomCleanModel(models.Model):\n            title = models.CharField(max_length=50)\n        \n        class CustomCleanModelForm(forms.ModelForm):\n            class Meta:\n                model = CustomCleanModel\n                fields = \"__all__\"\n    \n                cleaned_data = super().clean()\n                if \"t\" in cleaned_data[\"title\"]:\n                    raise ValidationError(\"Title cannot contain the letter 't'\")\n                return cleaned_data\n\n        form = CustomCleanModelForm(data={\"title\": \"Test title\"})\n        with self.assertRaises(ValidationError):\n            form.validate()\n\n\n\n", "    def test_model_form_with_custom_error_messages(self):\n        custom_form = CustomErrorMessageForm(data={\"name1\": \"@#$!!**@#$\", \"name2\": \"ABC\"})\n        self.assertFalse(custom_form.is_valid())\n        self.assertEqual(custom_form.errors[\"name1\"], [\"Form custom error message.\"])\n        self.assertEqual(custom_form.errors[\"name2\"], [\"Model custom error message.\"])\n\n", "    def test_setattr_raises_validation_error_on_instance_creation_with_fields(self):\n        form_class = modelform_factory(\n            model=StrictAssignmentAll, fields=[\"title\", \"text\"]\n        )\n        form = form_class(data={\"title\": \"testing setattr\", \"text\": \"hello\"})\n        # This line turns on the ValidationError; it avoids the model erroring\n        # when its own __init__() is called when creating form.instance.\n        form.instance._should_error = True\n        with self.assertRaises(ValidationError):\n            form.save()\n\n\n\n"], "sample_337": ["    def test_csrf_cookie_samesite_none(self):\n        req = self._get_request()\n        with self.settings(CSRF_COOKIE_NAME='csrfcookie', CSRF_COOKIE_SAMESITE='None'):\n            mw = CsrfViewMiddleware(token_view)\n            mw.process_view(req, token_view, (), {})\n            resp = mw(req)\n            self.assertEqual(resp.cookies['csrfcookie']['samesite'], 'None')\n", "    def test_csrf_cookie_cleared_on_logout(self):\n        \"\"\"\n        The CSRF cookie is cleared when the user logs out.\n        \"\"\"\n        req = self._get_request();\n        mw = CsrfViewMiddleware(token_view)\n        mw.process_request(req)\n        resp = mw(req)\n        csrf_cookie = self._read_csrf_cookie(req)\n        self.assertTrue(csrf_cookie)\n\n        # Simulate logout and remove session data\n        req.session.flush()\n\n        # Process the request again after logout\n        resp = mw(req)\n        csrf_cookie = self._read_csrf_cookie(req)\n        self.assertFalse(csrf_cookie)\n", "    def test_https_good_referer_same_domain_different_port(self):\n        \"\"\"\n        A POST HTTPS request from a subdomain on a different port than 443\n        should be accepted if the referer is on the same domain.\n        \"\"\"\n        self._test_https_good_referer_same_domain_different_port()\n", "    def test_https_good_referer_matches_cookie_domain_and_session_domain(self):\n        \"\"\"\n        A POST HTTPS request with a good referer should be accepted even when CSRF_COOKIE_DOMAIN and SESSION_COOKIE_DOMAIN differ.\n        \"\"\"\n        self._test_https_good_referer_matches_cookie_domain()\n", "    def test_custom_csrf_cookie_name(self):\n        \"\"\"\n        The CSRF cookie name can be customized using CSRF_COOKIE_NAME.\n        \"\"\"\n        req = self._get_request()\n        mw = CsrfViewMiddleware(token_view)\n        mw.process_view(req, token_view, (), {})\n        csrf_cookie = self._read_csrf_cookie(req)\n        self.assertTrue(csrf_cookie)\n        self.assertIn('csrf_token', req.cookies)\n\n"], "sample_916": ["    def check_class_inheritance(role, tag):\n        pattern = r'{role}-role:.*?class=[\"\\'](?P<classes>.*?)[\"\\'].*?</a>'.format(role=role)\n        result = re.search(pattern, output)\n        expect = '''\\", "    def roles(role, text):\n        pattern = r'{role}-role:.*?'\n        result = re.search(pattern, output)\n        expect = '''\\", "    def check(role, tag, expected_classes):\n        classes_found = classes(role, tag)\n        assert classes_found == set(expected_classes), (\n            f\"Classes for role `{role}` with tag `{tag}` \"\n            f\"expected: {expected_classes}, found: {classes_found}\"\n        )\n", "    def check_xref(role, target, text):\n        pattern = r'{role}-role:.*?{}<a .*?>{}</a>.*?'.format(target, target)\n        res = re.search(pattern, output)\n        if not res:\n            print(\"Pattern\\n\\t%s\\nnot found in %s\" % (pattern, test))\n            assert False\n", "    def test_xref_consistency_types(classes):\n        assert classes('type', 'code') == {'std::vector', 'int'}\n        assert classes('type', 'cite') == {'std::string'}\n        assert classes('tParam', 'code') == {'int'}\n        assert classes('tParam', 'cite') == {'T'}\n        assert classes('concept', 'code') == {'Iterator'}\n        assert classes('concept', 'cite') == {'Numeric'}\n"], "sample_1113": ["def test_block_index_slice():\n    M = BlockMatrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    B = BlockMatrix([[1, 2], [3, 4], [5, 6]])\n    assert M[:, :2].as_explicit() == ImmutableMatrix([[1, 2], [4, 5], [7, 8]])\n    assert M[:2, :].as_explicit() == ImmutableMatrix([[1, 2, 3], [4, 5, 6]])\n    assert M[1:, 1:].as_explicit() == ImmutableMatrix([[4, 5], [7, 8]])\n    assert M[::2, ::2].as_explicit() == ImmutableMatrix([[1, 3], [5, 7]])\n    assert B[1, :].as_explicit() == ImmutableMatrix([3, 4])\n    assert B[:2, 1:].as_explicit() == ImmutableMatrix([[2], [4]])\n\n\n\n", "compilation error", "def test_block_matrix_transpose():\n    from sympy.matrices.matrices import Matrix\n\n    k, l, m, n = symbols('k l m n', integer=True)\n    M = BlockMatrix([[MatrixSymbol('A', k, l), MatrixSymbol('B', k, m)],\n                     [MatrixSymbol('C', l, n), MatrixSymbol('D', l, m)]])\n\n    assert M.transpose().blocks[0, 0] == M.blocks[0, 0].transpose()\n    assert M.transpose().blocks[0, 1] == M.blocks[1, 0].transpose()\n    assert M.transpose().blocks[1, 0] == M.blocks[0, 1].transpose()\n    assert M.transpose().blocks[1, 1] == M.blocks[1, 1].transpose()\n\n", "def test_block_index_irregular_shapes():\n    A1 = MatrixSymbol('A1', 2, 3)\n    A2 = MatrixSymbol('A2', 3, 4)\n    A = BlockMatrix([[A1, A2]])\n    assert A[0, 0] == A1[0, 0]  \n    assert A[1, 3] == A2[1, 3]\n    assert A[0, 4] is None\n\n\n\n", "def test_block_diag_matrix():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 3, 3)\n    M = BlockDiagMatrix(A, B)\n    assert M[0, 0] == A[0, 0]\n    assert M[1, 1] == B[1, 1]\n    assert M[2, 2] == B[2, 2]\n    assert M[0, 2] == 0\n    assert M[2, 0] == 0\n\n"], "sample_303": ["    def test_runshell(self):\n        with mock.patch('subprocess.run') as mock_run:\n            self.client.runshell(parameters={'arg1': 'value1'})\n            mock_run.assert_called_once_with(\n                args=[\n                    # args should be populated here based on \n                    # self.client.settings_to_cmd_args_env()\n                ],\n                env={\n                    # env should be populated here based on \n                    # self.client.settings_to_cmd_args_env()\n                },\n                check=True\n            )\n", "    def test_runshell(self):\n        with mock.patch.object(subprocess, 'run') as mock_run:\n            self.client.runshell(parameters={\"arg1\": \"val1\"})\n            mock_run.assert_called_once_with(\n                args=self.client.executable_name,\n                env=self.client.settings_to_cmd_args_env(\n                    self.client.connection.settings_dict, \n                    {\"arg1\": \"val1\"}\n                ),\n                check=True,\n            )\n", "    def test_runshell(self):\n        with mock.patch('subprocess.run') as mock_run:\n            self.client.runshell(parameters={'some_parameter': 'value'})\n            mock_run.assert_called_once_with(\n                args=mock.ANY,\n                env=mock.ANY,\n                check=True\n            )\n", "    def test_runshell_calls_settings_to_cmd_args_env(self):\n        with mock.patch.object(self.client, 'settings_to_cmd_args_env') as mock_func:\n            self.client.runshell(None)\n            mock_func.assert_called_once_with(self.client.connection.settings_dict, None)\n", "    def test_runshell_calls_settings_to_cmd_args_env(self):\n        with mock.patch.object(self.client, 'settings_to_cmd_args_env') as mock_method:\n            self.client.runshell(None)\n            mock_method.assert_called_once_with(self.client.connection.settings_dict, None)\n"], "sample_289": ["    def test_setdefault(self):\n        d = CaseInsensitiveMapping({'Accept': 'application/json'})\n        self.assertEqual(d.setdefault('Content-Type', 'text/plain'), 'text/plain')\n        self.assertEqual(list(d.items()), [('Accept', 'application/json'), ('Content-Type', 'text/plain')])\n        self.assertEqual(d.setdefault('Accept', 'application/rss'), 'application/json')\n        self.assertEqual(list(d.items()), [('Accept', 'application/json'), ('Content-Type', 'text/plain')])\n", "    def test_setdefault_with_case_insensitive_key(self):\n        self.assertEqual(self.dict1.setdefault('accept', 'new_value'), 'application/json')\n        self.assertEqual(self.dict1.setdefault('Foo', 'new_value'), 'new_value')\n        self.assertEqual(self.dict1['accept'], 'application/json')\n        self.assertEqual(self.dict1['Foo'], 'new_value')\n", "    def test_setdefault(self):\n        self.assertEqual(self.dict1.setdefault('new', 'value'), 'value')\n        self.assertEqual(self.dict1['new'], 'value')\n        self.assertEqual(self.dict1.setdefault('Accept', 'new_value'), 'application/json')\n        self.assertEqual(self.dict1['Accept'], 'application/json')\n", "    def test_setdefault(self):\n        self.assertEqual(self.dict1.setdefault('somekey', 'default'), 'default')\n        self.assertEqual(self.dict1['somekey'], 'default')\n        self.assertEqual(self.dict1.setdefault('Accept', 'new_value'), 'application/json')\n        self.assertEqual(self.dict1['Accept'], 'application/json')\n", "    def test_setdefault(self):\n        self.assertEqual(self.dict1.setdefault('missing_key', 'default'), 'default')\n        self.assertEqual(self.dict1['missing_key'], 'default')\n        self.assertEqual(self.dict1.setdefault('Accept', 'new_value'), 'application/json')\n        self.assertEqual(self.dict1['Accept'], 'application/json')\n"], "sample_733": ["def test_vectorizer_encoding_errors():\n    vect = CountVectorizer(strip_accents='unicode')\n    text = \"caf\u00e9\"\n    assert_raises(UnicodeEncodeError, vect.fit, [text])\n\n\n", "    def test_vectorizer_stop_words_custom():\n        stop_words = [\"the\", \"a\", \"an\"]\n        vect = CountVectorizer(stop_words=stop_words)\n        docs = [\"This is a test document.\", \"Another example document.\"]\n        X = vect.fit_transform(docs)\n        assert_array_equal(X.shape, (2, 4))\n        assert_array_equal(vect.get_feature_names(), [\n            'is', 'test', 'document', 'example'])\n", "    def test_hashingvectorizer_custom_analyzer():\n        vect = HashingVectorizer(analyzer=lambda x: list(x))\n        X = vect.fit_transform(JUNK_FOOD_DOCS)\n        assert_array_equal(X.toarray().shape, (len(JUNK_FOOD_DOCS), len(JUNK_FOOD_DOCS[0])))\n", "def test_vectorizer_with_custom_analyzer():\n\n        return [token for token in text.lower().split() if len(token) > 2]\n\n    vect = CountVectorizer(analyzer=custom_analyzer)\n    X = vect.fit_transform(JUNK_FOOD_DOCS)\n\n    assert_array_equal(X[0].toarray(), [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1])\n    assert_equal(vect.get_feature_names(), ['junk', 'food', 'drink', 'this', 'place'])\n    assert_equal(vect.vocabulary_, {'junk': 0, 'food': 1, 'drink': 2, 'this': 3, 'place': 4})\n\n\n    \n\n\n\n", "compilation error"], "sample_1007": ["def test_binomial_expand():\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True)\n    x = Symbol('x')\n\n    assert expand_func(binomial(n, 0)) == 1\n    assert expand_func(binomial(n, n)) == 1\n    assert expand_func(binomial(n, 1)) == n*x\n    assert expand_func(binomial(n, 2)) == n*(n - 1)*(x**2)/2\n    assert expand_func(binomial(n, 3)) == n*(n - 1)*(n - 2)*(x**3)/6\n    assert expand_func(binomial(n, k)) == \\\n        n*(n - 1)*(n - 2)*...*(n - k + 1)*(x**k)/factorial(k)\n    assert expand_func(binomial(x, n)) == x**n\n\n\n", "def test_binomial_special_cases():\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True)\n    p = Symbol('p', real=True)\n\n    assert binomial(n, 0) == 1\n    assert binomial(n, n) == 1\n    assert binomial(n, k) == binomial(n, n - k)\n    assert binomial(n, k).rewrite(Product) == 'Product(_i, (_i, 0, k))'\n    assert binomial(n, k).rewrite(sum) == sum(Binomial(n, i) for i in range(k + 1))\n    assert binomial(n, n - 1) == n\n    assert binomial(n, 1) == n\n    assert binomial(n, k) == factorial(n) / (factorial(k) * factorial(n - k))\n    assert binomial(n, k).subs(n, k) == 1\n    assert binomial(n, k).subs(k, n) == 1\n    assert binomial(n, k).subs(n, k + 1) == binomial(k + 1, k)\n\n    assert binomial(n, k).evalf(4) == binomial(n, k).evalf()\n\n    assert binomial(n, k).is_integer is True\n    assert binomial(n, k).is_positive is True\n    assert binomial(n, 0).is_even is True\n    assert binomial(n, n).is_even is True\n    assert binomial(n, 1).is_odd is True\n\n    assert binomial(n, k).is_real is True\n    assert binomial(n, k).is_real is True\n    assert binomial(n, k).is_complex is False\n    assert binomial(n, k).is_rational is True\n    assert binomial(n, k).is_algebraic is True\n\n    assert binomial(n, k).as_reduced_fraction() == binomial(n, k)\n\n    assert binomial(n, k).limit(n, oo) == 0 if k < n else 1\n    assert binomial(n, k).limit(k, oo) == 0 if k > n else 1\n    assert", "def test_binomial_rewrite_to_gamma():\n    n = Symbol('n', integer=True, nonnegative=True)\n    k = Symbol('k', integer=True, nonnegative=True)\n    assert binomial(n, k).rewrite(gamma).simplify() == \\\n        gamma(n + 1) / (gamma(k + 1) * gamma(n - k + 1))\n\n\n\n", "def test_binomial_rewrite_special_cases():\n    n = Symbol('n',  integer=True)\n    k = Symbol('k', integer=True)\n\n    assert binomial(n, 0).rewrite(gamma) == 1\n    assert binomial(n, n).rewrite(gamma) == 1\n    assert binomial(n, n - 1).rewrite(gamma) == n/1\n    assert binomial(n, 1).rewrite(gamma) == n/1\n\n    assert binomial(n, k).rewrite(Product) == \\\n        Product(i, (i, 1, k)) * (n - k + 1) / factorial(k) \n\n\n\n", "def test_binomial_expand():\n    x = Symbol('x')\n    n = Symbol('n', integer=True)\n\n    assert expand_func(binomial(n, 0)) == 1\n    assert expand_func(binomial(n, 1)) == n*x\n    assert expand_func(binomial(n, 2)) == n*(n - 1)*x**2/2\n    assert expand_func(binomial(n, 3)) == n*(n - 1)*(n - 2)*x**3/6\n    assert expand_func(binomial(n, 4)) == n*(n - 1)*(n - 2)*(n - 3)*x**4/24\n    assert expand_func(binomial(n, 5)) == n*(n - 1)*(n - 2)*(n - 3)*(n - 4)*x**5/120\n\n\n\n"], "sample_783": ["def test_missing_indicator_multiple_missing_values(X, missing_values):\n    indicator = MissingIndicator(missing_values=missing_values)\n    X_trans = indicator.fit_transform(X)\n    assert_array_equal(X_trans, np.array([[True, False, True],\n                                          [False, False, False],\n                                          [False, True, False]]))\n", "def test_imputation_sparse_mixed_data_types(strategy):\n    X = sparse.csc_matrix([\n        [1., 2, np.nan],\n        [4, 5, 6],\n        [7, 8, np.nan]\n    ])\n    X_dense_part = X.toarray()\n    imputer = SimpleImputer(strategy=strategy, fill_value=0)\n    X_trans = imputer.fit_transform(X)\n\n    assert_array_equal(X_trans.toarray(), X_dense_part)\n    assert isinstance(X_trans, sparse.csc_matrix)\n", "def test_missing_indicator_sparse_input(X, missing_values):\n    indicator = MissingIndicator(missing_values=missing_values)\n    X_trans = indicator.fit_transform(X)\n\n    assert isinstance(X_trans, (sparse.csr_matrix, sparse.csc_matrix))\n    assert X_trans.shape == X.shape\n", "def test_missing_indicator_with_different_sparse_formats():\n    X_fit = sparse.csc_matrix([[1, np.nan, 3], [np.nan, 4, np.nan]])\n    X_trans = sparse.csr_matrix([[1, np.nan, 3], [np.nan, 4, np.nan]])\n    indicator = MissingIndicator(missing_values=np.nan, sparse='auto')\n    X_fit_mask = indicator.fit_transform(X_fit)\n    X_trans_mask = indicator.transform(X_trans)\n\n    assert X_fit_mask.format == 'csc'\n    assert X_trans_mask.format == 'csr'\n\n", "    def test_missing_indicator_sparse_with_pipeline(sparse):\n        from sklearn.pipeline import Pipeline\n        from sklearn.impute import SimpleImputer\n        from sklearn.preprocessing import StandardScaler\n\n        X = sparse_random_matrix(100, 100, density=0.1 if sparse else None)\n        missing_values = X.data[0] if sparse else 0\n\n        pipeline = Pipeline([\n            ('imputer', SimpleImputer(missing_values=missing_values)),\n            ('indicator', MissingIndicator(missing_values=missing_values)),\n            ('scaler', StandardScaler())\n        ])\n\n        pipeline.fit(X)\n        # Test that the pipeline outputs the correct data type, even with sparse\n        # input.\n        X_trans_sparse = pipeline.transform(X)\n        assert isinstance(X_trans_sparse, sparse_random_matrix if sparse else np.ndarray)\n\n\n\n"], "sample_711": ["def test_isinitpath_handles_simple_paths() -> None:\n    class FakeSession:\n        rootpath = Path.cwd()\n        _initialpaths = {\n            Path.cwd(),\n            Path(\"some/other/dir\"),\n            Path(\"yet/another/dir\"),\n        }\n\n    session = cast(pytest.Session, FakeSession)\n    assert session.isinitpath(Path.cwd())\n    assert session.isinitpath(Path(\"some/other/dir\"))\n    assert session.isinitpath(Path(\"yet/another/dir\"))\n    assert not session.isinitpath(Path(\"not/found\"))\n    assert not session.isinitpath(Path(\"nonexistent/path\"))\n\n\n\n", "def test_deprecated_node_constructors(pytester: Pytester) -> None:\n    \"\"\"Ensure we warn when deprecated constructors are used.\"\"\"\n    pytester.makeconftest(\"\"\"\n        import pytest\n        from _pytest.nodes import Node\n        from _pytest.nodes import Collector\n        \n            class MyNode(Node):\n                    warnings.warn(\"Deprecated constructor called\", DeprecationWarning)\n                    super().__init__(*args, **kwargs)\n            return MyNode\n        \n            class MyCollector(Collector):\n                    warnings.warn(\"Deprecated constructor called\", DeprecationWarning)\n                    super().__init__(*args, **kwargs)\n            return MyCollector\n    \"\"\")\n    \n    pytester.runpytest(\n        \"test_node.py\",\n        capture=\"all\",\n        extra_args=[\"--collect-only\"],\n    )\n    # Check if DeprecationWarnings were issued during construction.\n    output = pytester.getoutput()\n    assert \"deprecated constructor called\" in output\n    #  Make sure no tests are executed, as we only want to check the warnings\n    assert \"collected\" in output.lower()\n\n\n\n", "def test_node_warn_deprecated_message() -> None:\n    with pytest.raises(DeprecationWarning, match=\"deprecated\"):\n        nodes.Node.from_parent(None, fspath=None)  # type: ignore[arg-type]\n", "def test_node_warn_is_now_a_warning(pytester: Pytester) -> None:\n    items = pytester.getitems(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    with pytest.warns(Warning, match=\"deprecated\"):\n        items[0].warn(DeprecationWarning(\"deprecated\"))\n", "def test_file_constructor_fspath_and_path_are_merged() -> None:\n    with pytest.raises(TypeError, match=\"fspath and path\"):\n        nodes.File(fspath=\"a\", path=Path(\"b\"))\n\n"], "sample_52": ["    def test_model_choice_field_with_limit_choices_to(self):\n        f = forms.ModelChoiceField(\n            Category.objects.all(), limit_choices_to={'name__startswith': 'A'}\n        )\n        self.assertEqual(len(f.choices), 1)\n        self.assertEqual(list(f.choices), [\n            ('', '---------'),\n            (self.c2.pk, 'A test'),\n        ])\n", "    def test_value_error(self):\n        f = forms.ModelChoiceField(Category.objects.all())\n        with self.assertRaisesMessage(ValueError, \"Invalid choice\"):\n            f.clean(\"invalid\")\n\n        with self.assertRaisesMessage(ValueError, \"Invalid choice\"):\n            f.clean(1234567890)\n        \n        # If the ModelChoiceField is tied to a form, a ValueError should be thrown\n        class ModelChoiceForm(forms.Form):\n            category = forms.ModelChoiceField(Category.objects.all())\n        form = ModelChoiceForm(data={'category': 'invalid'})\n        self.assertEqual(form.errors['category'], ['Invalid choice.'])\n", "    def test_custom_label_template(self):\n        class CustomModelChoiceForm(forms.Form):\n            category = forms.ModelChoiceField(\n                Category.objects.all(),\n                label='Choose a category',\n                widget=forms.Select(attrs={'class': 'custom-select'}),\n                template_name=\"{% if value %}{{ value }}% else { %}{{ label }} %}\"\n            )\n\n        form = CustomModelChoiceForm()\n        self.assertHTMLEqual(\n            form.fields['category'].widget.render('name', None),\n            '''<select class=\"custom-select\">", "    def test_choice_label(self):\n        class CustomModelChoiceField(forms.ModelChoiceField):\n                return obj.slug\n\n        f = CustomModelChoiceField(Category.objects.all())\n        self.assertEqual(list(f.choices), [\n            ('', '---------'),\n            (self.c1.pk, 'entertainment'),\n            (self.c2.pk, 'test'),\n            (self.c3.pk, 'third-test'),\n        ])\n\n\n", "    def test_select_related_from_choices(self):\n        class AuthorWithCategory(models.Model):\n            name = models.CharField(max_length=100)\n            category = models.ForeignKey(Category, on_delete=models.SET_NULL, null=True, related_name='authors')\n\n        Author.objects.create(name='Test Author')\n        Category.objects.create(name='Test Category')\n\n        class ArticleForm(forms.ModelForm):\n            author = forms.ModelChoiceField(AuthorWithCategory.objects.all(),\n                                            to_field_name='category__slug')\n\n            class Meta:\n                model = Article\n                fields = ['author', 'pub_date']\n        form = ArticleForm()\n        choices = form.fields['author'].choices\n        self.assertEqual([x[1] for x in choices], ['Test Category'])\n\n\n\n"], "sample_863": ["        def fit(self, X, y=None, **fit_params):\n            if fit_params != {'b': 1}:\n                raise ValueError\n            return self\n", "        def fit(self, X, y=None, **fit_params):\n            for transformer in self.transformers_:\n                transformer[1].fit(X, y, **fit_params)\n            return self\n", "compilation error", "        def fit(self, X, y=None, **fit_params):\n            if fit_params != {'b': 1}:\n                raise ValueError\n            return self", "    def test_pipeline_with_custom_transformer_missing_fit():\n        class NoFitTransf(BaseEstimator):\n                return X * 2\n\n        pipe = Pipeline([('transf', NoFitTransf()), ('clf', DummyClassifier())])\n        X = iris.data\n        y = iris.target\n        pipe.fit(X, y)\n"], "sample_804": ["compilation error", "def test_one_hot_encoder_sparse_missing_values():\n    enc = OneHotEncoder(sparse=True)\n    X = np.array([['abc', 1, 55], ['def', np.nan, 55]]).T\n    with pytest.raises(ValueError, match=\"Input contains NaN\"):\n        enc.fit(X)\n\n\n", "def test_one_hot_encoder_sparse_output_dtype():\n    enc = OneHotEncoder(sparse=True, dtype='int32')\n    X = [['abc', 2, 55], ['def', 1, 55]]\n    trans = enc.fit_transform(X)\n    assert isinstance(trans, scipy.sparse.csr_matrix)\n    assert trans.dtype == 'int32'\n\n\n", "def test_one_hot_encoder_sparse_transform():\n    X = np.array([['abc', 2, 55], ['def', 1, 55]])\n    enc = OneHotEncoder(sparse=True)\n    sparse_data = enc.fit_transform(X)\n    dense_data = sparse_data.toarray()\n    assert isinstance(sparse_data, sparse.csr_matrix)\n    assert_array_equal(dense_data,\n                       [[1, 0, 1, 1],\n                        [0, 1, 0, 1]])\n    \n    # Test transform on new data\n    new_data = np.array([['def', 1, 55], ['abc', 2, 55]])\n    sparse_transform = enc.transform(new_data)\n    sparse_transform_array = sparse_transform.toarray()\n    assert isinstance(sparse_transform, sparse.csr_matrix)\n    assert_array_equal(sparse_transform_array,\n                       [[0, 1, 0, 1],\n                        [1, 0, 1, 0]])\n\n\n\n", "def test_one_hot_encoder_empty_categories():\n    enc1 = OneHotEncoder(categories=[[], []], sparse=False)\n    X = [[1, 2], [3, 4]]\n    with pytest.raises(ValueError, match=\"Categories should not be empty\"):\n        enc1.fit(X)\n"], "sample_648": ["    def test_pytest_param_id_allows_none_or_string(s) -> None:\n        assert pytest.param(id=s)\n", "    def test_marker_expr_eval_unexpected_result_type(pytester: Pytester) -> None:\n        foo = pytester.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.internal_err\n                pass\n        \"\"\"\n        )\n        result = pytester.runpytest(foo, \"-m\", \"internal_err == True\")\n        result.stderr.fnmatch_lines([\n            \"ERROR: Wrong expression passed to '-m': internal_err == True: \"\n            \"Expected a boolean, got <class 'str'>: internal_err\"\n        ])\n        assert result.ret == ExitCode.USAGE_ERROR\n", "    def test_empty_str_param_marker_arg(pytester: Pytester) -> None:\n        p = pytester.makepyfile(\n            test_empty_str_param_marker_arg=\"\"\"\n            import pytest\n\n            @pytest.mark.parametrize(\"obj_type\", [pytest.param(\"\", marks=[pytest.mark.empty])])\n                pass\n        \"\"\"\n        )\n\n        reprec = pytester.inline_run(p)\n        reprec.assertoutcome(passed=1)\n", "    def test_parameterset_for_parametrize_marks_custom(pytester: Pytester) -> None:\n        pytester.makeini(\n            \"\"\"\n        [pytest]\n        custom_empty_parameters_set=skipif\n        \"\"\".format(\n            EMPTY_PARAMETERSET_OPTION\n        )", "    def test_keyword_with_none_type(pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n        import pytest\n\n        @pytest.mark.skipif(None, reason=\"skip if None\")\n            assert 1 == 1\n        \"\"\"\n        )\n        result = pytester.runpytest()\n        result.assertoutcome(passed=1)\n"], "sample_29": ["    def test_write_latex_empty_cosmo(self, write, tmp_path):\n        fp = tmp_path / \"test_write_latex_empty_cosmo.tex\"\n        EmptyCosmo = type(\"EmptyCosmo\", (Cosmology,), {})\n        write(fp, format=\"latex\", cosmology=EmptyCosmo())\n        tbl = QTable.read(fp)\n        assert len(tbl) == 0\n", "    def test_write_latex_empty_cosmo(self, write, tmp_path):\n        fp = tmp_path / \"test_write_latex_empty_cosmo.tex\"\n        empty_cosmo = Cosmology()\n        write(fp, format=\"latex\", cosmology=empty_cosmo)\n        tbl = QTable.read(fp)\n        assert len(tbl) == len(empty_cosmo.__parameters__)\n", "    def test_write_latex_empty_cosmology(self, write, tmp_path):\n        \"\"\"Test writing an empty cosmology to LaTeX.\"\"\"\n        fp = tmp_path / \"test_empty_cosmology.tex\"\n        empty_cosmo = Cosmology()\n        write(fp, format=\"latex\", cosmology=empty_cosmo)\n        tbl = QTable.read(fp)\n        assert len(tbl) == 0\n", "    def test_write_latex_no_overwrite(self, write, tmp_path):\n        fp = tmp_path / \"test_write_latex_no_overwrite.tex\"\n        write(fp, format=\"latex\", overwrite=False)\n        assert not os.path.exists(fp)\n        \n", "    def test_latex_names_false(self, write, tmp_path):\n        fp = tmp_path / \"test_latex_names_false.tex\"\n        write(fp, format=\"latex\", latex_names=False)\n        tbl = QTable.read(fp)\n        for column_name in tbl.colnames[2:]:\n            assert column_name not in _FORMAT_TABLE.values()\n"], "sample_1071": ["def test_check_dimensions():\n    from sympy.physics.units.util import check_dimensions\n    assert check_dimensions(3*meter + 2*meter) == 5*meter\n    assert check_dimensions(3*meter + 2*second) == check_dimensions(3*meter + 2*second)\n    with raises(ValueError):\n        check_dimensions(3*meter + 2*second)\n    with raises(ValueError):\n        check_dimensions(3 * meter + 2)\n    with raises(ValueError):\n        check_dimensions(3 * meter + 2 * kilogram)\n", "compilation error", "def test_check_dimensions():\n    from sympy.physics.units import meter, second\n\n    assert check_dimensions(meter + meter) == 2*meter\n    assert check_dimensions(meter + 5) == meter + 5\n    assert check_dimensions(meter * second + meter * second) == 2*meter*second\n    with raises(ValueError):\n        check_dimensions(meter + second)\n    with raises(ValueError):\n        check_dimensions(meter + 1*meter)\n    with raises(ValueError):\n        check_dimensions(meter + 2 * meter**2)\n", "compilation error", "def test_check_dimensions():\n    assert check_dimensions(2*meter + 3*second) == 2*meter + 3*second\n    assert check_dimensions(2*meter + 3*meter) == 5*meter\n    raises(ValueError, lambda: check_dimensions(2*meter + 3*second))\n    raises(ValueError, lambda: check_dimensions(2*meter + 3*gram))\n    raises(ValueError, lambda: check_dimensions(2*meter + 3*newton / second))\n    raises(ValueError, lambda: check_dimensions(2*(meter + 3*second)))\n    assert check_dimensions(2*meter*second + 3*meter*second) == 5*meter*second\n"], "sample_1040": ["def test_print_matrix_symbols_with_dimensions():\n    A = MatrixSymbol('A', 2, 3)\n    assert mpp.doprint(A) == '<msub><mi>A</mi><mn>2</mn></msub><mn>3</mn></mi>'\n    assert mp.doprint(A) == '<matrix>\n  <mrow>\n    <mi>A</mi>\n  </mrow>", "def test_print_matrix_symbol_with_size():\n    A = MatrixSymbol('A', 2, 3)\n    assert mathml(A, printer='presentation') == '<mi>A<sub>2x3</sub></mi>'\n    assert mathml(A, printer='content') == '<apply><bvar><ci>x</ci><ci>y</ci></bvar></apply>'\n", "compilation error", "def test_print_matrix_element():\n    A = MatrixSymbol('A', 2, 2)\n    assert mathml(A[0, 0], printer='presentation') == '<msub><mi>A</mi><mn>0</mn></msub><mn>0</mn></msub>'\n    assert mathml(A[0, 0], printer='content') == '<apply><index/><ci>A</ci><cn>0</cn><cn>0</cn></apply>'\n    assert mathml(A[1, 1], printer='presentation') == '<msub><mi>A</mi><mn>1</mn></msub><mn>1</mn></msub>'\n    assert mathml(A[1, 1], printer='content') == '<apply><index/><ci>A</ci><cn>1</cn><cn>1</cn></apply>'\n\n\n\n\n", "def test_print_matrix():\n    A = Matrix([[1, 2], [3, 4]])\n    assert mpp.doprint(A) == '<mtable><mtr><mtd><mn>1</mn></mtd><mtd><mn>2</mn></mtd></mtr><mtr><mtd><mn>3</mn></mtd><mtd><mn>4</mn></mtd></mtr></mtable>'\n    assert mp.doprint(A) == '<apply><matrix/><mpadded width=\"0\"><mn>1</mn></mpadded><mpadded width=\"0\"><mn>2</mn></mpadded></apply>'\n    assert mathml(A, printer=\"presentation\", mat_symbol_style=\"bold\") == '<bold><mtable><mtr><mtd><mn>1</mn></mtd><mtd><mn>2</mn></mtd></mtr><mtr><mtd><mn>3</mn></mtd><mtd><mn>4</mn></mtd></mtr></mtable></bold>'\n    assert mathml(A, printer=\"content\", mat_symbol_style=\"bold\") == '<apply><matrix/><mpadded width=\"0\"><mn>1</mn></mpadded><mpadded width=\"0\"><mn>2</mn></mpadded></apply>'\n\n"], "sample_22": ["def test_matrix_product():\n    # Test with different input shapes\n    m1 = np.array([[1, 2], [3, 4]])\n    m2 = np.array([[5, 6], [7, 8]])\n    m3 = np.array([[9, 10], [11, 12]])\n\n    # 2x2 matrices\n    assert_allclose(matrix_product(m1, m2), m1 @ m2)\n    \n    # Test stacking and matrix multiplication\n    m_stack = np.stack((m1, m2), axis=0)\n    m_result = matrix_product(m_stack, m3)\n    assert m_result.shape == (2, 2, 2, 2)\n    for i in range(2):\n        assert_allclose(m_result[i], m_stack[i] @ m3)\n\n    # Test with a single 2x2 matrix\n    assert_allclose(matrix_product(m1), m1)\n    \n    with pytest.raises(ValueError):\n        matrix_product(m1, m2, m3)  # too many arguments\n\n\n\n", "compilation error", "def test_matrix_product():\n    m1 = rotation_matrix(45 * u.deg, \"x\")\n    m2 = rotation_matrix(45 * u.deg, \"y\")\n    m3 = rotation_matrix(45 * u.deg, \"z\")\n\n    expected = rotation_matrix(90 * u.deg, \"z\")\n    assert_allclose(matrix_product(m1, m2, m3), expected, atol=1e-12)\n    assert_allclose(matrix_product(m3, m2, m1), expected, atol=1e-12) \n\n    m4 = np.zeros((2, 3, 3))\n    m4[:, 0, 0] = 1\n    assert_allclose(matrix_product(m4, m1), m4)   # matrix product with a zero matrix\n    \n", "def test_matrix_product():\n    m1 = np.array([[1, 2], [3, 4]])\n    m2 = np.array([[5, 6], [7, 8]])\n    m3 = np.array([[9, 10], [11, 12]])\n\n    assert_allclose(\n        matrix_product(m1, m2, m3), np.dot(np.dot(m1, m2), m3), atol=1e-12\n    )\n\n    # Test with multi-dimensional arrays\n    m1 = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n    m2 = np.array([[[9, 10], [11, 12]], [[13, 14], [15, 16]]])\n    assert_allclose(\n        matrix_product(m1, m2), np.dot(m1, m2), atol=1e-12\n    )\n", "def test_matrix_product():\n    m1 = rotation_matrix(45 * u.deg, \"x\")\n    m2 = rotation_matrix(45 * u.deg, \"y\")\n    m3 = rotation_matrix(45 * u.deg, \"z\")\n\n    result = matrix_product(m1, m2, m3)\n    expected = rotation_matrix(90 * u.deg, \"z\")\n    assert_allclose(result, expected, atol=1e-12)\n\n    # Test with a stack of matrices\n    m1_stack = np.tile(m1, (2, 1, 1))\n    m2_stack = np.tile(m2, (2, 1, 1))\n    result_stack = matrix_product(m1_stack, m2_stack)\n    expected_stack = np.tile(rotation_matrix(90 * u.deg, \"z\"), (2, 1, 1))\n    assert_allclose(result_stack, expected_stack, atol=1e-12)\n"], "sample_864": ["def test_mean_shift_seeds_empty():\n    # Test MeanShift with empty seeds.\n    ms = MeanShift(seeds=[])\n    msg = \"All input points should be used as seeds.\"\n    with pytest.raises(ValueError, match=msg):\n        ms.fit(X)\n", "def test_meanshift_bin_seeding():\n    # Test bin seeding with MeanShift\n    ms = MeanShift(bin_seeding=True, bandwidth=1.2)\n    labels = ms.fit_predict(X)\n    assert len(np.unique(labels)) == n_clusters\n\n\n\n", "def test_mean_shift_non_finite_values():\n    # Test MeanShift with non-finite values\n    X = np.array([[-np.inf, 0], [np.inf, np.inf], [0, 1]])\n    msg = r\"Input contains inf or nan\"\n    with pytest.raises(ValueError, match=msg):\n        MeanShift().fit(X)\n", "def test_meanshift_bandwidth_0():\n    # Test MeanShift with bandwidth=0\n    ms = MeanShift(bandwidth=0)\n    labels = ms.fit_predict(X)\n    assert_array_equal(labels, np.unique(labels))\n", "def test_meanshift_on_sparse_data():\n    X_dense = np.array([[1, 1], [2, 1], [1, 0], [4, 7], [3, 5], [3, 6]])\n    X_sparse = sparse.csr_matrix(X_dense)\n    ms = MeanShift(bandwidth=2)\n    labels_dense = ms.fit(X_dense).labels_\n    labels_sparse = ms.fit(X_sparse).labels_\n    assert_array_equal(labels_dense, labels_sparse)\n"], "sample_12": ["def test_angle_from_string_with_units_and_decimals():\n    # Test parsing of angles with various units and fractional seconds\n    angles = [\n        '10.123456789d',\n        '10.123deg',\n        '10\u00b012\\'34.56789\" ',  \n        '10h12m34.56789s',\n        '10h12m34s',\n        '10h12m34.56789s',\n        '10d12m34.56789s',\n        '10d12m34s',\n        '123.456789deg',\n        '123.456789h'\n    ]\n    for angle in angles:\n        a = Angle(angle)\n        assert a.unit == u.deg\n        assert isinstance(a.value, float) \n\n\n\n", "def test_angle_with_invalid_string():\n    with pytest.raises(ValueError):\n        Angle('1d 2.34m 5s')\n    with pytest.raises(ValueError):\n        Angle('abc')\n\n\n\n", "def test_angle_to_galactic():\n    # Test angle conversion to galactic coordinates\n    angle = Angle(\n        '10d20m30.12345678s', unit='deg'\n    ).to(u.deg)\n    gal = angle.to_galactic()\n    assert isinstance(gal, Angle)\n    assert gal.unit == u.deg\n    # assert some values are not 'nan'\n\n\n\n", "def test_angle_comparison_with_nan():\n    a = Angle(0 * u.deg)\n    b = Angle(np.nan * u.deg)\n    assert not (a == b)\n    assert not (a != b)\n    assert a < b is np.nan\n    assert a > b is np.nan\n    assert a <= b is np.nan\n    assert a >= b is np.nan\n\n\n", "def test_angle_array_wrap_at():\n    \"\"\"Test wrapping an array of angles.\"\"\"\n    a = Angle([100, 170, -10, 360, -20, 400] * u.deg)\n    wrapped = a.wrap_at(360 * u.deg)\n    expected = Angle([100, 170, -10, 0, 20, 40] * u.deg)\n    assert_allclose(wrapped.value, expected.value)\n\n    wrapped = a.wrap_at(180 * u.deg)\n    expected = Angle([100, 170, -10, 180, -20, 40] * u.deg)\n    assert_allclose(wrapped.value, expected.value)\n\n    # Test with an angle wrapped outside the desired range\n    a = Angle([100, 170, -10, 360, -20, 500] * u.deg)\n    wrapped = a.wrap_at(300 * u.deg)\n    expected = Angle([100, 170, -10, 300, -20, 140] * u.deg)\n    assert_allclose(wrapped.value, expected.value)\n\n\n\n"], "sample_761": ["def test_imputation_add_indicator_shape(imputer_constructor, X, missing_values,\n                                        expected_shape, feature_indices):\n    imputer = imputer_constructor(missing_values=missing_values, add_indicator=True)\n    X_out = imputer.fit_transform(X)\n    assert X_out.shape == expected_shape\n    assert_array_equal(imputer.indicator_.features_, feature_indices)\n", "def test_missing_indicator_sparse_csr_with_imp_values():\n    # test MissingIndicator with sparse CSR matrix and imputation\n    X = sparse.csr_matrix([[1, 2, 3], [4, 5, np.nan], [7, 8, 9]])\n    mi = MissingIndicator(missing_values=np.nan, features='all', sparse=True)\n    X_trans = mi.fit_transform(X)\n    assert X_trans.data.dtype == bool\n    assert X_trans.shape == (3, 3)\n    assert_allclose(X_trans.toarray(), np.array([[True, True, True],\n                                                [True, True, False],\n                                                [True, True, True]]))\n\n", "def test_missing_indicator_indicator_shape(param_features, n_features):\n    X = np.array([[0, 1, 2, 3],\n                  [np.nan, np.nan, 6, 7],\n                  [8, 9, np.nan, np.nan],\n                  [10, 11, 12, 13]])\n    indicator = MissingIndicator(missing_values=np.nan,\n                                features=param_features)\n    indicator.fit(X)\n    assert indicator.indicator_.shape[1] == n_features\n\n\n\n", "def test_imputation_add_indicator_with_different_strategies(missing_values, strategy):\n    X = np.array([\n        [missing_values, 1,      5,       missing_values, 1],\n        [2,      missing_values, 1,       missing_values, 2],\n        [6,      3,      missing_values,  missing_values, 3],\n        [1,      2,      9,       missing_values, 4]\n    ])\n    imputer = SimpleImputer(missing_values=missing_values,\n                            add_indicator=True,\n                            strategy=strategy)\n    X_trans = imputer.fit_transform(X)\n    assert X_trans.shape[1] == X.shape[1] + 1\n\n", "compilation error"], "sample_817": ["def test_constant_features():\n    # Test VarianceThreshold with constant features.\n    X = np.array([[1, 1, 1, 1],\n                  [1, 1, 1, 1],\n                  [1, 1, 1, 1]])\n    sel = VarianceThreshold(threshold=0.).fit(X)\n    assert_array_equal([True, True, True, True], sel.get_support())\n\n    X = csr_matrix(X)\n    sel = VarianceThreshold(threshold=0.).fit(X)\n    assert_array_equal([True, True, True, True], sel.get_support())\n\n", "def test_threshold_zero():\n    # Test threshold=0 behavior.\n    for X in [data, csr_matrix(data), csc_matrix(data), bsr_matrix(data)]:\n        sel = VarianceThreshold(threshold=0.).fit(X)\n        assert_array_equal(np.arange(len(data[0])), sel.get_support(indices=True))\n", "def test_sparse_threshold_peakto_peak():\n    X = csr_matrix([[1, 0, 0],\n                    [1, 0, 0],\n                    [1, 0, 0]])\n    selector = VarianceThreshold(threshold=0)\n    selector.fit(X)\n    assert_array_equal(selector.variances_, np.zeros(X.shape[1]))\n\n\n", "def test_threshold_zero():\n    # Test VarianceThreshold with threshold=0.\n    for X in [data, csr_matrix(data), csc_matrix(data), bsr_matrix(data)]:\n        sel = VarianceThreshold(threshold=0).fit(X)\n        assert_array_equal(np.arange(len(data[0])), sel.get_support(indices=True))\n", "def test_threshold_zero():\n    # Test VarianceThreshold with threshold=0\n    for X in [data, csr_matrix(data), csc_matrix(data), bsr_matrix(data)]:\n        sel = VarianceThreshold(threshold=0).fit(X)\n        assert_array_equal(np.arange(len(data[0])), sel.get_support(indices=True)) \n"], "sample_412": ["    def test_urlize_with_trim_url_limit(self):\n        items = (\n            (\n                \"Search for https://www.verylong.example.com/this-is-a-very-long-url-that-should-be-truncated\",\n                'Search for <a href=\"https://www.verylong.example.com/this-is-a-very-long-url-that-should-be-truncated\">https://www.verylong.example.com/this-is-a-very-long-url-that-should-...</a>',\n            ),\n            (\"Search for https://www.short.example.com\", 'Search for <a href=\"https://www.short.example.com\">https://www.short.example.com</a>'),\n            (\"Search for www.example.com\", 'Search for <a href=\"http://www.example.com\">www.example.com</a>'),\n        )\n        for value, output in items:\n            with self.subTest(value=value):\n                self.assertEqual(urlize(value, trim_url_limit=60), output)\n\n", "    def test_urlize_trailing_punctuation(self):\n        self.assertEqual(urlize(\"This is a test, isn't it?.\"),\n                         'This is a test, isn\\'t it?.'\n                         )\n        self.assertEqual(urlize(\"This is a test!.\"),\n                         'This is a test!.'\n                         )\n", "    def test_urlize_trim_url_limit(self):\n        self.assertEqual(\n            urlize(\n                \"This is a long URL: https://www.example.com/this-is-a-very-long-url-that-is-way-too-long-for-a-simple-example\",\n                trim_url_limit=15,\n            ),\n            \"This is a long URL: https://www.example.com/this-is-a-very-l...\"\n        )\n\n", "compilation error", "    def test_urlize_with_trim_url_limit(self):\n        test_strings = (\n            (\n                \"This is a long URL: http://www.example.com/this-is-a-very-long-url-that-is-longer-than-the-trim-url-limit-of-20-characters\",\n                \"This is a long URL: http://www.example.com/this-is-a-very-long-url-that-is-longer-than-the-trim-url-limit-of-20-characters\",\n            ),\n            (\n                \"This is a long URL: http://www.example.com/this-is-a-very-long-url-that-is-longer-than-the-trim-url-limit-of-20-characters\",\n                \"This is a long URL: http://www.example.com/this-is-a-very-long-url-that-is-longer-than-the-trim-url-limit-of-20-characters\",\n            ),\n            (\n                \"This is a short URL: http://www.example.com\",\n                \"This is a short URL: http://www.example.com\",\n            ),\n        )\n        for value, expected in test_strings:\n            with self.subTest(value=value):\n                self.assertEqual(urlize(value, trim_url_limit=20), expected)\n\n\n\n\n"], "sample_390": ["    def test_was_modified_since_future_time(self):\n        mtime = 1343416141107817\n        header = http_date(mtime)\n        self.assertTrue(was_modified_since(header, mtime))\n", "    def test_was_modified_since_large_mtime(self):\n        \"\"\"\n        Large mtimes are handled correctly by was_modified_since (#21473).\n        \"\"\"\n        large_mtime = 2**63 - 1\n        header = http_date(large_mtime)\n        self.assertFalse(was_modified_since(header, large_mtime))\n", "    def test_was_modified_since_empty_header(self):\n        mtime = 1516239042\n        self.assertTrue(was_modified_since(None, mtime))\n", "    def test_was_modified_since_zero_size(self):\n        \"\"\"\n        Empty files should be served if not modified since the user last downloaded it (#18675).\n        \"\"\"\n        mtime = 1343416141\n        header = http_date(mtime)\n        self.assertFalse(was_modified_since(header, mtime, 0))\n", "    def test_was_modified_since_empty_header(self):\n        \"\"\"\n        was_modified_since should handle an empty If-Modified-Since header\n        gracefully.\n        \"\"\"\n        mtime = 1343416141\n        self.assertTrue(was_modified_since(header=None, mtime=mtime))\n"], "sample_138": ["    def test_post_processing_failure_handles_manifest(self):\n        self._create_file('test/faulty.css')\n\n        with self.assertRaises(Exception):\n            self._run_collectstatic()\n\n        manifest_content = json.loads(storage.staticfiles_storage.read_manifest())\n        self.assertNotIn('faulty.css', manifest_content['paths'])\n        self.assertIn('error', manifest_content)\n", "    def test_manifest_cache_persistence_after_collection(self):\n        manifest_path = self.staticfiles_storage.path(self.staticfiles_storage.manifest_name)\n\n        # 1. Collectstatic for the first time\n        self.run_collectstatic()\n\n        # 2. Check if the manifest is created\n        self.assertTrue(os.path.exists(manifest_path))\n\n        # 3. Modify a file to trigger a re-collection\n        with open(self._get_filename_path(\"test.txt\"), 'w') as f:\n            f.write(\"Updated content\")\n        self.run_collectstatic()\n\n        # 4. Check if the manifest is still valid and updated\n        manifest_content = self.staticfiles_storage.read_manifest()\n        self.assertIn('version', manifest_content)\n        self.assertGreater(manifest_content['version'], 0)\n        self.assertIn(self._get_filename_path(\"test.txt\"), manifest_content['paths'])\n", "    def test_post_process_order_independent(self):\n        temp_file_css = self._get_filename_path('temp_file.css')\n        with open(temp_file_css, 'w') as f:\n            f.write(\n                f\"body {{ background-image: url('other.d41d8cd98f00.css'); }}\",\n            )\n        temp_file_js = self._get_filename_path('temp_file.js')\n        with open(temp_file_js, 'w') as f:\n            f.write(\"alert('hi');\")\n        self.run_collectstatic()\n        with open(temp_file_css, 'r') as f:\n            content = f.read()\n        self.assertIn(\n            f\"background-image: url({self.hashed_file_path('other.d41d8cd98f00.css')});\",\n            content,\n        )\n        self.assertIn(self.hashed_file_path('temp_file.js'), content)\n", "    def test_manifest_write_protection(self):\n        manifest_path = self.static_files_storage.path(self.static_files_storage.manifest_name)\n        os.chmod(manifest_path, 0o444)  # Make manifest read-only\n\n        with self.assertRaises(PermissionError):\n            self.run_collectstatic()\n\n\n", "    def test_manifest_is_updated_on_change(self):\n        initial_manifest = storage.staticfiles_storage.load_manifest()\n        file_path = self._get_filename_path('cached/styles.css')\n        with open(file_path, 'w') as f:\n            f.write(\"/* initial content */\")\n        self.run_collectstatic()\n        updated_manifest = storage.staticfiles_storage.load_manifest()\n        self.assertNotEqual(initial_manifest, updated_manifest)\n\n        with open(file_path, 'w') as f:\n            f.write(\"/* updated content */\")\n        self.run_collectstatic()\n        final_manifest = storage.staticfiles_storage.load_manifest()\n        self.assertNotEqual(updated_manifest, final_manifest)\n"], "sample_673": ["def test_is_setup_py_does_not_contain_invalid_imports(tmpdir):\n    setup_py = tmpdir.join(\"setup.py\")\n    setup_py.write('from invalid_module import setup; setup(name=\"foo\")')\n    assert not _is_setup_py(setup_py)\n", "    def test_is_setup_py_with_comments(tmpdir, content):\n        setup_py = tmpdir.join(\"setup.py\")\n        setup_py.write(content)\n        assert _is_setup_py(setup_py)\n", "compilation error", "    def test_is_setup_py_is_a_directory(tmpdir):\n        setup_dir = tmpdir.mkdir(\"setup\")\n        setup_file = setup_dir.join(\"setup.py\")\n        setup_file.write('from setuptools import setup; setup(name=\"foo\")')\n        assert _is_setup_py(setup_dir)\n", "    def test_is_setup_py_fails_on_invalid_syntax(tmpdir):\n        setup_py = tmpdir.join(\"setup.py\")\n        setup_py.write('from setuptools import setup; setup(name=\"foo') # Missing closing parenthesis\n        assert not _is_setup_py(setup_py)\n"], "sample_176": ["    def test_mti_inheritance_multiple_model_removal(self):\n        Animal = ModelState('app', 'Animal', [\n            (\"id\", models.AutoField(primary_key=True)),\n        ])\n        Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n        Cat = ModelState('app', 'Cat', [], bases=('app.Animal',))\n        changes = self.get_changes([Animal, Dog, Cat], [Animal])\n        self.assertNumberMigrations(changes, 'app', 1) \n        self.assertOperationTypes(changes, 'app', 0, ['DeleteModel', 'DeleteModel'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='Dog')\n        self.assertOperationAttributes(changes, 'app', 0, 1, name='Cat')\n\n", "    def test_mti_inheritance_change_bases(self):\n        \"\"\"Adding a base to a MTI model should make the migration.\"\"\"\n        Animal = ModelState('app', 'Animal', [\n            (\"id\", models.AutoField(primary_key=True)),\n        ], options={'verbose_name': 'Animal'})\n        Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n        changes = self.get_changes([Animal, Dog], [Animal, Dog, ModelState('app', 'Cat', [], bases=('app.Animal',))])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, [\"CreateModel\"])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='Cat')\n        \n", "    def test_remove_mti_single_model(self):\n        \"\"\"\n        Removing a model with MTI and no other base models deletes it.\n        \"\"\"\n        Animal = ModelState('app', 'Animal', [\n            (\"id\", models.AutoField(primary_key=True)),\n        ])\n        Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n        changes = self.get_changes([Animal, Dog], [])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['DeleteModel', 'DeleteModel'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='Animal')\n        self.assertOperationAttributes(changes, 'app', 0, 1, name='Dog')\n    \n", "    def test_mti_inheritance_m2m_removal(self):\n        Dog = ModelState('app', 'Dog', [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"breeds\", models.ManyToManyField(\"Breed\")),\n        ])\n        Breed = ModelState('app', 'Breed', [\n            (\"id\", models.AutoField(primary_key=True)),\n        ])\n        changes = self.get_changes([Dog, Breed], [Breed])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['DeleteModel'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='Breed')\n        \n", "    def test_mti_inheritance_model_addition(self):\n        Animal = ModelState('app', 'Animal', [\n            (\"id\", models.AutoField(primary_key=True)),\n        ])\n        Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n        changes = self.get_changes([Animal], [Animal, Dog])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, [\"CreateModel\", \"CreateModel\"])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='Animal')\n        self.assertOperationAttributes(changes, 'app', 0, 1, name='Dog')\n"], "sample_952": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_944": ["def test_stringify_advanced_types(annotation: Any, expected: str):\n    assert stringify(annotation) == expected\n", "    def test_stringify_type_hints_builtin_types(annotation):\n        assert stringify(annotation) == str(annotation).__name__\n", "def test_stringify_type_hints_ForwardRef():\n    from typing import ForwardRef  # type: ignore\n    assert stringify(ForwardRef(\"myint\")) == \"myint\"  \n", "def test_stringify_forward_ref_with_aliased_type():\n    from typing import ForwardRef, TypeVar\n    T = TypeVar('T')\n    MyType = ForwardRef('MyType')  # type: ignore\n    \n    # Assuming MyType is aliased as int somewhere else in the code\n    # e.g. from types import MyType as int\n    \n    assert stringify(MyType) == \"int\"\n", "def test_stringify_typing_special_types(annotation, expected):\n    assert stringify(annotation) == expected  \n"], "sample_1155": ["compilation error", "compilation error", "def test_irrational_with_integer_powers():\n    assert construct_domain([sqrt(2)**2, sqrt(2)**3, 1]) == (ZZ, [ZZ(2), ZZ(2*sqrt(2)), ZZ(1)])\n    assert construct_domain([GoldenRatio**2, GoldenRatio**3, 1]) == (ZZ, [ZZ(GoldenRatio**2), ZZ(GoldenRatio**3), ZZ(1)])\n    assert construct_domain([Catalan**2, Catalan**3, 1]) == (ZZ, [ZZ(Catalan**2), ZZ(Catalan**3), ZZ(1)])\n", "def test_golden_ratio_and_catalan():\n    alg = QQ.algebraic_field(GoldenRatio)\n\n    assert construct_domain([GoldenRatio, Catalan], extension=True) == (\n        alg,\n        [alg.convert(GoldenRatio), alg.convert(Catalan)]\n    )\n", "compilation error"], "sample_366": ["    def test_iso8601_duration(self):\n        test_values = (\n            ('P1D', timedelta(days=1)),  \n            ('PT10H', timedelta(hours=10)),\n            ('PT10M', timedelta(minutes=10)),\n            ('PT10S', timedelta(seconds=10)),\n            ('P1DT10H', timedelta(days=1, hours=10)),\n            ('P1DT10H30M', timedelta(days=1, hours=10, minutes=30)),\n            ('P1DT10H30M15S', timedelta(days=1, hours=10, minutes=30, seconds=15)),\n            ('P1DT10H30M15S.123456', timedelta(days=1, hours=10, minutes=30, seconds=15, microseconds=123456)),\n            ('P-1D', timedelta(days=-1)),\n            ('PT-10H', timedelta(hours=-10)),\n            ('PT-10M', timedelta(minutes=-10)),\n            ('PT-10S', timedelta(seconds=-10)),\n            ('P-1DT10H', timedelta(days=-1, hours=10)),\n            ('P-1DT10H30M', timedelta(days=-1, hours=10, minutes=30)),\n            ('P-1DT10H30M15S', timedelta(days=-1, hours=10, minutes=30, seconds=-15)),\n            ('P-1DT10H30M15S.123456', timedelta(days=-1, hours=10, minutes=30, seconds=-15, microseconds=-123456)),\n            ('P1DT10H30M15S.123456Z', timedelta(days=1, hours=10, minutes=30, seconds=15, microseconds=123456)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):", "    def test_iso8601(self):\n        test_values = (\n            ('P1Y2M3DT4H5M6S', timedelta(years=1, months=2, days=3, hours=4, minutes=5, seconds=6)),\n            ('P1Y', timedelta(years=1)),\n            ('P2M3D', timedelta(days=61)),\n            ('P1D', timedelta(days=1)),\n            ('PT1H', timedelta(hours=1)),\n            ('PT30M', timedelta(minutes=30)),\n            ('PT1.2S', timedelta(seconds=1.2)),\n            ('PT1H30M', timedelta(hours=1, minutes=30)),\n            ('PT10H20M30S', timedelta(hours=10, minutes=20, seconds=30)),\n            ('PT10H20M30.123S', timedelta(hours=10, minutes=20, seconds=30, milliseconds=123)),\n            ('P-1Y', timedelta(years=-1)),\n            ('P-2M', timedelta(days=-61)),\n            ('P-1D', timedelta(days=-1)),\n            ('PT-1H', timedelta(hours=-1)),\n            ('PT-30M', timedelta(minutes=-30)),\n            ('PT-1.2S', timedelta(seconds=-1.2)),\n            ('PT-1H30M', timedelta(hours=-1, minutes=-30)),\n            ('PT-10H20M30S', timedelta(hours=-10, minutes=-20, seconds=-30)),\n            ('PT-10H20M30.123S', timedelta(hours=-10, minutes=-20, seconds=-30, milliseconds=-123)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n\n\n", "    def test_iso8601_duration(self):\n        test_values = (\n            ('P1D', timedelta(days=1)),\n            ('P1DT1H', timedelta(days=1, hours=1)),\n            ('P1DT1H30M', timedelta(days=1, hours=1, minutes=30)),\n            ('P1D2H30M1S', timedelta(days=1, hours=2, minutes=30, seconds=1)),\n            ('P1DT2H30M1S.1234567', timedelta(days=1, hours=2, minutes=30, seconds=1, microseconds=1234567)),\n            ('PT1H30M1S', timedelta(hours=1, minutes=30, seconds=1)),\n            ('PT1H30M1S.1234567', timedelta(hours=1, minutes=30, seconds=1, microseconds=1234567)),\n            ('P1Y', timedelta(days=365)),\n            ('P1Y2M', timedelta(days=365 + 61)),  # Account for leap years\n            ('P1Y2D3H4M5S', timedelta(days=365 + 2, hours=3, minutes=4, seconds=5)),\n            ('P1Y2M3D4H5M6S', timedelta(days=365 + 61 + 3, hours=4, minutes=5, seconds=6)),\n            ('PT123H45M6S', timedelta(hours=123, minutes=45, seconds=6)),\n            ('PT23H59M59S', timedelta(hours=23, minutes=59, seconds=59)),\n            ('P-1D', timedelta(-1)),\n            ('P-1DT-1H', timedelta(days=-1, hours=-1)),\n            ('P-1DT-1H-30M', timedelta(days=-1, hours=-1, minutes=-30)),\n            'P0DT0H0M0S', timedelta(0)),\n            'P0", "    def test_iso8601_duration(self):\n        test_values = (\n            ('P1D', timedelta(days=1)),\n            ('PT1H', timedelta(hours=1)),\n            ('PT1M', timedelta(minutes=1)),\n            ('PT1S', timedelta(seconds=1)),\n            ('P1DT1H', timedelta(days=1, hours=1)),\n            ('P1D2H3M4S', timedelta(days=1, hours=2, minutes=3, seconds=4)),\n            ('P1DT1H2M3S.123456789', timedelta(days=1, hours=1, minutes=2, seconds=3, microseconds=123456789)),\n            ('PT10H30M', timedelta(hours=10, minutes=30)),\n            ('PT23H59M59.999999S', timedelta(hours=23, minutes=59, seconds=59, microseconds=999999)),\n            ('PT10M30S.123S', timedelta(minutes=10, seconds=30, microseconds=123000)),\n            ('P1DT10H30M59S', timedelta(days=1, hours=10, minutes=30, seconds=59)),\n            ('PT10H30M59S.123456789', timedelta(hours=10, minutes=30, seconds=59, microseconds=123456789)),\n            ('P-1D', timedelta(days=-1)),\n            ('PT-1H', timedelta(hours=-1)),\n            ('PT-1M', timedelta(minutes=-1)),\n            ('PT-1S', timedelta(seconds=-1)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n\n\n\n", "    def test_iso8601_duration(self):\n        test_values = (\n            ('PT15S', timedelta(seconds=15)),\n            ('PT1H30M', timedelta(hours=1, minutes=30)),\n            ('PT1D2H30M', timedelta(days=1, hours=2, minutes=30)),\n            ('P2DT3H15M45S', timedelta(days=2, hours=3, minutes=15, seconds=45)),\n            ('PT1.5H', timedelta(hours=1, minutes=30)),  # Fractions of hours\n            ('PT10.1M', timedelta(minutes=10, seconds=6)),   # Fractions of minutes\n            ('PT10.1S', timedelta(seconds=10.1)),    # Fractions of seconds\n            ('P1Y', timedelta(days=365)),  # Years\n            ('P1Y2MT1H30M', timedelta(days=365, hours=1, minutes=30)),  \n            ('PT10D', timedelta(days=10)),\n            ('PT10D3H', timedelta(days=10, hours=3)),\n            ('PT10D3H15M', timedelta(days=10, hours=3, minutes=15)),\n            ('PT10D3H15M45S', timedelta(days=10, hours=3, minutes=15, seconds=45)),\n            ('P1Y2M', timedelta(days=365, minutes=120)),\n            ('P1Y2M3D1H', timedelta(days=365 + 73, hours=1)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n\n\n\n"], "sample_360": ["    def test_alias_key_prefix(self):\n        with override_settings(\n            CACHES={\n                'default': {\n                    'BACKEND': 'django.core.cache.backends.locmem.LocMemCache',\n                    'LOCATION': 'default',\n                    'KEY_PREFIX': 'default_',\n                },\n                'other': {\n                    'BACKEND': 'django.core.cache.backends.locmem.LocMemCache',\n                    'LOCATION': 'other',\n                    'KEY_PREFIX': 'other_',\n                },\n            }\n        ):\n            cache_default = caches['default']\n            cache_other = caches['other']\n            self.assertEqual(cache_default.get_prefix(), 'default_')\n            self.assertEqual(cache_other.get_prefix(), 'other_')\n\n\n", "    def test_cache_key_with_user_object(self):\n        request = self.factory.get(self.path)\n        request.user = User(pk=1)\n        template = engines['django'].from_string(\"This is a test\")\n        response = TemplateResponse(request, template)\n        learn_cache_key(request, response)\n        self.assertIsNotNone(get_cache_key(request))\n        self.assertIn(str(request.user.pk), get_cache_key(request))\n", "    def test_nonexistent_setting(self):\n        msg = \"The cache setting 'nonexistent' is not defined.\"\n        with self.assertRaisesMessage(MissingCacheBackendError, msg):\n            caches['nonexistent']\n", "    def test_update_cache_middleware_constructor_with_key_prefix(self):\n        middleware = UpdateCacheMiddleware(empty_response, key_prefix='my_prefix')\n        self.assertEqual(middleware.key_prefix, 'my_prefix')\n", "    def test_cache_with_key_prefix(self):\n        with override_settings(\n            CACHE_MIDDLEWARE_KEY_PREFIX='testprefix',\n        ):\n            middleware = CacheMiddleware(hello_world_view)\n            request = self.factory.get('/view/')\n            response = middleware(request)\n\n            # Assert that the key prefix is included in the cache key\n            key = get_cache_key(request)\n            self.assertEqual(key.startswith('testprefix.'), True)\n"], "sample_653": ["    def test_log_in_request_logcapture(testdir):\n        log_file = testdir.tmpdir.join(\"pytest.log\").strpath\n\n        testdir.makeini(\n            \"\"\"\n            [pytest]\n            log_file={}\n            log_file_level = INFO\n            log_cli=true\n            \"\"\".format(\n                log_file\n            )\n        )\n        testdir.makeconftest(\n            \"\"\"\n            import logging\n            import pytest\n\n                capture('logcapture')\n\n                logging.info({'test_msg': 'logreport'})\n", "def test_log_in_pytest_itemcollected(testdir):\n    log_file = testdir.tmpdir.join(\"pytest.log\").strpath\n\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file={}\n        log_file_level = INFO\n        log_cli=true\n        \"\"\".format(\n            log_file\n        )\n    )\n    testdir.makeconftest(\n        \"\"\"\n        import logging\n\n            logging.info('itemcollected: {}'.format(item.name))\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n                assert True\n        \"\"\"\n    )\n    testdir.runpytest()\n    with open(log_file) as rfh:\n        contents = rfh.read()\n        assert contents.count('itemcollected: test_first') == 1\n", "def test_log_in_pytest_collection_finish(testdir):\n    log_file = testdir.tmpdir.join(\"pytest.log\").strpath\n\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file={}\n        log_file_level = INFO\n        log_cli=true\n        \"\"\".format(\n            log_file\n        )\n    )\n    testdir.makeconftest(\n        \"\"\"\n        import logging\n        logger = logging.getLogger(__name__)\n\n            logger.info(\"Collection Finished\")\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n            assert True\n    \"\"\"\n    )\n    testdir.runpytest()\n    with open(log_file) as rfh:\n        contents = rfh.read()\n        assert \"Collection Finished\" in contents\n", "def test_log_level_override_with_config(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n            assert logging.getLogger().level == logging.WARNING\n    \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_level = DEBUG\n        log_cli=true\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*DEBUG*\", \"* test_log_level_override.py *\" ,\"* 1 passed in *\"])\n\n\n", "    def test_log_in_plugin_report(testdir):\n        log_file = testdir.tmpdir.join(\"pytest.log\").strpath\n\n        testdir.makeini(\n            \"\"\"\n            [pytest]\n            log_file={}\n            log_file_level = INFO\n            log_cli=true\n            \"\"\".format(\n                log_file\n            )\n        )\n        testdir.makeconftest(\n            \"\"\"\n            import logging\n            import pytest\n\n            class MyPlugin:\n                    logging.info(\"MyPlugin sessionfinish\")\n\n                logging.info(\"pytest_plugin_report_collected\")\n\n            pytest.pluginmanager.register_plugin(MyPlugin())\n            \"\"\"\n        )\n        testdir.makepyfile(\n            \"\"\"\n                assert True\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        with open(log_file) as rfh:\n            contents = rfh.read()\n            assert \"MyPlugin sessionfinish\" in contents\n            assert \"pytest_plugin_report_collected\" in contents\n"], "sample_1101": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1025": ["def test_codegen_array_printer():\n    from sympy.codegen.array_utils import CodegenArrayTensorProduct, CodegenArrayContraction, CodegenArrayDiagonal, CodegenArrayPermuteDims, CodegenArrayElementwiseAdd\n    \n    \n    A_ = MatrixSymbol(\"A\", 2, 2)\n    B_ = MatrixSymbol(\"B\", 2, 2)\n\n    # Test for CodegenArrayTensorProduct\n    product = CodegenArrayTensorProduct(\n        [A_, B_], [(0, 0), (0, 1), (1, 0), (1, 1),]\n    )\n    assert pycode(product) == \"numpy.einsum('ij,kl->ijkl', A, B)\"\n\n    # Test for CodegenArrayContraction\n    contraction = CodegenArrayContraction(\n        expr=product, contraction_indices=[(0, 0)],\n    )\n    assert pycode(contraction) == \"numpy.einsum('ijkl->ik', A, B)\" \n    \n    # Test for CodegenArrayDiagonal\n    diagonal = CodegenArrayDiagonal(expr=A_, diagonal_indices=[(1, 0)])\n    assert pycode(diagonal) == \"numpy.diagonal(A, 0, axis1=1, axis2=0)\"\n\n    # Test for CodegenArrayPermuteDims\n    permute = CodegenArrayPermuteDims(expr=A_, permutation=[(1, 0)])\n    assert pycode(permute) == \"numpy.transpose(A, [1, 0])\"\n\n    # Test for CodegenArrayElementwiseAdd\n    element_wise_add = CodegenArrayElementwiseAdd(expr=A_, args=[B_])\n    assert pycode(element_wise_add) == \"numpy.add(A, B)\" \n", "compilation error", "def test_codegen_array_expression():\n    from sympy.codegen.array_utils import CodegenArrayTensorProduct\n    from sympy.codegen.ast import CodegenArrayDiagonal, CodegenArrayElementwiseAdd\n    \n    x = symbols('x')\n    y = symbols('y')\n    i = symbols('i')\n    j = symbols('j')\n\n    # Test for CodegenArrayTensorProduct\n    base_tensor = CodegenArrayTensorProduct(\n        'A',  # name of array\n        [[x, y], [x, y]],  # tensor values\n        subranks=[2, 2], # ranks of subarrays\n        base_indices=['i', 'j'], # indices of base array\n    )\n    product = pycode(base_tensor)\n    assert product == 'numpy.einsum(\"ij,ij->\", array_A_ij, array_A_ij)'\n    \n    # Test for CodegenArrayDiagonal\n    diag = CodegenArrayDiagonal(\n        MatrixSymbol(\"A\", 2, 2),\n        diagonal_indices=[(0, 1)],\n        name='diagonal_A'\n    )\n    product = pycode(diag)\n    assert product == 'numpy.diagonal(A, 0, axis1=0, axis2=1)'\n\n\n", "def test_codegen_array_printing():\n    from sympy.codegen.array_utils import CodegenArrayTensorProduct, CodegenArrayContraction, CodegenArrayDiagonal\n    from sympy.matrices import MatrixSymbol\n\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n\n    # Test CodegenArrayTensorProduct\n    expr = CodegenArrayTensorProduct(A, B)\n    assert pycode(expr, printer=NumPyPrinter()) == 'numpy.einsum(\", , :,: \", A, B)'\n    assert pycode(expr, printer=SciPyPrinter()) == 'scipy.sparse.coo_matrix([1], ([0], [0]), shape=(2, 2)) + numpy.einsum(\", , :,: \", A, B)'\n\n    # Test CodegenArrayContraction\n    expr = CodegenArrayContraction(A, [(0, 0)])\n    assert pycode(expr, printer=NumPyPrinter()) == 'numpy.einsum(\"i,j->i\", A)'\n    assert pycode(expr, printer=SciPyPrinter()) == 'scipy.sparse.coo_matrix([1], ([0], [0]), shape=(2, 2)) + numpy.einsum(\"i,j->i\", A)'\n\n    # Test CodegenArrayDiagonal\n    expr = CodegenArrayDiagonal(A, [])\n    assert pycode(expr, printer=NumPyPrinter()) == 'numpy.diagonal(A)'\n    assert pycode(expr, printer=SciPyPrinter()) == 'scipy.sparse.coo_matrix([1], ([0], [0]), shape=(2, 2)) + numpy.diagonal(A)'\n\n\n\n", "compilation error"], "sample_666": ["def test_capture_method_chaining(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n            from _pytest.capture import capture\n            with capture.tee_stdout():\n                print(\"hello\")\n            with capture.tee_stderr():\n                print(\"world\", file=sys.stderr)\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-s\", str(p))\n    assert result.ret == 0\n    result.stdout.fnmatch_lines(\n        \"\"\"\n        *hello*\n        \"\"\"\n    )\n    result.stderr.fnmatch_lines(\n        \"\"\"\n        *world*\n        \"\"\"\n    )\n", "compilation error", "compilation error", "def test_capture_in_subprocess_with_encoding(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            capsys.write(\"hello\\n\".encode(\"latin-1\"))\n            captured = capsys.readouterr()\n            assert captured.out == \"hello\\n\"\n    \"\"\"\n    )\n    result = testdir.runpytest_subprocess(\"-s\", \"--capture=no\")\n    result.stdout.fnmatch_lines(\n        \"\"\"\n        *test_me*\n        *hello\\n*\n    \"\"\"\n    )\n\n", "def test_encoding_when_using_capture_fixture(testdir):\n    testdir.makeconftest(\n        \"\"\"\n        import sys\n\n            funcargs[\"encoding\"] = sys.stdout.encoding\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            return pytest.capture.Capture()\n\n            my_capture.start_capturing()\n            sys.stdout.write(\"hello\")\n            captured = my_capture.readouterr()\n            my_capture.stop_capturing()\n            assert captured.out.encode(encoding) == b\"hello\"\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    assert result.ret == 0\n"], "sample_967": ["def test_custom_mathjax_inline_tags(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    html = (r'<div class=\"math notranslate nohighlight\">\\s*<span>\\\\(a\\^2\\+b\\^2=c\\^2\\\\)</span>\\s*</div>')\n    assert re.search(html, content, re.S) \n", "def test_custom_mathjax_inline_display_markers(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    html = (r'<div class=\"math notranslate nohighlight\">\\s*'\n            r'\\[a\\^2\\+b\\^2=c\\^2\\]\\s*</div>')\n    assert re.search(html, content, re.S)\n", "def test_mathjax_custom_inline_tags(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    html = r'<span class=\"math notranslate nohighlight\">\\s*\\{a\\^2\\+b\\^2=c\\^2\\}</span>'\n    assert re.search(html, content, re.S)\n", "def test_mathjax_inline_config(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    assert r'\\(' in content\n    assert r'\\)' in content\n", "def test_mathjax_display_inlines(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    assert 'MathJax.Hub.Queue([\"Typeset\",MathJax.InputJax.TeX,{\"input\":' in content\n\n\n\n"], "sample_1156": ["def test_asinh_rewrite():\n    x = Symbol('x')\n    assert asinh(x).rewrite(log) == log(x + sqrt(x**2 + 1))\n    assert asinh(x).rewrite(atanh) == I*atanh(sqrt(x**2 + 1) / x)\n    assert asinh(x).rewrite(acosh) == acosh(sqrt(x**2 + 1)) - I*pi/2\n\n\n\n", "compilation error", "def test_sech_expansion():\n    x, y = symbols('x,y')\n    assert sech(x+y).expand(trig=True) == \\\n        1 / sqrt(cosh(2*x + 2*y))\n    assert sech(2*x).expand(trig=True) == 2*sech(x)*sech(x)\n    assert sech(3*x).expand(trig=True).expand() == \\\n        sech(x)**3 * cosh(x)**2 + 3*sech(x)*cosh(x)**2\n", "def test_acosh_series():\n    x = Symbol('x')\n    assert acosh(x).series(x, 1, 10) == I*pi/2 + sqrt(x - 1) * (1 + (1/2)*x**(-1/2) + (1*3/8)*x**(-3/2) + (1*3*5/16)*x**(-5/2) + O(x**(-7/2)))\n", "def test_issue_6719():\n    x = Symbol('x')\n    assert simplify(acoth(tanh(x)) - x) == 0\n"], "sample_1141": ["def test_matrix_multiplication_with_symbols():\n    x, y, z = symbols('x y z')\n    A = Matrix([[x, y], [z, 1]])\n    B = Matrix([[x, z], [y, 1]])\n    assert (A * B).doit() == Matrix([[x**2 + y*z, x*z + y], [x*z + z*1, y*z + 1]])\n\n\n", "def test_matrix_equality():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = Matrix([[1, 2], [3, 4]])\n    D = Matrix([[1, 2], [3, 4]])\n    assert A == A\n    assert A != B\n    assert C == D\n    assert C != B\n    assert (A == C).should_be_True\n    assert (A != C).should_be_False\n    assert (A == B).should_be_False\n    assert (A != B).should_be_True\n    raises(TypeError, lambda: A == 1)\n    raises(TypeError, lambda: A != 1)\n    raises(TypeError, lambda: A == \"a\")\n    raises(TypeError, lambda: A != \"a\")\n\n\n\n", "    def test_issue_10897():\n        A = MatrixSymbol('A', 2, 2)\n        B = MatrixSymbol('B', 2, 2)\n        C = MatrixSymbol('C', 2, 2)\n        expr = (A*B + A*C).subs(A, Identity(2))\n        assert expr.args == (B + C)\n", "def test_matrix_subs():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n\n    assert (A + B).subs({A: C}) == C + B\n    assert (A + B).subs({A: C, B: D}) == C + D\n    assert (A*B).subs({A: C}) == C*B\n    assert (A*B).subs({A: C, B: D}) == C*D\n    assert (MatPow(A, 2)).subs({A: C}) == C**2\n    assert (A.T).subs({A: C}) == C.T\n    assert (A.I).subs({A: C}) == C.I\n\n\n\n", "def test_matrix_equality():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n    assert A == A\n    assert A != B\n    assert A == C is False\n    assert (A + B) == (A + B)\n    assert (A + B) != (A - B)\n    assert (A * B) == (A * B)\n    assert (A * B) != (A / B)\n    assert (A * B).T == (B.T * A.T)\n    assert (A*A).T == (A.T * A.T)\n    assert (A*B).I == (A.I * B.I)\n"], "sample_338": ["    def test_operation_with_custom_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    name='CustomNamePerson', fields=[]\n                ),\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'customnameperson')\n", "    def test_add_field_to_existing_model(self):\n        before = [\n            ModelState('app', 'Book', [\n                ('id', models.AutoField(primary_key=True)),\n                ('title', models.CharField(max_length=200)),\n            ]),\n        ]\n        after = [\n            ModelState('app', 'Book', [\n                ('id', models.AutoField(primary_key=True)),\n                ('title', models.CharField(max_length=200)),\n                ('author', models.ForeignKey('Author', models.CASCADE)),\n            ])\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['AddField'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='author')\n\n\n", "    def test_suggest_name_with_custom_operations(self):\n        class Operation(migrations.Operation):\n                super().__init__()\n                self.name = name\n\n        class Migration(migrations.Migration):\n            operations = [\n                Operation('custom_operation_1'),\n                Operation('custom_operation_2'),\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'custom_operation_1_custom_operation_2')\n", "    def test_operation_with_field_modifications(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.AlterField(\n                    model_name='Person',\n                    old_field='id',\n                    new_field='user_id',\n                ),\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'change_person_id_to_user_id')\n\n", "    def test_runsql_operation(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.RunSQL('CREATE TABLE IF NOT EXISTS test_table (id integer PRIMARY KEY);'),\n            ]\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'create_test_table')\n"], "sample_158": ["    def test_foreign_object_through_m2m_field(self):\n        class Parent(models.Model):\n            a = models.CharField(max_length=255)\n            b = models.CharField(max_length=255)\n            c = models.CharField(max_length=255)\n            children = models.ManyToManyField(\"self\", symmetrical=False, through='ParentChild')\n\n        class ParentChild(models.Model):\n            parent = models.ForeignKey(Parent, models.CASCADE, related_name='children_via_m2m')\n            child = models.ForeignKey(Parent, models.CASCADE, related_name='+')\n\n        class Child(models.Model):\n            parent = models.ForeignObject(\n                Parent,\n                on_delete=models.SET_NULL,\n                from_fields=('a', 'b', 'c'),\n                to_fields=('a', 'b', 'c'),\n                related_name='children',\n            )\n\n        field = Child._meta.get_field('parent')\n        self.assertEqual(field.check(from_model=Child), [\n            Error(\n                \"Cannot use ForeignObject with a parent model that has 'ManyToManyField' to itself. \"\n                \"ForeignObject requires a unique set of fields on the parent model.\",\n                hint=(\n                    \"Either remove the 'ManyToManyField' on the parent model or \"\n                    \"use a different field for the ForeignObject link.\"\n                ),\n                obj=field,\n                id='fields.E311',\n            ),\n        ])\n\n\n", "    def test_superset_foreign_object_with_unique_constraint(self):\n        class Parent(models.Model):\n            a = models.PositiveIntegerField()\n            b = models.PositiveIntegerField()\n            c = models.PositiveIntegerField()\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(fields=['a', 'b'], name='unique_ab'),\n                ]\n\n        class Child(models.Model):\n            a = models.PositiveIntegerField()\n            b = models.PositiveIntegerField()\n            value = models.CharField(max_length=255)\n            parent = models.ForeignObject(\n                Parent,\n                on_delete=models.SET_NULL,\n                from_fields=('a', 'b'),\n                to_fields=('a', 'b'),\n                related_name='children',\n            )\n\n        field = Child._meta.get_field('parent')\n        self.assertEqual(field.check(from_model=Child), [])\n", "    def test_m2m_through_field_with_cascade(self):\n        class Fan(models.Model):\n            pass\n\n        class Event(models.Model):\n            invitees = models.ManyToManyField(\n                Fan,\n                through='Invitation',\n                through_fields=('invitee', 'event'),\n                related_name='events'\n            )\n\n        class Invitation(models.Model):\n            event = models.ForeignKey(Event, models.CASCADE)\n            invitee = models.ForeignKey(Fan, models.CASCADE)\n            inviter = models.ForeignKey(Fan, models.CASCADE, related_name='+')\n\n        field = Event._meta.get_field('invitees')\n        self.assertEqual(field.check(from_model=Event), [])\n\n\n\n", "    def test_order_matters(self):\n        class Fan(models.Model):\n            foo = models.CharField(max_length=10)\n\n        class Event(models.Model):\n            invitees = models.ManyToManyField(Fan, through='Invitation', through_fields=('fan', 'event_invitee'))\n\n        class Invitation(models.Model):\n            event = models.ForeignKey(Event, models.CASCADE)\n            fan = models.ForeignKey(Fan, models.CASCADE)\n            event_invitee = models.ForeignKey(Event, models.CASCADE)\n\n        field = Event._meta.get_field('invitees')\n        self.assertEqual(field.check(from_model=Event), [\n            Error(\n                \"'Invitation.fan' is not a foreign key to 'Event'.\",\n                hint=\"Did you mean one of the following foreign keys to 'Event': event?\",\n                obj=field,\n                id='fields.E339',\n            ),\n            Error(\n                \"'Invitation.event_invitee' is not a foreign key to 'Fan'.\",\n                hint=\"Did you mean one of the following foreign keys to 'Fan': invitee, inviter?\",\n                obj=field,\n                id='fields.E339',\n            ),\n        ])\n\n", "    def test_explicit_related_name_clash(self):\n        class Model(models.Model):\n            related_name_clash = models.ManyToManyField(\"self\", symmetrical=False, related_name='clash')\n\n        self.assertEqual(Model.check(), [\n            Error(\n                \"Reverse accessor for 'Model.related_name_clash' clashes with field name 'Model.clash'.\",\n                hint=(\n                    \"Rename field 'Model.clash', or add/change a related_name \"\n                    \"argument to the definition for field 'Model.related_name_clash'.\"\n                ),\n                obj=Model._meta.get_field('related_name_clash'),\n                id='fields.E302',\n            ),\n            Error(\n                \"Reverse query name for 'Model.related_name_clash' clashes with field name 'Model.clash'.\",\n                hint=(\n                    \"Rename field 'Model.clash', or add/change a related_name \"\n                    \"argument to the definition for field 'Model.related_name_clash'.\"\n                ),\n                obj=Model._meta.get_field('related_name_clash'),\n                id='fields.E303',\n            ),\n        ])\n\n\n\n"], "sample_502": ["def test_subplot_title():\n    fig, axes = plt.subplots(2, 1)\n    axes[0].set_title('Axis 1')\n    axes[1].set_title('Axis 2')\n    plt.close(fig) \n", "def test_subplots_reuse_projection():\n    fig = plt.figure()\n    ax1 = plt.subplot(121, projection='polar')\n\n\n    ax2 = plt.subplot(122, projection='polar')\n    assert ax1.name == 'polar'\n    assert ax2.name == 'polar'\n    plt.close(fig)\n", "def test_subplots_kwarg_collision():\n    fig, axes = plt.subplots(1, 2, projection='polar')\n    with pytest.raises(ValueError, match=\"Both 'projection' and 'subplot_kw'\"\n                       \"args are provided\"):\n        plt.subplots(1, 2, projection='rectilinear',\n                     subplot_kw={'projection': 'polar'})\n", "def test_subplot_projection_kwargs():\n    # Check if subplot() accepts and uses projection kwargs\n    ax1 = plt.subplot(projection='polar')\n    ax2 = plt.subplot(projection='rectilinear')\n    assert ax1.name == 'polar'\n    assert ax2.name == 'rectilinear'\n\n    # Check if subplots with same projection kwargs reuse the same axes\n    ax3 = plt.subplot(projection='polar')\n    assert ax1 is ax3\n\n    # Check if subplots can be mixed with and without projection kwargs\n    ax4 = plt.subplot(121)\n    ax5 = plt.subplot(122, projection='polar')\n    assert ax1 is not ax4\n    assert ax5.name == 'polar'\n\n    plt.close()\n\n", "def test_subplot_reuse_projection():\n    fig = plt.figure()\n    ax = plt.subplot(1, 2, 1, projection='polar')\n    ax1 = plt.subplot(1, 2, 1, projection='polar')\n    ax2 = plt.subplot(1, 2, 2, projection='rectilinear')\n    # This will delete ax / ax1\n    ax3 = plt.subplot(1, 2, 1, projection='polar')\n    assert ax is not None\n    assert ax1 is ax\n    assert ax2 is not ax\n    assert ax3 is ax1\n\n    assert ax not in fig.axes\n    assert ax2 in fig.axes\n    assert ax3 in fig.axes\n\n    assert ax.name == 'polar'\n    assert ax2.name == 'rectilinear'\n    assert ax3.name == 'polar'\n\n\n\n"], "sample_655": ["        def test_capture_mixed_output({0}):\n            print(\"hello\")\n            sys.stderr.write(\"world\\\\n\")\n            logger.warning(\"Warning message\")\n            captured = {0}.readouterr()\n            assert captured.out == \"hello\\\\n\"\n            assert captured.err == \"world\\\\n\"\n\n            # Check the order of messages\n            assert captured.out.split(\"\\n\")[-1] == \"hello\"\n            assert captured.err.split(\"\\n\")[-1] == \"world\"\n\n            assert \"Warning message\" in captured.err\n", "    def test_capture_with_live_logging_no_flush(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import logging\n            import sys\n            import time\n            logger = logging.getLogger(__name__)\n\n                print(\"hello\")\n                sys.stderr.write(\"world\\\\n\")\n                time.sleep(0.1) # Add a delay so flush doesn't work\n                logging.info(\"something\")\n                print(\"next\")\n                logging.info(\"something\")\n            \"\"\"\n        )\n\n        result = testdir.runpytest_subprocess(\"--log-cli-level=INFO\")\n        assert result.ret == 0\n", "def test_capture_with_live_logging_and_redirect(testdir):\n    # Issue 3819\n    # capture should work with live cli logging and stdout/stderr redirection\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import sys\n\n        logger = logging.getLogger(__name__)\n\n            sys.stdout = open(\"ignore.txt\", \"w\")\n            sys.stderr = open(\"ignore.txt\", \"w\")\n            print(\"hello\")\n            sys.stderr.write(\"world\\\\n\")\n            with capture.StdCapture() as cap:\n                logging.info(\"something\")\n            out, err = cap.readouterr()\n            assert out == \"hello\\n\"\n            assert err == \"world\\n\"\n            sys.stdout = sys.__stdout__\n            sys.stderr = sys.__stderr__\n    \"\"\"\n    )\n    result = testdir.runpytest_subprocess(\"--log-cli-level=INFO\")\n    assert result.ret == 0\n\n    assert \"hello\" in open(\"ignore.txt\", \"r\").read()\n    assert \"world\" in open(\"ignore.txt\", \"r\").read()\n    assert \"something\" in result.stdout.str()\n\n", "compilation error", "        def test_mixed_output(capsys):\n            print(\"hello\")\n            sys.stderr.write(\"world\\\\n\")\n            logger.info(\"This is a log message\")\n            captured = capsys.readouterr()\n            assert captured.out == \"hello\\n\"\n            assert captured.err == \"world\\n\"\n            assert \"This is a log message\" in captured.out \n            \n            print(\"next\")\n            \"\"\""], "sample_993": ["def test_FreeGroupElm_cyclic_conjugates():\n    w = x*y*x*y*x\n    conjugates = list(w.cyclic_conjugates())\n    assert len(conjugates) == 5\n    assert w in conjugates\n    assert w.is_cyclic_conjugate(x*y*x*y*x)\n    assert w.is_cyclic_conjugate(y*x*y*x*x)\n    assert not w.is_cyclic_conjugate(x*y*y*x*x)\n    assert not w.is_cyclic_conjugate(x*x*y*y*x)\n\n\n\n", "def test_FreeGroupElm_cyclic_conjugates():\n    w = x**2*y**5\n    assert set(w.cyclic_conjugates()) == \\\n        {x**2*y**5, x*y**5*x, x**2*y**5*x**-1, x*y**5*x**-1,\n         y**5*x**2*y**-1*x, y**5*x**2, y**5*x**2*x**-1}\n    assert set( (x*y*x*y*x).cyclic_conjugates()) == \\\n        {x*y*x**2*y, x**2*y*x*y, y*x*y*x**2, y*x**2*y*x}\n", "def test_FreeGroupElm_cyclic_conjugates():\n    w1 = x**2*y**5\n    assert len(w1.cyclic_conjugates()) == 5\n    assert w1 in w1.cyclic_conjugates()\n    assert (y**5*x**2) in w1.cyclic_conjugates()\n    assert (y**-1*x**2*y**5) in w1.cyclic_conjugates()\n\n    w2 = x**-3*y**-1*x**5\n    assert len(w2.cyclic_conjugates()) == 5\n    assert w2 in w2.cyclic_conjugates()\n    assert (y**-1*x**2 in w2.cyclic_conjugates())\n    assert (x**2*y**-1 in w2.cyclic_conjugates())\n\n\n\n", "def test_FreeGroupElm_cyclic_conjugates():\n    w = x**2*y*x**2*y\n    assert len(w.cyclic_conjugates()) == 5\n    assert w in w.cyclic_conjugates()\n    assert y*x**2*y*x**2 in w.cyclic_conjugates()\n    assert x*y*x**2*y*x**2 in w.cyclic_conjugates()\n    assert x**2*y*x**2*y*x in w.cyclic_conjugates()\n\n\n\n", "def test_FreeGroupElm_cyclic_conjugates():\n    w = x**2*y**5\n    assert len(w.cyclic_conjugates()) == 5\n    assert w in w.cyclic_conjugates()\n    assert w.cyclic_subword(1, 2) in w.cyclic_conjugates()\n    assert w.cyclic_subword(0, 2) in w.cyclic_conjugates()\n    assert w.cyclic_conjugates() != set()\n    assert w.cyclic_conjugate(w.cyclic_subword(1, 2))\n    assert w.cyclic_conjugate(w.cyclic_subword(1, 2)) != w\n\n\n"], "sample_512": ["def test_subplot_reuse_with_projection():\n    fig = plt.figure()\n    ax1 = plt.subplot(111, projection='polar')\n    ax2 = plt.subplot(111, projection='polar')\n    assert ax1 is ax2\n    ax3 = plt.subplot(111, projection='rectilinear')\n    assert ax3 is not ax1\n    assert ax3 is not ax2\n\n\n", "def test_autoscale_view_limits_with_empty_data():\n    fig, ax = plt.subplots()\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    # Ensure autoscale_view_limits is called with empty data\n    ax.autoscale_view()\n    assert ax.get_xlim() == (0, 1)\n    assert ax.get_ylim() == (0, 1)\n\n", "def test_subplots_share_x_and_y_limits():\n    fig, axes = plt.subplots(2, 2)\n    axes[0, 0].set_xlim(0, 1)\n    axes[0, 0].set_ylim(0, 1)\n    \n    # Check that other axes share the same limits\n    assert axes[0, 1].get_xlim() == (0, 1)\n    assert axes[0, 1].get_ylim() == (0, 1)\n    assert axes[1, 0].get_xlim() == (0, 1)\n    assert axes[1, 0].get_ylim() == (0, 1)\n    assert axes[1, 1].get_xlim() == (0, 1)\n    assert axes[1, 1].get_ylim() == (0, 1) \n", "def test_subplots_with_sharex_sharey():\n    fig, axes = plt.subplots(2, 2, sharex=True, sharey=True)\n    assert axes[0, 0].get_shared_x_axes().issubset(axes) and axes[0, 1].get_shared_x_axes().issubset(axes) and axes[1, 0].get_shared_x_axes().issubset(axes) and axes[1, 1].get_shared_x_axes().issubset(axes)\n    assert axes[0, 0].get_shared_y_axes().issubset(axes) and axes[0, 1].get_shared_y_axes().issubset(axes) and axes[1, 0].get_shared_y_axes().issubset(axes) and axes[1, 1].get_shared_y_axes().issubset(axes)\n", "def test_stacked_subplot_reuse():\n    fig = plt.figure()\n    ax1 = plt.subplot(2, 2, 1)\n    ax2 = plt.subplot(2, 2, 2)\n    ax3 = plt.subplot(2, 2, 3)\n    ax4 = plt.subplot(2, 2, 4)\n\n    plt.close(fig)\n    fig = plt.figure()\n    ax1 = plt.subplot(2, 2, 1)\n    ax2 = plt.subplot(2, 2, 2)\n    ax3 = plt.subplot(2, 2, 3)\n    ax4 = plt.subplot(2, 2, 4)\n\n    assert ax1 is plt.subplot(2, 2, 1)\n    assert ax2 is plt.subplot(2, 2, 2)\n    assert ax3 is plt.subplot(2, 2, 3)\n    assert ax4 is plt.subplot(2, 2, 4)\n\n\n"], "sample_425": ["    def test_serialize_custom_serializer(self):\n        class MyCustomSerializer(BaseSerializer):\n                return f'MyCustom({obj})', {}\n\n        field = models.CharField(default=42, serialize=MyCustomSerializer())\n        string, imports = MigrationWriter.serialize(field)\n        self.assertEqual(string, 'models.CharField(default=MyCustom(42))')\n        self.assertEqual(imports, {'from migrations.test_writer import MyCustomSerializer'})\n", "    def test_register_serializer_on_instance(self):\n        class CustomModel(models.Model):\n            pass\n\n        class CustomSerializer(BaseSerializer):\n                return \"custom(%r)\" % self.value, {}\n\n        MigrationWriter.register_serializer(CustomModelSerializer, CustomSerializer)\n        serialized = MigrationWriter.serialize(CustomModel())\n\n        self.assertEqual(serialized, (\"custom(None)\", {}))\n", "    def test_serialize_custom_object(self):\n        class CustomObject:\n                self.value = value\n\n            return CustomObject(value)\n\n        string = MigrationWriter.serialize(get_custom_obj(42))\n        self.assertEqual(string, \"CustomObject(42)\")\n", "    def test_serialize_custom_serializer(self):\n        class CustomField(models.Field):\n            pass\n\n        class CustomSerializer(BaseSerializer):\n                return \"CustomField(%r)\" % obj, {}\n\n        CustomField.register_serializer(CustomSerializer)\n        with self.assertRaises(ValueError) as context:\n            MigrationWriter.serialize(CustomField())\n        self.assertIn(\n            \"CustomField has no default serializer\", str(context.exception)\n        )\n", "    def test_serialize_deconstructible_instances(self):\n        class MyDeconstructible(deconstructible):\n                self.value = value\n\n            @classmethod\n                return cls.__name__, ((\"value\", self.value),)\n\n        string, imports = MigrationWriter.serialize(MyDeconstructible(123))\n        self.assertEqual(\n            string, \"migrations.test_writer.MyDeconstructible(value=123)\"\n        )\n        self.assertEqual(imports, {\"from migrations.test_writer import MyDeconstructible\"})\n\n\n\n"], "sample_1169": ["def test_issue_19661_mixed_indices():\n    a = Symbol('0')\n    b = Symbol('1')\n    assert latex(Commutator(Bd(a)**2, B(b))) == '- \\\\left[b_{0}^{2},b^{\\\\dagger_{1}}\\\\right]'\n", "def test_issue_20540():\n    ii, jj = symbols('i j', below_fermi=True)\n    aa, bb = symbols('a b', above_fermi=True)\n    k, l = symbols('k l', below_fermi=True, cls=Dummy)\n    c, d = symbols('c d', above_fermi=True, cls=Dummy)\n    \n    # test for potential issues with dummy ordering in terms with multiple lines\n    expr = atv(k, l, c, d)*att(aa, c, ii, k)*att(d, bb, jj, l)*att(bb, c, ii, l)\n    assert substitute_dummies(expr) == substitute_dummies(expr) \n\n\n\n", "def test_commutator_with_dummy_order_ambiguous():\n    a, b = symbols('a b', cls=Dummy)\n    c, d = symbols('c d', cls=Dummy)\n    expr = Commutator( Bd(a) * B(b), Bd(c) * B(d) )\n    assert substitute_dummies(expr) ==  Commutator( Bd(a) * B(b), Bd(d) * B(c) )\n    assert substitute_dummies(expr) ==  Commutator( Bd(c) * B(d), Bd(a) * B(b) ) \n    assert substitute_dummies(expr) ==  Commutator( Bd(a) * B(b), Bd(c) * B(d) )  \n\n\n\n", "def test_issue_20007_internal_exchange():\n    i, j, k, l = symbols('i j k l', below_fermi=True)\n    a, b, c, d = symbols('a b c d', above_fermi=True)\n\n    expr = atv(i, j, c, d)*att(a, b, i, j)\n    assert latex(expr) == r'v^{ij}_{cd} t^{ab}_{ij}'\n\n    expr = atv(i, j, c, d)*att(b, a, i, j)\n    assert latex(expr) == r'v^{ij}_{cd} t^{ba}_{ij}'\n", "def test_commutator_with_dummy_indices():\n    i, j = symbols('i j', cls=Dummy)\n    a = Symbol('a')\n    b = Symbol('b')\n    expr = Commutator(a*B(i), B(j)*a)\n    assert latex(expr) == ' \\\\left[ a b^{\\\\dagger}_{i}, b_{j} a \\\\right]'\n"], "sample_1181": ["def test_numpy_array_indexing():\n    if not np:\n        skip(\"NumPy not installed\")\n    expr = Array((1, 2, 3))[0]\n    assert NumPyPrinter().doprint(expr) == 'numpy.array((1, 2, 3))[0]'\n    expr = Array((1, 2, 3))[1:]\n    assert NumPyPrinter().doprint(expr) == 'numpy.array((1, 2, 3))[1:]'\n    expr = Array((1, 2, 3))[::2]\n    assert NumPyPrinter().doprint(expr) == 'numpy.array((1, 2, 3))[::2]'\n\n", "def test_numpy_array_diagonal():\n    if not np:\n        skip(\"NumPy not installed\")\n\n    a = Array(((1, 2), (3, 4)))\n    f = lambdify(a, a.diagonal(1, 0), 'numpy')\n    assert np.array_equal(f(a), np.array([1, 4]))\n\n    b = Array(((1, 2, 3), (4, 5, 6)))\n    f = lambdify(b, b.diagonal(1, 0), 'numpy')\n    assert np.array_equal(f(b), np.array([2, 5]))\n\n    c = Array(((1, 2), (3, 4), (5, 6)))\n    raises(ValueError, lambda: lambdify(c, c.diagonal(1, 0), 'numpy'))\n\n", "def test_scipy_array_to_numpy():\n    if not np:\n        skip(\"NumPy not installed\")\n\n    from sympy.tensor.array import Array\n    from sympy.core import Symbol\n    a = Symbol('a')\n    arr = Array([[a, a**2], [a**3, a**4]])\n    f = lambdify(a, arr, 'numpy')\n\n    assert np.array_equal(f(2), np.array([[2, 4], [8, 16]]))\n", "def test_numpy_array_contraction():\n    if not np:\n        skip(\"NumPy not installed\")\n    a = Array((1, 2), [1, 2, 3, 4])\n    b = Array((2, 1), [5, 6])\n    exp_result = Array((1, 1), [1*5 + 2*6])\n    assert np.array_equal(NumPyPrinter().doprint(a.contract(b)).eval(),\n                         exp_result.eval())\n\n    a = Array((1, 2), [1, 2])\n    b = Array((2, 1), [3, 4])\n    exp_result = Array((1, 1), [1*3 + 2*4])\n    assert np.array_equal(NumPyPrinter().doprint(a.contract(b)).eval(),\n                         exp_result.eval()) \n\n    a = Array((2, 3), [[1, 2, 3], [4, 5, 6]])\n    b = Array((3, 1), [[7], [8], [9]])\n    exp_result = Array((2, 1), [[1*7 + 2*8 + 3*9], [4*7 + 5*8 + 6*9]])\n    assert np.array_equal(NumPyPrinter().doprint(a.contract(b)).eval(),\n                         exp_result.eval())  \n", "def test_scipy_array_diagonal():\n    if not np:\n        skip(\"NumPy not installed\")\n\n    from sympy.tensor.array import Array\n\n    a = Array(((1, 2), (3, 4)))\n    diagonals = [(0, 1), (1, 0)]\n    f = lambdify((a, diagonals), a.diagonal(diagonals), 'numpy')\n\n    assert np.array_equal(f(a, diagonals), np.array([5, 3]))\n\n    diagonals = [(1, 1)]\n    f = lambdify((a, diagonals), a.diagonal(diagonals), 'numpy')\n    assert np.array_equal(f(a, diagonals), np.array([3]))\n\n\n\n"], "sample_62": ["    def test_registration_with_custom_adminsite(self):\n        custom_admin_site = CustomSite()\n        custom_admin_site.register(Person, NameAdmin)\n        self.assertIsInstance(custom_admin_site._registry[Person], NameAdmin)\n\n        self.assertFalse(self.default_site.is_registered(Person))\n", "    def test_registration_with_custom_admin_site(self):\n        custom_site = CustomSite()\n        custom_site.register(Person, NameAdmin)\n        self.assertIsInstance(custom_site._registry[Person], NameAdmin)\n        self.assertNotEqual(custom_site._registry[Person], self.default_site._registry.get(Person))\n", "    def test_custom_site_admin_url_patterns(self):\n        class CustomSite(AdminSite):\n            site_url = '/account/'\n        custom_site = CustomSite()\n\n        custom_site.register(Person)(NameAdmin)\n        from django.urls import path\n        urls = custom_site.get_urls()\n\n        # Assert that the site URL is '/account/'\n        for url in urls:\n            if url.startswith('/account/'):\n                return  # Found the expected pattern\n        self.fail(\"Custom site URL pattern not found.\")\n\n\n", "    def test_registration_with_custom_admin_site(self):\n        custom_site = CustomSite()\n        custom_site.register(Person, NameAdmin)\n        self.assertIsInstance(custom_site._registry[Person], NameAdmin)\n\n        self.assertNotIsInstance(site._registry[Person], NameAdmin)\n", "    def test_empty_value_display(self):\n        self.assertEqual(site._empty_value_display, '-')\n        site._empty_value_display = 'N/A'\n        self.assertEqual(site._empty_value_display, 'N/A')\n\n        self.site = CustomSite()\n        # Custom site should have its own empty value display\n        self.assertNotEqual(site._empty_value_display, self.site._empty_value_display) \n\n"], "sample_879": ["def test_ordinal_encoder_categories_dtype_object():\n    \"\"\"Ensure categories are treated as objects.\"\"\"\n    X = np.array([[\"a\", \"b\"], [\"b\", \"a\"]], dtype=object)\n    oe = OrdinalEncoder(categories=[np.array([\"a\", \"b\"])])\n    oe.fit(X)\n\n    assert_array_equal(oe.categories_[0], np.array([\"a\", \"b\"]))\n    assert oe.categories_[0].dtype == object\n\n\n\n", "def test_ordinal_encoder_with_categories_and_missing_values():\n    \"\"\"Check OrdinalEncoder behavior when categories and missing values are specified.\"\"\"\n    X = np.array([[\"a\", np.nan], [\"b\", 1], [\"c\", np.nan]], dtype=object)\n\n    oe = OrdinalEncoder(\n        categories=[[\"a\", \"b\", \"c\"], [0, 1]],\n        encoded_missing_value=-1,\n        handle_unknown=\"use_encoded_value\",\n        unknown_value=-2,\n    ).fit(X)\n\n    X_trans = oe.transform(X)\n    assert_allclose(X_trans, [[0, -1], [1, 0], [2, -1]])\n\n    # Inverse transform should map back to original values\n    X_roundtrip = oe.inverse_transform(X_trans)\n    assert_array_equal(X_roundtrip, X)\n\n\n", "def test_ord_encoder_with_unknown_value(\n    X_train, X_test, handle_unknown, encoded_missing_value, expected_X_trans", "def test_ordinal_encoder_invalid_categories_input():\n    \"\"\"Check that OrdinalEncoder raises ValueError for invalid categories input.\"\"\"\n    with pytest.raises(ValueError, match=r\"categories must be a list of lists\"):\n        OrdinalEncoder(categories=1).fit(np.arange(10).reshape(-1, 1))\n\n    with pytest.raises(ValueError, match=r\"All categories should have the same length\"):\n        OrdinalEncoder(categories=[['a', 'b'], ['c']]).fit(np.arange(10).reshape(-1, 1))\n\n\n", "def test_ordinal_encoder_categorical_dtype():\n    \"\"\"Check OrdinalEncoder handles categorical dtypes.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    X = pd.Categorical([\"a\", \"b\", \"c\"])\n    oe = OrdinalEncoder()\n    oe.fit(X)\n    X_trans = oe.transform(X)\n    assert X_trans.dtype == np.int32\n    assert_allclose(X_trans, [[0], [1], [2]])\n    oe_fit_transform = OrdinalEncoder().fit_transform(X)\n    assert oe_fit_transform.dtype == np.int32\n\n    assert_allclose(oe_fit_transform, X_trans)\n\n\n\n"], "sample_1170": ["compilation error", "def test_issue_13599():\n    from sympy.combinatorics import Permutation\n    p = Permutation('1 2 3')\n    assert str(p) == '1 2 3'\n", "compilation error", "def test_MatrixElement_printing():\n    # test cases for issue #11821\n    A = MatrixSymbol(\"A\", 1, 3)\n    B = MatrixSymbol(\"B\", 1, 3)\n    C = MatrixSymbol(\"C\", 1, 3)\n\n    assert(str(A[0, 0]) == \"A[0, 0]\")\n    assert(str(3 * A[0, 0]) == \"3*A[0, 0]\")\n\n    F = C[0, 0].subs(C, A - B)\n    assert str(F) == \"(A - B)[0, 0]\"\n\n    assert str(A[0, 0] + B[0, 1]) == \"A[0, 0] + B[0, 1]\"\n\n\n\n", "def test_issue_21756():\n    from sympy.concrete.sums import Sum, Sum\n    assert str(Sum(x**2, (x, 0, oo), evaluate=False)) == 'Sum(x**2, x, 0, oo)' \n"], "sample_516": ["def test_text_bbox():\n    fig, ax = plt.subplots()\n    ax.text(0.1, 0.1, 'This text has a bbox', bbox=dict(facecolor='blue'))\n    ax.set_axis_off()\n", "def test_text_path_correction():\n    fig, ax = plt.subplots()\n    ax.text(0.5, 0.5, r\"$\\int_0^\\infty e^{-x^2} dx$\",\n            ha='center', va='center',\n            fontsize=20)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    fig.savefig(io.BytesIO(), format='pdf')\n", "def test_unicode_support():\n    fig, ax = plt.subplots()\n    text = \"\u4f60\u597d\u4e16\u754c\uff01\u8fd9\u662f\u4e00\u4e2a\u6d4b\u8bd5\u6587\u5b57\u7684Unicode\u652f\u6301\u3002\"\n    ax.text(0.5, 0.5, text, ha='center', va='center', fontsize=12)\n", "def test_fontpath_fallback():\n    \"\"\"Test fallback to system fonts when font path is invalid\"\"\"\n    # Create a non-existing font file path\n    invalid_font_path = '/path/to/nonexistent/font.ttf'\n    \n    # Set the font path\n    font_props = FontProperties(fname=invalid_font_path)\n\n    # Create a figure and set the font\n    fig, ax = plt.subplots()\n    ax.text(0.5, 0.5, \"Test\", fontproperties=font_props, ha='center', va='center')\n\n    # Save the figure as PDF\n\n\n", "def test_font_properties():\n    fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n\n    axes[0].text(0.5, 0.5, 'Property 1',\n                 fontsize='large',\n                 fontname='serif',\n                 fontweight='bold',\n                 rotation=45,\n                 horizontalalignment='center',\n                 verticalalignment='center',\n                 transform=axes[0].transAxes)\n\n    axes[1].text(0.5, 0.5, 'Property 2',\n                 fontsize='small',\n                 fontname='sans-serif',\n                 fontstyle='italic',\n                 color='red',\n                 rotation=90,\n                 horizontalalignment='center',\n                 verticalalignment='center',\n                 transform=axes[1].transAxes)\n\n"], "sample_15": ["        def test_jv_invalid_units(self, function):\n            with pytest.raises(u.UnitsError):\n                function(1.0 * u.m, 2.0 * u.s)\n\n\n", "    def test_jv_invalid_units(self, function):\n        with pytest.raises(u.UnitsError):\n            function(1.0 * u.m, 2.0 * u.s)\n\n", "        def test_jv_invalid_units(self, function):\n            with pytest.raises(u.UnitsError):\n                function(1.0 * u.m, 2.0 * u.s)\n\n", "        def test_jv_invalid_units(self, function):\n            with pytest.raises(u.UnitsError):\n                function(\n                    2.0 * u.m / (2.0 * u.m), 3.0 * u.s / (6.0 * u.s)\n                )\n", "        def test_jv_invalid_units(self, function):\n            with pytest.raises(u.UnitsError):\n                function(1.0 * u.m / (2.0 * u.s), 2.0 * u.kg / (3.0 * u.m))\n"], "sample_469": ["    def test_extra_alias(self):\n        qs = (\n            Book.objects.extra(select={\"random_value\": \"42\"})\n            .alias(\n                extra_alias=F(\"random_value\"),\n            )\n            .values(\"title\", \"extra_alias\")\n        )\n        self.assertCountEqual(\n            qs,\n            [\n                {\"title\": \"The Definitive Guide to Django: Web Development Done Right\", \"extra_alias\": 42},\n                {\"title\": \"Practical Django Projects\", \"extra_alias\": 42},\n                {\"title\": \"Artificial Intelligence: A Modern Approach\", \"extra_alias\": 42},\n                {\"title\": \"Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp\", \"extra_alias\": 42},\n            ],\n        )\n", "    def test_nested_alias(self):\n        qs = Book.objects.alias(\n            rating_change=Value(1) - F(\"rating\"),\n        ).alias(\n            final_rating=F(\"rating\") + F(\"rating_change\"),\n        )\n        self.assertIs(hasattr(qs.first(), \"rating_change\"), False)\n        self.assertIs(hasattr(qs.first(), \"final_rating\"), False)\n        for book in qs:\n            with self.subTest(book=book):\n                self.assertEqual(book.final_rating, book.rating + 1)\n\n\n\n", "    def test_alias_with_foreign_key(self):\n        qs = Book.objects.alias(\n            publisher_name=F(\"publisher__name\"),\n        )\n        self.assertIs(hasattr(qs.first(), \"publisher_name\"), False)\n        self.assertQuerySetEqual(\n            qs,\n            [\n                \"Apress\",\n                \"Apress\",\n                \"Apress\",\n                \"Apress\",\n                \"Apress\",\n            ],\n        )\n", "    def test_alias_with_subquery(self):\n        subquery_qs = (\n            Book.objects.annotate(avg_rating=Avg(\"rating\"))\n            .values(\"avg_rating\")\n        )\n        qs = (\n            Book.objects.annotate(\n                subquery_avg_rating=Subquery(subquery_qs)\n            )\n            .filter(subquery_avg_rating__gt=4)\n        )\n        self.assertIs(hasattr(qs.first(), \"subquery_avg_rating\"), False)\n        self.assertCountEqual(qs, [self.b1, self.b2])\n", "    def test_related_names_alias(self):\n        qs = Book.objects.alias(authors_alias=F(\"authors\"))\n        msg = (\n            \"'authors_alias' is not a valid field. Select_related is required when \"\n            \"using 'authors' alias.\"\n        )\n        with self.assertRaisesMessage(AttributeError, msg):\n            qs.values(\"authors_alias\").first()\n"], "sample_784": ["def test_calibration_cross_validation():\n    \"\"\"Test calibration with cross-validation\"\"\"\n    n_samples = 100\n    X, y = make_classification(n_samples=3 * n_samples, n_features=6,\n                               random_state=42)\n    sample_weight = np.random.RandomState(seed=42).uniform(size=y.size)\n\n    X -= X.min()  # MultinomialNB only allows positive X\n\n    # split train and test\n    X_train, y_train, sw_train = \\\n        X[:n_samples], y[:n_samples], sample_weight[:n_samples]\n    X_test, y_test = X[n_samples:], y[n_samples:]\n\n    # Naive Bayes with calibration\n    for method in ['isotonic', 'sigmoid']:\n        clf = MultinomialNB()\n        cal_clf = CalibratedClassifierCV(clf, method=method, cv=3)\n        cal_clf.fit(X_train, y_train, sample_weight=sw_train)\n        prob_pos_calibrated = cal_clf.predict_proba(X_test)[:, 1]\n\n        # Check that brier score has improved after calibration\n        brier_score_uncalibrated = brier_score_loss(y_test,\n                                                  clf.predict_proba(X_test)[:, 1])\n        brier_score_calibrated = brier_score_loss(y_test,\n                                                  prob_pos_calibrated)\n        assert_greater(brier_score_uncalibrated, brier_score_calibrated)\n\n\n\n", "def test_calibration_with_imbalanced_classes():\n    \"\"\"Test calibration with imbalanced classes.\"\"\"\n    from sklearn.datasets import make_classification\n\n    n_samples = 100\n    X, y = make_classification(n_samples=n_samples,\n                               n_features=2,\n                               n_classes=2,\n                               weights=[0.1, 0.9],\n                               random_state=42)\n    X_train, y_train = X[:n_samples // 2], y[:n_samples // 2]\n    X_test, y_test = X[n_samples // 2:], y[n_samples // 2:]\n\n    clf = RandomForestClassifier(n_estimators=10, random_state=42)\n    clf.fit(X_train, y_train)\n\n    for method in ['isotonic', 'sigmoid']:\n        cal_clf = CalibratedClassifierCV(clf, method=method, cv=3)\n        cal_clf.fit(X_train, y_train)\n        cal_probs = cal_clf.predict_proba(X_test)[:, 1]\n\n        # Check if calibration improves brier score loss\n        uncal_loss = brier_score_loss(y_test, clf.predict_proba(X_test)[:, 1])\n        cal_loss = brier_score_loss(y_test, cal_probs)\n        assert_greater(uncal_loss, cal_loss)\n\n\n\n", "    def test_calibration_extreme_inputs():\n        X = np.array([-100, 100])\n        y = np.array([0, 1])\n        clf = LinearSVC()\n        clf.fit(X, y)\n        clf_prob = CalibratedClassifierCV(clf, method=\"sigmoid\", cv=2)\n\n        # Extremely small positive probability\n        prob_pred = clf_prob.predict_proba(np.array([-1000]))[0][1]\n        assert_almost_equal(prob_pred, 0.0)\n\n        # Extremely large positive probability\n        prob_pred = clf_prob.predict_proba(np.array([1000]))[0][1]\n        assert_almost_equal(prob_pred, 1.0)\n\n        # Extremely small negative probability\n        prob_pred = clf_prob.predict_proba(np.array([-1000]))[0][0]\n        assert_almost_equal(prob_pred, 0.0)\n\n        # Extremely large negative probability\n        prob_pred = clf_prob.predict_proba(np.array([1000]))[0][0]\n        assert_almost_equal(prob_pred, 0.0)\n", "def test_calibration_dataset_with_missing_values():\n    \"\"\"Test calibration with a dataset containing missing values.\"\"\"\n    # Create a dataset with missing values\n    X, y = make_classification(n_samples=100, n_features=5,\n                               n_missing=20, random_state=42)\n\n    # Apply a classifier with missing value imputation\n    pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy='mean')),\n        ('clf', CalibratedClassifierCV(LinearSVC(random_state=42),\n                                       method='isotonic', cv=2))\n    ])\n    pipeline.fit(X, y)\n\n    # Check that calibration works correctly even with missing values\n    y_pred = pipeline.predict_proba(X)\n    log_loss_uncalibrated = log_loss(y, pipeline.named_steps['clf'].predict(X))\n    log_loss_calibrated = log_loss(y, y_pred)\n    assert_greater(log_loss_uncalibrated, log_loss_calibrated)\n\n\n", "def test_calibration_multiclass_log_loss(method):\n    \"\"\"Test that calibration decreases log-loss for multiclass\n\n    \"\"\"\n    n_classes = 3\n    X, y = make_classification(n_samples=100, n_features=2,\n                               n_classes=n_classes, random_state=42)\n    X_train, y_train = X[::2], y[::2]\n    X_test, y_test = X[1::2], y[1::2]\n\n    clf = RandomForestClassifier(n_estimators=10, random_state=42)\n    clf.fit(X_train, y_train)\n    clf_probs = clf.predict_proba(X_test)\n    loss = log_loss(y_test, clf_probs)\n\n    calibrated_clf = CalibratedClassifierCV(clf, method=method, cv=3)\n    calibrated_clf.fit(X_train, y_train)\n    calibrated_clf_probs = calibrated_clf.predict_proba(X_test)\n    calibrated_loss = log_loss(y_test, calibrated_clf_probs)\n    assert_greater(loss, calibrated_loss) \n"], "sample_1158": ["compilation error", "def test_issue_18196():\n    a = Symbol('a')\n    b = Symbol('b')\n\n    assert sympify('a + b') == Add(a, b)\n    assert sympify('a +  b') == Add(a, b)\n    assert sympify('a +   b') == Add(a, b)\n\n    assert sympify('a + b + c') == Add(a, b, c)\n    assert sympify('a +  b + c') == Add(a, b, c)\n    assert sympify('a +   b + c') == Add(a, b, c)\n    assert sympify(' a + b + c') == Add(a, b, c)\n\n\n", "compilation error", "def test_issue_18313():\n    a = sympify(5)\n    b = sympify(x)\n    assert a + b == a + b\n    assert a.subs(x, 2) == a + 2\n    assert (a + b).subs(x, 2) == a + 2\n", "def test_issue_18609():\n    a = sympify(\"2**3\")\n    assert isinstance(a, Pow)\n    assert a.base == 2\n    assert a.exp == 3\n    a = sympify(\"2**3.14\")\n    assert isinstance(a, Pow)\n    assert a.base == 2\n    assert isinstance(a.exp, Float)\n    assert a.exp.evalf() == 3.14\n"], "sample_371": ["    def test_sensitive_post_parameters_method_decorator(self):\n        class MyClass:\n            @method_decorator(sensitive_post_parameters())\n                return HttpResponse()\n\n        instance = MyClass()\n        response = instance.a_view(HttpRequest())\n        self.assertEqual(response.status_code, 200)\n", "    def test_sensitive_post_parameters_class_method(self):\n        class MyClass:\n            @method_decorator(sensitive_post_parameters())\n                return HttpResponse()\n\n        instance = MyClass()\n        response = instance.a_view(self.rf.get('/'))\n        self.assertEqual(response.status_code, 200)\n\n", "    def test_sensitive_variables_method_decorator(self):\n        class MyClass:\n            @method_decorator(sensitive_variables)\n                return HttpResponse()\n\n        instance = MyClass()\n        response = instance.a_view(self.rf.get('/'))\n        self.assertEqual(response.status_code, 200)\n", "    def test_sensitive_variables_with_nested_lists(self):\n        @sensitive_variables\n            return {'nested_list': [[{'password': 'secret'}, 'other_data'], None]}\n\n        response = self.client.post('/test/', data=test_func({'data': ''}))\n        self.assertContains(response, 'XXXXXXXXXXXXXXXXXXXX', status_code=500)  \n        self.assertNotContains(response, 'secret', status_code=500)  \n", "    def test_sensitive_post_parameters_method_decorator(self):\n        class MyClass:\n            @method_decorator(sensitive_post_parameters())\n                return HttpResponse()\n\n        instance = MyClass()\n        response = instance.a_view(HttpRequest())\n        self.assertIsInstance(response, HttpResponse)\n"], "sample_576": ["    def test_legend_handles_large_datasets(self, xy, long_df):\n\n        df = long_df[[\"x\", \"y\", \"a\", \"b\"]].sample(1000, random_state=42)\n        color = df[\"a\"]\n        p = Plot(df).add(MockMark(), color=color).plot()\n        legend = p._figure.legends[0]\n        assert len(legend.legendHandles) > 10\n        assert len(legend.get_texts()) > 10\n", "    def test_legend_handles_missing_values(self, long_df):\n\n        df = long_df.copy()\n        df[\"y\"] = df[\"y\"].fillna(pd.NA)\n        p = Plot(df, y=[\"y\"]).pair(x=[\"x\"]).plot()\n        legend, = p._figure.legends\n        assert legend.get_texts() != []\n        for text in legend.get_texts():\n            assert text.get_text() in [\"y\", \"NA\"]\n", "    def test_legend_order_custom(self, xy):\n        s = pd.Series([\"a\", \"b\", \"a\", \"c\"], name=\"s\")\n        p = Plot(**xy).add(MockMark(), color=s).order(color=[\"c\", \"a\", \"b\"]).plot()\n        e, = p._legend_contents\n        labels = [\"c\", \"a\", \"b\"]\n        assert e[-1] == labels\n", "    def test_legend_handles_explicit_title(self, xy):\n\n        p = Plot(**xy, color=[\"a\", \"b\", \"c\", \"d\"]).add(MockMark()).plot(legend_title=\"My Legend\")\n        legend = p._figure.legends[0]\n        assert legend.get_title().get_text() == \"My Legend\"\n\n\n\n", "    def test_legend_handles_missing_values(self, long_df):\n\n        p = Plot(long_df, y=\"y\").pair(x=[\"x\", \"z\"], color=\"a\").plot()\n        e, = p._legend_contents\n\n        labels = categorical_order(pd.Series(long_df[\"a\"]))\n\n        assert e[0] == (\"a\", \"a\")\n        assert e[-1] == labels\n\n        artists = e[1]\n        for a, label in zip(artists, labels):\n            assert isinstance(a, mpl.artist.Artist)\n            assert a.value == label\n            assert a.variables == [\"color\"]\n\n"], "sample_83": ["    def test_tag_compile_function(self):\n        @self.library.tag('name', compile_function=lambda parser, token: Node())\n            return Node()\n        self.assertEqual(self.library.tags['name'], lambda parser, token: Node())\n", "    def test_tag_compile_function(self):\n            return Node()\n        self.library.tag('name', compile_function)\n        self.assertEqual(self.library.tags['name'], compile_function)\n", "    def test_tag_compile_function(self):\n            return Node()\n        self.library.tag('name', func)\n        self.assertEqual(self.library.tags['name'], func)\n", "    def test_tag_compile_function(self):\n            return Node()\n        self.library.tag('name', func)\n        self.assertIsInstance(self.library.tags['name'], functools.wraps(func))\n", "    def test_tag_compile_function(self):\n            return Node()\n        self.library.tag('name', compile_func)\n        self.assertEqual(self.library.tags['name'], compile_func)\n\n"], "sample_640": ["def test_is_overload_stub() -> None:\n    code = astroid.extract_node(\n        \"\"\"\n    from typing import List\n    \n        return []\n    \"\"\"\n    )\n    assert not utils.is_overload_stub(code[0])\n\n    code = astroid.extract_node(\n        \"\"\"\n    from typing import List\n    \n    @overload\n        ...\n    \"\"\"\n    )\n    assert utils.is_overload_stub(code[0])\n\n\n\n", "def test_is_overload_stub() -> None:\n    code = astroid.extract_node(\n        \"\"\"\n    from typing import overload\n    @overload\n        pass\n        pass\n    \"\"\"\n    )\n    assert utils.is_overload_stub(code.nodes[0])\n    assert not utils.is_overload_stub(code.nodes[1])\n\n    code = astroid.extract_node(\n        \"\"\"\n        pass\n    \"\"\"\n    )\n    assert not utils.is_overload_stub(code.nodes[0])\n\n\n", "def test_is_overload_stub() -> None:\n    code = astroid.extract_node(\"\"\"\n    from typing import overload  #@\n    @overload\n        pass  \n        pass\n    \"\"\")\n    assert utils.is_overload_stub(code[1])\n    assert not utils.is_overload_stub(code[2])\n\n\n\n", "def test_check_method_name() -> None:\n    assert utils.check_method_name(\"staticmethod\") is None\n    assert utils.check_method_name(\"classmethod\") is None\n    assert utils.check_method_name(\"my_method\") == \"my_method\"\n    assert utils.check_method_name(\"_my_method\") == \"_my_method\"\n", "def test_overloading_stub() -> None:\n    code = astroid.extract_node(\n        \"\"\"\n    from typing import overload\n    @overload\n    @overload\n        return str(x)\n    \"\"\"\n    )\n    assert utils.is_overload_stub(code[0])\n    assert utils.is_overload_stub(code[1])\n    assert not utils.is_overload_stub(code[2]) \n"], "sample_801": ["def test_big_nested_pipeline():\n    # Test a very deep nested pipeline\n    from sklearn.preprocessing import StandardScaler, OneHotEncoder\n    from sklearn.impute import SimpleImputer\n    from sklearn.compose import ColumnTransformer\n    from sklearn.pipeline import Pipeline\n\n    steps = [\n        (\n            'preprocessing',\n            Pipeline(\n                [\n                    ('imputer', SimpleImputer(strategy='mean')),\n                    ('scaler', StandardScaler()),\n                    ('ohe', OneHotEncoder(handle_unknown='ignore'))\n                ]\n            )\n        ),\n        ('classifier', LogisticRegression(random_state=0))\n    ]\n\n    pipeline = Pipeline(steps)\n    expected = \"\"\"", "def test_custom_indent():\n    pp = _EstimatorPrettyPrinter(compact=True, indent=5, indent_at_name=True)\n    lr = LogisticRegression()\n    expected = \"\"\"\n                    LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                      fit_intercept=True,\n                                      intercept_scaling=1, l1_ratio=None,\n                                      max_iter=100,\n                                      multi_class='warn', n_jobs=None,\n                                      penalty='l2', random_state=None,\n                                      solver='warn', tol=0.0001, verbose=0,\n                                      warm_start=False)\"\"\"\n    expected = expected[1:]  # remove first \\n\n    assert pp.pformat(lr) == expected\n\n\n\n", "def test_empty_estimator():\n    empty_estimator = type(\"EmptyEstimator\", (BaseEstimator,), {})()\n    expected = \"EmptyEstimator()\"\n    assert expected == empty_estimator.__repr__()\n", "def test_complex_estimator():\n    class ComplexEstimator(BaseEstimator):\n            self.param1 = param1\n            self.param2 = param2\n            self.param3 = param3\n\n            return self\n\n            return X * 2\n\n    estimator = ComplexEstimator(param1=\"value1\", param2=10, param3=True)\n    expected = \"\"\"", "def test_n_max_elements_to_show_with_dicts():\n    n_max_elements_to_show = 30\n    pp = _EstimatorPrettyPrinter(\n        compact=True, indent=1, indent_at_name=True,\n        n_max_elements_to_show=n_max_elements_to_show\n    )\n\n    # Test with dictionaries\n    params = {'param1': [1, 2, 3, 4, 5], 'param2': {'a': 1, 'b': 2, 'c': 3}}\n    estimator = BaseEstimator(params=params)\n    expected = r\"\"\""], "sample_20": ["def test_mask_serialization_with_invalid_data(tmp_path, mask_type):\n    filename = tmp_path / \"test.fits\"\n\n    t = simple_table(masked=True)\n    # Set a mask with invalid data type\n    if mask_type == \"bool\":\n        t[\"a\"].mask = [True, False, True]\n    elif mask_type == \"ndarray\":\n        t[\"a\"].mask = np.array([True, False, True], dtype=float)\n    else:\n        raise ValueError(f\"Invalid mask_type: {mask_type}\")\n\n    t.write(filename, overwrite=True)\n    t2 = Table.read(filename)\n\n    assert t2[\"a\"].mask.dtype == bool\n    assert np.all(t2[\"a\"].mask == t[\"a\"].mask)\n\n\n\n", "def test_fits_write_with_missing_column_names(tmp_path):\n    filename = tmp_path / \"test_missing_names.fits\"\n    t = Table([np.arange(3), np.arange(3, 6)])\n    t.write(filename, format=\"fits\", overwrite=True, names=[\"col1\", \"col2\"])\n\n    with fits.open(filename) as hdul:\n        assert hdul[1].columns.names == [\"col1\", \"col2\"]\n", "def test_masked_col_metadata(tmp_path):\n    filename = tmp_path / \"test.fits\"\n    t = simple_table(masked=True)\n    t[\"a\"].info.description = \"hello\"\n    t[\"a\"].info.meta[\"test\"] = \"value\"\n    t.write(filename)\n\n    t2 = Table.read(filename)\n    assert t2[\"a\"].description == \"hello\"\n    assert t2[\"a\"].info.meta[\"test\"] == \"value\"\n\n\n\n", "def test_masked_keywords(tmp_path):\n    filename = tmp_path / \"test.fits\"\n    t = simple_table(masked=True)\n    t.write(filename, serialize_method=\"data_mask\")\n    t2 = Table.read(filename)\n    assert t.masked is False\n    assert t2.masked is False\n    assert np.all(t[\"a\"].mask == t2[\"a\"].mask)\n\n\n\n    \n", "def test_read_with_bad_extension(tmp_path):\n    filename = tmp_path / \"test.fits\"\n    with pytest.raises(RuntimeError) as exc:\n        Table.read(\n            filename,\n            hdu=1,\n            extname=\"nonexistent\"\n        )\n    assert \"Could not find extension 'nonexistent'\" in str(exc.value)\n\n\n"], "sample_1105": ["def test_matmul_inverse():\n    assert MatMul(A, Inverse(A)).doit() == Identity(n)\n    assert MatMul(Inverse(A), A).doit() == Identity(n)\n    assert MatMul(A, Inverse(A**2)) == MatMul(A, MatPow(A, -2)).doit() == Identity(n)\n    assert MatMul(Inverse(A), A**2) == MatMul(MatPow(A, -1), A**2).doit() == Identity(n)\n\n", "def test_matmul_inverse():\n    assert MatMul(A, Inverse(A)).doit() == Identity(n)\n    assert MatMul(Inverse(A), A).doit() == Identity(n)\n    assert MatMul(A, Inverse(A), B).doit() == MatMul(A, B)\n    assert MatMul(A, Inverse(A), B, evaluate=False).doit() == MatMul(A, B, evaluate=False)\n    assert MatMul(Inverse(A), A, B, evaluate=False).doit() == MatMul(B, evaluate=False)\n", "def test_matmul_zero_matrix_scalar():\n    assert MatMul(0, C) == ZeroMatrix(C.shape[0], C.shape[1])\n    assert MatMul(C, 0) == ZeroMatrix(C.shape[0], C.shape[1])\n", "def test_matmul_with_arbitrary_shapes():\n    A = MatrixSymbol('A', 2, 3)\n    B = MatrixSymbol('B', 3, 4)\n    C = MatrixSymbol('C', 4, 5)\n\n    assert MatMul(A, B, C).shape == (2, 5)\n", "def test_matmul_inverse():\n    assert MatMul(A, Inverse(A)) == Identity(n)\n    assert MatMul(Inverse(A), A) == Identity(n)\n    assert MatMul(2*A, Inverse(A)) == 2*Identity(n)\n    assert MatMul(Inverse(2*A), 2*A) == Identity(n)\n"], "sample_959": ["def test_domain_cpp_parse_implicit_ref_object():\n    output = (app.builder.build_all())\n    text = (\".. cpp:class:: MyClass\\n\"\n            \"    :members:\\n\"\n            \"        .. cpp:member:: member_var\"\n            \"\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc, addnodes.index, desc))\n    assert_node(doctree[0], addnodes.index, entries=[('single', 'MyClass (C++ class)', '_CPPv467MyClass', '', None)])\n    assert_node(doctree[2], addnodes.index, entries=[('single', 'member_var (C++ member)', '_CPPv467MyClass8member_var', '', None)])\n", "def test_domain_cpp_parse_index_entry_noindexentry(app):\n    text = (\".. cpp:function:: void f()\\n\"\n            \"   :noindexentry:\\n\"\n            \".. cpp:function:: void g()\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc, addnodes.index, desc))\n    assert_node(doctree[0], addnodes.index, entries=[('single', 'f (C++ function)', '_CPPv41fv', '', None)])\n    assert_node(doctree[2], addnodes.index, entries=[('single', 'g (C++ function)', '_CPPv41gv', '', None)])\n", "compilation error", "compilation error", "compilation error"], "sample_590": ["    def test_concat_multi_index_coords(self):\n        x = pd.MultiIndex.from_product([[1, 2, 3], [\"a\", \"b\"]])\n        y = pd.MultiIndex.from_product([[4, 5], [\"c\", \"d\"]])\n        data = np.random.random((len(x), len(y)))\n        arr = DataArray(data, dims=[\"x\", \"y\"], coords={\"x\": x, \"y\": y})\n\n        expected = DataArray(\n            np.array([data[i] for i in range(len(x))]),\n            dims=[\"w\", \"x\", \"y\"],\n            coords={\"x\": x, \"y\": y},\n        )\n        actual = concat([arr.isel(x=slice(2)), arr.isel(x=slice(2, None))], \"w\")\n        assert_identical(expected, actual)\n", "    def test_concat_different_coordinatess(self, data):\n        ds1 = Dataset({\"x\": ((\"y\",), np.array([1, 2, 3], dtype=float))}, {\"y\": [1, 2, 3]})\n        ds2 = Dataset({\"x\": ((\"y\",), np.array([4, 5, 6], dtype=float))}, {\"y\": [4, 5, 6]})\n        \n        expected = Dataset({\"x\": ((\"y\",), np.array([1, 2, 3, 4, 5, 6], dtype=float))}, {\"y\": [1, 2, 3, 4, 5, 6]})\n        actual = concat([ds1, ds2], dim=\"y\", coords=\"all\")\n        assert_identical(expected, actual)\n\n\n\n\n        \n", "    def test_concat_broadcast(self):\n        data = create_test_data()\n        # test broadcasting\n        split_data = [\n            data.isel(dim1=slice(0, 3)),\n            data.isel(dim1=slice(3, None)),\n        ]\n        a = split_data[0].expand_dims(dim=\"foo\", axis=0)\n        b = split_data[1].expand_dims(dim=\"foo\", axis=0)\n        expected = data.rename({\"foo\": \"concat_dim\"})\n        actual = concat([a, b], dim=\"concat_dim\")\n        assert_identical(expected, actual)\n\n\n\n\n\n\n\n        # test broadcasting with multiple dimensions\n        split_data = [\n            data.isel(dim1=slice(0, 3)).expand_dims(dim=[\"foo\", \"bar\"], axis=[0, 1]),\n            data.isel(dim1=slice(3, None)).expand_dims(dim=[\"foo\", \"bar\"], axis=[0, 1]),\n        ]\n        a = split_data[0]\n        b = split_data[1]\n        expected = data.rename({\"foo\": \"concat_dim\", \"bar\": \"concat_dim2\"})\n        actual = concat([a, b], dims=[\"concat_dim\", \"concat_dim2\"])\n        assert_identical(expected, actual)\n", "    def test_concat_with_missing_coords(self):\n        ds1 = Dataset({\"a\": ((\"x\", \"y\"), np.random.randn(2, 3)), \"b\": ((\"x\", \"y\"), np.random.randn(2, 3))}, coords={\"x\": [0, 1]})\n        ds2 = Dataset({\"a\": ((\"x\", \"y\"), np.random.randn(2, 3)), \"b\": ((\"x\", \"y\"), np.random.randn(2, 3))}, coords={\"y\": [0, 1]})\n        with raises_regex(ValueError, \"datasets are not aligned\"):\n            concat([ds1, ds2], dim=\"x\")\n\n        with raises_regex(ValueError, \"datasets are not aligned\"):\n            concat([ds1, ds2], dim=\"y\")\n", "    def test_concat_different_coords(self):\n        ds1 = Dataset({\"a\": ((\"x\", \"y\"), np.random.random((2, 3))), \"x\": [0, 1]}, coords={\"y\": [\"A\", \"B\"]})\n        ds2 = Dataset({\"a\": ((\"x\", \"y\"), np.random.random((2, 3))), \"x\": [0, 1]}, coords={\"y\": [1, 2]})\n\n        with raises_regex(ValueError, \"coordinate 'y' in some datasets but not others\"):\n            concat([ds1, ds2], dim=\"x\")\n"], "sample_777": ["def test_gradient_boosting_fit_on_empty_data(estimator):\n    X = np.array([])\n    y = np.array([])\n    with pytest.raises(ValueError, match=\"Input data must be of size at least\"):\n        estimator.fit(X, y)\n", "def test_gradient_boosting_with_categorical_features():\n    X, y = datasets.make_classification(n_features=10,\n                                        n_informative=5,\n                                        n_redundant=0,\n                                        random_state=0)\n    # Create some categorical features\n    X['cat'] = np.random.choice(['A', 'B', 'C'], size=len(X))\n\n    # Fit with categorical features\n    gbc = GradientBoostingClassifier(max_depth=3, random_state=0)\n    gbc.fit(X, y)\n    assert_equal(gbc.n_features_, len(X.columns))\n\n\n\n", "    def test_gradient_boosting_init_with_non_estimator():\n        with pytest.raises(TypeError, match=r\"init must be an estimator instance\"):\n            GradientBoostingClassifier(init='non_estimator').fit(X, y)\n", "def test_gradient_boosting_with_presort_as_list(EstimatorClass):\n    X, y = datasets.make_classification(n_samples=100, random_state=0)\n\n    # Test with a list of tuples as presort\n    presort_list = [(0, 1, 2), (1, 0, 2)]\n    estimator = EstimatorClass(presort=presort_list)\n    estimator.fit(X, y)\n\n\n\n", "def test_gradient_boosting_init_unhashable_type():\n    class Custom unfitted estimator:\n            pass\n\n            pass\n\n            pass\n\n    with pytest.raises(TypeError, match=\"init parameter must be hashable\"):\n        GradientBoostingClassifier(init=Custom()).fit(X, y)\n"], "sample_317": ["    def test_get_object_with_custom_path(self):\n        response = self.client.get('/syndication/articles/%s/' % self.e1.pk)\n        doc = minidom.parseString(response.content)\n        feed = doc.getElementsByTagName('rss')[0]\n        chan = feed.getElementsByTagName('channel')[0]\n        items = chan.getElementsByTagName('item')\n\n        self.assertChildNodeContent(items[0], {\n            'comments': '/blog/%s/comments' % self.e1.pk,\n            'description': 'Article description: My first article',\n            'link': 'http://example.com/blog/%s/' % self.e1.pk,\n            'title': 'Title: My first article',\n            'pubDate': rfc2822_date(timezone.make_aware(self.a1.published, TZ)),\n        })\n", "    def test_get_object_with_object_not_found(self):\n        response = self.client.get('/syndication/rss2/articles/999999/')\n        self.assertEqual(response.status_code, 404)\n", "    def test_exclude_item(self):\n        response = self.client.get('/syndication/exclude-item/')\n        doc = minidom.parseString(response.content)\n        chan = doc.getElementsByTagName('channel')[0]\n        items = chan.getElementsByTagName('item')\n        self.assertEqual(len(items), Entry.objects.count() - 1)\n", "    def test_feed_object_creation_custom_fields(self):\n        with self.subTest(custom_fields=True):\n            feed = Rss201rev2Feed(title='Custom Fields Test',\n                                 link='https://example.com',\n                                 description='Test feed with custom fields',\n                                 )\n            doc = feed.writeString('utf-8')\n            self.assertIn(\n                '<title>Custom Fields Test</title>',\n                doc\n            )\n          \n", "    def test_get_object_with_no_article(self):\n        response = self.client.get('/syndication/rss2/articles/9999/')\n        self.assertEqual(response.status_code, 404)\n"], "sample_131": ["    def test_migrate_test_setting_true(self, mocked_migrate, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        saved_settings = copy.deepcopy(connection.settings_dict)\n        try:\n            connection.settings_dict['TEST']['MIGRATE'] = True\n            with mock.patch.object(creation, '_create_test_db'):\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n            mocked_migrate.assert_called_once()\n        finally:\n            connection.settings_dict = saved_settings\n", "    def test_migrate_test_setting_true(self, mocked_migrate, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        saved_settings = copy.deepcopy(connection.settings_dict)\n        try:\n            connection.settings_dict['TEST']['MIGRATE'] = True\n            with mock.patch.object(creation, '_create_test_db'):\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n            mocked_migrate.assert_called_once()\n        finally:\n            connection.settings_dict = saved_settings\n", "    def test_migrate_test_setting_true(self, mocked_migrate, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        saved_settings = copy.deepcopy(connection.settings_dict)\n        try:\n            connection.settings_dict['TEST']['MIGRATE'] = True\n            with mock.patch.object(creation, '_create_test_db'):\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n            mocked_migrate.assert_called_once_with(\n                verbosity=0,\n                interactive=False,\n                database=DEFAULT_DB_ALIAS,\n                run_syncdb=True,\n            )\n        finally:\n            connection.settings_dict = saved_settings\n", "    def test_migrate_test_setting_true(self, mocked_migrate, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        saved_settings = copy.deepcopy(connection.settings_dict)\n        try:\n            connection.settings_dict['TEST']['MIGRATE'] = True\n            with mock.patch.object(creation, '_create_test_db'):\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n            mocked_migrate.assert_called_once_with(\n                verbosity=0,\n                interactive=False,\n                database=DEFAULT_DB_ALIAS,\n                run_syncdb=True,\n            )\n        finally:\n            connection.settings_dict = saved_settings\n\n\n\n", "    def test_migrate_test_setting_true(self, mocked_migrate, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        saved_settings = copy.deepcopy(connection.settings_dict)\n        try:\n            connection.settings_dict['TEST']['MIGRATE'] = True\n            with mock.patch.object(creation, '_create_test_db'):\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n            mocked_migrate.assert_called_once()\n        finally:\n            connection.settings_dict = saved_settings\n"], "sample_514": ["def test_colorbar_auto_offset_text():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.random.randn(10, 10))\n    cb = fig.colorbar(pc, ax=ax, orientation='vertical',\n                      label='My Colorbar')\n\n    # check that the offsetText is not present when explicitly set to None\n    cb.set_offset_text(None)\n    assert cb.ax.yaxis.offsetText is None\n\n    # check that the offsetText is present and in the correct place by default\n    cb.set_offset_text(None) # reset to default\n    assert cb.ax.yaxis.offsetText is not None\n    assert isinstance(cb.ax.yaxis.offsetText, matplotlib.text.Text)\n    # check that the offset text is not in the path of the colorbar\n    ax.set_position([0.05, 0.05, 0.9, 0.9])\n    fig.draw_without_rendering()\n    assert cb.ax.yaxis.offsetText.get_bbox().y0 > cb.ax.get_position().y1\n\n\n\n", "def test_colorbar_text_kwargs():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.random.randn(10, 10))\n    cb = fig.colorbar(pc)\n    cb.ax.tick_params(labelsize=12)\n    cb.ax.yaxis.set_label_coords(0.5, 1)\n    cb.set_label('my label', fontsize=18, color='red')\n    cb.ax.set_xlabel('my xlabel', fontsize=10, color='blue')\n    assert cb.ax.yaxis.get_label_coords()[0] == 0.5\n    assert cb.ax.yaxis.get_label_coords()[1] == 1\n    assert cb.ax.yaxis.get_label_text() == 'my label'\n    assert cb.ax.yaxis.get_label_fontsize() == 18\n    assert cb.ax.yaxis.get_label_color() == 'red'\n    assert cb.ax.get_xlabel() == 'my xlabel'\n    assert cb.ax.get_xlabel_fontsize() == 10\n    assert cb.ax.get_xlabel_color() == 'blue'\n\n\n", "def test_colorbar_fraction_with_pad():\n    fig, ax = plt.subplots()\n    im = ax.imshow([[1, 2], [3, 4]])\n    cb = fig.colorbar(im, ax=ax, fraction=0.2, pad=0.1)\n    fig.canvas.draw()\n    # check that the fraction is roughly correct:\n    bbox = cb.ax.get_position()\n    assert 0.2 - 0.01 < bbox.height < 0.2 + 0.01\n    assert 0.1 < bbox.x0 < 0.1 + 0.01\n\n\n\n", "def test_colorbar_fraction():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.random.randn(10, 10))\n    cb = fig.colorbar(pc, ax=ax, fraction=0.1)\n    assert cb.ax.get_position().height / fig.get_size_inches()[1] == 0.1\n\n\n\n", "def test_colorbar_text_rotation_issue_10839():\n    fig, ax = plt.subplots()\n    im = ax.imshow([[1, 2, 3], [4, 5, 6]])\n    cb = fig.colorbar(im, ax=ax, orientation='horizontal')\n    cb.ax.yaxis.set_tick_params(rotation=90)\n    fig.draw()\n    # Check that the label text is rotated as expected.\n    assert cb.ax.yaxis.get_ticklabels()[0].get_rotation() == 90\n\n\n\n"], "sample_659": ["    def test_raises_in_generator(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n                yield 1\n                raise ValueError(\"test\")\n\n                with pytest.raises(ValueError) as excinfo:\n                    list(my_generator())\n                assert excinfo.type == ValueError\n                assert excinfo.value.args[0] == \"test\"\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines([\"*1 passed*\"])\n", "    def test_raises_with_context_manager_as_argument(self):\n        class MyCustomContext(dummy_context_manager):\n            class Error(Exception):\n                pass\n\n                return\n\n                if exc_type is MyCustomContext.Error:\n                    raise exc_value\n\n        with pytest.raises(MyCustomContext.Error) as excinfo:\n            with MyCustomContext():\n                raise MyCustomContext.Error(\"This is a custom error!\")\n\n        assert excinfo.type is MyCustomContext.Error\n", "    def test_raises_with_explicit_type(self):\n        try:\n            with pytest.raises(ValueError, type=ValueError):\n                raise ValueError()\n        except pytest.raises.Exception as e:\n            assert e.msg == \"DID NOT RAISE {}\".format(repr(ValueError))\n        else:\n            assert False, \"Expected pytest.raises.Exception\"\n\n", "    def test_raises_with_custom_exception_type(self):\n        class MyCustomException(Exception):\n            pass\n\n        with pytest.raises(MyCustomException):\n            raise MyCustomException(\"Custom exception!\")\n\n", "    def test_raises_exception_with_no_traceback(self):\n        with pytest.raises(ValueError):\n            raise ValueError(\"message\") from None\n        with pytest.raises(ValueError) as excinfo:\n            raise ValueError(\"message\") from None\n        assert excinfo.traceback is None\n\n"], "sample_819": ["def test_voting_regressor():\n    \"\"\"Test VotingRegressor with multiple regressors.\"\"\"\n    reg1 = DummyRegressor(strategy='mean')\n    reg2 = DummyRegressor(strategy='median')\n    reg3 = DummyRegressor(strategy='quantile', quantile=0.25)\n    ereg = VotingRegressor(estimators=[('mean', reg1), ('median', reg2),\n                                       ('quantile', reg3)],\n                           weights=[1, 2, 1])\n    X_r_train, X_r_test, y_r_train, y_r_test = train_test_split(\n        X_r, y_r, test_size=0.25, random_state=42)\n    ereg.fit(X_r_train, y_r_train)\n    y_r_pred = ereg.predict(X_r_test)\n    assert_all_close(y_r_pred, np.average([reg1.predict(X_r_test),\n                                         reg2.predict(X_r_test),\n                                         reg3.predict(X_r_test)],\n                                        axis=0, weights=[1, 2, 1]))\n\n", "def test_oob_score_regressor():\n    \"\"\"Checks if oob_score works as expected with RegressionClassifier.\"\"\"\n    reg1 = RandomForestRegressor(random_state=123, n_estimators=100, oob_score=True)\n    reg2 = VotingRegressor([('rf', reg1)], weights=[1])\n\n    X_train, X_test, y_train, y_test = train_test_split(iris.data[:, :2],\n                iris.target, test_size=0.2, random_state=123)\n    reg1.fit(X_train, y_train)\n    reg2.fit(X_train, y_train)\n    assert isinstance(reg1.oob_score_, float)\n    assert isinstance(reg2.oob_score_, float)  \n    assert abs(reg1.oob_score_ - reg2.oob_score_) < 1e-3 \n    \n", "def test_voting_classifier_with_custom_estimator():\n    class CustomEstimator(BaseEstimator, ClassifierMixin):\n            pass\n\n            return np.zeros(len(X))\n\n            return np.array([[0.5, 0.5] for _ in range(len(X))])\n\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = CustomEstimator()\n    eclf = VotingClassifier(estimators=[('lr', clf1), ('cust', clf2)],\n                            voting='soft', weights=[1, 0.5])\n    eclf.fit(X, y)\n    assert_array_equal(eclf.predict(X), clf1.predict(X))\n    assert_array_almost_equal(eclf.predict_proba(X),\n                              clf1.predict_proba(X))\n\n", "def test_copy():\n    clf1 = LogisticRegression(random_state=123, C=1.0)\n    clf2 = RandomForestClassifier(random_state=123, max_depth=None)\n    clf3 = GaussianNB()\n    eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n                            voting='soft', weights=[1, 2, 1])\n    eclf.fit(X, y)\n\n    new_eclf = eclf.copy()\n    assert_not_equal(id(eclf), id(new_eclf))\n    assert_equal(eclf.estimators, new_eclf.estimators)\n    assert_equal(eclf.voting, new_eclf.voting)\n    assert_array_equal(eclf.weights, new_eclf.weights)\n    assert_equal(eclf.classes_, new_eclf.classes_)\n    assert_equal(eclf.n_outputs_, new_eclf.n_outputs_)\n\n    new_eclf.set_params(rf__max_depth=5)\n    assert not hasattr(eclf, 'rf__max_depth')\n\n\n", "compilation error"], "sample_736": ["def test_multiclass_ovr_vs_multinomial():\n    #Test for multiple classes scenarios, comparing one-vs-rest\n    # (ovr) with multinomial output.\n    rng = np.random.RandomState(0)\n    n_samples = 100\n    n_features = 20\n    n_classes = 5\n\n    X = rng.randn(n_samples, n_features)\n    y = np.array([rng.randint(0, n_classes) for _ in range(n_samples)])\n\n    # OVR\n    ovr_clf = LogisticRegression(multi_class='ovr', solver='lbfgs',\n                                  random_state=42)\n    ovr_clf.fit(X, y)\n    ovr_probs = ovr_clf.predict_proba(X)\n\n    # Multinomial\n    multinomial_clf = LogisticRegression(multi_class='multinomial', solver='lbfgs',\n                                         random_state=42)\n    multinomial_clf.fit(X, y)\n    multinomial_probs = multinomial_clf.predict_proba(X)\n\n    # Check that the probabilities are similar for both approaches.\n    assert_array_almost_equal(ovr_probs, multinomial_probs, decimal=4)\n\n    # Test for different solvers.\n    for solver in ['newton-cg', 'saga', 'liblinear']:\n        ovr_clf = LogisticRegression(multi_class='ovr', solver=solver,\n                                      random_state=42)\n        ovr_clf.fit(X, y)\n        ovr_probs = ovr_clf.predict_proba(X)\n\n        multinomial_clf = LogisticRegression(multi_class='multinomial', solver=solver,\n                                             random_state=42)\n        multinomial_clf.fit(X, y)\n        multinomial_probs = multinomial_clf.predict_proba(X)\n\n        assert_array_almost_equal(ovr_probs, multinomial_probs, decimal=4)\n\n    # Test with sparse data\n    X_sparse = sparse.csr_matrix(X)\n    for solver in ['saga', 'liblinear']:\n        ovr_clf = Logistic", "compilation error", " def test_robust_classification_with_outliers():\n     # Test the robustness of LogisticRegression to outliers in the data\n     rng = np.random.RandomState(0)\n     X = np.random.rand(1000, 2)\n     y = (X[:, 0] + X[:, 1] > 0.5).astype(int)  \n     # Introduce outliers\n     outliers_indices = rng.choice(X.shape[0], size=50, replace=False)\n     X[outliers_indices, :] = rng.uniform(0, 1, size=(50, 2))\n     # Fit a model with L1 regularization\n     clf = LogisticRegression(penalty='l1', C=0.1, solver='liblinear')\n     clf.fit(X, y)\n     # Check if the model is robust to outliers by examining the coefficients\n     # The coefficients should be mostly zero due to L1 regularization,\n     # with only a few features having significant weights.\n     assert (np.abs(clf.coef_) < 1e-2).all()\n     \n", "def test_fit_intercept_and_l1_penalty():\n    X, y = make_classification(n_samples=50, n_features=20, random_state=0)\n    # Test that fitting intercept isn't possible with l1 penalty\n    lr_l1 = LogisticRegression(penalty=\"l1\", C=1.0, fit_intercept=True,\n                                solver='liblinear')\n    with assert_raises(ValueError):\n        lr_l1.fit(X, y)\n    # Test that setting fit_intercept=False works with l1 penalty\n    lr_l1_no_intercept = LogisticRegression(penalty=\"l1\", C=1.0,\n                                           fit_intercept=False,\n                                           solver='liblinear')\n    lr_l1_no_intercept.fit(X, y)\n\n\n\n", "def test_multi_class_ovr_scaling():\n    # Test that the class weights are correctly applied to each class\n    # when multi_class='ovr'.\n    X, y = make_classification(n_samples=10, n_features=2, random_state=0,\n                               n_classes=3)\n    y_bin = np.where(y == 2, 0, 1)\n\n    clf_ovr_balanced = LogisticRegression(multi_class='ovr',\n                                          solver='liblinear',\n                                          random_state=0)\n    clf_ovr_balanced.fit(X, y)\n    coef_balanced = clf_ovr_balanced.coef_\n\n    clf_ovr_scaled = LogisticRegression(multi_class='ovr',\n                                         solver='liblinear',\n                                         class_weight={0: 0.5, 1: 2.},\n                                         random_state=0)\n    clf_ovr_scaled.fit(X, y_bin)\n    coef_scaled = clf_ovr_scaled.coef_\n\n    # Verify that the coefficients for the balanced case are different\n    # from the scaled case by comparing the difference of squared values\n    assert_array_almost_equal(\n        np.sum(np.square(coef_balanced - coef_scaled)), 0, decimal=5)\n\n"], "sample_1005": ["def test_ComplexNumber_printing():\n    from sympy.abc import I\n    z = Complex(2,3)\n    assert latex(z) == r\"2 + 3 i\"\n    z = Complex(-2, 3)\n    assert latex(z) == r\"- 2 + 3 i\"\n    z = Complex(2, -3)\n    assert latex(z) == r\"2 - 3 i\"\n    z = Complex(2, -3*I)\n    assert latex(z) == r\"2 - 3 i\" \n    z = Complex(2*I, 3)\n    assert latex(z) == r\"3 i + 2\" \n", "def test_Indexed():\n    from sympy.tensor.indexed import Indexed\n    x = Indexed('x', 1)\n    y = Indexed('y', 2)\n    assert latex(x) == r'x_{1}'\n    assert latex(y) == r'y_{2}'\n    assert latex(x + y) == r'x_{1} + y_{2}'\n    assert latex(x*y) == r'x_{1} y_{2}'\n", "def test_Indexed():\n    from sympy.tensor.indexed import Indexed\n    x = Indexed('x')\n    y = Indexed('y')\n    z = Indexed('z')\n\n    assert latex(x) == r'x'\n    assert latex(x[1]) == r'x_{1}'\n    assert latex(x[1, 2]) == r'x_{1, 2}'\n    assert latex(y[1] * x[2]) == r'y_{1} x_{2}'\n    assert latex(x[1] + y[2] + z[3]) == r'x_{1} + y_{2} + z_{3}'\n", "def test_Matrix_printing():\n    # Test cases for issue #11821\n    A = MatrixSymbol(\"A\", 2, 2)\n    B = MatrixSymbol(\"B\", 2, 2)\n\n    assert latex(A + B) == r\"A + B\"\n    assert latex(A - B) == r\"A - B\"\n    assert latex(A * B) == r\"A B\"\n    assert latex(A / B) == r\"A / B\"\n    assert latex(A.transpose()) == r\"A^{T}\"\n    assert latex(A.inv()) == r\"A^{-1}\"\n", "def test_issue_15168():\n    from sympy.matrices import MatrixSymbol\n    A = MatrixSymbol('A', 2, 2)\n    assert latex(A.T) == r\"A^{T}\"\n\n\n"], "sample_240": ["    def test_multiple_calls_to_make_token_with_same_user(self):\n        user = User.objects.create_user('repeattestuser', 'test5@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        tk2 = p0.make_token(user)\n        self.assertEqual(tk1, tk2)\n", "    def test_token_generation_with_custom_fields(self):\n        from django.test import TestCase\n\n        class CustomUser(User):\n            email = CustomEmailField()\n\n        CustomUser.objects.create_user('user', 'test@example.com', 'password')\n        p0 = PasswordResetTokenGenerator()\n        user = CustomUser.objects.get(username='user')\n        tk1 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk1), True)\n\n\n", "    def test_token_serialization(self):\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        # Check that the token can be serialized and deserialized.\n        serialized_token = str(tk1)\n        deserialized_token = base36_to_int(serialized_token.split('-')[0])\n        self.assertEqual(tk1, f\"{serialized_token.split('-')[0]}-{p0._make_token_with_timestamp(user, deserialized_token)}\")\n\n\n\n", "    def test_token_custom_hashing_algorithm(self):\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        with self.settings(DEFAULT_HASHING_ALGORITHM='sha512'):\n            generator = PasswordResetTokenGenerator()\n            generator.algorithm = 'sha256'\n            token = generator.make_token(user)\n            self.assertIs(generator.check_token(user, token), True)\n", "    def test_token_generation_with_legacy_user_field_handling(self):\n        \"\"\"\n        Validates that tokens work correctly with a custom email field\n        that's handled as legacy.\n        \"\"\"\n        class LegacyEmailUser(User):\n            email = CustomEmailField()\n\n        # Create a user model with a custom email field.\n        LegacyEmailUser.objects.create_user('legacyuser', 'test5@example.com', 'testpw')\n\n        p0 = PasswordResetTokenGenerator()\n        user = LegacyEmailUser.objects.get(username='legacyuser')\n        tk1 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk1), True)\n\n"], "sample_853": ["def test_transform_target_regressor_pipeline():\n    X, y = friedman\n    transformer = StandardScaler()\n    regressor = LinearRegression()\n\n    pipeline = Pipeline([\n        ('transformer', transformer),\n        ('regressor', regressor)\n    ])\n    tt = TransformedTargetRegressor(regressor=pipeline)\n    tt.fit(X, y)\n\n    # check that the regressor was trained with the transformed data\n    assert isinstance(tt.regressor_, Pipeline)\n    assert issubclass(type(tt.regressor_.named_steps['regressor']),\n                      LinearRegression)\n    assert_allclose(tt.regressor_.named_steps['regressor'].coef_,\n                    regressor.coef_)\n    \n    # make sure the pipeline output is equivalent to the direct fit\n    y_pred_pipeline = tt.predict(X)\n    y_pred_direct = pipeline.fit(X, y).predict(X)\n    assert_allclose(y_pred_pipeline, y_pred_direct)\n", "def test_transform_target_regressor_fit_with_sample_weight(transformer):\n    X, y = friedman\n    regr = TransformedTargetRegressor(regressor=DummyCheckerListRegressor(),\n                transformer=transformer,\n                check_inverse=False)\n    with pytest.raises(TypeError, match=r\"fit\\(\\) got an unexpected \"\n                       \"keyword argument 'sample_weight'\"):\n        regr.fit(X.tolist(), y.tolist(), sample_weight=np.ones(y.shape[0]))\n\n\n", "compilation error", "def test_transform_target_regressor_multioutput_fit(transformer):\n    X, y = friedman\n    y_multi = np.vstack((y, y ** 2)).T\n    regr = TransformedTargetRegressor(regressor=DummyRegressor(),\n                transformer=transformer)\n    regr.fit(X, y_multi)\n    # check the regressor output\n    assert regr.regressor_.coef_.shape[0] == y_multi.shape[1]\n", "def test_transform_target_regressor_pipeline(transformer_type):\n    X, y = friedman\n    if transformer_type is not None:\n        transformer = transformer_type()\n    else:\n        transformer = None\n\n    pipeline = Pipeline([\n        ('transformer', transformer),\n        ('regressor', TransformedTargetRegressor(regressor=LinearRegression()))\n    ])\n    pipeline.fit(X, y)\n    y_pred = pipeline.predict(X)\n    assert_allclose(y_pred,\n                    pipeline.named_steps['regressor'].transformer_.inverse_transform(\n                        pipeline.named_steps['transformer'].transform(y)))\n"], "sample_1174": ["def test_issue_19178():\n    from sympy import symbols, Abs, Im, Re\n    x, y = symbols('x y', complex=True)\n    assert Abs(x + y) == sqrt(Re(x + y)**2 + Im(x + y)**2) \n    assert Im(x + y) == Im(x) + Im(y)\n    assert Re(x + y) == Re(x) + Re(y) \n", "def test_issue_8402():\n    from sympy import Abs, Symbol, diff\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    z = Symbol('z', real=True)\n    assert Abs(diff(x, y)) == 0\n    assert Abs(diff(x, x)) == 1\n    assert Abs(diff(x, z)) == 0\n    assert Abs(diff(y*x, x)) == Abs(y)\n    assert Abs(diff(x**2, x)) == 2*Abs(x)\n    assert Abs(diff(x*y*z, x)) == Abs(y*z)\n    assert Abs(diff(sin(x), x)) == Abs(cos(x))\n    assert Abs(diff(x**2 + y**2, x)) == 2*Abs(x)\n    assert Abs(diff(exp(x), x)) == Abs(exp(x))\n", "compilation error", "def test_issue_19672():\n    x = Symbol('x', complex=True)\n    a = Symbol('a', real=True)\n    assert conjugate(x + a).func is conjugate\n    assert conjugate(x*a).func is conjugate\n    assert conjugate(x**a).func is conjugate\n", "compilation error"], "sample_485": ["    def test_urlize_trim_url_limit(self):\n        items = (\n            (\n                \"This is a long URL: http://long-url.com/this-is-a-very-long-description-that-will-be-truncated\",\n                \"This is a long URL: http://long-url.com/this-is-a-very-l...\",\n            ),\n            (\n                \"This is a short URL: http://short-url.com\",\n                \"This is a short URL: http://short-url.com\",\n            ),\n        )\n        for value, output in items:\n            with self.subTest(value=value):\n                self.assertEqual(urlize(value, trim_url_limit=50), output)\n", "    def test_urlize_with_trim_url_limit(self):\n        items = (\n            (\"https://www.example.com/longurllongurllongurllongurl\",\n             '<a href=\"https://www.example.com/longurllongurllongurllongurl\">https://www.example.com/longurllongurllongurllongurl'\n             '...</a>'),\n            (\"https://www.example.com/verylongurl/path/to/a/verylongfilename\"\n             \".txt\",\n             '<a href=\"https://www.example.com/verylongurl/path/to/a/verylongfilename'\n             '.txt\">https://www.example.com/verylongurl/path/to/a/verylongfilename'\n             '.txt...</a>'),\n        )\n        for value, output in items:\n            with self.subTest(value=value):\n                self.assertEqual(urlize(value, trim_url_limit=50), output)\n", "    def test_urlize_with_trim_url_limit(self):\n        long_url = \"https://www.example.com/this-is-a-really-long-url-that-needs-to-be-trimmed\"\n        trimmed_url = \"https://www.example.com/this-is-a-really-long-url-that-needs-to-be-trimmed\u2026\"\n\n        with self.subTest(limit=20, expected=trimmed_url):\n            self.assertEqual(\n                urlize(\"Visit {} for more details.\".format(long_url), trim_url_limit=20),\n                \"Visit {} for more details.\".format(trimmed_url),\n            )\n\n        with self.subTest(limit=50):\n            self.assertEqual(\n                urlize(\"Visit {} for more details.\".format(long_url), trim_url_limit=50),\n                \"Visit {} for more details.\".format(long_url),\n            )\n", "    def test_urlize_with_trim_url_limit(self):\n        texts = (\n            (\"This is a very long URL: http://www.long-url.com/this-is-a-very-long-url-that-needs-to-be-trimmed\",\n             \"This is a very long URL: <a href=\\\"http://www.long-url.com/this-is-a-very-long-url-that-needs-to-be-trimmed\\\">http://www.long-url.com/this-is-a-very-long-url-that-needs-to-be-trimmed...</a>\"),\n            (\"This is a short URL: http://www.short-url.com\",\n             \"This is a short URL: <a href=\\\"http://www.short-url.com\\\">http://www.short-url.com</a>\"),\n        )\n        for text, expected in texts:\n            with self.subTest(text=text):\n                self.assertEqual(urlize(text, trim_url_limit=25), expected)\n\n\n\n", "    def test_urlize_with_trailing_punctuation(self):\n        for value, expected in [\n            (\n                \"This is a URL: www.example.com,.\",\n                'This is a URL: <a href=\"http://www.example.com\">www.example.com</a>,',\n            ),\n            (\n                \"This is a URL: www.example.com;.\",\n                'This is a URL: <a href=\"http://www.example.com\">www.example.com</a>;.',\n            ),\n            (\n                \"This is a URL: www.example.com?!\",\n                'This is a URL: <a href=\"http://www.example.com\">www.example.com</a>?!',\n            ),\n        ]:\n            with self.subTest(value=value):\n                self.assertEqual(urlize(value), expected)\n\n\n\n"], "sample_948": ["def test_build_domain_cpp_template_param_qualified_name(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"warn-template-param-qualified-name\")\n    assert len(ws) == 2\n    assert \"WARNING: cpp:type reference target not found: T::typeWarn\" in ws[0]\n    assert \"WARNING: cpp:type reference target not found: T::U::typeWarn\" in ws[1]\n", "def test_build_domain_cpp_duplicate_function_param_ref(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"duplicate-function-param\")\n    assert len(ws) == 1\n    assert \"WARNING: cpp:identifier reference target not found: param_name\" in ws[0]\n", "def test_xref_with_scope_resolution_operator(app, status, warning):\n    app.builder.build_all()\n    text = (\n        \".. cpp:class:: MyClass\\n\"\n        \"  .. cpp:member:: foo\\n\"\n        \"    :cpp:scope: MyClass\\n\"\n        \"\\n\"\n        \".. cpp:member:: bar\\n\"\n        \"  :cpp:scope: MyClass::\\n\"\n    )\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc, addnodes.index, desc))\n    assert_node(doctree[0], addnodes.index, entries=[('single', 'MyClass (C++ class)', '_CPPv41fv', '', None)])\n    assert_node(doctree[2], addnodes.index, entries=[('single', 'MyClass::bar (C++ member)', '_CPPv41fv', '', None)])\n\n", "compilation error", "def test_build_domain_cpp_field_role_nested(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"field-role\")\n    assert len(ws) == 0\n"], "sample_645": ["def test_logging_to_file(caplog, testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logging.basicConfig(filename='test.log', level=logging.INFO)\n            logging.info('This message should be in the file')\n            assert 'This message should be in the file' in caplog.text\n    \"\"\"\n    )\n    result = p.runpytest()\n    result.stdout.fnmatch_lines([\"*This message should be in the file*\"])\n    assert result.ret == 0\n\n    with open('test.log', 'r') as f:\n        content = f.read()\n        assert 'This message should be in the file' in content\n\n\n\n", "def test_log_file_level(pytester: Pytester, tmp_path: Path) -> None:\n    log_file = tmp_path / \"test.log\"\n    pytester.makepyfile(\n        \"\"\"import pytest\n        import logging\n        \n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_file_level == logging.WARNING\n\n            logging.warning(\"WARNING message will be written to file\")\n            logging.info(\"INFO message won't be written to file\")\n            assert \"WARNING message will be written to file\" in open(log_file).read()\n            assert \"INFO message won't be written to file\" not in open(log_file).read()\n\n        \"\"\"\n    )\n    pytester.makeini(\n        f\"\"\"\n        [pytest]\n        log_file = {log_file}\n        log_file_level = WARNING\n    \"\"\"\n    )\n\n    result = pytester.runpytest()\n    assert result.ret == 0\n", "def test_log_level_file(caplog):\n    caplog.set_level(logging.INFO)\n    logger.info(\"b\u016b\")\n\n    with open(\"test.log\", \"w\") as f:\n        f.write(\"\")  # Create an empty file\n    \n    assert \"b\u016b\" in open(\"test.log\", \"r\").read()\n", "def test_log_file_setup(caplog, pytestconfig: Config):\n    old_log_file = pytestconfig.getini(\"log_file\")\n    try:\n        pytestconfig.setini(\"log_file\", \"test_log_file.txt\")\n        with caplog.at_level(logging.INFO):\n            logger.info(\"Log message for test_log_file.txt\")\n\n        assert \"Log message for test_log_file.txt\" in open(\"test_log_file.txt\").read()\n    finally:\n        pytestconfig.setini(\"log_file\", old_log_file)\n", "def test_caplog_handles_nested_calls(caplog):\n    caplog.set_level(logging.INFO)\n\n    logger.info(\"outer level\")\n    with caplog.at_level(logging.WARNING, logger=sublogger.name):\n        sublogger.warning(\"inner level\")\n        with caplog.at_level(logging.DEBUG, logger=sublogger.name):\n            sublogger.debug(\"deep inner level\")\n    assert \"outer level\" in caplog.text\n\n\n\n"], "sample_610": ["def test_cftimeindex_to_datetime64():\n    index = xr.cftime_range(\"2000\", periods=5, calendar=\"360_day\")\n    result = index.to_datetime64()\n    \n    expected = pd.to_datetime(index.values, unit=\"s\")\n    assert result.equals(expected)\n    \n    # Test with a different calendar\n    index = xr.cftime_range(\"2000\", periods=5, calendar=\"noleap\")\n    result = index.to_datetime64()\n    expected = pd.to_datetime(index.values, unit=\"s\")\n    assert result.equals(expected)\n", "def test_cftimeindex_to_timestamp_array():\n    index = xr.cftime_range(\"2000-01-01\", periods=3)\n    result = index.to_timestamp_array()\n    expected = np.array([\n        pd.Timestamp(\"2000-01-01T00:00:00\"),\n        pd.Timestamp(\"2000-01-02T00:00:00\"),\n        pd.Timestamp(\"2000-01-03T00:00:00\")\n    ])\n    np.testing.assert_array_equal(result, expected)\n\n\n", "def test_infer_freq_mixed_types():\n    cf_indx = xr.cftime_range(\"2000-01-01\", periods=3, freq=\"D\")\n    pd_indx = pd.date_range(\"2000-01-01\", periods=3, freq=\"D\")\n    mixed_array = xr.DataArray(np.array([cf_indx, pd_indx]))\n    with pytest.raises(ValueError, match=\"mixed types of datetime-like objects\"):\n        xr.infer_freq(mixed_array)\n", "def test_infer_freq_mixed_types():\n    cf_indx = xr.cftime_range(\"2000-01-01\", periods=3, freq=\"D\")\n    pd_indx = pd.date_range(\"2000-01-01\", periods=3, freq=\"D\")\n    mixed_data = xr.DataArray(np.concatenate((cf_indx.values, pd_indx.values)))\n    assert xr.infer_freq(mixed_data) is None\n", "def test_infer_freq_calendar_mismatch():\n    cf_indx = xr.cftime_range(\"2000-01-01\", periods=3, freq=\"D\", calendar=\"noleap\")\n    pd_indx = pd.date_range(\"2000-01-01\", periods=3, freq=\"D\")\n    assert xr.infer_freq(cf_indx) == \"D\"\n    assert xr.infer_freq(pd_indx) == \"D\"\n\n    with pytest.raises(ValueError, match=\"Calendar mismatch\"):\n        xr.infer_freq(cf_indx, pd_indx)\n"], "sample_796": ["def test_huber_with_intercept_not_fitted():\n    X, y = make_regression_with_outliers()\n    huber = HuberRegressor(fit_intercept=False, alpha=0.0, max_iter=100)\n    huber.fit(X, y)\n    assert huber.intercept_ == 0.0\n", "def test_huber_with_sparse_features():\n    # Test that HuberRegressor works correctly when features are sparse.\n\n    n_samples = 100\n    n_features = 5000  \n    rng = np.random.RandomState(0)\n\n    # Generate some sparse data\n    X = sparse.csr_matrix((rng.rand(n_samples, n_features),\n                           (rng.randint(n_features, size=n_samples),\n                            rng.randint(n_features, size=n_samples))))\n    y = rng.rand(n_samples)\n\n    huber = HuberRegressor(fit_intercept=True, epsilon=1.35, alpha=0.01)\n    huber.fit(X, y)\n\n    assert_almost_equal(huber.coef_.shape, (n_features,))\n\n    # Check that the predictions are correct\n    y_pred = huber.predict(X)\n    assert_array_almost_equal(y_pred, huber.predict(X.toarray()), 3)\n", "def test_huber_large_epsilon_convergence():\n    # Test convergence with very large epsilon\n    X, y = make_regression_with_outliers()\n    huber = HuberRegressor(fit_intercept=True, epsilon=1e10, alpha=0.01)\n\n    # Make sure the Huber regression converges with such a large epsilon\n    huber.fit(X, y)\n    assert huber.n_iter_ > 0\n\n\n\n", "def test_huber_intercept_with_zeros():\n    # Test that the intercept is correctly estimated when X has zeros\n\n    X = np.array([[1, 0],\n                  [0, 1],\n                  [1, 1],\n                  [1, 0]])\n    y = np.array([2, 3, 5, 2])\n\n    huber = HuberRegressor(fit_intercept=True, alpha=0.01, max_iter=100)\n    huber.fit(X, y)\n    assert huber.intercept_ != 0.0\n", "def test_huber_coef_scaling():\n    # Test the scaling behavior of the coefficients when epsilon changes.\n    X, y = make_regression_with_outliers()\n    huber_large = HuberRegressor(fit_intercept=True, alpha=0.0, epsilon=10)\n    huber_large.fit(X, y)\n    huber_small = HuberRegressor(fit_intercept=True, alpha=0.0, epsilon=0.1)\n    huber_small.fit(X, y)\n    assert_array_almost_equal(huber_large.coef_ / huber_small.coef_,\n                              np.sqrt(huber_large.epsilon / huber_small.epsilon),\n                              4)\n"], "sample_641": ["def test__get_pdata_path_zero_recur(\n    path: str, recur: int, pylint_home: Path, expected: Path", "def test__get_pdata_path_zero_recur(\n    path: str, recur: int, pylint_home: Path, expected: Path", "def test__get_pdata_path_empty_base(\n    path: str, recur: int, pylint_home: Path, expected: Path", "def test_load_results_not_found(\n    path: str, recur: int, pylint_home: Path, expected: Path", "def test_load_results_non_existing_file(\n    path: str, recur: int, pylint_home: Path"], "sample_915": ["    def _wrapped_meth(self):\n        pass\n    ", "    def decorated_method(self):\n        \"\"\"docstring.\"\"\"\n        pass\n", "    def cls_meth(cls):\n        pass\n\n", "    def decorated_meth(cls):\n        return cls.meth()\n", "    def decorator_method(cls):\n        return cls.meth\n    "], "sample_551": ["def test_polycollection_shaded():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    verts = np.array([\n        [[-1, -1, 0], [1, -1, 0], [1, 1, 0], [-1, 1, 0]],\n        [[0, -1, 1], [0, 1, 1], [1, 1, 1], [1, -1, 1]]\n    ])\n    colors = [[[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0]]\n              , [[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0]]]\n    poly = Poly3DCollection(verts, facecolors=colors, shade=True)\n    ax.add_collection3d(poly)\n    ax.set_xlim([-1.5, 1.5])\n    ax.set_ylim([-1.5, 1.5])\n    ax.set_zlim([0, 1.5])\n\n\n", "def test_poly3d_legend():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    verts1 = [((0, 0, 0), (1, 0, 0), (1, 1, 0), (0, 1, 0))]\n    verts2 = [((0, 1, 0), (1, 1, 0), (1, 1, 1), (0, 1, 1))]\n    colors1 = ['blue', 'red', 'green', 'yellow']\n    colors2 = ['cyan', 'magenta', 'orange', 'purple']\n\n    poly1 = Poly3DCollection(verts1, facecolors=colors1)\n    poly2 = Poly3DCollection(verts2, facecolors=colors2)\n\n    ax.add_collection3d(poly1)\n    ax.add_collection3d(poly2)\n\n    leg = ax.legend([poly1, poly2], ['Poly1', 'Poly2'], loc='upper right')\n\n\n", "def test_patch_collection_3d_legends():\n    fig, ax = plt.subplots(subplot_kw=dict(projection='3d'))\n    polygons = [\n        [(0, 0, 0), (1, 0, 0), (1, 1, 0), (0, 1, 0)],\n        [(0, 0, 1), (1, 0, 1), (1, 1, 1), (0, 1, 1)],\n    ]\n    col = Poly3DCollection(polygons, facecolors='hsv', edgecolors='black')\n    ax.add_collection3d(col)\n    leg = ax.legend([col], ['patches'])\n    plt.show()\n", "def test_contour_filled_legend():\n    from matplotlib.colors import ListedColormap\n\n    x, y = np.mgrid[0:10, 0:10]\n    z = np.sin(x * y)\n\n    fig, ax = plt.subplots(subplot_kw=dict(projection='3d'))\n    cmap = ListedColormap(['blue', 'green', 'red'])\n    cs = ax.contourf(x, y, z, levels=[-1, 0, 1], cmap=cmap, zdir='z')\n\n    artists, labels = cs.legend_elements()\n    assert labels == ['$ -1$', '$0$', '$1$']\n\n    # Ensure the legend patches are filled correctly\n    for artist, label in zip(artists, labels):\n        assert isinstance(artist, patches.Patch) and isinstance(artist.get_facecolor(), tuple)\n        assert artist.get_label() == label\n\n\n\n", "def test_patch_collection_3d():\n    fig, ax = plt.subplots(subplot_kw=dict(projection='3d'))\n    verts = [[(0, 0, 0), (1, 0, 0), (1, 1, 0), (0, 1, 0)],\n             [(0, 0, 1), (1, 0, 1), (1, 1, 1), (0, 1, 1)]]\n    poly = Poly3DCollection(verts, facecolors='r', edgecolors='k')\n    ax.add_collection(poly)\n\n    ax.set_xlim(0, 1.5)\n    ax.set_ylim(0, 1.5)\n    ax.set_zlim(0, 1.5)\n\n\n\n\n"], "sample_705": ["def test_pytester_assert_outcomes_warning_on_stdout_stderr(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n            print(\"warning message\")\n            warnings.warn(\"some custom warning\")\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1, warnings=1)\n", "def test_pytester_assert_outcomes_with_mixed_outcomes(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n            assert True\n\n            assert False\n\n            warnings.warn(\"Some warning\")\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1, failed=1, warnings=1)\n\n\n\n", "def test_pytester_assert_outcomes_warnings_with_message(\n    pytester: Pytester,", "def test_pytester_assert_outcomes_warnings_with_multiple_warnings(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import warnings\n\n            warnings.warn(\"warning 1\")\n            warnings.warn(\"warning 2\")\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1, warnings=2)\n", "compilation error"], "sample_260": ["    def test_create_model_order_with_respect_to(self):\n        \"\"\"\n        AlterOrderWithRespectTo should optimize into CreateModel.\n        \"\"\"\n        managers = [('objects', EmptyManager())]\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[(\"name\", models.CharField(max_length=255))],\n                    options={'verbose_name': 'Foo'},\n                    bases=(UnicodeModel,),\n                    managers=managers,\n                ),\n                migrations.AlterOrderWithRespectTo(\"Foo\", \"bar\"),\n            ],\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[\n                        (\"name\", models.CharField(max_length=255)),\n                    ],\n                    options={'verbose_name': 'Foo'},\n                    bases=(UnicodeModel,),\n                    managers=managers,\n                ),\n            ],\n        )\n\n\n", "    def test_optimize_through_fields_with_fk(self):\n        \"\"\"\n        field-level through checking is working with FK dependencies. This should\n        manage to collapse model Foo to nonexistence, and model Bar to a single\n        CharField called \"name\".\n        \"\"\"\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\"Foo\", [(\"name\", models.CharField(max_length=255))]),\n                migrations.CreateModel(\"Bar\", [(\"size\", models.IntegerField())]),\n                migrations.AddField(\"Foo\", \"bar_fk\", models.ForeignKey(\"migrations.Bar\", on_delete=models.CASCADE)),\n                migrations.AddField(\"Bar\", \"width\", models.IntegerField()),\n                migrations.AlterField(\"Foo\", \"name\", models.CharField(max_length=255)),\n                migrations.RenameField(\"Bar\", \"size\", \"dimensions\"),\n                migrations.RemoveField(\"Foo\", \"name\"),\n                migrations.RemoveField(\"Bar\", \"dimensions\"),\n                migrations.DeleteModel(\"Foo\"),\n            ],\n            [\n                migrations.CreateModel(\"Bar\", [(\"name\", models.CharField(max_length=255))]),\n            ],\n        )\n", "    def test_optimize_through_fields_with_fk(self):\n        \"\"\"\n        field-level through checking is working, even when there are FKs involved.\n        This should manage to collapse model Foo to nonexistence, and model Bar\n        to a single IntegerField called \"bar_width\",\n        \"\"\"\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\"Foo\", [(\"name\", models.CharField(max_length=255))]),\n                migrations.CreateModel(\"Bar\", [(\"size\", models.IntegerField())]),\n                migrations.AddField(\"Foo\", \"age\", models.IntegerField()),\n                migrations.AddField(\"Bar\", \"width\", models.IntegerField()),\n                migrations.AddField(\"Foo\", \"bar\", models.ForeignKey(\"Bar\", on_delete=models.CASCADE)),\n                migrations.AlterField(\"Foo\", \"age\", models.IntegerField()),\n                migrations.RenameField(\"Bar\", \"size\", \"bar_width\"),\n                migrations.RemoveField(\"Foo\", \"age\"),\n                migrations.RenameModel(\"Foo\", \"Phou\"),\n                migrations.RemoveField(\"Bar\", \"bar_width\"),\n                migrations.RenameModel(\"Phou\", \"Fou\"),\n                migrations.DeleteModel(\"Fou\"),\n            ],\n            [\n                migrations.CreateModel(\"Bar\", [(\"bar_width\", models.IntegerField())]),\n            ],\n        )\n", "    def test_optimize_through_fields_with_fk(self):\n        \"\"\"\n\n\n        field-level through checking is working. This should manage to collapse\n        model Foo to nonexistence, and model Bar to a single IntegerField\n        called \"width\".\n        \"\"\"\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\"Foo\", [(\"name\", models.CharField(max_length=255))]),\n                migrations.CreateModel(\"Bar\", [(\"size\", models.IntegerField())]),\n                migrations.AddField(\"Foo\", \"age\", models.IntegerField()),\n                migrations.AddField(\"Bar\", \"width\", models.IntegerField()),\n                migrations.AddField(\"Foo\", \"bar\", models.ForeignKey(\"migrations.Bar\", models.CASCADE)),\n                migrations.AlterField(\"Foo\", \"age\", models.IntegerField()),\n                migrations.RenameField(\"Bar\", \"size\", \"dimensions\"),\n                migrations.RemoveField(\"Foo\", \"age\"),\n                migrations.RenameModel(\"Foo\", \"Phou\"),\n                migrations.RemoveField(\"Bar\", \"dimensions\"),\n                migrations.RenameModel(\"Phou\", \"Fou\"),\n                migrations.DeleteModel(\"Fou\"),\n            ],\n            [\n                migrations.CreateModel(\"Bar\", [(\"width\", models.IntegerField())]),\n            ],\n        )\n\n\n\n", "    def test_create_model_add_field_order_matters(self):\n        \"\"\"\n        AddField should optimize with the correct order, even if there are multiple\n        AddField operations.\n       \"\"\"\n        manager = EmptyManager()\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[\n                        (\"bar\", models.IntegerField()),\n                        (\"baz\", models.CharField(max_length=255)),\n                    ],\n                    options={'verbose_name': 'Foo'},\n                    bases=(UnicodeModel,),\n                    managers=[manager],\n                ),\n                migrations.AddField(\"Foo\", \"qux\", models.TextField()),\n                migrations.AddField(\"Foo\", \"quux\", models.CharField(max_length=255)),\n            ],\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[\n                        (\"bar\", models.IntegerField()),\n                        (\"baz\", models.CharField(max_length=255)),\n                        (\"qux\", models.TextField()),\n                        (\"quux\", models.CharField(max_length=255)),\n                    ],\n                    options={'verbose_name': 'Foo'},\n                    bases=(UnicodeModel,),\n                    managers=[manager],\n                ),\n            ],\n        )\n"], "sample_482": ["    def test_safe_string(self):\n        output = self.engine.render_to_string(\"escapeseq_safe\", \n        {\"a\": [mark_safe(\"<p>\"), mark_safe(\"x&y\")]})\n        self.assertEqual(output, \"<p/>, x&amp;y\")\n", "    def test_safe(self):\n        output = self.engine.render_to_string(\n            \"escapeseq_safe\",\n            {\"a\": [mark_safe(\"x&y\"), mark_safe(\"<p>\")]},\n            {\"b\": [\"x&y\", \"<p\"]},\n        )\n        self.assertEqual(output, \"x&y, <p> -- x&y, <p>\")\n", "    def test_safe_input(self):\n        output = self.engine.render_to_string(\"escapeseq_safe\", {\"a\": [\"x&y\", \"<p>\"]})\n        self.assertEqual(output, \"x&y, <p>\")\n", "    def test_safe(self):\n        output = self.engine.render_to_string(\n            \"escapeseq_safe\",\n            {\"a\": [\"x&y\", \"<p>\"], \"b\": [mark_safe(\"x&y\"), mark_safe(\"<p>\")]},\n        )\n        self.assertEqual(output, \"x&amp;y, &lt;p&gt; -- x&y, <p>\")\n", "    def test_safe_input(self):\n        output = self.engine.render_to_string(\"escapeseq_already_safe\", {\"a\": [mark_safe(\"x&y\"), mark_safe(\"<p>\")]})\n        self.assertEqual(output, \"x&y, <p>\")\n"], "sample_395": ["    def test_get_template_directories_no_dirs(self):\n        self.assertEqual(autoreload.get_template_directories(), set())\n", "    def test_get_template_directories_custom_loader(self):\n        custom_loader_dir = Path(__file__).parent / \"custom_loader\"\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            {\n                custom_loader_dir,\n                ROOT / \"templates\",\n            },\n        )\n", "    def test_get_template_directories_with_nested_dirs(self):\n        self.override_settings(\n            TEMPLATES=[\n                {\n                    \"DIRS\": [\n                        ROOT / \"templates\" / \"nested\",\n                    ],\n                    \"BACKEND\": \"django.template.backends.django.DjangoTemplates\",\n                }\n            ]\n        )\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            {\n                ROOT / \"templates\" / \"nested\",\n            },\n        )\n", "    def test_get_template_directories_custom_loader(self):\n        with mock.patch(\n            \"my_custom_template_loader.Loader.get_dirs\", return_value=[\"custom_templates\"]\n        ) as mock_get_dirs:\n            self.assertSetEqual(\n                autoreload.get_template_directories(),\n                {\n                    Path.cwd() / \"custom_templates\",\n                },\n            )\n            mock_get_dirs.assert_called_once() \n", "    def test_nested_template_directories(self):\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            {\n                ROOT / \"templates\",\n                ROOT / \"templates_nested\",\n            },\n        )\n"], "sample_257": ["    def test_contains_contained_by_with_invalid_type(self):\n        with self.assertRaises(TypeError):\n            NullableJSONModel.objects.filter(value__d__contains=1)\n        with self.assertRaises(TypeError):\n            NullableJSONModel.objects.filter(value__contains=1)\n        with self.assertRaises(TypeError):\n            NullableJSONModel.objects.filter(value__contained_by=1)\n\n\n", "    def test_contains_contained_by_with_key_transform_error(self):\n        with self.assertRaises(TypeError):\n            NullableJSONModel.objects.filter(value__d__contains=KeyTransform('f', 'value'))\n\n", "    def test_key_transform_with_null_value(self):\n        obj = NullableJSONModel.objects.create(value={'a': 'b', 'c': None})\n        self.assertEqual(\n            NullableJSONModel.objects.filter(value__c=KeyTransform('c', 'value')).count(),\n            1,\n        )\n", "    def test_contains_contained_by_with_null_values(self):\n        obj = NullableJSONModel.objects.create(value={'baz': {'a': 'b', 'c': None}})\n        self.assertIs(NullableJSONModel.objects.filter(value__baz__contains={'a': 'b'}).exists(), True)\n        self.assertIs(NullableJSONModel.objects.filter(value__baz__contains={'c': None}).exists(), True)\n        self.assertIs(NullableJSONModel.objects.filter(value__baz__contained_by={'a': 'b', 'c': None}).exists(), True)\n", "    def test_contains_contained_by_with_key_transform_subquery(self):\n        obj = NullableJSONModel.objects.create(value={'bar': {'foo': 'bar'}})\n        subquery_lookup = Subquery(\n            NullableJSONModel.objects.filter(value__bar__baz__contains='f').values('id')\n        )\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__bar__baz__contains='f'\n            ).filter(id__in=subquery_lookup),\n            [obj],\n        )\n"], "sample_1029": ["def test_Reals():\n    sT(S.Reals, \"Reals\")\n", "compilation error", "def test_Reals():\n    sT(S.Reals, \"Reals\")\n", "def test_Reals():\n    sT(S.Reals, \"Reals\")\n", "def test_Reals():\n    sT(S.Reals, \"Reals\")\n"], "sample_919": ["    def check_role_usage(role, expected_count):\n        pattern = f'{role}-role'\n        count = len(re.findall(pattern, output))\n        assert count == expected_count, f'Role `{role}` found {count} times, expected {expected_count}'", "compilation error", "    def types(role, tag):\n        pattern = (r'{role}-role:.*?'\n                   r'<(?P<tag>{tag}) .*?type=[\"\\'](?P<types>.*?)[\"\\'].*?>'\n                   r'.*'\n                   r'</(?P=tag)>').format(role=role, tag=tag)\n        result = re.search(pattern, output)\n        expect = '''\\", "    def check_xref_consistency(role, tag, expected_classes):\n        classes_found = classes(role, tag)\n        for cls in expected_classes:\n            assert cls in classes_found, f\"Expected class '{cls}' not found for role '{role}' and tag '{tag}'\"\n", "    def check(role, text, file):\n        pattern = '<li><p>%s<a .*?><code .*?><span .*?>%s</span></code></a></p></li>' % (role, text)\n        res = re.search(pattern, file)\n        if not res:\n            print(\"Pattern\\n\\t%s\\nnot found in %s\" % (pattern, file))\n            assert False\n"], "sample_889": ["def test_calibration_display_multiple_estimators(pyplot, iris_data_binary, method):\n    X, y = iris_data_binary\n    clf1 = LogisticRegression().fit(X, y)\n    clf2 = DecisionTreeClassifier().fit(X, y)\n\n    viz = CalibrationDisplay.from_estimator(\n        [clf1, clf2],\n        X,\n        y,\n        method=method,\n        n_bins=5,\n    )\n\n    ax = viz.ax_\n\n    # Check that each estimator has its own line and legend entry\n    legend_handles, legend_labels = ax.get_legend_handles_labels()\n    assert len(legend_handles) == 2\n    assert all(\n        label in legend_labels\n        for label in [\n            clf1.__class__.__name__,\n            clf2.__class__.__name__,\n            \"Perfectly calibrated\",\n        ]\n    )\n\n\n\n", "def test_calibration_with_non_calibrated_estimator(data):\n    class UncalibratedClassifier(BaseEstimator):\n            pass\n\n            return np.zeros(len(X))\n\n    clf = UncalibratedClassifier()\n    with pytest.raises(ValueError, match=\"is not calibrated\"):\n        CalibratedClassifierCV(clf).fit(*data)\n", "def test_calibration_with_prefit_estimator(method):\n    from sklearn.linear_model import LogisticRegression\n    X, y = make_classification(n_samples=100, random_state=42)\n    clf = LogisticRegression().fit(X, y)\n    calibrated_clf = CalibratedClassifierCV(clf, method=method)\n    _ = calibrated_clf.fit(X, y)\n    assert calibrated_clf.estimator_ is clf\n", "def test_calibration_with_invalid_method(data):\n    X, y = data\n    with pytest.raises(ValueError, match=\"Invalid method\"):\n        CalibratedClassifierCV(estimator=LogisticRegression(), method=\"wrong_method\").fit(X, y)\n", "def test_calibration_with_prefit_estimator_invalid_type(data):\n    \"\"\"Test that a TypeError is raised when trying to use a Prefit estimator that\n    is not calibrated.\"\"\"\n    X, y = data\n    with pytest.raises(TypeError, match=\"Calibration object is already trained\"):\n        CalibratedClassifierCV(estimator=DecisionTreeClassifier().fit(X, y)).fit(X, y)\n"], "sample_1190": ["def test_issue_22691():\n    from sympy.physics.units import Quantity, meter\n\n    q1 = Quantity('q1')\n    q2 = Quantity('q2')\n    q3 = Quantity('q3')\n\n    SI.set_quantity_dimension(q1, meter**2)\n    SI.set_quantity_dimension(q2, meter)\n    SI.set_quantity_dimension(q3, meter**3)\n\n    # Test that combining quantities from different dimensions\n    # does not raise an error\n    expr1 = q1*q2\n    expr2 = q1/q3\n    expr3 = q2 + q3\n    assert SI._collect_factor_and_dimension(expr1) is not None\n    assert SI._collect_factor_and_dimension(expr2) is not None\n    assert SI._collect_factor_and_dimension(expr3) is not None\n", "def test_get_prefixed_unit():\n    assert meter.get_prefixed_unit() == meter\n    assert centimeter.get_prefixed_unit() == centimeter\n    assert kilometer.get_prefixed_unit() == kilometer\n    assert (meter**2).get_prefixed_unit() == Quantity(\"square_meter\")\n\n\n", "def test_issue_22058():\n    from sympy.physics.units import meter, second, Quantity\n    q = Quantity('q')\n    q.set_global_relative_scale_factor(S(1), meter/second)\n    assert q.units == meter/second  # unit info should be retained\n", "compilation error", "def test_issue_23241():\n    from sympy.physics.units import (Quantity, Symbol)\n    from sympy.physics.units.systems.si import dimsys_SI\n\n    x = Symbol('x')\n    y = Symbol('y')\n    q = Quantity('q')\n    SI.set_quantity_dimension(q, length*time)\n    q.set_global_relative_scale_factor(1, meter*second)\n\n    assert SI.get_dimensional_expr(q*x/y) == dimsys_SI.get_dimensional_expr(q) * dimsys_SI.get_dimensional_expr(x) / dimsys_SI.get_dimensional_expr(y)\n\n\n"], "sample_908": ["    def test_unparse_class_and_function_defs(source, expected):  \n        module = ast.parse(source)\n        assert ast.unparse(module.body[0]) == expected \n", "    def test_unparse_arguments():\n        args = ast.arguments(\n            args=['a', 'b'],\n            defaults=[ast.NameConstant(value=1)],\n            kwonlyargs=[],\n            kw_defaults={},\n        )\n        expected = \"a, b=1\"\n        assert ast.unparse_arguments(args) == expected\n\n", "    def test_unparse_arguments():\n        source = \"\"\"", "def test_unparse_function_definition(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0]) == expected\n", "    def test_unparse_assignment(source, expected):\n        module = ast.parse(source)\n        assert ast.unparse(module.body[0]) == expected \n"], "sample_713": ["def test_ridge_cv_solver_choice():\n    X, y = make_regression(n_samples=100, n_features=10, n_informative=5, random_state=0)\n    for solver in ['svd', 'sparse_cg', 'lsqr', 'cholesky']:\n        ridge = RidgeCV(solver=solver, cv=5)\n        ridge.fit(X, y)\n        assert_equal(ridge.best_params_['solver'], solver)\n\n\n", "def test_ridge_multiclass_classification_with_balanced_class_weights():\n    X, y = make_classification(\n        n_samples=100,\n        n_features=20,\n        n_classes=3,\n        weights=[0.2, 0.5, 0.3],\n        random_state=0,\n    )\n\n    ridge = RidgeClassifier(class_weight=\"balanced\")\n    ridge.fit(X, y)\n    assert ridge.score(X, y) > 0.65  # Ensure reasonable accuracy\n\n", "def test_ridge_multiclass_solver_switch():\n    X, y = make_classification(n_samples=100, n_features=10, n_classes=3, random_state=42)\n    y = LabelBinarizer().fit_transform(y)\n\n    for solver in ['saga', 'sag', 'lsqr']:\n        ridge_class = RidgeClassifier(alpha=1., solver=solver, multi_class='ovr')\n        ridge_class.fit(X, y)\n        \n        # switch solver\n        ridge_class_new = RidgeClassifier(alpha=1., solver='lbfgs', multi_class='ovr')\n        ridge_class_new.fit(X, y)\n\n        assert_almost_equal(ridge_class.coef_, ridge_class_new.coef_)\n", "def test_ridge_with_categorical_features():\n    from sklearn.preprocessing import OneHotEncoder\n    X, y = make_classification(n_samples=100, n_features=5,\n                               n_informative=3, n_redundant=1,\n                               random_state=42)\n\n    # Create categorical feature\n    X_cat = np.random.randint(0, 2, size=(X.shape[0], 1))\n\n    # Encode categorical feature\n    encoder = OneHotEncoder(handle_unknown='ignore')\n    X_cat_encoded = encoder.fit_transform(X_cat).toarray()\n\n    # Combine encoded categorical feature and numerical features\n    X = np.concatenate((X, X_cat_encoded), axis=1)\n\n    ridge = Ridge(alpha=1.)\n    ridge.fit(X, y)\n    assert_array_equal(ridge.coef_.shape, (X.shape[1],))\n", "def test_ridge_solver_consistency():\n    rng = np.random.RandomState(42)\n    X, y = make_regression(n_samples=100, n_features=5, random_state=42)\n    alphas = [0.01, 0.1, 1.0, 10.0]\n\n    for solver in ['svd', 'sparse_cg', 'cholesky', 'lsqr']:\n        for alpha in alphas:\n            ridge_a = Ridge(alpha=alpha, solver=solver)\n            ridge_b = Ridge(alpha=alpha, solver=solver)\n            ridge_a.fit(X, y)\n            ridge_b.fit(X, y)\n            assert_array_almost_equal(ridge_a.coef_, ridge_b.coef_,\n                                      decimal=4)\n\n\n\n"], "sample_208": ["    def test_complex_dependency(self):\n        address = ModelState('a', 'Address', [\n            ('id', models.AutoField(primary_key=True)),\n            ('country', models.ForeignKey('b.DeliveryCountry', models.CASCADE)),\n        ])\n        person = ModelState('a', 'Person', [\n            ('id', models.AutoField(primary_key=True)),\n        ])\n        apackage = ModelState('b', 'APackage', [\n            ('id', models.AutoField(primary_key=True)),\n            ('person', models.ForeignKey('a.Person', models.CASCADE)),\n        ])\n        country = ModelState('b', 'DeliveryCountry', [\n            ('id', models.AutoField(primary_key=True)),\n        ])\n        changes = self.get_changes([], [address, person, apackage, country])\n        self.assertNumberMigrations(changes, 'a', 2)\n        self.assertNumberMigrations(changes, 'b', 1)\n        self.assertOperationTypes(changes, 'a', 0, [\"CreateModel\", \"CreateModel\"])\n        self.assertOperationTypes(changes, 'a', 1, [\"AddField\"])\n        self.assertMigrationDependencies(changes, 'a', 0, [])\n        self.assertMigrationDependencies(changes, 'a', 1, [('a', 'auto_1'), ('b', 'auto_1')])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'b', 1)\n        self.assertOperationTypes(changes, 'b', 0, [\"CreateModel\"])\n        self.assertMigrationDependencies(changes, 'b', 0, [('__setting__', 'AUTH_USER_MODEL')])\n", "    def test_combined_operations(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Person', fields=[]),\n                migrations.AddField('Person', 'animal_foo', models.CharField(max_length=100)),\n                migrations.DeleteModel('Person'),\n            ]\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'delete_person_add_animal_foo')\n", "    def test_order_with_respect_to_fk_in_rename_field(self):\n        before = [\n            ModelState('testapp', 'author', [\n                ('id', models.AutoField(primary_key=True)),\n                ('book', models.ForeignKey('otherapp.Book', models.CASCADE)),\n            ]),\n            ModelState('otherapp', 'book', [\n                ('id', models.AutoField(primary_key=True)),\n            ]),\n        ]\n        after = [\n            ModelState('testapp', 'author', [\n                ('id', models.AutoField(primary_key=True)),\n                ('book_id', models.ForeignKey('otherapp.Book', models.CASCADE)),\n            ]),\n            ModelState('otherapp', 'book', [\n                ('id', models.AutoField(primary_key=True)),\n            ]),\n        ]\n\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"RenameField\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, from_field='book', to_field='book_id', model_name='author')\n\n", "    def test_rename_table_with_fk_to_field(self):\n        changes = self.get_changes(\n            [self.author_empty, self.book],\n            [self.author_empty, self.book_with_author_name_and_book_name],\n        )\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['RenameTable', 'AddField'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='book', new_name='book_with_author_name_and_book_name')\n", "    def test_add_model_with_fields_after_delete_model(self):\n        before = [\n            ModelState('app', 'readable', [\n                ('id', models.AutoField(primary_key=True)),\n                ('title', models.CharField(max_length=200)),\n            ]),\n        ]\n        after = [\n            ModelState('app', 'readable', [\n                ('id', models.AutoField(primary_key=True)),\n            ]),\n            ModelState('app', 'book', [\n                ('title', models.CharField(max_length=200)),\n            ], bases=('app.readable',)),\n            ModelState('app', 'person', [\n                ('id', models.AutoField(primary_key=True)),\n                ('name', models.CharField(max_length=200)),\n            ]),\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['RemoveField', 'CreateModel', 'CreateModel'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='title', model_name='readable')\n        self.assertOperationAttributes(changes, 'app', 0, 1, name='book')\n        self.assertOperationAttributes(changes, 'app', 0, 2, name='person')\n\n"], "sample_458": ["    def test_floatformat03(self):\n        output = self.engine.render_to_string(\n            \"floatformat03\", {\"a\": \"1.42\"}\n        )\n        self.assertEqual(output, \"1.42\")\n", "    def test_nan(self):\n        self.assertEqual(floatformat(float(\"nan\")), \"nan\")\n        self.assertEqual(floatformat(Decimal(\"nan\")), \"nan\")\n", "    def test_nan(self):\n        self.assertEqual(floatformat(float(\"nan\")), \"nan\")\n        self.assertEqual(floatformat(Decimal(\"nan\")), \"nan\")\n", "    def test_custom_locale(self):\n        with translation.override(\"en-US\", deactivate=True):\n            self.assertEqual(floatformat(66666.666, \"2\"), \"66666.67\")  \n            self.assertEqual(floatformat(66666.666, \"2u\"), \"66666.67\") \n            with self.settings(\n                USE_THOUSAND_SEPARATOR=True,\n                NUMBER_GROUPING=3,\n                THOUSAND_SEPARATOR=\"!\",\n            ):\n                self.assertEqual(floatformat(66666.666, \"2gu\"), \"66!666.67\")\n                self.assertEqual(floatformat(66666.666, \"2ug\"), \"66!666.67\")\n            # Invalid suffix.\n            self.assertEqual(floatformat(66666.666, \"u2\"), \"66666.666\") \n", "    def test_floatformat_rounding(self):\n        with self.settings(NUMBER_FORMAT=\"%10.5f\"):\n            self.assertEqual(floatformat(12345.6789, 2), \"12345.68\")\n            self.assertEqual(floatformat(12345.67890123, 5), \"12345.67890\")\n            self.assertEqual(floatformat(12345.67890123, 3), \"12345.679\")\n        \n        with self.settings(NUMBER_FORMAT=\"%10.5f\"):\n            self.assertEqual(floatformat(-12345.6789, 2), \"-12345.68\")\n            self.assertEqual(floatformat(-12345.67890123, 5), \"-12345.67890\")\n            self.assertEqual(floatformat(-12345.67890123, 3), \"-12345.679\")\n"], "sample_1187": ["def test_hyperplane_parameters():\n    triangle = Polygon(Point(0, 3), Point(5, 3), Point(1, 1))\n    a, b = hyperplane_parameters(triangle)\n    assert a == -0.2 and b == 3 \n\n\n", "    def test_hyperplane_intersection():\n        # Test intersection of a hyperplane with a simple polygon\n        polygon = Polygon([(0, 0), (1, 0), (1, 1), (0, 1)])\n        hyperplane = hyperplane_parameters(polygon)[0]\n        assert hyperplane_intersection(polygon, hyperplane) == [(0.5, 0.5)]\n", "def test_hyperplane_parameters_3d():\n    cube = [[(0, 0, 0), (0, 0, 5), (0, 5, 0), (0, 5, 5), (5, 0, 0),\\\n             (5, 0, 5), (5, 5, 0), (5, 5, 5)],\\\n            [2, 6, 7, 3], [3, 7, 5, 1], [7, 6, 4, 5], [1, 5, 4, 0],\\\n            [3, 1, 0, 2], [0, 4, 6, 2]]\n    vertices = cube[0]\n    faces = cube[1:]\n    hp_params = hyperplane_parameters(faces, vertices)\n\n    assert hp_params[0] == (1, 0, 0, 0)\n    assert hp_params[1] == (0, 1, 0, 0)\n    assert hp_params[2] == (0, 0, 1, 0)\n    assert hp_params[3] == (0, 0, 0, 1) \n\n\n", "def test_hyperplane_parameters():\n    triangle = Polygon(Point(0, 3), Point(5, 3), Point(1, 1))\n    a, b = hyperplane_parameters(triangle)\n    assert a == -1 and b == 3\n", "compilation error"], "sample_216": ["    def test_add_index_with_model_removal(self):\n        before = [\n            ModelState('app', 'article', [\n                ('id', models.AutoField(primary_key=True)),\n                ('title', models.CharField(max_length=200)),\n            ], options={'indexes': [\n                models.Index(fields=['title'], name='article_title_idx'),\n            ]})\n        ]\n        after = [\n            ModelState('app', 'article', [\n                ('id', models.AutoField(primary_key=True)),\n            ])\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['DeleteIndex', 'DeleteModel'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='article_title_idx', model_name='article')\n        self.assertOperationAttributes(changes, 'app', 0, 1, name='article')\n\n", "    def test_change_field_order(self):\n        changes = self.get_changes([self.author_empty], [self.author_with_fields_changed_order])\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['RemoveField', 'AddField', 'AddField'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='biography', model_name='Author')\n        self.assertOperationAttributes(changes, 'testapp', 0, 1, name='name', model_name='Author')\n\n\n\n", "    def test_name_conflict_with_app_label(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('test_app.Person', fields=[])\n            ]\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), '0001_person')\n", "    def test_add_m2m_field_to_existing_model(self):\n        before = [\n            ModelState('app', 'book', [\n                ('id', models.AutoField(primary_key=True)),\n                ('title', models.CharField(max_length=200)),\n            ]),\n        ]\n        after = [\n            ModelState('app', 'book', [\n                ('id', models.AutoField(primary_key=True)),\n                ('title', models.CharField(max_length=200)),\n                ('authors', models.ManyToManyField('app.author')),\n            ]),\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, [\"AddField\"])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='authors', model_name='book')\n", "    def test_rename_field(self):\n        changes = self.get_changes([self.author_empty], [self.author_rename_field])\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"RenameField\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='full_name', new_name='new_full_name')\n"], "sample_91": ["    def test_permission_denied_template_not_found(self):\n        \"\"\"\n        If the custom permission denied template is not found,\n        the 403 Forbidden message should be returned.\n        \"\"\"\n        @override_settings(TEMPLATES=[{\n            'BACKEND': 'django.template.backends.django.DjangoTemplates',\n            'OPTIONS': {\n                'loaders': [\n                    ('django.template.loaders.locmem.Loader', {\n                        '403.html': 'This is a test template for a 403 error',\n                    }),\n                ],\n            },\n        }])\n            response = self.client.get('/permission_denied/')\n            self.assertContains(response, \"403 Forbidden\", status_code=403)\n        test_permission_denied_template_not_found()", "    def test_permission_denied_template_missing(self):\n        \"\"\"\n        If the 403 template is missing, a 403 response with default message is returned\n        \"\"\"\n        with override_settings(TEMPLATES=[]):  # Disable template loading\n            response = self.client.get('/technical403/')\n            self.assertEqual(response.status_code, 403)\n            self.assertContains(response, b'<h1>403 Forbidden</h1>', status_code=403)\n            self.assertContains(response, b'<p>Forbidden</p>', status_code=403)\n", "    def test_permission_denied_context(self):\n        request = self.request_factory.get('/forbidden/')\n        response = permission_denied(request, Exception('Testing technical 404.'))\n        self.assertContains(response, 'Testing technical 404.', status_code=403)\n\n\n\n\n", "    def test_custom_templates_with_context(self):\n        \"\"\"\n        404.html and 500.html templates should use the context provided.\n        \"\"\"\n        response = self.client.get('/server_error/')\n        self.assertContains(response, \"test template for a 500 error\", status_code=500)\n        response = self.client.get('/no_such_url/')\n        self.assertContains(response, 'path: /no_such_url/', status_code=404)\n        self.assertContains(response, 'exception: Resolver404', status_code=404)\n\n\n\n", "    def test_permission_denied_details(self):\n        \"\"\"\n        The permission_denied view passes exception details to the template\n        when a custom template is provided.\n        \"\"\"\n        @override_settings(TEMPLATES=[{'BACKEND': 'django.template.backends.django.DjangoTemplates',\n                                      'OPTIONS': {\n                                          'loaders': [\n                                              ('django.template.loaders.locmem.Loader', {\n                                                  '403.html': 'Permission Denied: {{ exception }}'\n                                              }),\n                                          ],\n                                      }\n                                  }])\n            request = self.request_factory.get('/forbidden/')\n            response = permission_denied(request, Exception('Testing permission denied.'))\n            self.assertContains(response, 'Permission Denied: Testing permission denied.', status_code=403)\n        test_helper()\n"], "sample_1068": ["def test_hypergeometric_function_printing():\n    assert octave_code(hypergeometric2F1(a, b, c, x)) == 'hypergeometric2F1(a, b, c, x)'\n", "def test_polylog():\n    assert octave_code(polylog(x, n)) == 'polylog(n, x)'\n", "def test_polylog():\n    assert octave_code(polylog(n, x)) == 'polylog(n, x)'\n    assert octave_code(polylog(n, x, 2)) == '% Not supported in Octave:\\n% polylog\\npolylog(n, x, 2)'\n\n\n\n\n", "def test_rising_factorial():\n    assert octave_code(RisingFactorial(x, 2)) == 'gamma(x + 2)/gamma(x)'\n    assert octave_code(RisingFactorial(x, 3)) == 'gamma(x + 3)/gamma(x)'\n    assert octave_code(RisingFactorial(x, y)) == 'gamma(x + y)/gamma(x)'\n    assert octave_code(RisingFactorial(x, -y)) == 'gamma(x - y)/gamma(x)'\n\n\n\n", "def test_hypergeometric():\n    assert octave_code(hypergeometric(x, y, z)) == 'hypergeom([x, y], [z], x)'\n\n"], "sample_480": ["    def test_key_expression_transform(self):\n        obj = NullableJSONModel.objects.create(value={\"a\": {\"b\": [1, 2, 3]}})\n        self.assertEqual(\n            NullableJSONModel.objects.annotate(\n                expression=ExpressionWrapper(\n                    KeyTransform(\"a__b\", \"value\"), output_field=IntegerField()\n                )\n            ).filter(expression__gte=1).get(),\n            obj,\n        )\n", "    def test_key_text_transform_nested_lookups(self):\n        qs = NullableJSONModel.objects.annotate(\n            a=KT(\"value__baz__a\"),\n            b=KT(\"value__baz__bax__foo\"),\n        ).filter(a=\"b\", b__contains=\"ar\")\n        self.assertSequenceEqual(qs, [self.objs[7]])\n", "    def test_key_text_transform_from_expression(self):\n        qs = NullableJSONModel.objects.annotate(\n            b=ExpressionWrapper(\n                KT(\"value__bax__foo\"), output_field=models.CharField()\n            )\n        ).filter(b__contains=\"ar\")\n        self.assertSequenceEqual(qs, [self.objs[7]])\n        qs = NullableJSONModel.objects.annotate(\n            c=ExpressionWrapper(\n                KT(\"value__o\"), output_field=models.CharField()\n            )\n        ).filter(c__contains=\"uot\")\n        self.assertSequenceEqual(qs, [self.objs[4]])\n", "    def test_contains_with_complex_key_transforms(self):\n        obj = NullableJSONModel.objects.create(value={\"d\": [{\"f\": {\"g\": \"h\"}}]})\n        tests = [\n            (\n                \"value__d__1__f__g\",\n                \"h\",\n                [obj],\n            ),\n            (\n                \"value__d__1__f__h\",\n                \"h\",\n                [obj],\n            ),\n            (\n                \"value__d__0__f__g\",\n                \"h\",\n                [],\n            ),\n        ]\n        for key_path, value, expected in tests:\n            with self.subTest(key_path=key_path):\n                self.assertSequenceEqual(\n                    NullableJSONModel.objects.filter(\n                        KeyTransform(key_path, \"value\")__contains=value\n                    ),\n                    expected,\n                )\n\n\n", "    def test_key_text_transform_from_expression(self):\n        qs = NullableJSONModel.objects.annotate(\n            b=ExpressionWrapper(KT(\"value__bax__foo\"), output_field=models.CharField()),\n        ).filter(b__contains=\"ar\")\n        self.assertSequenceEqual(qs, [self.objs[7]])\n        qs = NullableJSONModel.objects.annotate(\n            c=ExpressionWrapper(KT(\"value__o\"), output_field=models.CharField()),\n        ).filter(c__contains=\"uot\")\n        self.assertSequenceEqual(qs, [self.objs[4]])\n"], "sample_926": ["compilation error", "        def assert_match(self, expected_classes):\n            assert self.classes == expected_classes\n            for tag, content_classes in self.content_classes.items():\n                assert content_classes == expected_classes\n", "compilation error", "        def get_classes_for_tag(self, tag):\n            return self.content_classes.get(tag, set())\n", "        def assert_classes(self, tag, expected_classes):\n            assert self.content_classes[tag].issubset(expected_classes)\n"], "sample_203": ["    def test_file_upload_validator(self):\n        class MyForm(forms.Form):\n            file_upload = forms.FileField()\n\n        file_upload = SimpleUploadedFile('test.txt', b'test data', content_type='text/plain')\n        with self.subTest(file_upload=file_upload):\n            form = MyForm(data={'file_upload': file_upload})\n            self.assertTrue(form.is_valid())\n\n        with self.subTest(file_upload=SimpleUploadedFile('test.jpg')):\n            form = MyForm(data={'file_upload': SimpleUploadedFile('test.jpg', b'test data', content_type='image/jpeg')})\n            self.assertTrue(form.is_valid())\n\n        with self.subTest(file_upload=SimpleUploadedFile('test.pdf')):\n            form = MyForm(data={'file_upload': SimpleUploadedFile('test.pdf', b'test data', content_type='application/pdf')})\n            self.assertTrue(form.is_valid())\n\n        with self.subTest(invalid_extension):\n            form = MyForm(data={'file_upload': SimpleUploadedFile('test.bad', b'test data', content_type='text/plain')})\n            self.assertFalse(form.is_valid())\n            self.assertIn('invalid_extension', form.errors['file_upload'])\n", "    def test_file_extension_validator(self):\n        cases = [\n            ('not_allowed.txt', 'invalid_extension'),\n            ('allowed.jpg', None),\n            ('allowed.png', None),\n            ('allowed.gif', None),\n        ]\n\n        for filename, expected_code in cases:\n            with self.subTest(filename=filename):\n                class MyForm(forms.Form):\n                    file = forms.FileField()\n\n                uploaded_file = SimpleUploadedFile(filename, b'test content', content_type='image/jpeg')\n                form = MyForm({'file': uploaded_file})\n                if expected_code is not None:\n                    self.assertIs(form.is_valid(), False)\n                    self.assertEqual(form.errors['file'], ['File extension \u201c' + filename[filename.rfind('.') + 1:].lower() +\n                                                        '\u201d is not allowed. '\n                                                        'Allowed extensions are: [' + ', '.join(['jpg', 'png', 'gif']) +\n                                                        '].'])\n                else:\n                    self.assertIs(form.is_valid(), True)\n\n", "    def test_file_extension_validator(self):\n        cases = [\n            ('test.txt', 'txt'),\n            ('test.png', 'png'),\n            ('test.jpg', 'jpg'),\n            ('test.pdf', 'pdf'),\n            ('test.doc', 'doc'),\n            ('test.invalid', None),\n            ('test.gif', 'gif'),\n            ('test.jpeg', 'jpeg'),\n        ]\n        for filename, allowed_extension in cases:\n            with self.subTest(filename=filename):\n                upload = SimpleUploadedFile(filename, b'test')\n                validator = FileExtensionValidator(allowed_extensions=['txt', 'png', 'jpg'])\n                if allowed_extension is not None:\n                    self.assertIsNotNone(validator(upload))\n                else:\n                    with self.assertRaises(ValidationError):\n                        validator(upload)\n\n\n\n", "    def test_custom_message_with_file_validator(self):\n        class MyForm(forms.Form):\n            file = forms.FileField(\n                validators=[\n                    FileExtensionValidator(\n                        allowed_extensions=['.jpg', '.png', '.gif'],\n                        message=\"Only image files are allowed (jpg, png, gif).\",\n                    ),\n                ]\n            )\n\n        with self.assertRaises(ValidationError) as e:\n            MyForm({'file': SimpleUploadedFile('test.txt', b'test')}).clean()\n        self.assertEqual(e.exception.messages['file'], 'Only image files are allowed (jpg, png, gif).')\n\n", "    def test_file_extension_validator(self):\n        with self.subTest('Invalid extension'):\n            form = forms.Form({'file': SimpleUploadedFile('my_file.tmp', b'content', content_type='text/plain')})\n            validator = FileExtensionValidator(allowed_extensions=['txt'])\n            with self.assertRaises(ValidationError) as e:\n                validator(form.cleaned_data['file'])\n            self.assertEqual(e.exception.args[0], 'Invalid extension \"tmp\" is not allowed. Allowed extensions are: txt.')\n\n        with self.subTest('Valid extension'):\n            form = forms.Form({'file': SimpleUploadedFile('my_file.txt', b'content', content_type='text/plain')})\n            validator = FileExtensionValidator(allowed_extensions=['txt'])\n            validator(form.cleaned_data['file'])\n\n\n\n"], "sample_658": ["    def test_doctest_broken_objects(self, testdir):\n        testdir.makeconftest(\n            \"\"\"\n            import pytest\n\n            class Mock(Broken):\n                    pass\n\n        \"\"\"\n        )\n        testdir.makepyfile(\n            \"\"\"\n            from .Mock import Mock\n\n            class Example:\n                '''\n                >>> obj = Mock()\n                >>> obj.some_attribute\n                '''\n            \"\"\"\n        )\n        result = testdir.runpytest(\"--doctest-modules\")\n        result.stdout.fnmatch_lines([\"*1 failed*\"])\n", "    def test_doctest_allow_exception(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n                try:\n                    raise ValueError('expected')\n                except ValueError as e:\n                    return e.args[0]\n            \n                '''\n                >>> foo()\n                'expected'\n                '''\n                pass\n        \"\"\"\n        )\n        reprec = testdir.inline_run(\"--doctest-modules\")\n        reprec.assertoutcome(passed=1)\n", "    def test_doctest_broken_object(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            class Broken(object):\n                    raise KeyError(\"This should be an AttributeError\")\n\n                '''\n                >>> b = Broken()\n                >>> b.some_attr\n                KeyError(\"This should be an AttributeError\")\n                '''\n        \"\"\"\n        )\n        result = testdir.runpytest(\"--doctest-modules\")\n        result.stdout.fnmatch_lines(\n            [\"*KeyError('This should be an AttributeError')*\"]\n        )\n", "    def test_doctest_with_broken_object(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            class Broken(object):\n                    raise KeyError(\"This should be an AttributeError\")\n\n                '''\n                >>> try:\n                ...     broken = Broken()\n                ...     broken.nonexistent_attr\n                ... except AttributeError as e:\n                ...     assert str(e) == \"AttributeError: 'Broken' object has no attribute 'nonexistent_attr'\"\n                '''\n            \"\"\"\n        )\n        result = testdir.runpytest(\"--doctest-modules\")\n        result.stdout.fnmatch_lines([\"* 1 passed *\"]) \n", "    def test_doctest_broken_getattr(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            class Broken:\n                    raise KeyError(\"This should be an AttributeError\")\n\n                >>> b = Broken()\n                >>> b.nonexistent_attr\n                KeyError(\"This should be an AttributeError\")\n        \"\"\"\n        )\n        result = testdir.runpytest(\"--doctest-modules\")\n        result.stdout.fnmatch_lines([\"* 1 passed *\"])\n"], "sample_672": ["def test_cyclic_references():\n    class A:\n            self.b = b\n\n    class B:\n            self.a = a\n\n    a = A(B(a))\n    b = B(a)\n    assert saferepr(a) and saferepr(b) and all(\n        c in saferepr(o) for o in (a, b) for c in (\"<[\", \"object\", \">\")\n    )\n", "def test_recursive_repr():\n    class Recursive:\n            self.value = value\n            self.next = self if value else None\n\n            return f\"Recursive({self.value}, {self.next})\"\n\n    r = Recursive(1)\n    r.next = r\n    assert saferepr(r) == \"<[Recursive(1, Recursive(1, Recursive(1,...))] object at 0x...>\" \n", "def test_circular_references():\n    class A:\n            self.b = b\n\n    class B:\n            self.a = a\n\n    a = A(B(None))\n    b = B(a)\n    a.b = b\n    assert saferepr(a) != \"a <Circular>\"\n", "def test_cyclic_repr():\n    class A:\n            self.b = b\n\n    class B:\n            self.a = a\n\n    a = A(B(a))\n    b = B(a)\n    s = saferepr(a)\n    assert \"cyclic reference\" in s\n", "def test_saferepr_with_cyclic_references():\n    class A:\n            self.b = B()\n\n    class B:\n            self.a = A()\n\n    try:\n        saferepr(A())\n    except Exception as e:\n        assert \"cyclic\" in str(e)\n"], "sample_778": ["def test_nmf_nndsvd_random_state():\n    rng = np.random.RandomState(42)\n    X = rng.randn(10, 10)\n    nmf1 = NMF(n_components=5, init='nndsvd', random_state=0)\n    nmf2 = NMF(n_components=5, init='nndsvd', random_state=0)\n\n    W1 = nmf1.fit_transform(X)\n    W2 = nmf2.fit_transform(X)\n\n    assert_array_almost_equal(W1, W2, decimal=10)\n\n    nmf3 = NMF(n_components=5, init='nndsvd', random_state=1)\n    W3 = nmf3.fit_transform(X)\n\n    assert_array_almost_equal(W1, W3, decimal=10)\n", "def test_nmf_random_state():\n    n_samples = 5\n    n_features = 3\n    n_components = 2\n    random_state = 42\n    rng = np.random.RandomState(random_state)\n    X = rng.randn(n_samples, n_features)\n\n    # Test that random initialization is the same for a given random_state\n\n    model1 = NMF(n_components=n_components, random_state=random_state)\n    model1.fit(X)\n    W1 = model1.components_\n    H1 = model1.transform(X)\n\n    model2 = NMF(n_components=n_components, random_state=random_state)\n    model2.fit(X)\n    W2 = model2.components_\n    H2 = model2.transform(X)\n\n    assert_array_almost_equal(W1, W2, decimal=10)\n    assert_array_almost_equal(H1, H2, decimal=10)\n\n    # Test that different random_states lead to different initialization\n\n    random_state2 = 43\n    model3 = NMF(n_components=n_components, random_state=random_state2)\n    model3.fit(X)\n    W3 = model3.components_\n    H3 = model3.transform(X)\n\n    assert_array_not_equal(W1, W3, exact=False)\n    assert_array_not_equal(H1, H3, exact=False)\n\n\n", "def test_nmf_sparse_input_custom_init():\n    # Test that NMF with custom initialization works with sparse input\n\n    # Smoke test for the case of more components than features.\n    rng = np.random.mtrand.RandomState(42)\n    A = np.abs(rng.randn(30, 10))\n    A[:, 2 * np.arange(5)] = 0\n    A_sparse = csc_matrix(A)\n\n    n_components = 15\n    init_W = np.abs(rng.randn(30, n_components))\n    init_H = np.abs(rng.randn(n_components, 10))\n\n    for solver in ('cd', 'mu'):\n        model = NMF(solver=solver, n_components=n_components,\n                    init=('custom', init_W, init_H),\n                    random_state=0, tol=1e-2)\n        model.fit_transform(A_sparse)\n", "def test_nmf_early_stopping():\n    n_samples = 100\n    n_features = 50\n    n_components = 10\n    tol = 1e-4\n    max_iter = 1000\n\n    rng = np.random.mtrand.RandomState(42)\n    X = rng.randn(n_samples, n_features)\n    X = np.abs(X)\n    W0, H0 = nmf._initialize_nmf(X, n_components, init='random', random_state=42)\n\n    # Test early stopping by setting a relatively low tolerance\n    model = NMF(n_components=n_components, solver='mu',\n                tol=tol, max_iter=max_iter, random_state=42)\n    W, H, _ = model.fit(X, W0, H0)\n\n    assert_less(W.shape[1], n_components)\n    assert_less(H.shape[0], n_samples)\n    \n\n    # Test that the number of iterations is less than max_iter\n    assert_less(model.n_iter_, max_iter)\n\n    # Test that the reconstruction error is below the tolerance\n    reconstruction_error = _beta_divergence_dense(X, W, H, beta_loss=2) #Assuming beta_loss is 2 \n    assert_less(reconstruction_error, tol)\n\n", "def test_nmf_invalid_init():\n    # Test that an error is raised if init is not a valid string\n    # or a valid array\n    n_samples = 6\n    n_features = 5\n    n_components = 3\n    rng = np.random.mtrand.RandomState(42)\n    X = rng.randn(n_samples, n_features)\n\n    for invalid_init in (123, {'a':'b'}, True, None):\n        with pytest.raises(ValueError):\n            nmf.NMF(n_components=n_components, init=invalid_init)\n\n\n\n"], "sample_813": ["compilation error", "def test_ard_threshold_lambda():\n    # Test the effect of threshold_lambda on the number of active weights\n    X = np.random.rand(10, 5)\n    y = np.random.rand(10)\n    \n    # Test with a high threshold lambda\n    clf1 = ARDRegression(threshold_lambda=1e6)\n    clf1.fit(X, y)\n    assert_equal(np.sum(clf1.lambda_ > 1e-6), X.shape[1])\n\n    # Test with a low threshold lambda\n    clf2 = ARDRegression(threshold_lambda=1e-6)\n    clf2.fit(X, y)\n    assert_equal(np.sum(clf2.lambda_ > 1e-6), 0) \n", "compilation error", "compilation error", "compilation error"], "sample_873": ["def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n    feature_names_out_actual = sel.get_feature_names_out(input_features=feature_names)\n    assert_array_equal(feature_names_t, feature_names_out_actual)\n\n\n", "def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n    assert_array_equal(feature_names_t, sel.get_feature_names_out(input_features=feature_names))\n    assert_array_equal(feature_names_t, sel.get_feature_names_out())\n    with pytest.raises(ValueError):\n        sel.get_feature_names_out(input_features=list(\"ABCDEF\"))\n", "def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n    assert_array_equal(feature_names_t, sel.get_feature_names_out(input_features=feature_names))\n\n", "def test_get_feature_names_out():\n    sel = StepSelector(step=2)\n    sel.fit(X, y)\n    assert_array_equal(feature_names_t, sel.get_feature_names_out(feature_names))\n    assert_array_equal(feature_names_t, sel.get_feature_names_out())\n", "def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n    assert_array_equal(feature_names_t, sel.get_feature_names_out(input_features=feature_names))\n"], "sample_744": ["def test_power_transformer_missing_values():\n    X = np.abs(X_2d)\n    X[0, 0] = np.nan\n    X[1, 1] = np.nan\n    for standardize in [True, False]:\n        pt = PowerTransformer(method='box-cox', standardize=standardize)\n        with np.errstate(divide='ignore', invalid='ignore'):\n            X_trans = pt.fit_transform(X)\n\n        assert_raises(ValueError, pt.inverse_transform, X_trans)\n\n\n\n", "def test_power_transformer_custom_func():\n        return x ** 2\n\n    pt = PowerTransformer(method=custom_func)\n    X = np.abs(X_1col)\n\n    X_trans = pt.fit_transform(X)\n    assert_almost_equal(np.square(X), X_trans)\n\n    assert_almost_equal(X, pt.inverse_transform(X_trans))\n", "compilation error", "def test_power_transformer_y_data():\n    X = np.abs(X_1col)\n    pt = PowerTransformer(method='box-cox')\n    pt.fit(X)\n\n    y = np.random.rand(X.shape[0])\n    assert_raises(ValueError, pt.fit, y)\n    assert_raises(ValueError, pt.transform, y)\n    assert_raises(ValueError, pt.inverse_transform, y)\n", "def test_power_transformer_y_scale():\n    X = np.abs(X_1col)\n\n    for standardize in [True, False]:\n        pt = PowerTransformer(method='box-cox', standardize=standardize)\n        y = np.random.randn(X.shape[0])\n\n        if standardize:\n            pt.fit(X)\n            assert_raises(ValueError, pt.transform, y)\n        else:\n            pt.fit(X, y=y)\n            pt_transformed_y = pt.transform(y)\n            assert pt_transformed_y.shape == (X.shape[0], 1)\n\n\n\n"], "sample_118": ["    def test_nested_outerref_rhs(self):\n        tag = Tag.objects.create(name=self.au1.name)\n        tag.articles.add(self.a1)\n        qs = Tag.objects.annotate(\n            author_exists=Exists(\n                Article.objects.filter(\n                    author__name=OuterRef('name')\n                )\n            ),\n        )\n        self.assertEqual(qs.get(author_exists=True), tag)\n", "    def test_nested_outerref_rhs(self):\n        author = Author.objects.create(name='John Smith', alias='Johx')\n        author2 = Author.objects.create(name='Jane Doe', alias='Doe')\n        Article.objects.create(author=author, headline='Article 1')\n        Article.objects.create(author=author2, headline='Article 2')\n        \n        qs = Article.objects.filter(author=OuterRef('author_id'))\n        tags = Tag.objects.annotate(\n            article_exists=Exists(qs.filter(author__alias=OuterRef('name')))\n        )\n        self.assertEqual(tags.filter(article_exists=True).count(), 1)\n\n\n\n", "    def test_outerref_with_related_fields(self):\n        tag = Tag.objects.create(name=self.au1.alias)\n        tag.articles.add(self.a1)\n        qs = Tag.objects.annotate(\n            author_name=OuterRef('articles__author__name'),\n        ).filter(author_name=self.au1.name)\n        self.assertEqual(qs.count(), 1)\n        self.assertEqual(qs.get().name, self.au1.alias)\n", "    def test_nested_outerref_rhs(self):\n        tag = Tag.objects.create(name=self.au1.alias)\n        tag.articles.add(self.a1)\n        qs = Article.objects.annotate(\n            tag_exists=Exists(\n                Tag.objects.filter(name=OuterRef('headline'))\n            ),\n        ).filter(tag_exists=True)\n        self.assertEqual(qs.count(), 1)\n\n\n", "    def test_nested_outerref_rhs(self):\n        a1 = Article.objects.create(headline='1', author=self.au1)\n        a2 = Article.objects.create(headline='2', author=self.au2)\n        tag1 = Tag.objects.create(name=self.au1.alias)\n        tag2 = Tag.objects.create(name=self.au2.alias)\n        tag1.articles.add(a1)\n        tag2.articles.add(a2)\n        qs = Tag.objects.annotate(\n            has_article_from_author=Exists(\n                Article.objects.filter(author=OuterRef('name'))\n            ),\n        ).filter(has_article_from_author=True)\n        self.assertCountEqual(qs, [tag1, tag2])\n"], "sample_437": ["    def test_health_checks_close_at(self):\n        self.patch_settings_dict(conn_health_checks=True)\n        self.assertIsNone(connection.connection)\n        with patch.object(\n            connection, \"is_usable\", side_effect=AssertionError\n        ) as mocked_is_usable:\n            # Simulate request_finished.\n            self.patch.object(connection, \"close_at\", \"now() + timedelta(seconds=5)\")\n            self.run_query()\n\n        old_connection = connection.connection\n        time.sleep(4)\n        # Simulate request_finished.\n        connection.close_if_unusable_or_obsolete()\n        # A new connection is established after 5 seconds\n        new_connection = connection.connection\n        self.assertNotIs(old_connection, new_connection)\n\n\n\n\n\n", "    def test_health_checks_disabled_with_conn_max_age(self):\n        self.patch_settings_dict(conn_health_checks=False)\n        self.assertIsNone(connection.connection)\n        # Newly created connections are considered healthy without performing\n        # the health check.\n        with patch.object(connection, \"is_usable\", side_effect=AssertionError):\n            connection.settings_dict[\"CONN_MAX_AGE\"] = 1\n            self.run_query()\n            \n\n        old_connection = connection.connection\n        # Simulate request_finished.\n        connection.close_if_unusable_or_obsolete()\n        # Persistent connections are enabled (connection is not).\n        self.assertIs(old_connection, connection.connection)\n        # Health checks are not performed.\n        with patch.object(connection, \"is_usable\", side_effect=AssertionError):\n            self.run_query()\n            # Health check wasn't performed and the connection is unchanged.\n            self.assertIs(old_connection, connection.connection)\n            self.run_query()\n            # The connection is unchanged after the next query either during\n            # the current \"request\".\n            self.assertIs(old_connection, connection.connection)\n", "    def test_health_checks_conn_max_age(self):\n        self.patch_settings_dict(conn_health_checks=True)\n        self.assertIsNone(connection.connection)\n        # Newly created connections are considered healthy without performing\n        # the health check.\n        # Simulate connection with CONN_MAX_AGE set.\n        self.settings_dict_patcher.update({\"CONN_MAX_AGE\": 10})\n        with patch.object(connection, \"is_usable\", side_effect=AssertionError):\n            self.run_query()\n\n        old_connection = connection.connection\n        time.sleep(11)  # Wait longer than CONN_MAX_AGE\n        # Simulate request_finished.\n        connection.close_if_unusable_or_obsolete()\n        # A new connection is established due to exceeding CONN_MAX_AGE.\n        self.assertNotSame(old_connection, connection.connection)\n\n\n\n", "    def test_health_checks_enabled_autocommit_changed(self):\n        self.patch_settings_dict(conn_health_checks=True)\n        self.assertIsNone(connection.connection)\n        # Newly created connections are considered healthy without performing\n        # the health check.\n        with patch.object(connection, \"is_usable\", side_effect=AssertionError):\n            # Simulate outermost atomic block: changing autocommit for\n            # a connection.\n            with connection.atomic():\n                connection.set_autocommit(False)\n                self.run_query()\n                connection.commit()\n                connection.set_autocommit(True)\n\n        old_connection = connection.connection\n        # Simulate request_finished.\n        connection.close_if_unusable_or_obsolete()\n        # Persistent connections are enabled.\n        self.assertIs(old_connection, connection.connection)\n\n        # Simulate connection health check failing.\n        with patch.object(\n            connection, \"is_usable\", return_value=False\n        ) as mocked_is_usable:\n            # Simulate outermost atomic block: changing autocommit for\n            # a connection.\n            with connection.atomic():\n                connection.set_autocommit(False)\n                new_connection = connection.connection\n                self.assertIsNot(new_connection, old_connection)\n                # Only one health check per \"request\" is performed, so a query will\n                # carry on even if the health check fails. This query succeeds\n                # because the real connection is healthy and only the health check\n                # failure is mocked.\n                self.run_query()\n                connection.commit()\n                connection.set_autocommit(True)\n                # The connection is unchanged.\n                self.assertIs(new_connection, connection.connection)\n        self.assertEqual(mocked_is_usable.call_count, 1)\n\n        # Simulate request_finished.\n        connection.close_if_unusable_or_obsolete()\n        # The underlying connection is being reused further with health checks\n        # succeeding.\n        connection.set_autocommit(False)\n        self.run_query()\n        connection.commit()", "    def test_connection_close_at_not_used(self):\n        self.patch_settings_dict(conn_health_checks=True)\n        self.assertIsNone(connection.connection)\n        # Newly created connections are considered healthy without performing\n        # the health check.\n        with patch.object(connection, \"is_usable\", side_effect=AssertionError):\n            self.run_query()\n\n        old_connection = connection.connection\n        # Simulate the connection being 'closed' (but not actually closed)\n        # because it's about to be used in a new request\n        connection.close_at = time.monotonic() + 1\n\n        # Simulate request_started.\n        connection.close_if_unusable_or_obsolete()\n        self.assertIs(old_connection, connection.connection)\n\n        # Simulate a new \"request\" starts.\n\n        # The connection should be closed, even though it's healthy,\n        # because the `close_at` timestamp has been exceeded.\n        connection.close_if_unusable_or_obsolete()\n        new_connection = connection.connection\n        self.assertIsNot(old_connection, new_connection)\n"], "sample_970": ["compilation error", "compilation error", "compilation error", "compilation error", "def test_is_builtin_instance_method():\n    class MyInt(int):\n            pass\n\n    assert inspect.isbuiltin(MyInt().my_method) is False\n"], "sample_64": ["    def test_load_invalid_cookies(self):\n        # Test handling of invalid cookies (attributes not supported)\n        self.assertEqual(parse_cookie('name=value; foo=bar; baz=qux'), {'name': 'value', 'foo': 'bar', 'baz': 'qux'})\n", "    def test_load_mixed_data(self):\n        c = SimpleCookie()\n        c.load(\"\"\"name=val\n        ; foo=bar\n        ;  baz=qux \"\"\")\n        self.assertEqual(c['name'].value, 'val')\n        self.assertEqual(c['foo'].value, 'bar')\n        self.assertEqual(c['baz'].value, 'qux')\n", "    def test_load_invalid_cookie_types(self):\n        with self.assertRaises(TypeError):\n            SimpleCookie().load([1, 2, 3])\n        with self.assertRaises(TypeError):\n            SimpleCookie().load({'name': 123})\n        with self.assertRaises(TypeError):\n            SimpleCookie().load(None)\n        with self.assertRaises(TypeError):\n            SimpleCookie().load(set())\n", "    def test_cookie_with_unicode_value(self):\n        c = SimpleCookie()\n        c['name'] = '\u00e9\u00e0\u00e7\u00fc\u00f6'\n        self.assertEqual(c['name'].value, '\u00e9\u00e0\u00e7\u00fc\u00f6')\n        self.assertEqual(c.output(), \"name=%C3%A9%C3%A0%C3%A7%C3%BC%C3%B6\") \n\n", "    def test_max_age(self):\n        c = SimpleCookie()\n        c['name'] = 'val'\n        c['name']['max-age'] = 3600\n        self.assertEqual(c['name']['max-age'], 3600)\n        self.assertIn('Max-Age=3600', c.output())\n"], "sample_1097": ["def test_block_collapse_irregular_shape():\n    A = MatrixSymbol('A', 2, 3)\n    B = MatrixSymbol('B', 3, 2)\n    C = MatrixSymbol('C', 3, 3)\n    M = BlockMatrix([[A, B], [C, ZeroMatrix(3, 2)]])\n    assert block_collapse(M).blocks[0, 0] == A\n    assert block_collapse(M).blocks[0, 1] == B\n    assert block_collapse(M).blocks[1, 0] == C\n    assert block_collapse(M).blocks[1, 1] == ZeroMatrix(3, 2)\n", "compilation error", "def test_block_collapse_mixed_types():\n    A = MatrixSymbol('A', 2, 2)\n    B = ZeroMatrix(2, 2)\n    C = Matrix([[1, 2], [3, 4]])\n    X = BlockMatrix([[A, B], [C, B]])\n    assert block_collapse(X) == BlockMatrix([[A, C], [B, B]])\n", "def test_bc_inverse_1x1():\n    A = MatrixSymbol('A', 2, 2)\n    B = BlockMatrix([[A]])\n    assert blockinverse_1x1(Inverse(B)).blockshape == (1, 1)\n    assert blockinverse_1x1(Inverse(B)).blocks[0, 0] == A.I\n\n\n\n", "def test_block_collapse_structure():\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, m)\n    C = MatrixSymbol('C', m, n)\n    D = MatrixSymbol('D', m, m)\n    X = BlockMatrix([[A, B], [C, D]])\n\n    assert block_collapse(X.transpose()).structure == X.transpose().structure\n    assert block_collapse(X.inverse()).structure == X.inverse().structure\n    assert block_collapse(X + X).structure == X.structure + X.structure\n    assert block_collapse(X*X).structure == X.structure * X.structure\n"], "sample_836": ["compilation error", "def test_ovr_decision_function():\n    # Test with binary classifiers\n    predictions = np.array([[0, 1], [1, 0], [1, 0], [0, 1]])\n    confidences = np.array([[0.1, 0.9], [0.9, 0.1], [0.9, 0.1], [0.1, 0.9]])\n    n_classes = 2\n\n    expected_decision_function = np.array([[ -0.8,  0.8],\n                                            [  0.8, -0.8],\n                                            [  0.8, -0.8],\n                                            [ -0.8,  0.8]])\n    \n\n    decision_function = _ovr_decision_function(predictions, confidences, n_classes)\n\n    assert_array_almost_equal(decision_function, expected_decision_function)\n", "def test_ovr_decision_function():\n    # Check that the ovr decision function works as expected\n    predictions = np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]])\n    confidences = np.array([[0.2, 0.8, 0.1], [0.9, 0.1, 0.7], [0.3, 0.5, 0.2]])\n    n_classes = 3\n    decision_function = _ovr_decision_function(predictions, confidences, n_classes)\n    # Check that the decision function is correctly computed\n    expected_decision_function = np.array([\n        [-1.0, 0.8, -1.0],\n        [1.0, -1.0, 0.7],\n        [-1.0, 0.8, -1.0]\n    ])\n    assert_array_almost_equal(decision_function, expected_decision_function)\n", "def test_ovr_decision_function_with_multiclass():\n    predictions = np.array([\n        [0, 1, 0],\n        [1, 0, 0],\n        [0, 1, 1],\n        [0, 0, 1],\n        [0, 1, 0],\n        [1, 0, 0]\n    ])\n    confidences = np.array([\n        [0.1, 0.9, 0.0],\n        [0.9, 0.1, 0.0],\n        [0.0, 0.9, 0.1],\n        [0.0, 0.1, 0.9],\n        [0.1, 0.9, 0.0],\n        [0.9, 0.1, 0.0]\n    ])\n    n_classes = 3\n    result = _ovr_decision_function(predictions, confidences, n_classes)\n    expected_result = np.array([\n        [0, 1, 0],\n        [1, 0, 0],\n        [0, 1, 1],\n        [0, 0, 1],\n        [0, 1, 0],\n        [1, 0, 0]\n    ])\n    assert_array_equal(result, expected_result)\n\n\n\n", "def test_ovr_decision_function():\n    # Test the OVR decision function with a variety of examples\n    for group, group_examples in EXAMPLES.items():\n        if group in ['multilabel-indicator', 'unknown']:\n            continue\n        for example in group_examples:\n            n_classifiers = (len(group_examples[0]) *\n                             (len(group_examples[0]) - 1) // 2)\n            predictions = np.random.randint(0, 2, size=(len(example),\n                                                    n_classifiers))\n            confidences = np.random.rand(len(example), n_classifiers)\n            n_classes = len(set(np.unique(example)))\n\n            decisions = _ovr_decision_function(predictions, confidences,\n                                                n_classes)\n\n            # Compare with a trivial implementation for validation\n            trivial_decisions = np.zeros(decisions.shape)\n            for i in range(n_classes):\n                for j in range(i + 1, n_classes):\n                    trivial_decisions[:, i] += np.where(\n                        predictions[:, i * (n_classes - 1) // 2 + j] == 1,\n                        confidences[:, i * (n_classes - 1) // 2 + j], 0)\n                    trivial_decisions[:, j] -= np.where(\n                        predictions[:, i * (n_classes - 1) // 2 + j] == 1,\n                        confidences[:, i * (n_classes - 1) // 2 + j], 0)\n\n            assert_array_almost_equal(decisions, trivial_decisions)\n\n\n"], "sample_602": ["def test_to_netcdf_compute_false():\n    ds = xr.Dataset(\n        data_vars={\"a\": ((\"x\", \"y\"), np.arange(12).reshape(3, 4))},\n        coords={\"x\": np.arange(3), \"y\": np.arange(4)},\n    )\n    dataset_delayed = to_netcdf(ds, compute=False)\n    assert isinstance(dataset_delayed, dask.delayed.Delayed)\n    result = dataset_delayed.compute()\n    assert isinstance(result, bytes)\n\n\n\n", "def test_to_netcdf_with_compute_false():\n    ds = xr.Dataset(\n        data_vars=dict(a=np.arange(10)),\n        coords={\"x\": np.arange(10)},\n    )\n\n        pass\n\n    with patch.object(backends.NetCDF4Store, \"close\", mock_close):\n        delayed_result = to_netcdf(\n            ds,\n            engine=\"netcdf4\",\n            compute=False,\n        )\n\n        assert isinstance(delayed_result, object)\n        assert delayed_result.compute() == bytes\n\n\n", "def test_to_netcdf_with_compute_False():\n    ds = xr.Dataset(\n        data_vars={\"a\": ((\"x\", \"y\"), np.random.rand(5, 5))},\n        coords={\"x\": np.arange(5), \"y\": np.arange(5)},\n    )\n    f = BytesIO()\n\n    writes = to_netcdf(ds, target=f, compute=False)\n    writes = [w.compute() for w in writes]\n    assert isinstance(writes, list)\n    assert len(writes) == 1\n    assert isinstance(writes[0], bytes)\n\n\n\n", "def test_to_netcdf_with_compute_false():\n        ds['a'] = ds['a'] ** 2\n        return ds\n\n    dataset = xr.Dataset(\n        data_vars={'a': ((\"x\",), np.arange(5))},\n        coords={'x': np.arange(5)},\n    )\n\n    # Compute the result lazily\n    delayed_result = to_netcdf(dataset, None, compute=False, encoding={'a': {'zlib': True}})  \n\n    # Assert that the result hasn't been computed yet\n    assert not delayed_result.getvalue()\n\n    # Now, compute the result explicitly\n    computed_result = delayed_result.compute()\n\n    # Check if the result is as expected\n    assert isinstance(computed_result, bytes)\n    assert computed_result\n\n\n\n\n", "def test_to_zarr_append_dim():\n    ds = xr.Dataset(\n        \n        {\"a\": ((\"x\", \"y\"), np.arange(10).reshape((2, 5)))},\n        coords={\"x\": np.arange(2), \"y\": np.arange(5)},\n    )\n    store = xr.open_zarr(\"/tmp/test_append_dim\", mode=\"w\")\n\n    ds.to_zarr(store, mode=\"a\", append_dim=\"y\")  # Append along y\n\n    read_ds = xr.open_zarr(\"/tmp/test_append_dim\")\n    assert len(read_ds.a) == 10\n    assert read_ds.dims == (\"x\", \"y\")\n\n    store.close()\n\n\n\n"], "sample_702": ["def test_testdir_makefile_ext_empty_string_makes_file(testdir) -> None:\n    \"\"\"For backwards compat #8192\"\"\"\n    p1 = testdir.makefile(\"\", \"\")\n    assert \"test_testdir_makefile\" in str(p1)\n", "compilation error", "def test_testdir_makefile_ext_empty_string_makes_file(testdir) -> None:\n    \"\"\"For backwards compat #8192\"\"\"\n    p1 = testdir.makefile(\"\", \"\")\n    assert \"test_testdir_makefile\" in str(p1)\n", "compilation error", "def test_testdir_makefile_ext_invalid_raises_error(testdir) -> None:\n    with pytest.raises(ValueError, match=r\"Invalid file extension: \\.\\*\"):\n        testdir.makefile(\"foo.*\", \"\")\n"], "sample_357": ["    def test_mixed_operations(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Person', fields=[]),\n                migrations.RemoveField('Person', 'name'),\n                migrations.AlterField('Person', 'age', Field(...)),\n            ]\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'person_remove_name_alter_age')\n", "    def test_no_changes_no_migration_name(self):\n        before = [ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200)),\n        ])]\n        after = before.copy()\n        changes = self.get_changes(before, after)\n        migration = Migration('test_migration', 'test_app')\n        self.assertIs(migration.suggest_name(), '')\n\n", "    def test_operation_with_no_name_but_field_change(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.AddField(\n                    model_name='Person',\n                    name='age',\n                    field=models.IntegerField(default=0),\n                ),\n            ]\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'add_age')\n", "    def test_operation_with_custom_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Person', fields=[]),\n                migrations.RenameField(model_name='Person', old_name='id', new_name='user_id'),\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'person_rename_id_to_user_id')\n", "    def test_alter_field_to_null_non_null_field(self):\n        before = [\n            ModelState('app', 'Author', [\n                ('id', models.AutoField(primary_key=True)),\n                ('name', models.CharField(max_length=200, null=False)),\n            ]),\n        ]\n        after = [\n            ModelState('app', 'Author', [\n                ('id', models.AutoField(primary_key=True)),\n                ('name', models.CharField(max_length=200, null=True)),\n            ]),\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['AlterField'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='name')\n"], "sample_1074": ["def test_is_isomorphic():\n    G = PermutationGroup(Permutation(0,1,2), Permutation(0,2,3))\n    H = PermutationGroup(Permutation(1,2,0), Permutation(2,3,0))\n    assert G.is_isomorphic(H)\n\n    G = SymmetricGroup(3)\n    H = PermutationGroup(Permutation(0,1), Permutation(0,2))\n    assert not G.is_isomorphic(H)\n    \n    G = AlternatingGroup(4)\n    H = AlternatingGroup(4)\n    assert G.is_isomorphic(H)\n    G = SymmetricGroup(4)\n    H = AlternatingGroup(4)\n    assert not G.is_isomorphic(H)\n", "def test_is_solvable():\n    # A_5 is not solvable\n    assert AlternatingGroup(5).is_solvable is False\n\n    # all abelian groups are solvable\n    for i in (1, 2, 3):\n        C = CyclicGroup(i)\n        Ab = AbelianGroup(i, i + 2)\n        assert C.is_solvable\n\n    # symmetric groups of order less than 6 are solvable\n    for i in (1, 2, 3, 4, 5):\n        S = SymmetricGroup(i)\n        assert S.is_solvable\n    # the dihedral group D_6 is solvable\n    assert DihedralGroup(6).is_solvable\n\n    # the dihedral group D_7 is solvable\n    assert DihedralGroup(7).is_solvable\n\n    # the dihedral group D_8 is solvable\n    assert DihedralGroup(8).is_solvable\n\n    # a non-solvable group constructed manually should not be solvable\n    G = PermutationGroup(Permutation(0, 1, 2, 3, 4, 5)(6, 7), Permutation(1, 2, 3, 4),\n    Permutation(0, 7, 5, 6, 1, 2, 3, 4))\n    assert not G.is_solvable\n\n    # the alternating group A_4 is not solvable\n    assert AlternatingGroup(4).is_solvable is False\n\n\n", "def test_is_isomorphic():\n    G = SymmetricGroup(4)\n    H = AlternatingGroup(4)\n    assert not G.is_isomorphic(H)\n    G = SymmetricGroup(3)\n    H = AbelianGroup(3)\n    assert not G.is_isomorphic(H)\n    G = AbelianGroup(4)\n    H = AbelianGroup(4)\n    assert G.is_isomorphic(H)\n    G = DihedralGroup(4)\n    H = DihedralGroup(4)\n    assert G.is_isomorphic(H)\n\n\n", "def test_conjugacy_classes():\n    G = SymmetricGroup(4)\n    assert len(G.conjugacy_classes()) == 5\n    A = AlternatingGroup(4)\n    assert len(A.conjugacy_classes()) == 3\n    D = DihedralGroup(4)\n    assert len(D.conjugacy_classes()) == 4\n    C = CyclicGroup(7)\n    assert len(C.conjugacy_classes()) == 1\n\n", "def test_is_soluble():\n    # every abelian group is soluble\n    for i in (1, 2, 3):\n        C = CyclicGroup(i)\n        Ab = AbelianGroup(i, i + 2)\n        assert C.is_soluble\n        assert Ab.is_soluble\n    Ab = AbelianGroup(5, 7, 10)\n    assert Ab.is_soluble\n    # A5 is not soluble\n    assert AlternatingGroup(5).is_soluble is False\n"], "sample_1006": ["def test_binomial_special_cases():\n    x = Symbol('x')\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True)\n\n    assert binomial(n, 0) == 1\n    assert binomial(n, n) == 1\n    assert binomial(n, k) == binomial(n, n - k)\n    assert binomial(n, k).is_positive\n    assert binomial(n, k).is_integer\n\n\n", "def test_binomial_expand_func():\n    x = Symbol('x')\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True)\n\n    assert expand_func(binomial(n, 1)) == n\n    assert expand_func(binomial(n, 2)) == n*(n - 1)/2\n    assert expand_func(binomial(n, 3)) == n*(n - 1)*(n - 2)/6\n    assert expand_func(binomial(n, n - 1)) == n\n    assert expand_func(binomial(n, n)) == 1\n    assert expand_func(binomial(n, k)) == \\\n    binomial(n, k).rewrite(factorial)\n    try:\n        expand_func(binomial(x, S(1)/2))\n    except NotImplementedError:\n        pass # This is expected\n\n\n\n", "def test_binomial_expand():\n    x = Symbol('x')\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True)\n\n    assert expand_func(binomial(n, k)) == \\\n        binomial(n, k)\n    assert expand_func(binomial(n, k, evaluate=False)).equals(binomial(n, k))\n    assert expand_func(binomial(n, k + 1)).equals(\n        binomial(n, k)*(n - k)/(k + 1))\n    assert expand_func(binomial(n, k - 1)).equals(\n        binomial(n, k)*(k)/(n - k + 1))\n    assert expand_func(binomial(n, k) * x) == x*binomial(n, k)\n    assert expand_func(binomial(n, k) + binomial(n, k - 1)) == \\\n        binomial(n + 1, k)\n    assert expand_func(binomial(n, k) - binomial(n, k - 1)) == \\\n        binomial(n, k - 1)\n\n\n\n", "def test_binomial_rewrite_special_cases():\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True)\n\n    assert binomial(n, 0).rewrite(Product) == 'Product(_i, (_i, 1, 0)) '\n    assert binomial(n, n).rewrite(Product) == 'Product(_i, (_i, 1, n))'\n\n\n    assert binomial(0, n).rewrite(Product) == 'Product(_i, (_i, 1, 0))'\n\n    assert binomial(n, k).rewrite(S).is_comparable\n\n\n", "def test_binomial_expand():\n    x = Symbol('x')\n    y = Symbol('y')\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True)\n    m = Symbol('m', integer=True)\n    \n    assert binomial(n, k).expand() == \\\n        sum([binomial(n, i) * x**(n - i) * y**i for i in range(k + 1)])\n    assert binomial(n, 0).expand() == 1\n    assert binomial(n, n).expand() == 1\n    assert binomial(n, 1).expand() == n*x\n    assert binomial(n, 2).expand() == n*(n - 1)*x**2/2\n    assert binomial(n, 3).expand() == n*(n - 1)*(n - 2)*x**3/6\n    assert binomial(n, k).expand(func=True) == \\\n        sum([binomial(n, i) * x**(n - i) * y**i for i in range(k + 1)])\n    assert binomial(n, k + 1).expand() == 0\n    assert binomial(n, -1).expand() == 0\n    assert binomial(n, m).expand() == \\\n        sum([binomial(n, i) * x**(n - i) * y**i for i in range(min(n, m) + 1)])\n    assert binomial(n, m).expand(func=True) == \\\n        sum([binomial(n, i) * x**(n - i) * y**i for i in range(min(n, m) + 1)])\n\n\n\n"], "sample_1059": ["def test_jacobi():\n    n = Symbol(\"n\")\n    a = Symbol(\"a\")\n    b = Symbol(\"b\")\n\n    assert jacobi(0, a, b, x) == 1\n    assert jacobi(1, a, b, x) == (a + b + 1)/(2*a)*(x - 1)\n    assert jacobi(2, a, b, x) == ((a + b + 2)*(a + b + 1)/(2*a)*(a + b + 3)/\n               (2*a)*(x - 1)**2 + (a + b + 1)/(2*a)*(x - 1))\n    assert jacobi(3, a, b, x) == ((a + b + 3)*(a + b + 2)*(a + b + 1)/(\n        2*a)*(a + b + 4)/(2*a)*(x - 1)**3 + (a + b + 2)*(a + b + 1)/(\n        2*a)*(a + b + 3)/(2*a)*(x - 1)**2 + (a + b + 1)/(2*a)*(a + b + 2)/(\n        2*a)*(x - 1))\n    assert jacobi(4, a, b, x) == \\\n        ((a + b + 4)*(a + b + 3)*(a + b + 2)*(a + b + 1)/(2*a)*(a + b + 5)/(\n        2*a)*(x - 1)**4 + ... + (a + b + 1)/(2*a)*(a + b + 2)/(2*a)*(x - 1))\n\n    assert jacobi(-1, a, b, x) == 0\n    assert jacobi(n, a, b, x).rewrite(\"polynomial\").dummy_eq(Sum(((a + b + k)*(a +\n        b + k - 1)*(a + b + k - 2)*...*(a + b + 1))/(factorial(k)*(2*a)*\n        (2*a + 1)*(2*a + 2)*...*(2*a + 2*k - 1))*(x - ", "    def test_assoc_laguerre():\n        from sympy.polys.orthopolys import assoc_laguerre\n        n = Symbol(\"n\")\n        alpha = Symbol(\"alpha\")\n        x = Symbol(\"x\")\n\n        assert assoc_laguerre(0, alpha, x) == 1\n        assert assoc_laguerre(1, alpha, x) == -x + alpha + 1\n        assert assoc_laguerre(2, alpha, x) == x**2/2 - (2*alpha + 2)*x + \\\n            alpha*(alpha + 1) + 1\n        assert assoc_laguerre(3, alpha, x) == -x**3/6 + (3*alpha + 3)*x**2/2 - \\\n            (3*alpha**2 + 9*alpha + 6)*x + alpha*(alpha + 1)*(alpha + 2) + 1\n\n        X = assoc_laguerre(n, alpha, x)\n        assert isinstance(X, assoc_laguerre)\n\n        assert assoc_laguerre(n, alpha, 0) == binomial(n + alpha, alpha)\n        assert assoc_laguerre(n, alpha, oo) == oo\n\n        assert conjugate(assoc_laguerre(n, alpha, x)) == \\\n            assoc_laguerre(n, conjugate(alpha), conjugate(x))\n\n        _k = Dummy('k')\n        assert assoc_laguerre(n, alpha, x).rewrite(\"polynomial\").dummy_eq(\n            Sum(x**_k*RisingFactorial(-n, _k)*\n            (alpha + _k)!/(factorial(_k)**2), (_k, 0, n)))\n\n        assert diff(assoc_laguerre(n, alpha, x), x) == -assoc_laguerre(n - 1,\n            alpha + 1, x)\n        assert diff(assoc_laguerre(n, alpha, x), alpha).dummy_eq(\n            Sum(assoc_laguerre(n, alpha, x)*x**_k *\n             (2*n + alpha + 1)*RisingFactorial(-n, _k)*\n             _k/(factorial(_k)**2) , (_k, 0, n)))\n\n        raises(ValueError, lambda: assoc_", "compilation error", "def test_assoc_laguerre():\n    n = Symbol(\"n\")\n    a = Symbol(\"a\")\n    x = Symbol(\"x\")\n\n    assert assoc_laguerre(0, a, x) == 1\n    assert assoc_laguerre(1, a, x) == -x + a\n    assert assoc_laguerre(2, a, x) == x**2/2 - 2*x*a + a**2\n\n    assert assoc_laguerre(n, a, 0) == binomial(a + n, n)\n    assert assoc_laguerre(n, a, 1) == a*assoc_laguerre(n - 1, a, 1) + (\n        a - n\n    )\n\n    X = assoc_laguerre(n, a, x)\n    assert isinstance(X, assoc_laguerre)\n\n    assert assoc_laguerre(n, a, -x) == (-1)**n*assoc_laguerre(n, a, x)\n    assert unchanged(assoc_laguerre, -n + k, a, x)\n\n    assert conjugate(assoc_laguerre(n, a, x)) == \\\n        assoc_laguerre(n, conjugate(a), conjugate(x))\n\n    _k = Dummy('k')\n    assert assoc_laguerre(n, a, x).rewrite(\"polynomial\").dummy_eq(\n        Sum((-1)**_k*RisingFactorial(n + a, _k)*x**_k/factorial(_k)**2,\n            (_k, 0, n)))\n    assert diff(assoc_laguerre(n, a, x), x) == -assoc_laguerre(\n        n - 1, a + 1, x\n    )\n    assert diff(assoc_laguerre(n, a, x), a).dummy_eq(\n        Sum(assoc_laguerre(n - k, a + 1, x)*x**k/k, (_k, 0, n))\n    )\n    raises(ArgumentIndexError, lambda: assoc_laguerre(n, a, x).fdiff(1))\n    raises(ArgumentIndexError, lambda: assoc_laguerre(n, a, x).fdiff(3))\n", "def test_jacobi():\n    n = Symbol(\"n\")\n    a = Symbol(\"a\")\n    b = Symbol(\"b\")\n\n    assert jacobi(0, a, b, x) == 1\n    assert jacobi(1, a, b, x) == (2*a*x + 2*b)/(a + b + 2)\n    assert jacobi(2, a, b, x) == ((a*x**2 + b*x**2 + a*x + b*x + a*b)/(a + b + 2))\n    assert jacobi(3, a, b, x) ==  ((a*x**3 + b*x**3 + 3*a*x**2 + 3*b*x**2 + 3*a*x + 3*b*x + 3*a*b)/(a + b + 3)) \n\n    assert jacobi(-1, a, b, x) == 0\n    assert jacobi(n, S(1)/2, S(1)/2, x) == legendre(n, x)\n    assert jacobi(n, 0, 0, x) == 1\n    assert jacobi(n, 1, 0, x) == chebyshevu(n, x)\n    assert jacobi(n, 0, 1, x) == chebyshevt(n, x)\n\n    X = jacobi(n, a, b, x)\n    assert isinstance(X, jacobi)\n\n    assert jacobi(n, a, b, -x) == (-1)**n*jacobi(n, b, a, x)\n    assert jacobi(n, a, b, 0) == 2**n*sqrt(pi) * \\\n        gamma(a + n/2)*gamma(b + n/2)/(gamma(a)*gamma(b)*gamma(n + 1))\n    assert jacobi(n, a, b, 1) == gamma(a + n + 1)*gamma(b + n + 1)/(gamma(a + 1)*gamma(b + 1)*gamma(n + 1"], "sample_435": ["    def test_password_change_validation(self):\n        user = User.objects.get(username=\"testclient\")\n        data = {\n            \"old_password\": \"password\",\n            \"password1\": \"test\",\n            \"password2\": \"test\",\n        }\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(\n            form.errors[\"password1\"],\n            [\n                \"This password is too short. It must contain at least 12 characters.\",\n                \"Your password is too similar to your username.\",\n            ],\n        )\n\n\n\n        \n", "    def test_bug_20506_password_reset_token_reuse(self):\n        user = User.objects.create_user(\"testuser\", \"test@example.com\", \"test\")\n        password_reset_form = PasswordResetForm({\"email\": user.email})\n        password_reset_form.save()\n        # We should now have a token in the database\n        password_reset_token = PasswordResetToken.objects.get(user=user)\n        password_reset_form = PasswordResetForm({\"email\": user.email})\n        password_reset_form.save()\n        # Attempting to save with the same email and token should raise an exception\n        with self.assertRaises(IntegrityError):\n            password_reset_form.save()\n\n\n\n", "    def test_password_too_short(self):\n        user = User.objects.get(username=\"testclient\")\n        data = {\"password1\": \"a\", \"password2\": \"a\"}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertContains(form.errors[\"password1\"], \"Too short.\")\n        self.assertContains(form.errors[\"password2\"], \"Too short.\")\n        self.assertEqual(form.changed_data, [])\n\n\n\n\n", "    def test_password_too_weak(self):\n        user = User.objects.get(username=\"testclient\")\n        data = {\"password1\": \"weak\", \"password2\": \"weak\"}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertIn(\n            \"This password is too weak. Make it stronger.\", form.errors[\"password1\"]\n        )\n        self.assertEqual(form.changed_data, [])\n", "    def test_password_too_short(self):\n        user = User.objects.get(username=\"testclient\")\n        data = {\"password1\": \"p\", \"password2\": \"p\"}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors[\"password1\"], [form.error_messages[\"password_too_short\"]])\n        self.assertEqual(form.changed_data, []) \n"], "sample_232": ["    def test_expression_wrapper_key_transform_with_nested_keys(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.annotate(\n                expr=ExpressionWrapper(\n                    KeyTransform('d','value') >> KeyTransform('1', 'value'),\n                    output_field=IntegerField(),\n                ),\n            ).filter(expr__isnull=False),\n            [self.objs[4]],\n        )\n", "    def test_key_contains_nested(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__d__1__contains='g'),\n            [self.objs[4]],\n        )\n", "    def test_key_transform_in(self):\n        tests = [\n            (\n                'value__d__0__in',\n                [KeyTransform(0, 'value'), KeyTransform(0, Value('None'))],\n                [self.objs[4], self.objs[0]],\n            ),\n            (\n                'value__baz__a__in',\n                [KeyTransform('a', KeyTransform('baz', 'value'))],\n                [self.objs[7]],\n            ),\n            (\n                'value__baz__a__in',\n                [\n                    KeyTransform('a', KeyTransform('baz', 'value')),\n                    KeyTransform('b', KeyTransform('baz', 'value')),\n                ],\n                [self.objs[7]],\n            ),\n        ]\n        for lookup, value, expected in tests:\n            with self.subTest(lookup=lookup, value=value):\n                self.assertSequenceEqual(\n                    NullableJSONModel.objects.filter(**{lookup: value}),\n                    expected,\n                )\n", "    def test_key_transform_with_negation(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(~Q(value__baz__has_key='c')),\n            [self.objs[0], self.objs[1], self.objs[2], self.objs[5], self.objs[6]],\n        )\n", "    def test_key_transform_with_multiple_lookups(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__foo=KeyTransform('bar', KeyTransform('baz', 'value')),\n                value__baz__gt=10,\n            ),\n            [self.objs[7]],\n        )\n\n"], "sample_31": ["    def test_write_latex_with_kwargs(self, write, tmp_path, format):\n        fp = tmp_path / \"test_write_latex_with_kwargs.tex\"\n        kwargs = {\"format\": format, \"header\": False, \"justify\": \"left\"}\n        write(fp, **kwargs)\n", "    def test_write_latex_no_overwrite(self, write, tmp_path, format):\n        \"\"\"Test writing to a file that exists.\"\"\"\n        fp = tmp_path / \"test_write_latex_no_overwrite.tex\"\n        write(fp, format=format)\n        with pytest.raises(IOError, match=\"overwrite=True\"):\n            write(fp, format=format, overwrite=False)\n", "    def test_write_latex_no_metadata(self, write, tmp_path, format):\n        fp = tmp_path / \"test_write_latex_no_metadata.tex\"\n        write(fp, format=format, cosmology_in_meta=False)\n", "    def test_write_latex_empty_cosmology(self, write, tmp_path, format):\n        fp = tmp_path / \"test_write_latex_empty_cosmology.tex\"\n        empty_cosmo = Cosmology()\n        write(fp, format=format, cosmology=empty_cosmo)\n        with open(fp, \"r\") as f:\n            content = f.read()\n        assert \"$\\infty$\" in content\n", "    def test_write_latex_overwrite(self, write, tmp_path, format):\n        \"\"\"Test to write a LaTeX file with overwrite=True.\"\"\"\n        fp = tmp_path / \"test_write_latex_overwrite.tex\"\n        write(fp, format=format, overwrite=True)\n        # Verify the file was written (and overwrite worked)\n        assert os.path.exists(fp)\n\n\n"], "sample_1178": ["def test_BinaryOperator():\n    x = symbols('x')\n    y = symbols('y')\n    assert Add(x, y).func(*Add(x, y).args) == Add(x, y)\n    assert Add(x, y) != Add(x, 2*y)\n    assert Add(x, y) != Sub(x, y)\n\n    assert Mul(x, y).func(*Mul(x, y).args) == Mul(x, y)\n    assert Mul(x, y) != Mul(x, 2*y)\n    assert Mul(x, y) != Div(x, y)\n\n    assert Div(x, y).func(*Div(x, y).args) == Div(x, y)\n    assert Div(x, y) != Div(x, 2*y)\n    assert Div(x, y) != Mul(x, y)\n\n    assert Pow(x, y).func(*Pow(x, y).args) == Pow(x, y)\n    assert Pow(x, y) != Pow(x, 2*y)\n    assert Pow(x, y) != x**y\n\n    assert Sub(x, y).func(*Sub(x, y).args) == Sub(x, y)\n    assert Sub(x, y) != Sub(x, 2*y)\n\n    assert Lt(x, y).func(*Lt(x, y).args) == Lt(x, y)\n    assert Lt(x, y) != Lt(x, 2*y)\n    assert Lt(x, y) != Gt(x, y)\n\n    assert Gt(x, y).func(*Gt(x, y).args) == Gt(x, y)\n    assert Gt(x, y) != Gt(x, 2*y)\n    assert Gt(x, y) != Lt(x, y)\n\n    assert Eq(x, y).func(*", "def test_FunctionCall__with_keyword_arguments():\n    fc = FunctionCall('my_func', [x, y=3], keyword_arguments={'z': 4})\n    assert fc.function_args == [x]\n    assert fc.keyword_arguments == {'y': 3, 'z': 4}\n    assert fc == FunctionCall('my_func', [x, y=3], keyword_arguments={'z': 4})\n    assert fc != FunctionCall('my_func', [y, x, z=4], keyword_arguments={'y': 3})\n\n\n\n", "def test_FunctionDefinition_from_FunctionPrototype_with_defaults():\n    vx = Variable(x, type=real)\n    vn = Variable(n, type=integer)\n    fp = FunctionPrototype(real, 'power', [vx, vn, default=1])\n    body = [Assignment(x, x**n), Return(x)]\n    fd = FunctionDefinition.from_FunctionPrototype(fp, body)\n    assert len(fd.parameters) == 3\n    assert fd.parameters[2].default == 1\n    assert fd.parameters[2].name == 'default'\n\n\n\n", "   def test_IfThenElse():\n        x = symbols('x')\n        y = symbols('y')\n        then_block = [Assignment(z, x + y)]\n        else_block = [Assignment(z, x - y)]\n        ite = IfThenElse(Lt(x, y), then_block, else_block)\n        assert ite.condition == Lt(x, y)\n        assert ite.then_block.args == (then_block,)\n        assert ite.else_block.args == (else_block,)\n        assert ite.func(*ite.args) == ite\n        assert ite != IfThenElse(Gt(x, y), then_block, else_block)\n        assert ite != IfThenElse(Lt(x, y), [Assignment(z, x)], else_block)\n", "compilation error"], "sample_180": ["    def test_unique_constraint_duplicate_name(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(fields=['age'], name='unique_age'),\n                    models.UniqueConstraint(fields=['age'], name='unique_age'),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [\n            Error(\n                'Duplicate constraint name \"unique_age\".',\n                obj=Model,\n                id='models.E014',\n            ),\n        ])\n", "    def test_unique_constraint_with_expression(self):\n        class Model(models.Model):\n            name = models.CharField(max_length=255)\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['name'],\n                        name='unique_name_length',\n                        condition=models.Q(name__len__gt=10),\n                    ),\n                ]\n\n        errors = Model.check(databases=self.databases)\n        expected = [] if connection.features.supports_expression_in_unique_constraints else [\n            Warning(\n                '%s does not support unique constraints with expressions.'\n                % connection.display_name,\n                hint=(\n                    \"A constraint won't be created. Silence this warning if \"\n                    \"you don't care about it.\"\n                ),\n                obj=Model,\n                id='models.W037',\n            ),\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_unique_constraint_with_non_unique_field(self):\n        class Model(models.Model):\n            id = models.AutoField(primary_key=True)\n            field = models.CharField(max_length=100)\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['field'], \n                        name='unique_field',\n                        condition=models.Q(id=None),\n                    ),\n                ]\n\n        errors = Model.check(databases=self.databases)\n        expected = [\n            Error(\n                \"A UniqueConstraint cannot be applied to a field that \"\n                \"allows NULL values. The condition 'Q(id=None)' \"\n                \"is not unique because it allows NULL values in the field.\",\n                hint=None,\n                obj=Model,\n                id='models.E035',\n            ),\n        ]\n        self.assertEqual(errors, expected)\n\n\n\n", "    def test_unique_constraint_with_duplicate_names(self):\n        class Model(models.Model):\n            name = models.CharField(max_length=255)\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(fields=['name'], name='unique_name'),\n                    models.UniqueConstraint(fields=['name'], name='unique_name'),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [\n            Error(\n                \"'constraints' contains duplicate constraint names: ['unique_name', 'unique_name']\",\n                obj=Model,\n                id='models.E014',\n            ),\n        ])\n", "    def test_unique_constraint_with_circular_dependency(self):\n        class A(models.Model):\n            a = models.ForeignKey('B', on_delete=models.CASCADE)\n\n        class B(models.Model):\n            b = models.ForeignKey('A', on_delete=models.CASCADE)\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(fields=['b'], name='unique_b'),\n                ]\n\n        with self.assertRaises(CircularDependencyError):\n            B.check(databases=self.databases)\n"], "sample_1003": ["def test_allowed_flags():\n    allowed_flags({'domain': ZZ}, [])\n    raises(FlagError, lambda: allowed_flags({'domain': ZZ, 'frac': True}, []))\n    allowed_flags({'domain': ZZ, 'frac': True}, ['frac'])\n", "compilation error", "def test_allowed_flags():\n    allowed = {'domain': ZZ, 'frac': False}\n    allowed_flags = ['frac']\n\n    with raises(FlagError):\n        allowed_flags = ['auto']\n        allowed_flags.append('auto')\n        allowed_flags = allowed_flags\n        allowed_flags = sorted(allowed_flags)\n        allowed_flags = tuple(allowed_flags)\n        allowed_flags = list(allowed_flags)\n        allowed_flags = set(allowed_flags)\n        allowed_flags = allowed_flags.copy()\n        allowed_flags = allowed_flags.replace('auto', 'auto')\n        allowed_flags = allowed_flags.upper()\n        allowed_flags = allowed_flags.lower()\n        allowed_flags = allowed_flags.split('auto')\n        allowed_flags = allowed_flags.replace('auto', 'auto')\n        allowed_flags = allowed_flags.replace('auto', 'auto')\n    allowed_flags = sorted(allowed_flags)\n    allowed_flags = tuple(allowed_flags)\n    allowed_flags = list(allowed_flags)\n    allowed_flags = set(allowed_flags)\n    allowed_flags = allowed_flags.copy()\n    allowed_flags = allowed_flags.replace('auto', 'auto')\n    allowed_flags = allowed_flags.upper()\n    allowed_flags = allowed_flags.lower()\n    allowed_flags = allowed_flags.split('auto')\n    allowed_flags = allowed_flags.replace('auto', 'auto')\n    allowed_flags = allowed_flags.replace('auto', 'auto')\n\n    allowed_flags = ['auto']\n    allowed_flags.append('auto')\n    allowed_flags = allowed_flags\n    allowed_flags = sorted(allowed_flags)\n    allowed_flags = tuple(allowed_flags)\n    allowed_flags = list(allowed_flags)\n    allowed_flags = set(allowed_flags)\n    allowed_flags = allowed_flags.copy()\n    allowed_flags = allowed_flags.replace('auto', 'auto')\n    allowed_flags = allowed_flags.upper()\n    allowed_flags = allowed_flags.lower()\n    allowed_flags = allowed_flags.split('auto')\n    ", "def test_allowed_flags():\n    with raises(FlagError, match=\"'frac' flag\"):\n        allowed_flags({'domain': ZZ}, ['frac'])\n\n    allowed_flags({'domain': ZZ}, [])\n\n", "def test_allowed_flags():\n    allowed_flags(\n        {'domain': ZZ}, []\n    )\n\n    raises(FlagError, lambda: allowed_flags(\n        {'domain': ZZ}, ['frac']\n    ))\n\n    allowed_flags({'domain': ZZ, 'frac': True}, ['frac'])\n"], "sample_1096": ["def test_issue_19682():\n    i = Idx(\"i\", (0, 5))\n    j = Idx(\"j\")\n    A = IndexedBase('A')\n    assert A[i] == A[i]\n    assert A[i].subs(i, j) == A[j]\n    assert A[i].subs(i, 2) == A[2]\n    assert A[i].subs(i, j).is_indexed == True\n    assert A[i].subs(i, 2).is_indexed == False\n\n\n\n", "compilation error", "compilation error", "compilation error", "def test_Indexed_subs_symbol_with_integer_arguments():\n    i, j = symbols('i j', integer=True)\n    A = IndexedBase('A')\n    b = symbols('b', integer=True)\n    assert A[i].subs(i, b) == A[b]\n    assert A[i, j].subs(i, b) == A[b, j]\n\n\n\n"], "sample_665": ["compilation error", "def test_collect_ignore_with_conftest_and_test_args(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        \"\"\"\n    )\n    testdir.mkdir(\"sub\").ensure(\"conftest.py\").write(\n        textwrap.dedent(\n            \"\"\"\n        collect_ignore = [\"test_one\"]\n\n            session.config.hook.pytest_collect_file = lambda self, path, parent: None\n    \"\"\"\n        )\n    )\n    result = testdir.runpytest(\"-v\", \"sub\", \"test_two\")\n    result.stdout.fnmatch_lines([\"*1 passed in*\"])\n", "def test_collect_file_with_import_error(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import does_not_exist\n\n            assert does_not_exist.foo\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*ERROR collecting test_collect_file_with_import_error.py*:\"\n            \" ImportError: No module named 'does_not_exist'\",\n            \"*1 failed in* *\"\n        ]\n    )\n\n", "def test_collect_package_with_invalid_conftest(testdir):\n    testdir.makepyfile(\"def test_1(): pass\")\n    pydir = testdir.mkpydir(\"foopkg\")\n    pydir.join(\"__init__.py\").write(\"assert False\")\n    pydir.join(\"conftest.py\").write(\"assert 0\")  # Intentionally raise an error\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*ERROR collecting foopkg*/conftest.py*\",\n            \"E*assert 0*\",\n            \"*1 passed in*\",\n        ]\n    )\n", "def test_collect_skip_on_empty_tests(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n            pass\n        \"\"\"\n    )\n    result = testdir.runpytest(p, \"--collect-only\")\n    result.stdout.fnmatch_lines([\"collected 1 item\", \"*test_empty*\"])\n"], "sample_341": ["    def test_valid_with_form_errors(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '',  # <-- this is a missing required field\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertFalse(all_valid((formset, formset)))\n        expected_errors = [\n            {},\n            {'votes': ['This field is required.']},\n        ]\n        self.assertEqual(formset._errors, expected_errors)\n", "    def test_invalid(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': '',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [{'choice': ['This field is required.']}, {}]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n", "    def test_invalid(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [{'votes': ['This field is required.']}, {}]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n\n", "    def test_one_invalid_formset(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '',  # <-- this is invalid\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [{}, {'votes': ['This field is required.']}]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n\n\n\n", "    def test_formset_with_nested_formsets(self):\n        class NestedForm(Form):\n            name = CharField()\n\n        class NestedFormSet(BaseFormSet):\n            form = NestedForm\n\n        class ExternalForm(Form):\n            nested = NestedFormSet\n\n        ExternalFormSet = formset_factory(ExternalForm)\n        formset = ExternalFormSet(\n            {'external-0-nested-TOTAL_FORMS': '1', 'external-0-nested-0-name': 'test'}\n        )\n        self.assertTrue(formset.is_valid())\n        self.assertEqual(formset.cleaned_data['external-0-nested'][0]['name'], 'test')\n\n"], "sample_1123": ["def test_condset_subs():\n    c1 = ConditionSet(x, x > 1, S.Reals)\n    c2 = c1.subs(x, y)\n    assert c2.sym is y\n    assert c2.condition is c1.condition\n    assert c2.base_set is c1.base_set\n\n    c3 = ConditionSet(x, x > 1, S.Integers)\n    raises(ValueError, lambda: c3.subs(x, y))\n", "def test_contains_with_assumed_symbols():\n    n = Symbol('n', negative=True)\n    assert ConditionSet(n, n > 0, S.Integers) == EmptySet\n    assert ConditionSet(n, n > 0, S.Naturals).contains(2) is True\n    assert ConditionSet(n, n > 0, S.Naturals).contains(-2) is False\n    assert ConditionSet(n, n > 0, S.Integers).contains(n) is True\n", "def test_issue_18857():\n    from sympy.sets import Intersection,  ConditionSet\n    from sympy import Symbol, Eq,  S\n\n    x = Symbol('x')\n    y = Symbol('y')\n    c = ConditionSet(x, Eq(x, y), S.Reals)\n    i = Intersection(c, c)\n    assert i == c \n", "compilation error", "def test_CondSet_with_non_symbol_in_condition():\n    raises(ValueError, lambda: ConditionSet(x, x + 1 < 1, S.Integers))\n"], "sample_327": ["    def test_to_python_invalid_json(self):\n        field = JSONField()\n        with self.assertRaisesMessage(ValidationError,\n                                      'Enter a valid JSON.'):\n            field.to_python('[\"invalid, syntax')\n\n", "    def test_formfield_initial_none(self):\n        class JSONForm(Form):\n            json_field = JSONField()\n\n        form = JSONForm({'json_field': '[]'}, initial={'json_field': None})\n        self.assertEqual(form['json_field'].value(), 'null')\n        self.assertNotIn('This field is required.', form.errors)\n\n\n\n", "    def test_invalid_json_unicode(self):\n        field = JSONField()\n        with self.assertRaisesMessage(ValidationError, 'Enter a valid JSON.'):\n            field.clean('{\"a\": \"\u4f60\u597d\uff0c\u4e16\u754c\\x9f\"}')\n\n\n\n", "    def test_custom_encoder_decoder_no_match(self):\n        value = {'uuid': uuid.UUID('{c141e152-6550-4172-a784-05448d98204b}')}\n        encoded_value = '{\"uuid\": \"c141e152-6550-4172-a784-05448d98204b\"}'\n\n        class CustomDecoder(json.JSONDecoder):\n                return super().__init__(object_hook=None, *args, **kwargs)\n\n        field = JSONField(encoder=DjangoJSONEncoder, decoder=CustomDecoder)\n        with self.assertRaises(json.JSONDecodeError):\n            field.clean(encoded_value)\n", "    def test_custom_encoder_decoder_raises_error(self):\n        class CustomDecoder(json.JSONDecoder):\n                raise ValueError(\"Decoder raised an error\")\n\n        with self.assertRaises(ValidationError):\n            field = JSONField(encoder=DjangoJSONEncoder, decoder=CustomDecoder)\n            field.clean('{\"uuid\": \"c141e152-6550-4172-a784-05448d98204b\"}')\n"], "sample_843": ["def test_kernel_repr(kernel):\n    # Smoke-test for repr in kernels.\n    repr(kernel)\n\n", "    def test_hyperparameter_bounds(kernel):\n        # Test that hyperparameter bounds are respected.\n\n        # Access hyperparameter and check bounds.\n        for hyperparameter in kernel.hyperparameters:\n            if hyperparameter.bounds is None:\n                continue\n\n            # Check bounds are valid for the kernel.\n            lower_bound = hyperparameter.bounds[0]\n            upper_bound = hyperparameter.bounds[1]\n            value = getattr(kernel, hyperparameter.name)\n            assert lower_bound <= value <= upper_bound\n", "def test_kernel_bounds(kernel):\n    # Check that bounds are correctly applied.\n\n    for hyperparameter in kernel.hyperparameters:\n        \n        if hyperparameter.bounds == \"fixed\":\n            continue\n\n        bounds = hyperparameter.bounds\n        value = bounds[0]\n        kernel.set_params(**{hyperparameter.name: value})\n\n        with pytest.raises(ValueError):\n            kernel.set_params(**{hyperparameter.name: bounds[1] + 1})\n\n        value = bounds[1]\n        kernel.set_params(**{hyperparameter.name: value})\n\n        with pytest.raises(ValueError):\n            kernel.set_params(**{hyperparameter.name: bounds[0] - 1})\n\n", "def test_kernel_inverse():\n    # Check that inverse of kernel exists and can be computed.\n    K = kernel(X)\n    try:\n        K_inv = np.linalg.inv(K)\n    except np.linalg.LinAlgError:\n        # If kernel is not invertible, pass\n        return\n    # Check if multiplication with inverse kernel recovers the identity.\n    assert_array_almost_equal(np.dot(K, K_inv), np.eye(X.shape[0]))\n\n\n\n", "def test_kernel_theta_bounds(kernel):\n    # Check bounds of kernel parameters are set correctly\n    for hyperparameter in kernel.hyperparameters:\n        if hyperparameter.bounds is not None:\n            bounds = hyperparameter.bounds\n            value = getattr(kernel, hyperparameter.name)\n            assert bounds[0] <= value <= bounds[1]\n"], "sample_441": ["    def test_invalid_password_errors(self):\n        user = User.objects.get(username=\"testclient\")\n        data = {\"password1\": \"abc\", \"password2\": \"abc\"}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors[\"password1\"], [\n            form.error_messages[\"password_too_short\"]])\n\n\n\n", "    def test_invalid_password_length(self):\n        user = User.objects.get(username=\"testclient\")\n        data = {\"password1\": \"a\", \"password2\": \"a\"}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(\n            form.errors[\"password1\"],\n            [\n                \"The password must be at least 8 characters long.\",\n            ],\n        )\n        self.assertEqual(form.errors[\"password2\"], [\n            \"The password must be at least 8 characters long.\"\n        ])\n\n\n", "    def test_password_validation(self):\n        user = User.objects.get(username=\"testclient\")\n        data = {\"password1\": \"short\", \"password2\": \"short\"}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(\n            form.errors[\"password1\"],\n            [\n                *PASSWORD_VALIDATION_ERRORS[\n                    \"min_length\"\n                ]\n            ],\n        )\n        self.assertEqual(\n            form.errors[\"password2\"],\n            [\n                *PASSWORD_VALIDATION_ERRORS[\n                    \"min_length\"\n                ]\n            ],\n        )\n\n\n\n", "    def test_password_too_short(self):\n        user = User.objects.get(username=\"testclient\")\n        data = {\"password1\": \"a\", \"password2\": \"a\"}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors[\"password1\"], [form.error_messages[\"password_too_short\"]])\n        self.assertEqual(form.errors[\"password2\"], [form.error_messages[\"password_too_short\"]])\n        self.assertEqual(form.changed_data, [])\n\n\n", "    def test_password_too_short(self):\n        user = User.objects.get(username=\"testclient\")\n        data = {\"password1\": \"p\", \"password2\": \"p\"}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(\n            form.errors[\"password1\"],\n            [\n                \"The password must be at least 8 characters long.\",\n            ],\n        )\n        self.assertEqual(form.errors[\"password2\"], [\n            \"The password must be at least 8 characters long.\",\n        ])\n        self.assertEqual(form.changed_data, [])\n\n\n\n\n"], "sample_1049": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_765": ["compilation error", "def test_balanced_accuracy_score_with_multiclass_imbalanced():\n    y_true = np.array([0, 0, 1, 1, 2, 2, 2, 2])\n    y_pred = np.array([0, 1, 1, 0, 2, 2, 2, 2])\n    acc = balanced_accuracy_score(y_true, y_pred)\n    assert_almost_equal(acc, 0.6666667, decimal=6)\n\n    # Test with probability predictions\n    y_pred_prob = np.array([[0.8, 0.2], [0.2, 0.8], [0.9, 0.1], [0.1, 0.9],\n                           [0.1, 0.2, 0.7], [0.2, 0.3, 0.5], [0.2, 0.3, 0.5],\n                           [0.1, 0.2, 0.7]])\n\n    acc = balanced_accuracy_score(y_true, y_pred_prob)\n    assert_almost_equal(acc, 0.6666667, decimal=6)\n\n\n", "def test_balanced_accuracy_score_multiple_classes():\n    y_true = np.array([0, 1, 2, 0, 1, 2, 2])\n    y_pred = np.array([0, 1, 2, 0, 2, 2, 0])\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred), 0.6667)\n\n\n\n", "compilation error", "compilation error"], "sample_685": ["def test_file_handler_logging(testdir):\n    testdir.makefile(\n        \"\"\"\n        import time\n        import logging\n\n        logging.basicConfig(level=logging.DEBUG, filename='test.log', filemode='w')\n\n            logging.info(\"This message will be written to the file\")\n    \"\"\"\n    )\n    testdir.runpytest()\n\n    with open('test.log', 'r') as f:\n        assert \"This message will be written to the file\" in f.read()\n\n\n\n    testdir.remove('test.log')\n", "def test_custom_log_format(caplog):\n    \"\"\"\n    Test that the log format defined in the fixture is being used.\n    \"\"\"\n    caplog.set_level('INFO')\n\n    # Set a custom log format\n    testdir = pytest.Testdir()\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_format = %(asctime)s - %(name)s - %(levelname)s - %(message)s\n    \"\"\"\n    )\n\n    logger.info(\"some log message\")\n\n    assert f\"{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - __main__ - INFO - some log message\" in caplog.text\n", "def test_log_format_with_timestamp(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_format = %(asctime)s %(levelname)s: %(message)s\n        log_file = test.log\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n            logging.info(\"this is a test\")\n            assert \"this is a test\" in caplog.text\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n    assert result.ret == 0\n    with open('test.log', 'r') as f:\n        logfile_content = f.read()\n    assert  \"this is a test\" in logfile_content\n", "def test_caplog_handles_nested_at_level_calls(caplog):\n    caplog.set_level(logging.WARNING)\n\n    with caplog.at_level(logging.INFO):\n        sublogger.info(\"First sublogger info message\")\n\n    with caplog.at_level(logging.DEBUG) as inner_caplog:\n        inner_caplog.set_level(logging.CRITICAL)\n        sublogger.info(\"Sublogger info message within captured context\")\n\n    assert \"Sublogger info message within captured context\" in caplog.text\n    assert \"First sublogger info message\" in caplog.text\n\n", "        def test_fixture_access(caplog):\n            assert caplog.handler.level == 0\n            assert caplog.text == ''"], "sample_1102": ["compilation error", "compilation error", "compilation error", "compilation error", "def test_issue_18296():\n    x = Symbol('x')\n    y = Symbol('y')\n    assert cancel(x*y + 1, y*x + 1) == x*y + 1\n"], "sample_945": ["def test_module_index_sort(app):\n    text = \".. py:module:: docutils\\n\"\n    text += \".. py:module:: sphinx\\n\"\n    text += \".. py:module:: sphinx.builders\\n\"\n    text += \".. py:module:: sphinx.builders.html\\n\"\n    restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    assert index.generate() == (\n        [('s', [IndexEntry('sphinx', 0, 'index', 'module-sphinx', '', '', ''),\n                IndexEntry('sphinx.builders', 1, 'index', 'module-sphinx.builders', '', '', ''),\n                IndexEntry('sphinx.builders.html', 2, 'index', 'module-sphinx.builders.html', '', '', '')]),\n         ('d', [IndexEntry('docutils', 0, 'index', 'module-docutils', '', '', '')])],\n        False\n    )\n", "def test_python_python_use_unqualified_type_names_in_signature(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"n\"><a class=\"reference internal\" href=\"#foo.MyClass\" title=\"foo.MyClass\">'\n            '<span class=\"pre\">MyClass</span></a></span>' in content)\n\n\n\n", "def test_python_python_use_unqualified_type_names_with_as(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"n\"><a class=\"reference internal\" href=\"#foo.NameAlias\" title=\"foo.NameAlias\">'\n            '<span class=\"pre\">NameAlias</span></a></span>' in content)\n    assert ('<span class=\"n\"><a class=\"reference internal\" href=\"#foo.AnotherAlias\" title=\"foo.AnotherAlias\">'\n            '<span class=\"pre\">AnotherAlias</span></a></span>' in content) \n\n\n", "def test_python_python_use_unqualified_type_names_custom_role_alias(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"n\"><a class=\"reference internal\" href=\"#foo.Name\" title=\"foo.Name\">'\n            '<span class=\"pre\">Name</span></a></span>' in content)\n    assert '<span class=\"n\"><span class=\"pre\">foo.Age</span></span>' in content\n", "def test_python_python_use_unqualified_type_names_enabled(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"n\"><a class=\"reference internal\" href=\"#foo.Name\" title=\"foo.Name\">'\n            '<span class=\"pre\">Name</span></a></span>' in content)\n    assert '<span class=\"n\"><span class=\"pre\">foo.Age</span></span>' in content\n"], "sample_314": ["    def test_password_validation_errors(self):\n        user = User.objects.get(username='testclient')\n        data = {'password1': 'password', 'password2': 'password'}\n        form = AdminPasswordChangeForm(user, data)\n        # Simulate password validation errors\n        with patch('django.contrib.auth.password_validation.validate_password') as validate_password:\n            validate_password.side_effect = ValidationError([\"Invalid password: must contain at least one lowercase letter\"])\n            self.assertFalse(form.is_valid())\n            self.assertIn('password1', form.errors)\n            self.assertEqual(form.errors['password1'], [\"Invalid password: must contain at least one lowercase letter\"])\n", "    def test_password_reset_form_with_domain(self):\n        user = User.objects.get(username='testclient')\n        data = {'email': 'testclient@example.com'}\n        form = PasswordResetForm(data)\n        self.assertTrue(form.is_valid())\n\n        form.save(domain_override='testdomain.com')\n\n        self.assertEqual(form.cleaned_data['email'], data['email'])\n        self.assertEqual(len(mail.outbox), 1)\n        self.assertEqual(mail.outbox[0].subject, 'Custom password reset on testdomain.com')\n", "    def test_password_strength_validation(self):\n        user = User.objects.get(username='testclient')\n        data = {\n            'password1': 'weakpassword',\n            'password2': 'weakpassword',\n        }\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        password_errors = form.errors.get('password1')\n        self.assertIn(\n            'Password is too weak.', password_errors,\n        )\n\n\n\n", "    def test_password_reset_form_handles_unicode(self):\n        user = User.objects.create_user('mike123', 'mike@example.org', 'test123')\n        data = {\n            'email': 'm\u0131ke@example.org',\n        }\n        form = PasswordResetForm(data)\n        self.assertTrue(form.is_valid())\n        form.save()\n        self.assertEqual(len(mail.outbox), 1)\n        self.assertEqual(mail.outbox[0].to, ['m\u0131ke@example.org'])\n", "    def test_password_too_short(self):\n        user = User.objects.get(username='testclient')\n        data = {'password1': '123', 'password2': '123'}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertIn(\n            'This password is too short. It must contain at least 8 characters.',\n            form.errors['password1']\n        )\n        self.assertIn(\n            'This password is too short. It must contain at least 8 characters.',\n            form.errors['password2']\n        )\n"], "sample_1129": ["compilation error", "def test_loggamma():\n    from sympy import loggamma\n\n    expr = loggamma(x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'scipy.special.loggamma(x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.log(gamma(x))'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == 'math.log(gamma(x))'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr) == 'mpmath.loggamma(x)' \n", "def test_code_gen_ast_nodes():  \n    from sympy.codegen.ast import Symbol, Assignment,  If,  Function,  Real\n    \n    symbol = Symbol('x')\n    assignment = Assignment(symbol, 2)\n    if_expr = If(symbol > 1, symbol + 1, symbol - 1)\n    func = Function('my_func', (symbol,), if_expr)\n    real = Real(1)\n    \n    assert pycode(symbol) == 'x'\n    assert pycode(assignment) == 'x = 2'\n    assert pycode(if_expr) == 'if x > 1:\\n    return x + 1\\nelse:\\n    return x - 1'\n    assert pycode(func) == 'def my_func(x):\\n    if x > 1:\\n        return x + 1\\n    else:\\n        return x - 1'\n    assert pycode(real) == '1.0'\n", "def test_digamma():\n    from sympy import digamma\n\n    expr = digamma(x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'scipy.special.psi(x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == '  # Not supported in Python with NumPy:\\n  # digamma\\ndigamma(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == '  # Not supported in Python:\\n  # digamma\\ndigamma(x)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr) ==  'mpmath.psi(x)'\n", "def test_cosm1():\n    from sympy import cosm1\n\n    expr = cosm1(x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'scipy.special.cosm1(x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == '  # Not supported in Python with NumPy:\\n  # cosm1\\ncosm1(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == '  # Not supported in Python:\\n  # cosm1\\ncosm1(x)'\n"], "sample_381": ["    def test_suggest_name_with_complex_operations(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Person', fields=[]),\n                migrations.RenameModel('Person', 'User'),\n                migrations.AddField('User', 'age', models.IntegerField()),\n                migrations.RemoveField('User', 'name'),\n                migrations.AlterUniqueTogether(\n                    model_name='User',\n                    fields=('email',),\n                ),\n            ]\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(\n            migration.suggest_name(),\n            'user_rename_add_age_remove_name_alter_unique_together',\n        )\n\n", "    def test_conflict_with_existing_migration(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    name='Person',\n                    fields=[\n                        ('id', models.AutoField(primary_key=True)),\n                        ('name', models.CharField(max_length=200)),\n                    ],\n                ),\n            ]\n\n        app_label = 'test_app'\n        with isolate_lru_cache(apps.get_swappable_settings_name):\n            changes = self.get_changes([], [Migration(name='0001_initial', app_label=app_label)])\n        for change in changes:\n            self.assertEqual(change['migration_name'], '0001_initial')\n            self.assertEqual(change['app'], app_label)\n            self.assertEqual(change['operations'], [\n                {'type': 'CreateModel',\n                 'attributes': {'name': 'Person'}},\n            ])\n\n\n", "    def test_remove_model_w_fk_to_removed_model(self):\n        before = [\n            ModelState('testapp', 'Author', [\n                ('id', models.AutoField(primary_key=True)),\n            ]),\n            ModelState('testapp', 'Book', [\n                ('author', models.ForeignKey('testapp.Author', on_delete=models.CASCADE)),\n            ]),\n        ]\n        after = [\n            ModelState('testapp', 'Author', [])\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['DeleteModel', 'RemoveField'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='Author')\n        self.assertOperationAttributes(changes, 'testapp', 0, 1, name='author', model_name='Book') \n", "    def test_camel_case_operation_names(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    name='MyModel', fields=[('id', models.AutoField(primary_key=True))],\n                ),\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'mymodel')\n\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.AddField(\n                    model_name='MyModel', field=models.CharField(max_length=20)\n                ),\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'add_mymodel_charfield')\n\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.RemoveField(model_name='MyModel', field='myfield'),\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'remove_mymodel_myfield')\n\n\n\n", "    def test_suggest_name_with_custom_model_field_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    'Product', fields=[\n                        migrations.CharField('custom_name', max_length=200),\n                    ]\n                )\n            ]\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'product_custom_name')\n"], "sample_936": ["def test_stringify_forward_ref():\n    from typing import ForwardRef\n    class MyClass(object): pass\n    MyClassRef = ForwardRef('MyClass')\n    assert stringify(MyClassRef) == 'MyClass'\n\n\n", "def test_stringify_empty_tuple():\n    assert stringify(Tuple[()]) == \"Tuple[]\"\n", "def test_stringify_type_hints_GenericMeta():\n    from typing import Generic\n\n    class MyGeneric(Generic[T]):\n        pass\n\n    assert stringify(MyGeneric) == \"test_util_typing.MyGeneric[T]\"\n    assert stringify(MyGeneric[int]) == \"test_util_typing.MyGeneric[int]\"\n\n\n\n", "def test_stringify_ellipsis():\n    assert stringify(Ellipsis) == \"...\"\n    assert stringify(Callable[..., int]) == \"Callable[..., int]\"\n    assert stringify(List[str, ...]) == \"List[str, ...]\"\n", "def test_stringify_type_hints_ellipsis():\n    assert stringify(Ellipsis) == \"...\"\n\n\n"], "sample_1076": ["def test_codegen_ast_nodes_MatrixSolve():\n    from sympy.codegen.ast import Assignment, MatrixSolve, MatrixSymbol\n    prntr = PythonCodePrinter()\n    a = MatrixSymbol('A', 2, 2)\n    b = MatrixSymbol('b', 2, 1)\n    expr = MatrixSolve(a, b)\n    assert prntr.doprint(expr) == \"numpy.linalg.solve(A, b)\"\n", "def test_SymbolicMatrix():\n    from sympy.matrices import MatrixBase\n\n    class SymbolicMatrix(MatrixBase):\n            return printer.doprint(self)\n    \n    expr = SymbolicMatrix(2, 2)\n    prntr = SymPyPrinter()\n    assert prntr.doprint(expr) == 'sympy.Matrix((2, 2))'\n", "compilation error", "def test_issue_16097():\n    from sympy import MatPow, Matrix, eye, MatrixBase\n\n    A = MatrixSymbol(\"A\", 2, 2)\n    B = MatrixSymbol(\"B\", 2, 2)\n    expr1 = A**3 + 2*B**2\n    expr2 = MatPow(A, 3) + 2*MatPow(B, 2)\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr1) == 'numpy.linalg.matrix_power(A, 3) + 2*numpy.linalg.matrix_power(B, 2)'\n    assert prntr.doprint(expr2) == 'numpy.linalg.matrix_power(A, 3) + 2*numpy.linalg.matrix_power(B, 2)'\n", "def test_SymbolicArray_Print():\n    from sympy.codegen.array_utils import CodegenArrayTensorProduct, CodegenArrayContraction, CodegenArrayDiagonal, CodegenArrayPermuteDims, CodegenArrayElementwiseAdd\n\n    prntr = NumPyPrinter()\n    \n    # Test for CodegenArrayTensorProduct \n    a = CodegenArrayTensorProduct(\n        [symbols('x', real=True), symbols('y', real=True)],\n        [symbols('z', real=True), symbols('w', real=True)]\n    )\n\n    assert prntr.doprint(a) == 'numpy.einsum(' + \\\n        \"'xy,zw->xzyw', array = [numpy.array([[x, y]], dtype=float), numpy.array([[z, w]], dtype=float)]) \"\n\n    # Test for CodegenArrayContraction\n    b = CodegenArrayContraction(\n        CodegenArrayTensorProduct(\n            [symbols('x', real=True), symbols('y', real=True)],\n            [symbols('z', real=True), symbols('w', real=True)]\n        ),\n        contraction_indices=[(0, 0)]\n    )\n    assert prntr.doprint(b) == 'numpy.einsum(' + \\\n        \"'xy,zw->xwz', array = [numpy.array([[x, y]], dtype=float), numpy.array([[z, w]], dtype=float)]) \"\n\n    # Test for CodegenArrayDiagonal\n    c = CodegenArrayDiagonal(\n        MatrixSymbol(\"A\", 2, 2),\n        diagonal_indices=[(0, 0), (1, 1)]\n    )\n    assert prntr.doprint(c) == 'numpy.diagonal(A, 0, axis1=0, axis2=1)'\n\n    # Test for CodegenArrayPermuteDims\n    d = CodegenArrayPermuteDims(\n        MatrixSymbol(\"A\", 2, 2),\n        permutation=[(1, 0)]\n    )\n    assert prntr.doprint(d) == 'numpy.transpose(A, [1, 0])'\n    \n    # Test for CodegenArrayElementwiseAdd\n    e = CodegenArrayElementwiseAdd(\n        MatrixSymbol"], "sample_119": ["    def test_complex_related_filter(self):\n        query = Query(Item)\n        where = query.build_where(\n            Q(author__name__icontains='Alice') & Q(created__gt=F('author__created'))\n        )\n        \n        author_lookup = where.children[0]\n        self.assertIsInstance(author_lookup, Q)\n        self.assertEqual(author_lookup.children[0].connector, 'icontains')\n        self.assertIsInstance(author_lookup.children[0].children[0], Exact)\n        self.assertEqual(\n            author_lookup.children[0].children[0].lhs.target,\n            Author._meta.get_field('name')\n        )\n        \n        created_lookup = where.children[1]\n        self.assertIsInstance(created_lookup, Q)\n        self.assertEqual(created_lookup.children[0].connector, 'gt')\n        self.assertIsInstance(\n            created_lookup.children[0].children[0],\n            SimpleCol\n        )\n        self.assertEqual(\n            created_lookup.children[0].children[0].target,\n            Author._meta.get_field('created')\n        )\n", "    def test_related_order_by_with_lookup(self):\n        query = Query(Author)\n        query.add_ordering(('-items__name',))\n        where = query.build_where(Q(items__name__icontains='f'))\n        lookup = where.children[0]\n        self.assertIsInstance(lookup, Q)\n        self.assertEqual(lookup.connector, AND)\n        self.assertEqual(lookup.children[0].connector, AND)\n        self.assertEqual(lookup.children[0].children[0].lhs.target, Author._meta.get_field('id'))\n        self.assertEqual(lookup.children[0].children[1].lhs.target, Item._meta.get_field('name'))\n\n\n", "    def test_join_promotion(self):\n        query = Query(Item)\n        query.add_fields('author__name', 'author__num')\n        # Add fields using M2M relation.\n        query.add_fields('tags__name')\n        query.add_where(Q(author__name__icontains='john' | Q(tags__name__icontains='python')))\n\n        # Simulate votes by adding them to Query.\n        votes = {\n            'Item_author': 2,\n            'Item_tags': 2\n        }\n        join_promoter = JoinPromoter(connector=OR, num_children=2, negated=False)\n        join_promoter.add_votes(votes)\n        join_promoter.update_join_types(query)\n", "    def test_related_lookup_ordering(self):\n        q = Query(ObjectC)\n        q.add_ordering(\n            '-author__name')\n        self.assertEqual(q.order_by, [\n            ('author__name', 'DESC')])\n\n\n", "    def test_related_field_lookup(self):\n        query = Query(Author)\n        where = query.build_where(Q(book__title='Test Book'))\n        lookup = where.children[0]\n        self.assertIsInstance(lookup, Exact)\n        self.assertIsInstance(lookup.lhs, SimpleCol)\n        self.assertEqual(lookup.lhs.target, Book._meta.get_field('title'))\n        self.assertEqual(lookup.rhs, 'Test Book')\n\n\n\n"], "sample_627": ["compilation error", "compilation error", "compilation error", "compilation error", "def test_concat_with_unspecified_dim(  ) -> None:\n    ds1 = Dataset({\"a\": ((\"x\", \"y\"), np.arange(6).reshape((3, 2))), \"b\": ((\"x\",), [1, 2, 3])})\n    ds2 = Dataset({\"a\": ((\"x\", \"y\"), np.arange(6, 12).reshape((3, 2))), \"b\": ((\"x\",), [4, 5, 6])})\n\n    # check that if no dim is specified, it defaults to the first dimension\n    expected = Dataset({\"a\": ((\"x\", \"y\"), np.arange(12).reshape((6, 2))), \"b\": ((\"x\",), [1, 2, 3, 4, 5, 6])})\n    actual = concat([ds1, ds2])\n    assert_identical(actual, expected)\n\n    # check if the second dimension is selected if specified\n    expected = Dataset({\"a\": ((\"x\", \"y\"), np.arange(12).reshape((6, 2))), \"b\": ((\"y\",), [1, 2, 3, 4, 5, 6])})\n    actual = concat([ds1, ds2], dim=\"y\")\n    assert_identical(actual, expected)\n"], "sample_969": ["def test_stringify_type_hints_Annotated_with_args():\n    from typing import Annotated  # type: ignore\n    assert stringify(Annotated[str, \"foo\", \"bar\"], False) == \"Annotated[str, 'foo', 'bar']\"\n    assert stringify(Annotated[str, \"foo\", \"bar\"], True) == \"~typing.Annotated[str, 'foo', 'bar']\"\n", "def test_stringify_type_hints_Callable_varargs_args(annotation, expected):\n    assert stringify(annotation, False) == expected\n    assert stringify(annotation, True) == expected\n", "def test_stringify_type_hints_TypedDict():\n    from typing import TypedDict  # type: ignore\n\n    class MyTypedDict(TypedDict):\n        name: str\n        age: int\n\n    assert stringify(MyTypedDict, False) == \"MyTypedDict\"\n    assert stringify(MyTypedDict, True) == \"~typing.TypedDict[MyTypedDict]\"\n\n    assert stringify(List[MyTypedDict], False) == \"List[MyTypedDict]\"\n    assert stringify(List[MyTypedDict], True) == \"~typing.List[~typing.TypedDict[MyTypedDict]]\"\n\n", "compilation error", "def test_stringify_type_hints_match():\n    from typing import Match  # type: ignore\n    assert stringify(Match[str], False) == \"Match[str]\"\n    assert stringify(Match[str], True) == \"~typing.Match[str]\"\n"], "sample_978": ["def test_bspline_basis_with_non_integer_knots():\n    d = 2\n    knots = [0.5, 1.5, 2.5, 3.5]\n    splines = bspline_basis_set(d, knots, x)\n    assert splines[0] == Piecewise((x**2/2, Interval(0.5, 1.5).contains(x)),\n                   (Rational(-3, 2) + 3*x - x**2, Interval(1.5, 2.5).contains(x)),\n                   (Rational(9, 2) - 3*x + x**2/2, Interval(2.5, 3.5).contains(x)),\n                   (0, True))\n\n\n\n", "def test_repeated_knots_degree_2():\n    d = 2\n    knots = [0, 0, 1, 2, 2, 3]\n    splines = bspline_basis_set(d, knots, x)\n    b0 = Piecewise((x**2/2, Interval(0, 1).contains(x)),\n                   (Rational(1, 2) - x + x**2/2, Interval(1, 2).contains(x)),\n                   (0, True))\n    assert splines[0] == b0\n\n\n\n", "def test_high_degree_knots():\n    d = 4\n    knots = [0, 0, 0, 1, 2, 2, 2, 3, 4, 4, 4]\n    splines = bspline_basis_set(d, knots, x)\n    b0 = Piecewise(\n        (x**4/24, Interval(0, 1).contains(x)),\n        (Rational(1, 6) - Rational(1, 2) * x + Rational(1, 3) * x**2 - Rational(1, 24) * x**3, Interval(1, 2).contains(x)),\n        (Rational(19, 24) - Rational(1) * x + Rational(3) * x**2 - Rational(1) * x**3 + Rational(1, 24) * x**4, Interval(2, 3).contains(x)),\n        (Rational(11) - Rational(8) * x + Rational(6) * x**2 - Rational(1) * x**3 + Rational(1, 24) * x**4, Interval(3, 4).contains(x)),\n        (0, True)\n    )\n    assert splines[0] == b0 \n", "def test_degree_zero_non_integer_knots():\n    d = 0\n    knots = [1.5, 2.5, 3.5]\n    splines = bspline_basis_set(d, knots, x)\n    assert splines[0] == Piecewise((1, Interval(1.5, 2.5).contains(x)),\n                                  (0, True))\n    assert splines[1] == Piecewise((1, Interval(2.5, 3.5).contains(x)),\n                                  (0, True))\n\n", "def test_b_spline_basis_with_fractional_knots():\n    d = 2\n    knots = [0, Rational(1, 2), 1, Rational(3, 2), 2]\n    splines = bspline_basis_set(d, knots, x)  \n    b0 = Piecewise(\n        (x**2/2, Interval(0, Rational(1, 2)).contains(x)), \n        (Rational(3, 4) - x + x**2/2, Interval(Rational(1, 2), 1).contains(x)),\n        (Rational(1, 2) - x + x**2/2, Interval(1, Rational(3, 2)).contains(x)),\n        (Rational(1, 4) + x - x**2/2, Interval(Rational(3, 2), 2).contains(x)),\n        (0, True)\n    )\n    assert splines[0] == b0 \n"], "sample_313": ["    def test_template_dirs_with_spaces(self):\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            {\n                ROOT / 'path/with/spaces',\n                Path.cwd() / 'template_tests/path/with/spaces',\n            }\n        )   \n", "    def test_filesystem_loader_only(self):\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            {\n                Path.cwd() / 'template_tests/relative_path',\n            }\n        )\n", "    def test_nested_template_dirs(self):\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            {\n                ROOT / 'templates_extra',\n                ROOT / 'templates_extra_inner',\n            }\n        )\n", "    def test_get_template_directories_app_directories_only(self):\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            {\n                Path.cwd() / 'template_tests/templates',\n            }\n        )\n\n", "    def test_custom_template_loader_integration(self):\n        mock_custom_loader = mock.MagicMock()\n        mock_custom_loader.get_dirs.return_value = [EXTRA_TEMPLATES_DIR]\n        \n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            {\n                ROOT / 'templates_extra',\n            }\n        )\n        \n        mock_custom_loader.get_dirs.assert_called_once()\n"], "sample_8": ["    def test_masked_array_from_masked_array(self):\n        \"\"\"Check that we can initialize a MaskedArray from a numpy.ma.MaskedArray.\"\"\"\n        np_ma = np.ma.MaskedArray(self.ma)\n        masked_array_from_masked_array = Masked(np_ma)\n        assert type(masked_array_from_masked_array) is Masked\n        assert type(masked_array_from_masked_array.data) is self._data_cls\n        assert type(masked_array_from_masked_array.mask) is np.ndarray\n        assert_array_equal(masked_array_from_masked_array.data, self.a)\n        assert_array_equal(masked_array_from_masked_array.mask, self.mask_a)\n", "    def test_masked_array_from_masked_array(self):\n        \"\"\"Check that we can initialize a MaskedArray from an existing MaskedArray.\"\"\"\n        np_ma = np.ma.MaskedArray(self.ma)\n        a_copy = np_ma.copy()\n        assert type(a_copy) is np.ma.MaskedArray\n        assert type(a_copy.data) is self._data_cls\n        assert type(a_copy.mask) is np.ndarray\n        assert_array_equal(a_copy.data, self.a)\n        assert_array_equal(a_copy.mask, self.mask_a)\n", "    def test_numpy_masked_array_methods(self):\n        np_ma = np.ma.MaskedArray(self.ma)\n        \n        # Test basic arithmetic operations\n        np_ma_sum = np_ma.sum()\n        assert np_ma_sum == self.ma.sum()\n        np_ma_mean = np_ma.mean()\n        assert np_ma_mean == self.ma.mean()\n        np_ma_std = np_ma.std()\n        assert np_ma_std == self.ma.std()\n        np_ma_var = np_ma.var()\n        assert np_ma_var == self.ma.var()\n\n        # Test non-arithmetic operations\n        np_ma_any = np_ma.any()\n        assert np_ma_any == self.ma.any()\n        np_ma_all = np_ma.all()\n        assert np_ma_all == self.ma.all()\n        np_ma_argmax = np_ma.argmax()\n        assert np_ma_argmax == self.ma.argmax()\n        np_ma_argmin = np_ma.argmin()\n        assert np_ma_argmin == self.ma.argmin()\n\n        # Test masked slicing\n        np_masked_slice = np_ma[self.mask_a]\n        assert_array_equal(np_masked_slice, np.ma.masked_where(~self.mask_a, self.a))\n", "    def test_masked_array_from_array_and_mask(self):\n        new_ma = np.ma.MaskedArray(self.a, mask=self.mask_a)\n        assert type(new_ma) is np.ma.MaskedArray\n        assert type(new_ma.data) is self._data_cls\n        assert type(new_ma.mask) is np.ndarray\n        assert_array_equal(new_ma.data, self.a)\n        assert_array_equal(new_ma.mask, self.mask_a)\n", "    def test_masked_array_from_masked_array(self):\n        \"\"\"Check that we can initialize a MaskedArray properly.\"\"\"\n        np_ma1 = np.ma.MaskedArray(self.ma)\n        np_ma2 = numpy.ma.MaskedArray(np_ma1)\n        assert type(np_ma2) is np.ma.MaskedArray\n        assert type(np_ma2.data) is self._data_cls\n        assert type(np_ma2.mask) is np.ndarray\n        assert_array_equal(np_ma2.data, self.a)\n        assert_array_equal(np_ma2.mask, self.mask_a)\n\n"], "sample_695": ["def test_nodeid_with_extra_keyword_matches(pytester: Pytester) -> None:\n    \"\"\"Test that extra keyword matches are added to the nodeid.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.extra_match\n            pass\n    \"\"\"\n    )\n\n    items = pytester.getitems()\n    assert items[0].nodeid.endswith(\"::test_something::extra_match\")\n", "def test_isinitpath() -> None:\n    from _pytest.compat import legacy_path\n\n    class FakeSession:\n        config = type(None)()\n        _initialpaths = frozenset([Path.cwd() / \"tests\"])\n        rootpath = Path.cwd() / \"tests\"\n\n    session = FakeSession()\n\n    assert session.isinitpath(legacy_path(\"tests/test_file.py\"))\n    assert session.isinitpath(legacy_path(\"tests/test_file\"))\n\n    assert not session.isinitpath(legacy_path(\"src/test_file.py\"))\n    assert not session.isinitpath(legacy_path(\"another_tests/test_file.py\"))\n", "def test_item_add_report_section() -> None:\n    item = nodes.Item(name=\"test\")\n    item.add_report_section(\"call\", \"stdout\", \"This is some test output\")\n    assert item._report_sections == [(\"call\", \"stdout\", \"This is some test output\")]\n\n\n    item.add_report_section(\"setup\", \"stderr\", \"Setup error\")\n    assert item._report_sections == [\n        (\"call\", \"stdout\", \"This is some test output\"),\n        (\"setup\", \"stderr\", \"Setup error\"),\n    ]\n\n    item.add_report_section(\"teardown\", \"stdout\", \"Some teardown output\")\n    assert item._report_sections == [\n        (\"call\", \"stdout\", \"This is some test output\"),\n        (\"setup\", \"stderr\", \"Setup error\"),\n        (\"teardown\", \"stdout\", \"Some teardown output\"),\n    ]\n\n\n\n", "def test_iterparentnodeids_edge_cases(pytester: Pytester) -> None:\n    for nodeid in [\"\", \"::\", \"a::\", \"a::b::\", \"::a\", \"::a::\"]:\n        nodes = list(nodes.iterparentnodeids(nodeid))\n        assert nodes == [\"\"] * len(nodes)\n", "def test_iterparentnodeids_with_sep(nodeid: str, expected_parts: List[str]) -> None:\n    result = list(nodes.iterparentnodeids(nodeid))\n    assert result == expected_parts\n"], "sample_907": ["compilation error", "def test_domain_cpp_parse_undeclared_roles(app):\n    text = (\".. cpp:undeclared_role:: foo\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.undeclared_role,\n                          addnodes.literal_strong, \"undeclared_role\"))\n", "compilation error", "compilation error", "def test_domain_cpp_parse_xref_with_unicode_characters(app):\n    text = (\".. cpp:function::  \ud835\udc53()\\n\"\n            \".. cpp:function::  \ud835\udc54()\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc, addnodes.index, desc))\n    assert_node(doctree[0], addnodes.index, entries=[('single', '\ud835\udc53 (C++ function)', '_CPPv41fv', '', None)])\n    assert_node(doctree[2], addnodes.index, entries=[('single', '\ud835\udc54 (C++ function)', '_CPPv41gv', '', None)])\n"], "sample_682": ["compilation error", "compilation error", "compilation error", "compilation error", "        def test_importorskip_existing():\n            pytest.importorskip(\"os\")"], "sample_1014": ["def test_ndim_array_with_symbols():\n    from sympy import symbols\n\n    x, y, z = symbols('x y z')\n    arr = ImmutableDenseNDimArray([[x, y], [z, x*y]])\n\n    assert arr[0, 0] == x\n    assert arr[1, 1] == x*y\n\n    assert arr.free_symbols == {x, y, z}\n\n    arr_with_tuple_index = arr[(0, 0)]\n    assert isinstance(arr_with_tuple_index, Indexed)\n    assert arr_with_tuple_index == x\n\n    arr_with_tuple_index_diff = arr[(1, 1)].diff(x)\n    assert arr_with_tuple_index_diff == y\n\n\n", "def test_reshape_with_symbolic_indices():\n    x, y = symbols('x y')\n    md = ImmutableDenseNDimArray([[x, y], [x**2, y**2]])\n    reshaped = md.reshape(2, 2)\n    assert reshaped == ImmutableDenseNDimArray([[x, y], [x**2, y**2]])\n    reshaped = md.reshape(1, 4)\n    assert reshaped.shape == (1, 4)\n    assert reshaped.rank() == 1\n\n    sd = ImmutableSparseNDimArray([[x, y], [x**2, y**2]])\n    reshaped = sd.reshape(2, 2)\n    assert reshaped == ImmutableSparseNDimArray([[x, y], [x**2, y**2]])\n    reshaped = sd.reshape(1, 4)\n    assert reshaped.shape == (1, 4)\n    assert reshaped.rank() == 1\n\n", "def test_reshape_invalid_input():\n    arr = ImmutableDenseNDimArray([1, 2, 3, 4], (2, 2))\n    with raises(ValueError):\n        arr.reshape(3, 3)\n    with raises(ValueError):\n        arr.reshape((3, 3, 3))\n    with raises(ValueError):\n        arr.reshape((1, 6))\n", "def test_transpose():\n    md = ImmutableDenseNDimArray(range(10, 34), (2, 3, 4))\n    mt = md.transpose((2, 0, 1))\n    expected_array = ImmutableDenseNDimArray(md._array, (4, 2, 3))\n    assert mt == expected_array\n\n    sd = ImmutableSparseNDimArray(range(10, 34), (2, 3, 4))\n    st = sd.transpose((2, 0, 1))\n    assert st.shape == (4, 2, 3)\n    assert st == ImmutableSparseNDimArray(sd._sparse_array, (4, 2, 3))\n\n", "def test_reshape_validation():\n    arr = ImmutableDenseNDimArray([1, 2, 3, 4, 5, 6], (2, 3))\n    with raises(ValueError, match=\"Invalid reshape parameters\"):\n        arr.reshape(1, 12)\n    with raises(ValueError, match=\"Invalid reshape parameters\"):\n        arr.reshape((1, 1, 12))\n"], "sample_504": ["def test_colorbar_label_position():\n    fig, ax = plt.subplots()\n    im = ax.imshow([[1, 2], [3, 4]])\n    cbar = fig.colorbar(im, label='cbar', location='left')\n    assert cbar.ax.get_ylabel() == 'cbar'\n    \n    # check that the label position is near the bottom of the colorbar\n    label_position = cbar.ax.get_yaxis_label_position()\n    assert label_position == 'left'\n\n\n\n\n", "def test_colorbar_fraction_position():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.random.rand(10, 10))\n    cax = fig.add_axes([0.85, 0.15, 0.05, 0.7],\n                       label='colorbar')\n\n    cb = fig.colorbar(pc, cax=cax)\n    assert cax.get_position().x0 == 0.85\n    assert cax.get_position().y0 == 0.15\n    assert cax.get_position().width == 0.05\n    assert cax.get_position().height == 0.7\n\n", "def test_colorbar_with_ticks_and_label_rotation():\n    fig, ax = plt.subplots()\n    im = ax.imshow([[1, 2], [3, 4]])\n    cb = fig.colorbar(im, ax=ax, label='cbar')\n    cb.ax.tick_params(rotation=45, labelsize=8)\n\n    plt.draw()\n\n", "def test_colorbar_fraction_position():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.arange(100).reshape(10, 10))\n    cb = fig.colorbar(pc, ax=ax, fraction=0.1, pad=0.05)\n    assert 0.05 <= cb.ax.get_position().bounds[0] < 0.15\n\n", "def test_colorbar_position_with_gridspec():\n    fig = plt.figure(figsize=(6, 4))\n    gs = gridspec.GridSpec(2, 2, height_ratios=[1, 3])\n    ax1 = fig.add_subplot(gs[0])\n    pc = ax1.pcolormesh(np.arange(100).reshape(10, 10))\n    cbar = fig.colorbar(pc, ax=ax1, location='right', shrink=0.8,\n                        pad=0.05,\n                        fraction=0.04)\n\n    # Make sure the colorbar doesn't overlap with the main axes\n    assert cbar.ax.get_position().x0 > ax1.get_position().x1\n    assert cbar.ax.get_position().ymax < ax1.get_position().y1\n\n\n\n"], "sample_759": ["def test_encoder_categorical_feature_indices():\n    X = np.array([['a', 2, 1], ['b', 1, 2], ['a', 3, 1]])\n    enc = OneHotEncoder(categorical_features=[0, 2])\n    feature_indices = enc.get_feature_names_out('X')\n    expected_indices = ['X_0_a', 'X_0_b', 'X_2_1', 'X_2_2', 'X_2_3']\n    assert_array_equal(feature_indices, expected_indices)\n", "def test_one_hot_encoder_sparse_output():\n    X = np.array([[1, 2], [3, 4]])\n    enc = OneHotEncoder(sparse=True)\n    X_tr = enc.fit_transform(X)\n    assert isinstance(X_tr, sparse.csr_matrix)\n    assert X_tr.shape == (2, 4)\n", "def test_encoder_sparse_output():\n    X = np.array([[1, 2], [3, 4]])\n    enc = OneHotEncoder(sparse=True)\n    X_tr = enc.fit_transform(X)\n    assert isinstance(X_tr, sparse.csr_matrix)\n    enc = OrdinalEncoder(sparse=True)\n    X_tr = enc.fit_transform(X)\n    assert isinstance(X_tr, sparse.csr_matrix)\n\n\n\n", "compilation error", "def test_one_hot_encoder_sparse_categorical_feature_indices():\n    X = np.array([[1, 'a', 55],\n                  [2, 'b', 55]], dtype=object).T\n    enc = OneHotEncoder(sparse=True, categories='auto')\n    X_tr = enc.fit_transform(X)\n    assert isinstance(X_tr, sparse.csr_matrix)\n    assert_array_equal(X_tr.getnnz(axis=0), [1, 1, 1])\n\n    X_tr_indices = enc.transform(X).toarray()\n    assert isinstance(X_tr_indices, np.ndarray)\n    assert_array_equal(X_tr_indices[0, :], [0, 1, 0])\n    assert_array_equal(X_tr_indices[1, :], [0, 0, 1])\n\n    # check if get_feature_names_out works for sparse matrix\n\n    names = enc.get_feature_names_out(X_tr.getnnz(axis=0))\n    assert_array_equal(names, ['x0_1', 'x1_a', 'x2_55'])\n\n"], "sample_581": ["compilation error", "    def index2():\n        return flask.request.endpoint\n    ", "    def index():\n        return flask.request.endpoint\n", "    def index2():\n        return flask.request.endpoint\n", "    def index2():\n        return flask.request.endpoint\n"], "sample_596": ["def test_concat_different_dims(self):\n    da1 = DataArray([1, 2], dims=\"x\")\n    da2 = DataArray([3, 4], dims=\"y\")\n    with raises_regex(ValueError, \"Dimensions 'x' and 'y'\"):\n        concat([da1, da2], dim=\"z\")\n\n    da1 = DataArray([1, 2], dims=(\"x\", \"y\"))\n    da2 = DataArray([3, 4], dims=(\"x\", \"z\"))\n    with raises_regex(ValueError, \"Dimensions 'y' and 'z' are not compatible\"):\n        concat([da1, da2], dim=\"x\")\n\n    da1 = DataArray([[1, 2], [3, 4]], dims=(\"x\", \"y\"))\n    da2 = DataArray([[5, 6], [7, 8]], dims=(\"x\", \"z\"))\n    with raises_regex(ValueError, \"Dimensions 'y' and 'z' are not compatible\"):\n        concat([da1, da2], dim=\"x\")\n\n    da1 = DataArray([1, 2, 3], dims=\"x\")\n    da2 = DataArray([4, 5, 6], dims=\"y\")\n    with raises_regex(ValueError, \"Cannot concatenate\"):\n        concat([da1, da2], dim=\"z\")\n\n    da1 = DataArray([1, 2], dims=(\"x\", \"y\"))\n    da2 = DataArray([3, 4], dims=(\"x\", \"y\"))\n    assert_identical(concat([da1, da2], dim=\"y\"), DataArray([[1, 2, 3, 4],], dims=(\"x\", \"y\")))\n\n\n\n", "    def test_concat_preserve_attributes(self):\n        da1 = DataArray([1, 2], coords=[(\"x\", [1, 2])], attrs={\"a\": 1, \"b\": \"hello\"})\n        da2 = DataArray([3, 4], coords=[(\"x\", [3, 4])], attrs={\"a\": 2, \"c\": \"world\"})\n        actual = concat([da1, da2], dim=\"x\")\n        assert actual.attrs == {\"a\": 1, \"b\": \"hello\", \"c\": \"world\"}\n", "    def test_concat_merge_multiple_non_dim_coords(self):\n        da1 = DataArray([1, 2, 3], dims=\"x\", coords={\"x\": [1, 2, 3], \"y\": 1, \"z\": [0, 1, 2]})\n        da2 = DataArray([4, 5, 6], dims=\"x\", coords={\"x\": [4, 5, 6], \"y\": 2, \"z\": [3, 4, 5]})\n\n        expected = DataArray(range(1, 7), dims=\"x\", coords={\"x\": range(1, 7), \"y\": [1, 2], \"z\": [0, 1, 2, 3, 4, 5]})\n\n        for coords in [\"different\", \"minimal\"]:\n            actual = concat([da1, da2], \"x\", coords=coords)\n            assert_identical(actual, expected)\n\n        with raises_regex(ValueError, \"'y' not present in all datasets\"):\n            concat([da1, da2], dim=\"x\", coords=\"all\")\n", "def test_concat_fill_value_with_complex_fill_argument(fill_value):\n    da1 = DataArray([1, 2], dims=[\"x\", \"y\"], coords={\"x\": [1, 2], \"y\": [0, 1]})\n    da2 = DataArray([3, 4], dims=[\"x\", \"y\"], coords={\"x\": [1, 2], \"y\": [2, 3]})\n    if isinstance(fill_value, dict):\n        fill_value_dict = fill_value\n    else:\n        fill_value_dict = {\"a\": fill_value}\n    expected = DataArray(\n        [[1, 2, fill_value_dict[\"a\"]],\n         [3, 4, fill_value_dict[\"a\"]]], dims=[\"y\", \"x\"],\n        coords={\"x\": [1, 2], \"y\": [0, 1, 2, 3]}\n    )\n\n    actual = concat([da1, da2], dim=\"y\", fill_value=fill_value_dict)\n    assert_identical(actual, expected)\n\n\n\n", "    def test_concat_merge_multiple_non_dim_coords(self):\n        da1 = DataArray([[1, 2], [3, 4]], dims=(\"x\", \"y\"), coords={\"x\": [1, 2], \"y\": [1, 2], \"z\": 10})\n        da2 = DataArray([[5, 6], [7, 8]], dims=(\"x\", \"y\"), coords={\"x\": [3, 4], \"y\": [1, 2], \"z\": 20})\n\n        expected = DataArray(\n            [[1, 2, 5, 6], [3, 4, 7, 8]],\n            dims=(\"x\", \"y\"),\n            coords={\"x\": [1, 2, 3, 4], \"y\": [1, 2, 1, 2], \"z\": [10, 20]},\n        )\n\n        for coords in [\"different\", \"minimal\"]:\n            actual = concat([da1, da2], \"x\", coords=coords)\n            assert_identical(actual, expected)\n"], "sample_454": ["    def test_expressions_without_opclasses(self):\n        msg = (\n            \"UniqueConstraint.opclasses cannot be used with expressions. \"\n            \"Use django.contrib.postgres.indexes.OpClass() instead.\"\n        )\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                Lower(\"field_1\"),\n                name=\"test_func\",\n                opclasses=[\"jsonb_path_ops\"],\n            )\n", "    def test_opclass_with_case_insensitive_index(self):\n        class ModelWithOpClass(models.Model):\n            name = models.CharField(max_length=255)\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=[\"name\"],\n                        name=\"name_ci_uniq\",\n                        opclasses=[\"ci_ops\"],\n                    )\n                ]\n\n        with connection.cursor() as cursor:\n            cursor.execute(\n                \"CREATE INDEX idx_name ON ModelWithOpClass (name) USING gin (ci_ops)\"\n            )\n        obj1 = ModelWithOpClass.objects.create(name=\"Test\")\n        obj2 = ModelWithOpClass.objects.create(name=\"TEST\")\n        with self.assertRaises(IntegrityError):\n            ModelWithOpClass.objects.create(name=\"Test\")\n\n\n", "    def test_expressions_with_include(self):\n        msg = \"UniqueConstraint.include cannot be used with expressions.\"\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                Lower(\"field\"),\n                name=\"test_func_include\",\n                include=[\"other_field\"],\n            )\n", "    def test_expressions_with_include(self):\n        msg = \"UniqueConstraint.include cannot be used with expressions.\"\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                Lower(\"field\"),\n                name=\"test_func_include\",\n                include=[\"nonexistent_field\"],\n            )\n", "    def test_expression_with_condition(self):\n        constraint = models.UniqueConstraint(\n            Lower(\"name\"),\n            name=\"name_lower_without_color_uniq\",\n            condition=models.Q(color__isnull=True),\n        )\n        with self.assertRaisesMessage(\n            ValueError,\n            \"UniqueConstraint with expressions cannot be combined with a condition.\",\n        ):\n            constraint.validate(UniqueConstraintProduct, UniqueConstraintProduct(name=\"test\"))\n\n"], "sample_113": ["    def test_field_description(self):\n        self.assertEqual(views.get_field_description(fields.BooleanField()), 'Boolean (Either True or False)')\n        self.assertEqual(views.get_field_description(DescriptionLackingField()), '')\n\n\n        class CustomFieldWithDescription(models.Field):\n            description = \"Another custom field type\"\n        \n        self.assertEqual(views.get_field_description(CustomFieldWithDescription()), \"Another custom field type\")\n\n", "    def test_field_with_default(self):\n        class MyField(models.Field):\n            description = \"My custom field\"\n            default = \"My default value\"\n        self.assertEqual(\n            views.get_readable_field_data_type(MyField()),\n            'My custom field (default: My default value)'\n        )\n", "    def test_simplify_regex(self):\n        self.assertEqual(utils.simplify_regex('^Hello(.*)World$'), r'^Hello(.+?)World$')\n        self.assertEqual(utils.simplify_regex('(?P<name>[\\w\\d]+)'), r'\\g<name>')\n        self.assertEqual(utils.simplify_regex('(?!\\d)'), r'(?!\\d)')\n        self.assertEqual(utils.simplify_regex('a.*b'), r'a.*b')\n\n        self.assertEqual(utils.simplify_regex('(?P<name>[\\w\\d]+)(?!\\d)'), r'\\g<name>(?!\\d)')\n        self.assertEqual(utils.simplify_regex('^a.*b$'), r'^a.*b$')\n        self.assertEqual(utils.simplify_regex('(?P<name>[\\w\\d]+)(?!\\d){2}'), r'\\g<name>(?!\\d){2}')\n\n\n\n", "    def test_field_description_rendering(self):\n        class MyModel(models.Model):\n            name = models.CharField(max_length=100, description=\"This is the name field.\")\n\n        with self.subTest('field description'):\n            response = self.client.get(reverse('django-admindocs-models-detail', args=['admin_docs', 'model_name']))\n            self.assertContains(response, '<td>name - This is the name field.</td>')\n\n\n\n\n", "    def test_filter_rendering(self):\n        response = self.client.get(reverse('django-admindocs-filters'))\n        for filter_name in ['lower', 'upper', 'slugify']:\n            self.assertContains(response, f'<h3><a href=\"/admindocs/filters/{{ filter_name }}\">{{ filter_name }}</a></h3>', html=True)\n\n\n\n"], "sample_443": ["    def test_backend_constructor_cache_type(self):\n        with self.subTest(\"Memcached\"):\n            test_caches = CacheHandler(\n                {\n                    \"memcache\": {\n                        \"BACKEND\": \"django.core.cache.backends.memcached.MemcachedCache\",\n                        \"LOCATION\": [\"127.0.0.1:11211\"],\n                    },\n                }\n            )\n            cache = test_caches[\"memcache\"]\n            self.assertIsInstance(cache, MemcachedCache)\n        with self.subTest(\"Redis\"):\n            test_caches = CacheHandler(\n                {\n                    \"redis\": {\n                        \"BACKEND\": \"django.core.cache.backends.redis.RedisCache\",\n                        \"LOCATION\": \"redis://127.0.0.1:6379/0\",\n                    },\n                }\n            )\n            cache = test_caches[\"redis\"]\n            self.assertIsInstance(cache, RedisCache)\n\n\n", "    def test_get_cache_key_with_vary_headers(self):\n        request = self.factory.get(self.path)\n        template = engines[\"django\"].from_string(\"This is a test\")\n        response = TemplateResponse(HttpRequest(), template)\n        response.headers[\"Vary\"] = \"Accept-Encoding\"\n        learn_cache_key(request, response)\n        key = get_cache_key(request)\n        self.assertIn(\"Accept-Encoding\" in key, key)\n\n\n\n\n", "    def test_cache_response_with_headers(self):\n        template = engines[\"django\"].from_string(\"This is a test\")\n        response = TemplateResponse(HttpRequest(), template)\n        response.headers[\"Content-Type\"] = \"text/html; charset=utf-8\"\n        response.headers[\"X-Custom-Header\"] = \"Some value\"\n        key_prefix = \"my_prefix\"\n        caches[\"default\"].set(\"my_key\", response)\n\n        request = self.factory.get(self.path)\n        get_cache_key(request, key_prefix=key_prefix)\n        cached_response = caches[\"default\"].get(\"my_key\")\n\n        self.assertEqual(cached_response.content, b\"This is a test\")\n        self.assertEqual(cached_response.headers[\"Content-Type\"], \"text/html; charset=utf-8\")\n        self.assertIn(\"X-Custom-Header\", cached_response.headers)\n        self.assertEqual(cached_response.headers[\"X-Custom-Header\"], \"Some value\")\n", "    def test_empty_response(self):\n        response = EmptyResponse()\n        middleware = CacheMiddleware(empty_response)\n        request = self.factory.get(self.path)\n        response = middleware.process_response(request, response)\n\n        # Check that the response was not modified because it's empty.\n\n        self.assertEqual(response.content, b'')\n\n\n\n", "    def test_cache_backend_config(self):\n        with self.assertRaises(TypeError):\n            CacheHandler({\n                \"foo\": {\n                    \"BACKEND\": \"django.core.cache.backends.locmem.LocMemCache\",\n                    \"INVALID_OPTION\": True,\n                }\n            })\n        with self.assertRaises(ValueError):\n            CacheHandler({\n                \"foo\": {\n                    \"BACKEND\": \"django.core.cache.backends.locmem.LocMemCache\",\n                    \"TIMEOUT\": \"invalid\"\n                }\n            })\n"], "sample_620": ["def test_concat_mixed_dim_sizes(self) -> None:\n    ds1 = Dataset(\n        {\"a\": ((\"x\", \"y\"), np.array([[1, 2], [3, 4]])), \"b\": ((\"x\", \"y\"), np.array([[5, 6], [7, 8]])),},\n        {\"x\": [0, 1], \"y\": [0, 1]},\n    )\n    ds2 = Dataset(\n        {\"a\": ((\"x\", \"y\"), np.array([[9, 10], [11, 12], [13, 14]])), \"b\": ((\"x\", \"y\"), np.array([[15, 16], [17, 18], [19, 20]])),},\n        {\"x\": [0, 1, 2], \"y\": [0, 1]},\n    )\n\n    with pytest.raises(ValueError, match=r\"Dimensions 'x' and 'y' not aligned.*\"):\n        concat([ds1, ds2], dim=\"x\")\n\n    with pytest.raises(ValueError, match=r\"Dimensions 'x' and 'y' not aligned.*\"):\n        concat([ds1, ds2], dim=\"y\")\n\n\n\n", "def test_concat_different_dimensions() -> None:\n    ds1 = Dataset({\"foo\": ((\"x\", \"y\"), np.ones((2, 3))), \"x\": [0, 1]})\n    ds2 = Dataset({\"foo\": ((\"y\", \"z\"), np.ones((3, 4))), \"y\": [2, 3]})\n\n    with pytest.raises(ValueError, match=r\"Dimensions 'x' and 'y' are not aligned\"):\n        concat([ds1, ds2], dim=\"x\")\n", "def test_concat_string_arrays_with_different_index_lengths() -> None:\n    ds1 = Dataset(data=DataArray([\"a\", \"b\"], dims=\"x\"))\n    ds2 = Dataset(data=DataArray([\"c\", \"d\", \"e\"], dims=\"x\"))\n\n    with pytest.raises(ValueError, match=r\"Index lengths must be the same\"):\n        concat([ds1, ds2], dim=\"x\")\n", "compilation error", "compilation error"], "sample_1194": ["def test_julia_indexed():\n    x = IndexedBase('x', shape=(10,))\n    i = Idx('i')\n    y = IndexedBase('y', shape=(5,))\n    j = Idx('j')\n    expr = x[i] + y[j]\n    assert julia_code(expr) == \"x[i] + y[j]\"\n    assert julia_code(expr, contract=False) == \"x[i] + y[j]\"\n    assert julia_code(expr, contract=True) == \"sum(x[i] + y[j] for i in 0:9 for j in 0:4)\"\n\n\n\n", "compilation error", "def test_indexed():\n    x = IndexedBase('x', shape=(3,))\n    y = IndexedBase('y', shape=(3,))\n    i = Idx('i')\n    assert julia_code(x[i]**2 + y[i]) == \"x[i] .^ 2 + y[i]\"\n    assert julia_code(x[i] * y[i]) == \"x[i] .* y[i]\"\n    assert julia_code(x[i] * y[i+1]) == \"x[i] .* y[i + 1]\"\n    assert julia_code(x[i:i + 2]) == \"x[i : i + 2]\"\n    assert julia_code(x[[i, i + 1]]) == \"x[i, i + 1]\"\n    assert julia_code((x[i] + y[i])**2) == \"(x[i] + y[i]) .^ 2\"\n\n\n\n", "def test_julia_indexed():\n    x = IndexedBase('x', shape=(5,))\n    y = IndexedBase('y', shape=(3,))\n    i = Idx('i', x.shape[0])\n    j = Idx('j', y.shape[0])\n    expr = x[i] - y[j]\n    assert julia_code(expr) == \"x[i] - y[j]\"\n    assert julia_code(sum(x[i] for i in range(x.shape[0]))) == \"sum(x[i for i in 1:size(x))\"\n    indexed_func = lambda x: x[i] + x[j]\n    assert julia_code(indexed_func(x)) == \"x[i] + x[j]\"\n    indexed_func = lambda x: x[i] + y[j]\n    assert julia_code(indexed_func(x)) == \"x[i] + y[j]\"\n", "def test_julia_Indexed():\n    from sympy.core import IndexedBase, Idx, Symbol\n    x = Symbol('x')\n    y = Symbol('y')\n\n    i = Idx('i', 5)\n    j = Idx('j', 3)\n    A = IndexedBase('A')\n    B = IndexedBase('B')\n\n    # test assignment\n    assert julia_code(A[i] = x) == 'A[i] = x'\n    assert julia_code(B[i, j] = x + y) == 'B[i, j] = x + y'\n    assert julia_code(A[i] = y**2) == 'A[i] = y .^ 2'\n    assert julia_code((A[i] + B[i, j]) * 2) == '(A[i] + B[i, j]) * 2'\n\n    # test expressions\n    assert julia_code(A[i] + A[i]) == 'A[i] + A[i]'\n    assert julia_code(A[i] * B[i, j]) == 'A[i] * B[i, j]'\n    assert julia_code(sum(A[i])) == 'sum(A[i])'\n    assert julia_code(sum(B[i, j])) == 'sum(B[i, j])'\n    assert julia_code(A[i] * x + B[i, j] * y) == 'A[i] * x + B[i, j] * y'\n\n    # test loop\n    assert julia_code(for i in range(5): A[i] = x) == (\n        \"for i in 1:5\\n\"\n        \"    A[i] = x\\n\"\n        \"end\"\n    )\n"], "sample_927": ["        def assert_consistency(self):\n            for tag in self.content_classes:\n                assert self.classes.issubset(self.content_classes[tag]), (\n                    f\"Classes for role '{self.name}' with tag '{tag}' \"\n                    f\"should be a subset of its root classes. \"\n                    f\"Found {self.content_classes[tag]} but expected a subset of {self.classes}\"\n                )\n", "        def assert_same_classes(self, tag, msg=None):\n            if msg is None:\n                msg = f\"Classes for '{tag}' should be the same as '{self.name}'\"\n            assert self.content_classes[tag] == self.classes, msg\n", "        def get_role_classes(self, tag=''):\n            return self.classes if tag == '' else self.content_classes.get(tag)\n", "    def check_class_references(role, tag, expected_classes):\n        classes_found = self.classes[role]\n        assert expected_classes.issubset(classes_found), (\n            f\"Role '{role}' with tag '{tag}' expected classes {expected_classes} \"\n            f\"but found {classes_found}\"\n        )\n", "        def assert_same_classes(self, tag_name, other_role):\n            assert self.content_classes.get(tag_name, set()) == \\\n                other_role.content_classes.get(tag_name, set())\n"], "sample_816": ["def test_vectorizer_build_analyzer_callable(Estimator):\n        return [word for word in doc.split() if word.isupper()]\n    vect = Estimator(analyzer=analyzer)\n    X = vect._build_analyzer('hello WORLD')\n    assert_array_equal(X, ['WORLD'])\n", "def test_sparse_output_dtype_default_consistent(vectorizer_class):\n    X_dense = np.array([[1, 2, 3], [4, 5, 6]])\n    vec = vectorizer_class()\n    vec.fit_transform(X_dense)\n    X_sparse = vec.transform(X_dense)\n    assert X_sparse.dtype == np.float64\n", "compilation error", "def test_invalid_ngram_range_error_message(Estimator):\n    vec = Estimator()\n    # Ensures the error message is helpful by specifying which parameter is invalid\n    message = r\"Invalid value for ngram_range=\\d+.*lower boundary larger than the upper boundary.\"\n    with pytest.raises(ValueError, match=message):\n        vec.fit_transform([\"hello\"])\n", "def test_custom_stop_words_handling_in_fit_transform(Estimator):\n    data = [\n        \"This is a document with some stop words.\",\n        \"This is another document.\",\n    ]\n    custom_stop_words = [\"This\", \"is\"]\n    vec = Estimator(stop_words=custom_stop_words)\n    X_fitted = vec.fit_transform(data)\n    vocabulary = vec.vocabulary_\n    for doc in data:\n        tokens = vec.build_tokenizer()(doc)\n        for token in tokens:\n            if token in custom_stop_words:\n                assert token not in vocabulary\n                assert X_fitted.getnnz(X_fitted.getrow(vocabulary[token])) == 0\n            else:\n                assert token in vocabulary\n                assert X_fitted.getnnz(X_fitted.getrow(vocabulary[token])) > 0\n\n"], "sample_141": ["    def test_datetime_encoding(self):\n        dt = datetime.datetime(2023, 10, 26, 10, 30)\n        self.assertEqual(\n            json.dumps({'dt': dt}, cls=DjangoJSONEncoder),\n            '{\"dt\": \"2023-10-26T10:30:00\"}'\n        ) \n\n\n\n", "    def test_null_fields(self):\n        data = {'pk': 1, 'model': 'serializers.article', 'fields': {'headline': 'Test', 'pub_date': '2023-04-27T12:00:00', 'categories': [1], 'author': 1, 'some_null_field': None}}\n        json_data = json.dumps(data, cls=DjangoJSONEncoder)\n        self.assertEqual(json_data, '{\"pk\": 1, \"model\": \"serializers.article\", \"fields\": {\"headline\": \"Test\", \"pub_date\": \"2023-04-27T12:00:00\", \"categories\": [1], \"author\": 1, \"some_null_field\": null}}')\n\n\n", "    def test_decimal_field_serialization(self):\n        class Article(models.Model):\n            price = models.DecimalField(max_digits=10, decimal_places=2)\n\n        article = Article(price=123.45)\n        json_data = serializers.json.Serializer().serialize([article])\n        self.assertIn('\"price\": \"123.45\"', json_data)\n\n\n\n", "    def test_datetime_encoding(self):\n        dt = datetime.datetime(2023, 10, 26, 10, 30, 0)\n        self.assertEqual(json.dumps({'dt': dt}, cls=DjangoJSONEncoder), '{\"dt\": \"2023-10-26T10:30:00\"}')\n", "    def test_date_time_serialization(self):\n        now = datetime.datetime.now()\n        s = serializers.json.Serializer()\n        json_data = s.serialize([Score(score=5.0, created_at=now, updated_at=now)])\n        self.assertIn('\"created_at\": \"'+now.isoformat()+'\"', json_data)\n        self.assertIn('\"updated_at\": \"'+now.isoformat()+'\"', json_data)\n\n\n\n"], "sample_623": ["    def test_preferred_chunks_with_list(self, shape, pref_chunks, req_chunks):\n        initial = self.create_dataset(shape, pref_chunks)\n        final = xr.open_dataset(\n            initial, engine=PassThroughBackendEntrypoint, chunks=req_chunks\n        )\n        self.check_dataset(initial, final, explicit_chunks(pref_chunks, shape))\n", "    def test_chunks_with_backend_defaults(\n        self, shape, pref_chunks, req_chunks, expected_chunks", "    def test_backend_adjusts_chunks(self, shape, pref_chunks):\n        initial = self.create_dataset(shape, pref_chunks)\n\n        class BackendThatAdjustsChunks(xr.backends.BackendEntrypoint):\n                self, filename_or_obj, *, drop_variables=None\n            ) -> xr.Dataset:\n                return initial\n            \n                # Simulate the backend trying to adjust chunks\n                # (e.g., due to memory limitations)\n                data_chunks = dataset[var_name].chunks\n                if isinstance(data_chunks, dict):\n                    data_chunks = [\n                        (chunk,) if isinstance(chunk, int) else chunk\n                        for chunk in data_chunks.values()\n                    ]\n                return tuple((chunk // 2,) for chunk in data_chunks)\n\n        with pytest.warns(UserWarning, match=\"Chunks were adjusted\"):\n            final = xr.open_dataset(\n                initial, engine=BackendThatAdjustsChunks, chunks=None\n            )\n\n        expected_chunks = explicit_chunks(\n            pref_chunks, shape\n        )\n        self.check_dataset(initial, final, expected_chunks)\n", "    def test_prefered_chunks_with_none_request(self, shape, pref_chunks, req_chunks):\n        \"\"\"When chunks is None, use preferred chunks.\"\"\"\n        initial = self.create_dataset(shape, pref_chunks)\n        final = xr.open_dataset(\n            initial, engine=PassThroughBackendEntrypoint, chunks=req_chunks\n        )\n        self.check_dataset(initial, final, explicit_chunks(pref_chunks, shape))\n", "    def test_chunks_larger_than_preferred(\n        self, shape, pref_chunks, req_chunks"], "sample_462": ["    def test_choicefield_empty_values(self):\n        f = ChoiceField(choices=[(\"1\", \"One\"), (\"2\", \"Two\")], empty_values=[\"\", None])\n        self.assertEqual(\"\", f.clean(\"\"))\n        self.assertEqual(None, f.clean(None))\n        self.assertEqual(\"1\", f.clean(1))\n        self.assertEqual(\"1\", f.clean(\"1\"))\n        msg = \"'Select a valid choice. 3 is not one of the available choices.'\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            f.clean(\"3\")\n\n\n\n", "    def test_choicefield_required_with_empty_choices(self):\n        f = ChoiceField(choices=[], required=True)\n        with self.assertRaisesMessage(ValidationError, \"'This field is required.'\"):\n            f.clean()\n", "    def test_choicefield_empty_values(self):\n        f = ChoiceField(choices=[(\"1\", \"One\"), (\"2\", \"Two\")], empty_values=[\"\", \" \"])\n        self.assertEqual(\"\", f.clean(\"\"))\n        self.assertEqual(\"1\", f.clean(1))\n        self.assertEqual(\"1\", f.clean(\"1\"))\n        msg = \"'Select a valid choice. 3 is not one of the available choices.'\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            f.clean(\"3\")\n", "    def test_choicefield_required_error(self):\n        f = ChoiceField(choices=[(\"J\", \"John\"), (\"P\", \"Paul\")], required=True)\n        self.assertEqual(\n            f.error_messages[\"required\"], \"This field is required.\"\n        )\n\n\n\n", "    def test_choicefield_empty_value(self):\n        f = ChoiceField(choices=[(\"1\", \"One\"), (\"2\", \"Two\")], empty_value=\"\")\n        self.assertEqual(\"\", f.clean(\"\"))\n        self.assertEqual(\"1\", f.clean(1))\n        self.assertEqual(\"1\", f.clean(\"1\"))\n\n\n\n"], "sample_245": ["    def test_project_locale_paths_with_custom_template_dir(self):\n        \"\"\"\n        When custom TEMPLATE_DIRS are set, locale files are stored \n        in the specified directories.\n        \"\"\"\n        with override_settings(LOCALE_PATHS=[self.test_dir],\n                               TEMPLATE_DIRS=[self.test_dir]):\n            management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n            project_de_locale = os.path.join(self.test_dir, 'de', 'LC_MESSAGES', 'django.po')\n            self.assertTrue(os.path.exists(project_de_locale))\n", "    def test_custom_template_paths(self):\n        with override_settings(TEMPLATE_DIRS=[os.path.join(self.test_dir, 'templates_extra')]):\n            management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n            with open(self.PO_FILE) as fp:\n                po_contents = fp.read()\n                self.assertMsgId('This message is from a custom template path', po_contents)\n", "    def test_complex_locale_path(self):\n        \"\"\"\n        Regression test for #21627:\n        *  LOCALE_PATHS can contain arbitrary paths.\n        \"\"\"\n        with override_settings(LOCALE_PATHS=['/locale/path/here',\n                                              os.path.join(self.test_dir, 'project_locale')]):\n            management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n            project_de_locale = os.path.join(self.test_dir, 'project_locale', 'de', 'LC_MESSAGES', 'django.po')\n            self.assertTrue(os.path.exists(project_de_locale))\n", "    def test_custom_language_domain(self):\n        \"\"\"\n        Tests that users can specify a custom language domain for the generated .pot file.\n        \"\"\"\n        with override_settings(LANGUAGE_CODE='en', LANGUAGES=[('en', 'English'), ('de', 'German')]):\n            management.call_command('makemessages', locale=['de'], verbosity=0, language_domain='my_domain')\n            self.assertTrue(os.path.exists('locale/de/LC_MESSAGES/my_domain.pot'))\n            with open('locale/de/LC_MESSAGES/my_domain.pot', encoding='utf-8') as fp:\n                pot_contents = fp.read()\n                self.assertIn('Content-Type: text/plain; charset=UTF-8', pot_contents)\n                self.assertIn('my_domain', pot_contents)\n\n\n", "    def test_custom_layout_with_multiple_locale_paths(self):\n        \"\"\"\n        *  translations for an app containing a locale folder are stored in that folder\n        * translations outside of that app are in LOCALE_PATHS[0]\n        *  multiple locale paths are supported\n        \"\"\"\n        locale_paths = [os.path.join(self.test_dir, 'project_locale1'),\n                        os.path.join(self.test_dir, 'project_locale2')]\n        with override_settings(LOCALE_PATHS=locale_paths):\n            management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n            project1_de_locale = os.path.join(\n                self.test_dir, 'project_locale1', 'de', 'LC_MESSAGES', 'django.po')\n            project2_de_locale = os.path.join(\n                self.test_dir, 'project_locale2', 'de', 'LC_MESSAGES', 'django.po')\n            app_de_locale = os.path.join(\n                self.test_dir, 'app_with_locale', 'locale', 'de', 'LC_MESSAGES', 'django.po')\n            self.assertTrue(os.path.exists(project1_de_locale))\n            self.assertTrue(os.path.exists(project2_de_locale))\n            self.assertTrue(os.path.exists(app_de_locale))\n"], "sample_792": ["def test_discrete_prior_smoothing():\n    # Test smoothing of discrete prior for unobserved targets\n    for cls in [BernoulliNB, MultinomialNB]:\n        X = np.array([[0, 1], [1, 0]])\n        y = np.array([0, 1])\n\n        # Test with 'fit_prior=False'\n        clf = cls(fit_prior=False)\n        clf.fit(X, y)\n        prior = np.exp(clf.class_log_prior_)\n        assert_array_almost_equal(prior, np.array([0.5, 0.5]))\n\n        # Test with 'fit_prior=True'  and additional classes\n        clf = cls(fit_prior=True, classes=[0, 1, 2])\n        clf.fit(X, y)\n        prior = np.exp(clf.class_log_prior_)\n        assert_array_almost_equal(prior, np.array([0.3333, 0.3333, 0.3333]))\n\n        # Test with partial_fit\n        clf = cls(fit_prior=True, classes=[0, 1, 2])\n        clf.partial_fit(X[:1], y[:1], classes=[0, 1, 2])\n        clf.partial_fit(X[1:], y[1:], classes=[0, 1, 2])\n        prior = np.exp(clf.class_log_prior_)\n        assert_array_almost_equal(prior, np.array([0.5, 0.5, 0]))\n\n\n\n", "def test_predict_proba_discrete_with_unknown_classes():\n    # Test for unknown classes in predict_proba, both Bernoulli and Multinomial.\n\n    X = np.array([[0, 1, 0], [1, 1, 1], [0, 0, 1]])\n    y = np.array([0, 1, 2])  \n\n    for cls in [BernoulliNB, MultinomialNB]:\n        clf = cls()\n        clf.fit(X, y)\n\n        # Try predicting for a sample with a class not in the training data\n        new_sample = np.array([[0, 1, 0]])\n        with pytest.raises(ValueError):\n            clf.predict_proba(new_sample, classes=[0, 1, 3])\n\n        # Try predicting for a sample with unknown classes in a list\n\n        new_sample = np.array([[0, 1, 0]])\n        with pytest.raises(ValueError):\n            _, predicted_class = clf.predict_proba(new_sample,\n                                                  classes=[0, 2, 3])\n\n\n\n", "def test_missing_values():\n    # Test handling of missing values with GaussianNB\n\n    # Create a dataframe with missing values\n    data = {'feature1': [1, 2, np.nan, 4],\n            'feature2': [5, np.nan, 7, 8],\n            'target': [0, 1, 0, 1]}\n    df = pd.DataFrame(data)\n\n    # Test GaussianNB with default imputation\n    clf = GaussianNB()\n    clf.fit(df)\n    # Check that imputation was done correctly\n    assert_array_almost_equal(clf.theta_, np.array([[0.28, 2.14]]))\n\n    # Test with explicit imputation\n    clf = GaussianNB(handle_missing='value', missing_value=0)\n    clf.fit(df)\n    assert_array_almost_equal(clf.theta_, np.array([[0.28, 2.14]]))\n\n\n    # Test with invalid imputation strategy\n    with pytest.raises(ValueError):\n        clf = GaussianNB(handle_missing='invalid')\n        clf.fit(df)\n", "def test_nb_predict_proba_zero_probability_alpha():\n    # Test that predict_proba handles cases where probability is 0 with\n    # alpha smoothing\n    X = np.array([[1, 0, 0], [0, 1, 1], [1, 1, 0]])\n    y = np.array([0, 1, 0])\n\n    # Train a MultinomialNB model with alpha=0\n    clf = MultinomialNB(alpha=0)\n    clf.fit(X, y)\n\n    # Predict probabilities where probability of a class is 0 in the training data\n    test_data = np.array([[0, 0, 1]])\n\n    # Check that probabilities are not 0 but a very small value\n    proba = clf.predict_proba(test_data)\n    for i in range(proba.shape[1]):\n        assert_greater(proba[0][i], 0)\n    \n    # Similarly for BernoulliNB\n    clf = BernoulliNB(alpha=0)\n    clf.fit(X, y)\n    test_data = np.array([[0, 0, 1]])\n    proba = clf.predict_proba(test_data)\n    for i in range(proba.shape[1]):\n        assert_greater(proba[0][i], 0)\n", "def test_partial_fit_predict_proba_with_missing_values():\n    # Test prediction proba with missing values after partial_fit\n    X = np.array([[1, 2, np.nan], [4, np.nan, 6], [7, 8, 9]])\n    y = np.array([0, 1, 2])\n\n    clf = MultinomialNB()\n    clf.partial_fit(X[:1], y[:1], classes=[0, 1, 2])\n    clf.partial_fit(X[1:2], y[1:2], classes=[0, 1, 2])\n\n    result = clf.predict_proba(X)\n    assert_array_almost_equal(result, result)\n"], "sample_840": ["compilation error", "def test_partial_least_squares_regression_with_missing_data():\n    # Test handling of missing data\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n\n    # Introduce random missing values in X\n    mask = np.random.rand(X.shape[0], X.shape[1]) < 0.1\n    X[mask] = np.nan\n\n    # Test with different PLS variants\n    for clf in [pls_.PLSRegression(), pls_.PLSCanonical(),\n                pls_.PLSSVD()]:\n        clf.fit(X, Y)\n        assert clf.n_iter_  > 0  # Check that the algorithm converged\n        \n        # Ensure that the fitted model can handle missing values in X \n        # during prediction\n        X_new = X.copy()\n        X_new[mask] = np.nan\n        y_pred = clf.predict(X_new)\n\n        assert y_pred.shape == Y.shape  # Correct shape\n\n        # Check if the model parameters are changed due to missing values\n        # (This might be too strict, as some algorithms might adjust parameters)\n        # assert_array_almost_equal(clf.x_weights_, clf.x_weights_previous,\n        #                         atol=1e-4)  \n\n\n\n", "def test_missing_values():\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n\n    # Introduce some missing values\n    X[0, 1] = np.nan\n    X[2, 3] = np.nan\n    Y[1, 0] = np.nan\n\n    for clf in [pls_.PLSCanonical(), pls_.PLSRegression(), pls_.PLSSVD()]:\n        clf.fit(X, Y)\n        # Check if the model was fit correctly despite missing values\n        assert clf.y_scores_.shape == (X.shape[0], clf.n_components)\n\n\n", "def test_pls_with_missing_data():\n    from sklearn.datasets import load_breast_cancer\n    data = load_breast_cancer()\n    X = data.data\n    Y = data.target\n\n    # Introduce some missing data\n    missing_indices = np.random.choice(X.shape[0], size=10, replace=False)\n    X[missing_indices] = np.nan\n\n    for clf in [pls_.PLSCanonical(), pls_.PLSRegression(),\n                pls_.PLSSVD()]:\n        # Test with different imputation strategies\n        for strategy in ['mean', 'median', 'kNN']:\n            imp_X = SimpleImputer(strategy=strategy).fit_transform(X)\n            clf.fit(imp_X, Y)\n\n\n", "def test_sparse_data():\n    from scipy.sparse import csr_matrix\n\n    d = load_linnerud()\n    n_samples, n_features = d.data.shape\n\n    # Create a sparse matrix\n    sparse_X = csr_matrix(d.data)\n\n    # Initialize PLS models\n    pls_nipals = pls_.PLSCanonical(algorithm=\"nipals\")\n    pls_svd = pls_.PLSSVD()\n\n    # Fit models with sparse data\n    pls_nipals.fit(sparse_X, d.target)\n    pls_svd.fit(sparse_X, d.target)\n\n    # Check that the models were trained successfully\n    assert pls_nipals.n_components_ == pls_nipals.n_components\n    assert pls_svd.n_components_ == pls_svd.n_components\n\n    # Ensure that the data was handled correctly\n    assert sparse_X.shape == pls_nipals.x_loadings_.shape\n    assert sparse_X.shape == pls_svd.x_loadings_.shape\n    \n    # Verify that the results are consistent with dense data\n    dense_pls_nipals = pls_.PLSCanonical(algorithm=\"nipals\").fit(d.data, d.target)\n    dense_pls_svd = pls_.PLSSVD().fit(d.data, d.target)\n\n    assert_array_almost_equal(pls_nipals.x_loadings_, dense_pls_nipals.x_loadings_)\n    assert_array_almost_equal(pls_svd.x_loadings_, dense_pls_svd.x_loadings_)\n\n\n"], "sample_162": ["    def test_locale_path_is_not_created_if_it_doesnt_exist(self):\n        \"\"\"\n        If a locale path is specified but doesn't exist, an error is raised.\n        \"\"\"\n        with override_settings(LOCALE_PATHS=['/wrong/path/']):\n            with self.assertRaises(CommandError):\n                management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n", "    def test_custom_locale_path(self):\n        with override_settings(LOCALE_PATHS=[os.path.join(self.test_dir, 'custom_locale')]):\n            management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n            custom_locale = os.path.join(self.test_dir, 'custom_locale', LOCALE, 'LC_MESSAGES', 'django.po')\n            self.assertTrue(os.path.exists(custom_locale))\n\n\n", "    def test_custom_locale_paths_from_settings(self):\n        \"\"\"\n        Custom locale paths from settings override default and project paths.\n        \"\"\"\n        with override_settings(LOCALE_PATHS=[os.path.join(self.test_dir, 'custom_locale'),]):\n            management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n            custom_de_locale = os.path.join(\n                self.test_dir, 'custom_locale', 'de', 'LC_MESSAGES', 'django.po')\n            self.assertTrue(os.path.exists(custom_de_locale))\n", "    def test_custom_translation_directory(self):\n        with override_settings(LOCALE_PATHS=[os.path.join(self.test_dir, 'project_locale')],\n                              TRANSLATIONS_DIR='locales'):\n            management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n            project_de_locale = os.path.join(self.test_dir, 'locales', 'de', 'LC_MESSAGES', 'django.po')\n            self.assertTrue(os.path.exists(project_de_locale))\n", "    def test_custom_translation_function(self):\n        from django.utils.translation import override\n        with override('en', lambda s: f'{s} - CUSTOM'):\n            management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n            with open(self.PO_FILE) as fp:\n                po_contents = fp.read()\n                self.assertMsgId('This is a test string - CUSTOM', po_contents)\n\n"], "sample_191": ["    def test_snapshot_files_handles_stat_errors(self, mock_stat):\n        mock_stat.side_effect = OSError()\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n            snapshot = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot)\n            self.assertEqual(snapshot[self.existing_file], (0, 0, 0, 0, 0, 0, 0, 0, 0))\n\n\n\n", "    def test_should_reconsider_file_if_deleted(self, mock_stat):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n            snapshot = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot)\n            mock_stat.side_effect = OSError(errno.ENOENT, 'No such file or directory')\n            with self.tick_twice():\n                self.reloader.snapshot_files()\n            self.assertNotIn(self.existing_file, dict(self.reloader.snapshot_files()))\n", "    def test_snapshot_files_handles_nonexistent_files_gracefully(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.nonexistent_file]):\n            snapshot1 = dict(self.reloader.snapshot_files())\n            self.assertNotIn(self.nonexistent_file, snapshot1)\n            snapshot2 = dict(self.reloader.snapshot_files())\n            self.assertNotIn(self.nonexistent_file, snapshot2)\n", "    def test_snapshot_files_handles_same_mtime(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n            snapshot1 = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot1)\n            # Keep the mtime the same\n            self.increment_mtime(self.existing_file)\n            snapshot2 = dict(self.reloader.snapshot_files())\n            self.assertEqual(snapshot1[self.existing_file], snapshot2[self.existing_file])\n", "    def test_snapshot_files_handles_non_existing_files(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[]):\n            snapshot = dict(self.reloader.snapshot_files())\n            self.assertEqual(snapshot, {})\n\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.nonexistent_file]):\n            snapshot = dict(self.reloader.snapshot_files())\n            self.assertEqual(snapshot, {})\n"], "sample_1209": ["def test_latex_repr():\n    m = PREFIXES['m']\n    assert str(m) == \"m\"\n    assert m._latex(None) == r'\\text{m}'\n    assert PREFIXES['micro']._latex(None) == r\"\\mu\" \n", "def test_prefix_latex():\n    m = PREFIXES['m']\n    assert m._latex(None) == r'\\text{m}'\n    assert kilo._latex(None) == r'\\text{k}'\n    assert micro._latex(None) == r\"\\mu\"\n", "def test_prefix_creation():\n    name1 = \"gigawatts\"\n    abbrev1 = \"GW\"\n    exponent1 = 9\n    base1 = 10\n    prefix1 = Prefix(name1, abbrev1, exponent1, base1)\n    assert prefix1.name == name1\n    assert prefix1.abbrev == abbrev1\n    assert prefix1.scale_factor == 10**exponent1\n    assert prefix1.base == base1\n\n    name2 = \"kibibytes\"\n    abbrev2 = \"KiB\"\n    exponent2 = 10\n    base2 = 2\n    prefix2 = Prefix(name2, abbrev2, exponent2, base2)\n    assert prefix2.name == name2\n    assert prefix2.abbrev == abbrev2\n    assert prefix2.scale_factor == 2**exponent2\n    assert prefix2.base == base2\n", "def test_prefix_str_repr():\n    assert str(PREFIXES['m']) == 'm'\n    assert repr(PREFIXES['m']) == \"Prefix('meter', 'm', 0)\"\n    assert str(kibi) == 'Ki'\n    assert repr(kibi) == \"Prefix('kibi', 'Y', 10, 2)\"\n", "def test_prefix_latex():\n    assert PREFIXES['m']._latex(None) == r'\\text{m}'\n    assert PREFIXES['y']._latex(None) == r'\\text{Y}'\n    assert PREFIXES['m'].__repr__() == \"Prefix('milli', 'm', -3)\" \n"], "sample_175": ["    def test_fast_delete_with_related_fields_on_related_object(self):\n        u = User.objects.create(\n            avatar=Avatar.objects.create(desc='a', large_field='b')\n        )\n        a = Avatar.objects.get(pk=u.avatar_id)\n\n        # Ensure that the related field values exist before deletion\n        self.assertEqual(a.large_field, 'b')\n\n        with self.assertNumQueries(2):\n            a.delete()\n\n        self.assertFalse(u.avatar)\n        self.assertIsNone(a.large_field)\n", "    def test_fast_delete_with_null_fields(self):\n        \"\"\"\n        #26403 - Ensure fast-delete works correctly when there are nullable fields\n        in the model.\n        \"\"\"\n        user = User.objects.create(\n            name='John Doe', nullable_field=None, avatar=Avatar.objects.create()\n        )\n        a = Avatar.objects.get(pk=user.avatar_id)\n        # 1 query to fast-delete the user, 1 query to delete the avatar\n        self.assertNumQueries(2, a.delete)\n        self.assertIsNone(user.pk)\n        self.assertFalse(User.objects.exists())\n        self.assertFalse(Avatar.objects.exists())\n", "    def test_fast_delete_with_related_deletion(self):\n        user = User.objects.create(\n            avatar=Avatar.objects.create()\n        )\n        avatar = Avatar.objects.get(pk=user.avatar_id)\n        avatar.delete()\n        self.assertFalse(Avatar.objects.exists())\n        self.assertFalse(User.objects.exists())\n        \n        # Check if the related deletion cascade works with multiple objects\n        user2 = User.objects.create(\n            avatar=Avatar.objects.create()\n        )\n        user3 = User.objects.create(\n            avatar=Avatar.objects.create()\n        )\n        Avatar.objects.filter(pk__in=[user2.avatar_id, user3.avatar_id]).delete()\n        self.assertFalse(Avatar.objects.exists())\n        self.assertEqual(User.objects.count(), 0)", "    def test_fast_delete_non_existent_parent(self):\n        \"\"\"\n        Fast deleting a child object that has a non-existent parent should\n        result in a `DoesNotExist` exception.\n        \"\"\"\n        with self.assertRaises(DoesNotExist):\n            Parent.objects.get(pk=999).delete()\n", "    def test_fast_delete_cascade_with_select_related(self):\n        user = User.objects.create(avatar=Avatar.objects.create())\n        with self.assertNumQueries(2):\n            user.delete()\n        self.assertFalse(User.objects.exists())\n        self.assertFalse(Avatar.objects.exists())\n\n\n\n"], "sample_71": ["    def test_non_numeric_input(self):\n        self.assertRaises(TypeError, nformat, 'abc', '.')\n        self.assertRaises(TypeError, nformat, True, '.')\n", "    def test_zero(self):\n        self.assertEqual(nformat(0, '.'), '0')\n        self.assertEqual(nformat(0, '.', decimal_pos=2), '0.00')\n        self.assertEqual(nformat(Decimal(0), '.'), '0')\n        self.assertEqual(nformat(Decimal(0), '.', decimal_pos=2), '0.00')\n        self.assertEqual(nformat(0, '.', grouping=2, thousand_sep=','), '0')\n        self.assertEqual(nformat(0, '.', grouping=2, thousand_sep=',', force_grouping=True), '0')\n", "    def test_grouping_with_zero(self):\n        self.assertEqual(nformat(1230, '.', grouping=3, thousand_sep=','), '1,230')\n        self.assertEqual(nformat(12300, '.', grouping=3, thousand_sep=','), '12,300')\n        self.assertEqual(nformat(123000, '.', grouping=3, thousand_sep=','), '123,000')\n", "    def test_grouping_with_decimal_pos(self):\n        self.assertEqual(nformat(123456789, '.', decimal_pos=3, grouping=3, thousand_sep=','), '123,456,789.000')\n        self.assertEqual(nformat(123456789.123, '.', decimal_pos=3, grouping=3, thousand_sep=','), '123,456,789.123')\n", "    def test_empty_string(self):\n        self.assertEqual(nformat(''), '')\n"], "sample_88": ["    def test_send_messages_after_flush(self):\n        \"\"\"\n        Send messages after flushing the server's output queue\n        \"\"\"\n        backend = smtp.EmailBackend(username='', password='')\n        email = EmailMessage('Subject', 'Content', 'from@example.com', ['to@example.com'])\n        backend.send_messages([email])\n        self.server.flush_sink()\n        with self.assertRaises(SMTPException):\n            backend.send_messages([email])\n\n", "    def test_send_mail_after_server_stopped(self):\n        \"\"\"\n        Sending mail after the server is stopped raises an exception.\n        \"\"\"\n        with self.assertRaises(SMTPException):\n            send_mail('Subject', 'Content', 'from@example.com', ['to@example.com'])\n", "    def test_send_mail_after_server_stop(self):\n        \"\"\"\n        send_mail() shouldn't raise an exception if SMTP server is stopped.\n        \"\"\"\n        with self.assertRaises(SMTPException):\n            send_mail('Subject', 'Content', 'from@example.com', ['to@example.com'])\n", "    def test_server_stopped_during_auth(self):\n        \"\"\"\n        Attempting to authenticate against a stopped SMTP server raises an exception.\n        \"\"\"\n        backend = smtp.EmailBackend(username='username', password='password')\n        with self.assertRaises(ConnectionResetError):\n            with backend:\n                pass\n", "    def test_connection_not_closed_after_server_stopped(self):\n        \"\"\"\n        When the backend is closed, ensure that its connection is not kept open\n        even if the underlying server is stopped.\n        \"\"\"\n        with self.backend:\n            pass\n        self.assertIsNone(self.backend.connection)\n"], "sample_305": ["    def test_null_related_not_promoted(self):\n        # Null related field doesn't promote the join\n        Book.objects.create(contact=None)\n        qs = Book.objects.annotate(Count('contact__name'))\n        self.assertIn(' LEFT OUTER JOIN ', str(qs.query))\n\n\n", "    def test_nullable_fk_promotion(self):\n        qs = Book.objects.filter(contact__isnull=True).annotate(Count('contact__name'))\n        self.assertNotIn(' INNER JOIN ', str(qs.query))\n        self.assertIn(' LEFT OUTER JOIN ', str(qs.query))\n", "    def test_no_promotion_with_related_name(self):\n        qs = Charlie.objects.select_related('alfa').annotate(Count('bravo__charlie'))\n        self.assertIn(' LEFT OUTER JOIN ', str(qs.query))\n\n\n\n", "    def test_nullable_fk_promoted(self):\n        qs = Book.objects.filter(contact__isnull=True).annotate(Count('contact__name'))\n        self.assertIn(' LEFT OUTER JOIN ', str(qs.query))\n", "    def test_non_nullable_fk_with_annotate_count(self):\n        qs = Book.objects.annotate(\n            Count('authors')\n        ).filter(\n            authors__name__contains='Peter'\n        )\n        self.assertIn(' INNER JOIN ', str(qs.query))\n\n"], "sample_1116": ["compilation error", "def test_derivative():\n    K = MatrixSymbol('K', 2, 2)\n    X = MatrixSymbol('X', 2, 2)\n    assert K.I._eval_derivative_matrix_lines(X).shape == (2, 2, 2)\n", "def test_derivative_matrix_lines():\n    arg = MatrixSymbol('A', 2, 2)\n    expr = Inverse(arg)\n    lines = expr._eval_derivative_matrix_lines(arg)\n    assert len(lines) == arg.rows * arg.cols\n    for line in lines:\n        assert isinstance(line.first_pointer, MatPow)\n        assert isinstance(line.second_pointer, MatPow)\n        assert line.first_pointer.base == arg\n        assert line.second_pointer.base == arg \n        assert line.first_pointer.exp == -1\n        assert line.second_pointer.exp == 1\n", "def test_derivative():\n    A = MatrixSymbol('A', 2, 2)\n    x = symbols('x')\n    dA = A.diff(x)\n    assert isinstance(Inverse(A).diff(x), Inverse)\n    assert (Inverse(A).diff(x)).arg == dA\n    assert Inverse(A).T.diff(x) == (Inverse(A).diff(x)).T\n", "def test_inverse_derivative():\n    x = symbols('x')\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    expr = Inverse(A)\n\n    # Test derivative with respect to a matrix entry\n    with raises(NotImplementedError):\n        expr.diff(A[0, 0], x) \n    \n    \n    # Test derivative with respect to a scalar\n    with raises(NotImplementedError):\n        expr.diff(x)\n    \n\n    # Test derivative of inverse of a product\n    C = A*B\n    expr = Inverse(C)\n    dC_dx = C.diff(A, x)\n    d_expr_dx = expr.diff(x)\n    assert isinstance(d_expr_dx[0], MatPow) \n"], "sample_336": ["    def test_lookahead_and_lookbehind_together(self):\n        test_urls = [\n            ('/lookahead-lookbehind-positive', {'city': 'a-city'}, '/lookahead+/lookbehind+/a-city/'),\n            ('/lookahead-lookbehind-negative', {'city': 'a-city'}, '/lookahead-/lookbehind-/a-city/'),\n            ('/lookbehind-lookahead-positive', {'city': 'a-city'}, '/lookbehind+/lookahead+/a-city/'),\n            ('/lookbehind-lookahead-negative', {'city': 'a-city'}, '/lookbehind-/lookahead-/a-city/'),\n        ]\n        for url, kwargs, expected in test_urls:\n            with self.subTest(url=url, kwargs=kwargs):\n                self.assertEqual(reverse(url, kwargs=kwargs), expected)\n", "    def test_reverse_lookahead_args_order(self):\n        with self.subTest(url='/lookahead-positive/arg1/arg2/'):\n            kwargs = {'city': 'a-city', 'arg1': '1', 'arg2': '2'}\n            self.assertEqual(reverse('lookahead-positive', kwargs=kwargs), '/lookahead+/a-city/1/2/')\n", "    def test_lookahead_empty_string(self):\n        with self.subTest(url='/lookahead-/-/'):\n            self.assertEqual(resolve('/lookahead-/-/').kwargs, {'city': ''})\n\n", "    def test_ambiguous_lookahead_and_lookbehind(self):\n        with self.assertRaises(ImproperlyConfigured):\n            path(\n                r'^lookahead?(?P<city>a-city)/(?P<arg1>[0-9]+)/(?P<arg2>[0-9]+)/$',\n                views.empty_view,\n                name='ambiguous_pattern',\n            )\n", "    def test_invalid_lookahead_pattern(self):\n        msg = 'Invalid lookahead pattern: (?=no-match)'\n        with self.assertRaisesMessage(ImproperlyConfigured, msg):\n            path('lookahead-invalid/(?=(no-match))', views.empty_view)\n"], "sample_953": ["    def test_quickstart_sep_option(tempdir):\n        answers = {\n            'Root path': tempdir,\n            'Separate source and build': 'y',\n            'Name prefix for templates': '.',\n            'Project name': 'STASI\u2122',\n            'Author name': 'Wolfgang Sch\u00e4uble & G\\'Beckstein',\n            'Project version': '2.0',\n        }\n        qs.term_input = mock_input(answers)\n        d = {}\n        qs.ask_user(d)\n        qs.generate(d)\n\n        srcdir = tempdir / 'source'\n        assert srcdir.isdir()\n        builddir = tempdir / 'build'\n        assert builddir.isdir()\n\n", "    def test_quickstart_with_template_variables(tempdir):\n        answers = {\n            'Root path': tempdir,\n            'Project name': 'Sphinx Template Test',\n            'Author name': 'Template Tester',\n        }\n        qs.term_input = mock_input(answers)\n        d = {}\n        qs.ask_user(d)\n        d['templatedir'] = tempdir\n\n        # Set some template variables\n        d['variables'] = [\n            'NAME=Template Project',\n            'AUTHOR=Another Author',\n        ]\n\n        qs.generate(d)\n\n        conffile = tempdir / 'conf.py'\n        assert conffile.isfile()\n        ns = {}\n        exec(conffile.read_text(), ns)\n\n        assert ns['project'] == 'Template Project'\n        assert ns['author'] == 'Another Author'\n", "    def test_quickstart_with_custom_template(tempdir):\n        answers = {\n            'Root path': tempdir,\n            'Project name': 'Custom Template Test',\n            'Author name': 'Georg Brandl',\n            'Project version': '0.1',\n        }\n        qs.term_input = mock_input(answers)\n        d = {}\n        qs.ask_user(d)\n        template_dir = path.join(tempdir, 'templates')\n        path.mkdir(template_dir)\n        with open(path.join(template_dir, 'quickstart/root_doc.rst_t'), 'w') as f:\n            f.write('This is a custom template!')\n        qs.generate(d, templatedir=tempdir)\n\n        masterfile = tempdir / 'source' / 'index.rst'\n        assert masterfile.isfile()\n        with open(masterfile, 'r') as f:\n            content = f.read()\n        assert 'This is a custom template!' in content\n\n\n\n", "def test_quickstart_with_template_dir(tempdir):\n    template_dir = path.join(tempdir, 'templates')\n    path.mkdir(template_dir)\n\n    answers = {\n        'Root path': tempdir,\n        'Project name': 'Sphinx Test',\n        'Author name': 'Georg Brandl',\n        'Project version': '0.1',\n        'templatedir': template_dir,\n    }\n    qs.term_input = mock_input(answers)\n    d = {}\n    qs.ask_user(d)\n    qs.generate(d, templatedir=template_dir)\n\n    conffile = tempdir / 'conf.py'\n    assert conffile.isfile()\n", "def test_quickstart_with_custom_templatedir(tempdir):\n    answers = {\n        'Root path': tempdir,\n        'Project name': 'Sphinx Test',\n        'Author name': 'Georg Brandl',\n        'Project version': '0.1',\n    }\n    custom_template_dir = path.join(tempdir, 'templates')\n    path.mkdir(custom_template_dir)\n    with open(path.join(custom_template_dir, 'conf.py_t'), 'w') as f:\n        f.write('extensions = []')\n    qs.term_input = mock_input(answers, needanswer=True)\n    d = {}\n    d['templatedir'] = custom_template_dir\n    qs.ask_user(d)\n    qs.generate(d)\n\n    conffile = tempdir / 'conf.py'\n    assert conffile.isfile()\n    ns = {}\n    exec(conffile.read_text(), ns)\n    assert ns['extensions'] == []\n\n\n\n"], "sample_774": ["def test_one_hot_encoder_categorical_features():\n    X = np.array([[1, 2], [3, 4]])\n    # test that passing a list of indices works for categorical_features\n    onehot = OneHotEncoder(categorical_features=[0], sparse=False)\n    assert_array_equal(onehot.fit_transform(X).toarray(),\n                        [[1, 0, 0], [0, 1, 0]])\n    # test that passing a list of indices with \n    # dtype of object works for categorical_features\n\n    onehot = OneHotEncoder(categorical_features=[0], sparse=False)\n    X_obj = np.array([[1, 2], [3, 4]], dtype=object)\n    assert_array_equal(onehot.fit_transform(X_obj).toarray(),\n                        [[1, 0, 0], [0, 1, 0]])\n    # test that passing a list of indices works for categorical_features\n    # also checks for error when categorical_features is not a list\n    with pytest.raises(ValueError, match=\"categorical_features\"):\n        OneHotEncoder(categorical_features='list')\n    with pytest.raises(ValueError, match=\"categorical_features\"):\n        OneHotEncoder(categorical_features=1)\n    with pytest.raises(ValueError, match=\"categorical_features\"):\n        OneHotEncoder(categorical_features=[1, 2, 3, 4, 5,\n                                         len(X[0]) + 1])\n\n", "def test_one_hot_encoder_sparse_output():\n    enc = OneHotEncoder(sparse=True)\n    X = [['abc', 2, 55], ['def', 1, 55]]\n    X_tr = enc.fit_transform(X)\n    assert isinstance(X_tr, sparse.csr_matrix)\n\n    enc = OneHotEncoder(sparse=True)\n    X = np.array([['Male', 1], ['Female', 3]])\n    X_tr = enc.fit_transform(X)\n    assert isinstance(X_tr, sparse.csr_matrix)\n\n\n", "def test_one_hot_encoder_empty_categories():\n    with pytest.raises(ValueError, match=\"Categories cannot be empty\"):\n        OneHotEncoder(categories=[[], []])\n\n    with pytest.raises(ValueError, match=\"Categories cannot be empty\"):\n        OneHotEncoder(categories=[[], [], []])\n\n    with pytest.raises(ValueError, match=\"Categories cannot be empty\"):\n        OneHotEncoder(categories=[None, None])\n", "def test_one_hot_encoder_invalid_categorical_dtypes():\n    enc = OneHotEncoder(categories=[\n        ['a', 'b'],\n        [np.int16(1), np.int16(2)],\n    ])\n    X = [['a', 1], ['b', 2]]\n\n    with pytest.raises(ValueError, match=\"All lists in categories\"):\n        enc.fit_transform(X)\n\n    enc = OneHotEncoder(categories=[\n        np.array(['a', 'b'], dtype=np.str_),\n        [np.int16(1), np.int16(2)],\n    ])\n    X = [['a', 1], ['b', 2]]\n    with pytest.raises(ValueError, match=\"All lists in categories\"):\n        enc.fit_transform(X)\n\n\n\n", "compilation error"], "sample_1111": ["def test_complex_output():\n    x = Symbol('x')\n    lines = [\n        '     3 |          .                                                    ',\n        '        |         .                                                   ',\n        '        |        .                                                    ',\n        '        |       .                                                     ',\n        '        |      .                                                      ',\n        '        |     .                                                       ',\n        '        |    .                                                        ',\n        '        |   .                                                         ',\n        '        |  .                                                          ',\n        '        | .                                                           ',\n        '       1 |------------------------------------------------------------',\n        '        |                                                            ',\n        '        |                                                            ',\n        '        |                                                            ',\n        '        |                                                            ',\n        '        |                                                            ',\n        '        |                                                            ',\n        '        |                                                            ',\n        '     -1 |------------------------------------------------------------',\n        '         -1                         0                          1'\n    ]\n    assert lines == list(textplot_str(x**1j, -1, 1))\n\n\n\n", "def test_multiple_variables():\n    x = Symbol('x')\n    y = Symbol('y')\n    lines = [\n        '      1 |          .                                                  ',\n        '        |                        .                                     ',\n        '        |                       .                                      ',\n        '        |                      .                                       ',\n        '        |                     .                                        ',\n        '        |                       .                                      ',\n        '        |                      .                                       ',\n        '        |                     .                                        ',\n        '        |                    .                                         ',\n        '   0.5 |-------------------------------------------------------------',\n        '        |                                                              ',\n        '        |                                                              ',\n        '        |                                                              ',\n        '        |                                                              ',\n        '        |                                                              ',\n        '        |                                                              ',\n        '        |                                                              ',\n        '        |                                                              ',\n        ' -0.5 |______________________________________________________________',\n        '         -1                         0                          1'\n    ]\n    assert lines == list(textplot_str(x*y, -1, 1))\n", "def test_quadratic():\n    x = Symbol('x')\n    lines = [\n        '      2 |                                        .               ',\n        '        |                                     .        .         ',\n        '        |                                    .              .     ',\n        '        |                                   .                 .    ',\n        '        |                                 .                    .   ',\n        '        |                                .                      .  ',\n        '        |                               .                        . ',\n        '        |                             .                           ',\n        '        |                            .                            ',\n        '        |                        .                              ',\n        '       1.5 |-------..----------------------------------------',\n        '        |                    .                             .      ',\n        '        |                   .                              .     ',\n        '        |                  .                                .    ',\n        '        |                 .                                     ',\n        '        |                .                                      ',\n        '        |               .                                       ',\n        '        |              .                                        ',\n        '        |             .                                         ',\n        '        |            .                                          ',\n        '        |           .                                           ',\n        '        |          .                                            ',\n        '     -1.5 |_______________________________________________________',\n        '         -2                        0                          2'\n    ]\n    assert lines == list(textplot_str(x**2 - 1, -2, 2))\n", "def test_log_negative():\n    x = Symbol('x')\n    with pytest.raises(ValueError):\n        lines = list(textplot_str(log(x), -1, 0))\n", "def test_constant():\n    x = Symbol('x')\n    lines = [\n        '      1 |=========================================================',\n        '        |=========================================================',\n        '        |=========================================================',\n        '        |=========================================================',\n        '        |=========================================================',\n        '   0.5 |=========================================================',\n        '        |=========================================================',\n        '        |=========================================================',\n        '        |=========================================================',\n        '        |=========================================================',\n        '      0 |=========================================================',\n        '        |=========================================================',\n        '        |=========================================================',\n        '        |=========================================================',\n        '        |=========================================================',\n        '     -1 |=========================================================',\n        '         -1                         0                          1'\n    ]\n    assert lines == list(textplot_str(S(3), -1, 1))\n"], "sample_1009": ["compilation error", "compilation error", "def test_Vector_scalar_multiplication():\n    N = ReferenceFrame('N')\n    v1 = 2 * N.x + 3 * N.y\n    v2 = 5 * N.z\n\n    assert (2 * v1) == 4 * N.x + 6 * N.y\n    assert (v1 * 2) == 4 * N.x + 6 * N.y\n    assert (-3 * v1) == -6 * N.x - 9 * N.y\n    assert (v1 * -3) == -6 * N.x - 9 * N.y\n    assert (v1 * v2) == 0\n    a = symbols('a')\n    v3 = a * N.x + a * N.y\n    assert (v3 * a) == a**2 * N.x + a**2 * N.y\n    assert (a * v3) == a**2 * N.x + a**2 * N.y \n\n\n", "def test_vector_separate():\n    N = ReferenceFrame('N')\n    A = N.orientnew('A', 'Axis', [q3, N.z])\n    v1 = q2 * A.x + q3 * N.y\n    v2 = q3 * B.x + v1\n    d = v1.separate()\n    assert d == {A: q2 * A.x, N: q3 * N.y}\n\n\n\n    d2 = v2.separate()\n    assert d2 == {A: q2 * A.x + q3 * B.x, B:\n    q3 * B.x, N: q3 * N.y}\n\n\n", "compilation error"], "sample_0": ["def test_correlations_are_handled_correctly():\n    # Check that correlations are handled\n    # correctly during propagation operations for all uncertainty types\n\n    # 1D case\n    nddata = NDData(np.arange(10), uncertainty=StdDevUncertainty(np.ones(10)))\n    nddata.uncertainty.correlation = np.arange(10) / 9\n\n    # Propagate some values and check that the correlation is\n    # reflected in the propagated uncertainty.\n    # ... (add specific tests for each operation (add, subtract,\n    # multiply, divide) and different scenarios)\n\n    # 2D case\n    nddata_2d = NDData(np.arange(100).reshape(10, 10),\n                      uncertainty=StdDevUncertainty(np.ones(100)))\n    nddata_2d.uncertainty.correlation = np.arange(100) / 99\n    # ... (add specific tests for each operation (add, subtract,\n    # multiply, divide) and different scenarios)\n\n\n", "def test_self_conversion_via_variance_conversion_fails(UncertClass):\n    uncert = np.arange(1, 11).reshape(2, 5) * u.adu\n    start_uncert = UncertClass(uncert)\n    with pytest.raises(IncompatibleUncertaintiesException):\n        start_uncert.represent_as(UnknownUncertainty)\n", "def test_represent_as_unknown_type_raises_exception(UncertClass):\n    uncert = np.arange(1, 11).reshape(2, 5) * u.adu\n    start_uncert = UncertClass(uncert)\n    with pytest.raises(ValueError):\n        start_uncert.represent_as(UnknownUncertainty)  \n\n", "def test_self_conversion_via_variance_unsupported(UncertClass):\n    uncert = np.arange(1, 11).reshape(2, 5) * u.adu\n    start_uncert = UncertClass(uncert)\n    with pytest.raises(NotImplementedError):\n        start_uncert.represent_as(UncertClass)\n", "def test_self_conversion_via_variance_unsupported(UncertClass):\n    uncert = np.arange(1, 11).reshape(2, 5) * u.adu\n    start_uncert = UncertClass(uncert)\n    with pytest.raises(IncompatibleUncertaintiesException):\n        start_uncert.represent_as(UncertClass)\n"], "sample_182": ["    def test_count_union_with_empty_queryset(self):\n        qs1 = Number.objects.filter(pk__in=[])\n        self.assertEqual(qs1.union(Number.objects.all()).count(), 10)\n        self.assertEqual(Number.objects.all().union(qs1).count(), 10)\n", "    def test_intersection_with_subquery(self):\n        qs1 = Number.objects.filter(num__lt=5)\n        subquery = Number.objects.filter(num__gt=2).values('num')\n        intersection = qs1.intersection(subquery)\n        self.assertNumbersEqual(intersection, [3, 4])\n\n\n\n", "    def test_union_with_distinct_and_ordering(self):\n        qs1 = Number.objects.filter(num__lte=1).distinct()\n        qs2 = Number.objects.filter(num__gte=2, num__lte=3).distinct()\n        self.assertNumbersEqual(qs1.union(qs2).order_by('-num'), [3, 2, 1, 0])\n\n\n\n", "    def test_union_with_distinct(self):\n        qs1 = Number.objects.all()\n        qs2 = Number.objects.all()\n        self.assertNumbersEqual(qs1.union(qs2, all=True), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], ordered=False)\n", "    def test_union_with_null_and_non_null(self):\n        ReservedName.objects.create(name='a', order=2)\n        qs1 = ReservedName.objects.filter(name='a').annotate(order=Value(None, IntegerField()))\n        qs2 = ReservedName.objects.filter(name='b', order=2)\n        union_qs = qs1.union(qs2)\n        self.assertNotEqual(list(union_qs.values_list('order', flat=True)), [2, None])\n        self.assertEqual(list(union_qs.values_list('order', flat=True)), [2, None])\n"], "sample_519": ["def test_subplots_within_subfigures():\n    fig = plt.figure(constrained_layout=True)\n\n    subfig = fig.add_subfigure(gs.SubgridSpec(1, 2))\n\n    subfig.add_subplot(1, 1, 1)\n\n    subfig = fig.add_subfigure(gs.SubgridSpec(2, 1))\n    subfig.add_subplot(1, 1, 1)\n    subfig.add_subplot(1, 1, 2)\n\n    for i in range(3):\n        for j in range(2):\n            fig.add_subplot(2, 2, i * 2 + j)\n\n\n", "def test_subplots_positioning():\n    fig = plt.figure()\n    gs = fig.add_gridspec(2, 3)\n    ax1 = fig.add_subplot(gs[0, 0])\n    ax2 = fig.add_subplot(gs[0, 1:])\n    ax3 = fig.add_subplot(gs[1, :])\n    assert ax1.get_position() != ax2.get_position()\n    assert ax1.get_position() != ax3.get_position()\n    assert ax2.get_position() != ax3.get_position()\n", "def test_subfigure_titles():\n    fig = plt.figure()\n    sub = fig.subfigures(1, 2)\n\n    # Test that subfigure titles are set correctly\n    sub[0].suptitle('Left Side')\n    sub[1].suptitle('Right Side')\n\n    # Test that subplot titles are set correctly\n    ax = sub[0].subplots(1, 1)[0]\n    ax.set_title('Subplot Title')\n    assert ax.get_title() == 'Subplot Title'\n\n    fig.tight_layout()\n    plt.show()\n\n\n\n", "def test_subplot_duplicate_labels():\n    with pytest.raises(ValueError, match=\"duplicate labels\"):\n        fig = plt.figure()\n        fig.add_subplot(111, label=\"A\")\n        fig.add_subplot(111, label=\"A\")\n\n", "def test_add_subplot_nesting():\n    fig = plt.figure()\n    gs = fig.add_gridspec(1, 2)\n    sub_fig = fig.add_subfigure(gs[0], facecolor='pink')\n    sub_fig2 = fig.add_subfigure(gs[1], facecolor='coral')\n    ax1 = sub_fig.subplots(2, 1)\n    ax2 = sub_fig2.subplots()\n    for ax in ax1 + ax2:\n        ax.set_facecolor('yellow')\n"], "sample_990": ["compilation error", "compilation error", "compilation error", "def test_cosh_expansion():\n    x, y = symbols('x,y')\n    assert cosh(x+y).expand(trig=True) == cosh(x)*cosh(y) + sinh(x)*sinh(y)\n    assert cosh(2*x).expand(trig=True) == cosh(x)**2 + sinh(x)**2\n    assert cosh(3*x).expand(trig=True).expand() == \\\n        cosh(x)**3 + 3*cosh(x)*sinh(x)**2\n", "def test_cosh_expansion():\n    x, y = symbols('x,y')\n    assert cosh(x+y).expand(trig=True) == cosh(x)*cosh(y) + sinh(x)*sinh(y)\n    assert cosh(2*x).expand(trig=True) == 2*cosh(x)**2 - 1\n    assert cosh(3*x).expand(trig=True).expand() == \\\n        cosh(x)**3 + 3*cosh(x)*sinh(x)**2\n"], "sample_438": ["    def test_get_related_object_with_no_content_type(self):\n        with self.assertRaises(models.ProtectedError):\n            Answer.question.get_related_object()\n", "    def test_get_cached_generic_relation_with_related_model(self):\n        question = Question.objects.create(text=\"What is your name?\")\n        answer = Answer.objects.create(text=\"Answer\", question=question)\n\n        with self.assertNumQueries(0):\n            self.assertEqual(answer.question, question)\n            related_question = answer.question.related\n            self.assertEqual(related_question, question)\n            self.assertNotIsInstance(related_question, Question.objects.all().first()) \n", "    def test_get_object_with_deleted_content_type(self):\n        # Create a Question and an Answer.\n        question = Question.objects.create(text=\"What is your name?\")\n        answer = Answer.objects.create(text=\"Answer\", question=question)\n\n        # Delete the Question's content type.\n        content_type = question.content_type\n        content_type.delete()\n\n        with self.assertRaises(models.ObjectDoesNotExist):\n            # Attempt to retrieve the Answer's question.\n            Answer.objects.get(pk=answer.pk).question\n\n", "    def test_get_object_cache_respects_non_existing_objects(self):\n        question_pk = 12345\n        post = Post.objects.create(title=\"Non-Existing Answer\", parent_id=question_pk)\n        \n\n        with self.assertRaises(ObjectDoesNotExist):\n            post.parent\n", "    def test_get_content_type_with_arguments(self):\n        # This test checks that get_content_type() allows specifying the content type\n        # correctly and handles the case where the object has no content type associated\n        # with it.\n\n        answer = Answer.objects.create(text=\"Answer\", question=None)\n        self.assertEqual(answer.question.get_content_type(), None)\n\n        question = Question.objects.create(text=\"Question text\")\n        answer.question = question\n        answer.save()\n        self.assertEqual(answer.question.get_content_type(), models.ContentType.objects.get_for_model(Question))\n\n\n\n"], "sample_1079": ["def test_plane_intersection():\n    p = Point3D(1, 1, 1)\n    q = Point3D(2, 2, 2)\n    plane = Plane((1, 2, 3), (4, 5, 6), (7, 8, 9))\n    raises(TypeError, lambda: p.intersection(plane))\n\n\n", "    def test_projection():\n        p = Point3D(1, 0, 0)\n        v = Point3D(1, 1, 1)\n        proj = p.projection(v)\n        assert proj == Point3D(1, 0, 0)\n        p = Point3D(1, 1, 1)\n        v = Point3D(1, 0, 0)\n        proj = p.projection(v)\n        assert proj == Point3D(1, 0, 0)\n", "def test_direction_cosine():\n    p = Point3D(1, 2, 3)\n    p2 = Point3D(4, 5, 6)\n    assert p.direction_cosine(p2) == [sqrt(14)/sqrt(50), sqrt(29)/sqrt(50), sqrt(35)/sqrt(50)]\n", "compilation error", "def test_projection():\n    p = Point3D(1, 2, 3)\n    line = Line3D(Point3D(0, 0, 0), direction_vector=Vector(1, 1, 1))\n    proj = p.projection(line)\n    assert isinstance(proj, Point3D)\n    assert proj.distance(p) <= p.distance(Point3D(0,0,0))\n    assert proj.distance(line.point_at(0)) <= p.distance(line.point_at(0))\n\n\n\n\n"], "sample_991": ["def test_issue_18973():\n    n = Symbol('n', integer=True)\n    p = Product(n**n, (n, 1, 5))\n    assert simplify(p) == 5**5*1**2*2**2*3**3*4**4\n", "def test_issue_11851():\n    from sympy import Product, oo\n    k = Symbol('k')\n    assert Product(1/(k**2 + 1), (k, 0, oo)).doit() == oo \n", "def test_issue_13546_continued():\n    n = Symbol('n')\n    k = Symbol('k')\n    p = Product(n + 1 / 2**k, (k, 0, n-1))\n    assert p.is_convergent() is S.false\n", "def test_issue_17850():\n    x, y = symbols('x y', nonzero=True)\n    p = Product(x**(S(1)/n), (n, 1, oo)).doit()\n    assert p == oo\n\n", "def test_issue_14559():\n    x, y = symbols('x, y')\n    p = Product(x**n, (n, 0, oo))\n    assert p.is_convergent() is S.false\n    assert product(x**n, (n, 0, oo)) == p.doit()\n"], "sample_212": ["    def process_response(self, request, response):\n        request.thread_and_connection = request_lifecycle()\n        return response\n        \n        \n        \n        \n", "compilation error", "    def test_sync_to_async_thread_and_connection(self):\n        async def get_response(request):\n            return HttpResponse()\n\n        class SimpleMiddleWare(MiddlewareMixin):\n                request.thread_and_connection = request_lifecycle()\n                return response\n\n        middleware_instance = SimpleMiddleWare(get_response)\n        request = HttpRequest()\n        request.thread_and_connection = None\n        async_to_sync(middleware_instance)(request)\n        self.assertEqual(request.thread_and_connection, (\n            threading.get_ident(), id(connection)))\n", "            def process_response(self, request, response):\n                request.thread_and_connection = request_lifecycle()\n                return response\n", "        def process_response(self, request, response):\n            if hasattr(request, 'thread_and_connection'):\n                self.assertEqual(request.thread_and_connection, request_lifecycle())\n"], "sample_86": ["    def test_keep_lazy(self):\n        @keep_lazy\n            return a + b\n\n        @keep_lazy_text\n            return f\"Hello, {name}!\"\n\n        self.assertEqual(add_strings('hello', 'world'), 'helloworld')\n        self.assertEqual(greet('Alice'), \"Hello, Alice!\")\n\n        with self.subTest('lazy arguments'):\n            lazy_result = lazy(lambda: 'hello world')\n            self.assertEqual(add_strings('hi', lazy_result), 'hihello world')\n            with self.assertRaises(TypeError):\n                add_strings(lazy_result, 'there')\n\n        with self.subTest('text return'):\n            lazy_greet = lazy(lambda: 'Alice', str)\n            self.assertEqual(greet(lazy_greet), \"Hello, Alice!\")\n\n\n\n", "    def test_keep_lazy(self):\n        @keep_lazy(int)\n            return x + 1\n\n        @keep_lazy(str)\n            return text.upper()\n\n        # All arguments are non-lazy\n        self.assertEqual(add_one(2), 3)\n        self.assertEqual(uppercase(\"hello\"), \"HELLO\")\n\n        # One argument is lazy\n        lazy_result = add_one(lazy(lambda: 3))\n        self.assertEqual(lazy_result(), 4)\n\n        # Two arguments are lazy\n        lazy_result = uppercase(lazy(lambda: \"world\"), lazy(lambda: \"!\"))\n        self.assertEqual(lazy_result(), \"WORLD!\")\n\n        self.assertEqual(add_one(lazy(lambda: 2), lazy(lambda: 3)), 5)\n", "    def test_keep_lazy_text(self):\n        @keep_lazy_text\n            return text.upper()\n\n        self.assertEqual(my_func(lazystr(\"hello\")), \"HELLO\")\n        self.assertEqual(my_func(\"hello\"), \"HELLO\")\n\n", "    def test_lazy_repr_callable(self):\n        original_object = lambda x: x * 2\n        lazy_obj = lazy(lambda: original_object, callable)\n        self.assertEqual(repr(original_object), repr(lazy_obj()))\n\n\n", "    def test_keep_lazy(self):\n            return a + b\n\n        @keep_lazy(int)\n            return wrapped(x, y)\n\n        # All lazy\n        lazy_a = lazy(lambda: 1, int)\n        lazy_b = lazy(lambda: 2, int)\n        result = add(lazy_a, lazy_b)\n        self.assertEqual(result(), 3)\n\n        # Mixed\n        lazy_a = lazy(lambda: 1, int)\n        result = add(1, lazy_a)\n        self.assertEqual(result(), 2)\n        # No lazy arguments\n        result = add(1, 2)\n        self.assertEqual(result, 3)\n"], "sample_169": ["    def test_in_lookup(self):\n        values = [\n            {'a': 'b', 'c': 14},\n            {'a': 'b', 'c': 13},\n            {'a': 'c', 'c': 14},\n        ]\n        objs = [\n            NullableJSONModel.objects.create(value=v) for v in values\n        ]\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__in=[v for v in values]),\n            objs,\n        )\n", "    def test_array_size(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__baz__size=2),\n            [self.objs[7]],\n        )\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__d__size=1),\n            [self.objs[4]],\n        )\n", "    def test_deep_lookup_with_transform(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__d__1__f__gt='g'),\n            [self.objs[4]],\n        )\n", "    def test_mixed_lookup_with_isnull(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__d__0__isnull=False, value__j__isnull=True\n            ),\n            [self.objs[4]],\n        )", "    def test_key_transform_filter_with_null(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__d__0__isnull=True),\n            self.objs[:3] + self.objs[5:]\n        )\n"], "sample_521": ["def test_projection_limits():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    ax.set_xlim(-1, 1)\n    ax.set_ylim(-1, 1)\n    ax.set_zlim(-1, 1)\n    ax.plot([0, 1], [0, 1], [0, 1])\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_zlabel('Z')\n", "def test_scatter_color_gradient(fig_test, fig_ref):\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    theta = np.linspace(0, 2 * np.pi, 100)\n    phi = np.linspace(0, np.pi, 100)\n    theta, phi = np.meshgrid(theta, phi)\n    x = np.sin(theta) * np.cos(phi)\n    y = np.sin(theta) * np.sin(phi)\n    z = np.cos(theta)\n    sc = ax.scatter(x, y, z, c=phi, cmap='viridis')\n    fig.colorbar(sc)\n    assert sc.get_cmap() == 'viridis'\n\n    for ax in fig_test.axes:\n        if ax.get_projection() == '3d':\n            ax.set_xlim3d(-1.2, 1.2)\n            ax.set_ylim3d(-1.2, 1.2)\n            ax.set_zlim3d(-1.2, 1.2)\n\n\n", "def test_axis_access(method, expected):\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n\n    for m, e in zip(method, expected):\n        getattr(ax, m)() == e\n", "def test_legend_handles(projection):\n    # Test legend handle placement in different 3D projections\n    fig, axs = plt.subplots(1, 3, figsize=(10, 4),\n                            constrained_layout=True,\n                            subplot_kw={'projection': projection})\n    for ax in axs:\n        ax.plot([1, 2, 3], [4, 5, 6], [7, 8, 9])\n        ax.legend()\n\n    # Verify legend handles are drawn correctly\n    for i, ax in enumerate(axs):\n        handles = ax.get_legend_handles_labels()[0]\n        if projection == '3d':\n            assert len(handles) == 1\n            assert handles[0].get_text() == 'Line2D'\n            assert handles[0].get_color() == 'blue'\n        elif projection == '3d':\n            assert len(handles) == 1\n            assert handles[0].get_text() == 'Line2D'\n            assert handles[0].get_color() == 'blue'\n        else:\n            assert len(handles) == 1\n            assert handles[0].get_text() == 'Line2D'\n            assert handles[0].get_color() == 'blue'\n\n\n", "def test_axes_labels_3d():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection=\"3d\")\n\n    ax.set_xlabel('X-axis')\n    ax.set_ylabel('Y-axis')\n    ax.set_zlabel('Z-axis')\n\n    assert ax.get_xlabel() == 'X-axis'\n    assert ax.get_ylabel() == 'Y-axis'\n    assert ax.get_zlabel() == 'Z-axis'\n\n\n"], "sample_476": ["    def test_multiple_assignments_in_constructor(self):\n        p = self.PersonModel(mugshot=self.file1, headshot=self.file2,\n                         profile_pic=self.file1)\n\n        self.check_dimensions(p, 4, 8, \"mugshot\")\n        self.check_dimensions(p, 8, 4, \"headshot\")\n        self.check_dimensions(p, 4, 8, \"profile_pic\")\n\n        p.save()\n        self.check_dimensions(p, 4, 8, \"mugshot\")\n        self.check_dimensions(p, 8, 4, \"headshot\")\n        self.check_dimensions(p, 4, 8, \"profile_pic\")\n", "    def test_save_and_update_dimensions(self):\n        p = self.PersonModel(name=\"Joe\")\n        p.mugshot = self.file1\n        p.headshot = self.file2\n        p.save()\n\n        # Dimensions should be updated when the model is saved.\n        self.check_dimensions(p, 4, 8, \"mugshot\")\n        self.check_dimensions(p, 8, 4, \"headshot\")\n\n        # Updating an image field should update its dimensions.\n        p.mugshot = self.file2\n        p.save()\n        self.check_dimensions(p, 8, 4, \"mugshot\")\n        self.check_dimensions(p, 8, 4, \"headshot\")\n\n        # Updating a non-image field should not affect image dimensions.\n        p.name = \"Jane\"\n        p.save()\n        self.check_dimensions(p, 8, 4, \"mugshot\")\n        self.check_dimensions(p, 8, 4, \"headshot\")\n\n", "    def test_different_storage_backend(self):\n        class MyStorage(Storage):\n                return None\n\n        p = self.PersonModel(name=\"Joe\")\n        p1 = self.PersonModel(name=\"Bob\", mugshot=self.file1, headshot=self.file2)\n\n        p.mugshot.storage = MyStorage()\n        p.mugshot.save(\"mug1\", self.file1)\n\n        p1.mugshot.storage = MyStorage()\n        p1.mugshot.save(\"mug2\", self.file1)\n\n        p1.headshot.storage = MyStorage()\n        p1.headshot.save(\"head1\", self.file2)\n\n        with self.assertNumQueries(3):\n            result1 = self.PersonModel.objects.filter(name=\"Joe\").first()\n            result2 = self.PersonModel.objects.filter(name=\"Bob\").first()\n\n        self.assertIsNotNone(result1.mugshot)\n        self.assertIsNotNone(result2.mugshot)\n        self.assertIsNotNone(result2.headshot)\n        self.assertEqual(result1.mugshot.storage, MyStorage())\n        self.assertEqual(result2.mugshot.storage, MyStorage())\n        self.assertEqual(result2.headshot.storage, MyStorage())\n\n\n\n", "    def test_update_dimensions_fields(self):\n        p = self.PersonTwoImages(name=\"Joe\")\n        p.mugshot.save(\"mug\", self.file1)\n        p.save()\n\n        # Dimensions should get set if file is saved.\n        self.check_dimensions(p, 4, 8, \"mugshot\")\n        self.check_dimensions(p, None, None, \"headshot\")\n\n        # The dimensions fields should update if an existing image is updated\n        p.mugshot.save(\"mug\", self.file2)\n        self.check_dimensions(p, 8, 4, \"mugshot\")\n        # Dimensions should not change for the headshot field though\n        self.check_dimensions(p, None, None, \"headshot\")\n\n\n\n        # Now set the headshot and save\n        p.headshot.save(\"head\", self.file1)\n        self.check_dimensions(p, 8, 4, \"mugshot\")\n        self.check_dimensions(p, 4, 8, \"headshot\") \n", "    def test_concurrent_uploads(self):\n        \"\"\"\n        Tests that concurrent uploads with the same name don't overwrite each other.\n        \"\"\"\n        from django.test.multiprocessing import ForkedProcess\n        import multiprocessing\n\n        class UploadTask(ForkedProcess):\n                p = self.PersonModel(name=\"Joe\")\n                p.mugshot.save(\"mug\", self.file1)\n                p.save()\n\n        with self.assertNumQueries(2):\n            tasks = [UploadTask(target=self.file1) for _ in range(10)]\n            tasks[0].start()\n            tasks[1].start()\n            for task in tasks[2:]:\n                task.start()\n            \n            for task in tasks:\n                task.join()\n\n        p = self.PersonModel.objects.get(name=\"Joe\")\n        self.assertEqual(p.mugshot.path, os.path.join(temp_storage_dir, \"mug\"))\n"], "sample_829": ["def test_incremental_pca_sparse_batch_size():\n    # Test IncrementalPCA with sparse data and varying batch sizes.\n    n_samples = 100\n    n_features = 50\n    X = sparse.csr_matrix(np.random.randn(n_samples, n_features))\n\n    batch_sizes = [1, 10, 50, 100]\n\n    for batch_size in batch_sizes:\n        ipca = IncrementalPCA(n_components=5, batch_size=batch_size)\n        ipca.fit(X)\n\n        # Check if the components are consistent\n        assert_allclose_dense_sparse(ipca.components_, ipca.components_)\n\n    # Test with a very small batch size that splits the data\n    ipca = IncrementalPCA(n_components=5, batch_size=1)\n    ipca.fit(X)\n    assert_allclose_dense_sparse(ipca.components_, ipca.components_)\n", "def test_incremental_pca_sparse_whiten():\n    # Test that whitening works correctly with sparse input data.\n    rng = np.random.RandomState(1999)\n    n_samples = 1000\n    n_features = 100\n\n    X = rng.rand(n_samples, n_features)\n    X_sparse = sparse.csr_matrix(X)\n\n    ipca = IncrementalPCA(whiten=True, n_components=50, batch_size=100)\n    ipca.fit(X_sparse)\n\n    X_transformed_sparse = ipca.transform(X_sparse)\n    X_transformed = ipca.transform(X)\n\n    assert_almost_equal(np.sum(X_transformed_sparse * X_transformed_sparse.transpose(0, 2, 1)),\n                        np.eye(50), decimal=3)\n    assert_almost_equal(np.sum(X_transformed * X_transformed.transpose()),\n                        np.eye(50), decimal=3)\n\n\n\n", "def test_incremental_pca_partial_fit_sparse():\n    # Test that fit and partial_fit get equivalent results with sparse data.\n    rng = np.random.RandomState(1999)\n    n, p = 50, 3\n    X = rng.randn(n, p)\n    X_sparse = scipy.sparse.csr_matrix(X)\n    X_sparse_batch = X_sparse[0:n//2]\n    X_sparse_rest = X_sparse[n//2:]\n    ipca = IncrementalPCA(n_components=2, batch_size=n//2)\n\n    ipca.partial_fit(X_sparse_batch)\n    ipca.partial_fit(X_sparse_rest)\n\n    ipca_full = IncrementalPCA(n_components=2, batch_size=n//2).fit(X_sparse)\n    assert_almost_equal(ipca.components_, ipca_full.components_, decimal=3)\n", "def test_incremental_pca_sparse_scaling():\n    # Test that scaling works correctly with sparse input\n    n_samples = 500\n    n_features = 10\n    rng = np.random.RandomState(1999)\n    X = rng.rand(n_samples, n_features)\n    X_sparse = sparse.csc_matrix(X)\n    \n    ipca = IncrementalPCA(n_components=2, batch_size=100, copy=False)\n    ipca.fit(X_sparse)\n    \n    # Test that scaling was properly applied\n    assert_almost_equal(np.sum(ipca.components_**2), 1.0, \n                        decimal=3)\n    \n    #Test that scaling is not applied if not specified\n    ipca_no_scaling = IncrementalPCA(n_components=2, batch_size=100, copy=False, whiten=False)\n    ipca_no_scaling.fit(X_sparse)\n    assert_allclose_dense_sparse(ipca.components_, ipca_no_scaling.components_, atol=1e-8)\n", "def test_incremental_pca_sparse_whitening():\n    # Test that whitening works correctly with sparse input\n    rng = np.random.RandomState(1999)\n    n_samples = 100\n    n_features = 100\n    X = rng.rand(n_samples, n_features)\n    X_sparse = sparse.csr_matrix(X)\n\n    ipca = IncrementalPCA(whiten=True, n_components=20, batch_size=10).fit(X_sparse)\n    Xt_ipca = ipca.transform(X_sparse)\n\n    # Ensure that the transformed data is whitened\n    assert_almost_equal(\n        np.mean(Xt_ipca**2, axis=0), np.ones(Xt_ipca.shape[1]), \n        decimal=3\n    )\n\n\n"], "sample_751": ["    def check_sample_weights(estimator):\n        estimator.fit(X, y, sample_weight=sample_weights)\n        assert hasattr(estimator, 'sample_weight_')\n        assert_array_almost_equal(estimator.sample_weight_, sample_weights)\n", "        def __init__(self, **kwargs):\n            super(WeightTracker, self).__init__(**kwargs)\n            self.received_weights = None\n", "    def test_sample_weight_adaboost_regressor_custom_weights(self):\n        X, y = datasets.make_regression(n_samples=100, n_features=5, random_state=0)\n        sample_weights = np.random.rand(100)\n        \n        # Check that when sample_weights are provided, they are properly used\n        clf = AdaBoostRegressor(\n            DecisionTreeRegressor(),\n            random_state=0,\n        )\n        clf.fit(X, y, sample_weight=sample_weights)\n        \n        # Check that the weights didn't default to uniform\n        assert_array_almost_equal(clf.estimator_weights_, sample_weights / np.sum(sample_weights))\n", "    def test_sample_weight_adaboost_regressor():\n        \"\"\"\n        AdaBoostRegressor should work without sample_weights in the base estimator\n\n        The random weighted sampling is done internally in the _boost method in\n        AdaBoostRegressor.\n        \"\"\"\n        X = [[1, 2], [3, 4], [5, 6]]\n        y = [1, 2, 3]\n\n        class DummyEstimator(BaseEstimator):\n\n                pass  \n\n        reg = AdaBoostRegressor(base_estimator=DummyEstimator(),\n                                random_state=0)\n        reg.fit(X, y)\n        assert_equal(len(reg.estimators_), len(DummyEstimator(\n        )))\n\n", "    def test_sample_weight_adaboost_classifier():\n        \"\"\"\n        AdaBoostClassifier should work without sample_weights in the base estimator\n\n        The random weighted sampling is done internally in the _boost method in\n        AdaBoostClassifier.\n        \"\"\"\n        class DummyEstimator(BaseEstimator):\n\n                pass\n\n        X, y = datasets.make_classification(n_samples=1000, n_features=10,\n                                            n_informative=5, n_redundant=0,\n                                            random_state=42)\n        clf = AdaBoostClassifier(base_estimator=DummyEstimator(),\n                                  random_state=0)\n        clf.fit(X, y, sample_weight=None)\n\n\n"], "sample_606": ["compilation error", "compilation error", "def test_polyfit(use_dask) -> None:\n    if use_dask and not has_dask:\n        pytest.skip(\"requires dask\")\n\n    x = np.arange(10)\n    y = 1.0 + x + 2.0 * x ** 2\n    da_x = xr.DataArray(x, dims=\"x\")\n    da_y = xr.DataArray(y, dims=\"x\")\n\n    if use_dask:\n        da_x = da_x.chunk({\"x\": 2})\n        da_y = da_y.chunk({\"x\": 2})\n\n    coeffs = xr.polyfit(da_x, da_y, degree=2)\n\n    expected_coeffs = np.polyfit(x, y, 2)\n    assert_allclose(coeffs.data, expected_coeffs)\n\n\n\n", "def test_polyval_invalid(use_dask) -> None:\n    if use_dask and not has_dask:\n        pytest.skip(\"requires dask\")\n\n    xcoord = np.arange(10)\n    da = xr.DataArray(np.arange(100).reshape((10, 10)), dims=(\"x\", \"y\"), coords={\"x\": xcoord})\n    coeffs = xr.DataArray([[2, 1, 1], [3, 2, 1]], dims=(\"degree\",))\n    with pytest.raises(ValueError, match=r\"dimensions\"):\n        xr.polyval(da.x, coeffs)\n\n    coeffs = xr.DataArray(\n        [[2, 1, 1], [3, 2, 1]],\n        dims=(\"d\", \"degree\"),\n        coords={\"d\": [0, 1], \"degree\": [2, 1, 0]},\n    )\n    with pytest.raises(ValueError, match=r\"inconsistent dimensions\"):\n        xr.polyval(da.x, coeffs)\n\n\n    coeffs = xr.DataArray(\n        [[2, 1, 1], [3, 2, 1]],\n        dims=(\"d\", \"degree\"),\n        coords={\"d\": [0, 1], \"degree\": [2, 1, 0]},\n    )\n\n    with pytest.raises(ValueError, match=r\"dimension\"):\n        xr.polyval(da.y, coeffs)\n\n", "def test_polyfit() -> None:\n    xcoord = np.arange(10)\n    da = xr.DataArray(\n        np.stack((1.0 + xcoord + 2.0 * xcoord ** 2, 1.0 + 2.0 * xcoord + 3.0 * xcoord ** 2)),\n        dims=(\"d\", \"x\"),\n        coords={\"x\": xcoord, \"d\": [0, 1]},\n    )\n    coeffs, residuals, rank, singular = xr.polyfit(da.x, da.variable, 2)\n    coeffs_expected = np.array([[2, 1, 1], [3, 2, 1]])\n    assert_allclose(coeffs, coeffs_expected)\n"], "sample_770": ["def test_davies_bouldin_score():\n    assert_raises_on_only_one_label(davies_bouldin_score)\n\n    assert_raises_on_all_points_same_cluster(davies_bouldin_score)\n\n\n    # Assert the value is 0 when all samples are equals\n    assert_equal(0., davies_bouldin_score(np.ones((10, 2)),\n                                        [0] * 10))\n\n    # General case \n\n    X = ([[0, 0], [1, 1]] * 5 + [[3, 3], [4, 4]] * 5 +\n         [[0, 4], [1, 3]] * 5 + [[3, 1], [4, 0]] * 5)\n    labels = [0] * 10 + [1] * 10 + [2] * 10 + [3] * 10\n    \n    score = davies_bouldin_score(X, labels)\n    \n    #Assert the result\n    assert_greater(score, 0)\n", "def test_davies_bouldin_score_empty_clusters():\n    # Test Davies-Bouldin score when clusters are empty.\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    empty_labels = [0] * 2 + [1] * 1\n    assert_equal(davies_bouldin_score(X, empty_labels), 0.0)\n\n\n\n\n", "    def test_davies_bouldin_score():\n        # Assertions for Davies-Bouldin score\n        assert_raises_on_only_one_label(davies_bouldin_score)\n\n        assert_raises_on_all_points_same_cluster(davies_bouldin_score)\n\n        # Test with all samples in one cluster\n        X = np.ones((10, 2))\n        labels = np.zeros(10)\n        assert_equal(0.0, davies_bouldin_score(X, labels))\n\n        # Test with two clusters of equal size and samples\n        X = np.array([[0, 0], [1, 1], [2, 2], [3, 3], [4, 4],\n                      [0.5, 0.5], [1.5, 1.5], [2.5, 2.5],\n                      [3.5, 3.5], [4.5, 4.5]])\n        labels = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\n        assert_greater(davies_bouldin_score(X, labels), 0)\n\n        # Test with two clusters with different sizes\n        X = np.array([[0, 0], [1, 1], [2, 2], [3, 3], [4, 4],\n                      [0.5, 0.5], [1.5, 1.5], [2.5, 2.5],\n                      [3.5, 3.5], [4.5, 4.5],\n                      [5, 5], [6, 6], [7, 7], [8, 8], [9, 9]])\n        labels = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n        assert_greater(davies_bouldin_score(X, labels), 0)\n\n", "def test_davies_bouldin_score():\n    assert_raises_on_only_one_label(davies_bouldin_score)\n    assert_raises_on_all_points_same_cluster(davies_bouldin_score)\n\n    # Assert the score is 0 when all samples are the same.\n    assert_equal(0., davies_bouldin_score(np.ones((10, 2)),\n                                        [0] * 10))\n\n    # General case\n    X = ([[0, 0], [1, 1]] * 5 + [[3, 3], [4, 4]] * 5)\n    labels = [0] * 10 + [1] * 10\n    score = davies_bouldin_score(X, labels)\n    pytest.approx(score, 0.5)\n", "def test_davies_bouldin_score():\n    assert_raises_on_only_one_label(davies_bouldin_score)\n\n    assert_raises_on_all_points_same_cluster(davies_bouldin_score)\n\n    # Assert the value is 1. when all clusters are equal\n    X = np.array([[[0, 0], [0, 0]], [[1, 1], [1, 1]] * 2])\n    labels = np.array([0, 0, 1, 1, 1, 1])\n    assert_equal(davies_bouldin_score(X, labels), 1)\n\n    # General case\n    X = ([[0, 0], [1, 1]] * 5 + [[3, 3], [4, 4]] * 5 +\n         [[0, 4], [1, 3]] * 5 + [[3, 1], [4, 0]] * 5)\n    labels = [0] * 10 + [1] * 10 + [2] * 10 + [3] * 10\n    assert_greater(davies_bouldin_score(X, labels), 0)\n\n"], "sample_805": ["compilation error", "compilation error", "def test_regression_metrics_with_nan():\n    y_true = [1, 2, np.nan, 4]\n    y_pred = [1, 2, np.nan, 4]\n\n    with pytest.raises(ValueError, match=\"y_true and y_pred must have the same shape\"):\n        mean_squared_error(y_true, y_true[:-1])\n\n    with pytest.raises(ValueError, match=\"y_true and y_pred must have the same shape\"):\n        mean_squared_error(y_true[:-1], y_true)\n\n    with pytest.raises(ValueError, match=\"y_true and y_pred must have the same shape\"):\n        mean_absolute_error(y_true, y_true[:-1])\n\n    with pytest.raises(ValueError, match=\"y_true and y_pred must have the same shape\"):\n        mean_absolute_error(y_true[:-1], y_true)\n\n    # Check that the metrics handle NaNs correctly and return NaN\n    mse = mean_squared_error(y_true, y_pred)\n    mae = mean_absolute_error(y_true, y_pred)\n    assert np.isnan(mse)\n    assert np.isnan(mae)\n\n", "compilation error", "def test_multioutput_r2_score_with_empty_arrays():\n    with pytest.raises(ValueError) as exc:\n        r2_score(y_true=[], y_pred=[])\n    assert \"y_true and y_pred must have at least one sample each\" in str(exc.value)\n\n    with pytest.raises(ValueError) as exc:\n        r2_score(y_true=[[]], y_pred=[[]])\n    assert \"y_true and y_pred must have at least one sample each\" in str(exc.value)\n"], "sample_1093": ["    def test_sympy_printing():\n        from sympy import MatrixSymbol, symbols, Function\n\n        p = SymPyPrinter()\n\n        x, y = symbols('x y')\n        A = MatrixSymbol('A', 2, 2)\n\n        assert p.doprint(A) == 'A'\n        assert p.doprint(x**y) == 'x**y'\n        assert p.doprint(Function('f', x)) == 'f(x)'\n\n\n\n", "compilation error", "def test_SymPyPrinter_assoc_legendre():\n    from sympy import assoc_legendre\n\n    x = symbols('x')\n    n = symbols('n')\n    l = symbols('l')\n    expr = assoc_legendre(n, l, x)\n\n    prntr = SymPyPrinter()\n    assert prntr.doprint(expr) == 'sympy.special.eval_legendre(n, l, x)'\n", "def test_SymPyPrinter_MatrixSolve():\n    from sympy.codegen import MatrixSolve\n    from sympy.matrices import MatrixSymbol\n    prntr = SymPyPrinter()\n    A = MatrixSymbol(\"A\", 2, 2)\n    b = MatrixSymbol(\"b\", 2, 1)\n    expr = MatrixSolve(A, b)\n    assert prntr.doprint(expr) == 'solve(A, b)'\n\n", "compilation error"], "sample_206": ["    def test_close_after_delete(self):\n        \"\"\"\n        Calling close() on a FileField after delete ensures the file is\n        properly closed.\n        \"\"\"\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            with override_settings(MEDIA_ROOT=Path(tmp_dir)):\n                with TemporaryUploadedFile('foo.txt', 'text/plain', 1, 'utf-8') as tmp_file:\n                    d = Document.objects.create(myfile=tmp_file)\n                    d.myfile.delete()\n                    d.myfile.close()\n\n\n\n", "    def test_file_field_attribute_proxy(self):\n        d = Document.objects.create(myfile='something.txt')\n        self.assertIsInstance(d.myfile, FileField)\n        self.assertEqual(d.myfile.name, 'something.txt')\n\n        # Test that FileField attribute proxy respects the storage object\n        self.assertEqual(d.myfile.storage, d.myfile.field.storage)\n\n        # Test that FileField attribute proxy allows setting a new file on the instance\n        with open(os.path.join(self.settings.MEDIA_ROOT, 'something_else.txt'), 'rb') as f:\n            d.myfile = File(f)\n        self.assertEqual(d.myfile.name, 'something_else.txt')\n", "    def test_save_from_contentfile(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            with override_settings(MEDIA_ROOT=Path(tmp_dir)):\n                content = ContentFile(b'Test content', name='testfile.txt')\n                d = Document(myfile=content)\n                d.save()\n                self.assertTrue(os.path.exists(os.path.join(tmp_dir, 'unused', 'testfile.txt')))\n\n", "    def test_field_file_is_closed_when_deleting_object(self):\n        with TemporaryUploadedFile('myfile.txt', 'text/plain', 0, 'UTF-8') as tmp_file:\n            d = Document.objects.create(myfile=tmp_file)\n            self.assertTrue(d.myfile.closed, \"File is closed after creation\")\n\n            d.delete()\n            self.assertTrue(d.myfile.closed, \"File is closed after object deletion\") \n", "    def test_file_field_pre_save(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            with override_settings(MEDIA_ROOT=Path(tmp_dir)):\n                with TemporaryUploadedFile('foo.txt', 'text/plain', 1, 'utf-8') as tmp_file:\n                    d = Document(myfile=tmp_file)\n                    # Simulate a pre-save signal\n                    d._pre_save()\n                    self.assertTrue(os.path.exists(os.path.join(tmp_dir, 'unused', 'foo.txt')))\n"], "sample_36": ["def test_biweight_midcovariance_axis_3d():\n    \"\"\"Test a 3D array with the axis keyword.\"\"\"\n    with NumpyRNGContext(12345):\n        nz = 3\n        ny = 4\n        nx = 5\n        data = normal(5, 2, (nz, ny, nx))\n        cov = biweight_midcovariance(data, axis=0)\n        assert cov.shape == (ny, nx)\n\n        y = 0\n        covs = []\n        for i in range(nx):\n            covs.append(biweight_midcovariance(data[:, y, i]))\n        covs = np.array(covs)\n        assert_allclose(cov, covs)\n", "def test_biweight_midcorrelation_constant():\n    x = np.ones(5)\n    y = np.ones(5)\n    assert_allclose(biweight_midcorrelation(x, y), 1.0)\n\n    x = np.ones(5)\n    y = 2 * np.ones(5)\n    assert_allclose(biweight_midcorrelation(x, y), 1.0)\n", "compilation error", "def test_biweight_midcovariance_large_arrays():\n    \"\"\"\n    Test biweight_midcovariance with large arrays (to test for potential\n    memory issues or performance bottlenecks).\n    \"\"\"\n    n = 10000\n    data = np.random.randn(n, n)\n    with NumpyRNGContext(12345):\n        cov = biweight_midcovariance(data)\n\n\n", "def test_biweight_midcovariance_with_missing_values():\n    # test handling of missing values\n    d = np.array([1, 2, np.nan, 4, 5])\n    with catch_warnings(category=RuntimeWarning):\n        cov = biweight_midcovariance(d)\n    assert np.isnan(cov[0, 0])\n    \n    with catch_warnings(category=RuntimeWarning):\n        cov_modify = biweight_midcovariance(d, modify_sample_size=True)\n    assert np.isnan(cov_modify[0, 0])  \n\n"], "sample_989": ["def test_issue_10478():\n    assert Float('1.23e+10', 10) == Float('12300000000')\n    assert Float('1.23e-10', 10) == Float('0.000000000123')\n", "def test_issue_11957():\n    assert Float('1.234567890123456e-10').as_mpf() == mpmath.mpf('1.234567890123456e-10')\n", "def test_issue_10815():\n    assert Float('1.234567890123456789012345678901234567890').n(50) \\\n        == '1.234567890123456789012345678901234567890'\n    assert Float('1.2345678901234567890123456789012345678901234567890').n(50) \\\n        == '1.2345678901234567890123456789012345678901234567890'\n", "def test_issue_10442():\n    assert 1/Rational(0) == S.Infinity\n    assert 1/Rational(0, 1) == S.Infinity\n    assert -1/Rational(0) == S.NegativeInfinity\n    assert -1/Rational(0, 1) == S.NegativeInfinity\n", "def test_sympy_float_to_numpy():\n    from sympy.utilities.pytest import skip\n    from sympy.external import import_module\n    np = import_module('numpy')\n    if not np:\n        skip('numpy not installed. Abort numpy tests.')\n\n    x = Float(2.718281828459045)\n    y = np.float64(x)\n    assert np.allclose(x, y)\n    \n    x = Float(1.414213562373095)\n    y = np.float64(x)\n    assert np.allclose(x, y)\n    \n\n    x = Float(pi)\n    y = np.float64(x)\n    assert np.allclose(x, y)\n\n    x = Float(sqrt(2))\n    y = np.float64(x)\n    assert np.allclose(x, y)\n"], "sample_278": ["    def test_complex_group_by(self):\n        expr = ExpressionWrapper(\n            CombinedExpression(\n                Expression(F('cost')),\n                Combinable.ADD,\n                Expression(F('tax')),\n            ),\n            output_field=IntegerField(),\n        )\n        self.assertEqual(expr.get_group_by_cols(alias=None), ['cost', 'tax'])\n\n", "    def test_non_grouping_expression_in_group_by(self):\n        expr = ExpressionWrapper(F('cost') + F('tax'), output_field=IntegerField())\n        with self.assertRaises(GroupingError):\n            expr.get_group_by_cols(alias=None)\n", "    def test_group_by_with_alias(self):\n        expr = ExpressionWrapper(Value(3), output_field=IntegerField(), alias=\"foo\")\n        self.assertEqual(expr.get_group_by_cols(alias=None), [\"\"])\n", "    def test_complex_group_by(self):\n        expr = ExpressionWrapper(\n            F('a') + F('b') * Value(2),\n            output_field=IntegerField(),\n            alias='complex_sum',\n        )\n        self.assertEqual(\n            expr.get_group_by_cols(alias=None),\n            ['a', 'b']\n        )\n", "    def test_group_by_with_alias(self):\n        expr = ExpressionWrapper(Value(3), output_field=IntegerField())\n        self.assertEqual(expr.get_group_by_cols(alias='my_alias'), ['my_alias'])\n\n"], "sample_932": ["compilation error", "        def assert_layout_consistency(self):\n            \"\"\"Asserts that the classes used in the layout are consistent.\"\"\"\n            for tag, content_classes in self.content_classes.items():\n                assert content_classes.issubset(self.classes), (\n                    f\"Inconsistent classes for role '{self.name}' \"\n                    f\"using tag '{tag}': \"\n                    f\"Expected {self.classes}, got {content_classes}\"\n                )\n            assert len(self.classes) > 0, (\n                f\"Role '{self.name}' doesn't have any defined classes.\"\n            )\n\n\n", "        def check_classes(self, expected_classes):\n            assert self.classes == expected_classes, (\n                f'Expected {self.name} classes: {expected_classes}\\n'\n                f'Got:\\t{self.classes}'\n            )\n            for tag, tag_classes in self.content_classes.items():\n                assert tag_classes == expected_classes, (\n                    f'Expected {self.name} classes for tag {tag}: {expected_classes}\\n'\n                    f'Got:\\t{tag_classes}'\n                )\n", "        def assert_classes(self, expected_classes):\n            assert self.classes == expected_classes\n            for tag, content_classes in self.content_classes.items():\n                assert content_classes == expected_classes\n", "        def test_xref_consistency(self):\n            for tag in self.content_classes:\n                assert self.classes.issubset(self.content_classes[tag])\n\n\n\n\n"], "sample_1050": ["def test_indexed_base():\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(p[0, 1]) == 'p[0, 1]'\n", "compilation error", "def test_indexed_base():\n    x = IndexedBase('x')\n    p = IndexedBase('p')\n    expr = p[1, 2] + x[0, 1]\n    p = PythonCodePrinter()\n    assert p.doprint(expr) == 'p[1, 2] + x[0, 1]' \n", "def test_indexing():\n    prntr = PythonCodePrinter()\n    x = symbols('x')\n    A = MatrixSymbol(\"A\", 2, 2)\n    assert prntr.doprint(A[0, 1]) == 'A[0, 1]'\n    assert prntr.doprint(p[x]) == 'p[x]' \n", "compilation error"], "sample_436": ["    def test_missing_app(self):\n        self.write_settings(\"settings.py\", sdict={\"INSTALLED_APPS\": [\"app1\"]})\n        args = [\"dumpdata\", \"nonexistent\"]\n        out, err = self.run_manage(args)\n        self.assertOutput(err, \"app_label 'nonexistent' not found in INSTALLED_APPS\")\n\n\n\n", "    def test_suggestions_with_trailing_arguments(self):\n        args = [\"rnserver\", \"--settings=test_project.settings\", \"some\", \"extra\", \"args\"]\n        out, err = self.run_django_admin(args)\n        self.assertNoOutput(out)\n        self.assertOutput(err, \"Unknown command: 'rnserver'. Did you mean runserver?\")\n", "    def test_suggestions_for_nonexistent_command(self):\n        args = [\"nonexistent_command\", \"--settings=test_project.settings\"]\n        out, err = self.run_django_admin(args)\n        self.assertNoOutput(out)\n        self.assertOutput(\n            err, \"Unknown command: 'nonexistent_command'. For help, type 'django-admin help'.\"\n        )\n", "    def test_suggestions_multiple_options(self):\n        args = [\"rnserver\", \"--settings=test_project.settings\", \"--port=8000\"]\n        out, err = self.run_django_admin(args)\n        self.assertNoOutput(out)\n        self.assertOutput(\n            err,\n            \"Unknown command: 'rnserver'. Did you mean runserver?\\n\"\n            \"Try using --help for more information.\",\n        )\n", "    def test_suggestions_with_invalid_settings(self):\n        args = [\"rnserver\", \"--settings=not_there.settings\"]\n        out, err = self.run_django_admin(args)\n        self.assertNoOutput(out)\n        self.assertOutput(\n            err,\n            'ERROR: \"not_there.settings\" is not a valid setting file. '\n            'Please provide a valid settings module path.',\n        )\n"], "sample_703": ["def test_multiple_identifiers(expr: str, expected: bool) -> None:\n    matcher = {\"ident1\": True, \"ident2\": True, \"ident3\": True, \"ident4\": True}.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "def test_multiple_idents(\n    expr: str, expected: bool, expected_locals: dict", "    def test_nested_expressions(expr: str) -> None:\n        matcher = {\"true\": True, \"false\": False}.__getitem__\n        assert evaluate(expr, matcher) is expected\n", "def test_bool_literals(expr: str, expected: bool) -> None:\n    matcher = {\"True\": True, \"False\": False}.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "def test_multiple_idents(expr: str) -> None:\n    matcher = {\"ident1\": True, \"ident2\": True, \"ident3\": True, \"ident4\": True}.__getitem__\n    assert evaluate(expr, matcher)\n\n\n\n"], "sample_1189": ["    def  test_lambdify_with_matrices():\n        if not numpy:\n            skip(\"numpy not installed\")\n\n        A = MatrixSymbol('A', 2, 2)\n        B = MatrixSymbol('B', 2, 2)\n        f1 = lambdify([A, B], A + B)\n        result = f1(numpy.array([[1, 2], [3, 4]]), numpy.array([[5, 6], [7, 8]]))\n        assert numpy.array_equal(result, numpy.array([[6, 8], [10, 12]]))\n", "def test_lambdify_special_functions():\n    if not scipy:\n        skip(\"scipy not installed\")\n    x = symbols('x')\n    funcs = [\n        erfi,\n        erfc,\n        gammaln,\n        gamma,\n        hyperu\n    ]\n\n    for func in funcs:\n        f = lambdify(x, func(x), modules='scipy')\n        result = f(1.2)\n        reference = func(1.2).evalf()\n        assert abs(result - reference) < 1e-10\n\n\n", "def test_issue_24085():\n    if not scipy:\n        skip(\"scipy not installed\")\n\n    x = symbols('x')\n    y = symbols('y')\n    f = lambdify((x, y),  sin(x) * tan(y), modules='scipy')\n    assert abs(f(1, 1) - np.sin(1) * np.tan(1)) < 1e-10\n", "    def test_issue_23339():\n        if not scipy:\n            skip(\"scipy not installed\")\n        x = symbols(\"x\")\n        f = lambdify(x,  digamma(x), modules='scipy')\n        assert abs(f(2) - digamma(2).evalf()) < 1e-10\n\n", "def test_lambdify_with_IndexedBase():\n    if not numpy:\n        skip(\"numpy not installed\")\n\n    a = IndexedBase('a')\n    x = symbols('x')\n    i = symbols('i')\n    expr = a[x] + a[i]\n    f = lambdify((x, i), expr, 'numpy')\n    assert f(1, 2).shape == (1,)\n    assert f(1, 2) == 1 + a[2]\n    expr = Sum(a[i] for i in range(x))\n    f = lambdify((x), expr)\n    assert f(3) == a[0] + a[1] + a[2]\n"], "sample_323": ["    def test_minimize_rollbacks_app_dependencies(self):\n        a1_impl = FakeMigration('a1')\n        a2_impl = FakeMigration('a2')\n        a1 = ('a', '1')\n        a2 = ('a', '2')\n        b1_impl = FakeMigration('b1')\n        b2_impl = FakeMigration('b2')\n        b1 = ('b', '1')\n        b2 = ('b', '2')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_node(b2, b2_impl)\n        graph.add_dependency(None, b1, a1)\n        graph.add_dependency(None, b2, a2)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            b1: b1_impl,\n            a2: a2_impl,\n            b2: b2_impl,\n        })\n\n        plan = executor.migration_plan({a1})\n\n        self.assertEqual(plan, [(a2_impl, True), (b2_impl, True)])\n", "    def test_minimize_rollbacks_multiple_apps(self):\n        \"\"\"Minimize rollbacks when target has multiple app dependencies.\"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        c1_impl = FakeMigration('c1')\n        c1 = ('c', '1')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_node(c1, c1_impl)\n        graph.add_dependency(None, b1, a1)\n        graph.add_dependency(None, c1, a1)\n        graph.add_dependency(None, a2, a1)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            b1: b1_impl,\n            c1: c1_impl,\n            a2: a2_impl,\n        })\n\n        plan = executor.migration_plan({a1})\n\n        self.assertEqual(plan, [(a2_impl, True), (b1_impl, True), (c1_impl, True)])\n\n\n\n", "    def test_minimize_rollbacks_complex(self):\n        \"\"\"Minimize rollbacks with dependencies across multiple apps and circles.\"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        c1_impl = FakeMigration('c1')\n        c1 = ('c', '1')\n        d1_impl = FakeMigration('d1')\n        d1 = ('d', '1')\n\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_node(c1, c1_impl)\n        graph.add_node(d1, d1_impl)\n        graph.add_dependency(None, b1, a1)\n        graph.add_dependency(None, c1, a2)\n        graph.add_dependency(None, d1, c1)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            b1: b1_impl,\n            a2: a2_impl,\n            c1: c1_impl,\n            d1: d1_impl,\n        })\n\n        plan = executor.migration_plan({a1})\n\n        self.assertEqual(plan, [(a2_impl, True), (c1_impl, True), (d1_impl, True)])\n", "    def test_migrate_with_custom_graph(self):\n        \"\"\"\n        Tests applying migrations with a custom graph\n        \"\"\"\n        class CustomGraph(MigrationGraph):\n                super().__init__()\n                self.graph = graph\n\n                # For this test, we always add dependencies\n                self.graph.add_dependency(*args, **kwargs)\n\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_dependency(None, b1, a1)\n        graph.add_dependency(None, a2, a1)\n\n        custom_graph = CustomGraph(graph)\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(custom_graph, {\n            a1: a1_impl,\n            b1: b1_impl,\n            a2: a2_impl,\n        })\n\n        plan = executor.migration_plan({a1})\n\n        self.assertEqual(plan, [(a2_impl, True)])\n\n", "    def test_normalize_plan(self):\n        \"\"\"\n        Normalize plan ensures migrations ordered as expected for\n        complex dependency trees.\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        b2_impl = FakeMigration('b2')\n        b2 = ('b', '2')\n        c1_impl = FakeMigration('c1')\n        c1 = ('c', '1')\n        c2_impl = FakeMigration('c2')\n        c2 = ('c', '2')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_node(b2, b2_impl)\n        graph.add_node(c1, c1_impl)\n        graph.add_node(c2, c2_impl)\n        graph.add_dependency(None, b1, a1)\n        graph.add_dependency(None, b2, a2)\n        graph.add_dependency(None, c1, b1)\n        graph.add_dependency(None, c2, b2)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {})\n\n        plan = executor.migration_plan({a1, b1, c1})\n\n        self.assertEqual(plan, [\n            (a1_impl, False), (b1_impl, False), (a2_impl, False), (b2_impl, False), (c1_impl, False), (c2_impl, False)\n        ])\n\n\n\n"], "sample_421": ["    def test_invalid_then_constructor_args(self):\n        msg = \"Then() supports a literal, callable, or F object\"\n        with self.assertRaisesMessage(TypeError, msg):\n            When(Q(pk__in=[]), then=object())\n        with self.assertRaisesMessage(TypeError, msg):\n            When(Q(pk__in=[]), then=Case())\n", "    def test_when_ordering(self):\n        self.assertQuerysetEqual(\n            CaseTestModel.objects.annotate(\n                test=Case(\n                    When(integer__lte=1, then=Value(1)),\n                    When(integer__lte=2, then=Value(2)),\n                    When(integer__lte=3, then=Value(3)),\n                    default=Value(4),\n                ),\n            ).order_by(\"integer\", \"test\"),\n            [\n                (1, 1),\n                (2, 2),\n                (3, 3),\n                (4, 4),\n            ],\n            transform=attrgetter(\"integer\", \"test\"),\n        )\n", "    def test_multiple_when_conditions(self):\n        case = Case(\n            When(Q(pk__gt=1) & Q(integer=1), then=Value(\"match1\")),\n            When(Q(pk__gt=1) & Q(integer=2), then=Value(\"match2\")),\n            default=Value(\"default\"),\n        )\n        self.assertEqual(case.as_sql_with_params(extra_params={\"pk\": 2, \"integer\": 1}), \"CASE WHEN ((pk > 1) AND (integer = 1)) THEN 'match1' WHEN ((pk > 1) AND (integer = 2)) THEN 'match2' ELSE 'default' END\")\n        self.assertEqual(case.as_sql_with_params(extra_params={\"pk\": 3, \"integer\": 1}), \"CASE WHEN ((pk > 1) AND (integer = 1)) THEN 'match1' WHEN ((pk > 1) AND (integer = 2)) THEN 'match2' ELSE 'default' END\")\n        self.assertEqual(case.as_sql_with_params(extra_params={\"pk\": 1, \"integer\": 1}), \"CASE WHEN ((pk > 1) AND (integer = 1)) THEN 'match1' WHEN ((pk > 1) AND (integer = 2)) THEN 'match2' ELSE 'default' END\")\n        self.assertEqual(case.as_sql_with_params(extra_params={\"pk\": 2, \"integer\": 2}), \"CASE WHEN ((pk > 1) AND (integer = 1)) THEN 'match1' WHEN ((pk > 1) AND (integer = 2)) THEN 'match2' ELSE 'default' END\")\n\n", "    def test_when_kwargs_order(self):\n        with self.assertRaisesMessage(TypeError, \"Keyword arguments must be 'then', 'else', and 'output_field'\"):\n            Case(When(Q(pk__gt=0)), string=\"1\", output_field=TextField())\n", "    def test_when_lookup_chain(self):\n        class NestedModel(models.Model):\n            name = models.CharField(max_length=100)\n            fk = models.ForeignKey('CaseTestModel', on_delete=models.CASCADE)\n\n        nested = NestedModel.objects.create(name=\"nested\", fk=CaseTestModel.objects.get(pk=1))\n        self.assertQuerysetEqual(\n            CaseTestModel.objects.annotate(\n                test=Case(\n                    When(fk_rel__isnull=False, then=Value(nested.name)),\n                    default=Value(\"missing\"),\n                ),\n            ),\n            [\n                (\n                    CaseTestModel.objects.get(pk=1),\n                    \"missing\",\n                ),\n            ],\n            transform=attrgetter(\"name\", \"test\"),\n        )\n\n\n"], "sample_110": ["    def test_pickle_exists_with_annotations_still_usable(self):\n        group = Group.objects.create(name='group')\n        Event.objects.create(title='event', group=group)\n        groups = Group.objects.annotate(\n            has_event=models.Exists(\n                Event.objects.filter(group_id=models.OuterRef('id')),\n            ),\n            latest_event=models.Max('event__when'),\n        )\n        groups2 = pickle.loads(pickle.dumps(groups))\n        self.assertSequenceEqual(groups2.filter(has_event=True), [group])\n\n", "    def test_pickle_related_querysets_with_subquery(self):\n        Group.objects.create(name='Group {}'.format(i))\n        # create events and make sure they have related groups\n        for i in range(1, 3):\n            Event.objects.create(title='Event {}'.format(i), group=Group.objects.get(name='Group {}'.format(i)))\n\n        qs = Group.objects.annotate(\n            event_count=models.Subquery(\n                Event.objects.filter(group=models.OuterRef('id')).count()\n            ),\n        )\n        self.assert_pickles(qs)\n", "    def test_pickle_with_order_by_using_custom_field_with_lookup(self):\n        models.CharField(max_length=100, db_column='event_name').\n            # ... code for testing ...\n", "    def test_pickle_window_expressions(self):\n        g = Group.objects.create(name='Ponies')\n\n            qs = Event.objects.filter(group=g.id).annotate(\n                foo=window_fn(\n                    expression=models.F('when'),\n                    partition_by=['group_id'],\n                    order_by=['when'],\n                    frame='ROWS BETWEEN 1 PRECEDING AND CURRENT ROW',\n                )\n            )\n            dumped = pickle.dumps(qs.query)\n            reloaded = pickle.loads(dumped)\n            with self.assertNumQueries(1):\n                qs2 = Event.objects.filter(group=g.id).values('foo')\n                self.assertSQL(qs2, expected_sql)\n                self.assertSequenceEqual(list(qs2), list(reloaded))\n\n        test_window_expression(\n            window_fn=models.Window(\n                function=models.Avg,\n                output_field=models.FloatField(name='foo'),\n            ),\n            expected_sql='SELECT \"event\".\"id\", AVG(\"event\".\"when\") AS \"foo\" FROM \"event\" WHERE \"event\".\"group_id\" = 1 GROUP BY \"event\".\"group_id\" ORDER BY \"event\".\"when\" LIMIT 1 OFFSET 0',  # noqa\n        )\n\n\n\n", "    def test_pickle_subquery_queryset_still_usable(self):\n        group = Group.objects.create(name='group')\n        Event.objects.create(title='event', group=group)\n        groups = Group.objects.annotate(\n            event_title=models.Subquery(\n                Event.objects.filter(group_id=models.OuterRef('id')).values('title'),\n            ),\n        )\n        groups2 = pickle.loads(pickle.dumps(groups))\n        self.assertSequenceEqual(groups2.filter(event_title='event'), [group])\n\n\n\n"], "sample_445": ["    def test_reversed_depth(self):\n        t = self.t + self.oneyear + self.onemonth + self.oneweek + self.oneday + self.onehour\n        self.assertEqual(timeuntil(t, self.t, depth=2), \"1\\xa0year, 1\\xa0month\")\n", "    def test_timeuntil_with_future_date(self):\n        \"\"\"Test timeuntil with a future date.\"\"\"\n        future_date = datetime.date(2024, 10, 26)\n        self.assertEqual(timeuntil(future_date), \"2\\xa0years, 2\\xa0months, 15\\xa0days\")\n", "    def test_timeuntil_with_future_datetimes(self):\n        future = datetime.datetime(2024, 1, 1)\n        self.assertEqual(timeuntil(self.t, future), \"2\\xa0years, 5\\xa0months, 1\\xa0week, 1\\xa0day\")\n", "    def test_timeuntil_reversed(self):\n        self.assertEqual(timeuntil(self.t + self.oneday), \"1\\xa0day\")\n", "    def test_empty_time_span(self):\n        self.assertEqual(timesince(self.t, self.t), \"0\\xa0minutes\")\n        self.assertEqual(timeuntil(self.t, self.t), \"0\\xa0minutes\")\n"], "sample_842": ["def test_kernel_caching(kernel):\n    # Test caching behavior in kernels.\n    K1 = kernel(X)\n    K2 = kernel(X)\n    assert_array_almost_equal(K1, K2)\n\n\n", "def test_kernel_repr(kernel):\n    # Test repr of kernels.\n    repr_str = repr(kernel)\n    assert repr_str.startswith(kernel.__class__.__name__)\n\n\n", "def test_kernel_repr(kernel):\n    # Test representation of kernels.\n    repr_kernel = repr(kernel)\n    # Check that repr includes the kernel class and its parameters\n    assert kernel.__class__.__name__ in repr_kernel\n    for hyperparameter in kernel.hyperparameters:\n        assert str(getattr(kernel, hyperparameter.name)) in repr_kernel\n", "def test_kernel_with_invalid_input(kernel):\n    # Check kernel object raises an error with invalid input.\n    with pytest.raises(ValueError):\n        kernel(X, Y=X)  # Assuming kernels should not handle pairwise inputs \n", "def test_kernel_with_missing_parameters(kernel):\n    # Test that kernels with missing parameters raise an error.\n    with pytest.raises(ValueError):\n        Kernel(kernel.get_params().pop(\"dummy_param\"))\n\n"], "sample_1205": ["compilation error", "compilation error", "compilation error", "compilation error", "def test_PolyElement_is_univariate():\n    R, x, y = ring(\"x,y\", ZZ)\n    assert x.is_univariate is True\n    assert (x+y).is_univariate is False\n    R, x = ring(\"x\", ZZ)\n    assert x.is_univariate is True\n\n"], "sample_224": ["    def test_group_by_subquery_annotation_with_filter(self):\n        \"\"\"\n        Subquery annotations with filters are included in the GROUP BY\n        if they are grouped against.\n        \"\"\"\n        long_books_count_qs = Book.objects.filter(\n            publisher=OuterRef('pk'),\n            pages__gt=400,\n            rating__gte=4,\n        ).values(\n            'publisher'\n        ).annotate(count=Count('pk')).values('count')\n        long_books_count_breakdown = Publisher.objects.annotate(\n            long_books_count=Subquery(long_books_count_qs, IntegerField()),\n        ).filter(long_books_count__gt=0).values('long_books_count')\n        self.assertEqual(list(long_books_count_breakdown), [{'long_books_count': 2}])\n", "    def test_aggregate_over_subquery(self):\n        avg_rating_by_publisher = Book.objects.annotate(\n            avg_rating_by_publisher=Avg('rating', output_field=DecimalField())\n        ).values('publisher').annotate(avg_rating=Avg('avg_rating_by_publisher'))\n        self.assertEqual(\n            list(avg_rating_by_publisher), [\n                {'publisher': 'Apress', 'avg_rating': 4.5},\n                {'publisher': 'Morgan Kaufmann', 'avg_rating': 4.6666666666666664},\n                {'publisher': \"Jonno's House of Books\", 'avg_rating': 4.0},\n                {'publisher': 'Prentice Hall', 'avg_rating': 4.75},\n                {'publisher': 'Sams', 'avg_rating': 4.5},\n            ]\n        )\n", "    def test_aggregation_subquery_annotation_grouping(self):\n        if connection.vendor == 'mysql' and 'ONLY_FULL_GROUP_BY' in connection.sql_mode:\n            self.skipTest(\n                'GROUP BY optimization does not work properly when '\n                'ONLY_FULL_GROUP_BY mode is enabled on MySQL, see #31331.'\n            )\n        subquery_qs = Author.objects.filter(\n            book__publisher=OuterRef('pk'),\n        ).values('name')\n        publisher_qs = Publisher.objects.annotate(\n            author_count=Count('book__author'),\n            author_names=Subquery(subquery_qs, output_field=ArrayField(CharField())),\n        ).values('author_count', 'author_names')\n        self.assertEqual(\n            dict(publisher_qs),\n            {\n                'author_count': [1, 2, 3],\n                'author_names': [\n                    ['Adrian Holovaty'],\n                    ['Brad Dayley', 'James Bennett', 'Peter Norvig'],\n                    ['Jeffrey Forcier'],\n                ]\n            }\n        )\n", "    def test_subquery_expression_aggregate(self):\n        recent_books_qs = Book.objects.filter(\n            pubdate__year=OuterRef('year')\n        ).values('pubdate').annotate(count=Count('pk'))\n        publisher_qs = Publisher.objects.annotate(\n            recent_books_count=Subquery(recent_books_qs),\n        ).values('name', 'recent_books_count').order_by('name')\n        self.assertEqual(\n            list(publisher_qs), [\n                {'name': 'Apress', 'recent_books_count': 1},\n                {'name': 'Morgan Kaufmann', 'recent_books_count': 1},\n                {'name': \"Jonno's House of Books\", 'recent_books_count': 0},\n                {'name': 'Prentice Hall', 'recent_books_count': 1},\n                {'name': 'Sams', 'recent_books_count': 1},\n            ]\n        )\n", "    def test_order_by_unaggregated_expression(self):\n        qs = Book.objects.annotate(\n            age_sum=Sum('authors__age'),\n        ).order_by('age_sum')\n        self.assertCountEqual(list(qs), [\n            self.b4,\n            self.b5,\n            self.b3,\n            self.b1,\n            self.b2,\n            self.b6,\n        ])\n\n\n\n"], "sample_184": ["    def test_unique_constraint_include_with_condition(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['age'],\n                        name='unique_age_include_condition',\n                        condition=models.Q(age__gte=100),\n                        include=['id'],\n                    ),\n                ]\n\n        errors = Model.check(databases=self.databases)\n        expected = [] if connection.features.supports_partial_indexes else [\n            Warning(\n                '%s does not support unique constraints with conditions '\n                'and non-key columns.',\n                hint=(\n                    \"A constraint won't be created. Silence this warning if \"\n                    \"you don't care about it.\"\n                ),\n                obj=Model,\n                id='models.W040',\n            ),\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_unique_constraint_include_pointing_to_fk(self):\n        class Target(models.Model):\n            pass\n\n        class Model(models.Model):\n            fk = models.ForeignKey(Target, models.CASCADE, related_name='target')\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['fk_id'],\n                        include=['id'],\n                        name='name',\n                    ),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [])\n\n\n", "    def test_unique_constraint_include_with_joined_fields(self):\n        class Model(models.Model):\n            name = models.CharField(max_length=10)\n            field1 = models.PositiveSmallIntegerField()\n            field2 = models.PositiveSmallIntegerField()\n            parent = models.ForeignKey('self', models.CASCADE)\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['name', 'field1'],\n                        include=['parent__field2'],\n                        name='name1',\n                    ),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [\n            Error(\n                \"'constraints' refers to the joined field 'parent__field2'.\",\n                obj=Model,\n                id='models.E041',\n            ),\n        ])\n", "    def test_unique_constraint_include_with_condition(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['age'],\n                        name='unique_age_include_condition',\n                        condition=models.Q(age__gte=100),\n                        include=['id'],\n                    ),\n                ]\n\n        errors = Model.check(databases=self.databases)\n        expected = [] if connection.features.supports_partial_indexes else [\n            Warning(\n                '%s does not support unique constraints with conditions and non-key columns.'\n                % connection.display_name,\n                hint=(\n                    \"A constraint won't be created. Silence this warning if \"\n                    \"you don't care about it.\"\n                ),\n                obj=Model,\n                id='models.W040',\n            ),\n        ]\n        self.assertEqual(errors, expected)\n\n", "    def test_unique_constraint_include_with_condition(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['age'],\n                        name='unique_age_include_condition',\n                        include=['id'],\n                        condition=models.Q(age__gte=100),\n                    ),\n                ]\n\n        errors = Model.check(databases=self.databases)\n        expected = [] if connection.features.supports_covering_indexes else [\n            Warning(\n                '%s does not support unique constraints with conditions and '\n                'non-key columns.',\n                hint=(\n                    \"A constraint won't be created. Silence this warning if you \"\n                    \"don't care about it.\"\n                ),\n                obj=Model,\n                id='models.W040',\n            ),\n        ]\n        self.assertEqual(errors, expected)\n"], "sample_538": ["def test_invalid_transform_combinations():\n    t1 = mtransforms.Affine2D()\n    t2 = mtransforms.Affine2D()\n\n    # Ensure that mixing incompatible transform types raises an error.\n    with pytest.raises(ValueError, match=\"Incompatible transform types\"):\n        t1 + mtransforms.LogTransform(10)\n\n    with pytest.raises(ValueError, match=\"Incompatible transform types\"):\n        t1 * mtransforms.LogTransform(10)\n\n    with pytest.raises(ValueError, match=\"Incompatible transform types\"):\n        t1.inverted() + mtransforms.LogTransform(10)\n\n    # Ensure that chaining incompatible transforms raises an error.\n    with pytest.raises(ValueError, match=\"Incompatible transform types\"):\n        t1.transform(t2) + mtransforms.LogTransform(10)\n\n\n", "def test_transform_non_affine_invalidated():\n    point = [1.0, 1.0]\n    t = mtransforms.Affine2D()\n    assert_almost_equal(point, t.transform(t.inverted().transform(point)))\n    # Change and access the transform\n    t.translate(1.0, 1.0).get_matrix()\n    assert_almost_equal(point, t.transform(t.inverted().transform(point)))\n", "compilation error", "def test_transform_invalid_path():\n    t = mtransforms.Affine2D()\n    # A Path with differing numbers of vertices\n    with pytest.raises(ValueError):\n        t.transform_path_non_affine(Path([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5]]))\n\n    # A Path with differing data types\n    with pytest.raises(TypeError):\n        t.transform_path_non_affine(Path([[1, '2'], [3], [4, 5]]))\n\n\n", "compilation error"], "sample_309": ["    def test_parsing_rfc1123_with_timezone_offset(self):\n        parsed = parse_http_date('Sun, 06 Nov 1994 08:49:37 +0200')\n        self.assertEqual(\n            datetime.fromtimestamp(parsed, timezone.utc).astimezone(timezone(timedelta(hours=2))),\n            datetime(1994, 11, 6, 8, 49, 37),\n        )\n\n", "    def test_parsing_errors(self):\n        with self.assertRaisesMessage(ValueError, \"is not in a valid HTTP date format\"):\n            parse_http_date('invalid date')\n        with self.assertRaisesMessage(ValueError, \"is not in a valid HTTP date format\"):\n            parse_http_date('Fri, 01 Jan 2007 01:54:21')\n        with self.assertRaisesMessage(ValueError, \"is not in a valid HTTP date format\"):\n            parse_http_date('Mon, Jan 01 2007 01:54:21 GMT')\n\n\n", "    def test_parsing_year_greater_than_2037(self):\n        with self.assertRaises(ValueError):\n            parse_http_date('Sun Nov  6 08:49:37 2038')\n", "    def test_parsing_invalid_date(self):\n        with self.assertRaises(ValueError):\n            parse_http_date('Foo')\n\n        with self.assertRaises(ValueError):\n            parse_http_date('Sun  6 Nov 1994 08:49:37 GMT')\n", "    def test_parsing_future_dates(self):\n        # Ensure correct parsing of dates in the future.\n        future_date = datetime(2050, 12, 31, 23, 59, 59, tzinfo=timezone.utc)\n        parsed = parse_http_date(future_date.strftime('%a, %d-%b-%y %H:%M:%S GMT'))\n        self.assertEqual(datetime.fromtimestamp(parsed, timezone.utc), future_date)\n"], "sample_213": ["    def test_file_like_object_upload(self):\n        class MyFile:\n                self.content = content\n                return self.content\n                return len(self.content)\n                \n        my_file = MyFile(b\"Content from MyFile\")\n        self.storage.save('my_file.txt', my_file)\n        with open(self.storage.path('my_file.txt'), 'rb') as f:\n            self.assertEqual(f.read(), b\"Content from MyFile\")\n\n\n\n", "    def test_file_like_object_from_stream(self):\n        with open(f\"{self.temp_dir}/input.txt\", \"wb\") as f:\n            f.write(b\"This is some content\")\n        with open(f\"{self.temp_dir}/input.txt\", \"rb\") as f:\n            content = ContentFile(f)\n            self.storage.save(\"output.txt\", content)\n        self.assertTrue(self.storage.exists(\"output.txt\"))\n        with self.storage.open(\"output.txt\") as out:\n            self.assertEqual(out.read(), b\"This is some content\")\n", "    def test_save_with_file_like_object(self):\n        class MyFile(object):\n                self.content = content\n\n                return self.content\n\n        my_file = MyFile(b\"Some content\")\n        self.storage.save('my_file', my_file)\n        with open(self.storage.path('my_file'), 'rb') as f:\n            self.assertEqual(f.read(), b\"Some content\")\n", "    def test_file_like_object_saving(self):\n        class MyFile:\n                self.content = content\n\n                return self.content\n\n                pass\n\n        with self.assertNumQueries(1):\n            self.storage.save('myfile.txt', MyFile(b\"Hello, world!\"))\n        with open(self.storage.path('myfile.txt'), 'rb') as f:\n            self.assertEqual(f.read(), b\"Hello, world!\")\n\n\n", "    def test_file_like_objects_with_read_bytes(self):\n        with self.app_client.post(\n            '/upload/',\n            files={'file': StringIO(b'file content')}\n        ) as response:\n            self.assertEqual(response.status_code, 200)\n            self.assertEqual(response.json(), {'filename': 'uploaded_file'})\n"], "sample_955": ["    def test_unparse_star_expressions():\n        assert ast.unparse(ast.parse(\"(*args, **kwargs)\").body[0].value) == \"(*args, **kwargs)\"\n        assert ast.unparse(ast.parse(\"(*args)\").body[0].value) == \"(*args)\"\n        assert ast.unparse(ast.parse(\"(**kwargs)\").body[0].value) == \"(**kwargs)\"\n", "    def test_unparse_starred_arguments():\n        source = \"\"\"", "def test_unparse_complex_args():\n    source = \"\"\"\n        pass\n    \"\"\"\n    module = ast.parse(source)\n    args_str = ast.unparse(module.body[0].args, source)\n    assert args_str == \"a: int, b: str = 'default', *args: int, **kwargs: str\"\n", "    def test_complex_expression():\n        source = \"\"\"\n            return a + b * c - d / e\n        \"\"\"\n        module = ast.parse(source)\n        expression = module.body[0].body[0].value\n        assert ast.unparse(expression, source) == \"a + b * c - d / e\"\n", "    def test_unparse_attribute_deep(source, expected):\n        module = ast.parse(source)\n        assert ast.unparse(module.body[0].value, source) == expected \n"], "sample_942": ["def test_noindexentry_kwargs(app):\n    text = (\".. py:function:: f(a, b=1)\\n\"\n            \"   :noindexentry:\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc, ((addnodes.index, desc))))\n    assert_node(doctree[0], addnodes.index, entries=[('pair', 'built-in function; f(a, b=1)', 'f', '', None)])\n    assert_node(doctree[2][0], addnodes.index, entries=[]) \n", "compilation error", "def test_py_version_and_signature(app):\n    text = (\".. py:versionadded:: 3.10\\n\"\n            \".. py:versionchanged:: 3.11\\n\"\n            \".. py:module:: example\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, [addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"module \"],\n                [desc_addname, \"example.\"],\n                [desc_name, \"example\"])],\n                [desc_content, ()])],\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"added in \"],\n                [pending_xref, \"3.10\"]),\n                [desc_content, ()])],\n                          addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"changed in \"],\n                [pending_xref, \"3.11\"]),\n                [desc_content, ()])])])\n    assert 'example' in domain.objects\n    assert domain.objects['example'] == ('index', 'example', 'module', False)\n\n\n\n", "def test_noindexentry_method(app):\n    text = (\".. py:class:: Class\\n\"\n            \"   .. py:method:: meth\\n\"\n            \"      :noindexentry:\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc, addnodes.index, desc))\n    assert_node(doctree[0], addnodes.index, entries=[('single', 'Class (built-in class)', 'Class', '', None)])\n    assert_node(doctree[2], addnodes.index, entries=[])\n", "def test_pyfunction_noindexentry(app):\n    text = (\".. py:function:: f()\\n\"\n            \"   :noindexentry:\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc, addnodes.index))\n    assert_node(doctree[0], addnodes.index, entries=[])\n\n\n\n\n"], "sample_58": ["    def test_renderer_inheritance(self):\n        class CustomRenderer(DjangoTemplates):\n                return f'<span>{value}</span>'\n\n        class MyForm(Form):\n            renderer = CustomRenderer\n\n            name = CharField()\n\n        f = MyForm({'name': 'John'})\n        self.assertHTMLEqual(str(f), '<span>John</span>')\n", "    def test_renderer_override_on_each_field(self):\n        class MyForm(Form):\n            field1 = CharField()\n            field2 = CharField()\n\n                super().__init__(*args, **kwargs)\n                self.renderer1 = DjangoTemplates()\n                self.renderer2 = CustomRenderer()\n\n        form = MyForm()\n        self.assertEqual(form.field1.renderer, form.renderer1)\n        self.assertEqual(form.field2.renderer, form.renderer2)\n", "    def test_renderer_overriding_form(self):\n        class MyForm(Form):\n            field = CharField()\n\n        class MyRenderer(DjangoTemplates):\n                return f'Custom field: {field.label_tag()}'\n\n        form = MyForm()\n        form.renderer = MyRenderer()\n        self.assertEqual(form.as_p(), '<p>Custom field: <label for=\"id_field\">Field:</label></p>')\n", "    def test_renderer_context(self):\n        class MyForm(Form):\n            name = CharField()\n            email = EmailField()\n\n        fake_request =  HttpRequest()\n        fake_request.user = User()\n        fake_request.POST = {'name': 'John Doe', 'email': 'john.doe@example.com'}\n\n        form = MyForm(fake_request.POST)\n        context = {'form': form}\n        \n        renderer = form.renderer\n        self.assertIn('form', renderer.render(None, context)['form'])\n        self.assertEqual(renderer.render(None, context)['form'].get('name'), 'John Doe')\n        self.assertEqual(renderer.render(None, context)['form'].get('email'), 'john.doe@example.com')\n", "    def test_renderer_override(self):\n        class MyForm(Form):\n            field1 = CharField()\n\n        form = MyForm(renderer=CustomRenderer())\n        self.assertIsInstance(form.renderer, CustomRenderer)\n\n\n"], "sample_1015": ["compilation error", "compilation error", "def test_ccode_Type_with_attributes():\n    a = symbols('a', real=True)\n    t1 = Type('int')\n    t2 = Type('float')\n    t3 = Type('_Float128')\n\n    assert ccode(t1) == 'int'\n    assert ccode(t2) == 'float'\n    assert ccode(t3) == '_Float128'\n\n    t4 = Type('bool', const=True)\n    assert ccode(t4) == 'const bool'\n    t5 = Type('int', const=True, pointer=True)\n    assert ccode(t5) == 'const int*'\n    t6 = Type('float', array=True, size=10)\n    assert ccode(t6) == 'float [10]'\n    t7 = Type('int', array=True, size=10, const=True)\n    assert ccode(t7) == 'const int [10]'\n    t8 = Type('int', array=True, size=a)\n    assert ccode(t8) == 'int [a]'\n\n\n", "def test_ccode_complex():\n    assert ccode(x + 1j) == 'x + 1j'\n    assert ccode(1j*x) == '1j*x'\n    assert ccode(complex(x, y)) == 'complex(x, y)'\n    assert ccode(Abs(x + 1j)) == 'cabs(x + 1j)'\n    assert ccode(Arg(x + 1j)) == 'carg(x + 1j)'\n    assert ccode(Re(x + 1j)) == 'creal(x + 1j)'\n    assert ccode(Im(x + 1j)) == 'cimag(x + 1j)'\n", "def test_ccode_Array():\n    x = symbols('x')\n    a = Array(x, 2)\n    assert ccode(a) == 'x[2]'\n    assert ccode(a[0]) == 'x[0]'\n    assert ccode(a[[1, 0]]) == 'x[1], x[0]'\n\n\n\n"], "sample_814": ["    def test_verbose_logging(self):\n        #Test that verbose logging works correctly\n        X, y = datasets.make_classification(random_state=0)\n        gb = GradientBoostingClassifier(n_estimators=10, verbose=2)\n        gb.fit(X, y)\n        \n        #Check if logs contain expected messages\n        for log_message in gb.logger.info_messages:\n            assert 'loss' in log_message\n            assert 'estimator' in log_message\n        gb.logger.info_messages = []\n        gb.fit(X, y, verbose=False)\n        assert len(gb.logger.info_messages) == 0\n\n\n\n", "    def test_gradient_boosting_with_init_n_estimators(init_estimator):\n        # Test that n_estimators in init estimator is ignored when n_estimators is\n        # set in GradientBoostingClassifier/Regressor.\n\n        X, y = make_classification()\n\n        init_est = init_estimator(n_estimators=100)\n\n        gb = GradientBoostingClassifier(n_estimators=50, init=init_est)\n        gb.fit(X, y)\n        assert gb.n_estimators_ == 50\n\n        gb = GradientBoostingRegressor(n_estimators=50, init=init_est)\n        gb.fit(X, y)\n        assert gb.n_estimators_ == 50\n", "    def test_gradient_boosting_missing_values():\n        # Check how GradientBoostingClassifier handle missing values\n        X = [[1, 2, np.nan], [4, 5, 6], [7, 8, np.nan]]\n        y = [0, 1, 0]\n\n        # Test with default handling of missing values:\n        gbc = GradientBoostingClassifier()\n        gbc.fit(X, y)\n        assert_array_equal(gbc.predict([[9, 10, 11]]), [0])\n\n        # Test with specifying missing values\n        gbc = GradientBoostingClassifier(missing_values=np.nan)\n        gbc.fit(X, y)\n        assert_array_equal(gbc.predict([[9, 10, 11]]), [0])\n\n        # Test with missing values in every features\n        X_all_missing = [[np.nan, np.nan, np.nan],\n                         [np.nan, np.nan, np.nan],\n                         [np.nan, np.nan, np.nan]]\n        with pytest.raises(ValueError,\n                           match=\"No valid data found in the matrix\"):\n            gbc = GradientBoostingClassifier()\n            gbc.fit(X_all_missing, y)\n\n\n\n", "def test_gradient_boosting_with_n_estimators_set_to_1():\n    # Test behaviour when n_estimators is set to 1\n    X, y = datasets.make_classification(n_samples=10, random_state=0)\n\n    gbc = GradientBoostingClassifier(n_estimators=1, learning_rate=0.1,\n                                     max_depth=3, random_state=42)\n    gbc.fit(X, y)\n\n    assert_equal(gbc.n_estimators_, 1)\n    assert_array_equal(gbc.estimators_.shape, ((1, X.shape[1], X.shape[1])))\n", "def test_sparse_input_with_init():\n    X, y = datasets.make_classification(random_state=0, n_samples=50,\n                n_features=10, n_informative=5)\n    sparse_matrix = csc_matrix(X)\n\n    # Test that the init estimator accepts sparse input\n\n    init_est = DummyRegressor()\n\n    gb = GradientBoostingRegressor(init=init_est,\n                                    random_state=42).fit(sparse_matrix, y)\n    assert np.allclose(gb.predict(sparse_matrix), gb.predict(X))\n\n"], "sample_589": ["def test_interpolate_na_max_gap_nan_values(da_time):\n    da_time = da_time.copy()\n    da_time = da_time.astype(\"float64\")\n    da_time.loc[da_time.isnull()] = np.nan\n    expected = da_time.fillna(method=\"linear\", limit=None)\n    actual = da_time.interpolate_na(\"t\", max_gap=pd.Timedelta(\"1H\"))\n    assert_array_equal(actual, expected)\n", "def test_interpolate_na_max_gap_time_specifier_multiple_coords(\n    da_time, time_range_func", "def test_interpolate_na_time_specifier_method(\n    da_time, method, transform, time_range_func", "def test_interpolate_na_method_limits(da_time):\n    da_time = da_time.copy(data=[np.nan, 1, 2, np.nan, np.nan, 5, np.nan, np.nan, np.nan, np.nan, 10])\n    for method in [\"linear\", \"nearest\", \"zero\", \"slinear\", \"quadratic\", \"cubic\"]:\n        for limit in [None, 1, 2]:\n            actual = da_time.interpolate_na(\"t\", method=method, limit=limit)\n            expected = da_time.copy()\n            expected.data = expected.data\n            if limit is not None:\n                expected = expected.truncate(\n                    before=expected.index[0] + pd.Timedelta(hours=limit),\n                    after=expected.index[-1] - pd.Timedelta(hours=limit),\n                )\n\n            if method == \"linear\":\n                expected = expected.interpolate(method=\"linear\", limit=limit)\n\n            elif method == \"nearest\":\n                expected = expected.interpolate(method=\"nearest\", limit=limit)\n\n            elif method == \"zero\":\n                expected = expected.interpolate(method=\"zero\", limit=limit)\n\n            elif method == \"slinear\":\n                expected = expected.interpolate(method=\"slinear\", limit=limit)\n\n            elif method == \"quadratic\":\n                expected = expected.interpolate(method=\"quadratic\", limit=limit)\n\n            elif method == \"cubic\":\n                expected = expected.interpolate(method=\"cubic\", limit=limit)\n            assert_equal(actual, expected)\n", "def test_interpolate_na_method_time_specifier(da_time, method, time_range_func):\n    da_time[\"t\"] = time_range_func(\"2001-01-01\", freq=\"H\", periods=11)\n    expected = da_time.copy(data=[np.nan, 1, 2, 3, 4, 5, np.nan, np.nan, np.nan, np.nan, 10])\n    actual = da_time.interpolate_na(\"t\", method=method)\n    assert_equal(actual, expected)\n\n\n\n"], "sample_1138": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_876": ["def test_mlp_activation_hidden(activation):\n    # Test that different activation functions work as expected for hidden layer\n    mlp = MLPClassifier(\n        hidden_layer_sizes=(5, 2), activation=activation, random_state=1, max_iter=50\n    )\n    mlp.fit(X_digits[:300], y_digits[:300])\n    assert mlp.coefs_[0].shape == (\n        5,\n        X_digits.shape[1]\n        if activation in [\"relu\", \"tanh\", \"logistic\"]\n        else 6  # For 'Identity' activation, we expect the output layer to be same size as input\n    )\n    assert mlp.intercepts_[0].shape == (5,)\n", "    def test_mlp_early_stopping_activation(activation):\n        mlp = MLPEstimator(\n            max_iter=10, random_state=0, warm_start=True, early_stopping=True, activation=activation\n        )\n        mlp.fit(X_iris, y_iris)\n        mlp.set_params(max_iter=20)\n        mlp.fit(X_iris, y_iris)\n        assert len(mlp.validation_scores_) > 1\n\n", "def test_mlp_max_iter_with_validation_fraction(MLPEstimator):\n    # Tests that max_iter is respected when using validation_fraction\n    # Regression test for issue #11543.\n\n    X, y = X_digits[:300], y_digits[:300]\n    mlp = MLPEstimator(\n        hidden_layer_sizes=(5, 3), random_state=1, learning_rate_init=0.01\n    )\n    mlp.fit(X, y)\n    assert mlp.n_iter_ < mlp.max_iter_\n\n    mlp = MLPEstimator(\n        hidden_layer_sizes=(5, 3), random_state=1, learning_rate_init=0.01, max_iter=10\n    )\n    mlp.fit(X, y, validation_fraction=0.2)\n    assert mlp.n_iter_ <= mlp.max_iter_\n\n\n", "def test_mlp_early_stopping_with_validation_split(MLPEstimator):\n    # Check that early stopping works with validation_split\n    X = X_digits\n    y = y_digits\n\n    mlp = MLPEstimator(\n        max_iter=100, random_state=42, early_stopping=True, validation_split=0.2\n    )\n\n    with ignore_warnings(category=ConvergenceWarning):\n        mlp.fit(X, y)\n    assert len(mlp.validation_scores_) > 0\n", "def test_mlp_with_sparse_feature_scaling_and_validation():\n    from sklearn.preprocessing import MinMaxScaler\n    from sklearn.datasets import make_classification\n\n    X, y = make_classification(n_samples=100, n_features=50, random_state=42)\n    X_sparse = csr_matrix(X)\n\n    # Scale the sparse data\n    scaler = MinMaxScaler()\n    X_sparse_scaled = scaler.fit_transform(X_sparse)\n\n    mlp = MLPClassifier(\n        hidden_layer_sizes=(10, 5),\n        solver=\"adam\",\n        max_iter=200,\n        warm_start=False,\n        validation_fraction=0.2,\n        random_state=42,\n    )\n\n    mlp.fit(X_sparse_scaled, y)\n    assert mlp.validation_scores_ is not None\n    assert mlp.best_validation_score_ is not None\n\n\n\n"], "sample_808": ["def test_iforest_max_samples_auto():\n    X_train = [[0, 1], [1, 2], [2, 1]]\n    X_test = [[2, 1], [1, 1]]\n    clf = IsolationForest(max_samples='auto').fit(X_train)\n    clf.predict(X_test)\n", "def test_iforest_sparse_contamination_error():\n    # test that contamination parameter is properly handled\n    # when the input data type is sparse\n    X_train = csc_matrix([[1, 1], [1, 2],[2, 1]])\n    \n    with assert_raises(ValueError):\n        IsolationForest(max_samples=0.5, contamination='foobar').fit(X_train)\n    with assert_raises(ValueError):\n        IsolationForest(contamination=-1).fit(X_train)\n    with assert_raises(ValueError):\n        IsolationForest(contamination=1.1).fit(X_train)\n    with assert_raises(ValueError):\n        IsolationForest(contamination=5).fit(X_train)\n\n\n\n", "def test_iforest_sparse_random_state():\n    \"\"\"\n    Test if Isolation Forest with sparse data sets random state correctly.\n    \"\"\"\n    rng = check_random_state(0)\n    X_train = sp.csr_matrix(rng.randn(100, 20))\n    X_test = sp.csr_matrix(rng.randn(20, 20))\n\n    clf = IsolationForest(random_state=rng).fit(X_train)\n    with patch(\"sklearn.ensemble.iforest.numpy.random.seed\",\n               new_callable=Mock) as mock_seed:\n        # Simulate random state being set\n        mock_seed.return_value = rng.randint(0, 10000)\n        clf.isolation_forest._compute_tree(X_train,\n                                          y_train=None,\n                                          n_samples=50)\n    assert mock_seed.call_count == 1\n    assert mock_seed.call_args[0] == rng.randint(0, 10000)\n\n", "def test_iforest_sparse_fit_predict():\n    \"\"\"Test fit and predict on sparse input data.\"\"\"\n    rng = check_random_state(0)\n\n    # Create some sparse data\n    X_train = sp.csr_matrix(rng.randn(100, 5))\n    X_test = sp.csr_matrix(rng.randn(20, 5))\n\n    # Fit and predict with IsolationForest\n    clf = IsolationForest(max_samples=0.5).fit(X_train)\n    y_pred = clf.predict(X_test)\n    assert_array_equal(y_pred.astype(int), np.array(y_pred))\n", "def test_iforest_behaviour_legacy_threshold():\n    X = [[0.0], [1.0]]\n    clf = IsolationForest(behaviour='old', contamination=0.1)\n    clf.fit(X)\n    assert hasattr(clf, 'threshold_')\n    assert clf.threshold_ is not None\n\n    clf_new = IsolationForest(behaviour='new', contamination=0.1)\n    clf_new.fit(X)\n    assert not hasattr(clf_new, 'threshold_')\n"], "sample_595": ["def test_lower_upper(dtype):\n    values = xr.DataArray([\"HELLO\", \"WORLD\", \"hELLo\", \"nOOb\"]).astype(dtype)\n\n    result = values.str.lower()\n    expected = xr.DataArray([\"hello\", \"world\", \"hello\", \"noob\"]).astype(dtype)\n    assert_equal(result, expected)\n\n    result = values.str.upper()\n    expected = xr.DataArray([\"HELLO\", \"WORLD\", \"HELLO\", \"NOOB\"]).astype(dtype)\n    assert_equal(result, expected)\n\n\n\n", "def test_split(dtype):\n    values = xr.DataArray([\"hello world\", \"hello,world\", \" hello world!\"].astype(dtype))\n\n    result = values.str.split()\n    expected = xr.DataArray([[\"hello\", \"world\"], [\"hello\", \"world\"], [\"hello\", \"world\", \"\"]])\n    assert_equal(result, expected)\n\n    result = values.str.split(\",\", expand=True)  \n    expected = xr.DataArray([[\"hello\", \"world\"], [\"hello\", \"world\"], [\" hello\", \" world!\"]])\n    assert_equal(result, expected)\n\n    result = values.str.split(\" \", expand=True) \n    expected = xr.DataArray([[\"hello\", \"world\"], [\"hello\", \"world\"], [\"hello\", \"world\", \"\"]])\n    assert_equal(result, expected)\n\n    result = values.str.split(maxsplit=1)\n    expected = xr.DataArray([[\"hello\", \" world\"], [\"hello\", \"world\"], [\"hello\", \" world!\"]])\n    assert_equal(result, expected)\n\n    result = values.str.split(\" \", expand=True, n=2)\n    expected = xr.DataArray([[\"hello\", \"world\"], [\"hello\", \"world\"], [\"hello\", \"world\", \"\"]])\n    assert_equal(result, expected)\n\n\n\n", "compilation error", "def test_istitle(dtype):\n    values = [\"A\", \"a\", \"Bb\", \"AbCd\", \"4\", \"This Is A Test\", \"this is a test\", \"\"]\n    s = xr.DataArray(values).astype(dtype)\n    title_expected = [False, False, False, True, False, True, False, False]\n    assert_equal(s.str.istitle(), xr.DataArray(title_expected))\n", "def test_encode_decode_errors(dtype):\n    data = xr.DataArray([\"a\", \"b\", \"a\\xe4\"])\n    with pytest.raises(UnicodeDecodeError):\n        data.str.encode(\"ascii\").str.decode(\"utf-8\")\n   \n    with pytest.raises(UnicodeEncodeError):\n        xr.DataArray([\"\u4f60\u597d\"]).str.encode(\"ascii\")\n\n\n"], "sample_334": ["    def test_renderer_overrides_default_with_instance(self):\n        custom = CustomRenderer()\n        form = Form(renderer=custom)\n        self.assertEqual(form.renderer, custom)\n", "    def test_renderer_assignment_during_instance_creation(self):\n        custom = CustomRenderer()\n        form = Form(renderer=custom)\n        self.assertEqual(form.renderer, custom)\n\n\n", "    def test_renderer_overrides(self):\n        class CustomForm(Form):\n            default_renderer = DjangoTemplates()\n\n        class CustomRenderer(DjangoTemplates):\n                return 'RENDERED CUSTOM'\n\n        form = CustomForm()\n        form.renderer = CustomRenderer()\n        self.assertEqual(form.as_p(), 'RENDERED CUSTOM')\n", "    def test_renderer_attribute(self):\n        class CustomForm(Form):\n            default_renderer = DjangoTemplates()\n\n        form = CustomForm()\n        self.assertEqual(getattr(form, 'renderer', None), form.default_renderer)\n", "    def test_renderer_is_not_modified_by_form_data(self):\n        initial_renderer = DjangoTemplates()\n        form = Form(renderer=initial_renderer)\n        form.is_bound = True\n        form.data = {'testfield': 'test'}\n        self.assertIs(form.renderer, initial_renderer)\n"], "sample_247": ["    def test_alias_expression_with_extra_select(self):\n        qs = Book.objects.alias(\n            other_rating=F('rating') * 2,\n        ).extra(\n            select={'sum_rating': 'other_rating'}\n        ).values('sum_rating')\n        self.assertIs(hasattr(qs.first(), 'other_rating'), False)\n        self.assertCountEqual(qs, [\n            {'sum_rating': 9.0},\n            {'sum_rating': 8.0},\n            {'sum_rating': 9.0},\n            {'sum_rating': 10.0},\n            {'sum_rating': 10.0}])\n\n\n\n", "    def test_alias_in_subquery(self):\n        qs = Book.objects.annotate(\n            top_rating=Subquery(\n                Book.objects.alias(\n                    rating_alias=F('rating')\n                ).filter(rating_alias__gt=4).values('rating_alias').annotate(rating_sum=Sum('rating_alias')).order_by().values('rating_sum')[0]\n            ),\n        )\n        book = qs.first()\n        self.assertIsNone(hasattr(book, 'rating_alias'))\n        self.assertEqual(book.top_rating, 21.5)\n\n", "    def test_alias_complex_annotate(self):\n        qs = Book.objects.alias(\n            rating_difference=F('rating') - F('pubdate').year,\n            average_rating=Avg('rating'),\n        ).annotate(\n            is_high_rating=Case(\n                When(rating_difference__gt=5, then=Value(True)),\n                default=Value(False),\n            ),\n        )\n        self.assertIs(hasattr(qs.first(), 'rating_difference'), False)\n        self.assertIs(hasattr(qs.first(), 'average_rating'), False)\n        self.assertEqual(qs.count(), Book.objects.count())\n", "    def test_alias_with_related_field(self):\n        qs = Book.objects.alias(\n            publisher_name=F('publisher__name')\n        ).annotate(\n            publisher_name_uppercase=Upper('publisher_name'),\n        )\n        for book in qs:\n            with self.subTest(book=book):\n                self.assertEqual(book.publisher_name_uppercase, book.publisher.name.upper())\n\n", "    def test_annotate_distinct_on_alias(self):\n        qs = Book.objects.alias(rating_alias=F('rating') - 1)\n        self.assertCountEqual(\n            qs.annotate(distinct_rating=CountDistinct('rating_alias')).distinct('rating_alias'),\n            [{'distinct_rating': 1}],\n        )\n"], "sample_997": ["def test_invalid_repeated_decimals():\n    inputs = ['1.[', '1.[1', '1.[1]', '1.[', '1..[1]', '1.[1.', '0.1[1]', '0.1.[1]', '0.1[1.']\n    for text in inputs:\n        raises(TokenError, lambda: parse_expr(text))\n", "def test_implicit_multiplication_with_functions():\n    transformations = standard_transformations + (implicit_multiplication,)\n    x = Symbol('x')\n    f = Function('f')\n\n    assert parse_expr(\"f(x)*y\", transformations=transformations) == f(x) * y\n    assert parse_expr(\"f(x)*y*z\", transformations=transformations) == f(x) * y * z\n", "def test_implicit_multiplication_application():\n    transformations = standard_transformations + \\\n                      (implicit_multiplication_application,)\n    assert parse_expr(\"10sin**2 x**2 + 3xyz\", transformations=transformations) ==  3*x*y*z + 10*sin(x**2)**2\n\n    assert parse_expr(\"cot z + csc z\", transformations=transformations) == cot(z) + csc(z)\n\n", "def test_implicit_multiplication_function_arguments():\n    transformations = standard_transformations + \\\n                      (implicit_multiplication,)\n    x = Symbol('x')\n    y = Symbol('y')\n    f = Function('f')\n    assert parse_expr(\"f(2*x)\", transformations=transformations) == f(2*x)\n    assert parse_expr(\"f(x+y)\", transformations=transformations) == f(x+y)\n", "def test_issue_11150():\n    transformations = standard_transformations + \\\n                      (implicit_multiplication,)\n    assert parse_expr(\"2* sin(x)\", transformations=transformations) == 2*sin(x)\n"], "sample_143": ["    def test_format_lazy(self):\n        with self.subTest(str_format):\n            self.assertEqual(format_lazy('Hello, {name}!') % {'name': 'World'}, 'Hello, World!')\n        with self.subTest(lazy_format):\n            self.assertEqual(format_lazy('Hello, {name}!').format(name='World'), 'Hello, World!')\n", "    def test_format_lazy(self):\n        self.assertEqual(format_lazy(\"string {0} {1}\", \"a\", \"b\"), \"string a b\")\n        self.assertEqual(format_lazy(\"string {0} {1}\", lazystr(\"a\"), \"b\"), \"string a b\")\n        self.assertEqual(format_lazy(\"string {0} {1}\", \"a\", lazystr(\"b\")), \"string a b\")\n        self.assertEqual(format_lazy(\"string {0} {1}\", lazystr(\"a\"), lazystr(\"b\")), \"string a b\")\n", "    def test_format_lazy(self):\n        self.assertEqual(text._format_lazy('{} {} {}'.format(1, 2, 3)), '1 2 3')\n        self.assertEqual(text._format_lazy('{} {} {}'.format(1, 2, 3), 4, 5), '1 2 3 4 5')\n        self.assertEqual(text._format_lazy('{} {} {}'.format(lazystr('a'), lazystr('b'), lazystr('c'))), 'a b c')\n        self.assertEqual(text._format_lazy('{} {} {}'.format(lazystr('a'), lazystr('b'), lazystr('c')), 4, 5), 'a b c 4 5')\n", "    def test_format_lazy_string(self):\n        format_string = \"Hello, {}!\"\n        self.assertEqual(format_lazy(format_string, \"World\"), \"Hello, World!\")\n        self.assertEqual(format_lazy(format_string, lazystr(\"World\")), \"Hello, World!\")\n        self.assertEqual(format_lazy(format_string, \"World\", \"Extra\"), \"Hello, World!Extra\")\n", "    def test_format_lazy(self):\n\n            with self.subTest(format_string=format_string):\n                self.assertEqual(format_lazy(format_string, *args, **kwargs), format_string.format(*args, **kwargs))\n\n        test_format('Hello, {name}!', name='world')\n        test_format('The quick brown fox jumps over the {lazy_num} lazy dog.', lazy_num=lazystr(5))\n        test_format('The {1} {0} over the lazy dog.', 'quick', 'brown')\n        test_format('The {name} fox {action} over the lazy dog.', name='quick', action=lazystr('jumps'))\n        test_format('The {name} fox jumps {action} over the lazy dog.', name='quick', action='slowly')\n\n\n\n"], "sample_93": ["    def test_subquery_select_related(self):\n        class MyCharField(CharField):\n                return value.upper()\n\n        b1 = Book.objects.create(\n            isbn=\"abcde1\", name=\"test\", pages=10, rating=4.0,\n            price=9999.98, contact=self.a1, publisher=self.p1, pubdate=timezone.now())\n        b1.authors.add(self.a3)\n        b2 = Book.objects.create(\n            isbn=\"abcde2\", name=\"test\", pages=10, rating=4.0,\n            price=9999.98, contact=self.a1, publisher=self.p1, pubdate=timezone.now())\n        b2.authors.add(self.a2)\n\n        related_publisher = Publisher.objects.annotate(\n            publisher_name=Subquery(\n                Book.objects.filter(id=OuterRef('book__id')).values_list('publisher__name', flat=True)[0]\n            )\n        ).select_related('book')\n\n        list(related_publisher)\n", "    def test_subquery_annotate_aggregate(self):\n        class MySum(Sum):\n            pass\n\n        # test completely changing how the output is rendered\n            sql, params = compiler.compile(self.source_expressions[0])\n            substitutions = {'function': self.function.lower(), 'expressions': sql, 'distinct': ''}\n            substitutions.update(self.extra)\n            return self.template % substitutions, params\n        setattr(MySum, 'as_' + connection.vendor, lower_case_function_override)\n\n        qs = Book.objects.annotate(\n            sums=MySum(F('rating') + F('pages') + F('price'), output_field=IntegerField())\n        )\n        self.assertEqual(str(qs.query).count('sum('), 1)\n        b1 = qs.get(pk=self.b4.pk)\n        self.assertEqual(b1.sums, 383)\n\n        # test changing the dict and delegating\n            self.extra['function'] = self.function.lower()\n            return super(MySum, self).as_sql(compiler, connection)\n        setattr(MySum, 'as_' + connection.vendor, lower_case_function_super)\n\n        qs = Book.objects.annotate(\n            sums=MySum(F('rating') + F('pages') + F('price'), output_field=IntegerField())\n        )\n        self.assertEqual(str(qs.query).count('sum('), 1)\n        b1 = qs.get(pk=self.b4.pk)\n        self.assertEqual(b1.sums, 383)\n\n        # test overriding all parts of the template\n            substitutions = {'function': 'MAX', 'expressions': '2', 'distinct': ''}\n            substitutions.update(self.extra)\n            return self.template % substitutions, ()\n        setattr(MySum, 'as_' + connection.vendor, be_evil)\n\n        qs = Book.objects.annotate(\n            sums=MySum(", "    def test_annotate_with_f_expression_in_aggregate_function(self):\n        qs = Book.objects.annotate(\n            avg_price_with_discount=Avg(F('price') * (1 - F('discount')))\n        ).values('isbn', 'avg_price_with_discount')\n        self.assertQuerysetEqual(\n            qs, [\n                ('abcde0', Decimal('22.50')),\n                ('abcde1', Decimal('25.00')),\n                ('abcde2', Decimal('26.00')),\n                ('abcde3', Decimal('28.00')),\n                ('abcde4', Decimal('30.00'))\n            ],\n            lambda v: (v[0], v[1])\n        )\n\n", "    def test_aggregate_complex_expression(self):\n        class MyAdd(GenericFunction):\n            template = \"%(function)s(%(expressions)s)\"\n            function = \"ADD\"\n            output_field = DecimalField()\n\n                return self.template % {\n                    'function': self.function,\n                    'expressions': ', '.join(self.get_source_expressions())\n                }\n\n        qs = Book.objects.annotate(\n            total_price_rating=MyAdd(F('price'), F('rating'))\n        ).values('total_price_rating')\n        self.assertEqual(list(qs), [(Decimal(\"29.69\") + Decimal(4))])\n", "    def test_subquery_in_field_lookup(self):\n        class MyModel(models.Model):\n            pass\n\n        # This would normally raise an error, but we're testing the logic\n        with self.assertRaisesMessage(\n            TypeError,\n            'cannot be used in a lookup because it is a QuerySet expression'\n        ):\n            MyModel.objects.filter(pk__in=MyModel.objects.filter(pk=1))\n"], "sample_515": ["compilation error", "compilation error", "def test_colorbar_update():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.random.randn(10, 10))\n    cb = fig.colorbar(pc)\n\n    # modify the colormap\n    new_cmap = 'viridis'\n    cb.set_cmap(new_cmap)\n    assert cb.cmap.name == new_cmap\n\n    # modify the norm\n    new_norm = mcolors.Normalize(vmin=-10, vmax=10)\n    cb.set_norm(new_norm)\n    assert cb.norm.vmin == new_norm.vmin\n    assert cb.norm.vmax == new_norm.vmax\n\n    # modify the ticks\n    new_ticks = [0, 2, 4, 6, 8]\n    cb.set_ticks(new_ticks)\n    assert np.allclose(cb.ax.get_yticks(), new_ticks)\n\n    # modify the tick labels\n    new_ticklabels = ['a', 'b', 'c', 'd', 'e']\n    cb.ax.set_yticklabels(new_ticklabels)\n    assert np.array(cb.ax.get_yticklabels()) == new_ticklabels\n\n    # modify the label\n    new_label = 'My Label'\n    cb.set_label(new_label)\n    assert cb.ax.get_ylabel() == new_label\n\n    # modify the position\n    cb.set_position([0.1, 0.1, 0.8, 0.8])\n    assert np.allclose(cb.ax.get_position(), [0.1, 0.1, 0.8, 0.8])\n\n    # modify the orientation\n    cb.set_orientation('horizontal')\n    assert cb.ax.yaxis.get_visible() is False\n    assert cb.ax.xaxis.get_visible() is True\n\n\n\n", "def test_fraction_param():\n    fig, ax = plt.subplots(1, 2, figsize=(8, 3))\n    pc = ax[0].pcolormesh(np.random.randn(10, 10))\n    cb1 = fig.colorbar(pc, ax=ax[0], fraction=0.05)\n    pc = ax[1].pcolormesh(np.random.randn(10, 10))\n    cb2 = fig.colorbar(pc, ax=ax[1], fraction=0.2)\n\n\n\n", "def test_colorbar_label_location():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh([[1, 2], [3, 4]])\n    cb = fig.colorbar(pc, ax=ax, label='Custom Label', location='left')\n    assert cb.ax.get_xlabel() == 'Custom Label'\n    cb = fig.colorbar(pc, ax=ax, label='Custom Label', location='right')\n    assert cb.ax.get_xlabel() == 'Custom Label'\n    cb = fig.colorbar(pc, ax=ax, label='Custom Label', location='top')\n    assert cb.ax.get_ylabel() == 'Custom Label'\n    cb = fig.colorbar(pc, ax=ax, label='Custom Label', location='bottom')\n    assert cb.ax.get_ylabel() == 'Custom Label'\n\n\n\n"], "sample_227": ["    def test_emptylistfieldfilter_multiple_choices(self):\n        modeladmin = BookAdminWithMultipleEmptyFieldListFilter(Book, site)\n        request = self.request_factory.get('/')\n        request.user = self.alfred\n        changelist = modeladmin.get_changelist_instance(request)\n        filterspec = changelist.get_filters(request)[0][0]\n        self.assertEqual(filterspec.title, 'Author')\n        choices = list(filterspec.choices(changelist))\n        self.assertEqual(len(choices), 4)\n\n        self.assertEqual(choices[0]['display'], 'All')\n        self.assertIs(choices[0]['selected'], True)\n        self.assertEqual(choices[0]['query_string'], '?')\n\n        self.assertEqual(choices[1]['display'], 'Empty')\n        self.assertIs(choices[1]['selected'], False)\n        self.assertEqual(choices[1]['query_string'], '?author__isempty=1')\n\n        self.assertEqual(choices[2]['display'], 'Not empty')\n        self.assertIs(choices[2]['selected'], False)\n        self.assertEqual(choices[2]['query_string'], '?author__isempty=0')\n        \n        self.assertEqual(choices[3]['display'], 'Specific name')\n        self.assertIs(choices[3]['selected'], False)\n        self.assertEqual(choices[3]['query_string'], '?author__first_name__exact=alfred')\n\n", "    def test_emptylistfieldfilter_with_lookup_parameters(self):\n        modeladmin = DepartmentAdminWithEmptyFieldListFilter(Department, site)\n        request = self.request_factory.get('/', {'description__isempty': '1', 'code__startswith': 'D'})\n        request.user = self.alfred\n        changelist = modeladmin.get_changelist_instance(request)\n        queryset = changelist.get_queryset(request)\n        self.assertEqual(queryset.count(), 1)\n        self.assertEqual(queryset.first().description, '')\n\n\n", "    def test_emptylistfieldfilter_with_empty_choice(self):\n        modeladmin = BookAdminWithEmptyFieldListFilter(Book, site)\n        request = self.request_factory.get('/', {})\n        request.user = self.alfred\n        changelist = modeladmin.get_changelist_instance(request)\n        filterspec = changelist.get_filters(request)[0][0]\n        choices = list(filterspec.choices(changelist))\n        self.assertEqual(choices[0]['display'], 'All')\n        self.assertIs(choices[0]['selected'], True)\n        self.assertEqual(choices[0]['query_string'], '?')\n        self.assertEqual(choices[1]['display'], 'Empty')\n        self.assertIs(choices[1]['selected'], False)\n        self.assertEqual(choices[1]['query_string'], '?title__isempty=1')\n\n\n", "    def test_emptylistfieldfilter_with_boolean_field(self):\n        modeladmin = BookAdminWithEmptyFieldListFilter(Book, site)\n\n        request = self.request_factory.get('/')\n        request.user = self.alfred\n        msg = (\n            \"The list filter 'EmptyFieldListFilter' cannot be used with field \"\n            \"'is_best_seller' which cannot accept null values.\"\n        )\n        with self.assertRaisesMessage(ImproperlyConfigured, msg):\n            modeladmin.get_changelist_instance(request)\n\n", "    def test_emptylistfieldfilter_with_lookup_parameter(self):\n        modeladmin = DepartmentAdminWithEmptyFieldListFilter(Department, site)\n        request = self.request_factory.get('/', {'description__isempty': '1'})\n        request.user = self.alfred\n        changelist = modeladmin.get_changelist_instance(request)\n        queryset = changelist.get_queryset(request)\n        self.assertEqual(list(queryset), [Department.objects.get(code='EMPT')])\n\n        request = self.request_factory.get('/', {'description__isempty': '0'})\n        request.user = self.alfred\n        changelist = modeladmin.get_changelist_instance(request)\n        queryset = changelist.get_queryset(request)\n        self.assertEqual(list(queryset), [self.dev, self.design])\n"], "sample_404": ["    def test_conditional_compile_filter(self):\n        engine = self._engine()\n        t = engine.from_string(\"{% if 1 %}{{ foo|my_filter:arg1,arg2 }}{% endif %}\")\n        c = Context({\"foo\": 1})\n        result = t.render(c)\n        self.assertEqual(result, \"1\")\n        try:\n            t = engine.from_string(\"{% if 0 %}{{ foo|my_filter:arg1,arg2 }}{% endif %}\")\n            t.render(c)\n        except TemplateSyntaxError:\n            pass\n", "    def test_filters_with_arguments(self):\n        engine = self._engine()\n        t = engine.from_string(\n            \"{{ 'hello'|upper:'world' }}\"\n        )\n        self.assertEqual(t.render(Context()), \"HELLO WORLD\")\n\n\n\n", "    def test_invalid_filter_expression(self):\n        engine = self._engine()\n        msg = (\n            \"Could not parse some characters: 'a|b' from 'a|b'\"\n        )\n        with self.assertRaisesMessage(TemplateSyntaxError, msg):\n            engine.from_string(\"{{ a|b }}\")\n\n", "    def test_include_template_in_extends(self):\n        \"\"\"\n        #25975 -- Include template in extended blocks should get correct origin\n        \"\"\"\n        engine = self._engine(app_dirs=True)\n        parent = engine.from_string(\"{% block content %}parent{% endblock %}\")\n        child = engine.from_string(\n            \"{% extends parent %}{% block content %}{% include 'included_content.html' %}{% endblock %}\"\n        )\n        for node in child.nodelist:\n            if isinstance(node, TemplateNode):\n                self.assertEqual(node.origin, parent.origin)\n\n\n\n", "    def test_render_with_unicode_decode_error(self):\n        engine = self._engine()\n        template = engine.from_string(\"{{ nonexisting_variable }}\")\n        context = Context({\"nonexisting_variable\": \"\\u0080\\u0081\"})\n        with self.assertRaises(UnicodeDecodeError):\n            template.render(context)\n\n\n\n"], "sample_1153": ["compilation error", "def test_issue_18990():\n    from sympy import integrate, exp\n    x = Symbol('x')\n    assert integrate(exp(I*x), (x, 0, pi)) == (I*exp(I*pi) - I)/I\n", "compilation error", "def test_issue_16229():\n    from sympy import Symbol, Abs, Im, Re, real, imag\n    x = Symbol('x', real=True)\n    y = Symbol('y', complex=True)\n    assert (Im(x + y) == Im(y))\n    assert (Re(x + y) == Re(x))\n    assert (Abs(x + y) == Abs(x + y))\n    assert (real(x + y) == real(x) + Re(y))\n    assert (imag(x + y) == Im(y))\n", "def test_issue_15737():\n    x = Symbol('x', real=True)\n    y = Symbol('y', imaginary=True)\n    f = Function('f')\n    assert (f(x)**2).conjugate() == f(x)**2\n    assert (f(y)**2).conjugate() == f(y)**2\n"], "sample_195": ["    def test_sql_flush_truncates_cascade(self):\n        Author.objects.create(name='Author 1')\n        Book.objects.create(title='Book 1', author_id=1)\n        Book.objects.create(title='Book 2', author_id=1)\n\n        with self.subTest('No cascade'):\n            tables = [Author._meta.db_table]\n            sql = connection.ops.sql_flush(no_style(), tables, allow_cascade=False)\n            self.assertGreaterEqual(len(sql), 1)\n            self.assertEqual(sql[0].count('DELETE FROM sqlite_sequence'), 1)\n            self.assertEqual(sql[0].count('DELETE FROM \"auth_user\"'), 0)\n            self.assertEqual(sql[0].count('DELETE FROM \"auth_group\"'), 0)\n\n        with self.subTest('Cascade'):\n            tables = [Author._meta.db_table]\n            sql = connection.ops.sql_flush(no_style(), tables, allow_cascade=True)\n            self.assertGreaterEqual(len(sql), 3)\n            self.assertEqual(sql[0].count('DELETE FROM sqlite_sequence'), 1)\n            self.assertEqual(sql[1].count('DELETE FROM \"auth_user\"'), 1)\n            self.assertEqual(sql[2].count('DELETE FROM \"auth_group\"'), 1)\n\n\n\n", "    def test_sql_flush_with_cascade(self):\n        Author.objects.create(name='Author A')\n        Book.objects.create(title='Book A', author_id=1)\n        Book.objects.create(title='Book B', author_id=1)\n        tables = ['authors', 'books']\n        with self.assertNumQueries(2):\n            connection.ops.sql_flush(no_style(), tables, allow_cascade=True)\n        self.assertFalse(Author.objects.exists())\n        self.assertFalse(Book.objects.exists())\n\n\n\n", "    def test_sql_flush_with_tables(self):\n        with self.assertRaises(ValueError) as context:\n            connection.ops.sql_flush(no_style(), [Author._meta.db_table, Book._meta.db_table])\n        self.assertIn(\"ALLOW_CASCADE\", str(context.exception))\n        self.assertIn(\"TRUNCATE\", str(context.exception))\n        with transaction.atomic():\n            Author.objects.create(name='test_author')\n            Book.objects.create(title='test_book', author_id=1)\n        self.assertEqual(connection.ops.sql_flush(no_style(), [Author._meta.db_table, Book._meta.db_table]), [\n            'DELETE FROM \"Author\";',\n            'DELETE FROM \"Book\";',\n        ])\n\n\n\n", "    def test_sql_flush_with_tables(self):\n        Author.objects.create(name='John Doe')\n\n        self.assertGreater(len(connection.ops.sql_flush(no_style(), ['django_test_app_author'])), 0)\n", "    def test_sql_flush_with_tables(self):\n        with self.subTest('without cascade'):\n            Author.objects.create(name=\"Author 1\")\n            Book.objects.create(title=\"Book 1\", author=Author.objects.get(name=\"Author 1\"))\n            sql = connection.ops.sql_flush(no_style(), ['django_test_author', 'django_test_book'], allow_cascade=False)\n            self.assertIn('DELETE FROM django_test_book WHERE django_test_book.author_id = (SELECT id FROM django_test_author)', sql)\n            self.assertIn('DELETE FROM django_test_author', sql)\n        with self.subTest('with cascade'):\n            Author.objects.create(name=\"Author 1\")\n            Book.objects.create(title=\"Book 1\", author=Author.objects.get(name=\"Author 1\"))\n            sql = connection.ops.sql_flush(no_style(), ['django_test_author', 'django_test_book'], allow_cascade=True)\n            self.assertIn('DELETE FROM django_test_book', sql)\n            self.assertIn('DELETE FROM django_test_author', sql)\n"], "sample_304": ["    def test_domain_allowlist_deprecation(self):\n        msg = \"The domain_allowlist attribute is deprecated in favor of domain_whitelist.\"\n        with self.assertRaisesMessage(RemovedInDjango41Warning, msg):\n            validator = EmailValidator()\n            validator.domain_allowlist = ['mydomain']\n", "    def test_domain_whitelist_setting(self):\n        msg = (\n            'The domain_whitelist attribute is deprecated in favor of '\n            'domain_allowlist.'\n        )\n        validator = EmailValidator(domain_whitelist=['mydomain'])\n        with self.assertRaisesMessage(RemovedInDjango41Warning, msg):\n            validator.domain_whitelist\n", "    def test_domain_allowlist_access(self):\n        validator = EmailValidator()\n        validator.domain_allowlist = ['mydomain']\n        self.assertEqual(validator.domain_allowlist, validator.domain_whitelist)\n        with self.assertRaisesMessage(RemovedInDjango41Warning,\n                                    \"The domain_whitelist attribute is deprecated in favor of domain_allowlist.\"):\n            validator.domain_whitelist\n\n\n\n", "    def test_domain_allowlist(self):\n        validator = EmailValidator()\n        validator.domain_allowlist = ['mydomain']\n        self.assertEqual(validator.domain_allowlist, validator.domain_whitelist)\n        self.assertIsNone(validator('email@mydomain'))\n        self.assertEqual(validator.domain_allowlist, ['mydomain'])\n        with self.assertRaises(ValidationError):\n            validator('email@wrongdomain')\n", "    def test_domain_allowlist_warning(self):\n        with self.assertRaisesMessage(RemovedInDjango41Warning, \"The `domain_allowlist` attribute is deprecated. Use `domain_whitelist` instead.\"):\n            EmailValidator().domain_allowlist = ['mydomain'] \n    "], "sample_1198": ["def test_missing_parentheses():\n    parser = MathematicaParser()\n    assert parser._from_tokens_to_fullformlist(parser._from_mathematica_to_tokens(\"Sin[x]+Cos[y]\")) == [\"Plus\", \"Sin\", \"x\", \"Cos\", \"y\"]\n    assert parser._from_tokens_to_fullformlist(parser._from_mathematica_to_tokens(\"a + b * c\")) == [\"Plus\", \"a\", [\"Times\", \"b\", \"c\"]]\n    assert parser._from_tokens_to_fullformlist(parser._from_mathematica_to_tokens(\"a + b / c\")) == [\"Plus\", \"a\", [\"Power\", \"b\", \"-1\"], \"c\"]\n    assert parser._from_tokens_to_fullformlist(parser._from_mathematica_to_tokens(\"Log[x + y]\")) == [\"Log\", [\"Plus\", \"x\", \"y\"]]\n    assert parser._from_tokens_to_fullformlist(parser._from_mathematica_to_tokens(\"Sqrt[x^2 + y^2]\")) == [\"Sqrt\", [\"Plus\", [\"Power\", \"x\", \"2\"], [\"Power\", \"y\", \"2\"]]]\n    assert parser._from_tokens_to_fullformlist(parser._from_mathematica_to_tokens(\"x + y^2\")) == [\"Plus\", \"x\", [\"Power\", \"y\", \"2\"]]\n\n", "def test_mathematica_with_symbol():\n  d = {\n    'x + y': \"x + y\",\n    'x^2': \"x**2\",\n    'Sin[x]': \"sin(x)\",\n    'Cos[x]': \"cos(x)\",\n    'Tan[x]': \"tan(x)\",\n    'x [y]': \"x[y]\",\n    \"x/y\": \"x/y\",\n    'Sqrt[x]': \"sqrt(x)\",\n    'Log[x,y]': \"log(x,y)\",\n    '1 + 2*x': \"1 + 2*x\",\n    'sin(x) + cos(y)': \"sin(x) + cos(y)\",\n    'a*b/(c+d)': \"a*b/(c+d)\",\n    'a*b^c': \"a*b**c\",\n    'a^2 + b*c': \"a**2 + b*c\",\n    '1/sin(x)': \"1/sin(x)\",\n    'x(y+z)': \"x*(y+z)\",\n    '2*x*y': \"2*x*y\",\n    'log(x,y)': \"log(x,y)\",\n    'x[", "def test_mathematica_string_conversion():\n    parser = MathematicaParser()\n\n    # Test strings with Mathematica syntax\n    assert parse_mathematica(\"Print[\\\"Hello, World!\\\"]\") == Print(\"Hello, World!\")\n    assert parse_mathematica(\"StringJoin[\\\"Hello\\\", \\\" \\\" ,\\\"World!\\\" ]\") == Join(\"Hello\", \" \", \"World!\")\n    assert parse_mathematica(\"ToString[3.14159]\") == Str(3.14159)\n    assert parse_mathematica(\"StringReplaceAll[\\\"Hello, World!\\\", \\\"\\\\(\\\\)$\\\", \\\"\\\"]\") == \"Hello, World!\"\n    assert parse_mathematica(\"StringReplaceAll[\\\"Hello, \\\\(World\\\\)\\!\", \\\"\\\\(\\\\)\\\", \\\"\\\"]\") == \"Hello, World!\"\n\n\n\n", "def test_mathematica_unicode_symbols():\n    parser = MathematicaParser()\n    for e, expected in {\n        \"\u03b1\": [\"GreekLetterAlpha\"],\n        \"\u03b2\": [\"GreekLetterBeta\"],\n        \"\u03b3\": [\"GreekLetterGamma\"],\n        \"\u03b4\": [\"GreekLetterDelta\"],\n        \"\u03b5\": [\"GreekLetterEpsilon\"],\n        \"\u03b6\": [\"GreekLetterZeta\"],\n        \"\u03b7\": [\"GreekLetterEta\"],\n        \"\u03b8\": [\"GreekLetterTheta\"],\n        \"\u03b9\": [\"GreekLetterIota\"],\n        \"\u03ba\": [\"GreekLetterKappa\"],\n        \"\u03bb\": [\"GreekLetterLambda\"],\n        \"\u03bc\": [\"GreekLetterMu\"],\n        \"\u03bd\": [\"GreekLetterNu\"],\n        \"\u03be\": [\"GreekLetterXi\"],\n        \"\u03bf\": [\"GreekLetterOmicron\"],\n        \"\u03c0\": [\"GreekLetterPi\"],\n        \"\u03c1\": [\"GreekLetterRho\"],\n        \"\u03c3\": [\"GreekLetterSigma\"],\n        \"\u03c4\": [\"GreekLetterTau\"],\n        \"\u03c5\": [\"GreekLetterUpsilon\"],\n        \"\u03c6\": [\"GreekLetterPhi\"],\n        \"\u03c7\": [\"GreekLetterChi\"],\n        \"\u03c8\": [\"GreekLetterPsi\"],\n        \"\u03c9\": [\"GreekLetterOmega\"],\n        \"\u221e\": [\"Infinity\"],\n        \"\u2202\": [\"PartialD\"],\n        \"\u2207\": [\"Nabla\"],\n        \"\u2208\": [\"Element\"],\n        \"\u2209\": [\"NotElement\"],\n        \"\u2286\": [\"Subset\"],\n        \"\u2287\": [\"Superset\"],\n        \"\u2282\": [\"Subset\"],\n        \"\u2283\": [\"Superset\"],\n        \"\u2234\": [\"Therefore\"],\n        \"\u2235\": [\"Because\"],\n        \"\u2200\": [\"ForAll\"],\n        \"\u2203\": [\"Exists\"],\n        \"\u00ac\": [\"Not\"],\n        \"\u2192\": [\"Implies\"],\n        \"\u2227\": [\"And\"],\n        \"\u2228\": [\"Or\"],\n        \"\u21d2\": [\"Implies\"],\n        \"\u21d4\": [\"IfAndOnlyIf\"],\n    }.items():\n        assert parser._from_mathematica_to_tokens(e) == expected\n\n\n\n", "compilation error"], "sample_748": ["def test_refit_multioutput_data():\n    # Test refit with multi-output estimator\n\n    X, y = make_multilabel_classification(return_indicator=True,\n                random_state=0)\n\n    est_parameters = {\"max_depth\": [1, 2, 3, 4]}\n    cv = StratifiedKFold(random_state=0)\n\n    estimators = [\n        MultiOutputClassifier(\n            Estimator=DecisionTreeRegressor(random_state=0), n_jobs=-1),\n        MultiOutputClassifier(\n            Estimator=DecisionTreeClassifier(random_state=0), n_jobs=-1),\n    ]\n\n    # Test with grid search cv\n    for est in estimators:\n        grid_search = GridSearchCV(est, est_parameters, cv=cv, refit=True)\n        grid_search.fit(X, y)\n\n        res_params = grid_search.cv_results_['params']\n        for cand_i in range(len(res_params)):\n            est.set_params(**res_params[cand_i])\n            for i, (train, test) in enumerate(cv.split(X, y)):\n                est.fit(X[train], y[train])\n            y_pred = est.predict(X)\n            assert_array_almost_equal(y_pred, grid_search.best_estimator_.predict(X))\n\n    # Test with a randomized search\n    for est in estimators:\n        random_search = RandomizedSearchCV(est, est_parameters,\n                cv=cv, n_iter=3, refit=True)\n        random_search.fit(X, y)\n\n        res_params = random_search.cv_results_['params']\n        for cand_i in range(len(res_params)):\n            est.set_params(**res_params[cand_i])\n            for i, (train, test) in enumerate(cv.split(X, y)):\n                est.fit(X[train], y[train])\n            y_pred = est.predict(X)\n            assert_array_almost_equal(y_pred, random_search.best_estimator_.predict(X))\n\n\n\n", "def test_scorer_names():\n    # test that scorer names are correctly set\n    X, y = make_classification(random_state=0)\n\n    # test with make_scorer\n    scorer_name = 'my_scorer'\n    scorer = make_scorer(lambda predicted, true: np.mean(predicted == true),\n                         needs_proba=False,\n                         greater_is_better=True)\n    gs = GridSearchCV(SVC(), {'C': [0.1, 0.2]}, scoring=scorer)\n    assert_equal(gs.scorer_, {scorer_name: scorer})\n    gs.fit(X, y)\n    assert_equal(gs.cv_results_['mean_test_score'].dtype, np.float64)\n\n    # test with callable scorer\n        return np.mean(y_true == y_pred)\n\n    gs = GridSearchCV(SVC(), {'C': [0.1, 0.2]}, scoring=my_scorer)\n    assert_equal(gs.scorer_, {'my_scorer': my_scorer})\n    gs.fit(X, y)\n    assert_equal(gs.cv_results_['mean_test_score'].dtype, np.float64)\n\n    # test with string scorer\n    gs = GridSearchCV(SVC(), {'C': [0.1, 0.2]}, scoring='accuracy')\n    assert_equal(gs.scorer_, {'accuracy': make_scorer(accuracy_score)})\n    gs.fit(X, y)\n    assert_equal(gs.cv_results_['mean_test_score'].dtype, np.float64)\n\n\n\n", "def test_estimator_with_refit_param_error():\n    from sklearn.base import BaseEstimator\n\n    class MyEstimator(BaseEstimator):\n            self.param = param\n            return self\n            return X\n\n    estimator = MyEstimator(param=0)\n    grid_search = GridSearchCV(estimator, param_grid={'param': [1, 2, 3]}, refit=True)\n    with pytest.raises(ValueError):\n        grid_search.fit(X, y) \n", "compilation error", "        def fit(self, X, y=None):\n            pass\n"], "sample_447": ["    def test_annotation_and_alias_sql_injection(self):\n        crafted_alias = \"\"\"injected_name\" from \"annotations_book\"; --\"\"\"\n        msg = (\n            \"Column aliases cannot contain whitespace characters, quotation marks, \"\n            \"semicolons, or SQL comments.\"\n        )\n        with self.assertRaisesMessage(ValueError, msg):\n            Book.objects.annotate(**{crafted_alias: Value(1)})\n", "    def test_alias_with_nested_annotations(self):\n        qs = Book.objects.alias(\n            nested_rating=F(\"rating\") * 2,\n        ).annotate(\n            nested_rating_sum=Sum(F(\"nested_rating\")),\n            avg_rating=Avg(\"rating\"),\n        )\n        self.assertIs(hasattr(qs.first(), \"nested_rating\"), False)\n        for book in qs:\n            with self.subTest(book=book):\n                self.assertEqual(book.nested_rating_sum, 4 * book.rating)\n                self.assertEqual(book.avg_rating, book.rating)\n", "    def test_alias_with_case(self):\n        qs = Book.objects.alias(\n            condition=Case(\n                When(rating__gt=4, then=Value(True)),\n                default=Value(False),\n            ),\n        ).annotate(\n            is_highly_rated=F(\"condition\"),\n        )\n        for book in qs:\n            with self.subTest(book=book):\n                self.assertTrue(book.is_highly_rated if book.rating > 4 else not book.is_highly_rated)\n", "    def test_alias_with_subquery(self):\n        qs = (\n            Book.objects.alias(\n                top_rating_year=Subquery(\n                    Book.objects.filter(pubdate__year=OuterRef(\"pubdate__year\"))\n                    .order_by(\"-rating\")\n                    .values(\"rating\")[:1]\n                ),\n            )\n            .values(\"pubdate__year\", \"top_rating_year\")\n        )\n        self.assertCountEqual(\n            qs,\n            [\n                {\"pubdate__year\": 1991, \"top_rating_year\": 5.0},\n                {\"pubdate__year\": 1995, \"top_rating_year\": 4.0},\n                {\"pubdate__year\": 2007, \"top_rating_year\": 4.5},\n                {\"pubdate__year\": 2008, \"top_rating_year\": 4.0},\n                {\"pubdate__year\": 2008, \"top_rating_year\": 4.0},\n                {\"pubdate__year\": 2008, \"top_rating_year\": 4.0},\n            ],\n        )\n", "    def test_alias_with_nested_subquery(self):\n        qs = Book.objects.alias(\n            top_rating_year=Subquery(\n                Book.objects.filter(pubdate__year=OuterRef(\"pubdate__year\"))\n                .order_by(\"-rating\")\n                .values(\"rating\")[:1]\n            ),\n        )\n        self.assertCountEqual(\n            qs.values(\"pubdate__year\", \"top_rating_year\"),\n            [\n                {\"pubdate__year\": 1991, \"top_rating_year\": 5.0},\n                {\"pubdate__year\": 1995, \"top_rating_year\": 4.0},\n                {\"pubdate__year\": 2007, \"top_rating_year\": 4.5},\n                {\"pubdate__year\": 2008, \"top_rating_year\": 4.0},\n            ],\n        )\n"], "sample_721": ["    def test_check_array_dtype_object():\n        X = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n        X_object = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n        for i in range(len(X_object)):\n            X_object[i] = [object() for _ in range(len(X[0]))]\n        X_object_dt = np.array(X_object, dtype=object)\n        # Test with default dtype\n        assert_equal(check_array(X_object_dt).dtype, X_object_dt.dtype)\n\n        # Test with a specified dtype (should not change the dtype)\n        assert_equal(check_array(X_object_dt, dtype=np.float64).dtype,\n                     object)\n\n\n\n\n        \n", "    def test_check_array_dtype_array_like():\n        # Test that when `dtype` is specified as a numpy dtype, \n        # it is correctly applied to array-like inputs\n        X = np.array([1, 2, 3])\n        y = np.array([4, 5, 6], dtype=np.float64)\n\n        # Test with a single array-like input\n        X_checked = check_array(X, dtype=np.int32)\n        assert_equal(X_checked.dtype, np.int32)\n\n        # Test with a list of array-like inputs\n        X_list = [X, y]\n        X_checked_list = check_array(X_list, dtype=np.int32)\n        assert_equal(X_checked_list[0].dtype, np.int32)\n        assert_equal(X_checked_list[1].dtype, np.int32)\n\n        # Test with a tuple of array-like inputs\n        X_tuple = (X, y)\n        X_checked_tuple = check_array(X_tuple, dtype=np.int32)\n        assert_equal(X_checked_tuple[0].dtype, np.int32)\n        assert_equal(X_checked_tuple[1].dtype, np.int32)\n        # Test with empty array-like inputs\n        X_empty = np.array([])\n        assert_equal(check_array(X_empty, dtype=np.int32).shape, ())\n\n\n\n", "    def test_check_array_float_and_int_dtypes():\n        X_int = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=np.int64)\n        X_float = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]], dtype=np.float64)\n        X_mixed = np.array([[1, 2.0, 3], [4.0, 5, 6.0], [7, 8.0, 9]], dtype=object)\n\n        assert_equal(check_array(X_int).dtype, np.int64)\n        assert_equal(check_array(X_float).dtype, np.float64)\n        assert_equal(check_array(X_mixed).dtype, np.float64)  # object dtype is converted to float64\n", "    def test_check_array_dtype_warning_sparse():\n        X_csr = sp.csr_matrix([[1, 2], [3, 4]])\n        X_float64 = X_csr.toarray()\n        X_float32 = X_csr.toarray().astype(np.float32)\n        X_int64 = X_csr.toarray().astype(np.int64)\n\n        # Test warning without copy\n        X_checked = assert_no_warnings(check_array, X_csr, dtype=np.float64,\n                                       accept_sparse=True)\n        assert_equal(X_checked.dtype, np.float64)\n        assert_equal(X_checked.format, 'csr')\n\n        # Test warning with copy\n        X_checked = assert_no_warnings(check_array, X_csr, dtype=np.float64,\n                                       accept_sparse=True, copy=True)\n        assert_equal(X_checked.dtype, np.float64)\n        assert_not_equal(X_checked, X_csr)\n        assert_equal(X_checked.format, 'csr')\n\n        # Test with other dtypes\n        X_checked = assert_no_warnings(check_array, X_csr, dtype=np.float32,\n                                       accept_sparse=True)\n        assert_equal(X_checked.dtype, np.float32)\n        assert_equal(X_checked.format, 'csr')\n\n        # Test with warning\n        X_checked, y_checked = assert_warns_message(\n            DataConversionWarning, 'SomeEstimator',\n            check_X_y, X_csr, np.zeros(2), dtype=np.float64,\n            accept_sparse=True, warn_on_dtype=True, estimator=KNeighborsClassifier())\n        assert_equal(X_checked.dtype, np.float64)\n        assert_equal(X_checked.format, 'csr')\n\n\n", "    def test_check_array_nan_with_sparse():\n        X = sp.csr_matrix([[1, 2], [np.nan, 4]])\n        assert_warns(UserWarning, check_array, X, force_all_finite=True,\n                     accept_sparse=True)\n        assert_array_almost_equal(as_float_array(X, force_all_finite=True),\n                                   np.array([[1, 2], [float('nan'), 4]]))\n"], "sample_965": ["compilation error", "compilation error", "def test_getsourcelines_module():\n    source = inspect.getsourcelines(inspect)\n    assert source != ([], [])\n\n\n\n\n", "compilation error", "compilation error"], "sample_448": ["    def test_expressions_with_conditions(self):\n        msg = (\n            \"UniqueConstraint.condition cannot be used with expressions. Use \"\n            \"django.db.models.Q objects instead.\"\n        )\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                Lower(\"field\"),\n                name=\"test_func_condition\",\n                condition=models.Q(field='value'),\n            )\n", "    def test_constraint_violation_message(self):\n        with self.assertRaisesMessage(\n            ValidationError,\n            \"Constraint \u201cname_color_uniq\u201d is violated.\",\n        ):\n            UniqueConstraintProduct.objects.create(name=self.p1.name, color=self.p1.color)\n\n        with self.assertRaisesMessage(\n            ValidationError,\n            \"Constraint \u201cname_lower_uniq\u201d is violated.\",\n        ):\n            UniqueConstraintProduct.objects.create(name=self.p1.name.upper())\n\n\n\n", "    def test_expressions_with_deferrable(self):\n        msg = \"UniqueConstraint with expressions cannot be deferred.\"\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                Lower(\"field\"),\n                name=\"deferred_expression_unique\",\n                deferrable=models.Deferrable.DEFERRED,\n            )\n", "    def test_expressions_with_include(self):\n        msg = (\n            \"UniqueConstraint.include cannot be used with expressions. \"\n            \"Use django.contrib.postgres.indexes.OpClass() instead.\"\n        )\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                Lower(\"field\"), include=[\"other_field\"], name=\"test_func_include\"\n            )\n", "    def test_expressions_with_condition(self):\n        msg = (\n            \"UniqueConstraint.condition cannot be used with expressions.\"\n        )\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                Lower(\"field\"),\n                name=\"test_func_condition\",\n                condition=models.Q(field=models.F(\"other_field\")),\n            )\n"], "sample_728": ["def test_make_checkerboard_with_noise():\n    X, rows, cols = make_checkerboard(shape=(100, 100), n_clusters=(20, 5),\n                                    noise=0.1, shuffle=True, random_state=0)\n    assert_equal(X.shape, (100, 100), \"X shape mismatch\")\n    assert_equal(rows.shape, (100, 100), \"rows shape mismatch\")\n    assert_equal(cols.shape, (100, 100,), \"columns shape mismatch\")\n\n    for i in range(X.shape[0]):\n        for j in range(X.shape[1]):\n            assert_array_almost_equal(X[i, j], np.random.uniform(0.8, 1.2),\n                                      decimal=1)\n", "def test_make_checkerboard_cluster_sizes():\n    for n_clusters in [(2, 5), (3, 4), (10, 1)]:\n        X, _, _ = make_checkerboard(shape=(100, 100), n_clusters=n_clusters,\n                                    shuffle=True, random_state=0)\n        assert_equal(np.unique(X.shape).shape, (1,), \"Expected a 1D array for shape\")\n        assert_equal(X.shape[0], X.shape[1], \"Shape should be square\")\n\n\n\n", "compilation error", "def test_make_checkerboard_large_scale():\n    shape = (1000, 1000)\n    n_clusters = (50, 10)\n    X, rows, cols = make_checkerboard(\n        shape=shape, n_clusters=n_clusters, shuffle=True, random_state=0)\n    assert_equal(X.shape, shape, \"X shape mismatch\")\n    assert_equal(rows.shape, shape, \"rows shape mismatch\")\n    assert_equal(cols.shape, shape, \"columns shape mismatch\")\n    assert_all_finite(X)\n    assert_all_finite(rows)\n    assert_all_finite(cols)\n", "def test_make_checkerboard_error():\n    assert_raises(ValueError, make_checkerboard, shape=(100, 100),\n                  n_clusters=\"invalid\")\n"], "sample_163": ["    def test_user_change_password_view_permissions(self):\n        u = User.objects.get(email=\"staffmember@example.com\")\n        self.client.post(\n            reverse(\"auth_test_admin:auth_user_password_change\", args=(u.pk,)),\n            {\n                \"password1\": \"password1\",\n                \"password2\": \"password1\",\n            },\n        )\n        # Check if only users with permission to change password can access it.\n        u.user_permissions.add(get_perm(User, \"change_password\"))\n        self.login()\n        response = self.client.get(\n            reverse(\"auth_test_admin:auth_user_password_change\", args=(u.pk,))\n        )\n        self.assertEqual(response.status_code, 200)\n\n        u.user_permissions.remove(get_perm(User, \"change_password\"))\n        response = self.client.get(\n            reverse(\"auth_test_admin:auth_user_password_change\", args=(u.pk,))\n        )\n        self.assertEqual(response.status_code, 403)\n", "    def test_user_change_password_with_blank_passwords(self):\n        url = reverse(\n            \"auth_test_admin:auth_user_password_change\", args=(self.admin.pk,)\n        )\n        response = self.client.post(\n            url, {\"password1\": \"\", \"password2\": \"\"}\n        )\n        self.assertContains(response, \"Password cannot be blank.\")\n\n", "    def test_user_change_password_with_no_permission(self):\n        u = User.objects.get(email=\"staffmember@example.com\")\n        response = self.client.post(\n            reverse(\"auth_test_admin:auth_user_password_change\", args=(u.pk,)),\n            {\n                \"password1\": \"password1\",\n                \"password2\": \"password1\",\n            },\n        )\n        self.assertEqual(response.status_code, 403)\n", "    def test_user_change_password_blank_password(self):\n        data = self.get_user_data(self.admin)\n        data[\"password1\"] = \"\"\n        data[\"password2\"] = \"\"\n        response = self.client.post(\n            reverse(\"auth_test_admin:auth_user_password_change\", args=(self.admin.pk,)),\n            data,\n        )\n        self.assertContains(response, \"Password\")\n        self.assertEqual(response.status_code, 200)\n        self.logout()\n        self.login()\n\n", "    def test_user_change_password_requires_permission(self):\n        u = User.objects.get(email=\"staffmember@example.com\")\n        self.login()\n        with self.assertLoginRequired():\n            response = self.client.post(\n                reverse(\"auth_test_admin:auth_user_password_change\", args=(u.pk,)),\n                {\n                    \"password1\": \"password1\",\n                    \"password2\": \"password1\",\n                },\n            )\n        self.assertEqual(response.status_code, 302)\n        self.assertRedirects(response, reverse(\"auth_test_admin:auth_user_changelist\"))\n\n        # Now, make the user a regular user (not superuser) so they should not have permission.\n        u.is_superuser = False\n        u.save()\n        self.login()\n        response = self.client.post(\n            reverse(\"auth_test_admin:auth_user_password_change\", args=(u.pk,)),\n            {\n                \"password1\": \"password1\",\n                \"password2\": \"password1\",\n            },\n        )\n\n        # Make sure the user is redirected to the permission denied page\n        self.assertEqual(response.status_code, 403)\n\n\n"], "sample_491": ["    def test_form_bound_field_validation_errors(self):\n        class PersonForm(Form):\n            first_name = CharField(required=True)\n\n        data = {\"first_name\": \"\"}\n        form = PersonForm(data, auto_id=False)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors[\"first_name\"][0], \"This field is required.\")\n\n\n\n", "    def test_helper(self):\n        class PersonForm(Form):\n            first_name = CharField()\n\n        class CustomRenderer(DjangoTemplates):\n            form_template_name = \"forms_tests/custom_helper.html\"\n            field_template_name = \"forms_tests/custom_field.html\"\n\n        form = PersonForm(renderer=CustomRenderer())\n        self.assertHTMLEqual(form.as_p(), '<div>First name: <input type=\"text\" name=\"first_name\" required id=\"id_first_name\"></div>')\n\n\n\n", "    def test_custom_field_template_with_field_errors(self):\n        class MyForm(Form):\n            first_name = CharField()\n\n        f = MyForm({\"first_name\": \"This is too long!\"})\n        t = Template(\"{{ form.first_name.as_field_group }}\")\n        html = t.render(Context({\"form\": f}))\n        expected = \"\"\"\n        <label for=\"id_first_name\">First name:</label>\n        <p>Custom Field<p>\n        <input type=\"text\" name=\"first_name\" value=\"This is too long!\" required id=\"id_first_name\"><ul class=\"errorlist\"><li>Ensure this value has at most 10 characters (it has 17).</li></ul>\n        \"\"\"\n        self.assertHTMLEqual(html, expected)\n", "    def test_custom_renderer_with_context_data(self):\n        class CustomRenderer(DjangoTemplates):\n                context[\"custom_data\"] = \"This data is added by the custom renderer\"\n                return super().render(context, **kwargs)\n\n        @override_settings(FORM_RENDERER=\"forms_tests.tests.test_forms.CustomRenderer\")\n            form = Person()\n            return render(request, \"forms_tests/form_snippet.html\", {\"form\": form})\n\n        response = Client().get(\"/test/\")\n        self.assertContains(response, \"This data is added by the custom renderer\")\n", "    def test_custom_field_render_no_template_name(self):\n        class MyForm(Form):\n            first_name = CharField()\n\n        f = MyForm()\n\n        with self.assertRaises(ValueError):\n            f[\"first_name\"].render(template_name=None)\n\n\n"], "sample_1084": ["def test_issue_19046():\n    assert (ImageSet(Lambda(x, x**2), S.Complexes) == ImageSet(Lambda(x, x**2), S.Reals))\n    assert (ImageSet(Lambda(x, x**2), S.Naturals).is_subset(S.PositiveReals))\n\n\n\n", "compilation error", "compilation error", "    def test_imageset_rational_domain():\n        assert imageset(Lambda(x, x**2), S.Rationals) == S.Rationals\n        assert imageset(Lambda(x, x + 1), S.Rationals) == S.Rationals\n        assert imageset(Lambda(x, 1/x), S.Rationals) == S.Rationals.exclude(0)\n        assert imageset(Lambda(x, sin(x)), S.Rationals) != S.Rationals\n        assert imageset(Lambda(x, exp(x)), S.Rationals) != S.Rationals\n\n\n\n", "def test_issue_17007():\n    assert ImageSet(Lambda(x, x**2), S.Naturals) == ImageSet(Lambda(x, x**2), S.Integers)\n"], "sample_725": ["    def test_check_array_allow_nd_and_ensure_2d():\n        X = np.ones((10, 0, 28, 28))\n        y = np.ones(10)\n        X_checked, y_checked = check_X_y(X, y, allow_nd=True, ensure_2d=True)\n        assert_array_equal(X.shape, X_checked.shape)\n        assert_array_equal(y, y_checked)\n        X_checked, y_checked = check_X_y(X, y, allow_nd=False, ensure_2d=True)\n        assert_array_equal(X.shape, X_checked.shape)\n        assert_array_equal(y, y_checked)\n\n        X = np.ones((10, 0, 28, 28, 10))\n        y = np.ones(10)\n        X_checked, y_checked = check_X_y(X, y, allow_nd=True, ensure_2d=True)\n        assert_array_equal(X.shape, X_checked.shape)\n        assert_array_equal(y, y_checked)\n\n", "    def test_check_array_dtype_warning():\n        X_int_list = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n        X_float64 = np.asarray(X_int_list, dtype=np.float64)\n        X_float32 = np.asarray(X_int_list, dtype=np.float32)\n        X_int64 = np.asarray(X_int_list, dtype=np.int64)\n        integer_data = [X_int64, X_csr_int32]\n        float64_data = [X_float64, X_csr_float64]\n        float32_data = [X_float32, X_csr_float32, X_csc_float32]\n        for X in integer_data:\n            X_checked = assert_no_warnings(check_array, X, dtype=np.float64,\n                                           accept_sparse=True, warn_on_dtype=True)\n            assert_equal(X_checked.dtype, np.float64)\n\n            X_checked = assert_warns(DataConversionWarning, check_array, X,\n                                     dtype=np.float64,\n                                     accept_sparse=True, warn_on_dtype=True)\n            assert_equal(X_checked.dtype, np.float64)\n\n            # Check that the warning message includes the name of the Estimator\n            X_checked = assert_warns_message(DataConversionWarning,\n                                            'SomeEstimator',\n                                            check_array, X,\n                                            dtype=[np.float64, np.float32],\n                                            accept_sparse=True,\n                                            warn_on_dtype=True,\n                                            estimator='SomeEstimator')\n            assert_equal(X_checked.dtype, np.float64)\n\n            X_checked, y_checked = assert_warns_message(\n                DataConversionWarning, 'KNeighborsClassifier',\n                check_X_y, X, y, dtype=np.float64, accept_sparse=True,\n                warn_on_dtype", "    def test_check_array_with_mixed_dtypes():\n        X = np.array([[1, 2.0, 'a'], [4, 5.0, 'b']], dtype=object)\n        assert_true(check_array(X, dtype=object, accept_sparse=True, force_all_finite=False).dtype.kind == 'object')\n        assert_true(check_array(X, dtype=object, accept_sparse=True, force_all_finite=False).dtype.kind == 'object')\n        assert_true(check_array(X, dtype=np.array([np.int64, np.float64, str]).dtype,\n                              accept_sparse=True, force_all_finite=False).dtype.kind == 'object')\n        assert_raises_regex(TypeError, \"can't convert .* to dtype *\", check_array, X,\n                            dtype=np.int64, accept_sparse=True)\n\n\n\n", "    def test_check_array_dtype_errors():\n        X = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n\n        # invalid dtypes\n        for dtype in ['object', 'category', 'datetime64[ns]', \n                      [object()], sp.csr_matrix(X)]:\n            msg = (r\"Invalid dtype.*?only 'np\\.float[64|32]|'|np\\.int[32|64]'\"\n                   r\"|np\\.bool|'object'.*?is not allowed.\")\n            assert_raise_message(TypeError, msg, check_array, X, dtype=dtype)\n\n\n", "    def test_check_array_nan_in_sparse():\n        X_csr = sp.csr_matrix([[1, 2, np.nan], [4, 5, 6]])\n        X_csc = sp.csc_matrix([[1, 2, np.nan], [4, 5, 6]])\n        assert_raises(ValueError, check_array, X_csr, force_all_finite=False)\n        assert_raises(ValueError, check_array, X_csc, force_all_finite=False)\n\n\n        X_csr_checked = check_array(X_csr, force_all_finite=True)\n        X_csc_checked = check_array(X_csc, force_all_finite=True)\n\n        assert_array_equal(X_csr_checked.toarray(), X_csr.toarray())\n        assert_array_equal(X_csc_checked.toarray(), X_csc.toarray())\n"], "sample_170": ["    def test_sensitive_variables_decorator_with_arguments(self):\n        @sensitive_variables(exclude=['cooked_eggs'])\n            return password, cooked_eggs, scrambled\n\n        with self.settings(DEBUG=True):\n            response = self.client.post('/test_view/', data={'password': 'secret', 'cooked_eggs': 'delicious', 'scrambled': 'yes'})\n            response = self.client.post('/test_view/', data={'password': 'secret', 'cooked_eggs': 'delicious', 'scrambled': 'yes'})\n            self.assertContains(response, 'secret', status_code=500)\n\n            # Test with excluded variable\n            self.assertContains(response, 'delicious', status_code=500) \n\n", "    def test_sensitive_variables_on_class_methods(self):\n        class SensitiveClass:\n            @sensitive_variables\n                return password\n\n        instance = SensitiveClass()\n        with self.assertRaises(TypeError):\n            instance.sensitive_method(password='secret')\n\n\n\n", "    def test_sensitive_variables_with_multiple_arguments(self):\n        @sensitive_variables\n            return 'Function executed'\n\n        with self.settings(DEBUG=True):\n            response = self.client.post('/foo/', data={'password': 'secret', 'api_key': 'secret_key', 'name': 'admin'})\n            self.assertIn('Function executed', response.content.decode())\n            self.assertNotIn('secret', response.content.decode())\n            self.assertNotIn('secret_key', response.content.decode())\n", "    def test_sensitive_variables_with_kwargs(self):\n        @sensitive_variables\n            return password + kwargs['another_sensitive_var']\n        with self.assertRaises(TypeError):\n            test_func('secret', another_sensitive_var='more_secret')\n", "    def test_sensitive_variables_decorator_multiple_arguments(self):\n        @sensitive_variables\n            return password, api_key, secret_key\n\n        with self.settings(DEBUG=True):\n            response = self.client.post('/test_view/', {\n                'password': 'secret', 'api_key': 'my_api_key', 'secret_key': 'my_secret_key'\n            })\n            self.assertEqual(response.status_code, 500)\n\n        with self.settings(DEBUG=False):\n            response = self.client.post('/test_view/', {\n                'password': 'secret', 'api_key': 'my_api_key', 'secret_key': 'my_secret_key'\n            })\n            self.assertEqual(response.status_code, 500)\n            self.assertContains(response, 'secret')\n\n"], "sample_324": ["    def test_csrf_middleware_with_framework_version(self):\n        req = self._get_POST_request_with_token()\n        req.META['HTTP_HOST'] = 'www.example.com'\n        mw = CsrfViewMiddleware(post_form_view)\n        self.assertIs(mw._origin_verified(req), True)\n        resp = mw.process_view(req, post_form_view, (), {})\n        self.assertIsNone(resp)\n", "    def test_csrf_cookie_same_site_strict(self):\n        \"\"\"\n        The CSRF cookie should be set with SameSite=Strict.\n        \"\"\"\n        req = self._get_GET_no_csrf_cookie_request()\n        mw = CsrfViewMiddleware(token_view)\n        mw.process_view(req, token_view, (), {})\n        resp = mw(req)\n        self.assertEqual(resp.cookies[settings.CSRF_COOKIE_NAME]['samesite'], 'Strict')\n\n", "    def test_https_good_referer_mismatch_cookie_domain(self):\n        \"\"\"\n        A POST HTTPS request from a subdomain not allowed by\n        SESSION_COOKIE_DOMAIN should be rejected.\n        \"\"\"\n        req = self._get_POST_request_with_token()\n        req._is_secure_override = True\n        req.META['HTTP_REFERER'] = 'https://foo.nonexistent.com/'\n        req.META['SERVER_PORT'] = '443'\n        mw = CsrfViewMiddleware(post_form_view)\n        response = mw.process_view(req, post_form_view, (), {})\n        self.assertContains(response, 'Referer is not allowed', status_code=403)\n\n", "    def test_session_expiration(self):\n        #  This test creates a session with a specific expiration time \n        #  and then checks if the CSRF cookie is refreshed or invalidated\n        #  when the session expires.\n\n        req = self._get_GET_no_csrf_cookie_request()\n        mw = CsrfViewMiddleware(token_view)\n\n        # Set a session expiration time\n        req.session.expire_at = datetime.datetime.now() - datetime.timedelta(seconds=1)\n\n        # 1.  First request - should create a new CSRF cookie and set session expiration\n        mw.process_view(req, token_view, (), {})\n        mw(req)\n        self.assertIsNotNone(req.session.get(CSRF_SESSION_KEY))\n\n        # 2.  Second request - verify the session is expired and CSRF cookie is invalid\n        mw.process_view(req, token_view, (), {})\n        resp = mw(req)\n        self.assertIn('CSRF cookie is invalidated', resp.content.decode())\n        # ... (additional assertions for CSRF cookie invalidation)\n\n\n\n", "    def test_https_good_referer_matches_cookie_domain_with_different_port(self):\n        \"\"\"\n        A POST HTTPS request with a good referer from a different port\n        should be accepted from a subdomain that's allowed by SESSION_COOKIE_DOMAIN.\n        \n        \"\"\"\n        self._test_https_good_referer_matches_cookie_domain_with_different_port() \n"], "sample_82": ["    def test_custom_format_value(self):\n        class CustomSelectDateWidget(SelectDateWidget):\n                if value:\n                    year, month, day = value.split('-')\n                    return {'day': day, 'month': month, 'year': year}\n                return {'day': None, 'month': None, 'year': None}\n\n        widget = CustomSelectDateWidget()\n        self.assertEqual(widget.format_value('2000-12-15'), {'day': '15', 'month': '12', 'year': '2000'})\n        self.assertEqual(widget.format_value('0-01-01'), {'day': '01', 'month': '0', 'year': '0'})\n\n", "    def test_clean(self):\n        # Test cleaning with invalid input\n        self.assertEqual(self.widget.clean('abc'), None)\n        self.assertEqual(self.widget.clean('2020-14-30'), None)\n        self.assertEqual(self.widget.clean('2020-02-30'), '2020-02-29')\n        self.assertEqual(self.widget.clean('2020-02-30'), '2020-02-29')  \n\n\n", "    def test_errors_on_invalid_format(self):\n        with self.assertRaisesMessage(ValidationError, \"Enter a valid date\"):\n            form = forms.Form({'date': 'invalid date'})\n            form.fields['date'].widget.value_from_datadict(form.data, {}, 'date')\n", "    def test_validation(self):\n        form = Form(data={'mydate': '2020-13-30'})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['mydate'], [u'Enter a valid date.'])\n", "    def test_value_from_datadict_with_invalid_format(self):\n        with self.subTest(value='2000/12/1'):\n            data = {'field_year': '2000', 'field_month': '12', 'field_day': '1'}\n            self.assertEqual(self.widget.value_from_datadict(data, {}, 'field'), '2000-12-1')\n\n        with self.subTest(value='2000-13-1'):\n            data = {'field_year': '2000', 'field_month': '13', 'field_day': '1'}\n            self.assertEqual(self.widget.value_from_datadict(data, {}, 'field'), None)\n\n\n"], "sample_762": ["    def test_pickle_version_warning_is_not_issued_with_same_version():\n        iris = datasets.load_iris()\n        tree = DecisionTreeClassifier().fit(iris.data, iris.target)\n        tree_pickle_same = pickle.dumps(tree)\n        assert b\"version\" in tree_pickle_same\n        tree_restored = pickle.loads(tree_pickle_same)\n\n        # test that we can predict with the restored decision tree classifier\n        score_of_original = tree.score(iris.data, iris.target)\n        score_of_restored = tree_restored.score(iris.data, iris.target)\n        assert_equal(score_of_original, score_of_restored)\n", "def test_pickle_with_sparse_matrix():\n    from sklearn.preprocessing import StandardScaler\n    import scipy.sparse as sp\n\n    class TestEstimator(BaseEstimator, TransformerMixin):\n            self.scaler = scaler\n            \n            if self.scaler is None:\n                self.scaler = StandardScaler()\n            self.scaler.fit(X)\n\n            return self.scaler.transform(X)\n\n    # Create a sparse matrix\n    sparse_matrix = sp.rand(5, 5)\n\n    # Create an instance of the estimator\n    estimator = TestEstimator()\n\n    # Fit the estimator on the sparse matrix\n    estimator.fit(sparse_matrix)\n\n    # Pickle and unpickle the estimator\n    pickle_dumped = pickle.dumps(estimator)\n    restored_estimator = pickle.loads(pickle_dumped)\n\n    # Check that the restored estimator is equivalent to the original\n    assert_equal(estimator.scaler.n_features_, restored_estimator.scaler.n_features_)\n    assert_array_equal(estimator.scaler.mean_, restored_estimator.scaler.mean_)\n    assert_array_equal(estimator.scaler.scale_, restored_estimator.scaler.scale_)\n    assert_array_equal(estimator.scaler.transform(sparse_matrix),\n                       restored_estimator.scaler.transform(sparse_matrix))\n\n\n\n", "    def test_pickle_version_warning_is_issued_when_estimator_has_no_getstate():\n        iris = datasets.load_iris()\n        class NoGetStateEstimator(BaseEstimator):\n                pass\n\n                pass\n\n        estimator = NoGetStateEstimator()\n        tree_pickle_no_getstate = pickle.dumps(estimator)\n        message = pickle_error_message.format(estimator=\"NoGetStateEstimator\",\n                                            old_version=\"pre-0.18\",\n                                            current_version=sklearn.__version__)\n        assert_warns_message(UserWarning, message, pickle.loads,\n                             tree_pickle_no_getstate)\n\n", "def test_is_outlier_detector():\n    # test that is_outlier_detector correctly identifies outlier detectors\n    estimator = K()\n    assert is_outlier_detector(estimator)\n    estimator = Buggy()\n    assert not is_outlier_detector(estimator)\n\n", "def test_pickle_version_warning_is_issued_with_explicit_version_in_pickle():\n    iris = datasets.load_iris()\n    tree = DecisionTreeClassifier().fit(iris.data, iris.target)\n    tree_pickle_explicit = pickle.dumps(tree)\n    tree_pickle_explicit = tree_pickle_explicit.replace(\n        b\"version={}\".format(sklearn.__version__).encode(),\n        b\"version=wrong_version\".encode())\n    \n    message = pickle_error_message.format(estimator=\"DecisionTreeClassifier\",\n                                          old_version=\"wrong_version\",\n                                          current_version=sklearn.__version__)\n    assert_warns_message(UserWarning, message, pickle.loads, tree_pickle_explicit)\n"], "sample_414": ["    def test_null_value(self):\n        self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_widgets_event_add\")\n        )\n        self.selenium.find_element(By.ID, \"lookup_id_main_band\").click()\n        self.wait_for_and_switch_to_popup()\n        self.selenium.find_element(By.XPATH, \"//button[contains(text(), 'Close')]\").click()\n        self.selenium.switch_to.window(self.selenium.current_window_handle)\n        \n        # Check that the field renders with an empty value\n        self.assertEqual(\n            self.selenium.find_element(By.ID, \"id_main_band\").get_attribute(\"value\"),\n            \"\",\n        )\n\n", "    def test_clear_button(self):\n        from selenium.webdriver.common.by import By\n\n        self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_widgets_event_add\")\n        )\n\n        # Select some options\n        self.selenium.find_element(By.ID, \"lookup_id_main_band\").click()\n        self.wait_for_and_switch_to_popup()\n        link = self.selenium.find_element(By.LINK_TEXT, \"Bogey Blues\")\n        self.assertIn(\"/band/42/\", link.get_attribute(\"href\"))\n        link.click()\n\n        self.selenium.find_element(By.ID, \"lookup_id_supporting_bands\").click()\n        self.wait_for_and_switch_to_popup()\n        link = self.selenium.find_element(By.LINK_TEXT, \"Green Potatoes\")\n        self.assertIn(\"/band/98/\", link.get_attribute(\"href\"))\n        link.click()\n\n        # Click the clear button\n        self.selenium.switch_to.window(self.selenium.current_window_handle)\n        clear_button = self.selenium.find_element(By.CSS_SELECTOR, \"button.clear\")\n        clear_button.click()\n\n        # Make sure the fields are empty\n        self.assertEqual(\n            self.selenium.find_element(By.ID, \"id_main_band\").get_attribute(\"value\"), \"\"\n        )\n        self.assertEqual(\n            self.selenium.find_element(By.ID, \"id_supporting_bands\").get_attribute(\n                \"value\"\n            ),\n            \"\",\n        )\n\n", "    def test_clear_value(self):\n        from selenium.webdriver.common.by import By\n\n        self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_widgets_event_add\")\n        )\n        main_window = self.selenium.current_window_handle\n\n        # Set a value\n        self.selenium.find_element(By.ID, \"lookup_id_main_band\").click()\n        self.wait_for_and_switch_to_popup()\n        link = self.selenium.find_element(By.LINK_TEXT, \"Bogey Blues\")\n        self.assertIn(\"/band/42/\", link.get_attribute(\"href\"))\n        link.click()\n        self.selenium.switch_to.window(main_window)\n        self.wait_for_value(\"#id_main_band\", \"42\")\n\n        # Clear the value using the clear button\n        clear_button = self.selenium.find_element(By.ID, \"clear_id_main_band\")\n        clear_button.click()\n        self.assertEqual(self.selenium.find_element(By.ID, \"id_main_band\").get_attribute(\"value\"), \"\")\n\n        # Attempt to open the popup window\n        self.selenium.find_element(By.ID, \"lookup_id_main_band\").click()\n        self.wait_for_and_switch_to_popup()\n", "    def test_clear_button(self):\n        from selenium.webdriver.common.by import By\n\n        self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_widgets_event_add\")\n        )\n\n        # Select some bands\n        self.selenium.find_element(By.ID, \"lookup_id_main_band\").click()\n        self.wait_for_and_switch_to_popup()\n        self.selenium.find_element(By.LINK_TEXT, \"Bogey Blues\").click()\n        self.selenium.switch_to.window(self.selenium.current_window_handle)\n        self.selenium.find_element(By.ID, \"lookup_id_supporting_bands\").click()\n        self.wait_for_and_switch_to_popup()\n        self.selenium.find_element(By.LINK_TEXT, \"Green Potatoes\").click()\n        self.selenium.switch_to.window(self.selenium.current_window_handle)\n        self.wait_for_value(\"#id_main_band\", \"42\")\n        self.wait_for_value(\"#id_supporting_bands\", \"98\")\n\n        # Click clear button\n        clear_button = self.selenium.find_element(By.ID, \"clear_id_supporting_bands\")\n        clear_button.click()\n        self.wait_for_value(\"#id_supporting_bands\", \"\")\n\n\n\n", "    def test_multiple_selections(self):\n        from selenium.webdriver.common.by import By\n\n        self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_widgets_event_add\")\n        )\n        main_window = self.selenium.current_window_handle\n\n        # Open the popup window and click on a few bands\n        self.selenium.find_element(By.ID, \"lookup_id_supporting_bands\").click()\n        self.wait_for_and_switch_to_popup()\n        self.selenium.find_element(By.LINK_TEXT, \"Bogey Blues\").click()\n        self.selenium.find_element(By.LINK_TEXT, \"Green Potatoes\").click()\n        self.selenium.find_element(By.LINK_TEXT, \"Bogey Blues\").click()  # Select again\n\n        # The field now contains the selected bands' ids as comma-separated values\n        self.selenium.switch_to.window(main_window)\n        self.wait_for_value(\"#id_supporting_bands\", \"42,98,42\")\n\n\n"], "sample_24": ["    def test_masking_function_handling(self):\n        # Test that functions like mean, sum, etc. with optional 'out' argument\n        # create a new masked array if out is not provided.\n        a = np.array([1, 2, 3, 4, 5])\n        mask = np.array([False, True, False, True, False])\n        ma = Masked(a, mask=mask)\n        # Test functions with 'out' argument but defaulting to NumPy behavior.\n        for func in [np.mean, np.sum, np.min, np.max]:\n            out = func(ma)\n            assert isinstance(out, Masked)\n            # If we don't provide 'out', it should create a new array.\n            out2 = func(ma, out=None)\n            assert isinstance(out2, Masked)\n            assert out2 is not out\n", "    def test_apply_to_both(self):\n        for masked_func in APPLY_TO_BOTH_FUNCTIONS.keys():\n            unmasked_func = APPLY_TO_BOTH_FUNCTIONS[masked_func]\n            for a in (np.array([1, 2, 3]), Masked(np.array([1, 2, 3]), mask=[False, True, False])):\n                out_masked = masked_func(a)\n                out_unmasked = unmasked_func(a.unmasked)\n                assert_masked_equal(out_masked, out_unmasked)\n", "    def test_all_wrapped_functions(self):\n        assert all_wrapped_functions == all_wrapped\n", "    def test_special_cases(self):\n        # Test for functions with unusual behavior\n        #  or those requiring special handling\n        assert np.isnan(np.nan).all()\n        # Add more tests as needed for functions like \n        # np.finfo, np.iinfo, np.testing.assert_allclose, \n        # np.testing.assert_array_equal, etc. \n", "    def test_masked_array_indexing(self):\n        # Test indexing of MaskedArrays on various dimensions\n        mask = np.array([True, False, True, False], dtype=\"bool\")\n        a = np.arange(4)\n        ma = Masked(a, mask=mask)\n        \n        # Test indexing with single integer\n        assert_array_equal(ma[1], a[1])\n        assert_array_equal(ma[[1, 3]], a[[1, 3]])\n\n        # Test slicing with integer ranges\n        assert_array_equal(ma[1:3], a[1:3])\n\n        # Test boolean indexing\n        assert_array_equal(ma[mask], a[mask])\n\n        # Test fancy indexing with integer arrays\n        idx = np.array([0, 2])\n        assert_array_equal(ma[idx], a[idx])\n\n        # Test with slicing and boolean indexing\n        assert_array_equal(ma[1:3][mask], a[1:3][mask])\n\n\n\n"], "sample_522": ["def test_colorbar_aspect_with_bounds():\n    fig, ax = plt.subplots()\n    data = np.random.rand(10, 10)\n    pc = ax.pcolormesh(data, cmap='viridis')\n    cb = fig.colorbar(pc, ax=ax, aspect=10, bounds=[0.0, 1.0])\n    \n    # Check the aspect\n    assert cb.ax.get_position().width / cb.ax.get_position().height == 10\n\n    # Check the bounds are applied\n    ticks = cb.ax.get_yticks()\n    assert np.all(ticks >= 0.0)\n    assert np.all(ticks <= 1.0)\n", "compilation error", "def test_colorbar_aspect_with_custom_cmap():\n    fig, ax = plt.subplots(figsize=(4, 2))\n    x = np.arange(-3, 3, 0.1)\n    y = np.arange(-3, 3, 0.1)\n    X, Y = np.meshgrid(x, y)\n    Z = np.sin(X**2 + Y**2)\n\n    cmap = plt.cm.get_cmap('viridis', 100)\n    norm = plt.Normalize(vmin=-1, vmax=1)\n    pc = ax.pcolormesh(X, Y, Z, cmap=cmap, norm=norm)\n    cb = fig.colorbar(pc, ax=ax,  aspect=20)\n    assert cb.ax.get_position().width / cb.ax.get_position().height == 20\n", "def test_colorbar_alignment():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.random.randn(10, 10))\n    cb = fig.colorbar(pc, ax=ax, alignment='center')\n    assert cb.ax.yaxis.get_position().y0 == 0.5\n    cb.set_alignment('right')\n    assert cb.ax.yaxis.get_position().x0 == 1\n    cb.set_alignment('left')\n    assert cb.ax.yaxis.get_position().x0 == 0\n", "    def test_colorbar_fractional_location():\n        fig, ax = plt.subplots()\n        pc = ax.pcolormesh(np.rand(10, 10))\n        cb = fig.colorbar(pc, fraction=0.1, pad=0.1)\n        assert cb.ax.get_position().width * fig.get_size_inches()[0] > 0.1 - 0.1\n        assert cb.ax.get_position().height * fig.get_size_inches()[1] > 0.1 - 0.1\n"], "sample_272": ["    def test_minimize_rollbacks_long_branch(self):\n        \"\"\"\n        Minimize rollbacks when target is deep on a long dependency branch.\n        \n        (Imagine a long chain of migrations, where the target is\n        deeply nested within that chain) - This test should\n        verify that the rollback logic in migrate_plan considers\n        the entire dependency tree and only rolls back what's\n        strictly necessary.\n        \"\"\"\n        # Build a long chain of migrations with some branching.\n        # ... (add nodes and dependencies for a1, a2, a3, ..., aN, b1, b2, ... )\n\n        graph = MigrationGraph()\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {})\n\n        # ... (add logic to construct node and dependency sets)\n\n        plan = executor.migration_plan({a1})\n\n        # ... (Assertions to check that only the necessary\n        # migrations are in the plan)\n", "    def test_minimize_rollbacks_no_deps(self):\n        \"\"\"Don't roll back if no dependencies.\"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(b1, b1_impl)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {a1: a1_impl})\n\n        plan = executor.migration_plan({a1})\n\n        self.assertEqual(plan, [(b1_impl, False)])\n\n\n", "    def test_minimize_rollbacks_multiple_target_apps(self):\n        \"\"\"\n        Minimize rollbacks when target has multiple in-app children.\n\n        a: 1 <---- 3 <--\\\n              \\ \\- 2 <--- 4\n               \\       \\\n        b:      \\- 1 <--- 2\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        a3_impl = FakeMigration('a3')\n        a3 = ('a', '3')\n        a4_impl = FakeMigration('a4')\n        a4 = ('a', '4')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        b2_impl = FakeMigration('b2')\n        b2 = ('b', '2')\n        c1_impl = FakeMigration('c1')\n        c1 = ('c', '1')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(a3, a3_impl)\n        graph.add_node(a4, a4_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_node(b2, b2_impl)\n        graph.add_node(c1, c1_impl)\n        graph.add_dependency(None, a2, a1)\n        graph.add_dependency(None, a3, a1)\n        graph.add_dependency(None, a4, a2)\n        graph.add_dependency(None, a4, a3)\n        graph.add_dependency(None, b2, b1)\n        graph.add_dependency(None, b1, a1)\n        graph.add_dependency(None, b2, a2)\n        graph.add_dependency(None, c1, a2)\n\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader", "    def test_minimize_rollbacks_circular_dependency(self):\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_dependency(None, b1, a1)\n        graph.add_dependency(None, a2, b1) # Circular dependency!\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            b1: b1_impl,\n            a2: a2_impl,\n        })\n\n        plan = executor.migration_plan({a1})\n        self.assertEqual(plan, [(a2_impl, True)])\n", "    def test_minimize_rollbacks_complex(self):\n        \"\"\"Minimize rollbacks with interleaved dependencies.\"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        c1_impl = FakeMigration('c1')\n        c1 = ('c', '1')\n        d1_impl = FakeMigration('d1')\n        d1 = ('d', '1')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_node(c1, c1_impl)\n        graph.add_node(d1, d1_impl)\n        graph.add_dependency(None, a2, a1)\n        graph.add_dependency(None, b1, a1)\n        graph.add_dependency(None, c1, b1)\n        graph.add_dependency(None, d1, c1)\n\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            b1: b1_impl,\n            c1: c1_impl,\n            d1: d1_impl,\n            a2: a2_impl,\n        })\n\n        plan = executor.migration_plan({a1})\n        self.assertEqual(plan, [(a2_impl, True), (b1_impl, True), (c1_impl, True), (d1_impl, True)])\n"], "sample_850": ["def test_nystroem_sparse_inputs():\n    # Test Nystroem with sparse input matrices\n    rng = np.random.RandomState(42)\n    n_samples = 100\n    n_features = 1000\n    X = rng.rand(n_samples, n_features)\n    X_sparse = csr_matrix(X)\n\n    nystroem = Nystroem(n_components=50)\n    X_transformed_dense = nystroem.fit_transform(X)\n    X_transformed_sparse = nystroem.fit_transform(X_sparse)\n    assert_array_almost_equal(X_transformed_dense, X_transformed_sparse)\n\n    nystroem = Nystroem(kernel='rbf', n_components=50, gamma=1.0)\n    X_transformed_dense = nystroem.fit_transform(X)\n    X_transformed_sparse = nystroem.fit_transform(X_sparse)\n    assert_array_almost_equal(X_transformed_dense, X_transformed_sparse)\n", "def test_nystroem_sparse_data():\n    # Test Nystroem on sparse data\n    rnd = np.random.RandomState(42)\n    n_samples = 100\n    n_features = 500\n    X = csr_matrix(rnd.randint(0, 2, size=(n_samples, n_features)))\n    nystroem = Nystroem(n_components=10, random_state=42)\n    X_transformed = nystroem.fit_transform(X)\n    assert X_transformed.shape == (n_samples, 10)\n    assert isinstance(X_transformed, np.ndarray)\n", "def test_nystroem_sparse_data():\n    # test Nystroem with sparse input data, all kernels\n    rnd = np.random.RandomState(0)\n    n_samples = 1000\n    n_features = 100\n    X = rnd.rand(n_samples, n_features)\n    X_sparse = csr_matrix(X)\n\n    kernels = ['rbf', 'polynomial', 'linear', 'sigmoid', 'chi2']\n    for kernel in kernels:\n        nystroem = Nystroem(kernel=kernel, n_components=100, random_state=rnd)\n        X_dense = nystroem.fit_transform(X_sparse)\n        X_sparse_transformed = nystroem.transform(X_sparse)\n\n        assert_array_almost_equal(X_dense, X_sparse_transformed.toarray())\n\n", "    def rbf_kernel_with_constant(x, y, gamma=1.0, constant=0.0):\n        return rbf_kernel(x, y, gamma=gamma) + constant\n", "def test_nystroem_sparse_data():\n    # Test Nystroem with sparse data.\n    rnd = np.random.RandomState(42)\n    n_samples = 10000\n    n_features = 100\n    X = rnd.rand(n_samples, n_features)\n    X_sparse = csr_matrix(X)\n    nystroem = Nystroem(n_components=500)\n    X_transformed = nystroem.fit_transform(X_sparse)\n    X_transformed_dense = nystroem.transform(X_sparse)\n    assert_array_almost_equal(X_transformed, X_transformed_dense)\n"], "sample_615": ["def test_polyval_broadcast() -> None:\n    if not has_dask:\n        pytest.skip(\"test for dask.\")\n\n    x = np.arange(10)\n    xcoord = xr.DataArray(x, dims=(\"x\",), name=\"x\")\n    coeffs = xr.DataArray(\n        np.array([[1, 2], [3, 4]]),\n        dims=(\"d\", \"degree\"),\n        coords={\"d\": [0, 1], \"degree\": [1, 0]},\n    )\n    da = xr.DataArray(\n        np.stack((x, x**2), axis=0), dims=(\"d\", \"x\"), coords={\"x\": xcoord, \"d\": [0, 1]}\n    )\n    da_pv = xr.polyval(da.x, coeffs.to_dataset().polyval)\n\n    xr.testing.assert_allclose(da_pv.squeeze().values, da.values)\n", "    def test_polyval_raises_on_incompatible_dims() -> None:\n        x = np.arange(10)\n        da = xr.DataArray(np.arange(10).reshape(10, 1), dims=(\"x\", \"y\"))\n        coeffs = xr.DataArray(np.arange(3).reshape(1, 3), dims=(\"degree\",))\n        with pytest.raises(ValueError, match=r\"Dimensions of\"):\n            xr.polyval(da.x, coeffs)\n", "compilation error", "compilation error", "compilation error"], "sample_209": ["    def test_model_with_evaluate_method_ordering(self):\n        \"\"\"\n        You can filter by objects that have an 'evaluate' attr, and order them\n        \"\"\"\n        dept = Department.objects.create(pk=1, name='abc')\n        dept.evaluate = 'abc'\n\n        workers = Worker.objects.filter(department=dept).order_by('evaluate')\n        self.assertEqual(len(workers), 0)\n", "    def test_ordering_with_related_fields(self):\n        \"\"\"\n        Regression test for issue #4710: Ordering with related fields doesn't work\n        \"\"\"\n        Department.objects.create(name=\"Engineering\")\n        Department.objects.create(name=\"Marketing\")\n        w1 = Worker.objects.create(name=\"Alice\", department=Department.objects.get(name=\"Marketing\"))\n        w2 = Worker.objects.create(name=\"Bob\", department=Department.objects.get(name=\"Engineering\"))\n        w3 = Worker.objects.create(name=\"Charlie\", department=Department.objects.get(name=\"Marketing\"))\n        workers = Worker.objects.all().order_by('department__name', 'name')\n        expected_worker_names = ['Alice', 'Bob', 'Charlie']\n        self.assertEqual(workers.values_list('name', flat=True).tolist(), expected_worker_names)\n\n\n", "    def test_ordering_with_non_field(self):\n        \"\"\"\n        Regression test for #15064: Ordering by a non-field attribute\n        raises an error.\n        \"\"\"\n        # Create some Model instances with custom ordering\n        with self.settings(\n            INSTALLED_APPS=[\n                'model_regress'\n            ]\n        ):\n            worker1 = Worker.objects.create(department=Department.objects.create(name=\"IT\"), name=\"Full-time\")\n            worker2 = Worker.objects.create(department=Department.objects.create(name=\"IT\"), name=\"Part-time\")\n\n        # The following line should not raise an error\n        Worker.objects.all().order_by(\"non_existing_field\")\n", "    def test_ordering_with_respect_to(self):\n        \"\"\"\n        Regression test for #11862: Order by a related field\n        when there is no foreign key.\n        \"\"\"\n\n        # Create related objects\n        d = Department.objects.create(name=\"IT\", order=100)\n        d2 = Department.objects.create(name=\"HR\", order=200)\n\n        # Create worker objects associated with each department\n        w1 = Worker.objects.create(name=\"Alice\", department=d, order=1)\n        w2 = Worker.objects.create(name=\"Bob\", department=d, order=2)\n        w3 = Worker.objects.create(name=\"Charlie\", department=d2, order=3)\n\n        # Test ordering by department using 'order_with_respect_to'\n        workers = Worker.objects.filter(department=d).order_by(\n            'department__order'\n        )\n\n        # Make sure the workers are ordered by department order\n        self.assertEqual([w.name for w in workers], ['Alice', 'Bob'])\n\n        workers = Worker.objects.filter(department=d2).order_by(\n            'department__order'\n        )\n        self.assertEqual([w.name for w in workers], ['Charlie'])\n\n        # Test ordering across departments using 'order_with_respect_to'\n        all_workers = Worker.objects.all().order_by('department__order')\n        self.assertEqual(\n            [w.name for w in all_workers], ['Alice', 'Bob', 'Charlie']\n        )\n\n\n\n", "    def test_model_with_evaluate_method_pk(self):\n        \"\"\"\n        You can filter by objects that have an 'evaluate' attr using pk\n        \"\"\"\n        dept = Department.objects.create(pk=1, name='abc')\n        dept.evaluate = 'abc'\n        Worker.objects.filter(department=dept, pk=1)\n"], "sample_722": ["compilation error", "def test_sparse_max_iter():\n    from sklearn.datasets import make_sparse_toy\n    X, _ = make_sparse_toy(n_samples=100, n_features=50,\n                           random_state=42)\n\n    # Test with a very low max_iter to check that it stops early\n    km = KMeans(n_clusters=5, max_iter=1, random_state=42)\n    km.fit(X)\n    assert_less(km.n_iter_, 2)\n\n\n", "def test_k_means_non_increasing_inertia():\n\n    n_runs = 5\n    random_state = 42\n    n_init = 5\n    inertia_values = np.zeros((n_init, n_runs))\n    for i in range(n_init):\n        for j in range(n_runs):\n            km = KMeans(n_clusters=n_clusters, random_state=random_state,\n                        n_init=1)\n            inertia_values[i, j] = km.fit(X).inertia_\n\n    for i in range(n_init - 1):\n        for j in range(n_runs):\n            assert_true(inertia_values[i, j] >= inertia_values[i + 1, j],\n                        msg=\"Inertia %r should be decreasing when n_init\"\n                            \" is increasing.\" % inertia_values)\n", "def test_sparse_init_centers_from_fit():\n    from sklearn.datasets import load_iris\n    iris = load_iris()\n    X = iris.data\n    X_sparse = sp.csr_matrix(X)\n    km = KMeans(n_clusters=3, random_state=42).fit(X)\n    km_sparse = KMeans(n_clusters=3, random_state=42).fit(X_sparse)\n    assert_array_almost_equal(km.cluster_centers_, km_sparse.cluster_centers_)\n", "def test_sparse_k_means_init_with_too_few_centers():\n    from sklearn.datasets import load_iris\n\n    iris = load_iris()\n    X = iris.data\n    X_sparse = sp.csr_matrix(X)\n\n    # Provide less initial centers than clusters\n    init_centers = KMeans(n_clusters=4).fit(X).cluster_centers_[:2]\n    \n    km = KMeans(n_clusters=4, init=init_centers, n_init=1)\n    # Ensure the fit does not raise an error\n    km.fit(X_sparse)\n    # Expect cluster centers to fill the gaps \n    assert_equal(km.cluster_centers_.shape[0], 4)\n\n\n\n"], "sample_388": ["    def test_invalid_username(self):\n        \"\"\"\n        Tests the behavior when an invalid username is passed in the REMOTE_USER header.\n        \"\"\"\n        response = self.client.get(\"/remote_user/\", **{self.header: \"invalid_username\"})\n        self.assertTrue(response.context[\"user\"].is_anonymous)\n", "    def test_user_creation_with_email_header(self):\n        num_users = User.objects.count()\n        response = self.client.get(\n            \"/remote_user/\",\n            **{\n                self.header: \"newuser\",\n                self.email_header: \"user@example.com\",\n            },\n        )\n        self.assertEqual(response.context[\"user\"].username, \"newuser\")\n        self.assertEqual(response.context[\"user\"].email, \"user@example.com\")\n        self.assertEqual(User.objects.count(), num_users + 1)\n", "    def test_remote_user_middleware_with_auth_backends(self):\n        # Set up a user object that is not known to any backend\n        unknown_user_username = \"unknown_user_from_middleware\"\n        User.objects.create(username=unknown_user_username)\n\n        with self.assertWarnsMessage(\n            RemovedInDjango50Warning,\n            \"`.configure_user()` method in CustomRemoteUserBackend \"\n            f\"should take an `is_anonymous=True` argument.\",\n        ):\n            # Test authentication with the RemoteUserMiddleware\n            # while also having a custom authentication backend\n            response = self.client.get(\n                \"/remote_user/\",\n                **{\n                    RemoteUserTest.header: unknown_user_username,\n                    RemoteUserTest.email_header: \"unknown_user@example.com\",\n                },\n            )\n            self.assertEqual(response.context[\"user\"].username, unknown_user_username)\n            self.assertEqual(response.context[\"user\"].email, \"unknown_user@example.com\")\n\n", "    def test_unknown_user_with_email_header_missing(self):\n        num_users = User.objects.count()\n        response = self.client.get(\"/remote_user/\", **{self.header: \"newuser\"})\n        self.assertEqual(response.context[\"user\"].username, \"newuser\")\n        self.assertEqual(response.context[\"user\"].email, \"\")\n        self.assertEqual(User.objects.count(), num_users + 1)\n", "    def test_user_email_override(self):\n        \"\"\" Tests that the email address can be overridden when setting up a new user with REMOTE_USER and REMOTE_EMAIL. \"\"\"\n        User.objects.create(username=\"knownuser\")\n        response = self.client.get(\n            \"/remote_user/\",\n            **{\n                self.header: \"newuser\",\n                self.email_header: \"user@example.com\",\n            },\n        )\n        self.assertEqual(response.context[\"user\"].username, \"newuser\")\n        self.assertEqual(response.context[\"user\"].email, \"user@example.com\")\n\n"], "sample_250": ["    def test_empty_timezone(self):\n        my_birthday = datetime(1979, 7, 8, 22, 00, tzinfo=None)\n        self.assertEqual(dateformat.format(my_birthday, 'O'), '')\n        self.assertEqual(dateformat.format(my_birthday, 'e'), '')\n        self.assertEqual(dateformat.format(my_birthday, 'T'), '')\n        self.assertEqual(dateformat.format(my_birthday, 'Z'), '')\n\n\n\n", "    def test_formatchars_escaping(self):\n        my_birthday = datetime(1979, 7, 8, 22, 00)\n        self.assertEqual(dateformat.format(my_birthday, ' jS F Y '), ' 7th July 1979 ')\n        self.assertEqual(dateformat.format(my_birthday, ' jS F Y \\\\'), ' 7th July 1979 \\\\')\n\n\n", "    def test_escaped_characters(self):\n        dt = datetime(2023, 10, 26, 14, 30, 0)\n        self.assertEqual(dateformat.format(dt, r'a\\\\b'), 'a\\b')\n        self.assertEqual(dateformat.format(dt, r'\\(\\\\)') , r'(\\\\)')\n", "    def test_format_with_timezone_aware_datetime(self):\n        dt = datetime(2023, 10, 26, 10, 30, tzinfo=utc)\n        self.assertEqual(format(dt, 'O'), '+0000')\n        self.assertEqual(format(dt, 'e'), '')\n        self.assertEqual(format(dt, 'r'), 'Thu, 26 Oct 2023 10:30:00 +0000')\n        self.assertEqual(format(dt, 'T'), 'UTC')\n        self.assertEqual(format(dt, 'Z'), '0')\n", "    def test_alternative_month_names(self):\n        my_birthday = datetime(1979, 7, 8, 22, 00)\n        self.assertEqual(dateformat.format(my_birthday, 'E'), 'July')\n"], "sample_1188": ["def test_pretty_print_function():\n    f = Function('f')\n    expr = f(a*xpretty(v[0]) + b*xpretty(v[1]))\n    assert pretty(expr) == 'f(0 + b*i_N)' \n", "def test_pretty_print_ordering_of_components():\n    n = CoordSys3D('N')\n    v1 = n.i + n.j + n.k\n    v2 = n.k + n.i + n.j\n\n    assert pretty(v1) == 'i_N + j_N + k_N'\n    assert pretty(v2) == 'k_N + i_N + j_N'\n", "compilation error", "def test_pretty_print_vector_op():\n    from sympy.vector import (\n        grad, curl, div, cross, dot,\n        exp, I\n    )\n    N = CoordSys3D('N')\n    x = N.x\n    y = N.y\n    z = N.z\n    a = N.i\n    b = N.j\n    c = N.k\n    v = Vector(a + 2*b)\n\n    grad_u = grad(x**2 + y**2)\n    curl_v = curl(v*c)\n    div_v = div(v)\n\n    assert pretty(grad_u) == '2 i_N + 2 j_N'\n    assert pretty(curl_v) == '0 i_N + 0 j_N + 0 k_N'\n    assert pretty(div_v) == '2' \n\n\n", "def test_pretty_print_vector_operators():\n    from sympy.vector import Vector, cross, dot, normalize\n    N = CoordSys3D('N')\n    v1 = N.i + N.j\n    v2 = 2*N.i - N.k\n    \n    assert pretty(cross(v1, v2)) == '(2) k_N + (1) i_N'\n    assert pretty(dot(v1, v2)) == '2'\n    assert pretty(normalize(v1)) == '((1/\u221a2) i_N + (1/\u221a2) j_N)'\n\n\n"], "sample_1195": ["compilation error", "compilation error", "def test_nested_gamma_expressions():\n    i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15 = tensor_indices('i0:16', LorentzIndex)\n    mu, nu, rho, sigma = tensor_indices(\"mu, nu, rho, sigma\", LorentzIndex)\n    D = 4\n\n    t = G(i0)*G(i1)*G(-i0)*G(i2)*G(-i2)\n    r = kahane_simplify(t)\n    assert r.equals(4*G(i1))\n\n    t = G(i0)*G(i1)*G(-i0)*G(i2)*G(-i2)*G(i3)*G(-i3)\n    r = kahane_simplify(t)\n    assert r.equals(16*G(i1)*G(i3))\n\n    t = G(i0)*G(i1)*G(-i0)*G(mu)*G(-mu)*G(i2)*G(-i2)*G(nu)*G(-nu)\n    r = kahane_simplify(t)\n    assert r.equals(16*G(i1)*G(i2))\n\n    t = G(i0)*G(i1)*G(-i0)*G(mu)*G(-i1)*G(nu)*G(-nu)*G(rho)*G(-rho)\n    r = kahane_simplify(t)\n    assert r.equals(16*G(i0)*G(rho))\n\n    t = G(mu)*G(nu)*G(-mu)*G(nu)*G(rho)*G(-rho)*G(sigma)*G(-sigma)*G(i0)*G(i1)*G(-i0)*G(i2)*G(-i2)\n    r = kahane_simplify(t)\n    assert r.equals(16*G(i1)*G(i2))\n\n\n\n", "def test_contraction_handling():\n    i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15 = tensor_indices('i0:16', LorentzIndex)\n    mu, nu, rho, sigma = tensor_indices(\"mu, nu, rho, sigma\", LorentzIndex)\n    D = 4\n    t = G(i0)*G(i1)*G(-i0)*G(mu)\n    r = kahane_simplify(t)\n    assert r.equals(2*G(i1)*G(mu))\n    t = G(i0)*G(i1)*G(-i0)*G(mu)*G(-mu)\n    r = kahane_simplify(t)\n    assert r.equals(2*G(i1))\n\n", "def test_multiple_contractions():\n    i0,i1,i2,i3,i4,i5,i6,i7,i8,i9,i10,i11,i12,i13,i14,i15 = tensor_indices('i0:16', LorentzIndex)\n    mu, nu, rho, sigma = tensor_indices(\"mu, nu, rho, sigma\", LorentzIndex)\n    D = 4\n    t = G(i0)*G(i1)*G(i2)*G(-i0)*G(-i1)*G(i2) \n    r = kahane_simplify(t)\n    assert r.equals(2*G(i2))\n    t = G(i0)*G(i1)*G(i2)*G(-i0)*G(-i2)*G(i1) \n    r = kahane_simplify(t)\n    assert r.equals(2*G(i1))\n    t = G(i0)*G(i1)*G(i2)*G(-i0)*G(-i1)*G(-i2) \n    r = kahane_simplify(t)\n    assert r.equals(-2*G(i2))\n\n    t = G(i0)*G(i1)*G(i2)*G(i3)*G(-i0)*G(-i1)*G(-i2)*G(-i3) \n    r = kahane_simplify(t)\n    assert r.equals((D**4)*eye(4))\n\n    t = (G(mu)*G(nu)*G(rho)*G(sigma)*G(-mu)*G(-nu)*G(-rho)*G(-sigma))\n    r = kahane_simplify(t)\n    assert r.equals((D**4)*eye(4))\n\n    t = (G(i0)*G(i1)*G(i2)*G(i3)*G(i4)*G(i5)*G(i6)*G(i7)*G(-i0)*G(-i1)*G(-i2)*G(-i3)*G(-i4)*G(-i5)*G(-i6)*G(-i7))"], "sample_265": ["    def test_templatetag_discovery_module_with_name(self):\n        engine = DjangoTemplates({\n            'DIRS': [],\n            'APP_DIRS': False,\n            'NAME': 'django',\n            'OPTIONS': {\n                'libraries': {\n                    'my_tags': 'template_backends.apps.templatetags_module_with_name.templatetags.my_tags',\n                },\n            },\n        })\n\n        self.assertEqual(\n            engine.engine.libraries['my_tags'],\n            'template_backends.apps.templatetags_module_with_name.templatetags.my_tags',\n        )\n", "    def test_invalid_library_entry(self):\n        with self.assertRaisesMessage(InvalidTemplateLibrary, \"Invalid template library specified. \"\n                                                              \"Library entry must be a string.\"):\n            DjangoTemplates({\n                'DIRS': [],\n                'APP_DIRS': False,\n                'NAME': 'django',\n                'OPTIONS': {\n                    'libraries': {\n                        'invalid': [1, 2, 3]\n                    },\n                },\n            })\n\n    \n", "    def test_custom_template_loaders(self):\n        custom_loaders = [\n            'template_backends.loaders.CustomLoader',\n        ]\n        engine = DjangoTemplates({\n            'DIRS': [],\n            'APP_DIRS': True,\n            'NAME': 'django',\n            'OPTIONS': {\n                'template_loaders': custom_loaders,\n            },\n        })\n        self.assertEqual(engine.engine.loaders, [('template_backends.loaders.CustomLoader', [])])\n", "    def test_custom_template_loaders(self):\n        custom_loaders = [\n            'my_app.template_loaders.MyCustomLoader',\n        ]\n        engine = DjangoTemplates({'DIRS': [], 'APP_DIRS': True, 'NAME': 'django', 'OPTIONS': {'loaders': custom_loaders}})\n        self.assertEqual(engine.engine.loaders, custom_loaders)\n", "    def test_template_context_processors(self):\n        engine = DjangoTemplates({\n            'DIRS': [],\n            'APP_DIRS': False,\n            'NAME': 'django',\n            'OPTIONS': {\n                'context_processors': [\n                    lambda context: {'test_var': 'test_value'},\n                ],\n            },\n        })\n        template = engine.from_string('{{ test_var }}')\n        context = {}\n        result = template.render(context)\n        self.assertEqual(result, 'test_value')\n\n"], "sample_870": ["    def test_custom_kernel_does_not_modify_X(self):\n        kernel = CustomKernel(1.0)\n        X = np.arange(10).reshape(5, 2)\n        kernel.diag(X)\n        assert np.array_equal(X, np.arange(10).reshape(5, 2))\n\n\n", "compilation error", "compilation error", "    def test_custom_kernel_input_not_modified(self):\n        kernel = CustomKernel(1.0)\n        X = np.random.rand(5, 3)\n        X_ = kernel.diag(X)\n        assert_array_equal(X, X_)\n", "    def test_kernel_not_modified_during_fit():\n        kernel = CustomKernel()\n        X = np.array([[1, 2], [3, 4]])\n        gpr = GaussianProcessRegressor(kernel=kernel)\n        gpr.fit(X, y)\n        assert np.all(X == np.array([[1, 2], [3, 4]]))\n"], "sample_354": ["    def test_custom_permissions(self):\n        custom_permissions = [\n            ('my_custom_permission', 'Some permission'),\n            ('another_custom_permission', 'Another permission'),\n        ]\n        Permission._meta.permissions = custom_permissions\n        create_permissions(self.app_config, verbosity=0)\n\n        self.assertEqual(Permission.objects.count(), len(custom_permissions))\n        for permission in custom_permissions:\n            self.assertIn(permission, Permission.objects.values_list('codename', flat=True))\n\n\n\n", "    def test_create_permissions_with_custom_permissions(self):\n        custom_permissions = [\n            ('special_permission', 'A special permission'),\n        ]\n        Permission._meta.permissions = custom_permissions\n        create_permissions(self.app_config, verbosity=0)\n        self.assertEqual(Permission.objects.count(), 1)\n        self.assertEqual(Permission.objects.get().codename, 'special_permission')\n\n\n\n", "    def test_create_permissions_with_custom_permissions(self):\n        \"\"\"\n        Custom permissions should be created along with default permissions.\n        \"\"\"\n        custom_permissions = [\n            ('my_custom_permission', 'Some custom permission'),\n            ('another_custom_permission', 'Another custom permission'),\n        ]\n        Permission._meta.permissions = custom_permissions + self._original_permissions\n        create_permissions(self.app_config, verbosity=0)\n\n        self.assertEqual(\n            Permission.objects.count(),\n            len(custom_permissions) + len(self._original_permissions)\n        )\n        for permission_name, permission_display in custom_permissions:\n            self.assertEqual(\n                Permission.objects.get(codename=permission_name).name,\n                permission_display\n            )\n\n", "    def test_create_permissions_with_custom_permissions(self):\n        \"\"\"\n        create_permissions should include custom permissions when specified.\n        \"\"\"\n        custom_permissions = (\n            ('my_custom_permission', 'My custom permission'),\n        )\n        Permission._meta.permissions = custom_permissions + self._original_permissions\n        create_permissions(self.app_config, verbosity=0)\n        self.assertEqual(\n            Permission.objects.all().count(),\n            len(custom_permissions) + len(self._original_permissions)\n        )\n", "    def test_create_permissions_with_custom_permissions(self):\n        custom_perms = [\n            ('my_custom_permission', 'A custom permission'),\n            ('another_custom_permission', 'Another custom permission'),\n        ]\n        Permission._meta.permissions = custom_perms\n        create_permissions(self.app_config, verbosity=0)\n        self.assertEqual(Permission.objects.count(), len(custom_perms))\n\n        # Ensure the custom permissions were created with the correct names and labels\n        for i, perm in enumerate(custom_perms):\n            self.assertEqual(Permission.objects.get(codename=perm[0]).name, perm[1])\n\n\n\n"], "sample_545": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1150": ["def test_issue_17858():\n    assert 1 in Range(1, oo)\n    assert 1 in Range(1, oo, 1)\n    assert 1 not in Range(oo, 1, -1)\n    assert 1 not in Range(oo, 1)\n    assert oo in Range(oo, oo)\n    assert -oo in Range(-oo, -oo)\n", "compilation error", "def test_issue_18859():\n    assert ImageSet(Lambda(x, x**2), S.Reals).is_subset(S.Reals)\n    assert ImageSet(Lambda(x, x**2), S.Naturals).intersection(S.Naturals) == ImageSet(Lambda(x, x**2), S.Naturals)\n", "def test_issue_19017():\n    from sympy import Symbol, Interval, image_set, simplify\n    x, y = Symbol('x'), Symbol('y')\n    A = ImageSet(Lambda(x, x**2), Interval(-2, 2))\n\n    assert set(simplify(a) for a in A) == set(a**2 for a in interval(-2, 2))\n", "def test_issue_17985():\n    s = S.Naturals\n    t = S.Reals\n    assert s.intersection(t) == s\n    assert s.union(t) == t\n    assert s.difference(t) == S.EmptySet\n    assert t.intersection(s) == s\n    assert t.union(s) == t\n    assert t.difference(s) == S.Reals - S.Naturals\n"], "sample_10": ["    def test_remove_columns_with_generator_invalid():\n        t = table.table_helpers.simple_table(1)\n        with pytest.raises(ValueError, match=\"generator should yield column names\"):\n            t.remove_columns(lambda col: col > 1)\n", "def test_replace_with_invalid_type():\n    with pytest.raises(TypeError, match='Column type mismatch'):\n        t = table.Table()\n        t['a'] = [1, 2, 3]  # Int column\n        t['b'] = [[1, 2], [3, 4]]  # Invalid type for replacing column\n", "def test_sort_with_masked_array():\n    \"\"\"Test sorting a table that has a masked column.\n    \"\"\"\n    data = [[1, 2],\n            [3, np.nan],\n            [4, 5]]\n\n    t = table.Table(data, names=['a', 'b'])\n    t['b'] = np.ma.masked_array(t['b'], mask=[False, True, False])\n\n    t.sort('a')\n    assert np.all(t['a'] == [1, 3, 4])\n    assert np.all(t['b'] == [2, np.nan, 5])\n    assert np.all(t['b'].mask == [False, True, False])\n", "def test_add_column_same_name_existing_qtable():\n    \"\"\"Test adding a column with the same name as an existing column in a QTable.\"\"\"\n    t = table.QTable([[1, 2], [3, 4]], names=('a', 'b'))\n    t['c'] = [5, 6]\n    with pytest.raises(ValueError, match=r\"Column 'c' already exists\"):\n        t['c'] = [7, 8]\n\n    # Test re-assigning a QTable column with the same name\n    t['c'] = [9, 10]\n    assert np.all(t['c'] == [9, 10])\n\n\n", "def test_remove_all_columns():\n    t = table.table_helpers.simple_table(1)\n    t.remove_columns(t.colnames)\n    assert len(t.colnames) == 0\n\n\n"], "sample_1109": ["def test_issue_18689():\n    assert floor(floor(floor(x)) + 3) == floor(x) + 3\n    assert ceiling(ceiling(ceiling(x)) + 1) == ceiling(x) + 1\n    assert ceiling(ceiling(floor(x)) + 3) == floor(x) + 3\n\n", "compilation error", "def test_issue_18712():\n    assert floor(x + I*y).rewrite(floor) == floor(x + I*y)\n    assert ceiling(x + I*y).rewrite(ceiling) == ceiling(x + I*y)\n\n\n", "def test_floor_ceiling_with_symbols():\n    x, y, z = symbols('x,y,z')\n    assert floor(x + y + z).subs({x: 1, y: 2, z: 3}) == 6\n    assert ceiling(x + y + z).subs({x: 1, y: 2, z: 3}) == 6\n    assert floor(x*y*z).subs({x: 2, y: 3, z: 4}) == 24\n    assert ceiling(x*y*z).subs({x: 2, y: 3, z: 4}) == 24\n    assert floor(x*y/z).subs({x: 2, y: 3, z: 4}) == 1\n    assert ceiling(x*y/z).subs({x: 2, y: 3, z: 4}) == 1\n", "def test_issue_19068():\n    x, y = symbols('x,y')\n    assert floor(x*y).nseries(x, y, 100) == floor(y)*floor(x)\n    assert ceiling(x*y).nseries(x, y, 100) == ceiling(y)*ceiling(x)\n"], "sample_717": ["def test_load_fake_lfw_pairs():\n    lfw_pairs = fetch_lfw_pairs(subset='train', data_home=SCIKIT_LEARN_DATA,\n                download_if_missing=False)\n\n    # The data is in the shape of (n_samples, 2, 62, 47) due to the two\n    # images in every sample\n    assert_equal(lfw_pairs.pairs.shape, (10, 2, 62, 47))\n    assert_equal(lfw_pairs.data.shape, (10, 5828))\n    assert_array_equal(lfw_pairs.target, [1, 1, 0, 0, 1, 0, 1, 0, 0, 1])\n\n    assert_equal(lfw_pairs.target_names, ['Different persons', 'Same person'])\n\n", "def test_load_fake_lfw_pairs():\n    lfw_pairs = fetch_lfw_pairs(subset='train', data_home=SCIKIT_LEARN_DATA,\n                                 download_if_missing=False)\n\n    assert_equal(lfw_pairs.data.shape, (10, 5828))\n    assert_equal(lfw_pairs.pairs.shape, (10, 2, 62, 47))\n    assert_array_equal(lfw_pairs.target, [1, 1, 0, 0, 1, 0, 1, 1, 0, 1])\n\n    assert_array_equal(lfw_pairs.target_names, ['Different persons', 'Same person'])\n", "def test_load_fake_lfw_pairs():\n    lfw_pairs = fetch_lfw_pairs(subset='train', data_home=SCIKIT_LEARN_DATA,\n                                 download_if_missing=False)\n\n    assert_equal(lfw_pairs.data.shape, (10, 5828))\n    assert_equal(lfw_pairs.pairs.shape, (10, 2, 62, 47))\n    assert_array_equal(lfw_pairs.target,\n                       [0, 0, 1, 0, 0, 0, 1, 1, 0, 1])\n    assert_equal(len(lfw_pairs.target_names), 3)\n\n", "def test_load_fake_lfw_pairs():\n    lfw_pairs = fetch_lfw_pairs(data_home=SCIKIT_LEARN_DATA, subset='train',\n                                 min_faces_per_person=3,\n                                 download_if_missing=False)\n\n    # Check that the loaded data has the expected shape\n    assert_equal(lfw_pairs.data.shape, (10, 5828))\n    assert_equal(lfw_pairs.pairs.shape, (10, 2, 62, 47))\n    assert_equal(lfw_pairs.target.shape, (10,))\n\n    # Check that the target labels are as expected\n    expected_labels = [1, 0, 1, 0, 1, 0, 1, 0, 0, 1]\n    assert_array_equal(lfw_pairs.target, expected_labels)\n", "def test_load_lfw_pairs_subset():\n    for subset in ['train', 'test', '10_folds']:\n        lfw_pairs = fetch_lfw_pairs(subset=subset, data_home=SCIKIT_LEARN_DATA,\n                                     download_if_missing=False)\n\n        assert_equal(lfw_pairs.data.shape[1], 5828)\n        assert_equal(lfw_pairs.pairs.shape[2], 62)\n        assert_equal(lfw_pairs.pairs.shape[3], 47)\n        assert_equal(lfw_pairs.target.shape[0], lfw_pairs.data.shape[0])\n\n"], "sample_3": ["def test_writing_complex_subtypes():\n    t = Table()\n    t['a'] = np.array([1+1j, 2+2j], dtype=np.complex128)\n    out = StringIO()\n    t.write(out, format='ascii.ecsv')\n    txt = \"\"\"\n    # %ECSV 1.0\n    # ---\n    # datatype:\n    # - {name: a, datatype: complex}\n    # schema: astropy-2.0\n    a\n    (1+1j)\n    (2+2j)\n    \"\"\"\n    hdr = _get_ecsv_header_dict(out.getvalue())\n    hdr_exp = _get_ecsv_header_dict(txt)\n    assert hdr == hdr_exp\n    t2 = Table.read(out.getvalue(), format='ascii.ecsv')\n    assert t2['a'].dtype == np.complex128\n    assert np.all(t2['a'] == t['a'])\n", "def test_unicode_columns():\n    \"\"\"Test writing and reading ECSV files with unicode characters.\"\"\"\n    t = Table()\n    t['a'] = np.array(['\u4f60\u597d', '\u4e16\u754c', '\u3053\u3093\u306b\u3061\u306f'], dtype='U10')\n    out = StringIO()\n    t.write(out, format='ascii.ecsv')\n    t2 = Table.read(out.getvalue(), format='ascii.ecsv')\n    assert np.all(t['a'] == t2['a'])\n", "def test_complex_with_null_values():\n    \"\"\"Test handling of complex numbers with null values in subtypes.\"\"\"\n    t = Table()\n    t['c'] = np.ma.array([1 + 1j, 2 + 2j, np.nan],\n                         dtype=np.complex64, mask=[False, False, True])\n    out = StringIO()\n    t.write(out, format='ascii.ecsv')\n    t2 = Table.read(out.getvalue(), format='ascii.ecsv')\n    assert t2['c'].dtype == t['c'].dtype\n    assert np.all(t2['c'] == t['c'])\n    assert np.all(t2['c'].mask == t['c'].mask)\n\n", "def test_complex_masked_arrays():\n    t = Table()\n    t['a'] = np.ma.array([[1+1j, 2+2j], [3+3j, 4+4j]], mask=[[True, False], [False, True]], dtype=np.complex128)\n    out = StringIO()\n    t.write(out, format='ascii.ecsv')\n    t2 = Table.read(out.getvalue(), format='ascii.ecsv')\n    assert t2['a'].dtype.type is np.complex128\n    assert np.all(t2['a'] == t['a'])\n", "def test_read_complex_with_subtypes():\n    \"\"\"Test reading complex values with subtypes.\"\"\"\n    txt = \"\"\"\\"], "sample_604": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1073": ["def test_issue_5653_b():\n    assert sqrtdenest(sqrt(2 + sqrt(2 + sqrt(3)))) == sqrt(2 + sqrt(2 + sqrt(3)))\n\n\n", "def test_sqrt_biquadratic_denest():\n    assert sqrt_biquadratic_denest(sqrt((2*sqrt(2) + 4)*sqrt(2 + sqrt(2)) + 5*sqrt(2) + 8), a=None, b=None, r=None, d2=None) == sqrt(2) + sqrt(sqrt(2) + 2) + 2\n", "def test_sqrtdenest_complex():\n    assert sqrtdenest(sqrt(2 + sqrt(3) * I)).simplify() == sqrt(2 + sqrt(3) * I)\n    assert sqrtdenest(sqrt(2 + 2 * I)).simplify() == sqrt(2 + 2 * I)\n    assert sqrtdenest(sqrt(2 - 2 * I)).simplify() == sqrt(2 - 2 * I)\n    e = sqrt(2 + 2 * I)\n    assert sqrtdenest(e) == e\n    e = sqrt(2 - 2 * I)\n    assert sqrtdenest(e) == e\n    e = sqrt(1 - sqrt(2) * I)\n    assert sqrtdenest(e) == e\n    e = sqrt(1 + sqrt(2) * I)\n    assert sqrtdenest(e) == e\n    \n", "def test_issue_17464():\n    assert sqrtdenest(sqrt(2*r10 + 4*r2*sqrt(-2*r10 + 11) + 10)) == \\\n        sqrt(-2*r10 - 4*r2 + 8*r5 + 10)\n", "def test_issue_12420_cont():\n    assert sqrtdenest(sqrt(2*I + sqrt(4 + I))) == sqrt(2*I + sqrt(4 + I))\n    e = 2 + sqrt(2 + sqrt(2*I))\n    assert sqrtdenest(e) == e\n"], "sample_1177": ["def test_issue_23372():\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n    assert Abs(x + I*y).rewrite(exp_polar).args[0] == sqrt(x**2 + y**2)\n    assert Abs(z).rewrite(exp_polar).args[0] == Abs(z)\n    assert Abs(x + I*y + z).rewrite(exp_polar).args[0] == sqrt((x + Re(z))**2 + (y + Im(z))**2) \n", "def test_issue_23251():\n    x = Symbol('x', positive=True)\n    assert Abs(x**2) == x**2\n\n\n", "def test_issue_22297():\n    from sympy.simplify.simplify import simplify\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    assert simplify(Abs(x)**2 - x**2) == 0\n    assert simplify(Abs(x)**2 - x**2, nonneg=True) == 0\n    assert simplify(Abs(x + y)**2 - (x + y)**2, nonneg=True) == 0\n    assert simplify(Abs(x + y)**2 - (x + y)**2) == 0\n\n\n", "def test_issue_18791():\n    x = Symbol('x')\n    assert Abs(x**2).rewrite(Abs) == Abs(x**2)\n    assert Abs(-x**2).rewrite(Abs) == Abs(x**2)\n    assert (Abs(x**2).subs(x, I)).is_real is True\n", "def test_issue_23412():\n    from sympy import Symbol, simplify, Abs\n    x, y = symbols('x y')\n    assert simplify(Abs(x + I*y) * Abs(x - I*y)) == (x**2 + y**2)\n\n\n"], "sample_300": ["    def test_values(self):\n        query = Query(Item)\n        query.set_values(['id', 'name', 'modified'])\n        self.assertEqual(query.values_select, ('id', 'name', 'modified'))\n", "    def test_annotate_select_related(self):\n        query = Query(Item)\n        query.add_select_related('creator')\n        query.add_annotation('author_count', Func('creator__num', alias='author_count'))\n        clone = query.clone()\n        clone.add_select_related('note')\n        self.assertEqual(query.select_related, {'creator': {}})\n        self.assertEqual(clone.select_related, {'creator': {}, 'note': {}})\n\n", "    def test_filter_conditional_join_via_f(self):\n        query = Query(Item)\n        filter_expr = F('note__note')\n        where = query.build_where(filter_expr > 0)\n        lookup = where.children[0]\n        self.assertIsInstance(lookup, GreaterThan)\n        self.assertIsInstance(lookup.lhs, F)\n        self.assertEqual(lookup.lhs.target, 'note__note')\n        self.assertEqual(lookup.rhs, 0)\n", "    def test_filter_conditional_lookup(self):\n        query = Query(Author, alias_cols=False)\n        filter_expr = Func(output_field=BooleanField())\n        where = query.build_where(Q(name__endswith=filter_expr))\n        lookup = where.children[0]\n        self.assertIsInstance(lookup, Exact)\n        self.assertIsInstance(lookup.lhs, Col)\n        self.assertIsNone(lookup.lhs.alias)\n        self.assertEqual(lookup.lhs.target, Author._meta.get_field('name'))\n        self.assertIsInstance(lookup.rhs, Func)\n", "    def test_filter_conditional_join_with_alias(self):\n        query = Query(Item)\n        query.add_related_objects(lookups=[('note', 'note')])\n        filter_expr = Func('note__note', output_field=BooleanField())\n        where = query.build_where(filter_expr)\n        exact = where.children[0]\n        self.assertIsInstance(exact, Exact)\n        self.assertIsInstance(exact.lhs, Func)\n        self.assertIs(exact.rhs, True)\n\n\n\n"], "sample_149": ["    def test_builtin_permission_name_clash(self):\n        class Checked(models.Model):\n            class Meta:\n                permissions = [\n                    ('add_group', 'Can add group'),\n                    ('change_group', 'Can change group'),\n                ]\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The permission codenamed 'add_group' clashes with a builtin \"\n                \"permission for model 'auth_tests.Checked'.\",\n                obj=Checked,\n                id='auth.E005',\n            ),\n            checks.Error(\n                \"The permission codenamed 'change_group' clashes with a builtin \"\n                \"permission for model 'auth_tests.Checked'.\",\n                obj=Checked,\n                id='auth.E005',\n            ),\n        ])       \n", "    def test_builtin_permissions_max_length(self):\n        class Checked(models.Model):\n            class Meta:\n                verbose_name = 'some ridiculously long verbose name that is out of control' * 5\n                permissions = [\n                    ('change_checked', 'Can edit permission'),\n                    ('add_checked', 'Can add permission'),\n                ]\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The verbose_name of model 'auth_tests.Checked' must be at most 244 \"\n                \"characters for its builtin permission names to be at most 255 characters.\",\n                obj=Checked,\n                id='auth.E007',\n            ),\n        ])\n", "    def test_permission_name_length_with_non_ascii(self):\n        class Checked(models.Model):\n            class Meta:\n                permissions = [\n                    ('my_custom_permission', 'Some permission with \\u00e9'),\n                ]\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [])\n\n", "    def test_builtin_permission_names_length(self):\n        class Checked(models.Model):\n            class Meta:\n                app_label = 'auth_tests'  \n                verbose_name = 'Some ridiculously long verbose name that is out of control' * 5\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The verbose_name of model 'auth_tests.Checked' must be at most 244 \"\n                \"characters for its builtin permission names to be at most 255 characters.\",\n                obj=Checked,\n                id='auth.E007',\n            ),\n        ])\n", "    def test_builtin_permissions_max_length(self):\n        class Checked(models.Model):\n            class Meta:\n                verbose_name = 'some ridiculously long verbose name that is out of control' * 5\n                permissions = [\n                    ('can_change_checked', 'Can edit permission'),\n                ]\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The verbose_name of model 'auth_tests.Checked' must be at most 244 \"\n                \"characters for its builtin permission names to be at most 255 characters.\",\n                obj=Checked,\n                id='auth.E007',\n            ),\n        ])\n"], "sample_402": ["    def test_etag_modified(self):\n        \"\"\"\n        GZipMiddleware correctly generates a new ETag when the content changes.\n        \"\"\"\n\n        @override_settings(DEBUG=True)\n            response = HttpResponse(self.compressible_string)\n            response.headers[\"ETag\"] = '\"eggs\"'\n            return response\n\n        request = self.rf.get(\"/\", HTTP_ACCEPT_ENCODING=\"gzip, deflate\")\n        response1 = GZipMiddleware(get_response)(request)\n        etag1 = response1.headers[\"ETag\"]\n\n        # Modify the content\n        self.compressible_string = self.compressible_string + b\" \"\n\n        response2 = GZipMiddleware(get_response)(request)\n        etag2 = response2.headers[\"ETag\"]\n\n        self.assertNotEqual(etag1, etag2)\n", "    def test_compress_json(self):\n        \"\"\"\n        Compression is performed on responses with JSON content.\n        \"\"\"\n        import json\n\n        self.resp.content = json.dumps({\"data\": \"Hello, World!\"}).encode(\"utf-8\")\n        self.resp[\"Content-Type\"] = \"application/json; charset=UTF-8\"\n        r = GZipMiddleware(self.get_response)(self.req)\n        self.assertEqual(\n            json.loads(self.decompress(r.content)), {\"data\": \"Hello, World!\"}\n        )\n        self.assertEqual(r.get(\"Content-Encoding\"), \"gzip\")\n", "    def test_dont_set_etag_if_not_set(self):\n        \"\"\"\n        If the response doesn't set an ETag, GZipMiddleware shouldn't\n        set one either.\n        \"\"\"\n\n            response = HttpResponse(self.compressible_string)\n            return response\n\n        request = self.rf.get(\"/\", HTTP_ACCEPT_ENCODING=\"gzip, deflate\")\n\n        gzip_response = GZipMiddleware(get_response)(request)\n        self.assertIsNone(gzip_response.headers.get(\"ETag\"))\n\n\n\n", "    def test_etag_with_gzip_encoding_match(self):\n        \"\"\"\n        GZipMiddleware allows 304 Not Modified responses even with gzip\n        encoding in both requests.\n        \"\"\"\n\n            response = HttpResponse(self.compressible_string)\n            response.headers[\"ETag\"] = '\"eggs\"'\n            response.headers[\"Content-Encoding\"] = \"gzip\"\n            return response\n\n        request = self.rf.get(\"/\", HTTP_ACCEPT_ENCODING=\"gzip, deflate\")\n        response = GZipMiddleware(get_response)(request)\n        gzip_etag = response.headers[\"ETag\"]\n        next_request = self.rf.get(\n            \"/\",\n            HTTP_ACCEPT_ENCODING=\"gzip, deflate\",\n            HTTP_IF_NONE_MATCH=gzip_etag,\n        )\n        next_response = GZipMiddleware(\n            get_response\n        )(next_request)\n        self.assertEqual(next_response.status_code, 304)\n\n\n\n", "    def test_gzip_compression_level(self):\n        \"\"\"\n        The GZipMiddleware uses the GZIP_COMPRESSION_LEVEL setting for\n        determining the compression level.\n        \"\"\"\n        with override_settings(GZIP_COMPRESSION_LEVEL=9):\n            r = GZipMiddleware(self.get_response)(self.req)\n            # Check that we use a higher compression level\n            with gzip.GzipFile(fileobj=BytesIO(r.content)) as f:\n                compressed_data = f.read()\n                # Assert that the compressed data is smaller than the default\n                # compression level\n                self.assertLess(len(compressed_data), len(self.compressible_string))\n\n\n"], "sample_1042": ["def test_Indexed__getitem__str():\n    i, j = symbols('i j', integer=True)\n    A = Indexed('A', i, j)\n    assert str(A) == 'A[i, j]'\n    assert str(A[1]) == 'A[1, j]'\n    assert str(A[i]) == 'A[i, j]'\n", "def test_Indexed_limits():\n    i, j = symbols('i j', integer=True)\n    a = IndexedBase('a')\n    assert a[i].limits((i, 0, 1)) == (a[i] - a[i].subs(i, 0), a[i] - a[i].subs(i, 1))\n    assert a[i].limits((i, 1, 2)) == (a[i] - a[i].subs(i, 1), a[i] - a[i].subs(i, 2))\n\n\n\n", "def test_Indexed_limits_of_sum():\n    i, j, k = symbols('i j k', integer=True)\n    A = IndexedBase('A')\n    sum_a = Sum(A[i], (i, 0, 5))\n    assert sum_a.limits == (i, 0, 5)\n    assert sum_a.func(*sum_a.args) == sum_a\n\n", "def test_Indexed_subs_with_functions():\n    A = IndexedBase(\"A\")\n    i, j, k = symbols(\"i,j,k\")\n    x = symbols(\"x\")\n    f = Function(\"f\")\n\n    assert Subs(A[i], A[i], f(x)).diff(A[j]) == 0\n    assert Subs(A[i], A[i], f(x)).diff(x) == Derivative(f(x), x) * KroneckerDelta(i, j)\n    assert Subs(A[i] * x, x, f(x)).diff(A[j]) == A[j] * f(x) + A[i] * Derivative(f(x), x) * KroneckerDelta(i, j)\n    assert Subs(A[i] * x, A[i], f(x)).diff(A[j]) == f(x) * A[j] + A[i] * Derivative(f(x), x) * KroneckerDelta(i, j) \n    assert Subs(A[i] * x, x, f(x)).diff(x) == A[i] * Derivative(f(x), x)\n    assert Subs(A[i] * x, A[i], f(x)).diff(A[j]) == Derived(f(x), x) * A[j] + A[i] * Derivative(f(x), x) * KroneckerDelta(i, j)\n    assert Subs(A[i] * f(x), A[i], f(x)).diff(A[j]) == f(x) * A[j] + A[i] * Derivative(f(x), x)\n", "def test_Indexed_evalf():\n    i, j = symbols('i j', integer=True)\n    A = Indexed('A', i, j)\n    assert A.evalf(subs={A: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}) ==  A.subs({A: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}).evalf()\n\n    assert A.evalf(subs={i: 1, j: 2}) == 3.0\n"], "sample_706": ["def test_identifier_interpolation() -> None:\n    matcher = {\"foo\": lambda ident: ident == \"bar\"}\n    assert not evaluate(\"foo\", matcher)\n    assert evaluate(\"ident\", matcher) is not None\n\n\n", "    def test_ident_with_true_false(expr: str, expected: bool) -> None:\n        matcher = {\n            r\"$ident\": lambda ident: True\n        }.__getitem__\n        assert evaluate(expr, matcher) is expected        \n", "def test_nested_identifiers(expr: str, expected: bool) -> None:\n    matcher = {\"ident\": True}.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "def test_parentheses_precedence(expr: str) -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(expr, matcher)\n", "def test_nested_identifiers(expr: str, matcher: dict, expected: bool) -> None:\n    assert evaluate(expr, MatcherAdapter(lambda ident: matcher.get(ident, False))) is expected\n"], "sample_1100": ["compilation error", "compilation error", "def test_issue_15197():\n    p = symbols('p', positive=True)\n    x = symbols('x')\n    assert (p**x).as_real_imag() == (p**x, 0)\n    assert (p**(x + 1j)).as_real_imag() != (p**(x + 1j), 0)\n", "compilation error", "def test_issue_16021():\n    x, y = symbols('x y', real=True)\n    assert (x**2 + y**2).is_positive is None\n    assert (x**2 + y**2).is_nonnegative\n    assert (x**2 + y**2).is_nonpositive is None\t\n"], "sample_497": ["    def test_datetime_formatter():\n        # Test the datetime formatter for various formats.\n        fmts = [\n            '%Y-%m-%d %H:%M:%S',\n            '%Y-%m-%d',\n            '%H:%M:%S',\n            '%Y/%m/%d',\n            '%b %d, %Y'\n        ]\n        for fmt in fmts:\n            formatter = mticker.DateFormatter(fmt)\n            # Create a dummy datetime object\n            dt = datetime(2023, 10, 27, 10, 30, 0)\n            assert formatter.format(dt) == dt.strftime(fmt)\n\n", "def test_multiple_locales():\n    for fmt in [\n            mticker.EngFormatter(), mticker.PercentFormatter(),\n            mticker.FormatStrFormatter('%d'),\n    ]:\n        for locale in ['en_US', 'de_DE']:\n            with locale_context(locale):\n                assert fmt(1234.56789) == fmt(1234.56789)  # No change in output\n\n\n", "    def test_format_number(self, input, expected):\n        formatter = mticker.ScalarFormatter()\n        assert formatter.format_number(input, 2) == expected\n", "    def test_minorticks_rc_params(xminor, yminor, expect_xminor, expect_yminor):\n        fig = plt.figure()\n        rc = {'xtick.minor.visible': xminor,\n              'ytick.minor.visible': yminor}\n        with plt.rc_context(rc=rc):\n            ax = fig.add_subplot(1, 1, 1)\n        assert len(ax.xaxis.get_minor_ticks()) > 0 == expect_xminor\n        assert len(ax.yaxis.get_minor_ticks()) > 0 == expect_yminor\n", "    def test_LogFormatter_with_negative_numbers():\n        # test behavior around zero and negative values\n        fmt = mticker.LogFormatter()\n        assert fmt(-1) == '-1'  \n        assert fmt(0.1) == '0.1'\n        assert fmt(1e-6) == '1e-06'  \n        assert fmt(-1e-6) == '-1e-06'  \n        assert fmt(9.999) == '9.999'\n        assert fmt(10) == '10'\n        assert fmt(100) == '100'\n\n\n\n"], "sample_518": ["def test_default_joinstyle():\n    patch = Patch()\n    assert patch.get_joinstyle() == 'miter'\n", "def test_default_joinstyle():\n    patch = Patch()\n    assert patch.get_joinstyle() == 'miter'\n", "def test_default_joinstyle():\n    patch = Patch()\n    assert patch.get_joinstyle() == 'miter'\n", "def test_path_patch_transform():\n    fig, ax = plt.subplots()\n    verts = np.array([[0, 0], [1, 0], [1, 1], [0, 1]])\n    codes = [mpath.Path.MOVETO, mpath.Path.LINETO, mpath.Path.LINETO, mpath.Path.CLOSEPOLY]\n    path = mpath.Path(verts, codes)\n    patch = mpatches.PathPatch(path, facecolor='none', edgecolor='k', lw=2)\n    ax.add_patch(patch)\n    ax.set_xlim(-0.2, 1.2)\n    ax.set_ylim(-0.2, 1.2)\n    ax.set_aspect('equal')\n\n", "def test_default_joinstyle():\n    patch = Patch()\n    assert patch.get_joinstyle() == 'miter'\n"], "sample_734": ["compilation error", "compilation error", "def test_fowlkes_mallows_score_empty_labels():\n    assert_equal(fowlkes_mallows_score([], []), 0.0)\n    assert_equal(fowlkes_mallows_score([0], []), 0.0)\n    assert_equal(fowlkes_mallows_score([], [0]), 0.0)\n\n\n", "compilation error", "def test_contingency_matrix_large_arrays():\n    n_samples = 10000\n    n_clusters = 10\n    labels_a = np.random.randint(0, n_clusters, size=n_samples)\n    labels_b = np.random.randint(0, n_clusters, size=n_samples)\n\n    # Test with sparse contingency matrix\n    C_sparse = contingency_matrix(labels_a, labels_b, sparse=True)\n    assert_equal(C_sparse.shape, (n_clusters, n_clusters))\n\n    # Test with dense contingency matrix\n    C_dense = contingency_matrix(labels_a, labels_b)\n    assert_equal(C_dense.shape, (n_clusters, n_clusters))\n\n"], "sample_237": ["    def test_permission_codename_uniqueness(self):\n        class Checked(models.Model):\n            class Meta:\n                permissions = [\n                    ('edit_checked', 'Can edit checked'),\n                    ('another_edit_checked', 'Can edit checked'),\n                ]\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The permission codenamed 'another_edit_checked' is duplicated for \"\n                \"model 'auth_tests.Checked'.\",\n                obj=Checked,\n                id='auth.E006',\n            ),\n        ])\n", "    def test_partial_unique_username_warning(self):\n        class CustomUserPartiallyUnique(AbstractBaseUser):\n            username = models.CharField(max_length=30)\n            USERNAME_FIELD = 'username'\n\n            class Meta:\n                constraints = [\n                    UniqueConstraint(\n                        fields=['username'],\n                        name='partial_username_unique',\n                        condition=Q(password__isnull=False),\n                    ),\n                ]\n\n        with self.settings(AUTHENTICATION_BACKENDS=['my.custom.backend']):\n            errors = checks.run_checks(app_configs=self.apps.get_app_configs())\n            self.assertEqual(errors, [\n                checks.Warning(\n                    \"'CustomUserPartiallyUnique.username' is named as the \"\n                    \"'USERNAME_FIELD', but it is not unique.\",\n                    hint=(\n                        'Ensure that your authentication backend(s) can '\n                        'handle non-unique usernames.'\n                    ),\n                    obj=CustomUserPartiallyUnique,\n                    id='auth.W004',\n                ),\n            ])\n", "    def test_permissions_order(self):\n        class Checked(models.Model):\n            class Meta:\n                permissions = [\n                    ('permission_3', 'Permission 3'),\n                    ('permission_1', 'Permission 1'),\n                    ('permission_2', 'Permission 2'),\n                ]\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [])\n", "    def test_missing_unique_constraint(self):\n        class Checked(models.Model):\n            username = models.CharField(max_length=30)\n            USERNAME_FIELD = 'username'\n\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"'Checked.username' must be unique because it \"\n                \"is named as the 'USERNAME_FIELD'.\",\n                obj=Checked,\n                id='auth.E003',\n            ),\n        ])\n\n\n", "    def test_permission_names_unique_within_model(self):\n        class Checked(models.Model):\n            class Meta:\n                permissions = [\n                    ('change_checked', 'Can edit permission'),\n                    ('change_checked', 'Can edit permission (duplicate)'),\n                ]\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The permission codenamed 'change_checked' is duplicated for \"\n                \"model 'auth_tests.Checked'.\",\n                obj=Checked,\n                id='auth.E006',\n            ),\n        ])\n"], "sample_858": ["compilation error", "def test_voting_classifier_with_custom_class_weight():\n\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n\n    # Define a custom class weight\n    class_weights = {0: 0.2, 1: 0.8}\n    eclf = VotingClassifier(estimators=[\n        ('lr', clf1), ('rf', clf2)],\n        voting='soft',\n        weights=[1, 1],\n        class_weight=class_weights).fit(X, y)\n\n    \n    # Check that class weights are properly passed to the individual estimators\n    assert clf1.get_params()[\"class_weight\"] == class_weights\n    assert clf2.get_params()[\"class_weight\"] == class_weights\n    assert eclf.get_params()[\"class_weight\"] == class_weights\n\n", "compilation error", "compilation error", "compilation error"], "sample_79": ["    def test_datetime_formats(self):\n        from django.utils import timezone\n\n        now = timezone.now()\n        self.assertEqual(datetime.now().strftime('%Y-%m-%d'), datetime.now().strftime('%Y-%m-%d'))\n        self.assertEqual(datetime.now().strftime('%m/%d/%Y'), datetime.now().strftime('%m/%d/%Y'))\n        self.assertEqual(datetime.now().strftime('%b %d, %Y'), datetime.now().strftime('%b %d, %Y'))\n\n", "    def test_strings(self):\n        self.assertEqual(pluralize('0', 'es'), 'es')\n        self.assertEqual(pluralize('1', 'es'), 'es')\n        self.assertEqual(pluralize('2', 'es'), 'es')\n        self.assertEqual(pluralize('0', 'y,ies'), 'ies')\n        self.assertEqual(pluralize('1', 'y,ies'), 'y')\n        self.assertEqual(pluralize('2', 'y,ies'), 'ies')\n\n\n", "    def test_float_with_decimals(self):\n        self.assertEqual(pluralize(1.5), 's')\n        self.assertEqual(pluralize(2.5), 's')\n", "    def test_strings(self):\n        self.assertEqual(pluralize('1', 's'), '')\n        self.assertEqual(pluralize('0', 's'), 's')\n        self.assertEqual(pluralize('2', 's'), 's')\n", "    def test_str_types(self):\n        self.assertEqual(pluralize(\"0\", ''), 's')\n        self.assertEqual(pluralize(\"1\", ''), '')\n        self.assertEqual(pluralize(\"2\", ''), 's')\n"], "sample_273": ["    def test_inherited_pk_custom_field(self):\n        class Parent(models.Model):\n            id = models.IntegerField(primary_key=True)\n\n        class Child(Parent):\n            pass\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [])\n", "    def test_check_default_auto_field_in_model_with_concrete_fields(self):\n        class Model(models.Model):\n            name = models.CharField(max_length=100)\n            email = models.EmailField()\n\n        with self.subTest(\"django.db.models.AutoField\"):\n            self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [\n                Warning(\n                    self.msg,\n                    hint=self.hint,\n                    obj=Model,\n                    id='models.W042',\n                ),\n            ])\n\n        with self.subTest(\"custom AutoField subclass\"):\n            class CustomAutoField(models.AutoField):\n                pass\n\n            with override_settings(DEFAULT_AUTO_FIELD='CustomAutoField'):\n                self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [])\n\n", "    def test_auto_field_override_in_Meta(self):\n        class Model(models.Model):\n            class Meta:\n                default_auto_field = 'django.db.models.BigAutoField'\n                apps = self.apps\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [])\n\n", "    def test_non_auto_field_pk(self):\n        class Model(models.Model):\n            id = models.CharField(max_length=100, primary_key=True)\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [\n            Error(\n                \"Primary key field 'id' is not an instance of AutoField. \"\n                \"You should use django.db.models.AutoField.\",\n                obj=Model,\n                id='models.E040',\n            ),\n        ])\n", "    def test_auto_created_pk_with_explicit_fields(self):\n        class Model(models.Model):\n            name = models.CharField(max_length=100)\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [\n            Warning(self.msg, hint=self.hint, obj=Model, id='models.W042'),\n        ])\n"], "sample_423": ["    def test_alter_field_ordering_with_respect_to(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.AlterOrderWithRespectTo(\n                    name=\"Author\",\n                    related_name=\"books\",\n                    order_with_respect_to=\"name\",\n                )\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"author_order_by_name\")\n\n\n", "    def test_mixed_operations(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\"Person\", fields=[]),\n                migrations.RemoveField(\"Person\", \"name\"),\n                migrations.RenameField(\"Person\", \"age\", \"birth_year\"),\n                migrations.DeleteModel(\"Animal\"),\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"person_remove_name_rename_age_delete_animal\")\n", "    def test_operation_with_no_suggested_name_and_initial_true(self):\n        class Migration(migrations.Migration):\n            initial = True\n            operations = [migrations.RunSQL(\"SELECT 1 FROM person;\")]\n\n        migration = Migration(\"0001_initial\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"initial\")\n\n", "    def test_create_model_with_options(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    \"Person\",\n                    fields=[\n                        (\"id\", models.AutoField(primary_key=True)),\n                        (\"name\", models.CharField(max_length=200)),\n                    ],\n                    options={\"abstract\": True, \"db_table\": \"persons\"},\n                )\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"person\")\n", "    def test_rename_field_and_index_together_no_rename_conflict(self):\n        \"\"\"\n        When renaming a field involved in index_together, suggest name\n        should avoid conflict with existing fields.\n        \"\"\"\n        initial_author = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n            ],\n            {\n                \"index_together\": {(\"name\",)},\n            },\n        )\n        author_new_constraints = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"title\", models.CharField(max_length=200)),  # Renamed name to title\n            ],\n            {\n                \"index_together\": {(\"title\",)},\n            },\n        )\n        changes = self.get_changes(\n            [initial_author], [author_new_constraints], MigrationQuestioner({\"ask_rename\": True})\n        )\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameField\", \"AlterIndexTogether\"])\n        self.assertOperationAttributes(\n            changes,\n            \"testapp\",\n            0,\n            1,\n            name=\"author\",\n            index_together={(\"title\",)},\n        )\n\n"], "sample_244": ["    def test_non_form_errors_are_propagated_to_formset(self):\n        data = {\n            'choices-TOTAL_FORMS': '1',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-MAX_NUM_FORMS': '1',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '100',\n        }\n        ChoiceFormSet = formset_factory(Choice, max_num=1, validate_max=True)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertFalse(formset1.is_valid())\n        self.assertEqual(formset1.non_form_errors(), ['You may only specify 1 form.'])\n        self.assertEqual(formset1._errors, [{'non_field_errors': ['You may only specify 1 form.']}])\n\n\n\n", "    def test_formset_with_nested_formset(self):\n        class NestedForm(Form):\n            nested_field = CharField()\n\n        class ParentForm(Form):\n            nested_formset = formset_factory(NestedForm, can_delete=False, extra=1)\n\n        ParentFormSet = formset_factory(ParentForm)\n        data = {\n            'form-TOTAL_FORMS': '1',\n            'form-INITIAL_FORMS': '0',\n            'form-0-nested_form-TOTAL_FORMS': '2',\n            'form-0-nested_form-INITIAL_FORMS': '0',\n            'form-0-nested_form-0-nested_field': 'Nested',\n            'form-0-nested_form-1-nested_field': 'Nested',\n        }\n        formset = ParentFormSet(data, auto_id=False)\n        self.assertEqual(formset.is_valid(), True)\n        self.assertEqual(formset.cleaned_data[0]['nested_formset'][0]['nested_field'], 'Nested')\n        self.assertEqual(formset.cleaned_data[0]['nested_formset'][1]['nested_field'], 'Nested')\n\n        self.assertEqual(len(formset.cleaned_data[0]['nested_formset']), 2)\n\n", "    def test_invalid_formsets_with_errors(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '-1',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [\n            {'votes': ['Enter a valid number.']},\n            {},\n        ]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n", "    def test_invalid_with_different_error_messages(self):\n        class FormWithCustomError(Form):\n                raise ValidationError(\"This is a custom error\")\n\n        FormWithCustomErrorSet = formset_factory(FormWithCustomError)\n        formset = FormWithCustomErrorSet(\n            {'form-TOTAL_FORMS': '1', 'form-INITIAL_FORMS': '0'}\n        )\n        self.assertIs(all_valid((formset, formset)), False)\n        self.assertEqual(formset._errors, [{'field': ['This is a custom error']}])\n", "    def test_invalid(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': 'invalid',  \n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [{'votes': ['Enter a valid number.']}, {}]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n\n"], "sample_960": ["def test_python_python_use_unqualified_type_names_enabled(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"n\"><a class=\"reference internal\" href=\"#foo.Name\" '\n            'title=\"foo.Name\"><span class=\"pre\">Name</span></a></span>' in content)\n    assert ('<p><strong>name</strong> (Name) \u2013 blah blah</p>' in content)\n", "def test_py_version(app):\n    text = (\".. pyversion:: 3.7\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"pyversion \"],\n                                [desc_name, \"3.7\"])],\n                desc_content)]))\n    assert '3.7' in domain.objects\n\n\n\n", "def test_breathe_missing_namespace(app):\n    text = \".. py:class:: mymodule.MyClass\\n\"\n    restructuredtext.parse(app, text)\n    domain = app.env.get_domain('py')\n    assert 'mymodule.MyClass' in domain.objects\n    assert domain.objects['mymodule.MyClass']['index'] == ('index', 'mymodule.MyClass', 'class', False) \n", "def test_py_function_arg(app):\n    text = \".. py:function:: my_function\\n\" \\\n           \"    :param int x: blah blah\\n\" \\\n           \"    :param str y: blah blah\\n\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_addname, \"my_function\"],\n                desc_annotation, (list\n                                 ([addnodes.literal_strong, 'x'],\n                                 \" (\",\n                                 [pending_xref, addnodes.literal_emphasis, \"int\"],\n                                 \")\",\n                                 \" -- \",\n                                 \"blah blah\")),\n                                 (addnodes.literal_strong, 'y'],\n                                 \" (\",\n                                 [pending_xref, addnodes.literal_emphasis, \"str\"],\n                                 \")\",\n                                 \" -- \",\n                                 \"blah blah\")])),\n                [desc_content, nodes.field_list, nodes.field]))  \n", "def test_python_python_use_unqualified_type_names_enabled(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"n\"><a class=\"reference internal\" href=\"#foo.Name\" title=\"foo.Name\">'\n            '<span class=\"pre\">Name</span></a></span>' in content)\n    assert '<span class=\"n\"><span class=\"pre\">foo.Age</span></span>' in content\n    assert ('<p><strong>name</strong> (<a class=\"reference internal\" href=\"#foo.Name\" '\n            'title=\"foo.Name\"><em>Name</em></a>) \u2013 blah blah</p>' in content)\n    assert '<p><strong>age</strong> (<em>foo.Age</em>) \u2013 blah blah</p>' in content\n"], "sample_809": ["def test_mutual_info_sparse():\n    rng = check_random_state(0)\n    n_samples = 100\n    n_features = 5\n    X = csr_matrix((rng.rand(n_samples, n_features),\n                    (np.arange(n_samples),\n                     rng.randint(0, n_features, size=n_samples))))\n    y = rng.randint(0, 2, size=n_samples)\n\n    mi_dense = mutual_info_classif(X.toarray(), y)\n    mi_sparse = mutual_info_classif(X, y)\n\n    assert_array_almost_equal(mi_dense, mi_sparse)\n", "def test_mutual_info_regression_sparse():\n    # Test mutual_info_regression with sparse matrix input.\n    rng = check_random_state(0)\n    n_samples = 100\n    n_features = 5\n    X = csr_matrix((rng.rand(n_samples, n_features),\n                    (np.arange(n_samples),\n                     rng.randint(0, n_features, size=n_samples))))\n    y = rng.rand(n_samples)\n\n    mi = mutual_info_regression(X, y, random_state=0)\n    assert_array_equal(mi.dtype, np.float64)\n\n\n\n", "def test_mutual_info_sparse():\n    # Test mutual information with sparse matrices.\n    n_samples = 100\n    n_features = 5\n\n    # Create a sparse matrix with a few non-zero entries\n    X = csr_matrix((np.random.rand(n_samples, n_features),\n                     (np.random.randint(0, n_samples, size=n_samples),\n                      np.random.randint(0, n_features, size=n_samples))),\n                    shape=(n_samples, n_features))\n    y = np.random.randint(0, 2, size=n_samples)\n\n    mi_discrete_features = mutual_info_classif(X, y, discrete_features='auto',\n                                              random_state=0)\n    mi_discrete_features_explicit = mutual_info_classif(X, y,\n                                                      discrete_features=True,\n                                                      random_state=0)\n\n    # Assert that the results are the same\n    assert_array_equal(mi_discrete_features, mi_discrete_features_explicit)\n\n    mi_regression = mutual_info_regression(X, y, random_state=0)\n    assert_array_equal(mi_regression.shape, (n_features,))\n\n", "def test_mutual_info_estimate_sparse_data():\n    rng = check_random_state(0)\n    n_samples = 100\n    n_features = 5\n    X = csr_matrix((rng.rand(n_samples, n_features),\n                    (range(n_samples), range(n_features))))\n    y = rng.rand(n_samples)\n\n    mi_dense = mutual_info_regression(X.toarray(), y, random_state=0)\n    mi_sparse = mutual_info_regression(X, y, random_state=0)\n    assert_array_almost_equal(mi_dense, mi_sparse)\n", "def test_mutual_info_classif_sparse():\n    rng = check_random_state(0)\n    n_samples = 100\n    n_features = 5\n    X = csr_matrix(rng.rand(n_samples, n_features))\n    y = (rng.rand(n_samples) > 0.5).astype(int)\n\n    mi_sparse = mutual_info_classif(X, y, discrete_features='auto')\n    mi_dense = mutual_info_classif(X.toarray(), y, discrete_features='auto')\n\n    assert_array_equal(mi_sparse, mi_dense) \n"], "sample_1143": ["def test_issue_10826():\n    assert Float('1.23e-100', 100).as_mpf() == (0, int(12300000000000000000000000000000000000000000000000000000000000000000000000), 100)\n", "compilation error", "def test_issue_5900():\n    assert Float('1.234567890123456789012345678901234567890', 10).as_mpf() == mpf('1.234567890123456789')\n", "def test_issue_8918():\n    assert Float(1.0, 10).n(3) == 1.000\n    assert Float(1.0, 10).n(10) == 1.0000000000\n    assert Float(1.0, 10).n(11) == 1.00000000000\n", "def test_issue_10986():\n    assert Float('1e200').is_finite is True\n    assert Float('1e201').is_finite is False\n\n\n\n"], "sample_282": ["    def test_partially_required_field(self):\n        form = PartiallyRequiredForm({'f': 'some text'})\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['f'], 'some text')\n \n        form = PartiallyRequiredForm({'f': ''})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['f'], [ValidationError(\"This field is required.\")])\n", "    def test_partially_required_field(self):\n        form = PartiallyRequiredForm({'f': 'some text,'})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['f'], [\"This field is required.\"])\n\n        form = PartiallyRequiredForm({'f': 'some text,another text'})\n        self.assertTrue(form.is_valid())\n", "    def test_form_bound_data(self):\n        form = ComplexFieldForm({\n            'field1_0': 'some text',\n            'field1_1': ['J', 'P'],\n            'field1_2_0': '2007-04-25',\n            'field1_2_1': '06:24:00',\n        })\n        form.is_bound = True\n        self.assertEqual(form.data['field1'], 'some text,JP,2007-04-25 06:24:00')\n", "    def test_partially_required_field(self):\n        form = PartiallyRequiredForm({'f': 'a,b'})\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['f'], 'a,b')\n\n        form = PartiallyRequiredForm({'f': 'a'})\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['f'], 'a')\n\n        form = PartiallyRequiredForm({'f': ''})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['f'], ['This field is required.'])\n", "    def test_partially_required_field(self):\n        form = PartiallyRequiredForm(data={'f': ''})\n        form.is_valid()\n        self.assertFalse(form.errors)\n        self.assertEqual(form.cleaned_data['f'], '')\n"], "sample_156": ["    def test_renderer_override(self):\n        class MyForm(Form):\n            name = CharField()\n\n                return super().as_p()\n\n        form = MyForm()\n        renderer = DjangoTemplates()\n        form.renderer = renderer\n        self.assertEqual(form.as_p(), renderer.render(form))\n        \n", "    def test_attribute_class_with_instance(self):\n        class CustomForm(Form):\n            default_renderer = CustomRenderer\n\n        custom_instance = CustomRenderer()\n        form = CustomForm(renderer=custom_instance)\n        self.assertEqual(form.renderer, custom_instance)\n", "    def test_request_is_not_required_for_renderer_instance(self):\n        class CustomRenderer(DjangoTemplates):\n                super().__init__(*args, **kwargs)\n                self.request_was_passed = False\n\n                super().render(*args, **kwargs)\n\n        renderer = CustomRenderer()\n        form = Form(renderer=renderer)\n        form.as_p()\n        self.assertFalse(renderer.request_was_passed)\n", "    def test_renderer_override(self):\n        class MyForm(Form):\n            name = CharField()\n\n        renderer = CustomRenderer()\n        form = MyForm(renderer=renderer)\n        self.assertEqual(form.renderer, renderer)\n", "    def test_renderer_inheritance(self):\n        class CustomRenderer(DjangoTemplates):\n                context['my_custom_var'] = 'hello'\n                return super().render(context)\n\n        class FormWithRenderer(Form):\n            default_renderer = CustomRenderer\n\n        form = FormWithRenderer()\n        rendered_html = form.as_p()\n        self.assertIn('my_custom_var', rendered_html)\n"], "sample_1075": ["    def test_beta_derivative(self):\n        x = Symbol('x')\n        y = Symbol('y')\n        expr = beta(x, y)\n        with self.assertWarns(UserWarning):\n            d1 = diff(expr, x)\n            d2 = diff(expr, y)\n            assert d1 == beta(x, y) * (digamma(x) - digamma(x + y))\n            assert d2 == beta(x, y) * (digamma(y) - digamma(x + y))\n", "    def test_derivative_with_complex_arguments():\n        x = Symbol('x')\n        y = Symbol('y')\n        with raises(ArgumentIndexError):\n            diff(beta(x, y), 3)\n        with raises(ArgumentIndexError):\n            diff(beta(x, y), -1)\n", "    def test_derivatives():\n        x, y = Symbol('x'), Symbol('y')\n        expr = beta(x, y)\n        assert diff(expr, x) == beta(x, y) * (digamma(x) - digamma(x + y))\n        assert diff(expr, y) == beta(x, y) * (digamma(y) - digamma(x + y))\n\n\n\n", "    def test_fdiff_index_error(self):\n        beta(Symbol('x'), Symbol('y')).fdiff(3)\n", "    def test_beta_derivative_wrt_complex(self):\n        x = Symbol('x', complex=True)\n        y = Symbol('y', complex=True)\n        with raises(ArgumentIndexError):\n            diff(beta(x,y), 3)\n        derivative_x = diff(beta(x,y), x)\n        derivative_y = diff(beta(x,y), y)\n        self.assertEqual(derivative_x, beta(x, y)*(digamma(x) - digamma(x + y)))\n        self.assertEqual(derivative_y, beta(x, y)*(digamma(y) - digamma(x + y)))\n"], "sample_626": ["    def test_safe_cast_to_index_mixed_types(self):\n        array = np.array([1, \"a\", pd.Timestamp(\"2023-01-01\")])\n        with pytest.raises(TypeError, match=\".*Cannot cast.*\"):\n            safe_cast_to_index(array)\n", "    def test_safe_cast_to_index_mixed_dtypes(self) -> None:\n        mixed_array = np.array([\"a\", 1, datetime(2023, 1, 1)])\n        with pytest.raises(TypeError, match=\".*mixed dtype.*\"):\n            safe_cast_to_index(mixed_array)\n", "    def test_safe_cast_to_index_mixed_dtype(self) -> None:\n        data = np.array([1, 2, 3, datetime(2023, 1, 1), \"a\", \"b\"])\n        with pytest.raises(ValueError, match=\"cannot convert mixed data types\"):\n            safe_cast_to_index(data)\n", "    def test_safe_cast_to_index_various_types(array, expected):\n        actual = safe_cast_to_index(array)\n        assert_array_equal(expected, actual)\n        assert expected.dtype == actual.dtype\n", "    def test_safe_cast_to_index_mixed_dummies(self, input):\n        expected = pd.Index(input.values, dtype=input.dtype)\n        actual = safe_cast_to_index(input.values)\n        assert_array_equal(expected, actual)\n        assert expected.dtype == actual.dtype\n"], "sample_444": ["    def test_manifest_hash_change(self):\n        # Write initial files to disk.\n        with open(self._get_filename_path(\"test.txt\"), \"w\") as f:\n            f.write(\"Initial content\")\n        with open(self._get_filename_path(\"styles.css\"), \"w\") as f:\n            f.write(f\"url('test.txt')\")\n\n        # Collect static files.\n        self.run_collectstatic()\n\n        # Change the content of test.txt, collect again\n        with open(self._get_filename_path(\"test.txt\"), \"w\") as f:\n            f.write(\"Changed content\")\n        self.run_collectstatic(clear=True)\n\n        # Check if the manifest hash changed and that the CSS file references the updated version.\n        _, manifest_hash = storage.staticfiles_storage.load_manifest()\n        self.assertNotEqual(manifest_hash, \"\")\n        with open(self._get_filename_path(\"styles.css\"), \"r\") as f:\n            content = f.read()\n        self.assertIn(\n            f\"url('{self.hashed_file_path('test.txt')})\", content\n        )\n", "    def test_manifest_hash_file_ordering(self):\n        filename_1 = \"cached/styles_1.css\"\n        filename_2 = \"cached/styles_2.css\"\n        filename_3 = \"cached/styles_3.css\"\n\n        self._save_file(self._get_filename_path(filename_1), \"/* @import url('styles_2.css'); */\")\n        self._save_file(self._get_filename_path(filename_2), \"/* @import url('styles_3.css'); */\")\n        self._save_file(self._get_filename_path(filename_3), \"content for styles_3.css\")\n\n        self.run_collectstatic()\n        manifest_hash, _ = storage.staticfiles_storage.load_manifest()\n        self.assertFileExists(self.hashed_file_path(filename_1))\n        self.assertFileExists(self.hashed_file_path(filename_2))\n        self.assertFileExists(self.hashed_file_path(filename_3))\n\n        # Re-collectstatic with a modified file (e.g, change a comment)\n        self._save_file(self._get_filename_path(filename_2), \"/* @import url('styles_3_updated.css'); */\")\n\n        self.run_collectstatic()\n        _, manifest_hash_new = storage.staticfiles_storage.load_manifest()\n        \n\n        self.assertNotEqual(manifest_hash, manifest_hash_new)\n        self.assertFileExists(self.hashed_file_path(filename_1))\n        self.assertFileExists(self.hashed_file_path(filename_2))\n        self.assertFileExists(self.hashed_file_path(filename_3))\n    \n", "    def test_hashed_filename_cache(self):\n        filepath_a = self._get_filename_path(\"cached/file_a.css\")\n        filepath_b = self._get_filename_path(\"cached/file_b.css\")\n        content_a = \"/* This is file A */\\n@import url('file_b.css');\"\n        content_b = \"/* This is file B */\"\n        with open(filepath_a, \"w\") as f:\n            f.write(content_a)\n        with open(filepath_b, \"w\") as f:\n            f.write(content_b)\n\n        # Collect static files, triggering hashing.\n        self.run_collectstatic()\n\n        # Get the hashed path for both files.\n        hashed_filepath_a = self.hashed_file_path(filepath_a)\n        hashed_filepath_b = self.hashed_file_path(filepath_b)\n\n        # Now when serving, it should use the cached hashed path for file B\n        with open(hashed_filepath_a, \"r\") as f:\n            content_a = f.read()\n\n        self.assertIn(hashed_filepath_b, content_a)\n\n        # Ensure the cached path is used even if the order is reversed\n        with open(hashed_filepath_b, \"r\") as f:\n            content_b = f.read()\n        self.assertIn(hashed_filepath_a, content_b)\n\n\n", "    def test_hashed_files_cache_ordering(self):\n        image_file = self._get_filename_path(\"image.png\")\n        css_file = self._get_filename_path(\"styles.css\")\n        with open(image_file, \"w\") as f:\n            f.write(\"Image Content\")\n        with open(css_file, \"w\") as f:\n            f.write(f\"url('{image_file.replace(self._temp_dir, '')}');\")\n        self.run_collectstatic()\n\n        # Ensure initial order matters.\n        with open(css_file, \"r\") as f:\n            first_content = f.read()\n\n        with open(image_file, \"w\") as f:\n            f.write(\"Updated Image Content\")\n        self.run_collectstatic()\n\n        with open(css_file, \"r\") as f:\n            second_content = f.read()\n        self.assertNotEqual(first_content, second_content)\n\n\n\n", "    def test_manifest_content_integrity(self):\n        with open(self._get_filename_path(\"cached/styles.css\"), \"w\") as f:\n            f.write(\"url(test.png)\\n\")\n\n        with open(self._get_filename_path(\"test.png\"), \"wb\") as f:\n            f.write(b\"test\")\n        self.run_collectstatic()\n        manifest_content, manifest_hash = storage.staticfiles_storage.load_manifest()\n        self.assertIn(\"test.png\", manifest_content)\n        self.assertNotEqual(manifest_hash, \"\")\n\n        with open(self._get_filename_path(\"test2.png\"), \"wb\") as f:\n            f.write(b\"test2\")\n        \n        self.run_collectstatic()\n        new_manifest_content, new_manifest_hash = storage.staticfiles_storage.load_manifest()\n        self.assertIn(\"test2.png\", new_manifest_content)\n        self.assertNotEqual(new_manifest_hash, manifest_hash)\n        self.assertNotEqual(new_manifest_content, manifest_content)\n"], "sample_1081": ["def test_is_semiperfect():\n    assert is_semiperfect(6) is True\n    assert is_semiperfect(12) is True\n    assert is_semiperfect(18) is True\n    assert is_semiperfect(28) is True\n    assert is_semiperfect(10) is False\n    assert is_semiperfect(20) is False\n    assert is_semiperfect(36) is False\n", "def test_is_semiperfect():\n    assert is_semiperfect(6) is True\n    assert is_semiperfect(12) is True\n    assert is_semiperfect(28) is True\n    assert is_semiperfect(15) is False\n    assert is_semiperfect(400) is False\n    assert is_semiperfect(496) is True\n    assert is_semiperfect(8128) is True\n    assert is_semiperfect(10000) is False\n", "compilation error", "def test_is_sum_of_divisors_perfect():\n    assert is_sum_of_divisors_perfect(6) is True\n    assert is_sum_of_divisors_perfect(15) is False\n    assert is_sum_of_divisors_perfect(28) is True\n    assert is_sum_of_divisors_perfect(400) is False\n    assert is_sum_of_divisors_perfect(496) is True\n    assert is_sum_of_divisors_perfect(8128) is True\n    assert is_sum_of_divisors_perfect(10000) is False\n\n\n\n", "def test_is_even():\n    assert is_even(0)\n    assert is_even(2)\n    assert is_even(100)\n    assert is_even(-2)\n    assert not is_even(1)\n    assert not is_even(3)\n    assert not is_even(123)\n"], "sample_254": ["    def test_inline_formset_error_input_border_with_stacked_and_tabular_inlines(self):\n        self.admin_login(username='super', password='secret')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_inlines_holder7_add'))\n        self.wait_until_visible('#id_dummy')\n        self.selenium.find_element_by_id('id_dummy').send_keys(1)\n        stacked_fields = ['id_inner7stacked_set-0-dummy', 'id_inner7stacked_set-1-dummy',\n                          'id_inner7stacked_set-2-dummy']\n        tabular_fields = ['id_inner7tabular_set-0-dummy', 'id_inner7tabular_set-1-dummy',\n                          'id_inner7tabular_set-2-dummy']\n\n        show_links = self.selenium.find_elements_by_link_text('SHOW')\n        for show_index, field_name in enumerate(stacked_fields + tabular_fields):\n            show_links[show_index].click()\n            self.wait_until_visible(field_name)\n            self.selenium.find_element_by_id(field_name).send_keys(1)\n\n        # Before save all inputs have a default border.\n        for inline_fields in ('stacked', 'tabular'):\n            for field_name in ('name', 'select', 'text'):\n                element_id = 'id_inner7%s_set-0-%s' % (inline_fields, field_name)\n                self.assertBorder(\n                    self.selenium.find_element_by_id(element_id),\n                    '1px solid #cccccc',\n                )\n        self.selenium.find_element_by_xpath('//input[@value=\"Save\"]').click()\n        # Test the red border around inputs by css selectors\n        stacked_selectors = ['.errors input', '.errors select', '.errors textarea']\n        tabular_selectors = [\n            'td ul.errorlist + input', 'td ul.errorlist + select', 'td ul.errorlist + textarea'\n        ]\n\n        for selector in stacked_selectors:", "    def test_inline_formset_error_message(self):\n        self.admin_login(username='super', password='secret')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_inlines_holder5_add'))\n        test_fields = ['#id_inner5stacked_set-0-dummy', '#id_inner5tabular_set-0-dummy']\n        for show_index, field_name in enumerate(test_fields):\n            show_link = self.selenium.find_element_by_link_text('SHOW')\n            show_link.click()\n            self.wait_until_visible('#' + field_name[1:])\n            self.selenium.find_element_by_id(field_name[1:]).send_keys(1)\n        with self.wait_page_loaded():\n            self.selenium.find_element_by_xpath('//input[@value=\"Save\"]').click()\n\n        for inline in ('stacked', 'tabular'):\n            errorlist = self.selenium.find_element_by_css_selector(\n                f'div#inner5{inline}stacked_set-group ul.errorlist'\n            )\n            self.assertContains(errorlist, 'Please correct the duplicate values below.')\n            self.assertContains(errorlist, 'Duplicate value')\n\n\n\n", "    def test_inline_formset_error_message_visibility(self):\n        self.admin_login(username='super', password='secret')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_inlines_holder5_add'))\n        stacked_inline_formset_selector = 'div#inner5stacked_set-group fieldset.module.collapse'\n        tabular_inline_formset_selector = 'div#inner5tabular_set-group fieldset.module.collapse'\n        # Inlines without errors, both inlines collapsed\n        self.selenium.find_element_by_xpath('//input[@value=\"Save\"]').click()\n        self.assertEqual(\n            len(self.selenium.find_elements_by_css_selector(stacked_inline_formset_selector + ' .errorlist li')), 0\n        )\n        self.assertEqual(\n            len(self.selenium.find_elements_by_css_selector(tabular_inline_formset_selector + ' .errorlist li')), 0\n        )\n        show_links = self.selenium.find_elements_by_link_text('SHOW')\n        self.assertEqual(len(show_links), 2)\n\n        # Inlines with errors, both inlines expanded\n        test_fields = ['#id_inner5stacked_set-0-dummy', '#id_inner5tabular_set-0-dummy']\n        for show_index, field_name in enumerate(test_fields):\n            show_links[show_index].click()\n            self.wait_until_visible(field_name)\n            self.selenium.find_element_by_id(field_name[1:]).send_keys(1)\n        hide_links = self.selenium.find_elements_by_link_text('HIDE')\n        self.assertEqual(len(hide_links), 2)\n        for hide_index, field_name in enumerate(test_fields):\n            hide_link = hide_links[hide_index]\n            self.selenium.execute_script('window.scrollTo(0, %s);' % hide_link.location['y'])\n            hide_link.click()\n            self.wait_until_invisible(", "    def test_inline_formset_no_errors(self):\n        self.admin_login(username='super', password='secret')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_inlines_holder5_add'))\n\n        # No errors, inlines collapsed.\n        self.selenium.find_element_by_xpath('//input[@value=\"Save\"]').click()\n        stacked_inline_formset_selector = 'div#inner5stacked_set-group fieldset.module.collapse'\n        tabular_inline_formset_selector = 'div#inner5tabular_set-group fieldset.module.collapse'\n        self.assertEqual(\n            len(self.selenium.find_elements_by_css_selector(stacked_inline_formset_selector + '.collapsed')), 1\n        )\n        self.assertEqual(\n            len(self.selenium.find_elements_by_css_selector(tabular_inline_formset_selector + '.collapsed')), 1\n        )\n\n        # Show inlines\n        self.selenium.find_element_by_link_text('SHOW').click()\n        self.selenium.find_element_by_link_text('SHOW').click()\n        stacked_inline_formset_selector = 'div#inner5stacked_set-group fieldset.module:not(.collapsed)'\n        tabular_inline_formset_selector = 'div#inner5tabular_set-group fieldset.module:not(.collapsed)'\n\n        self.assertEqual(\n            len(self.selenium.find_elements_by_css_selector(stacked_inline_formset_selector)), 1\n        )\n        self.assertEqual(\n            len(self.selenium.find_elements_by_css_selector(tabular_inline_formset_selector)), 1\n        )\n        \n        self.selenium.find_element_by_xpath('//input[@value=\"Save\"]').click()\n\n        # All inlines should be in expanded state.\n        self.assertEqual(\n            len(self.selenium.find_elements_by_css_selector(stacked_inline_formset_selector)), 1\n        )\n        self.assertEqual(\n            len(self.selenium.find_elements_by_", "    def test_stacked_inline_form_is_hidden_when_adding_a_nested_inline(self):\n        self.admin_login(username='super', password='secret')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_inlines_holder7_add'))\n\n        # Locate the 'Add another NestedInline' button\n        add_nested_button = self.selenium.find_element_by_link_text('Add another NestedInline')\n        add_nested_button.click()\n\n        # Check that the stacked inline is hidden\n        stacked_inline_element = self.selenium.find_element_by_css_selector('#inner7stacked_set-group')\n        self.assertEqual(stacked_inline_element.is_displayed(), False)\n\n\n"], "sample_633": ["def test_ignore_signatures_class_methods_with_docstring_fail() -> None:\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([SIMILAR_CLS_B, SIMILAR_CLS_A])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == (\n            f'''", "def test_ignore_signatures_with_docstrings_fail() -> None:\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([SIMILAR_CLS_A, SIMILAR_CLS_B])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == (\n            f'''", "def test_ignore_signatures_class_methods_fail_with_docstrings() -> None:\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([SIMILAR_CLS_B, SIMILAR_CLS_A])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == (\n            f'''", "def test_ignore_signatures_and_empty_functions_pass() -> None:\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run(\n            [\n                \"--ignore-signatures\",\n                \"--ignore-empty-functions\",\n                EMPTY_FUNCTION_1,\n                EMPTY_FUNCTION_2,\n            ]\n        )\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == \"\"\"", "def test_ignore_signatures_nested_class_methods_fail() -> None:\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([SIMILAR_CLS_B, SIMILAR_CLS_A])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == (\n            f'''"], "sample_463": ["    def test_order_of_operations_affects_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\"Person\", fields=[]),\n                migrations.AlterField(\n                    model_name=\"Person\",\n                    name=\"age\",\n                    field=models.IntegerField(null=True),\n                ),\n            ]\n\n        migration = Migration(\"0001_initial\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"person_person_age\")\n", "    def test_migration_name_with_extra_args(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\"CustomModel\", fields=[]),\n            ]\n            extra_kwargs = {\"custom_arg\": \"value\"}\n\n        migration = Migration(\"0001_initial\", \"test_app\", extra_kwargs=migration.extra_kwargs)\n        self.assertEqual(migration.suggest_name(), \"custom_model\")\n", "    def test_suggest_name_with_unicode_characters(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    \"\u00dcnic\u00f6de M\u00f8del\", fields=[]\n                )\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"unicode_m_del\")\n\n", "    def test_rename_field_in_index_together(self):\n        initial_author = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n                (\"age\", models.IntegerField()),\n            ],\n            {\n                \"index_together\": {(\"name\", \"age\")},\n            },\n        )\n        author_with_new_field = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"full_name\", models.CharField(max_length=200)),\n                (\"age\", models.IntegerField()),\n            ],\n            {\n                \"index_together\": {(\"full_name\", \"age\")},\n            },\n        )\n        changes = self.get_changes(\n            [initial_author], [author_with_new_field], MigrationQuestioner({\"ask_rename\": True})\n        )\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(\n            changes,\n            \"testapp\",\n            0,\n            [\n                \"RenameField\",\n                \"AlterIndexTogether\",\n            ],\n        )\n        self.assertOperationAttributes(\n            changes,\n            \"testapp\",\n            0,\n            1,\n            name=\"author\",\n            index_together={(\"full_name\", \"age\")},\n        )\n\n\n\n", "    def test_operation_with_same_base_name_and_different_fields(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    \"Person\", fields=[('id', models.AutoField(primary_key=True))],\n                ),\n                migrations.CreateModel(\n                    \"Person\", fields=[('id', models.AutoField(primary_key=True)), ('name', models.CharField(max_length=255))],\n                ),\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"person_add_name\")\n"], "sample_350": ["    def test_related_objects_in_combined_qs(self):\n        class Car(models.Model):\n            name = models.CharField(max_length=100)\n            owner = models.ForeignKey(Celebrity, on_delete=models.CASCADE)\n\n        Celebrity.objects.create(name='John Doe')\n        Car.objects.create(name='Ford', owner=Celebrity.objects.get(name='John Doe'))\n        qs1 = Celebrity.objects.filter(name='John Doe')\n        qs2 = Car.objects.filter(name='Ford')\n        self.assertEqual(\n            list(qs1.union(qs2).values('owner__name', 'name')),\n            [{'owner__name': 'John Doe', 'name': 'John Doe'}, {'owner__name': 'John Doe', 'name': 'Ford'}],\n        )\n\n", "    def test_get_with_related_objects_unsupported_on_combined_qs(self):\n        qs = Number.objects.all()\n        msg = 'Calling QuerySet.get() with related objects after %s() is not supported.'\n        combinators = ['union']\n        if connection.features.supports_select_difference:\n            combinators.append('difference')\n        if connection.features.supports_select_intersection:\n            combinators.append('intersection')\n        for combinator in combinators:\n            with self.subTest(combinator=combinator):\n                with self.assertRaisesMessage(NotSupportedError, msg % combinator):\n                    getattr(qs, combinator)(qs).get(pk=1, name__pk=1)\n", "    def test_subquery_ordering_with_union_all(self):\n        qs1 = Number.objects.filter(num__lt=5)\n        qs2 = Number.objects.filter(num__gt=5).order_by('num')\n        self.assertNumbersEqual(list(qs1.union_all(qs2).order_by('-num')), [9, 8, 7, 6, 4, 3, 2, 1, 0])\n", "    def test_get_with_complex_filter_unsupported_on_combined_qs(self):\n        qs = Number.objects.all()\n        msg = 'Calling QuerySet.get(...) with complex filters after %s() is not supported.'\n        combinators = ['union']\n        if connection.features.supports_select_difference:\n            combinators.append('difference')\n        if connection.features.supports_select_intersection:\n            combinators.append('intersection')\n        for combinator in combinators:\n            with self.subTest(combinator=combinator):\n                with self.assertRaisesMessage(NotSupportedError, msg % combinator):\n                    getattr(qs, combinator)(qs).get(num__gt=5, other_num__lt=5)\n\n\n\n\n", "    def test_union_with_subquery(self):\n        qs1 = Number.objects.filter(num__lt=5).values('num')\n        qs2 = Number.objects.filter(num__gt=5).values('num')\n        self.assertEqual(list(qs1.union(qs2).order_by('num')), list(range(10)))\n\n"], "sample_874": ["def test_step_parameter():\n    sel = StepSelector(step=3)\n    Xt_actual = sel.fit(X, y).transform(X)\n\n    expected_Xt = np.arange(0, 20, 3).reshape(2, 5)\n    assert_array_equal(expected_Xt, Xt_actual)\n", "def test_step_parameter():\n    sel = StepSelector(step=3)\n    Xt_actual = sel.fit(X, y).transform(X)\n    expected_inds = [0, 3, 6, 9]\n    assert_array_equal(Xt_actual[:, expected_inds], Xt)\n", "def test_step_parameter():\n    sel = StepSelector(step=3)\n    Xt_actual = sel.fit(X, y).transform(X)\n    expected_Xt = X[:, [0, 3, 6, 9]]\n    assert_array_equal(Xt_actual, expected_Xt)\n\n    sel = StepSelector(step=0)\n    Xt_actual = sel.fit(X, y).transform(X)\n    assert_array_equal(Xt_actual, np.zeros_like(X))\n\n    with pytest.raises(ValueError):\n        StepSelector(step=-1)\n\n", "def test_step_parameter():\n    sel = StepSelector(step=3)\n    Xt = sel.fit(X, y).transform(X)\n    assert_array_equal(Xt, X[:, [0, 3, 6, 9]])\n\n    with pytest.raises(ValueError):\n        sel = StepSelector(step=0)\n", "def test_step_parameter():\n    sel = StepSelector(step=2)\n    Xt_actual = sel.fit(X, y).transform(X)\n    Xt_expected = np.arange(0, 20, 2).reshape(2, 5)\n    assert_array_equal(Xt_expected, Xt_actual)\n\n    sel = StepSelector(step=1)\n    Xt_actual = sel.fit(X, y).transform(X)\n    assert_array_equal(Xt, Xt_actual)\n\n    with pytest.raises(ValueError):\n        sel = StepSelector(step=0)\n        sel.fit(X, y)\n\n\n\n"], "sample_197": ["    def test_reversed_depth(self):\n        t = self.t + self.oneyear + self.onemonth + self.oneweek + self.oneday + self.onehour\n        self.assertEqual(timeuntil(t, self.t, reversed=True, depth=1), '1\\xa0year')\n        self.assertEqual(timeuntil(t, self.t, reversed=True, depth=2), '1\\xa0year, 1\\xa0month')\n        self.assertEqual(timeuntil(t, self.t, reversed=True, depth=3), '1\\xa0year, 1\\xa0month, 1\\xa0week')\n        self.assertEqual(timeuntil(t, self.t, reversed=True, depth=4), '1\\xa0year, 1\\xa0month, 1\\xa0week, 1\\xa0day')\n        self.assertEqual(timeuntil(t, self.t, reversed=True, depth=5), '1\\xa0year, 1\\xa0month, 1\\xa0week, 1\\xa0day, 1\\xa0hour')\n        self.assertEqual(timeuntil(t, self.t, reversed=True, depth=6), '1\\xa0year, 1\\xa0month, 1\\xa0week, 1\\xa0day, 1\\xa0hour')\n", "    def test_future_datetime(self):\n        \"\"\"\n        `timesince` should return 0 minutes if the future datetime occurs\n        after now. (#22139)\n        \"\"\"\n        now = datetime.datetime.now()\n        future = now + datetime.timedelta(days=1)\n        self.assertEqual(timesince(future), '0\\xa0minutes')\n", "    def test_reversed_timeuntil(self):\n        \"\"\" TimeUntil should work correctly when reversed. \"\"\"\n        self.assertEqual(timeuntil(self.t - self.oneday, self.t), '1\\xa0day')\n        self.assertEqual(timeuntil(self.t - self.oneweek, self.t), '1\\xa0week')\n        self.assertEqual(timeuntil(self.t - self.onemonth, self.t), '1\\xa0month')\n        self.assertEqual(timeuntil(self.t - self.oneyear, self.t), '1\\xa0year')\n", "    def test_reverse_order_with_depth(self):\n        t = self.t + self.oneyear + self.onemonth + self.oneweek + self.oneday + self.onehour\n        self.assertEqual(timesince(t, self.t, reversed=True, depth=3), '1\\xa0year, 1\\xa0month, 1\\xa0week')\n        self.assertEqual(timeuntil(self.t, t, depth=3), '1\\xa0year, 1\\xa0month, 1\\xa0week')\n", "    def test_natural_time_strings_with_empty_time_strings(self):\n        \"\"\" Check that the function handles empty time strings gracefully. \"\"\"\n        with self.subTest():\n            self.assertEqual(timesince(self.t, self.t + self.oneminute, time_strings={}), '0\\xa0minutes')\n"], "sample_109": ["    def test_filter_select2_choices(self):\n        with translation.override('en'):\n            Band.objects.create(name='Band A', style='Rock')\n            Band.objects.create(name='Band B', style='Pop')\n\n            form = AlbumForm(initial={'band': Band.objects.first().pk})\n            attrs = form['band'].field.widget.get_context(name='my_field', value=None, attrs={})['widget']['attrs']\n            selected_option = '<option value=\"%s\" selected>Band A</option>' % Band.objects.first().pk\n            option = '<option value=\"%s\">Band B</option>' % Band.objects.last().pk\n\n            self.assertIn(selected_option, attrs['data-select2-data'])\n            self.assertIn(option, attrs['data-select2-data'])\n", "    def test_language_support(self):\n        with translation.override('es'):\n            form = AlbumForm()\n            output = form.as_table()\n            self.assertIn(\n                '<option value=\"\">Seleccione una opci\u00f3n</option>', output\n            ) \n", "    def test_i18n_support(self):\n        with translation.override('de'):\n            form = AlbumForm()\n            output = form.as_table()\n            self.assertIn('German-specific text', output)\n\n\n\n", "    def test_language_awareness(self):\n        with translation.override('es'):\n            form = AlbumForm()\n            output = form.as_table()\n            self.assertIn('<option value=\"\">', output)  # Empty option is present\n        self.assertNotIn('<option value=\"\" selected>', output)  # Empty option isn't selected by default\n\n", "    def test_select2_i18n(self):\n        with translation.override('es'):\n            form = AlbumForm()\n            output = form['band'].field.widget.get_context(name='my_field', value=None, attrs={})\n            self.assertIn('i18n--file', output)\n            self.assertEqual(output['i18n--file'][0], 'admin/js/vendor/select2/i18n/es.js') \n\n\n\n"], "sample_129": ["    def test_non_numeric_inputs(self):\n        self.assertEqual(floatformat('foo'), '')\n        self.assertEqual(floatformat(True), '1.0')\n        self.assertEqual(floatformat(False), '0.0')\n        self.assertEqual(floatformat(None), '')\n        self.assertEqual(floatformat([]), '')\n        self.assertEqual(floatformat(123, 'foo'), '123.0')\n", "    def test_float_dunder_method(self):\n        class FloatWrapper:\n                self.value = value\n\n                return self.value\n        self.assertEqual(floatformat(FloatWrapper(5.5)), '5.5')\n", "    def test_floatformat_with_localcontext(self):\n        with localcontext() as ctx:\n            ctx.prec = 3\n            self.assertEqual(floatformat(1.234567, 2), '1.23')\n            self.assertEqual(floatformat(1.2345678, 3), '1.235')\n\n\n", "    def test_floatformat_with_nan(self):\n        self.assertEqual(floatformat(float('nan')), 'nan')\n", "    def test_floatformat_with_context(self):\n        with localcontext() as ctx:\n            ctx.prec = 10  # Set precision to 10 decimal places\n            self.assertEqual(floatformat(Decimal('1.2345678901234567'), 5), '1.23457')  \n"], "sample_42": ["compilation error", "def test_physical_constants():\n    from astropy.constants import G, c, k_B, h\n    assert_allclose(G.to(u.m**3 / (u.kg * u.s**2)).value, 6.67430e-11)\n    assert_allclose(c.to(u.m/u.s).value, 299792458)\n    assert_allclose(k_B.to(u.J/u.K).value, 1.380649e-23)\n    assert_allclose(h.to(u.J*u.s).value, 6.62607015e-34)\n", "def test_velocity_unit():\n    v = 100 * u.km / u.s\n    assert v.unit.name == 'km/s'\n\n    v_mps = v.to(u.m / u.s)\n    assert v_mps.unit.name == 'm/s'\n\n    v_c = v.to(u.c)\n    assert v_c.unit.name == 'c'\n    assert v_c.value == 100 * u.km / u.s / u.c.value\n\n    with pytest.raises(u.UnitsError):\n        v.to(u.K)\n    \n\n", "    def test_doppler_optical_equivalency():\n        # Test the doppler_optical equivalency between frequency and velocity\n        nu = 143 * u.GHz\n        v = 100 * u.km / u.s\n        dlambda = nu.to(u.Angstrom, u.doppler_optical(v))\n        assert_quantity_allclose(dlambda, 0.080526316 * u.Angstrom)\n        dlambda_back = dlambda.to(u.km / u.s, u.doppler_optical(nu))\n        assert_quantity_allclose(dlambda_back, v)\n\n\n\n", "def test_compose_equivalencies_with_dimensionless_angles():\n    x = u.degree.compose(equivalencies=u.dimensionless_angles())\n    assert u.Unit(u.degree.to(u.radian)) in x.to(u.radian).physical_type\n    assert u.Unit(u.radian).compose(equivalencies=u.dimensionless_angles()) == u.radian\n\n    y = u.arcsec.compose(equivalencies=u.dimensionless_angles())\n    assert u.Unit(u.arcsec.to(u.radian)) in y.to(u.radian).physical_type\n    assert u.Unit(u.radian).compose(equivalencies=u.dimensionless_angles()) == u.radian\n\n    z = u.furlong.compose(equivalencies=u.dimensionless_angles())\n    assert 'furlong' in z\n    assert z.unit.symbol == 'furlong'\n"], "sample_592": ["    def test_summarize_attr(self):\n        cases = [\n            (\"key\", \"value\", \"    key: value\"),\n            (\"key\", 123, \"    key: 123\"),\n            (\"key\", 1.23, \"    key: 1.23\"),\n            (\"key\", True, \"    key: True\"),\n            (\"key\", False, \"    key: False\"),\n            (\"key\", None, \"    key: None\"),\n            (\"key\", np.ones(5), \"    key: [1. 1. 1. 1. 1.]\"),\n            (\"key\", np.array([1, 2, 3, 4, 5]), \"    key: [1 2 3 4 5]\"),\n            (\"key\", np.array([1.0, 2.0, 3.0, 4.0, 5.0]), \"    key: [1. 2. 3. 4. 5.]\"),\n        ]\n        for key, value, expected in cases:\n            actual = formatting.summarize_attr(key, value)\n            assert actual == expected\n\n\n\n", "    def test_summarize_attr(self):\n        cases = [\n            (None, \"\"),\n            (\"\", \"\"),\n            (\"   \", \"   \"),\n            (\"   key: value   \", \"   key: value   \"),\n            (\"key: \", \"key: \"),\n            (\"key: long value\", \"key: long value\"),\n            (\n                \"key: very long value that wraps and should be truncated\",\n                \"key: very long value that wraps and should be truncated...\",\n            ),\n            (123, \"123\"),\n            (123.456, \"123.456\"),\n            (True, \"True\"),\n            (False, \"False\"),\n            (pd.Timestamp(\"2000-01-01\"), \"2000-01-01\"),\n            (pd.Timedelta(\"1 day\"), \"1 days\"),\n        ]\n        for value, expected in cases:\n            actual = formatting.summarize_attr(\"key\", value)\n            assert actual == expected\n", "    def test_summarize_attr(self):\n        attrs = {\"a\": 1, \"b\": 2.5, \"c\": \"hello\", \"d\": [1, 2, 3], \"e\": np.array([1, 2, 3])}\n        expected = dedent(\n            \"\"\"\\\n                a: 1\n                b: 2.5\n                c: hello\n                d: [1, 2, 3]\n                e: [1 2 3]\"\"\"\n        ).strip()\n        actual = formatting.summarize_attr(attrs)\n        assert actual == expected\n", "    def test_summarize_attr(self):\n        cases = [\n            (\"key\", \"Short string\", \"    key: Short string\"),\n            (\"key\", 100 * \"Very long string \", \"    key: Very long string ...\"),\n            (\"key\", \"\\n\\n\\n\", \"    key: \\\\n\\\\n\\\\n\"),\n            (\"key\", \"\\t\\t\\t\", \"    key: \\t\\t\\t\"),\n            (\"key\", np.array([1, 2, 3]), \"    key: [1 2 3]\"),\n            (\"key\", np.array([1, 2, 3], dtype=float), \"    key: [1. 2. 3.]\"),\n            (\"key\", np.array([1, 2, 3], dtype=bool), \"    key: [True True True]\"),\n            (\"key\", np.array([1, 2, 3], dtype=\"float32\"), \"    key: [1. 2. 3.]\"),\n        ]\n        for key, value, expected in cases:\n            actual = formatting.summarize_attr(key, value)\n            assert actual == expected\n", "    def test_summarize_attr(self):\n        attrs = {\"a\": 1, \"b\": 2.5, \"c\": True, \"d\": \"hello world\"}\n        for key, value in attrs.items():\n            summary = formatting.summarize_attr(key, value)\n            if isinstance(value, (int, float)):\n                assert f\"    {key}: {value}\" in summary\n            elif isinstance(value, bool):\n                assert f\"    {key}: {value}\" in summary\n            else:\n                assert f\"    {key}: '{value}'\" in summary\n\n\n\n"], "sample_365": ["    def test_lazy_keep_lazy(self):\n        @keep_lazy(int)\n            return x * 2\n\n        @keep_lazy(str)\n            return str(x)\n\n        # Test with all arguments lazy\n        self.assertEqual(lazy_func(lazy(lambda: 3, int))(), 6)\n        self.assertEqual(lazy_func2(lazy(lambda: 'hello', str))(), 'hello')\n\n        # Test with some arguments not lazy\n        self.assertEqual(lazy_func(3, lazy(lambda: 4, int))(), 14)\n        self.assertEqual(lazy_func2('hello', lazy(lambda: 4, int))(), 'hello4')\n\n", "    def test_partition(self):\n        self.assertEqual(partition(lambda x: x % 2 == 0, range(10)), ([0, 2, 4, 6, 8], [1, 3, 5, 7, 9]))\n", "    def test_keep_lazy(self):\n            return a + b\n\n        @keep_lazy(int)\n            return func(a, b)\n\n        self.assertEqual(decorated_func(1, 2), 3)\n        self.assertEqual(decorated_func(1, lazy(lambda: 2, int)), 3)\n        self.assertEqual(decorated_func(lazy(lambda: 1, int), 2), 3)\n        self.assertEqual(decorated_func(lazy(lambda: 1, int), lazy(lambda: 2, int)), 3)\n \n        @keep_lazy_text\n            return a + ' ' + b\n\n        self.assertEqual(decorated_text_func('Hello', 'World'), 'Hello World')\n        self.assertEqual(decorated_text_func('Hello', lazy(lambda: 'World', str)), 'Hello World') \n        self.assertEqual(decorated_text_func(lazy(lambda: 'Hello', str), 'World'), 'Hello World')\n        self.assertEqual(decorated_text_func(lazy(lambda: 'Hello', str), lazy(lambda: 'World', str)), 'Hello World')\n", "    def test_keep_lazy(self):\n            return 42\n\n        @keep_lazy(int, str)\n            return arg1 + arg2\n\n        self.assertEqual(my_function(make_value(), \"hello\"), 42 + \"hello\")\n\n        lazy_arg = lazy(make_value, int)\n        self.assertEqual(my_function(lazy_arg(), \"world\"), 42 + \"world\")\n", "    def test_keep_lazy(self):\n            return arg1 + arg2\n\n        @keep_lazy(int)\n            return arg1 + arg2\n\n        # Test with all arguments lazy\n        lazy_arg1 = lazy(lambda: 1, int)\n        lazy_arg2 = lazy(lambda: 2, int)\n        self.assertEqual(lazy_func(lazy_arg1, lazy_arg2), 3)\n\n        # Test with some arguments lazy, some not\n        self.assertEqual(my_func(1, lazy(lambda: 2, int)), 3)\n        self.assertEqual(lazy_func(1, 2), 3)  # Function is evaluated when no lazy arguments are present.\n"], "sample_1154": ["def test_underdetermined():\n    sol = _linsolve([Eq(x + y, 0)], [x, y])\n    assert sol == {x: -y, y: y} \n", "def test__linsolve_underdetermined():\n    eqs = [x + y - 1]\n    sol = {x: -y, y: y}\n    assert _linsolve(eqs, (x, y)) == sol \n", "compilation error", "def test__linsolve_underdetermined():\n    assert _linsolve([Eq(x + y, 0)], [x, y]) == {x: -y, y: y}\n\n    assert _linsolve([Eq(x + y, 1), Eq(x + y, 2)], [x, y]) is None\n", "def test__linsolve_large_sparse():\n    from sympy.polys.matrices.sdm import SDM\n    \n    # Create a large sparse system\n    A = SDM([[1, 0] , [0, 1]])\n    b = SDM([1, 2])\n    eqs = [Eq(A.row(i), b.row(i)) for i in range(A.rows)]\n    \n    sol = _linsolve(eqs, (x, y))\n\n    assert sol == {x: 1, y: 2}\n"], "sample_841": ["    def test_ridge_regression_with_sample_weights(solver, sample_weight):\n        rng = check_random_state(42)\n        X = rng.rand(1000, 3)\n        true_coefs = [1, 2, 0.1]\n        y = np.dot(X, true_coefs)\n        \n        alpha = 1e-3\n        \n        # Test with different solvers and sample weights\n        for dtype in (np.float32, np.float64):\n            X_dtype = X.astype(dtype)\n            y_dtype = y.astype(dtype)\n            ridge = Ridge(alpha=alpha, solver=solver,\n                          sample_weight=sample_weight,\n                          tol=1e-6)\n            ridge.fit(X_dtype, y_dtype)\n            \n            y_pred = ridge.predict(X_dtype)\n\n            assert_allclose(ridge.coef_, true_coefs, rtol=0, atol=1e-6)\n            assert ridge.predict(X_dtype).dtype == dtype\n", "def test_ridge_regression_sample_weight_dense_sparse():\n    rng = np.random.RandomState(42)\n\n    n_samples = 100\n    n_features = 5\n\n    # Dense data\n    X_dense = rng.randn(n_samples, n_features)\n    y_dense = rng.randn(n_samples)\n\n    # Sparse data\n    X_sparse = sp.csr_matrix(rng.randn(n_samples, n_features))\n    y_sparse = rng.randn(n_samples)\n\n    alpha = 1.0\n\n    sample_weights_dense = rng.rand(n_samples)\n    sample_weights_sparse = rng.rand(n_samples)\n\n    # Test with dense data and sample_weights\n    ridge_dense = Ridge(alpha=alpha)\n    ridge_dense.fit(X_dense, y_dense, sample_weight=sample_weights_dense)\n    dense_coef = ridge_dense.coef_\n\n    # Test with sparse data and sample_weights\n    ridge_sparse = Ridge(alpha=alpha)\n    ridge_sparse.fit(X_sparse, y_sparse, sample_weight=sample_weights_sparse)\n    sparse_coef = ridge_sparse.coef_\n\n    assert_allclose(ridge_dense.coef_, ridge_sparse.coef_, atol=1e-4)\n\n\n\n", "def test_ridge_solver_error_check():\n    ridge = Ridge()\n    with pytest.raises(ValueError, match=\"Unknown solver\"):\n        ridge.fit(X_diabetes, y_diabetes, solver='ThisIsAnInvalidSolver')\n", "def test_ridge_solver_with_sparse_output():\n    X, y = _make_sparse_offset_regression(n_features=5, n_samples=20,\n                                           random_state=0)\n    X_csr = sp.csr_matrix(X)\n    true_coefs = np.array([1, 2, 0.1, 0.2, 0.3])\n\n    for solver in ('sparse_cg', 'auto'):\n        ridge = Ridge(alpha=1., solver=solver, return_sparse_output=True)\n        ridge.fit(X_csr, y)\n        sparse_coef = ridge.coef_\n\n        # Check if the sparse coefficient matrix is correctly obtained\n        assert isinstance(sparse_coef, sp.csr_matrix)\n        assert sparse_coef.shape == true_coefs.shape\n        assert_array_almost_equal(sparse_coef.toarray(), true_coefs,\n                                  decimal=3)\n\n    # Check for other solvers where output is dense\n    for solver in ('sag', 'lsqr', 'cholesky'):\n        ridge = Ridge(alpha=1., solver=solver)\n        ridge.fit(X_csr, y)\n        coef = ridge.coef_\n        assert isinstance(coef, np.ndarray)\n        assert coef.shape == true_coefs.shape\n        assert_array_almost_equal(coef, true_coefs, decimal=3) \n\n\n", "def test_ridge_classifier_multi_label_classification():\n    X, y = make_multilabel_classification(n_samples=10, random_state=0)\n    # While RidgeClassifier doesn't support multilabel directly, we can\n    # fit one class at a time\n    for target in range(y.shape[1]):\n        X_train = X[:, y[:, target] == 1]\n        y_train = (y[:, target] == 1).astype(int)\n        ridge_clf = RidgeClassifier()\n        ridge_clf.fit(X_train, y_train)\n\n\n"], "sample_835": ["def test_sparse_sample_weight():\n    # Check sparse input with sample weights\n\n    X, y = datasets.make_regression(n_samples=15, n_features=50, n_targets=1,\n                                    random_state=42)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n    for sparse_format in [csc_matrix, csr_matrix, lil_matrix, coo_matrix,\n                          dok_matrix]:\n        X_train_sparse = sparse_format(X_train)\n        X_test_sparse = sparse_format(X_test)\n        sample_weight = np.random.rand(X_train.shape[0])\n\n        # Trained on sparse format with sample weights\n        boost = AdaBoostRegressor(base_estimator=SVR(), random_state=1,\n                                 sample_weight=sample_weight)\n        boost.fit(X_train_sparse, y_train)\n\n        # Predict with sparse input and sample weights\n        sparse_results = boost.predict(X_test_sparse, sample_weight=sample_weight)\n        dense_results = boost.predict(X_test, sample_weight=sample_weight)\n        assert_array_almost_equal(sparse_results, dense_results)\n\n        # Check staged predict\n        sparse_results = boost.staged_predict(X_test_sparse,\n                                              sample_weight=sample_weight)\n        dense_results = boost.staged_predict(X_test,\n                                            sample_weight=sample_weight)\n        for sparse_res, dense_res in zip(sparse_results, dense_results):\n            assert_array_almost_equal(sparse_res, dense_res)\n\n\n\n", "def test_sample_weight_boosting():\n    \"\"\"\n    Check that sample_weight is used correctly for both classification and regression\n    \"\"\"\n    for algorithm in [\"SAMME\", \"SAMME.R\"]:\n        # Classification\n        clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(),\n                                 algorithm=algorithm, random_state=0)\n        X, y_class = datasets.make_classification(n_samples=100,\n                                                  n_features=10,\n                                                  n_informative=3,\n                                                  random_state=0)\n\n        sample_weight = np.random.rand(X.shape[0])\n        clf.fit(X, y_class, sample_weight=sample_weight)\n        y_pred = clf.predict(X)\n        assert (clf.classes_ == np.unique(y_class)).all()\n\n        # Regression\n        clf = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(),\n                                algorithm=algorithm, random_state=0)\n        X, y_regr = datasets.make_regression(n_samples=100, n_features=10,\n                                             random_state=0)\n\n        sample_weight = np.random.rand(X.shape[0])\n        clf.fit(X, y_regr, sample_weight=sample_weight)\n        y_pred = clf.predict(X)\n        assert (clf.n_estimators > 0)\n", "    def test_multi_class_classification():\n        # Check multi-class classification\n        X, y = datasets.make_classification(n_samples=100,\n                n_features=10, n_classes=3, random_state=42)\n        clf = AdaBoostClassifier(DecisionTreeClassifier(), algorithm=\"SAMME\")\n        clf.fit(X, y)\n        assert clf.classes_.shape[0] == 3\n        assert_array_equal(clf.predict(X), clf.predict_proba(X).argmax(axis=1))\n", "def test_missing_data():\n    from sklearn.ensemble import AdaBoostClassifier\n    from sklearn.ensemble import AdaBoostRegressor\n    from sklearn.datasets import make_classification\n    from sklearn.datasets import make_regression\n\n    X, y = make_classification(n_samples=100, n_features=5,\n                               n_informative=3, n_redundant=2)\n    X_with_missing = X.copy()\n    X_with_missing[:, 2] = np.nan\n\n    # Classification\n    clf = AdaBoostClassifier(n_estimators=10, random_state=0)\n    clf.fit(X, y)\n    assert_raises(ValueError, clf.fit, X_with_missing, y)\n\n    # Regression\n    reg = AdaBoostRegressor(n_estimators=10, random_state=0)\n    reg.fit(X, y)\n    assert_raises(ValueError, reg.fit, X_with_missing, y)\n", "def test_sparse_multidimensional_X():\n    # Check classification with sparse multidimensional input.\n\n    class CustomSVC(SVC):\n        \"\"\"SVC variant that records the nature of the training set.\"\"\"\n\n            \"\"\"Modification on fit caries data type for later verification.\"\"\"\n            super().fit(X, y, sample_weight=sample_weight)\n            self.data_type_ = type(X)\n            return self\n\n    X = np.random.rand(50, 3, 3)\n    y = np.random.randint(0, 2, 50)\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n    for sparse_format in [csc_matrix, csr_matrix, lil_matrix, coo_matrix,\n                          dok_matrix]:\n        X_train_sparse = sparse_format(X_train)\n        X_test_sparse = sparse_format(X_test)\n\n        # Trained on sparse multidimensional format\n        sparse_classifier = AdaBoostClassifier(\n            base_estimator=CustomSVC(probability=True),\n            random_state=1,\n            algorithm=\"SAMME\"\n        ).fit(X_train_sparse, y_train)\n\n        # Trained on dense format\n        dense_classifier = AdaBoostClassifier(\n            base_estimator=CustomSVC(probability=True),\n            random_state=1,\n            algorithm=\"SAMME\"\n        ).fit(X_train, y_train)\n\n        # predict\n        sparse_results = sparse_classifier.predict(X_test_sparse)\n        dense_results = dense_classifier.predict(X_test)\n        assert_array_equal(sparse_results, dense_results)\n\n        # decision_function\n        sparse_results = sparse_classifier.decision_function(X_test_sparse)\n        dense_results = dense_classifier.decision_function(X_test)\n        assert_array_almost_equal(sparse_results, dense_results)\n\n        # predict_log_proba\n        sparse_results = sparse_classifier.predict_log_proba(X_test_sparse)"], "sample_370": ["    def test_nested_prefetch_with_related_manager(self):\n        with self.assertNumQueries(1):\n            authors = AuthorWithAge.objects.prefetch_related(\n                Prefetch(\n                    'author',\n                    queryset=Author.objects.prefetch_related(\n                        Prefetch(\n                            'favorite_authors__likes_author',\n                            queryset=AuthorWithAge.objects.all(),\n                        )\n                    ),\n                )\n            )\n\n        with self.assertNumQueries(0):\n            self.assertEqual(authors[0].author.favorite_authors.first().likes_author, self.author2)\n", "    def test_nested_prefetches_using_queryset_with_pk(self):\n        with self.assertNumQueries(2):\n            authors = Author.objects.prefetch_related(\n                Prefetch(\n                    'books',\n                    queryset=Book.objects.filter(title=\"Poems\").select_related('first_time_authors'),\n                    to_attr='first_time_authors',\n                )\n            ).filter(pk=self.author.pk).all()\n        self.assertEqual(authors[0].first_time_authors, [self.first_time_authors[0]])\n\n\n\n", "    def test_nested_prefetch_related_with_to_attr(self):\n        with self.assertNumQueries(3):\n            rooms = Room.objects.filter(house__name='Big house').prefetch_related(\n                Prefetch(\n                    'house',\n                    queryset=House.objects.prefetch_related(\n                        Prefetch('rooms', to_attr='inner_rooms'),\n                    ),\n                    to_attr='nested_house',\n                )\n            )\n        for room in rooms:\n            self.assertEqual(len(room.nested_house.inner_rooms), 1)\n            self.assertEqual(room.nested_house.inner_rooms[0], room)\n", "    def test_nested_prefetch_with_related_managers(self):\n        with self.assertNumQueries(3):\n            objects = House.objects.prefetch_related(\n                Prefetch('rooms', queryset=Room.objects.prefetch_related('house'))\n            )\n            house = objects[0]\n\n        with self.assertNumQueries(0):\n            self.assertEqual(house.rooms[0].house, house)\n\n\n", "    def test_nested_prefetch_with_m2m(self):\n        with self.assertNumQueries(2):\n            rooms = Room.objects.prefetch_related(\n                'house', 'house__main_room',\n                Prefetch(\n                    'house__authors',\n                    queryset=Author.objects.prefetch_related(\n                        'addresses', 'addresses__city',\n                    ),\n                )\n            )\n            self.assertCountEqual(rooms, [self.room])\n\n        with self.assertNumQueries(0):\n            self.assertEqual(self.room.house.main_room, self.room.house.main_room)\n            self.assertEqual(self.room.house.authors.count(), 1)\n            self.assertEqual(self.room.house.authors.first().addresses.count(), 1)\n            self.assertEqual(self.room.house.authors.first().addresses.first().city, 'Anytown')\n"], "sample_985": ["compilation error", "compilation error", "    def test_as_ordered_dict():\n        x, y = symbols('x y')\n        assert Basic().as_ordered_dict() == {}\n        assert Basic(x).as_ordered_dict() == {0: x}\n        assert Basic(x, y).as_ordered_dict() == {0: x, 1: y}\n        assert Basic(x, y, x).as_ordered_dict() == {0: x, 1: y, 2: x}\n", "compilation error", "def test_func_call():\n    x, y, z = symbols('x y z')\n    f = Function('f')\n\n    assert f(x).func is f\n    assert f(x).args == (x,)\n    assert f(x + y).func is f\n    assert f(x + y).args == (x + y,)\n\n    assert (f(x) + f(y)).func is None\n    assert (f(x) * f(y)).func is None\n    assert (f(x) * 2).func is None\n\n    g = Function('g')(x)\n    assert g(x + y).func is g\n    assert g(x + y).args == (x + y,)\n\n    raises(TypeError, lambda: f(x, y))\n    raises(TypeError, lambda: f(x)(y))\n\n\n\n"], "sample_670": ["def test_whitespace_around_operators(expr: str, expected: bool) -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "def test_nested_parens(expr: str, expected: bool) -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "def test_idents_with_operators(ident: str, expected: bool) -> None:\n        matcher = {ident: True}.__getitem__\n        assert evaluate(expr, matcher) is expected\n", "def test_nested_strings(expr: str, expected: bool) -> None:\n    matcher = {\"ident\": True}.__getitem__\n    assert evaluate(expr.replace(\"{ident}\", \"true\"), matcher) is expected\n", "def test_nested_expressions(expr: str, expected: bool) -> None:\n    matcher = {\"ident1\": True, \"ident2\": True, \"ident3\": False}.__getitem__\n    assert evaluate(expr, matcher) is expected\n"], "sample_1030": ["def test_closest_points():\n    points = [(1, 1), (1, 2), (3, 1), (-5, 2), (15, 4)]\n    assert closest_points(*points) == {(Point2D(1, 1), Point2D(1, 2))}\n    points = [(1, 1), (1, 2), (3, 1), (-5, 2), (15, 4), (15, 4.1)]\n    assert closest_points(*points) == {(Point2D(1, 1), Point2D(1, 2))}\n\n", "compilation error", "def test_closest_points():\n    p = [Point2D(1, 1), Point2D(1, 2), Point2D(3, 1), Point2D(5, 2), Point2D(15, 4)]\n    assert closest_points(*p) == {(Point2D(1, 1), Point2D(1, 2))}\n\n\n", "def test_farthest_points():\n    p = [(1, 1), (1, 2), (3, 1), (-5, 2), (15, 4)]\n    assert farthest_points(*p) == {(Point2D(1, 1), Point2D(15, 4))}\n", "def test_farthest_points():\n    p = [(1, 1), (1, 2), (3, 1), (-5, 2), (15, 4)]\n    assert farthest_points(*p) == {(Point2D(1, 1), Point2D(15, 4))}\n"], "sample_276": ["    def test_field_description_render_with_placeholders(self):\n        class PlaceHolderField(models.CharField):\n            description = 'Placeholder value with %s'\n\n        field = PlaceHolderField()\n        self.assertEqual(views.get_readable_field_data_type(field), 'Placeholder value with %s')\n\n\n\n", "    def test_field_no_description(self):\n        field = models.CharField(max_length=100, blank=True)\n        self.assertEqual(views.get_readable_field_data_type(field), 'CharField (up to 100 characters)')\n\n\n", "    def test_special_fields(self):\n        from django.db import models\n\n        class TextFieldWithDescription(models.TextField):\n            description = \"This is a special text field\"\n\n        class ForeignKeyToYourself(models.ForeignKey):\n                super().__init__(to=to, on_delete=models.CASCADE, *args, **kwargs)\n\n        self.assertEqual(views.get_readable_field_data_type(TextFieldWithDescription()), \"This is a special text field\")\n        self.assertEqual(views.get_readable_field_data_type(ForeignKeyToYourself(to='self')),\n                         'ForeignKey to self')\n\n\n\n", "    def test_template_detail_with_existing_template(self):\n        existing_template = \"\"\"\n        {% extends 'admin_doc/base.html' %}\n        {% block content %}\n        <h1>This is a template detail</h1>\n        {% endblock %}\n        \"\"\"\n        with open('admin_doc/template_detail.html', 'w') as f:\n            f.write(existing_template)\n        response = self.client.get(reverse('django-admindocs-templates', args=['admin_doc/template_detail.html']))\n        self.assertContains(response, '<h1>This is a template detail</h1>')\n        os.remove('admin_doc/template_detail.html')\n", "    def test_field_descriptions_are_rendered(self):\n        class PersonWithDescription(models.Model):\n            first_name = models.CharField(max_length=50)\n            last_name = models.CharField(max_length=50)\n            extra_field = CustomField(description=\"This is an extra field\")\n\n        with self.assertRaises(AttributeError):\n            views.get_readable_field_data_type(PersonWithDescription.extra_field)\n\n\n\n"], "sample_712": ["def test_one_hot_encoder_handle_unknown_strategy():\n    X = np.array([[0, 2, 1], [1, 0, 3], [1, 0, 4]])\n    for handle_unknown in ['error', 'ignore', 'fill']:\n        enc = OneHotEncoder(handle_unknown=handle_unknown)\n        with ignore_warnings(category=DeprecationWarning):  \n            enc.fit(X)\n            X_transformed = enc.transform(X)\n\n        if handle_unknown == 'error':\n            assert_raises(ValueError, lambda: enc.transform(np.array([[5, 0, 1]])))\n        elif handle_unknown == 'ignore':\n            assert_array_equal(X_transformed.toarray(),\n                               np.array([[1., 0., 1., 0., 0.],\n                                         [0., 1., 0., 1., 0.],\n                                         [0., 1., 0., 1., 1.]]))\n        elif handle_unknown == 'fill':\n            assert_array_equal(X_transformed.toarray(),\n                               np.array([[1., 0., 1., 0., 0.],\n                                         [0., 1., 0., 1., 0.],\n                                         [0., 1., 0., 1., 1.]]))\n\n\n", "def test_one_hot_encoder_sparse_output():\n    X = np.array([[0, 1], [2, 3]])\n    enc = OneHotEncoder(sparse=True)\n    X_tr = enc.fit_transform(X)\n    assert isinstance(X_tr, sparse.csr_matrix)\n    assert X_tr.shape == (2, 4)\n\n\n\n", "compilation error", "compilation error", "compilation error"], "sample_785": ["        def __repr__(self):\n            return f'MockSplitter(a={self.a}, b={self.b}, c={self.c})'\n", "        def __repr__(self):\n            return f\"MockSplitter(a={self.a}, b={self.b}, c={self.c})\"\n", "        def __repr__(self):\n            return f'MockSplitter(a={self.a}, b={self.b}, c={self.c})'\n", "        def __repr__(self):\n            return f'MockSplitter(a={self.a}, b={self.b}, c={self.c})'\n", "        def __repr__(self):\n            return f\"MockSplitter(a={self.a}, b={self.b}, c={self.c})\"\n"], "sample_635": ["    def test_skip_all_other_docstrings_but_init(self) -> None:\n        \"\"\"Function that matches \"skip all but init\" 'no-docstring-rgx' config option\n        Only __init__ should be checked\n        \"\"\"\n        node = astroid.extract_node(\n            \"\"\"\n        #pylint disable=missing-module-docstring, too-few-public-methods,\n        class MyClass:\n                '''\n                My init docstring\n                :param my_param: My first param\n                '''\n                '''\n                My private method docstring\n                :param my_param: My private param\n                '''\n        \"\"\"\n        )\n        with self.assertAddsMessages(\n            MessageTest(\n                msg_id=\"missing-param-doc\", node=node.body[0], args=(\"my_param\",)\n            )\n        ):\n            self.checker.visit_functiondef(node.body[0])\n        with self.assertNoMessages():\n            self.checker.visit_functiondef(node.body[1])\n\n", "    def test_all_functions_docstring_rgx(self) -> None:\n        \"\"\"Function that matches \"check all functions\" 'no-docstring-rgx' config option\n        An error message is emitted for all functions.\n        \"\"\"\n        node = astroid.extract_node(\n            \"\"\"\n        #pylint disable=missing-module-docstring, too-few-public-methods,\n        class MyClass:\n                '''\n                My init docstring\n                '''\n                '''some docstring'''\n        \"\"\"\n        )\n        with self.assertAddsMessages(\n            MessageTest(\n                msg_id=\"missing-param-doc\", node=node.body[0], args=(\"my_param\",)\n            ),\n            MessageTest(msg_id=\"missing-param-doc\", node=node.body[1], args=(\"my_param\",)),\n        ):\n            self.checker.visit_functiondef(node.body[0])\n", "    def test_missing_return_type_docstring_sphinx(self) -> None:\n        \"\"\"Example of a function missing return type documentation in a Sphinx style docstring\n        \"\"\"\n        node = astroid.extract_node(\n            \"\"\"\n            '''docstring ...\n\n            :param arg: An argument.\n            '''\n            pass\n        \"\"\"\n        )\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"missing-return-type-doc\", node=node)\n        ):\n            self.checker.visit_functiondef(node)\n", "    def test_param_doc_string_order_sphinx(self) -> None:\n        \"\"\"\n        Ensure that the order of parameters in docstring and function definition matches in Sphinx style\n        \"\"\"\n        node = astroid.extract_node(\n            \"\"\"\n        class Foo(object):\n                '''docstring ...\n\n                :param arg1: First parameter.\n                :param arg3: Third parameter.\n                :param arg2: Second parameter.\n                '''\n                pass\n        \"\"\"\n        )\n        with self.assertAddsMessages(\n            MessageTest(\n                msg_id=\"param-doc-string-order-mismatch\",\n                node=node.body[0],\n                args=(\"arg1\", \"arg2\", \"arg3\"),\n            ),\n        ):\n            self.checker.visit_functiondef(node.body[0])\n\n\n", "    def test_missing_return_type_annotation_sphinx(self) -> None:\n        \"\"\"Example of a function with a missing return type annotation\n        in a Sphinx style docstring\n        \"\"\"\n        node = astroid.extract_node(\n            \"\"\"\n        class Foo(object):\n            @property\n                '''docstring ...\n\n                :raises RuntimeError: Always\n                '''\n                raise RuntimeError()\n        \"\"\"\n        )\n\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"missing-return-type-doc\", node=node)\n        ):\n            self.checker.visit_functiondef(node)\n\n"], "sample_1095": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_167": ["def test_naturaltime_with_milliseconds(self):\n    test_list = [\n        now - datetime.timedelta(microseconds=500000),\n        now - datetime.timedelta(microseconds=1500000),\n        now + datetime.timedelta(microseconds=1000000),\n        now + datetime.timedelta(microseconds=3000000),\n    ]\n    result_list = [\n        'a second ago',\n        'a second ago',\n        'a second from now',\n        'a second from now',\n    ]\n    orig_humanize_datetime, humanize.datetime = humanize.datetime, MockDateTime\n    try:\n        with translation.override('en'):\n            self.humanize_tester(test_list, result_list, 'naturaltime')\n    finally:\n        humanize.datetime = orig_humanize_datetime\n", "    def test_naturaltime_with_microseconds(self):\n        test_list = [\n            now - datetime.timedelta(microseconds=123456),\n            now - datetime.timedelta(microseconds=567890),\n            now - datetime.timedelta(microseconds=1000000),\n            now - datetime.timedelta(microseconds=1000000000),\n        ]\n        result_list = [\n            'now',\n            'now',\n            'now',\n            'now',\n        ]\n        self.humanize_tester(test_list, result_list, 'naturaltime')\n", "compilation error", "    def test_naturaltime_fuzzy(self):\n        test_list = [\n            datetime.datetime(2012, 3, 5, 12, 30, tzinfo=utc),\n            datetime.datetime(2012, 3, 3, 12, 30, tzinfo=utc),\n            datetime.datetime(2012, 3, 1, 12, 30, tzinfo=utc),\n            datetime.datetime(2011, 3, 1, 12, 30, tzinfo=utc),\n        ]\n        result_list = [\n            'a week ago',\n            '2 weeks ago',\n            '1 month ago',\n            'over a year ago',\n        ]\n        with translation.override('en'):\n            self.humanize_tester(test_list, result_list, 'naturaltime')\n", "    def test_naturaltime_with_datetime_args(self):\n        test_list = [\n            datetime.datetime(2012, 3, 9, 2, 30),\n            datetime.datetime(2012, 3, 9, 2, 29),\n            datetime.datetime(2012, 3, 9, 2, 28),\n            datetime.datetime(2012, 3, 8, 2, 30),\n            datetime.datetime(2012, 3, 10, 2, 30),\n            datetime.datetime(2012, 3, 9, 12, 30),\n            datetime.datetime(2012, 3, 9, 12, 29),\n            datetime.datetime(2012, 3, 9, 12, 28),\n            datetime.datetime(2012, 3, 8, 12, 30),\n            datetime.datetime(2012, 3, 10, 12, 30),\n            datetime.datetime.now(tz=utc),\n        ]\n        result_list = [\n            'now',\n            '29 seconds ago',\n            'before a minute ago',\n            '1 day ago',\n            'tomorrow',\n            '10 hours ago',\n            '10 hours ago',\n            '10 hours ago',\n            '1 day ago',\n            '1 day from now',\n            'now',\n        ]\n        with translation.override('en'):\n            self.humanize_tester(test_list, result_list, 'naturaltime')\n"], "sample_981": ["compilation error", "compilation error", "def test_perm_equality():\n    p = Permutation([0, 1, 2, 3])\n    q = Permutation([0, 1, 2, 3])\n    r = Permutation([3, 2, 1, 0])\n    assert p == q\n    assert p != r\n    assert _af_parity([0, 1, 2, 3]) == 0\n    assert _af_parity([3, 2, 1, 0]) == 1\n    assert (Permutation([0, 1, 2, 3]) == Permutation([0, 1, 2, 3]))\n    assert (Permutation([0, 1, 2, 3]) != Permutation([3, 2, 1, 0]))\n    assert type(Permutation([0, 1, 2, 3])) == Permutation\n\n\n", "def test_inversion_vector():\n    p = Permutation([0, 1, 2, 3])\n    assert p.inversion_vector() == [0, 0, 0, 0]\n    q = Permutation([3, 2, 1, 0])\n    assert q.inversion_vector() == [3, 2, 1, 0]\n    p = Permutation([2, 5, 1, 0, 3, 6, 4])\n    assert p.inversion_vector() == [2, 1, 0, 0, 0, 0, 0]\n    q = Permutation([[6], [5], [0, 1, 2, 3, 4]])\n    assert q.inversion_vector() == [3, 1, 0, 0, 0, 0, 0]\n    assert Permutation([0]).inversion_vector() == [0]\n    assert Permutation([0, 1]).inversion_vector() == [0]\n    assert Permutation([1, 0]).inversion_vector() == [1]\n    assert Permutation([1, 2, 0]).inversion_vector() == [1]\n    assert Permutation([2, 1, 0]).inversion_vector() == [1]\n\n\n\n", "def test_inversions():\n    p = Permutation([2, 5, 1, 6, 3, 0, 4])\n    assert p.inversions() == 7\n    q = Permutation([3, 2, 1, 0])\n    assert q.inversions() == 0\n    r = Permutation([1, 3, 2, 0])\n    assert r.inversions() == 2\n    assert Permutation([0, 1, 2, 3]).inversions() == 0\n    assert Permutation([3, 1, 2, 0]).inversions() == 2\n    assert Permutation([1, 0, 2, 3]).inversions() == 1\n    assert Permutation([3, 2, 0, 1]).inversions() == 3\n\n    # test the merge-sort with a longer permutation\n    big = list(p) + list(range(p.max() + 1, p.max() + 130))\n    assert Permutation(big).inversions() == 7\n\n    a = [Permutation.unrank_nonlex(4, i) for i in range(24)]\n    assert [p.inversions() for p in a] == list(range(24))\n"], "sample_222": ["    def test_spooled_temporary_file_move(self):\n        with tempfile.SpooledTemporaryFile(max_size=1024) as temp_file:\n            temp_file.write(b\"This is some test data.\")\n            temp_file.seek(0)\n            with file_move_safe(temp_file, \"test.txt\", allow_overwrite=True) as moved_file:\n                self.assertEqual(moved_file.read(), b\"This is some test data.\")\n", "    def test_spooled_temporary_file_in_non_exclusive_lock(self):\n        with tempfile.SpooledTemporaryFile() as temp:\n            temp.write(b\"foo bar baz quux\\n\")\n            with open(temp.name, 'rb') as f:\n                locks.lock(f, locks.LOCK_SH)\n                # Try to open the file again (should not block)\n                with open(temp.name, 'rb') as f2:\n                    self.assertEqual(f2.read(), b\"foo bar baz quux\\n\")\n", "    def test_spooled_temp_file_seek(self):\n        with tempfile.SpooledTemporaryFile() as temp:\n            temp.write(b\"foo bar baz quux\\n\")\n            temp.seek(0)\n            data = temp.read()\n            self.assertEqual(data, b\"foo bar baz quux\\n\")\n            temp.seek(3)\n            self.assertEqual(temp.read(), b\" bar baz quux\\n\")\n\n\n\n", "    def test_spooled_temp_seek_and_read(self):\n        with tempfile.SpooledTemporaryFile() as temp:\n            temp.write(b\"foo bar baz quux\\n\")\n            temp.seek(0)\n            django_file = File(temp, name=\"something.txt\")\n            self.assertEqual(django_file.read(4), b'foo ')\n            self.assertEqual(django_file.tell(), 4)\n            django_file.seek(7)\n            self.assertEqual(django_file.read(4), b' bar')\n", "    def test_spooled_temp_file_content(self):\n        with tempfile.SpooledTemporaryFile(max_size=100) as temp:\n            temp.write(b\"Testing spooled temporary file content... \")\n            temp.seek(0)\n            content = temp.read()\n            self.assertEqual(content, b\"Testing spooled temporary file content... \")\n\n        with tempfile.SpooledTemporaryFile() as temp:\n            # Test overflow behavior:\n            temp.write(b\"This will overflow the buffer \")\n            temp.write(b\"and potentially be written to disk\")\n            temp.seek(0)\n            content = temp.read()\n            self.assertTrue(b\"This will overflow the buffer \" in content)\n            self.assertTrue(b\"and potentially be written to disk\" in content)\n\n\n"], "sample_1078": ["def test_indexed_sorting():\n    i, j = symbols('i j', integer=True)\n    a = IndexedBase(\"a\")\n    assert sorted([a[i], a[j]]) == [a[i], a[j]]\n    assert sorted([a[i], a[i], a[j]]) == [a[i], a[i], a[j]]\n    assert sorted([a[j], a[i]]) == [a[i], a[j]]\n\n\n\n", "def test_Indexed_subs_with_complex_indices():\n    i, j = symbols('i j', integer=True)\n    A = Indexed('A', i, i + j)\n    assert A.subs(i, 1).subs(j, 2) == A.subs(i, 1 + j, j)\n    assert (A.subs(i, 1) + A.subs(i, 2)).subs(j, 2) == A.subs(i, 1 + j, 2) + A.subs(i, 2 + j, 2)\n\n\n\n", "def test_Indexed_subs_with_non_integer():\n    i, j = symbols('i j', integer=True)\n    A = Indexed('A', i, j)\n    with pytest.raises(TypeError):\n        A.subs(i, 2.5)\n    with pytest.raises(TypeError):\n        A.subs(j, 2.5)\n", "def test_Indexed_limits_of_integration():\n    i, j = symbols('i j', integer=True)\n    A = Indexed('A', i, j)\n    assert (A).integrate((i, 0, 1)).args == (A,)\n    assert (A).integrate((i, 0, 1)).limits == (i, 0, 1)\n    assert (A).integrate((i, 0, 1), (j, 2, 3)).args == (A,)\n    assert (A).integrate((i, 0, 1), (j, 2, 3)).limits == ((i, 0, 1), (j, 2, 3))\n", "def test_Indexed_subs_with_function():\n    n = symbols(\"n\", integer=True)\n    i = Idx(\"i\", (0, n))\n    A = IndexedBase(\"A\")\n    f = Function(\"f\")\n\n    assert Subs(f(A[i]), f(A[i]), A[i]).diff(A[i]) == 1\n    assert Subs(f(A[i]), f(A[i]), A[j]).diff(A[j]) == 0\n    assert Subs(f(A[i]), f(A[i]), A[j]).diff(A[i]) == 0\n\n    assert Subs(f(A[i])**2, f(A[i]), A[i]).diff(A[i]) == 2 * f(A[i])\n    assert Subs(f(A[i])**2, f(A[i]), A[j]).diff(A[j]) == 0\n    assert Subs(f(A[i])**2, f(A[i]), A[j]).diff(A[i]) == 0\n\n\n\n    g = Function(\"g\")\n    assert Subs(f(A[i]) + g(A[i]), f(A[i]) + g(A[i]), A[i]).diff(A[i]) == 1\n    assert Subs(f(A[i]) + g(A[i]), f(A[i]) + g(A[i]), A[j]).diff(A[j]) == 0\n    assert Subs(f(A[i]) + g(A[i]), f(A[i]) + g(A[i]), A[j]).diff(A[i]) == 0\n\n\n\n    assert Subs(f(A[i]) + g(A[i]), f(A[i]) + g(A[i]), A[i]).diff(A[i]) == 1\n    assert Subs(f(A[i]) + g(A[i]), f(A[i]) + g(A[i]), A[j]).diff(A[j]) == 0\n    assert Subs(f(A[i]) + g(A[i]), f(A[i]) + g(A[i]),"], "sample_654": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_45": ["    def test_trunc_func_with_tz_aware_datetimefields(self):\n        start_datetime = timezone.make_aware(datetime(2015, 6, 15, 14, 30, 50, 321), tz=pytz.UTC)\n        end_datetime = timezone.make_aware(datetime(2016, 6, 15, 14, 10, 50, 123), tz=pytz.UTC)\n        self.create_model(start_datetime, end_datetime)\n        self.create_model(end_datetime, start_datetime)\n\n        melb = pytz.timezone('Australia/Melbourne')\n        \n        # Test with timezone aware datetime fields\n        class DTModelWithTzAware(models.Model):\n            start_datetime = models.DateTimeField(timezone=True)\n            end_datetime = models.DateTimeField(timezone=True)\n\n        DTModelWithTzAware.objects.create(start_datetime=start_datetime, end_datetime=end_datetime)\n\n        self.assertQuerysetEqual(\n            DTModelWithTzAware.objects.annotate(\n                truncated=TruncMinute('start_datetime')\n            ).order_by('start_datetime'),\n            [\n                (start_datetime, truncate_to(start_datetime, 'minute')),\n                (end_datetime, truncate_to(end_datetime, 'minute'))\n            ],\n            lambda m: (m.start_datetime, m.truncated)\n        )\n        self.assertEqual(\n            DTModelWithTzAware.objects.annotate(\n                truncated=TruncHour('start_datetime', tzinfo=melb)\n            ).order_by('start_datetime').first().truncated.tzinfo,\n            melb\n        )\n\n", "    def test_trunc_func_with_timezone_and_output_field_none(self):\n        start_datetime = datetime(2015, 6, 15, 23, 30, 1, 321)\n        end_datetime = datetime(2015, 6, 16, 13, 11, 27, 123)\n        start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n        end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n\n        melb = pytz.timezone('Australia/Melbourne')\n        with self.assertRaises(TypeError) as context:\n            DTModel.objects.annotate(\n                truncated=Trunc('start_datetime', 'day', output_field=None, tzinfo=melb)\n            ).get()\n        self.assertIn(\"Output field must be provided for Trunc operation\", str(context.exception))\n", "    def test_trunc_func_with_timezone_and_timezone_aware_datetime(self):\n        start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n        end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n        start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n        end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n        melb = pytz.timezone('Australia/Melbourne')\n\n        for func in [TruncYear, TruncQuarter, TruncMonth, TruncWeek, TruncDay, TruncHour, TruncMinute, TruncSecond]:\n            with self.subTest(func=func):\n                self.assertQuerysetEqual(\n                    DTModel.objects.annotate(\n                        truncated=func('start_datetime', tzinfo=melb)\n                    ).order_by('start_datetime'),\n                    [\n                        (start_datetime, truncate_to(start_datetime.astimezone(melb), func.__name__.lower(), melb)),\n                        (end_datetime, truncate_to(end_datetime.astimezone(melb), func.__name__.lower(), melb))\n                    ],\n                    lambda m: (m.start_datetime, m.truncated)\n                )\n\n", "    def test_trunc_func_with_custom_timezone(self):\n        start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n        end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n        if settings.USE_TZ:\n            start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n            end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n        custom_timezone = pytz.timezone('Europe/Madrid')\n\n        with timezone.override(custom_timezone):\n            self.assertQuerysetEqual(\n                DTModel.objects.annotate(\n                    truncated=TruncMonth('start_datetime', output_field=DateTimeField(), tzinfo=custom_timezone)\n                ).order_by('start_datetime'),\n                [\n                    (start_datetime, truncate_to(start_datetime, 'month', custom_timezone)),\n                    (end_datetime, truncate_to(end_datetime, 'month', custom_timezone))\n                ],\n                lambda m: (m.start_datetime, m.truncated)\n            )\n\n", "    def test_trunc_with_ambiguous_datetime(self):\n        \"\"\"\n        Test truncation when source datetime is ambiguous (e.g., DST rollover)\n        \"\"\"\n        start_datetime = datetime(2017, 3, 12, 1, 0, 0)\n        start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n        self.create_model(start_datetime)\n        \n        with self.assertRaises(pytz.AmbiguousTimeError):\n            DTModel.objects.annotate(truncated=TruncHour('start_datetime')).first()\n\n"], "sample_393": ["    def test_po_file_updated_on_change(self):\n        \"\"\"PO files are updated when there are new changes.\"\"\"\n        # Make a change in the source code\n        with open(os.path.join(self.test_dir, \"app_with_locale\", \"templates/test.html.py\"), \"a\") as f:\n            f.write(\"\\n    <p>This is a new line</p>\")\n\n        out, po_contents = self._run_makemessages()\n        self.assertNotEqual(po_contents, self.original_po_contents)\n        # Assert that the new line was translated\n        self.assertIn('This is a new line', po_contents)\n", "    def test_po_updated_when_changes_occur(self):\n        \"\"\"PO file is updated when there are changes.\"\"\"\n        with open(self.PO_FILE, \"a\") as fp:\n            fp.write(\n                \"\\nmsgid \\\"This is a new string\\\"\\nmsgstr \\\"This is the translation\\\"\\n\"\n            )\n        _, po_contents = self._run_makemessages()\n        self.assertNotEqual(po_contents, self.original_po_contents)\n        self.assertIn(\n            \"\\nmsgid \\\"This is a new string\\\"\\nmsgstr \\\"This is the translation\\\"\",\n            po_contents,\n        )\n", "    def test_po_updated_if_changes_in_source(self):\n        # Introduce a change in the source file\n        with open(os.path.join(self.test_dir, \"app_with_locale\", \"__init__.py\"), \"a\") as f:\n            f.write(\"\\nprint('New line added')\")\n\n        _, po_contents = self._run_makemessages()\n        # Assert the po file is updated\n        self.assertNotEqual(po_contents, self.original_po_contents)\n", "    def test_po_updated_on_change(self):\n        # Modify a single translation to trigger an update.\n        content = self.original_po_contents.replace(\n            \"This is a string.\", \"This is an updated string.\"\n        )\n        with open(self.PO_FILE, \"w\") as fp:\n            fp.write(content)\n        _, po_contents = self._run_makemessages()\n        self.assertNotEqual(po_contents, self.original_po_contents)\n", "    def test_po_updated_on_change(self):\n        \"\"\"PO files are updated when content has changed.\"\"\"\n        with open(self.PO_FILE, \"a\") as fp:\n            fp.write(\"\\nmsgid \\\"This message should not be in the original file\\\"\\nmsgstr \\\"\\\"\")\n\n        _, po_contents = self._run_makemessages()\n        self.assertNotEqual(po_contents, self.original_po_contents)\n"], "sample_1126": ["def test_bra_ket():\n    from sympy.physics.quantum.state import Ket, Bra\n\n    psi = Ket('psi')\n    phi = Ket('phi')\n    a = Bra('a')\n    b = Bra('b')\n\n    assert Dagger(psi) == Bra(psi)\n    assert Dagger(phi) == Bra(phi)\n    assert Dagger(a) == Ket(a)\n    assert Dagger(b) == Ket(b)\n", "compilation error", "def test_dagger_symbols():\n    a, b = symbols('a b', cls=Dagger)\n    assert a.is_Dagger is True\n    assert b.is_Dagger is True\n    assert Dagger(a) == a\n    assert Dagger(b) == b\n", "def test_composite_expressions():\n    x = symbols('x', complex=True)\n    y = symbols('y', complex=True)\n    A = Operator('A')\n    B = Operator('B')\n\n    assert Dagger(Mul(A, B)) == Dagger(B)*Dagger(A)\n    assert Dagger(x + A) == Dagger(x) + Dagger(A)\n    assert Dagger(x*A) == conjugate(x)*Dagger(A)\n    assert Dagger(A**2) == Dagger(A)**2\n", "def test_bra_ket():\n    from sympy.physics.quantum.state import Ket, Bra\n    a = symbols('a')\n    b = symbols('b')\n    assert Dagger(Ket(a)) == Bra(a)\n    assert Dagger(Bra(b)) == Ket(b)\n"], "sample_40": ["def test_proper_motion():\n    pm = 0.2 * u.mas / u.yr\n    distance = 100 * u.pc\n    proper_motion_velocity = pm.to(u.km / u.s / u.yr, u.proper_motion(distance))\n    assert_allclose(proper_motion_velocity.value, 0.0569568)\n\n\n", "def test_inverse_equivalencies():\n    # ensure we can also get equivalencies for inverse transformations\n    inv_equivalencies = u.spectral().inverse()\n    assert set(inv_equivalencies) == set(u.spectral())\n    # test some conversions\n    nu = u.Quantity(143 * u.GHz, equivalencies=u.spectral())\n    assert_allclose(nu.to(u.erg/u.Hz, equivalencies=inv_equivalencies).value, 1.0)\n\n    lam = u.Quantity(u.Quantity(143 * u.GHz, equivalencies=u.spectral()).to(u.um), equivalencies=inv_equivalencies)\n    assert_allclose(lam.value, 2.1*u.micrometer)\n\n\n\n", "def test_inverse_equivalencies():\n    from .._astropy_units_ import (\n        u,\n    )\n    with pytest.raises(ValueError):\n        u.m.to_equivalent(u.kg, equivalencies=[(u.m,)])\n\n    with pytest.raises(ValueError):\n        u.m.to_equivalent(u.kg, equivalencies=[(u.m, 5.0)])\n\n    with pytest.raises(u.UnitsError):\n        u.m.to_equivalent(u.kg, equivalencies=[(u.m, u.l)])\n    \n    # test inverse equivalencies\n    \n    # Make sure that the inverse equivalency exists\n    omega_B = 2 * np.pi * (50 * u.arcsec) ** 2\n    new_beam = (5 * u.beam).to(u.sr, u.equivalencies.beam_angular_area(omega_B))\n    inverse_beam = new_beam.to(u.beam, u.equivalencies.beam_angular_area(omega_B))\n    assert_allclose(inverse_beam.value, 5)\n    \n    # test practical case, we should get back\n    # the original beam value\n    flux_density = (5 * u.Jy / u.beam).to(u.MJy / u.sr,\n        u.equivalencies.beam_angular_area(omega_B))\n    # we can use the inverse equivalency\n    # to get the flux density back in the original units\n    flux_density_original = flux_density.to(u.Jy / u.beam,\n        u.equivalencies.beam_angular_area(omega_B))\n    assert_allclose(flux_density_original.value, 5 * u.Jy / u.beam)\n\n\n\n", "def test_velocity_equivalencies():\n    from astropy.constants import c\n    v_km_s = 100 * u.km / u.s\n    v_lambda = v_km_s.to(u.lambda_unit, equivalencies=u.velocity_lambda(c.value))\n    assert_quantity_allclose(v_lambda.value, -100 * u.Angstrom)\n    v_theta = 100 * u.arcsec / u.s * 100 * u.pc\n    v_theta = (v_theta * 100 * u.pc).to(u.km/u.s, equivalencies=u.velocity_theta())\n    assert_quantity_allclose(v_theta.value, 3e-4 * u.km / u.s)\n", "def test_custom_equivalencies():\n    class MyEquivalency(u.Equivalency):\n            super().__init__(\n                unit_a=a, unit_b=b,\n                equivalency_function=lambda x: x * a.to_value(b)\n            )\n\n    u.add_equivalency(MyEquivalency(1 * u.m, 100 * u.cm))\n    assert_allclose(1 * u.m.to_value(u.cm), 100)\n    assert_allclose(100 * u.cm.to_value(u.m), 1)\n\n    with pytest.raises(ValueError):\n        u.add_equivalency(MyEquivalency(1 * u.m, 1 * u.m))\n\n    with pytest.raises(TypeError):\n        u.add_equivalency(MyEquivalency(1 * u.m, [1, 2, 3]))\n"], "sample_872": ["def test_label_ranking_avg_precision_score_with_multiple_label_per_sample():\n    y_true = np.array([[0, 1, 1], [1, 0, 1], [0, 1, 0]])\n    y_score = np.array([[0.1, 0.8, 0.5], [0.5, 0.2, 0.7], [0.6, 0.3, 0.9]])\n    result = label_ranking_average_precision_score(y_true, y_score)\n    assert result == pytest.approx(0.67)\n", "def test_label_ranking_avg_precision_score_should_handle_empty_y_true():\n    # Test that label_ranking_avg_precision_score handles empty y_true correctly.\n    y_score = np.array([[0.5, 0.9, 0.6], [0, 0, 1]])\n    result = label_ranking_average_precision_score(y_true=[], y_score)\n    assert result == 0.0\n", "def test_label_ranking_average_precision_score_with_sample_weight(\n    y_true, y_score, sample_weight, expected_result", "def test_label_ranking_avg_precision_score_should_handle_empty_label_ranking_correctly():\n    # Test that the function correctly handles empty label rankings.\n    y_true = np.array([])\n    y_score = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\n    result = label_ranking_average_precision_score(y_true, y_score)\n    assert result == pytest.approx(0.0)\n", "def test_label_ranking_avg_precision_score_with_ties():\n    # Test handling of ties in y_true\n    y_true = np.array([\n        [0, 1, 1],\n        [0, 1, 2],\n        [1, 2, 2],\n    ])\n    y_score = np.array([\n        [0.3, 0.7, 0.9],\n        [0.2, 0.8, 0.6],\n        [0.1, 0.5, 0.9],\n    ])\n    result = label_ranking_average_precision_score(y_true, y_score)\n    assert result == pytest.approx(0.5)\n\n"], "sample_348": ["    def test_action_not_callable(self):\n        class BandAdmin(ModelAdmin):\n            actions = ('NotACallable',)\n\n        self.assertIsInvalid(\n            BandAdmin, Band,\n            'An action must be a callable object, but \"NotACallable\" is not.',\n            id='admin.E125',\n        )\n", "    def test_actions_with_permissions(self):\n        @admin.action(permissions=['special_permission'])\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (my_action,)\n\n        self.assertIsInvalid(\n            BandAdmin, Band,\n            'BandAdmin must define a has_special_permission() method for '\n            'the my_action action.',\n            id='admin.E129',\n        )\n\n\n", "        def action2(modeladmin, request, queryset):\n            pass\n", "    def test_actions_invalid_type(self):\n        class BandAdmin(ModelAdmin):\n            actions = 1\n\n        self.assertIsInvalid(\n            BandAdmin, Band,\n            \"The value of 'actions' must be a list or tuple.\",\n            id='admin.E128',\n        )\n", "    def test_actions_no_permissions(self):\n        @admin.action\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (action,)\n\n        self.assertIsInvalid(\n            BandAdmin, Band,\n            'Each action in BandAdmin.actions must define its permissions.',\n            id='admin.E131',\n        )\n"], "sample_33": ["def test_to_native_string():\n    assert misc.to_native_string(b'hello world') == 'hello world'\n    assert misc.to_native_string(u'hello world') == 'hello world'\n    assert misc.to_native_string(u'hello \\xc3\\x85 world') == 'hello world'\n    # TODO: Add a test for byteslike objects that aren't bytes or unicode strings.\n", "def test_ordered_descriptor_container_inheritance():\n    class Base(metaclass=misc.OrderedDescriptorContainer):\n        _inherit_descriptors_ = (OrderedDescriptor,)\n        \n        class OrderedDescriptor(OrderedDescriptor):\n            _class_attribute_ = 'ordered_descriptors'\n            \n        \n    class Subclass(Base):\n        pass\n\n    assert hasattr(Subclass, 'ordered_descriptors')\n    assert isinstance(Subclass.ordered_descriptors, OrderedDict)\n    assert Subclass.ordered_descriptors == OrderedDict()\n    \n    \n    with pytest.raises(ValueError) as e:\n        Subclass.ordered_descriptors['new_descriptor'] = OrderedDescriptor()\n    assert str(e.value) == \"OrderedDescriptor instances cannot be added to 'ordered_descriptors' after class definition\"\n\n\n\n", "compilation error", "compilation error", "def test_ordered_descriptor_container():\n    class TypedAttribute(OrderedDescriptor):\n        _class_attribute_ = '_typed_attributes'\n        _name_attribute_ = 'name'\n\n            super().__init__()\n            self.type = type\n\n            if obj is None:\n                return self\n            return obj.__dict__[self.name]\n\n            if not isinstance(value, self.type):\n                raise ValueError(f'{obj.__class__.__name__}.{self.name} must be of type {self.type!r}')\n            obj.__dict__[self.name] = value\n\n    class Point2D(metaclass=OrderedDescriptorContainer):\n        x = TypedAttribute((float, int))\n        y = TypedAttribute((float, int))\n\n    p1 = Point2D(1.0, 2.0)\n    p2 = Point2D('a', 'b')\n\n    assert Point2D._typed_attributes == OrderedDict([\n        ('x', <__main__.TypedAttribute object at ...>),\n        ('y', <__main__.TypedAttribute object at ...>)\n    ])\n    \n    assert p1.x == 1.0\n    assert p1.y == 2.0\n\n    assert p2.x == 'a'\n    assert p2.y == 'b'\n    \n    \n    class Point2D_Inherit(Point2D):\n        z = TypedAttribute((float, int))\n        _inherit_descriptors_ = (TypedAttribute,)\n\n    p3 = Point2D_Inherit(1.0, 2.0, 3.0)\n    assert Point2D_Inherit._typed_attributes == OrderedDict([\n        ('x', <__main__.TypedAttribute object at ...>),\n        ('y', <__main__.TypedAttribute object at ...>),\n        ('z', <__main__.TypedAttribute object at ...>)\n    ])\n    assert p3.z == 3.0\n\n\n\n"], "sample_639": ["compilation error", "compilation error", "def test_check_consistency():\n    checker = OtherBasicChecker()\n    checker.msgs = {\"W1234\": (\"message\", \"symbol\", \"description\"), \"W5678\": (\"message\", \"symbol\", \"description\")}\n    checker.check_consistency()\n", "compilation error", "compilation error"], "sample_18": ["    def test_view(self):\n        q_view = self.q.view(dtype=object)\n        assert_info_equal(q_view, self.q)\n", "    def test_structured_arithmetic(self):\n        q1 = self.q +  self.q\n        assert_no_info(q1)\n        q2 = self.q * 2\n        assert_no_info(q2)\n        q3 = self.q / self.q[\"p\"]\n        assert_no_info(q3)\n        q4 = q3 * u.s\n        assert_no_info(q4) \n", "    def test_array_methods(self):\n        q = self.q.copy()\n        assert_info_equal(q.reshape(-1, 2), self.q)\n        assert_info_equal(q.transpose(), self.q)\n        assert_info_equal(q.T, self.q)\n        assert_info_equal(q.flatten(), self.q)\n        assert_info_equal(q.astype(np.float32), self.q)\n", "    def test_info_propagation_from_array_creation(self):\n        array_with_info = np.array([(1.0, 2.0), (3.0, 4.0)], dtype=[(\"p\", \"f8\"), (\"v\", \"f8\")],\n                                  info=self.q.info)\n        q = u.Quantity(array_with_info, \"m, m/s\")\n        assert_info_equal(q, self.q)\n\n", "    def test_getattr(self):\n        q_p = getattr(self.q, \"p\")\n        assert_no_info(q_p)\n\n\n\n\n"], "sample_50": ["    def test_empty_dbinfo(self):\n        with self.assertRaises(ValueError):\n            DatabaseClient.runshell_db({})\n", "    def test_empty_params(self):\n        self.assertEqual(\n            self._run_it({}), (\n                ['psql'], None\n            )\n        )\n", "    def test_empty_params(self):\n        self.assertEqual(\n            self._run_it({}), (\n                ['psql'],\n                None,\n            )\n        )\n", "    def test_empty_params(self):\n        self.assertEqual(\n            self._run_it({}), (\n                ['psql'],\n                None,\n            )\n        )\n", "    def test_empty_password(self):\n        self.assertEqual(\n            self._run_it({\n                'database': 'dbname',\n                'user': 'someuser',\n                'password': '',\n                'host': 'somehost',\n                'port': '444',\n            }), (\n                ['psql', '-U', 'someuser', '-h', 'somehost', '-p', '444', 'dbname'],\n                '',\n            )\n        )\n"], "sample_634": ["    def test_expand_modules_with_ignore_paths(\n        self, files_or_modules, ignore_paths, expected", "    def test_expand_modules_ignore_paths(self, files_or_modules, ignore_paths, expected):\n        ignore_list, ignore_list_re = [], []\n        modules, errors = expand_modules(\n            files_or_modules,\n            ignore_list,\n            ignore_list_re,\n            ignore_paths,\n        )\n        modules.sort(key=lambda d: d[\"name\"])\n        assert modules == expected\n        assert not errors\n", "        def test_expand_modules_with_list(self, files_or_modules):\n            \"\"\"Test expand_modules with list of files\"\"\"\n            ignore_list, ignore_list_re = [], []\n            modules, errors = expand_modules(\n                files_or_modules,\n                ignore_list,\n                ignore_list_re,\n                get_global_option(self, \"ignore-paths\"),\n            )\n            modules.sort(key=lambda d: d[\"name\"])\n            expected = [this_file, unittest_lint] \n            assert modules == expected \n            assert not errors\n\n", "    def test_expand_modules_ignore_paths(self, files_or_modules, ignore_paths):\n        \"\"\"Test expand_modules with custom ignore-paths\"\"\"\n        ignore_list, ignore_list_re = [], []\n        ignore_list_paths_re = [re.compile(p) for p in ignore_paths]\n        modules, errors = expand_modules(\n            files_or_modules,\n            ignore_list,\n            ignore_list_re,\n            ignore_list_paths_re,\n        )\n        modules.sort(key=lambda d: d[\"name\"])\n        assert modules == [\n            this_file\n        ]  # Assuming the ignored files are not in the output\n        assert not errors\n\n", "    def test_expand_modules_ignore_paths(self, files_or_modules, ignore_paths, expected):\n        ignore_list, ignore_list_re = [], []\n        modules, errors = expand_modules(\n            files_or_modules,\n            ignore_list,\n            ignore_list_re,\n            ignore_paths,\n        )\n        modules.sort(key=lambda d: d[\"name\"])\n        assert modules == expected\n        assert not errors\n"], "sample_823": ["def test_check_mixed_input_types():\n    # Ensures that checks handle mixed input types gracefully.\n    XA = np.resize(np.arange(40), (5, 8)).astype(np.float32)\n    XB = np.resize(np.arange(40), (5, 8)).astype(np.int32)\n    XA_checked, XB_checked = check_pairwise_arrays(XA, XB)\n    assert XA_checked.dtype == XB_checked.dtype\n    assert XA_checked.dtype == np.float32\n\n\n\n", "compilation error", "def test_check_pairwise_arrays_with_empty_inputs():\n    # Ensure that check_pairwise_arrays handles empty arrays correctly.\n\n    XA = np.array([])\n    XB = np.array([])\n    XA_checked, XB_checked = check_pairwise_arrays(XA, XB)\n    assert XA_checked.size == 0\n    assert XB_checked.size == 0\n\n    XA = np.array([])\n    XB = np.array([[1, 2]])\n    with pytest.raises(ValueError):\n        check_pairwise_arrays(XA, XB)\n\n    XA = np.array([[1, 2]])\n    XB = np.array([])\n    with pytest.raises(ValueError):\n        check_pairwise_arrays(XA, XB)\n", "compilation error", "def test_check_invalid_array_types():\n    # Ensure an error is raised for invalid input types.\n\n    with pytest.raises(TypeError):\n        check_pairwise_arrays(1, np.array([1, 2]))\n\n    with pytest.raises(TypeError):\n        check_pairwise_arrays(np.array([1, 2]), \"hello\")\n\n    with pytest.raises(TypeError):\n        check_pairwise_arrays(object(), np.array([1, 2]))\n\n\n"], "sample_586": ["    def test_concat_invalid_dims(self):\n        data = Dataset(\n            {\"foo\": ((\"x\", \"y\"), np.random.random((2, 3))),}, {\"x\": [0, 1]},\n        )\n        with raises_regex(ValueError, \"Dimension 'z' not found\"):\n            concat([data, data], dim=\"z\")\n", "    def test_concat_data_vars(self):\n        data = Dataset(\n            {\"foo\": ((\"x\", \"y\"), np.random.random((3, 4))), \"bar\": ((\"x\", \"y\"), np.random.random((3, 4)))}\n        )\n        objs = [\n            data.isel(x=slice(1, 2)).copy(deep=True),\n            data.isel(x=slice(0, 1)).copy(deep=True),\n        ]\n\n        for data_vars in [\"all\", \"minimal\", \"different\", \"none\", []]:\n            actual = concat(objs, dim=\"x\", data_vars=data_vars)\n            expected = data.copy()\n            if data_vars == \"all\":\n                expected = expected.assign_coords(x=[0, 1, 2])\n            elif data_vars == \"minimal\":\n                expected = expected.assign_coords(x=[0, 1, 2])\n            elif data_vars == \"different\":\n                expected = expected.assign_coords(x=[0, 1, 2])\n            else:\n                expected = expected.isel(x=slice(0, 2))\n            assert_identical(expected, actual)\n", "    def test_concat_broadcast_equals(self):\n        ds1 = Dataset(\n            {\"foo\": ((\"x\", \"y\"), np.array([[1, 2], [3, 4]])},\n            {\"x\": [0, 1], \"y\": [0, 1]},\n        )\n        ds2 = Dataset({\"bar\": ((\"x\", \"y\"), np.array([[1], [2]])), \"x\": [0, 1]},)\n        actual = concat([ds1, ds2], \"y\", compat=\"broadcast_equals\")\n        expected = Dataset(\n            {\n                \"foo\": ((\"x\", \"y\"), np.array([[1, 2, 1], [3, 4, 2]])),\n                \"bar\": ((\"x\", \"y\"), np.array([[1, 1], [2, 2]])),\n            },\n            {\"x\": [0, 1], \"y\": [0, 1]},\n        )\n        assert_identical(expected, actual)\n\n\n\n        # broadcast_equals does not require dims to match (but \n        # compat='identical' does)\n        ds3 = Dataset({\"baz\": ((\"x\", \"z\"), np.array([[1, 2], [3, 4]])),  \"x\": [0, 1]},)\n        with raises_regex(ValueError, 'Dimensions mismatch'):\n            concat([ds1, ds3], \"y\", compat=\"identical\")\n\n\n\n", "    def test_concat_mismatched_coords(self):\n        ds1 = Dataset({\"x\": ((\"x\", \"y\"), np.random.randn(2, 3)), \"y\": (\"x\", [0, 1])})\n        ds2 = Dataset({\"x\": ((\"x\", \"y\"), np.random.randn(3, 4)), \"y\": (\"y\", [9, 10, 11])})\n        with raises_regex(ValueError, \"incompatible dimensions\"):\n            concat([ds1, ds2], dim=\"y\")\n\n        ds1 = Dataset({\"x\": ((\"x\", \"y\"), np.random.randn(2, 3)), \"y\": (\"x\", [0, 1])})\n        ds2 = Dataset({\"x\": ((\"x\", \"y\"), np.random.randn(2, 3)), \"y\": (\"x\", [0, 1])})\n        with raises_regex(ValueError, \"incompatible coordinates\"):\n            concat([ds1, ds2], dim=\"z\", coords=[\"y\"])\n\n\n\n", "    def test_concat_with_different_dims(self):\n        arr1 = DataArray(np.arange(6).reshape((2, 3)), dims=[\"x\", \"y\"])\n        arr2 = DataArray(np.arange(6).reshape((3, 2)), dims=[\"y\", \"x\"])\n        with raises_regex(ValueError, \"Dimensions 'x' and 'y' are\"):\n            concat([arr1, arr2], dim=\"x\")\n\n\n\n"], "sample_1104": ["compilation error", "def test_MatrixPow():\n    A = MatrixSymbol('A', 2, 2)\n    assert str(A**2) == \"A**2\"\n    assert str(A**-1) == \"A**(-1)\"\n    assert str((A**2)**3) == \"(A**2)**3\"\n", "def test_Complex():\n    assert str(1 + 2j) == \"1 + 2*I\"\n    assert str(complex(2, 3)) == \"2 + 3*I\"\n\n    assert sstr(1 + 2j) == \"1 + 2*I\"\n    assert sstr(complex(2, 3)) == \"2 + 3*I\"\n\n    assert sstr(1 + 2*I) == \"1 + 2*I\"\n    \n    assert str(complex(1, 2).conjugate()) == \"(1 - 2*I)\"\n\n    assert str(complex(1, 2).real) == \"1\"\n    assert str(complex(1, 2).imag) == \"2\"\n\n    assert str(complex(0, 1)) == \"I\"\n\n    assert str(complex(1, 0)) == \"1\"\n\n    assert str(complex(1, 2).as_real_imag()) == \"(1, 2)\"\n", "def test_transpose():\n    from sympy.matrices import Matrix, MatrixSymbol\n\n    A = Matrix([[1, 2], [3, 4]])\n    B = MatrixSymbol('B', 2, 2)\n\n    assert str(A.T) == \"[[1, 3], [2, 4]]\"\n    assert str(B.T) == \"B.T\"\n", "compilation error"], "sample_1086": ["def test_issue_16267():\n    from sympy import simplify\n    a, b = symbols('a b')\n    expr = simplify((a + 1)/(a**2 + a))\n    assert str(expr) ==  \"1/(a + 1)\"\n", "def test_issue_16082():\n    from sympy.polys.polyutils import poly_from_expr\n    expr = (x**2 + 1)**2\n    poly = poly_from_expr(expr, x)\n    assert str(poly) == 'x**4 + 2*x**2 + 1'\n", "compilation error", "def test_Wild():\n    w = Wild(\"x\")\n    assert str(w + w) == \"x_ + x_\"\n    assert str(w*w) == \"x_*x_\"\n    assert str(w**2) == \"x_**2\"\n    assert str(oo*w) == \"oo*x_\"\n    assert str(sqrt(w)) == \"sqrt(x_)\"\n    assert str(1/w) == \"1/x_\"\n\n\n", "def test_Piecewise_printing():\n    from sympy import Piecewise, symbols\n    x = symbols('x')\n    assert str(Piecewise((x, x > 1), (2*x, True))) == 'Piecewise((x, x > 1), (2*x, True))'\n"], "sample_939": ["    def test_unparse_slice(source, expected):\n        module = ast.parse(source)\n        assert ast.unparse(module.body[0].value, source) == expected\n", "def test_unparse_slice(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value, source) == expected\n", "    def test_unparse_keywords():\n        source = \"def myfunc(a, b=1, *args, **kwargs):\\n  return a + b\"\n        module = ast.parse(source)\n        args = module.body[0].args\n        expected = \"def myfunc(a, b=1, *args, **kwargs):\\n  return a + b\"\n        assert ast.unparse(args, source) == expected \n", "def test_unparse_subscript(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].value, source) == expected\n", "    def test_unparse_star_arg():\n        source = \"def func(a, *args): pass\"\n        module = ast.parse(source)\n        assert ast.unparse(module.body[0].args, source) == \"a, *args\"\n\n\n"], "sample_833": ["def test_logistic_regression_path_multi_class_ovr_vs_multinomial():\n    # Verify that logistic_regression_path produces different results for\n    # 'ovr' and 'multinomial' multi_class, and that the difference is as\n    # expected.\n    X, y = make_classification(n_samples=200, n_classes=3, n_informative=2,\n                               n_redundant=0, n_clusters_per_class=1,\n                               random_state=0, n_features=2)\n    Cs = [.00001, 1, 10000]\n    coefs_ovr, _, _ = _logistic_regression_path(X, y, penalty='l1', Cs=Cs,\n                                                solver='saga', random_state=0,\n                                                multi_class='ovr')\n    coefs_multinomial, _, _ = _logistic_regression_path(X, y, penalty='l1',\n                                                       Cs=Cs, solver='saga',\n                                                       random_state=0,\n                                                       multi_class='multinomial')\n\n    # We expect coefs to be different, as they are trained on different\n    # formulations\n\n    assert not np.allclose(coefs_ovr, coefs_multinomial, atol=1e-4)\n", "def test_logistic_regression_path_zero_support():\n    # Check that logistic_regression_path works correctly with no support vectors,\n    # which should result in an empty coefs array.\n    X, y = make_classification(n_samples=100, n_classes=2, n_features=2,\n                               random_state=0, n_informative=2, n_redundant=0)\n\n    # Create a LogisticRegression model with a very large penalty,\n    # effectively removing all support vectors\n    penalty = 'l2'\n    C = 1e-100\n    lr = LogisticRegression(penalty=penalty, C=C, solver='sag', random_state=0,\n                            multi_class='ovr')\n    lr.fit(X, y)\n\n    coefs, _, _ = _logistic_regression_path(X, y, penalty=penalty, Cs=[C],\n                                             solver='sag', random_state=0,\n                                             multi_class='ovr')\n\n    assert coefs.shape == (1, 1, 2)\n    assert np.allclose(coefs[0, 0, :], np.zeros(2))\n", "def test_logistic_regression_predict_proba_multi_class_ovr():\n    # Test predict_proba with multi_class='ovr' for multi-class classification.\n\n    n_samples = 100\n    n_features = 20\n    n_classes = 3\n    X, y = make_classification(n_samples=n_samples, n_classes=n_classes,\n                               n_features=n_features, random_state=0)\n    lr = LogisticRegression(multi_class='ovr', solver='saga',\n                            random_state=0)\n    lr.fit(X, y)\n\n    y_proba = lr.predict_proba(X)\n    assert y_proba.shape == (n_samples, n_classes)\n    assert np.all(np.sum(y_proba, axis=1) == 1)\n    assert np.all(np.min(y_proba, axis=1) >= 0)\n    assert np.all(np.max(y_proba, axis=1) <= 1)\n\n\n\n    # Test predict_proba with multi_class='multinomial'\n    lr_multi = LogisticRegression(multi_class='multinomial', solver='saga',\n                                random_state=0)\n    lr_multi.fit(X, y)\n    y_proba_multi = lr_multi.predict_proba(X)\n    assert y_proba_multi.shape == (n_samples, n_classes)\n    assert np.all(np.sum(y_proba_multi, axis=1) == 1)\n    assert np.all(np.min(y_proba_multi, axis=1) >= 0)\n    assert np.all(np.max(y_proba_multi, axis=1) <= 1)\n", "def test_logistic_regression_intercept(penalty, solver):\n    # Make sure intercept is handled correctly\n    X, y = make_classification(random_state=0)\n\n    lr = LogisticRegression(penalty=penalty, solver=solver, random_state=0,\n                            multi_class='ovr')\n    lr.fit(X, y)\n    assert lr.intercept_.size == 1 if lr.classes_.shape[0] == 2 else lr.classes_.shape[0]\n", "def test_logistic_regression_intercept_param(fit_intercept):\n    # Test that fit_intercept correctly affects intercept estimation\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([0, 1, 0])\n\n    lr_no_intercept = LogisticRegression(fit_intercept=False).fit(X, y)\n    lr_with_intercept = LogisticRegression(fit_intercept=True).fit(X, y)\n\n    # Coefficients should exclude intercept when fit_intercept=False\n    assert lr_no_intercept.intercept_.size == 0\n    assert lr_with_intercept.intercept_.size == 1\n\n    # Linear decision boundary should pass through the origin when\n    # fit_intercept=False\n    assert np.isclose(lr_no_intercept.coef_[0][0], -0.5)\n    assert np.isclose(lr_no_intercept.coef_[0][1], -0.5)\n\n    # Check that predicted values are consistent with the intercept\n    for lr in (lr_no_intercept, lr_with_intercept):\n        for x in X:\n            prediction = lr.predict_proba(x.reshape(1, -1))\n            if lr.fit_intercept:\n                prediction = lr.predict_proba(x.reshape(1, -1))\n                assert not np.isclose(prediction[0][0], 1,\n                                     atol=1e-5)\n            else:\n                assert np.isclose(prediction[0][0], 1, atol=1e-5)\n\n\n\n"], "sample_95": ["    def test_cache_control_decorator(self):\n        @cache_control(private=True, max_age=3600, must_revalidate=True)\n            return HttpResponse()\n        r = a_view(HttpRequest())\n        self.assertEqual(\n            set(r['Cache-Control'].split(', ')),\n            {'private', 'max-age=3600', 'must-revalidate'},\n        )\n\n\n\n", "    def test_cache_control_decorator(self):\n        @cache_control(private=True, max_age=3600, must_revalidate=True)\n            return HttpResponse()\n        r = a_view(HttpRequest())\n        self.assertEqual(\n            set(r['Cache-Control'].split(', ')),\n            {'max-age=3600', 'private', 'must-revalidate'},\n        )\n\n\n\n", "    def test_cache_control_decorator(self):\n        @cache_control(max_age=3600, no_cache=True, no_store=True, must_revalidate=True)\n            return HttpResponse()\n        r = a_view(HttpRequest())\n        self.assertEqual(\n            set(r['Cache-Control'].split(', ')),\n            {'max-age=3600', 'no-cache', 'no-store', 'must-revalidate'},\n        )\n\n\n\n", "    def test_cache_control_decorator(self):\n        @cache_control(private=True, max_age=1234)\n            return HttpResponse()\n        r = a_view(HttpRequest())\n        self.assertEqual(set(r['Cache-Control'].split(', ')),\n                         {'private', 'max-age=1234'})\n", "    def test_cache_control_decorator(self):\n        @cache_control(private=True, max_age=3600, must_revalidate=True)\n            return HttpResponse()\n        r = a_view(HttpRequest())\n        self.assertEqual(set(r['Cache-Control'].split(', ')), {'private', 'max-age=3600', 'must-revalidate'})\n"], "sample_159": ["    def test_models_without_permissions(self):\n        class Checked(models.Model):\n            pass\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [])\n", "    def test_permission_name_and_codename_uniqueness(self):\n        class Checked(models.Model):\n            class Meta:\n                permissions = [\n                    ('unique_name', 'Unique permission'),\n                    ('unique_name', 'Unique permission with duplicate name'),\n                ]\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The permission named 'unique_name' of model 'auth_tests.Checked' is duplicated for model 'auth_tests.Checked'.\",\n                obj=Checked,\n                id='auth.E008',\n            ),\n        ])\n\n\n", "    def test_user_model_with_custom_is_anonymous_authenticated(self):\n        class CustomUser(AbstractBaseUser):\n            username = models.CharField(max_length=30, unique=True)\n            USERNAME_FIELD = 'username'\n\n            is_anonymous = True\n            is_authenticated = True\n\n        errors = checks.run_checks(app_configs=self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Critical(\n                '%s.is_anonymous must be an attribute or property rather than '\n                'a method. Ignoring this is a security issue as anonymous '\n                'users will be treated as authenticated!' % CustomUser,\n                obj=CustomUser,\n                id='auth.C009',\n            ),\n            checks.Critical(\n                '%s.is_authenticated must be an attribute or property rather '\n                'than a method. Ignoring this is a security issue as anonymous '\n                'users will be treated as authenticated!' % CustomUser,\n                obj=CustomUser,\n                id='auth.C010',\n            ),\n        ])\n", "    def test_builtin_permissions_are_in_usage(self):\n        class Checked(models.Model):\n            class Meta:\n                permissions = [\n                    ('auth.change_password', 'Can change password'),\n                ]\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The permission codenamed 'auth.change_password' clashes with a builtin \"\n                \"permission for model 'auth_tests.Checked'.\",\n                obj=Checked,\n                id='auth.E005',\n            ),\n        ])\n", "    def test_permission_name_duplicates(self):\n        class Checked(models.Model):\n            class Meta:\n                permissions = [\n                    ('duplicate_permission', 'Permission 1'),\n                    ('duplicate_permission', 'Permission 2'),\n                ]\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The permission codenamed 'duplicate_permission' is duplicated for \"\n                \"model 'auth_tests.Checked'.\",\n                obj=Checked,\n                id='auth.E006',\n            ),\n        ])\n\n"], "sample_560": ["def test_loc_invalid_string():\n    fig, ax = plt.subplots()\n    with pytest.raises(ValueError, match=('loc must be string, coordinate '\n                       'tuple, or an integer 0-10, not \\'invalid\\')\n                       ):\n        ax.legend(loc='invalid')\n\n\n\n", "def test_loc_invalid_string():\n    fig, ax = plt.subplots()\n    with pytest.raises(ValueError, match=('Invalid loc: \\\\\"nonexistent\\\\\"')):\n        ax.legend(loc=\"nonexistent\")\n\n\n", "def test_loc_with_bbox_to_anchor():\n    fig, ax = plt.subplots()\n    ax.legend(loc='best', bbox_to_anchor=(1.2, 1.0))\n\n\n\n", "def test_loc_position_tuple():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], [4, 5, 6])\n    leg = ax.legend(loc=(0.5, 0.5))\n    assert leg.get_bbox_to_anchor()[0] == 0.5\n    assert leg.get_bbox_to_anchor()[1] == 0.5\n", "def test_loc_invalid_string():\n    fig, ax = plt.subplots()\n    with pytest.raises(ValueError, match=('loc must be string, coordinate '\n                       'tuple, or an integer 0-10, not None')):\n        ax.legend(loc=None)\n    \n    with pytest.raises(ValueError, match=('loc must be string, coordinate '\n                       'tuple, or an integer 0-10, not 1')):\n        ax.legend(loc=1)\n\n"], "sample_481": ["    def test_join09(self):\n        output = self.engine.render_to_string(\"join09\", {\"a\": [\"alpha\", \"beta & me\"]})\n        self.assertEqual(output, \"alpha:beta & me\")\n\n", "    def test_join09(self):\n        output = self.engine.render_to_string(\"join09\", {\"a\": [\"alpha\", \"beta & me\"]})\n        self.assertEqual(output, \"alpha & beta & me\")\n\n\n\n", "    def test_truncatechars_html(self):\n        value = \"<p>This is a long string that needs to be truncated to 20 characters.</p>\"\n        output = self.engine.render_to_string(\"test_truncatechars_html\", {\"value\": value})\n        self.assertEqual(output, '<ul>This is a long string that needs...</ul>')\n\n\n", "    def test_join_empty(self):\n        output = self.engine.render_to_string(\"join_empty\", {\"a\": []})\n        self.assertEqual(output, \"\")\n", "    def test_join09(self):\n        output = self.engine.render_to_string(\"join09\", {\"a\": [\"John\", \"Jane\"]})\n        self.assertEqual(output, \"John &amp; Jane\")\n\n\n"], "sample_949": ["def test_authors(app, status, warning):\n    app.build()\n    content = (app.outdir / 'python.1').read_text()\n    assert r'\\fIAuthors:\\fP Wolfgang Sch\u00e4uble & G\\'Beckstein' in content\n", "def test_image_rendering(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'python.1').read_text()\n    assert r'\\fI\\.\\. image:: sphinx.png\\fI' in content\n", "def test_man_show_urls(app, status, warning):\n    app.build()\n\n    content = (app.outdir / 'python.1').read_text()\n    assert r'<URL:http://example.com/>' in content\n", "def test_man_show_urls(app, status, warning):\n    app.config.man_show_urls = True\n    app.build()\n    content = (app.outdir / 'python.1').read_text()\n    assert 'http://example.com' in content \n", "def test_conf_issues(app, status, warning):\n    app.build()\n    content = (app.outdir / 'python.1').read_text()\n    assert \"The issue is:\\n\" in content\n    assert \"This is a workaround:\" in content \n"], "sample_982": ["def test_gcd():\n    assert gcd(0, 10) == 0\n    assert gcd(10, 0) == 10\n    assert gcd(10, 10) == 10\n    assert gcd(10, 5) == 5\n    assert gcd(15, 25) == 5\n    assert gcd(123456789, 987654321) == 1\n    assert gcd(987654321, 123456789) == 1\n    assert gcd(2*3*5*7, 2*3*5) == 2*3*5\n    assert gcd(2*3*5*7, 2*5*7*11) == 2*5*7\n    assert gcd(2*3*5*7*11*13, 2*3*5*7*11) == 2*3*5*7*11\n    assert gcd(2**10*3**5*5**2, 2**3*3**2*5**4) == 2**3*3**2*5**2\n    assert gcd(1234567890, 1234567891) == 1\n    assert gcd(1234567890, 1234567890) == 1234567890\n\n    m = Symbol(\"m\", integer=True)\n    n = Symbol(\"n\", integer=True)\n    assert gcd(m, n)\n    assert gcd(m, n).subs(m, 2**3*3**5, **n, 2**4*3**2) == 2**3*3**2\n    assert gcd(m, n).subs(m, 2**3*3**5*5**2, **n, 2**3*3**2*5**4) == 2**3*3**2*5**2\n\n", "def test_nextprime():\n    assert nextprime(2) == 3\n    assert nextprime(3) == 5\n    assert nextprime(10) == 11\n    assert nextprime(20) == 23\n    assert nextprime(9999) == 10009\n    assert nextprime(1000000000) == 1000000003\n    assert nextprime(1000000000000000000) == 1000000000000000003\n    assert nextprime(2**32 - 1) == 2**32\n    n = Symbol(\"n\", integer=True)\n    assert nextprime(n)\n    assert nextprime(n).subs(n, 2**32 - 1) == 2**32\n\n    \n", "compilation error", "compilation error", "compilation error"], "sample_1035": ["def test_grover_iteration_3():\n    numqubits = 3\n    basis_states = superposition_basis(numqubits)\n    v = OracleGate(numqubits, lambda qubits: qubits == IntQubit(7, nqubits=numqubits))\n    expected =  ((2/sqrt(pow(2, numqubits)))*basis_states) - IntQubit(7, nqubits=numqubits)\n    assert qapply(grover_iteration(basis_states, v)) == expected\n", "def test_apply_grover():\n    numqubits = 2\n    basis_states = superposition_basis(numqubits)\n    v = OracleGate(numqubits, return_one_on_two)\n    num_iterations = 5\n\n    result = apply_grover(basis_states, v, num_iterations)\n    expected = (-13*basis_states)/64 + 264*IntQubit(2, numqubits)/256\n\n    assert qapply(result) == qapply(expected)\n\n", "def test_apply_grover():\n    numqubits = 2\n    basis_states = superposition_basis(numqubits)\n    v = OracleGate(numqubits, return_one_on_one)\n    expected = IntQubit(1, nqubits=numqubits)\n    grover = apply_grover(basis_states, v, 3)\n    assert qapply(grover) == expected\n", "def test_apply_grover():\n    numqubits = 3\n    basis_states = superposition_basis(numqubits)\n    v = OracleGate(numqubits, lambda qubits: qubits == IntQubit(3))\n    target_state = basis_states.H*basis_states.H*basis_states\n    Grover_result = apply_grover(basis_states, v, iterations=20)\n    # assert that the state after apply_grover is close to the target state\n    similarity = (Grover_result.dot(target_state))/(Grover_result.norm()*target_state.norm())\n    assert similarity > 0.95\n", "def test_apply_grover():\n    numqubits = 3\n    basis_states = superposition_basis(numqubits)\n    v = OracleGate(numqubits, lambda qubits: qubits == IntQubit(4))\n    \n    # Apply Grover's algorithm\n    result = apply_grover(basis_states, v, 4)\n    \n    # Check that the output is close to the expected state\n    expected = (-11*basis_states)/16 + 169*IntQubit(4, numqubits)/256\n    assert qapply(expected) == result    \n"], "sample_389": ["    def test_prefixed_paths(self):\n        # Add SCRIPT_NAME prefix to relative paths.\n        self.set_script_name(\"/somesubpath/\")\n        for setting in (\"MEDIA_URL\", \"STATIC_URL\"):\n            for path in (\"/path/\", \"path/\", \".\", \"relative\"):\n                new_settings = {setting: path}\n                with self.settings(**new_settings):\n                    with self.subTest(script_name=\"/somesubpath/\", **new_settings):\n                        self.assertEqual(getattr(settings, setting), \"/somesubpath/\" + path)  \n", "    def test_prefixed_media_url(self):\n        # Test cases for MEDIA_URL, when SCRIPT_NAME is set.\n\n        self.set_script_name(\"/admin/\")\n        with self.settings(MEDIA_URL=\"/static/media/\",):\n            self.assertEqual(settings.MEDIA_URL, \"/admin/static/media/\")\n            self.assertEqual(settings.MEDIA_ROOT, os.path.join(BASE_DIR, \"static\", \"media\"))\n        \n        self.set_script_name(\"/admin/dashboard/\")\n        with self.settings(MEDIA_URL=\"/static/media/\",):\n            self.assertEqual(settings.MEDIA_URL, \"/admin/dashboard/static/media/\")\n            self.assertEqual(settings.MEDIA_ROOT, os.path.join(BASE_DIR, \"static\", \"media\"))\n\n        self.set_script_name(\"/admin/\")\n        with self.settings(MEDIA_URL=\"/media/\",):\n            self.assertEqual(settings.MEDIA_URL, \"/admin/media/\")\n            self.assertEqual(settings.MEDIA_ROOT, os.path.join(BASE_DIR, \"media\"))\n", "    def test_media_url_static_url_prefix_with_script_name(self):\n        # Test behavior when SCRIPT_NAME is set and MEDIA_URL/STATIC_URL are\n        # relative to it.\n        self.set_script_name(\"/admin/\")\n        tests = [\n            \"/media/\",\n            \"/static/\",\n            \"/media/images/\",\n            \"/static/css/\",\n        ]\n        for setting in (\"MEDIA_URL\", \"STATIC_URL\"):\n            for path in tests:\n                with self.settings(MEDIA_URL=path, STATIC_URL=path):\n                    self.assertEqual(getattr(settings, setting), f\"/admin/{path}\")\n", "    def test_media_url_prefixed(self):\n        # Add SCRIPT_NAME prefix to relative paths.\n        tests = (\"/media/\", \"/media/some/path/\")\n        for script_name in [\"/somesubpath\", \"/somesubpath/\", \"/\", \"\", None]:\n            for path in tests:\n                with self.subTest(script_name=script_name, path=path):\n                    new_settings = {\"MEDIA_URL\": path}\n                    with self.settings(**new_settings):\n                        self.set_script_name(\"%s%s\" % (script_name, path))\n                        self.assertEqual(getattr(settings, \"MEDIA_URL\"), path)\n                        self.assertEqual(\n                            getattr(settings, \"STATIC_URL\"), path\n                        )\n\n                    clear_script_prefix()\n", "    def test_default_media_url_with_script_name(self):\n        self.set_script_name(\"/somesubpath/\")\n        with self.settings(MEDIA_URL=None, STATIC_URL=None):\n            self.assertEqual(settings.MEDIA_URL, \"/somesubpath/media/\")\n"], "sample_440": ["    def test_update_conflicts_unique_fields_both(self):\n        self._test_update_conflicts(unique_fields=[\"number\", \"rank\"])\n", "    def test_update_conflicts_no_unique_fields(self):\n        self._test_update_conflicts([])\n", "    def test_update_conflicts_multiple_unique_fields(self):\n        self._test_update_conflicts(unique_fields=[\"number\", \"rank\"])\n", "    def test_update_conflicts_unique_fields_multiple_columns(self):\n        self._test_update_conflicts(unique_fields=[\"number\", \"rank\"])\n", "    def test_update_conflicts_unique_fields_both(self):\n        self._test_update_conflicts(unique_fields=[\"number\", \"rank\"])\n\n"], "sample_527": ["def test_toolmanager_toggle_tool():\n    with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n        plt.rcParams['toolbar'] = 'toolmanager'\n    fig = plt.gcf()\n    assert 'pan' in fig.canvas.manager.toolmanager.tools\n    fig.canvas.manager.toolmanager.toggle_tool('pan')\n    assert 'pan' not in fig.canvas.manager.toolmanager.tools\n    fig.canvas.manager.toolmanager.toggle_tool('pan')\n    assert 'pan' in fig.canvas.manager.toolmanager.tools\n", "def test_interactive_zoom_limits():\n    fig, ax = plt.subplots()\n    ax.plot(np.arange(10))\n    assert ax.get_navigate()\n\n    # Set initial limits\n    ax.set_xlim(1, 9)\n    ax.set_ylim(0, 5)\n\n    tb = NavigationToolbar2(fig.canvas)\n    tb.zoom()\n\n    # Zoom in\n    start_event = MouseEvent(\n        \"button_press_event\", fig.canvas, (0.1, 0.5), MouseButton.LEFT)\n    fig.canvas.callbacks.process(start_event.name, start_event)\n    stop_event = MouseEvent(\n        \"button_release_event\", fig.canvas, (0.9, 0.9), MouseButton.LEFT)\n    fig.canvas.callbacks.process(stop_event.name, stop_event)\n\n    assert ax.get_xlim() != (1, 9)\n    assert ax.get_ylim() != (0, 5)\n\n    # Zoom out completely\n    start_event = MouseEvent(\n        \"button_press_event\", fig.canvas, (0.1, 0.1), MouseButton.RIGHT)\n    fig.canvas.callbacks.process(start_event.name, start_event)\n    stop_event = MouseEvent(\n        \"button_release_event\", fig.canvas, (0.9, 0.9), MouseButton.RIGHT)\n    fig.canvas.callbacks.process(stop_event.name, stop_event)\n\n    assert ax.get_xlim() == (1, 9)\n    assert ax.get_ylim() == (0, 5)\n\n", "def test_toolmanager_add_tool():\n    with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n        plt.rcParams['toolbar'] = 'toolmanager'\n    fig = plt.gcf()\n    initial_len = len(fig.canvas.manager.toolmanager.tools)\n    new_tool = object()\n    fig.canvas.manager.toolmanager.add_tool(new_tool)\n    assert len(fig.canvas.manager.toolmanager.tools) == initial_len + 1\n    assert new_tool in fig.canvas.manager.toolmanager.tools\n", "compilation error", "def test_toolmanager_enabled_disabled():\n    with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n        plt.rcParams['toolbar'] = 'toolmanager'\n    fig = plt.gcf()\n    toolmanager = fig.canvas.manager.toolmanager\n    assert toolmanager.enabled is True\n    toolmanager.enabled = False\n    assert not toolmanager.enabled\n    toolmanager.enabled = True\n    assert toolmanager.enabled\n    toolmanager.disable_all()\n    assert not toolmanager.enabled\n    toolmanager.enable_all()\n    assert toolmanager.enabled  \n\n"], "sample_228": ["    def test_invalid(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '-1',  # <-- should be an invalid number\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [\n            {'choices-0-votes': ['This value must be greater than or equal to 0.']},\n            {},\n        ]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n\n\n", "    def test_invalid_one_form_is_flagged(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '',  \n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset,)), False)\n        expected_errors = [{'votes': ['This field is required.']}, {}]\n        self.assertEqual(formset._errors, expected_errors)\n", "    def test_invalid(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': 'invalid',  \n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [{'votes': ['Enter a valid number.']}, {}]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n", "    def test_invalid(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '',  \n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [{'votes': ['This field is required.']}, {}]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n", "    def test_valid_with_non_form_errors_all_valid(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',  # the number of forms rendered\n            'choices-INITIAL_FORMS': '0',  # the number of forms with initial data\n            'choices-MAX_NUM_FORMS': '2',  # max number of forms\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice, extra=1, validate_max=True)\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertEqual(all_valid((formset,)), False)\n        self.assertEqual(formset._errors, [{}, {}])\n\n\n"], "sample_487": ["    def test_actions_invalid_type(self):\n        class BandAdmin(ModelAdmin):\n            actions = \"hello\"\n\n        self.assertIsInvalid(\n            BandAdmin,\n            Band,\n            \"The value of 'actions' must be a list or tuple.\",\n            id=\"admin.E128\",\n        )\n\n", "    def test_actions_contain_valid_function_type(self):\n        class BandAdmin(ModelAdmin):\n            actions = [\n                lambda modeladmin, request, queryset: None,\n                42,  # Invalid object type\n            ]\n\n        self.assertIsInvalid(\n            BandAdmin,\n            Band,\n            \"Actions must be callable objects.\",\n            id=\"admin.E131\",\n        )\n", "    def test_actions_not_callable(self):\n        class BandAdmin(ModelAdmin):\n            actions = (None,)\n\n        self.assertIsInvalid(\n            BandAdmin,\n            Band,\n            \"The value of 'actions[0]' must be a callable.\",\n            id=\"admin.E128\",\n        )\n", "    def test_actions_invalid_action_type(self):\n        class InvalidAction:\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (InvalidAction,)\n\n        self.assertIsInvalid(\n            BandAdmin,\n            Band,\n            \"All actions must be callable and decorated with '@admin.action'.\",\n            id=\"admin.E131\",\n        )\n", "    def test_actions_valid_case(self):\n        @admin.action\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (action1,)\n\n        self.assertIsValid(BandAdmin, Band)\n"], "sample_834": ["def test_nca_fit_transform_pipeline():\n    X, y = make_classification(n_samples=30, n_features=5, n_classes=4,\n                               n_redundant=0, n_informative=5, random_state=0)\n\n    nca = NeighborhoodComponentsAnalysis(init='auto', n_components=2,\n                max_iter=100)\n\n    # Fit the model and then transform the data\n    X_transformed = nca.fit_transform(X, y)\n\n    # Check that the shapes are correct\n    assert X_transformed.shape[1] == nca.n_components\n\n    # Now, transform a new data set using the fitted model\n    X_new = rng.randn(10, 5)\n    X_new_transformed = nca.transform(X_new)\n\n    # Check that the shapes are correct\n    assert X_new_transformed.shape[1] == nca.n_components\n\n\n", "def test_tol_gradient(tol):\n    # Test the tolerance parameter in the gradient checking\n    rng = np.random.RandomState(42)\n    X = rng.rand(50, 10)\n    y = rng.randint(0, 2, size=50)\n\n    nca = NeighborhoodComponentsAnalysis(max_iter=1, tol=tol)\n    nca.fit(X, y)\n    check_grad(nca._loss_grad_lbfgs, nca.components_, \n               X, y, same_class_mask=y[:, np.newaxis] == y[np.newaxis, :])\n", "def test_nca_with_precomputed_transform():\n    # Test the functionality of using a precomputed transformation matrix.\n    rng = np.random.RandomState(42)\n    X = rng.randn(10, 5)\n    y = rng.randint(0, 2, size=10)\n    nca = NeighborhoodComponentsAnalysis(init='precomputed',\n                                          n_components=3)\n\n    # Create a fake precomputed transformation matrix\n    transform = rng.randn(3, 5)\n    nca.init = transform\n    \n    nca.fit(X, y)\n    \n    # test that the components are equal to the precomputed transform\n    assert_array_almost_equal(nca.components_, transform)\n\n    # test that the transformed data is equal to the original data\n    # transformed by the precomputed transform\n    X_transformed = nca.transform(X)\n    X_transformed_precomputed = np.dot(X, transform)\n    assert_array_almost_equal(X_transformed, X_transformed_precomputed)\n", "def test_nca_with_sparse_data():\n    # Test with sparse data to ensure the code can handle it.\n    from scipy.sparse import csr_matrix\n    rng = np.random.RandomState(42)\n    n_samples, n_features = 100, 500\n    X = rng.rand(n_samples, n_features)\n    y = rng.randint(0, 2, size=n_samples)\n    # Convert X to sparse matrix\n    X_sparse = csr_matrix(X)\n    nca = NeighborhoodComponentsAnalysis(n_components=2, random_state=42)\n    nca.fit(X_sparse, y)\n    # Transform the sparse data\n    X_embedded = nca.transform(X_sparse)\n    assert X_embedded.shape[1] == nca.n_components\n", "def test_tol():\n    X = iris_data\n    y = iris_target\n    \n    nca = NeighborhoodComponentsAnalysis(tol=1e-10, verbose=1)\n    nca.fit(X, y)\n    assert_array_almost_equal(nca.n_iter_, np.inf, decimal=3)\n\n"], "sample_287": ["    def test_check_fieldsets_for_nonexistent_fields(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            fieldsets = [\n                (None, {\n                    'fields': ['nonexistent_field']\n                }),\n            ]\n\n        errors = MyModelAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'fieldsets[0][1]['fields'][0]' is not a callable, an attribute \"\n                \"of 'MyModelAdmin', or an attribute of 'admin_checks.Song'.\",\n                obj=MyModelAdmin,\n                id='admin.E035'\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_ordering(self):\n        class SongAdmin(admin.ModelAdmin):\n            ordering = ['-title']\n\n        errors = SongAdmin(Song, AdminSite()).check()\n        self.assertEqual(errors, [])\n", "    def test_check_for_invalid_inline_field_names(self):\n        class MyInline(admin.TabularInline):\n            model = Song\n            extra = 1\n            fields = ['song_title', 'album_title']\n        class MyAdmin(admin.ModelAdmin):\n            inlines = [MyInline]\n        errors = MyAdmin(Album, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"'inline_fields' references 'song_title', which is not a field \"\n                \"of 'admin_checks.Song'.\"\n                , obj=MyInline,\n                id='admin.E003'\n            ),\n            checks.Error(\n                \"'inline_fields' references 'album_title', which is not a field \"\n                \"of 'admin_checks.Song'.\"\n                , obj=MyInline,\n                id='admin.E003'\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_check_fieldset_sublists_for_nonexistent_fields(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            fieldsets = [\n                (None, {\n                    'fields': ['title', 'album', 'nonexistent_field']\n                }),\n            ]\n\n        errors = MyModelAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'fieldsets[0][1][\"\n                \"'fields']' contains a nonexistent field \"\n                \"'nonexistent_field' on 'admin_checks.Song'.\",\n                obj=MyModelAdmin,\n                id='admin.E002',\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_fieldsets_with_non_lists(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            fieldsets = (\n                (None, {'fields': 'title'}),\n            )\n\n        errors = MyModelAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'fieldsets[0][1]['fields']' must be a list or tuple.\",\n                obj=MyModelAdmin,\n                id='admin.E008',\n            )\n        ]\n        self.assertEqual(errors, expected)\n\n\n"], "sample_895": ["def test_one_hot_encoder_with_handle_unknown(\n    transformer, input_data, expected_output, expected_names", "def test_output_dtypes_with_remainder(transformer, test_value):\n    \"\"\"\n    Test that the output data type is handled correctly for the remainder.\n\n    We are checking the `dtype` of the output of a `ColumnTransformer`\n    when using a `remainder` strategy.\n    \"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    df = pd.DataFrame({\"a\": [1, 2], \"b\": [3, 4]}, dtype=\"float64\")\n    ct = ColumnTransformer(\n        [(\"trans\", transformer, [\"b\"])], remainder=\"passthrough\"\n    )\n    X_trans = ct.fit_transform(df)\n    assert X_trans.dtype == \"float64\"\n    assert isinstance(X_trans, np.ndarray)\n\n    with pytest.raises(\n        TypeError,\n        match=r\"Cannot handle dtype 'object' in the remainder\",\n    ):\n        ct = ColumnTransformer(\n            [(\"trans\", transformer, [\"a\", \"b\"])], remainder=\"passthrough\"\n        )\n        ct.fit_transform(df)\n\n\n", "def test_column_transformer_fit_transform_with_numpy_output(remainder):\n    pd = pytest.importorskip(\"pandas\")\n    df = pd.DataFrame({\"a\": [1, 2], \"b\": [3, 4]})\n    ct = ColumnTransformer(\n        [(\"trans\", TransWithNames(), [\"a\"]), (\"other\", \"passthrough\", [\"b\"])],\n        remainder=remainder,\n    )\n    # Explicitly set output to numpy\n    ct.set_output(transform=\"numpy\")\n\n    X_trans = ct.fit_transform(df)\n    assert isinstance(X_trans, np.ndarray)  \n\n    if remainder == \"drop\":\n      assert X_trans.shape[1] == 1\n    elif remainder == \"passthrough\":\n      assert X_trans.shape[1] == 2\n", "def test_column_transformer_output_with_errors(remainder, fit_transform):\n    pd = pytest.importorskip(\"pandas\")\n    df = pd.DataFrame(\n        {\n            \"pet\": pd.Series([\"dog\", \"cat\", \"snake\"], dtype=\"category\"),\n            \"age\": [1.4, 2.1, 4.4],\n            \"height\": [20, 40, 10],\n            \"distance\": [20, pd.NA, 100],\n        }\n    )\n    ct = ColumnTransformer(\n        [\n            (\n                \"color_encode\",\n                OneHotEncoder(sparse_output=False, dtype=\"int16\"),\n                [\"pet\"],\n            ),\n            (\"age\", StandardScaler(), [\"age\"]),\n        ],\n        remainder=remainder,\n        verbose_feature_names_out=False,\n    ).set_output(transform=\"pandas\")\n    if fit_transform:\n        with pytest.raises(ValueError, match=\"Input DataFrame is not in the \"\n                              \"expected format\"):\n            ct.fit_transform(df)\n    else:\n        with pytest.raises(ValueError, match=\"Input DataFrame is not in the \"\n                              \"expected format\"):\n            ct.transform(df)\n\n\n\n", "def test_error_on_mismatched_indices_with_dataframe_output(transformers):\n    pd = pytest.importorskip(\"pandas\")\n    X = pd.DataFrame(\n        [[1.0, 2.2], [3.0, 1.0], [4.0, 5.0]], columns=[\"a\", \"b\"], index=[9, 10, 11]\n    )\n    ct = ColumnTransformer(transformers, verbose_feature_names_out=False)\n    ct.set_output(transform=\"pandas\")\n    with pytest.raises(ValueError, match=\"Concatenating DataFrames\"):\n        ct.fit_transform(X)\n\n\n\n"], "sample_912": ["def test_pymethod_signature_options(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:method:: meth\\n\"\n            \"      :param: arg1: int\\n\"\n            \"      :type: arg2: str\\n\"\n            \"   .. py:method:: meth2\\n\"\n            \"      :rtype: list\\n\"\n            \"      :raises: ValueError\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc,\n                                                  addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'meth() (Class method)', 'class.meth', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_name, \"meth\"],\n                                                     [desc_parameterlist,\n                                                      [desc_parameter,\n                                                       [desc_name, \"arg1\"],\n                                                       [desc_annotation, \": int\"]],\n                                                      [desc_parameter,\n                                       [desc_name, \"arg2\"],\n                                       [desc_annotation, \": str\"]]),\n                                                     [desc_content, ()])],\n                                   [desc_content, ()]))\n\n    assert_node(doctree[1][1][2], addnodes.index,\n                entries=[('single', 'meth2() (Class method)', 'class.meth2', '', None)])\n    assert_node(doctree[1][1][3], ([desc_signature, ([desc_name, \"meth2\"],\n                                                     [desc_parameterlist, ()],\n                                                     [desc_returns, \"list\"],\n                                                     [desc_raises,\n                                       \"ValueError\"]),\n", "def test_module_index_sort_order(app):\n    text = (\".. py:module:: sphinx\\n\"\n            \".. py:module:: sphinx.builders\\n\"\n            \".. py:module:: sphinx.config\\n\"\n            \".. py:module:: docutils\\n\")\n    restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    assert index.generate() == (\n        [('d', [IndexEntry('docutils', 0, 'index', 'module-docutils', '', '', '')]),\n         ('s', [IndexEntry('sphinx', 1, 'index', 'module-sphinx', '', '', ''),\n                IndexEntry('sphinx.builders', 2, 'index', 'module-sphinx.builders', '', '', ''),  # NOQA\n                IndexEntry('sphinx.config', 2, 'index', 'module-sphinx.config', '', '', '')])],\n        True\n    )\n", "def test_module_index_duplicate_entries(app):\n    text = (\".. py:module:: docutils\\n\"\n            \".. py:module:: sphinx\\n\"\n            \".. py:module:: sphinx\\n\")\n    restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    assert index.generate() == (\n        [('d', [IndexEntry('docutils', 0, 'index', 'module-docutils', '', '', '')]),\n         ('s', [IndexEntry('sphinx', 1, 'index', 'module-sphinx', '', '', '')])],\n        False\n    ) \n", "def test_module_index_nested_submodules(app):\n    text = \".. py:module:: sphinx\\n\"\n    text += \".. py:module:: sphinx.something\\n\"\n    text += \".. py:module:: sphinx.something.else\\n\"\n    restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    assert index.generate() == (\n        [('s', [IndexEntry('sphinx', 1, '', '', '', '', ''),\n                IndexEntry('sphinx.something', 2, 'index', 'module-sphinx.something', '', '', ''),\n                IndexEntry('sphinx.something.else', 2, 'index', 'module-sphinx.something.else', '', '', '')])],\n        False\n    )\n", "def test_module_index_with_optional_pymodule(app):\n    text = (\".. py:module:: docutils:\\n\"\n            \"    optional:\\n\"\n            \"\\n\"\n            \".. py:module:: sphinx\\n\")\n    restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    assert index.generate() == (\n        [('d', [IndexEntry('docutils', 0, 'index', 'module-docutils', '', '', '')]),\n         ('s', [IndexEntry('sphinx', 0, 'index', 'module-sphinx', '', '', '')])],\n        False\n    )\n\n\n"], "sample_178": ["    def test_invalid(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '',  # <-- invalid field\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [{'votes': ['This field is required.']}, {}]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n\n\n\n", "    def test_invalid(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset1, formset2)), False)\n", "    def test_non_empty_formset_is_valid_with_errors(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertFalse(all_valid((formset,)))\n        self.assertEqual(formset._errors, [{'votes': ['This field is required.']}, {}])\n", "    def test_invalid(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': 'invalid',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset1, formset2)), False)\n\n\n\n", "    def test_any_invalid(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': 'invalid',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(any_invalid((formset1, formset2)), True)\n        self.assertEqual(formset1._errors, [{}, {'votes': ['Enter a valid number.']}])\n        self.assertEqual(formset2._errors, [{}, {'votes': ['Enter a valid number.']}])\n\n\n\n"], "sample_473": ["    def test_get_script_name_complex_path(self):\n        script_name = get_script_name({\"SCRIPT_URL\": \"/path/to/app/\"} )\n\n\n        self.assertEqual(script_name, \"/path/to/app/\")\n", "    def test_get_script_name_empty(self):\n        \"\"\"Empty SCRIPT_URL should result in empty SCRIPT_NAME.\"\"\"\n        script_name = get_script_name({\"SCRIPT_URL\": \"\", \"PATH_INFO\": \"/something\"})\n        self.assertEqual(script_name, \"\")\n", "    def test_handle_exceptions(self):\n        # Test that exceptions in the view are properly handled.\n        environ = self.request_factory.get(\"/raise_exception/\").environ\n        with self.assertRaises(Exception):\n            WSGIHandler()(environ, lambda *a, **k: None)\n\n\n", "    def test_handle_400_response_with_headers(self):\n        response = self.client.get(\"/bad_request_headers/\")\n        self.assertEqual(response.status_code, 400)\n        self.assertEqual(response[\"Content-Type\"], \"text/html\")\n        self.assertIn(b\"<h1>Bad Request</h1>\", response.content)\n", "compilation error"], "sample_577": ["    def test_legend_position(self, long_df):\n\n        p = Plot(long_df).pair(x=[\"a\", \"b\"], y=[\"x\", \"y\"]).legend(loc=\"upper right\").plot()\n        legend = p._figure.legends[0]\n        assert legend.get_position().x > 0.8\n        assert legend.get_position().y < 0.2\n\n", "    def test_legend_handles_duplicate_keys_in_scales(self, long_df):\n\n        p = Plot(long_df)\n        p = p.pair(x=\"a\", y=\"b\").add(\n            MockMark(),\n            scale=dict(\n                color=dict(\n                    type=\"linear\",\n                    name=\"foo\",\n                    scales=dict(\n                        foo=lambda x: x,\n                        \"bar\": lambda x: x * 2,\n                    ),\n                )\n            ),\n        )\n        p = p.plot()\n        _, legend_contents = p._legend_contents\n        assert [t.get_text() for t in legend_contents[1][0].get_texts()] == [\"foo\"]\n", "    def test_legend_handles_empty_categories(self, xy):\n        s = pd.Series([None, \"a\", None, \"b\"], name=\"category\")\n        p = Plot(**xy).add(MockMark(), category=s).plot()\n        legend, = p._figure.legends\n\n        assert len(legend.get_texts()) == len(np.unique(s)) - 1\n        assert all(t.get_text() != \"\" for t in legend.get_texts())\n        \n", "    def test_legend_handles_categorical_and_numerical(self, long_df):\n\n        x = \"x\"\n        y = \"y\"\n        p = Plot(long_df, x=x, y=y).add(MockMark(), color=long_df[x]).add(\n            MockMark(), size=long_df[\"y\"]\n        ).plot()\n        legend, = p._figure.legends\n\n        assert len(legend.get_texts()) == len(long_df[x].unique())\n", "    def test_facet_legend_location(self, long_df):\n\n        p = Plot(long_df)\n        p = p.facet(row=\"a\", col=\"b\").plot()\n\n        legend = p._figure.legends[0]\n        assert legend.get_bbox_to_anchor() == (0.0, 1.0)\n\n        p = p.facet(row=\"a\", col=\"b\", legend_location=\"upper right\").plot()\n        legend = p._figure.legends[0]\n        assert legend.get_bbox_to_anchor() == (1.0, 1.0)\n        \n        p = p.facet(row=\"a\", col=\"b\", legend_location=\"lower left\").plot()\n        legend = p._figure.legends[0]\n        assert legend.get_bbox_to_anchor() == (0.0, 0.0)\n"], "sample_599": ["def test_CFScaleOffsetCoder_decode_offset_only():\n    original = xr.Variable(\n        (\"x\",), np.arange(10) * 10, encoding=dict(add_offset=5)\n    )\n    coder = variables.CFScaleOffsetCoder()\n    encoded = coder.encode(original)\n    roundtripped = coder.decode(encoded)\n    assert_identical(roundtripped, original)\n    assert roundtripped.data == np.arange(10) * 10 + 5\n", "def test_CFScaleOffsetCoder_decode_with_offset():\n    original = xr.Variable((\"x\",), [0, -1, 1], encoding={\"add_offset\": 5})\n    expected = xr.Variable((\"x\",), [5, 4, 6])\n    coder = variables.CFScaleOffsetCoder()\n    decoded = coder.decode(original)\n    assert_identical(expected, decoded) \n", "def test_CFScaleOffsetCoder_decode_with_offset():\n    original = xr.Variable((\"x\",), [0.0, 1.0, 2.0], encoding=dict(add_offset=5.0))\n    coder = variables.CFScaleOffsetCoder()\n    encoded = coder.encode(original)\n    roundtripped = coder.decode(encoded)\n    assert_identical(roundtripped, xr.Variable((\"x\",), [5.0, 6.0, 7.0]))\n\n\n", "def test_CFScaleOffsetCoder_decode_missing_scale_factor():\n    original = xr.Variable((\"x\",), [0, 1, 2], encoding={\"add_offset\": 5})\n    expected = xr.Variable((\"x\",), [5, 6, 7])\n    coder = variables.CFScaleOffsetCoder()\n    encoded = coder.encode(original)\n    roundtripped = coder.decode(encoded)\n    assert_identical(expected, roundtripped)\n\n\n\n", "def test_CFScaleOffsetCoder_roundtrip_mixed_dtypes(\n    data, scale, offset"], "sample_321": ["    def test_bad_csrf_cookie_missing(self):\n        \"\"\"\n        If the CSRF cookie is missing in a POST request, the middleware rejects\n        the incoming request.\n        \"\"\"\n        req = self._get_POST_no_csrf_cookie_request()\n        mw = CsrfViewMiddleware(post_form_view)\n        self._check_referer_rejects(mw, req)\n        response = mw.process_view(req, post_form_view, (), {})\n        self.assertEqual(response.status_code, 403)\n        msg = 'CSRF verification failed. Request missing CSRF token.'\n        self.assertContains(response, msg, status_code=403)\n\n", "    def test_csrf_cookie_cleared_on_logout(self):\n        \"\"\"Ensure CSRF cookie is cleared on logout.\"\"\"\n        req = self._get_GET_no_csrf_cookie_request()\n        mw = CsrfViewMiddleware(post_form_view)\n        mw.process_view(req, post_form_view, (), {})\n        resp = mw(req)\n        csrf_cookie = req.session.get(CSRF_SESSION_KEY)\n        self.assertIsNotNone(csrf_cookie)\n\n        # Simulate logout using session destruction\n        req.session.flush()\n\n        # Verify the cookie is cleared\n        mw.process_view(req, post_form_view, (), {})\n        resp = mw(req)\n        self.assertIsNone(req.session.get(CSRF_SESSION_KEY))\n", "    def test_https_good_referer_no_cookie_domain(self):\n        \"\"\"\n        A POST HTTPS request with a good referer is accepted even if\n        CSRF_COOKIE_DOMAIN is set but not in the referer.\n        \"\"\"\n        req = self._get_POST_request_with_token()\n        req._is_secure_override = True\n        req.META['HTTP_HOST'] = 'no-match.example.com'\n        req.META['HTTP_REFERER'] = 'https://example.com'\n        req.META['SERVER_PORT'] = '443'\n        mw = CsrfViewMiddleware(post_form_view)\n        self.assertIsNone(mw.process_view(req, post_form_view, (), {}))\n", "    def test_session_csrf_cookie_missing_on_redirect(self):\n        \"\"\"\n        If a GET request redirects to a view that requires CSRF protection,\n        a new CSRF cookie is generated and set in the session.\n        \"\"\"\n        req = self._get_GET_no_csrf_cookie_request()\n        view = RedirectView.as_view()(url='http://www.example.com/login')\n        mw = CsrfViewMiddleware(view)\n        resp = mw.process_view(req, view, (), {})\n        self.assertRedirects(resp, 'http://www.example.com/login', status_code=302)\n        self.assertTrue(req.session.get(CSRF_SESSION_KEY, False))\n\n\n", "    def test_https_good_referer_matches_cookie_domain_with_secure_cookie(self):\n        \"\"\"\n        A POST HTTPS request with a good referer should be accepted from a\n        subdomain that's allowed by SESSION_COOKIE_DOMAIN and when the\n        session cookie is marked secure.\n        \"\"\"\n        self._test_https_good_referer_matches_cookie_domain_with_secure_cookie()\n"], "sample_552": ["compilation error", "compilation error", "compilation error", "def test_savefig_metadata_metadata():\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.plot([1, 2, 3], [4, 5, 6])\n    fig.savefig(io.BytesIO(), format='png', metadata={'my_key': 'my_value'})\n", "    def test_savefig_metadata_custom(fig_test, fig_ref):\n        metadata = {'author': 'Bard',\n                    'title': 'Test Figure'}\n        fig_test.savefig(io.BytesIO(), format=fmt, metadata=metadata)\n        fig_ref.savefig(io.BytesIO(), format=fmt, metadata=metadata)\n"], "sample_869": ["def test_balanced_accuracy_score_with_sparse_array():\n    y_true = np.array([0, 1, 2, 0])\n    y_pred = np.array([0, 1, 2, 0])\n    sparse_y_true = csr_matrix(y_true)\n    sparse_y_pred = csr_matrix(y_pred)\n    balanced_acc = balanced_accuracy_score(sparse_y_true, sparse_y_pred)\n    assert balanced_acc == balanced_accuracy_score(y_true, y_pred)\n", "def test_brier_score_loss_multiclass():\n    y_true = np.array([0, 1, 2, 0])\n    y_pred = np.array([0.8, 0.6, 0.4, 0.2])\n    error_message = (\"Only binary classification is supported. \"\n                     \"Labels in y_true: {}\".format(np.array([0, 1, 2])))\n    with pytest.raises(ValueError, match=re.escape(error_message)):\n        brier_score_loss(y_true, y_pred)\n", "compilation error", "def test_average_precision_score_micro_average():\n    y_true = np.array([0, 1, 0, 1, 1])\n    y_scores = np.array([[0.1, 0.4, 0.3, 0.8, 0.9],\n                         [0.7, 0.3, 0.2, 0.1, 0.6]])\n    ap_micro = average_precision_score(y_true, y_scores, average='micro')\n    assert ap_micro == pytest.approx(0.6667)\n", "def test_balanced_accuracy_score_with_ties():\n    y_true = np.array([0, 1, 1, 0])\n    y_pred = np.array([0.5, 0.6, 0.4, 0.5])\n    balanced = balanced_accuracy_score(y_true, y_pred)\n    assert_almost_equal(balanced, 0.75, decimal=6)\n"], "sample_566": ["def test_subplots_with_subplotspec():\n    fig = plt.figure()\n    gs = fig.add_gridspec(2, 2)\n    ax1 = fig.add_subplot(gs[0, 0])\n    ax2 = fig.add_subplot(gs[0, 1])\n    ax3 = fig.add_subplot(gs[1, 0], sharex=ax1)\n    ax4 = fig.add_subplot(gs[1, 1], sharey=ax2)\n    assert ax1.get_position() == gs[0, 0].get_position()\n    assert ax2.get_position() == gs[0, 1].get_position()\n    assert ax3.get_position() == gs[1, 0].get_position()\n    assert ax4.get_position() == gs[1, 1].get_position()\n\n\n\n", "def test_savefig_bbox_inches_with_padding():\n    fig = plt.figure(figsize=(2, 2))\n    ax = fig.add_subplot(111)\n    ax.plot([0, 1], [0, 1])\n    bbox_inches_string = \"tight\"  \n    with io.BytesIO() as buffer:\n        fig.savefig(buffer, bbox_inches=bbox_inches_string, pad_inches=0.5)  \n    bbox_inches_dict = {\"tight\": (0, 0, 1, 1)}\n    with io.BytesIO() as buffer:\n        fig.savefig(buffer, bbox_inches=bbox_inches_dict[bbox_inches_string], pad_inches=0.5)   \n", "def test_savefig_metadata_filename(fmt):\n    fig = Figure()\n    metadata = {'my_key': 'my_value'}\n    with tempfile.NamedTemporaryFile(suffix=f'.{fmt}', delete=False) as f:\n        fig.savefig(f.name, format=fmt, metadata=metadata)\n        with open(f.name, 'rb') as file:\n            data = file.read()\n    assert metadata == fig.savefig(io.BytesIO(), format=fmt, metadata=metadata)._metadata\n\n    os.remove(f.name)\n\n", "def test_savefig_dpi_auto():\n    fig = Figure()\n    fig.savefig(io.BytesIO(), dpi='auto')\n\n    with pytest.raises(ValueError, match=\"dpi cannot be 'auto'.\") as e:\n        fig.savefig(io.BytesIO(), dpi='auto', format='pdf')\n    assert \"pdf output format does not support 'auto'\" in str(e.value)\n", "def test_savefig_bbox_inches():\n    fig = plt.figure(figsize=(5, 4))\n    ax = fig.add_subplot(111)\n    ax.set_xlim(0, 10)\n    ax.set_ylim(0, 10)\n    fig.tight_layout()\n    fig.savefig(io.BytesIO(), bbox_inches='tight', pad_inches=0)\n    fig.savefig(io.BytesIO(), bbox_inches='tight', pad_inches=0.1)\n\n\n\n"], "sample_1186": ["def test_array_shape_mismatch():\n    for ArrayType in mutable_array_types:\n        a = ArrayType([1, 2, 3, 4], (2, 2))\n        b = ArrayType([5, 6, 7, 8], (2, 3))\n        raises(ValueError, lambda: a + b)\n        raises(ValueError, lambda: a - b)\n        raises(ValueError, lambda: a * b)\n\n\n\n", "def test_array_slicing_negative_indices():\n    for ArrayType in array_types:\n        test_array = ArrayType([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n        assert test_array[:-1, :] == Array([[1, 2, 3, 4, 5]])\n        assert test_array[-2:, :] == Array([[8, 9, 10]])\n        assert test_array[:, :-1] == Array([[1, 2, 3, 4], [6, 7, 8, 9]])\n        assert test_array[:, -2:] == Array([[2, 3, 4], [7, 8, 9]]) \n", "def test_array_negative_indices():\n    for ArrayType in array_types:\n        test_array = ArrayType([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n        assert test_array[:, -1] == Array([5, 10])\n        assert test_array[:, -2] == Array([4, 9])\n        assert test_array[:, -3] == Array([3, 8])\n        assert test_array[:, -4] == Array([2, 7])\n        assert test_array[:, -5] == Array([1, 6])\n        assert test_array[:, 0] == Array([1, 6])\n        assert test_array[:, 1] == Array([2, 7])\n        assert test_array[:, 2] == Array([3, 8])\n        assert test_array[:, 3] == Array([4, 9])\n        assert test_array[:, 4] == Array([5, 10])\n\n        raises(ValueError, lambda: test_array[:, -6])\n        raises(ValueError, lambda: test_array[-3, :])\n\n        assert test_array[-1, -1] == 10\n\n\n", "def test_array_shape_errors():\n    for ArrayType in array_types:\n        raises(ValueError, lambda: ArrayType([1, 2, 3], shape=(2, 3)))\n        raises(ValueError, lambda: ArrayType([1, 2, 3, 4], shape=(3,)))\n        raises(TypeError, lambda: ArrayType([1, 2, 3], shape=(\"a\", 2)))\n        raises(ValueError, lambda: ArrayType([1, 2, 3, 4], shape=(3,0)))\n", "def test_array_creation_from_iterable():\n    for ArrayType in array_types:\n        test_array = ArrayType([1, 2, 3, 4, 5, 6], shape=(2, 3))\n        assert test_array.shape == (2, 3)\n        assert list(test_array) == [1, 2, 3, 4, 5, 6]\n\n        test_array = ArrayType((1, 2, 3, 4, 5, 6), shape=(2, 3))\n        assert test_array.shape == (2, 3)\n        assert list(test_array) == [1, 2, 3, 4, 5, 6]\n\n        test_array = ArrayType([[1, 2], [3, 4], [5, 6]], shape=(3, 2))\n        assert test_array.shape == (3, 2)\n        assert list(test_array) == [1, 2, 3, 4, 5, 6]\n\n        test_array = ArrayType({'0': 1, '1': 2, '2': 3, '3': 4, '4': 5, '5': 6}, shape=(2, 3))\n        assert test_array.shape == (2, 3)\n        assert list(test_array) == [1, 2, 3, 4, 5, 6]\n"], "sample_517": ["    def test_pdf_unicode_math():\n        plt.rcParams['pdf.fonttype'] = 42\n        plt.rcParams['mathtext.fontset'] = 'stixsans'\n        fig = plt.figure()\n        ax = fig.add_subplot(111)\n        ax.text(0.5, 0.5, r\"$\\sum_{\\i=0}^{\\infty} \\frac{1}{i!} \\pi^{2i} $\", size=20)\n        fig.canvas.draw()\n\n\n\n", "def test_text_unicode_support():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, '\u4f60\u597d\u4e16\u754c', ha=\"center\", va=\"center\", fontsize=20)\n    fig.canvas.draw()\n    assert text.get_text() == '\u4f60\u597d\u4e16\u754c'\n", "def test_unicode_text():\n    fig, ax = plt.subplots()\n    ax.text(0.5, 0.5, \"\u3053\u3093\u306b\u3061\u306f\", fontsize=16)\n    fig.canvas.draw()\n\n\n\n", "def test_unicode_text():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, \"\u4f60\u597d\u4e16\u754c\", fontsize=16)\n    fig.canvas.draw()\n    assert text.get_text() == \"\u4f60\u597d\u4e16\u754c\"\n\n\n", "def test_text_html_encoding():\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.text(0.5, 0.5, \"\u00e9\u00e0\u00e7\u00fc\u00f6\", fontsize=20)\n    fig.savefig(\"test.html\")\n\n"], "sample_43": ["def test_events_fitness_prior_p0():\n    events = Events(p0=0.1)\n    t = np.linspace(0, 1, 11)\n    x = np.ones_like(t)\n\n    # test if p0 is indeed used in the fitness function\n    fit1 = events.fitness(N_k=10, T_k=0.1)\n    events = Events(p0=0.01)\n    fit2 = events.fitness(N_k=10, T_k=0.1)\n\n    assert fit1 != fit2\n\n", "def test_p0_prior_not_specified():\n    rng = np.random.RandomState(0)\n    t = rng.rand(100)\n    x = np.ones_like(t)\n\n    bins = bayesian_blocks(t, x, fitness='events')\n    assert bins.size > 0\n    assert 'ncp_prior' not in bins.attrs\n\n\n\n", "def test_event_data_with_p0():\n    rng = np.random.RandomState(0)\n    x = np.concatenate([rng.rand(100),\n                        1 + rng.rand(200)])\n\n    bins1 = bayesian_blocks(x, fitness='events', p0=0.01)\n    bins2 = bayesian_blocks(x, fitness='events', p0=0.05)\n\n    assert_allclose(bins1, bins2, rtol=0.02)\n\n\n\n", "def test_p0_prior_computation():\n    fitness = Events(p0=0.01, gamma=None, ncp_prior=None)\n    N = 100\n    ncp_prior = fitness.compute_ncp_prior(N)\n    assert np.isclose(ncp_prior,  fitness.p0_prior(N), rtol=1E-6)\n\n    fitness = Events(p0=None, gamma=1.0, ncp_prior=None)\n    ncp_prior = fitness.compute_ncp_prior(N)\n    assert np.isclose(ncp_prior, -np.log(fitness.gamma), rtol=1E-6)\n\n    fitness = Events(p0=None, gamma=None, ncp_prior=-2.0)\n    ncp_prior = fitness.compute_ncp_prior(N)\n    assert np.isclose(ncp_prior, -2.0, rtol=1E-6)\n\n", "def test_custom_fitness_func():\n    class MyFitnessFunc(FitnessFunc):\n            return a_k * b_k\n\n    bins = bayesian_blocks(t, x, sigma, fitness=MyFitnessFunc())\n\n    assert_allclose(bins, [0, 0.45, 0.55, 1])\n\n"], "sample_311": ["    def test_known_url_appends_slash_if_missing_without_final_catch_all_view(self):\n        superuser = User.objects.create_user(\n            username='staff',\n            password='secret',\n            email='staff@example.com',\n            is_staff=True,\n        )\n        self.client.force_login(superuser)\n        known_url = reverse('admin10:admin_views_article_changelist')\n        response = self.client.get(known_url[:-1])\n        self.assertRedirects(response, known_url, status_code=301, fetch_redirect_response=False)\n", "    def test_final_catch_all_view_handles_empty_urls(self):\n        superuser = User.objects.create_superuser(\n            username='super',\n            password='secret',\n            email='super@example.com',\n        )\n        self.client.force_login(superuser)\n        empty_url = '/test_admin/admin/'\n        response = self.client.get(empty_url)\n        self.assertEqual(response.status_code, 404)\n\n\n", "    def test_final_catch_all_view_disabled_non_staff_user(self):\n        user = User.objects.create_user(\n            username='user',\n            password='secret',\n            email='user@example.com',\n            is_staff=False,\n        )\n        self.client.force_login(user)\n        unknown_url = '/test_admin/admin/unknown/'\n        response = self.client.get(unknown_url)\n        self.assertEqual(response.status_code, 403) \n", "    def test_final_catch_all_view_disabled(self):\n        site = AdminSite(name='admin11')\n        site.final_catch_all_view = None\n        with patch('django.urls.path', side_effect=lambda *args, **kwargs: None):\n            response = self.client.get('/test_admin/admin11/unknown/')\n        self.assertEqual(response.status_code, 404)\n\n", "    def test_final_catch_all_view_disabled(self):\n        admin_site = admin.AdminSite()\n        admin_site.final_catch_all_view = None\n\n        with override_settings(INSTALLED_APPS=['admin', 'admin_views', ], site=admin_site):\n            url = reverse('admin:admin_views_restaurant_changelist')\n            response = self.client.get(url)\n            self.assertEqual(response.status_code, 404)\n\n            url = reverse('admin:login')\n            response = self.client.get(url)\n            self.assertEqual(response.status_code, 200)\n\n"], "sample_749": ["def test_column_transformer_sparse_remainder_transformer_with_integer_array():\n    X_array = np.array([[0, 1, 2],\n                        [2, 4, 6],\n                        [8, 6, 4]]).astype(np.int32)\n\n    ct = ColumnTransformer([('trans1', Trans(), [0])],\n                           remainder=SparseMatrixTrans())\n\n    X_trans = ct.fit_transform(X_array)\n    assert sparse.issparse(X_trans)\n    # SparseMatrixTrans creates 3 features for each column. There is\n    # one column in ``transformers``, thus:\n    assert X_trans.shape == (3, 3 + 1)\n\n    exp_array = np.hstack(\n        (X_array[:, 0].reshape(-1, 1), np.eye(3)))\n    assert_array_equal(X_trans.toarray(), exp_array)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == 'remainder'\n    assert isinstance(ct.transformers_[-1][1], SparseMatrixTrans)\n    assert_array_equal(ct.transformers_[-1][2], [1, 2])\n", "def test_column_transformer_sparse_matrix_trans_with_remainder_transformers():\n    X_array = np.array([[0, 1, 2],\n                        [2, 4, 6],\n                        [8, 6, 4]]).T\n\n    ct = ColumnTransformer([('trans1', Trans(), [0])],\n                           remainder=[\n                               SparseMatrixTrans(),\n                               StandardScaler()\n                           ])\n\n    X_trans = ct.fit_transform(X_array)\n    assert sparse.issparse(X_trans)\n\n    #  SparseMatrixTrans creates 3 features for each column in the first\n    # remainder transformation, and StandardScaler transforms the second remainder\n    assert X_trans.shape == (3, 3 + 1 + 2)  \n    assert_array_equal(X_trans.toarray()[:, :3], np.eye(3))\n\n", "def test_column_transformer_sparse_remainder_transformer_dtypes():\n    X_array = np.array([[0, 1, 2],\n                        [2, 4, 6],\n                        [8, 6, 4]], dtype=np.float32)\n    ct = ColumnTransformer([('trans1', Trans(), [0])],\n                           remainder=SparseMatrixTrans())\n\n    X_trans = ct.fit_transform(X_array)\n    assert sparse.issparse(X_trans)\n    assert X_trans.dtype == np.float32\n\n    exp_array = np.hstack(\n        (X_array[:, 0].reshape(-1, 1), np.eye(3)))\n    assert_array_equal(X_trans.toarray(), exp_array)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == 'remainder'\n    assert isinstance(ct.transformers_[-1][1], SparseMatrixTrans)\n    assert_array_equal(ct.transformers_[-1][2], [1, 2])\n", "def test_column_transformer_sparse_matrix_trans_invalid_input():\n    with pytest.raises(ValueError):\n        ct = ColumnTransformer([('trans', SparseMatrixTrans(), [0])],\n                               remainder='passthrough')\n", "def test_column_transformer_pandas_remainder_transformer():\n    pd = pytest.importorskip('pandas')\n    X_df = pd.DataFrame({'first': [0, 1, 2], 'second': [2, 4, 6], 'third': [8, 6, 4]})\n\n    ct = ColumnTransformer([('trans1', Trans(), 'first')],\n                           remainder=SparseMatrixTrans())\n\n    X_trans = ct.fit_transform(X_df)\n    assert sparse.issparse(X_trans)\n    # SparseMatrixTrans creates 3 features for each column, thus:\n    assert X_trans.shape == (3, 3 + 1)\n\n    exp_array = np.hstack(\n        (X_df['first'].values.reshape(-1, 1), np.eye(3)))\n    assert_array_equal(X_trans.toarray(), exp_array)\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == 'remainder'\n    assert isinstance(ct.transformers_[-1][1], SparseMatrixTrans)\n    assert_array_equal(ct.transformers_[-1][2], ['second', 'third'])\n\n\n\n"], "sample_382": ["    def test_get_template_directories_with_nested_folders(self):\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            {\n                ROOT / 'templates',\n                Path(__file__).parent / 'subfolder',\n            }\n        )\n", "    def test_get_template_directories_multiple_dirs(self):\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            {\n                ROOT / 'templates_extra',\n                ROOT / 'templates',\n                ROOT / 'subtemplates',\n            }\n        )\n", "    def test_watch_for_template_changes_with_single_template_dir(self):\n        mock_reloader = mock.MagicMock()\n        autoreload.watch_for_template_changes(mock_reloader)\n        self.assertSequenceEqual(\n            sorted(mock_reloader.watch_dir.call_args_list),\n            [\n                mock.call(Path(__file__).parent / 'templates', '**/*')\n            ]\n        )\n", "    def test_get_template_directories_with_jinja2(self):\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            {\n                ROOT / 'absolute_str',\n                Path.cwd() / 'template_tests/relative_str',\n                Path.cwd() / 'template_tests/relative_path',\n            }\n        )\n", "    def test_template_directories_order(self):\n        self.assertSequenceEqual(\n            sorted(autoreload.get_template_directories()),\n            [\n                ROOT / 'templates_extra',\n            ]\n        )\n"], "sample_1193": ["def test_are_coplanar():\n    assert are_coplanar(Point(0, 0, 0), Point(1, 0, 0), Point(0, 1, 0)) == True\n    assert are_coplanar(Point(0, 0, 0), Point(1, 0, 0), Point(0, 0, 1)) == True\n    assert are_coplanar(Point(0, 0, 0), Point(1, 1, 0), Point(1, 0, 0)) == True\n    assert are_coplanar(Point(1, 1, 0), Point(2, 1, 0), Point(1, 0, 0)) == True\n    assert are_coplanar(Point(0, 0, 0), Point(1, 2, 3), Point(4, 5, 6)) == True\n    assert are_coplanar(Point(0, 1, 0), Point(1, 0, 0), Point(1, 1, 0)) == False\n    assert are_coplanar(Point(0, 0, 1), Point(1, 0, 0), Point(0, 1, 0)) == False\n    assert are_coplanar(Point(0, 0, 0), Point(1, 0, 0), Point(0, 0, 1), Point(1, 1, 1)) == True\n    assert are_coplanar(Point(0, 0, 0), Point(1, 0, 0), Point(0, 1, 0), Point(1, 1, 1)) == True\n    assert are_coplanar(Point(0, 0, 0), Point(1, 0, 0), Point(0, 1, 0), Point(1, 0, 1)) == True \n\n", "def test_are_coplanar():\n    from sympy.geometry.util import are_coplanar\n\n    assert are_coplanar(Point(0,0), Point(1,0), Point(2,0)) == True\n    assert are_coplanar(Point(0,0), Point(1,0), Point(1,1)) == False\n    assert are_coplanar(Point(0,0), Point(1,1), Point(1,2)) == False\n    assert are_coplanar(Point(0,0), Point(1,1), Point(2,2)) == True\n    assert are_coplanar(Point(0,0), Point(0,1), Point(1,1)) == True\n    assert are_coplanar(Point(0,0), Point(1,0), Point(0, 1)) == False\n    assert are_coplanar(Point(0,0), Point(1,0), Point(0,2)) == False\n\n    points = [Point2D(0,0), Point2D(1,1), Point2D(2,0), Point2D(3,1)]\n    assert are_coplanar(*points) == False\n\n\n", "def test_are_coplanar():\n    # Test cases for are_coplanar\n    p1 = Point(0, 0, 0)\n    p2 = Point(1, 0, 0)\n    p3 = Point(0, 1, 0)\n    p4 = Point(1, 1, 0)\n\n    assert are_coplanar(p1, p2, p3, p4) == True\n    \n    p5 = Point(0, 0, 1)\n    assert are_coplanar(p1, p2, p3, p5) == False\n\n    # Test with 3D points\n    p6 = Point(0, 0, 0)\n    p7 = Point(1, 0, 0)\n    p8 = Point(0, 1, 0)\n    p9 = Point(1, 1, 1)\n    assert are_coplanar(p6, p7, p8, p9) == False \n    p10 = Point(2, 0, 0)\n    assert are_coplanar(p6, p7, p8, p10) == False\n\n\n", "def test_are_coplanar():\n    raises(TypeError, lambda: are_coplanar(Point(0, 0), 3, 4))\n    assert are_coplanar(Point(0, 0), Point(1, 0), Point(2, 0))\n    assert are_coplanar(Point(0, 0), Point(1, 1), Point(2, 2))\n    assert are_coplanar(Point(1, 0), Point(2, 0), Point(3, 0))\n    assert not are_coplanar(Point(0, 0), Point(1, 0), Point(1, 1))\n    assert not are_coplanar(Point(0, 0), Point(1, 0), Point(1, 2))\n    p1 = Point(0, 0)\n    p2 = Point(1, 1)\n    p3 = Point(2, 2)\n    p4 = Point(3, 3)\n    assert are_coplanar(p1, p2, p3) and are_coplanar(p1, p2, p4) and \\\n        are_coplanar(p1, p3, p4)\n    assert not are_coplanar(p1, p2, Point(1, -1))\n", "def test_are_coplanar():\n    p = Point2D(0, 0)\n    q = Point2D(1, 0)\n    r = Point2D(2, 0)\n    s = Point2D(0, 1)\n    assert are_coplanar(p, q, r, s)\n    assert are_coplanar(p, q, r, p)\n\n    p = Point2D(0, 0)\n    q = Point2D(1, 0)\n    r = Point2D(2, 0)\n    s = Point2D(0, 1)\n    assert not are_coplanar(p, q, r, Point2D(0, 2))\n    assert not are_coplanar(p, q, r, Point2D(1, 1))\n    assert not are_coplanar(p, q, Point2D(1, 1), s)\n\n"], "sample_1065": ["def test_binomial_as_series():\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True)\n    x = Symbol('x')\n\n    assert binomial(n, k).series(x, 0, 3) == \\\n        binomial(n, k)*x**k + O(x**3)\n    assert binomial(n, k).series(x, 1, 3) == \\\n        binomial(n, k)*(x - 1)**k + O((x - 1)**3)\n    assert binomial(n, k).series(x, S(1)/2, 3) == \\\n        binomial(n, k)*(x - S(1)/2)**k + O((x - S(1)/2)**3)\n    assert binomial(n, k).series(x, -1, 3) == \\\n        binomial(n, k)*(x + 1)**k + O((x + 1)**3)\n    assert binomial(n, k).series(x, S(-1)/2, 3) == \\\n        binomial(n, k)*(x + S(1)/2)**k + O((x + S(1)/2)**3)\n\n\n\n", "compilation error", "def test_factorials_expand():\n    from sympy import factorial\n\n    assert expand(factorial(x + y)) == x*factorial(x)*factorial(y + 1)\n", "def test_binomial_series():\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True)\n\n    assert binomial(n, k).series(n, n, 5) == \\\n        binomial(n, k) + O(n/2)\n    assert binomial(n, k).series(n, k, 5) == \\\n        binomial(n, k) + O(n - k)\n    assert binomial(n, k).series(n, 0, 5) == \\\n        binomial(n, k) + O(n)\n    assert expand_func(binomial(S(13)/2, S(5)/2)) ==  1 - S(1)/2*S(1)/2 + S(1)/2*S(1)/2*S(3)/4 - S(1)/2*S(1)/2*S(3)/4*S(5)/8 + S(1)/2*S(1)/2*S(3)/4*S(5)/8*S(7)/16\n    assert binomial(n, n-1).series(n, n, 5) == \\\n        n + O(1)\n    assert binomial(n, 1).series(n, 1, 5) == \\\n        n + O(1)\n\n", "def test_binomial_series():\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True)\n\n    assert binomial(n, k).series(n, 0, 3) == \\\n        binomial(0, k)*n**k/factorial(k) + binomial(1, k)*n**(k + 1)/factorial(k + 1) + \\\n        binomial(2, k)*n**(k + 2)/factorial(k + 2) + O(n**3)\n\n\n"], "sample_418": ["    def test_none_type(self):\n        self.assertIs(length_is(None, 0), True)\n        self.assertIs(length_is(None, 1), False)\n", "    def test_float(self):\n        self.assertIs(length_is(3.14, 0), False)\n        self.assertIs(length_is(3.14, 1), False)\n        self.assertIs(length_is(3.14, 2), False)\n", "    def test_length_is12(self):\n        output = self.engine.render_to_string(\"length_is12\", {})\n        self.assertEqual(output, \"Length not 0\")\n\n\n\n", "    def test_length_is12(self):\n        output = self.engine.render_to_string(\"length_is12\", {})\n        self.assertEqual(output, \"Four\")\n", "    def test_list_with_none(self):\n        self.assertIs(length_is([1, None], 2), True)\n        self.assertIs(length_is([1, None], 3), False)\n"], "sample_312": ["    def test_add_squash(self):\n        node1 = Node(self.node1_children)\n        node2 = Node([('c', 4)], connector=Node.default)\n        node3 = node1.add(node2, Node.default)\n        self.assertEqual(len(node1), len(node1.children) + 1)\n        self.assertEqual(str(node1), \"(DEFAULT: ('a', 1), ('b', 2), ('c', 4))\")\n        self.assertEqual(node3, node2)\n", "    def test_add_squash_similar_node(self):\n        node1 = Node([('a', 1)], 'AND')\n        node2 = Node([('a', 2)], 'AND')\n        combined = node1.add(node2, 'AND', squash=True)\n        self.assertEqual(combined, node2)\n        expected_str = \"(AND: ('a', 1), ('a', 2))\"\n        self.assertEqual(str(node1), expected_str)\n        self.assertEqual(len(node1), 2)\n\n", "    def test_add_squash(self):\n        node1 = Node([('a', 1), ('b', 2)], connector='AND')\n        node2 = Node([('c', 3), ('d', 4)], connector='AND')\n        node3 = node1.add(node2, 'AND', squash=True)\n        self.assertEqual(len(node3.children), 2)\n        self.assertEqual(str(node3), \"(AND: ('a', 1), ('b', 2), ('c', 3), ('d', 4))\")\n        self.assertEqual(node3.connector, 'AND')\n", "    def test_add_squash_multiple_children(self):\n        node1 = Node([('a', 1), ('b', 2)], connector='AND')\n        node2 = Node([('c', 3), ('d', 4)], connector='AND')\n\n        # Squash multiple children\n        result = node1.add(node2, 'AND', squash=True)\n\n        self.assertEqual(len(result.children), 4)\n        self.assertEqual(str(result), \"(AND: ('a', 1), ('b', 2), ('c', 3), ('d', 4))\")\n", "    def test_add_squash_same_connector(self):\n        node1 = Node([('a', 1), ('b', 2)], 'AND')\n        node2 = Node([('c', 3)], 'AND')\n        result = node1.add(node2, 'AND', squash=True)\n        self.assertEqual(str(result), \"(AND: ('a', 1), ('b', 2), ('c', 3))\")\n        self.assertEqual(result, node1)  \n"], "sample_789": ["compilation error", "compilation error", "compilation error", "def test_random_state():\n    # Test that random_state is properly set and used.\n    rng = np.random.RandomState(0)\n    for alg in ['SAMME', 'SAMME.R']:\n        clf1 = AdaBoostClassifier(algorithm=alg, random_state=rng)\n        clf2 = AdaBoostClassifier(algorithm=alg, random_state=rng)\n        assert_equal(clf1.estimators_[0].random_state, rng.randint(1000))\n        assert_equal(clf2.estimators_[0].random_state, rng.randint(1000))\n        assert_equal(clf1.estimators_[0].random_state, clf2.estimators_[0].random_state)\n        \n        # should be same even after fitting\n        clf1.fit(X, y_class)\n        clf2.fit(X, y_class)\n        assert_equal(clf1.estimators_[0].random_state, clf2.estimators_[0].random_state)\n\n\n\n", "def test_sample_weight_adaboost_classifier():\n    \"\"\"\n    AdaBoostClassifier should work without sample_weights in the base estimator\n    The random weighted sampling is done internally in the _boost method in\n    AdaBoostClassifier.\n    \"\"\"\n    class DummyEstimator(BaseEstimator):\n\n            pass\n\n            return np.zeros(X.shape[0])\n\n    # Test with sample_weights provided in AdaBoostClassifier\n    X, y = datasets.make_classification(n_samples=100, n_features=20,\n                                        n_informative=10,\n                                        random_state=0)\n\n    clf = AdaBoostClassifier(base_estimator=DummyEstimator(),\n                            algorithm=\"SAMME\",\n                            random_state=0)\n    sample_weight = np.random.rand(X.shape[0])\n    clf.fit(X, y, sample_weight=sample_weight)\n    assert_equal(clf.estimators_[0].__class__.__name__, DummyEstimator.__name__)\n\n\n"], "sample_46": ["    def test_uuid_pk_filter(self):\n        pk1 = PrimaryKeyUUIDModel.objects.create(id=uuid.uuid4())\n        pk2 = PrimaryKeyUUIDModel.objects.create(id=uuid.uuid4())\n        q = PrimaryKeyUUIDModel.objects.filter(id=pk1.id)\n        self.assertSequenceEqual(list(q), [pk1])\n        q = PrimaryKeyUUIDModel.objects.filter(id=pk2.id)\n        self.assertSequenceEqual(list(q), [pk2])\n\n\n\n", "    def test_ordering(self):\n        f = UUIDModel.objects.create(field=uuid.UUID(\"123e4567-e89b-12d3-a456-426655440000\"))\n        s = UUIDModel.objects.create(field=uuid.UUID(\"fe123456-e89b-12d3-a456-426655440000\"))\n        o = UUIDModel.objects.create(field=uuid.UUID(\"abcdef12-e89b-12d3-a456-426655440000\"))\n        results = list(UUIDModel.objects.all().order_by('-field'))\n        self.assertEqual(results, [s, o, f])\n", "    def test_uuid_pk_auto_assignment(self):\n        with self.settings(UUID_FIELD_AUTO_ASSIGN_ON_CREATE=True):\n            instance = PrimaryKeyUUIDModel.objects.create()\n            loaded = PrimaryKeyUUIDModel.objects.get()\n            self.assertIsNotNone(loaded.pk)\n            self.assertEqual(loaded.pk, instance.pk)\n", "    def test_update_with_uuid_literal(self):\n        u1 = PrimaryKeyUUIDModel.objects.create()\n        with self.assertRaises(IntegrityError):\n            RelatedToUUIDModel.objects.update(uuid_fk='not-a-uuid')\n\n\n", "    def test_foreign_key_lookup_with_uuid(self):\n        related = RelatedToUUIDModel.objects.create(uuid_fk=UUIDModel.objects.create())\n        instance = UUIDModel.objects.filter(pk=related.uuid_fk_id).first()\n        self.assertIsNotNone(instance)\n"], "sample_935": ["        def assert_consistency(self):\n            for tag in self.content_classes:\n                assert self.content_classes[tag].issubset(self.classes), (\n                    f\"Role '{self.name}' content tag '{tag}' classes\"\n                    f\" ({self.content_classes[tag]})\"\n                    f\" is not a subset of root classes ({self.classes}).\"\n                )\n", "compilation error", "        def __init__(self, role, root, contents):\n            super().__init__(role, root, contents)\n            ", "        def __str__(self):\n            return f\"RoleClasses(name='{self.name}', classes={self.classes}, content_classes={self.content_classes})\"\n", "compilation error"], "sample_760": ["def test_scorer_memmap_output(name):\n    # test that the output of the scored function is actually a number\n\n    # We only test for numeric results\n    scorer, estimator = SCORERS[name], ESTIMATORS[name]\n    if name in MULTILABEL_ONLY_SCORERS:\n        score = scorer(estimator, X_mm, y_ml_mm)\n    else:\n        score = scorer(estimator, X_mm, y_mm)\n    assert isinstance(score, numbers.Number)\n", "def test_scorer_pos_label_none():\n    # Test that scorers handle pos_label=None correctly\n    X = np.array([[0], [1], [0], [1]])\n    y = np.array([0, 1, 0, 1])\n    \n    for name, scorer in SCORERS.items():\n        if name in MULTILABEL_ONLY_SCORERS:\n            continue  # Multilabel scorers don't use pos_label\n        \n        clf = DecisionTreeClassifier()\n        clf.fit(X, y)\n        \n        # pos_label is not specified\n        score1 = scorer(clf, X, y, pos_label=None)\n        \n        # pos_label is set to the most frequent class\n        score2 = scorer(clf, X, y, pos_label=np.argmax(np.bincount(y)))\n        \n        assert_almost_equal(score1, score2, err_msg=\"Scorer {0} behaves \"\n                             \"differently when pos_label is None vs \"\n                             \"default\".format(name))\n", "def test_cluster_scorer_memmap_input(scorer_name):\n    # Non-regression test for #6147: some score functions would\n    # return singleton memmap when computed on memmap data instead of scalar\n    # float values.\n    check_scorer_memmap(scorer_name)\n", "def test_scorer_with_proba_on_regressor():\n    # Test scorer with probabilities on regressors\n    score_name = 'neg_log_loss'\n    X, y = make_blobs(random_state=0, centers=2)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n    regressor = DecisionTreeRegressor()\n    regressor.fit(X_train, y_train)\n\n    with pytest.raises(ValueError, match=\"requires target to be a classification\"):\n        get_scorer(score_name)(regressor, X_test, y_test)\n\n    # Test with probabilities (dummy data)\n    regressor._predict_proba = regressor.predict\n    regressor.predict = lambda X: X\n    with pytest.raises(ValueError, match=\"The 'proba' parameter is\"):\n        get_scorer(score_name)(regressor, X_test, y_test, proba=True)\n", "def test_multimetric_scoring_with_different_scoring_names():\n    # Test that multimetric scoring works with different scoring names\n\n    scorers = {\n        'acc': make_scorer(accuracy_score),\n        'precision': make_scorer(precision_score),\n        'f1': make_scorer(f1_score)\n    }\n    estimator = LinearSVC(random_state=0)\n    estimator.fit([[1], [2], [3]], [1, 1, 0])\n\n    scores = _check_multimetric_scoring(estimator, scorers)\n    assert isinstance(scores, dict)\n    assert set(scorers.keys()) == set(scores.keys())\n    assert all([isinstance(scorer, _PredictScorer)\n                for scorer in list(scores.values())])\n    \n    # Test that it works with a combination of scorers\n    scores = _check_multimetric_scoring(estimator,\n                                       [('acc', make_scorer(accuracy_score)),\n                                        'precision',\n                                        {'f1': make_scorer(f1_score)}])\n    assert isinstance(scores, dict)\n    assert set(['acc', 'precision', 'f1']) == set(scores.keys())\n    assert all([isinstance(scorer, _PredictScorer)\n                for scorer in list(scores.values())])\n"], "sample_196": ["    def test_sql_flush_with_tables(self):\n        with transaction.atomic():\n            Author.objects.create(name='Testing Author')\n            Book.objects.create(title='Testing Book', author=Author.objects.first())\n        sql_list = connection.ops.sql_flush(no_style(), ['auth_user', 'auth_group', 'auth_permission', 'auth_user_groups', 'auth_user_permissions', 'contenttypes', 'blog_post'])\n        self.assertEqual(len(sql_list), 1)\n", "    def test_sql_flush_with_tables(self):\n        author = Author.objects.create(name='Author A')\n        book = Book.objects.create(title='Book A', author=author)\n\n        with transaction.atomic():\n            # Create a new table (without actually creating it, as that would be destructive)\n            with self.assertRaises(NotImplementedError):\n                connection.ops.sql_flush(no_style(), ['django_content_type'], ignore_conflicts=False)\n\n        self.assertEqual(Author.objects.count(), 1)\n        self.assertEqual(Book.objects.count(), 1)\n\n\n", "    def test_sql_flush_with_tables(self):\n        # Create a test table.\n        with transaction.atomic(using=self.connection.alias):\n            with self.connection.cursor() as cursor:\n                cursor.execute(\"\"\"\n                    CREATE TABLE sql_flush_test_table (\n                        id INTEGER PRIMARY KEY,\n                        name VARCHAR(255)\n                    )\n                \"\"\")\n        # Flush the table and check the SQL.\n        sql_list = connection.ops.sql_flush(no_style(), ['sql_flush_test_table'])\n        self.assertEqual(len(sql_list), 1)\n        self.assertIn(\"TRUNCATE TABLE sql_flush_test_table\", sql_list[0])\n        # Drop the table to cleanup.\n        with transaction.atomic(using=self.connection.alias):\n            with self.connection.cursor() as cursor:\n                cursor.execute(\"DROP TABLE sql_flush_test_table\")\n", "    def test_sql_flush_with_tables(self):\n        # Create some tables to flush\n        with transaction.atomic():\n            connection.create_all([Author, Book])\n            self.assertEqual(Author.objects.count(), 0)\n            self.assertEqual(Book.objects.count(), 0)\n\n        # Flush all tables and verify they are deleted\n        sql_list = connection.ops.sql_flush(no_style(), [Author, Book])\n        connection.ops.execute_sql_flush(sql_list)\n        self.assertEqual(Author.objects.count(), 0)\n        self.assertEqual(Book.objects.count(), 0)\n\n", "    def test_sql_flush_with_tables(self):\n        with transaction.atomic():\n            Author.objects.create(name='Test Author')\n            Book.objects.create(title='Test Book', author=Author.objects.get(name='Test Author'))\n        sql = connection.ops.sql_flush(no_style(), ['authors', 'books'])\n        self.assertGreater(len(sql), 0)\n\n        with transaction.atomic():\n            Author.objects.create(name='Another Test Author')\n            Book.objects.create(title='Another Test Book', author=Author.objects.get(name='Another Test Author'))\n        sql = connection.ops.sql_flush(no_style(), ['authors', 'books'])\n        self.assertGreater(len(sql), 0)  \n\n\n\n"], "sample_90": ["    def test_setattr_raises_validation_error_no_form(self):\n        \"\"\"\n        A model ValidationError called directly, not via form, should\n        result in a proper error being raised.\n        \"\"\"\n        try:\n            instance = StrictAssignmentAll()\n            instance._should_error = True\n            setattr(instance, 'title', 'testing setattr')\n        except forms.ValidationError as e:\n            self.assertEqual(str(e), 'Cannot set attribute\\nThis field cannot be blank.')\n        except Exception as e:\n            self.fail(f\"Unexpected exception: {e}\")\n\n\n\n", "    def test_setattr_raises_validation_error_field_specific_form_data(self):\n        \"\"\"\n        A model ValidationError using the dict form should put the error\n        message into the correct key of form.errors, even if the form data\n        is incorrect.\n        \"\"\"\n        form_class = modelform_factory(model=StrictAssignmentFieldSpecific, fields=['title'])\n        form = form_class(data={'title': '  '}, files=None)\n        form.instance._should_error = True\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors, {\n            'title': ['Cannot set attribute', 'This field cannot be blank.']\n        })\n", "    def test_model_form_validation_error_during_clean(self):\n        class ModelWithCleanError(models.Model):\n            pass\n\n        class ModelWithCleanErrorForm(forms.ModelForm):\n            class Meta:\n                model = ModelWithCleanError\n                fields = ['__all__']\n\n            raise forms.ValidationError('This is a clean error')\n\n        form = ModelWithCleanErrorForm(data={'invalid_data': 'test'})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors, {'__all__': ['This is a clean error']})\n", "    def test_setattr_does_not_raise_error_on_valid_data(self):\n        \"\"\"\n        If data is valid, __setattr__() should not raise a ValidationError.\n        \"\"\"\n        form_class = modelform_factory(model=StrictAssignmentFieldSpecific, fields=['title'])\n        form = form_class(data={'title': 'valid'}, files=None)\n        self.assertTrue(form.is_valid())\n        self.assertNotIn('title', form.errors)\n", "    def test_setattr_raises_validation_error_empty_form(self):\n        \"\"\"\n        A model ValidationError not using the dict form should put the error\n        message into __all__ (i.e. non-field errors) on the form.\n        \"\"\"\n        form_class = modelform_factory(model=StrictAssignmentAll, fields=['title'])\n        form = form_class()\n        # This line turns on the ValidationError; it avoids the model erroring\n        # when its own __init__() is called when creating form.instance.\n        form.instance._should_error = True\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors, {\n            '__all__': ['Cannot set attribute'],\n            'title': ['This field cannot be blank.']\n        })\n"], "sample_152": ["    def test_fast_delete_null_field(self):\n        u = User.objects.create(\n            avatar=Avatar.objects.create()\n        )\n        a = Avatar.objects.get(pk=u.avatar_id)\n        a.desc = None\n        a.save()\n        # 1 query to fast-delete the user\n        # 1 query to delete the avatar\n        self.assertNumQueries(2, a.delete)\n        self.assertFalse(User.objects.exists())\n        self.assertFalse(Avatar.objects.exists())\n", "    def test_fast_delete_with_signals(self):\n        calls_count = 0\n            nonlocal calls_count\n            calls_count += 1\n\n        models.signals.post_delete.connect(receiver, sender=Avatar)\n\n        a = Avatar.objects.create()\n        calls_count = 0\n        a.delete()\n        self.assertEqual(calls_count, 1)\n        models.signals.post_delete.disconnect(receiver, sender=Avatar)\n\n\n\n", "    def test_fast_delete_with_filter_and_update(self):\n        a = Avatar.objects.create(desc='a')\n        u = User.objects.create(avatar=a)\n        with self.assertNumQueries(2):\n            User.objects.filter(pk=u.pk).update(avatar=None)\n            a.delete()\n        \n        # Verify that the update query was executed and that avatar is NULL.\n        self.assertEqual(User.objects.get(pk=u.pk).avatar, None)\n        self.assertFalse(Avatar.objects.exists())\n\n", "    def test_fast_delete_foreign_key_with_null_update(self):\n        \"\"\"\n        #25932 - Fast deleting on backends that don't have the\n        `no_update_can_self_select` feature should work even when the foreign key\n        update to NULL is required\n        \"\"\"\n        user = User.objects.create(\n            avatar=Avatar.objects.create(desc=\"a desc\")\n        )\n        # We check that the avatar.desc is updated as expected.\n        self.assertEqual(Avatar.objects.get(pk=user.avatar_id).desc, \"a desc\")\n        user.delete()\n        # We check that the avatar is deleted.\n        self.assertFalse(Avatar.objects.filter(pk=user.avatar_id).exists())\n\n\n", "    def test_fast_delete_related_m2m_with_delete_related(self):\n        u = User.objects.create()\n        g = Group.objects.create()\n        m = M.objects.create()\n        u.m2m.add(m)\n        g.m2m.add(m)\n\n        # Related M2M object should be deleted together with the user on fast delete\n        with self.assertNumQueries(1):\n            u.delete(delete_related=True)\n        self.assertFalse(User.objects.filter(pk=u.pk).exists())\n        self.assertFalse(M.objects.filter(pk=m.pk).exists())\n        self.assertFalse(Group.objects.filter(pk=g.pk).exists())\n"], "sample_235": ["    def test_hooks_in_atomic_blocks(self):\n        with transaction.atomic():\n            with transaction.atomic():\n                transaction.on_commit(lambda: self.notify(1))\n            transaction.on_commit(lambda: self.notify(2))\n\n        self.assertDone([1, 2])\n\n\n\n", "    def test_hook_ordering_different_transactions(self):\n        with transaction.atomic():\n            transaction.on_commit(lambda: self.notify('transaction_1'))\n\n        with transaction.atomic():\n            transaction.on_commit(lambda: self.notify('transaction_2'))\n\n        self.assertDone(['transaction_1', 'transaction_2'])\n", "    def test_hook_called_even_with_explicit_rollback(self):\n        with transaction.atomic():\n            transaction.on_commit(lambda: self.notify('commit'))\n            transaction.rollback()\n        self.assertDone([], 'The transaction was rolled back, but the hook '\n                         'should still have been called')\n", "    def test_hook_cleanup_on_explicit_commit(self):\n        with transaction.atomic():\n            self.do(1)\n            transaction.commit()\n            transaction.on_commit(lambda: self.notify(10))\n        self.assertDone([1])\n", "    def test_hook_in_savepoint(self):\n            with transaction.atomic():\n                t = Thing.objects.create(num=i)\n                self.notify(t.num)\n\n        with transaction.atomic():\n            with transaction.atomic():\n                transaction.on_commit(lambda: on_commit(1))\n            transaction.on_commit(lambda: on_commit(2))\n\n        self.assertDone([1, 2])\n\n\n\n"], "sample_950": ["def test_python_python_use_unqualified_type_names_no_prefix(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"n\"><a class=\"reference internal\" href=\"#MyType\" title=\"MyType\">'\n            '<span class=\"pre\">MyType</span></a></span>' in content)\n", "def test_python_python_use_unqualified_type_names_enabled(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"n\"><a class=\"reference internal\" href=\"#Name\" title=\"Name\">'\n            '<span class=\"pre\">Name</span></a></span>' in content)\n    assert '<span class=\"n\"><span class=\"pre\">Age</span></span>' in content\n", "def test_python_python_use_unqualified_type_names_enabled(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"n\"><span class=\"pre\">Name</span></span>' in content)\n    assert ('<span class=\"n\"><span class=\"pre\">Age</span></span>' in content)\n", "def test_python_python_use_unqualified_type_names_enabled(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"n\"><span class=\"pre\">Name</span></span>' in content)\n    assert '<span class=\"n\"><span class=\"pre\">Age</span></span>' in content\n", "def test_noindexentry_with_object_name(app, status, warning):\n    text = (\".. py:function:: f()\\n\"\n            \"   :noindexentry:\\n\"\n            \".. py:function:: g()\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc, addnodes.index, desc))\n    assert_node(doctree[0], addnodes.index, entries=[('pair', 'built-in function; f()', 'f', '', None)])\n    assert_node(doctree[2], addnodes.index, entries=[('pair', 'built-in function; g()', 'g', '', None)])\n\n"], "sample_26": ["    def test_comphdu_invalid_compression(self):\n        with pytest.raises(ValueError, match=\"Unknown compression type\"):\n            fits.CompImageHDU(data=np.zeros(1), compression_type=\"NONEXISTENT\")\n", "    def test_comphdu_header_order(tmp_path):\n        \"\"\"\n        Regression test for https://github.com/astropy/astropy/issues/11776\n\n        Ensures that when writing a CompImageHDU, the order of keywords in\n        the header is preserved, including the order of Z-keywords.\n        \"\"\"\n        hdul = fits.HDUList()\n        data = np.ones((10, 10))\n        hdu = fits.CompImageHDU(\n            data,\n            header=fits.Header(\n                {\"EXTNAME\": \"my_ext\", \"ZBITPIX\": 16, \"ZNAXIS\": 2, \"A\": 1, \"B\": 2}\n            ),\n            compression_type=\"RICE_1\",\n        )\n        hdul.append(hdu)\n        hdul.writeto(tmp_path / \"test.fits\")\n\n        with fits.open(tmp_path / \"test.fits\") as hdul_read:\n            assert hdu_read[1].header[\"EXTNAME\"] == \"my_ext\"\n            assert hdu_read[1].header[\"ZBITPIX\"] == 16\n            assert hdu_read[1].header[\"ZNAXIS\"] == 2\n            assert hdu_read[1].header[\"A\"] == 1\n            assert hdu_read[1].header[\"B\"] == 2\n\n\n", "def test_comp_image_header():\n    with pytest.raises(ValueError, match=\"Invalid header keyword\"):\n        hdu = fits.CompImageHDU(np.zeros(10), header={\"BAD_KEYWORD\": 1})\n", "def test_comphdu_write_errors():\n    # Test for errors during writing.\n    # https://github.com/astropy/astropy/issues/809\n    # This is a simplified test, as CFITSIO is complex\n    # and there are many ways to potentially fail.\n\n    data = np.arange(9).reshape((3, 3))\n    hdu = fits.CompImageHDU(data, compression_type=\"RICE_1\")\n\n    # Test writing to a non-writable path\n    with pytest.raises(PermissionError):\n        hdu.writeto(\"/some/nonexistent/path/test.fits\", overwrite=True)\n\n    with pytest.raises(IOError):\n        # Try to write with no compression type\n        hdu._header.pop(\"ZCOMPR\")\n        hdu.writeto(self.temp(\"test.fits\"), overwrite=True)\n\n\n", "    def test_comphdu_data_copy(tmp_path):\n        # Ensure that writing a CompImageHDU with data=data.copy()\n        # does not result in a reference to the original data.\n        data = np.arange(10).reshape((2, 5))\n        hdu = fits.CompImageHDU(data=data.copy(), compression_type=\"RICE_1\")\n        hdu.writeto(tmp_path / \"test.fits\")\n\n        with fits.open(tmp_path / \"test.fits\") as hdul:\n            loaded_data = hdul[1].data\n\n        # Modify the original data and check that it does not affect\n        # the loaded data.\n        data[0, 0] = 99\n        assert_not_equal(loaded_data[0, 0], 99)\n"], "sample_771": ["def test_power_transformer_sparse_input(method):\n    # Check sparse matrix support\n    X = sparse.random(\n        data_density=0.2,\n        shape=(10, 5),\n        random_state=42\n    )\n    pt = PowerTransformer(method='box-cox', standardize=False)\n    pt.fit(X)\n    assert_raises(NotImplementedError, pt.transform, X) \n", "    def test_power_transformer_sparse(method, standardize):\n        # check that PowerTransformer works with sparse matrices\n        X = sparse.csr_matrix(np.abs(X_2d))\n        pt = PowerTransformer(method=method, standardize=standardize)\n        X_trans = pt.fit_transform(X)\n        assert isinstance(X_trans, sparse.csr_matrix)\n        X_inv = pt.inverse_transform(X_trans)\n        assert isinstance(X_inv, sparse.csr_matrix)\n\n\n", "def test_power_transformer_invalid_array_handle():\n    pt = PowerTransformer(method='box-cox')\n\n    # Test handling of invalid input types\n    for invalid_type in ([1], [True], [1, 2, 3, 4, 'a'], [dict]):\n        with pytest.raises(TypeError):\n            pt.fit(invalid_type)\n\n\n", "def test_power_transformer_sparse_matrix():\n    X = sparse.csr_matrix([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    for standardize in [True, False]:\n        pt = PowerTransformer(method='box-cox', standardize=standardize)\n        X_trans = pt.fit_transform(X)\n        assert isinstance(X_trans, sparse.csr_matrix)\n\n        pt = PowerTransformer(method='yeo-johnson', standardize=standardize)\n        X_trans = pt.fit_transform(X)\n        assert isinstance(X_trans, sparse.csr_matrix)\n", "def test_power_transformer_sparse_matrix():\n    # Test PowerTransformer with sparse matrices\n\n    X_sparse = sparse.csr_matrix([[1, 0, 2], [0, 3, 0], [4, 0, 0]])\n\n    for method in ['box-cox', 'yeo-johnson']:\n        pt = PowerTransformer(method=method)\n        pt.fit(X_sparse.toarray())\n        X_trans = pt.transform(X_sparse)\n        assert isinstance(X_trans, sparse.csr_matrix), X_trans\n\n        X_inv_trans = pt.inverse_transform(X_trans)\n        assert isinstance(X_inv_trans, sparse.csr_matrix), X_inv_trans\n\n        assert_array_almost_equal(X_sparse.toarray(), X_inv_trans.toarray())\n\n\n\n"], "sample_568": ["def test_surface3d_zsort_alpha():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n\n    x, y = np.mgrid[-2:2:0.1, -2:2:0.1]\n    z = np.sin(x)**2 + np.cos(y)**2\n    z[x.shape[0] // 2:, x.shape[1] // 2:] = np.inf\n\n    ax.plot_surface(x, y, z, cmap='jet', alpha=0.5)\n    ax.view_init(elev=45, azim=145)\n", "    def test_quiver3d_color():\n        fig = plt.figure()\n        ax = fig.add_subplot(projection='3d')\n\n        X, Y, Z = np.mgrid[-1:1:0.2, -1:1:0.2, -1:1:0.2]\n        U = X\n        V = Y\n        W = Z\n\n        scalar_colors = np.random.rand(X.size)\n\n        ax.quiver(X, Y, Z, U, V, W,\n                  color=scalar_colors,\n                  cmap='viridis')\n\n        ax.set_xlim([-1, 1])\n        ax.set_ylim([-1, 1])\n        ax.set_zlim([-1, 1])\n", "def test_add_collection_zdir():\n    fig_test = plt.figure()\n    ax_test = fig_test.add_subplot(projection='3d')\n    verts = np.array([[0, 0, 0], [1, 0, 0], [1, 1, 0], [0, 1, 0]])\n    faces = np.array([[0, 1, 2, 3]])\n    collection = art3d.Poly3DCollection(\n        [art3d.Path3D(verts, facecolors='red')],\n        facecolors='blue',\n        zdir='y'\n    )\n    ax_test.add_collection3d(collection)\n\n    fig_ref = plt.figure()\n    ax_ref = fig_ref.add_subplot(projection='3d')\n    verts = np.array([[0, 0, 0], [1, 0, 0], [1, 1, 0], [0, 1, 0]])\n    faces = np.array([[0, 1, 2, 3]])\n    collection = art3d.Poly3DCollection(\n        [art3d.Path3D(verts, facecolors='red')],\n        facecolors='blue',\n        zdir='y'\n    )\n    ax_ref.add_collection3d(collection)\n\n\n\n", "def test_surface3d_zsort_minus_inf():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n\n    x, y = np.mgrid[-2:2:0.1, -2:2:0.1]\n    z = np.sin(x)**2 + np.cos(y)**2\n    z[x.shape[0] // 2:, x.shape[1] // 2:] = -np.inf\n\n    ax.plot_surface(x, y, z, cmap='jet')\n    ax.view_init(elev=45, azim=145)\n", "def test_legend_bbox_outside():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n\n    ax.scatter([], [], [], label='Data 1')\n    ax.scatter([], [], [], label='Data 2')\n\n    legend = ax.legend(bbox_to_anchor=(1.1, 1.1), loc='upper right')\n    fig.canvas.draw()\n\n\n"], "sample_637": ["    def test_notes_rgx(self) -> None:\n        code = \"\"\"a = 1\n                # REM  \n                \"\"\"\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"fixme\", line=2, args=\"REM\", col_offset=17)\n        ):\n            self.checker.process_tokens(_tokenize_str(code))\n\n", "    def test_regex_codetag(self) -> None:\n        code = \"\"\"a = 1\n                # TODO\n                # FIXME \n                \"\"\"\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"fixme\", line=2, args=\"FIXME\", col_offset=17),\n            MessageTest(msg_id=\"fixme\", line=2, args=\"TODO\", col_offset=17),\n        ):\n            self.checker.process_tokens(_tokenize_str(code))\n", "    def test_regex_notes(self) -> None:\n        code = \"\"\"\n        # FIXME first\n        # XXX second\n        a = 1\n        # TODO third\n        \"\"\"\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"fixme\", line=1, args=\"FIXME first\", col_offset=17),\n            MessageTest(msg_id=\"fixme\", line=2, args=\"XXX second\", col_offset=17),\n            MessageTest(msg_id=\"fixme\", line=3, args=\"TODO third\", col_offset=17),\n        ):\n            self.checker.process_tokens(_tokenize_str(code))\n\n", "    def test_regex_codetag(self) -> None:\n        code = \"\"\"a = 1\n                # FIXmE\n                \"\"\"\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"fixme\", line=2, args=\"FIXmE\", col_offset=17)\n        ):\n            self.checker.process_tokens(_tokenize_str(code))\n", "    def test_notes_rgx(self) -> None:\n        code = \"\"\"a = 1\n                # REMARK\n                \"\"\"\n        self.config.notes_rgx = r\"REM|FIXME\"\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"fixme\", line=2, args=\"REMARK\", col_offset=17)\n        ):\n            self.checker.process_tokens(_tokenize_str(code))\n"], "sample_644": ["    def test_wildcard_import_in_list(self) -> None:\n        module = astroid.MANAGER.ast_from_module_name(\"wildcard_in_list\", REGR_DATA)\n        import_from = module.body[0]\n\n        msg = MessageTest(\n            msg_id=\"wildcard-import\",\n            node=import_from,\n            args=\"empty\",\n            confidence=UNDEFINED,\n            line=1,\n            col_offset=0,\n            end_line=1,\n            end_col_offset=19,\n        )\n        with self.assertAddsMessages(msg):\n            self.checker.visit_importfrom(import_from)\n\n", "    def test_ignored_import_failure(self) -> None:\n        module = astroid.MANAGER.ast_from_module_name(\"ignored_import\", REGR_DATA)\n        import_from = module.body[0]\n        with self.assertNoMessages():\n            self.checker.visit_importfrom(import_from)\n", "    def test_wrong_import_position(self) -> None:\n        module = astroid.MANAGER.ast_from_module_name(\"wrong_import_position\", REGR_DATA)\n        first_non_import = module.body[0]  \n        import_from = module.body[1]\n        msg = MessageTest(\n            msg_id=\"wrong-import-position\",\n            node=import_from,\n            args=(\n                \"standard import 'module_a.module_b'\",\n                \"first_non_import\",\n            ),\n            confidence=UNDEFINED,\n            line=2,\n            col_offset=0,\n            end_line=2,\n            end_col_offset=23,\n        )\n\n        with self.assertAddsMessages(msg):\n            self.checker.visit_importfrom(import_from)\n\n\n", "    def test_import_ordering_all_modules(capsys: CaptureFixture[str]) -> None:\n        Run(\n            [\n                f\"{os.path.join(REGR_DATA, 'import_ordering')}\",\n                \"-d all\",\n                \"-e wrong-import-order\",\n            ],\n            exit=False,\n        )\n        output, errors = capsys.readouterr()\n\n        assert len(output.split(\"\\n\")) == 5\n        assert errors == \"\"\n\n\n\n", "    def test_wildcard_import_all(self) -> None:\n        module = astroid.MANAGER.ast_from_module_name(\"wildcard_all\", REGR_DATA)\n        import_from = module.body[0]\n        self.checker._wildcard_import_is_allowed = lambda _: True\n        with self.assertNoMessages():\n            self.checker.visit_importfrom(import_from)\n"], "sample_741": ["def test_grid_search_pipeline_fit_with_missing_values():\n    X, y = make_classification(n_samples=50, random_state=42)\n    X[:, 1] = np.nan  # Introduce missing values\n\n    pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy='mean')),\n        ('classifier', LogisticRegression())\n    ])\n\n    param_grid = {'classifier__C': [0.1, 1, 10]}\n\n    grid_search = GridSearchCV(pipeline, param_grid, cv=5)\n    grid_search.fit(X, y)\n    assert_almost_equal(\n        grid_search.best_estimator_.score(X, y),\n        grid_search.best_score_\n    )\n\n    # Assert that the imputation step is called\n    assert hasattr(grid_search.best_estimator_, 'imputer_')\n\n\n\n", "compilation error", "def test_preprocessor_pipeline_grid_search():\n    from sklearn.pipeline import Pipeline\n    from sklearn.preprocessing import StandardScaler\n    \n\n    X = np.random.rand(100, 5)\n    y = np.random.randint(0, 2, size=100)\n\n    # Define a pipeline with a preprocessor and a classifier\n    preprocessor = StandardScaler()\n    \n    classifier = SVC(random_state=42)\n    pipeline = Pipeline([('preprocessor', preprocessor), ('classifier', classifier)])\n\n    param_grid = {'classifier__C': [0.1, 1, 10],\n                  'classifier__kernel': ['linear', 'rbf']}\n\n    grid_search = GridSearchCV(pipeline, param_grid, scoring='accuracy', cv=5)\n\n    # Fit the pipeline with the grid search\n    grid_search.fit(X, y)\n\n    # Check that the preprocessor was fitted\n    assert_true(grid_search.best_estimator_.named_steps['preprocessor'].fit_done_)\n\n\n", "def test_unknown_scorer_error():\n    X, y = make_classification(random_state=0)\n    estimator = LinearSVC()\n    scorer = 'unknown_scorer'\n    gs = GridSearchCV(estimator, {'C': [0.1, 1]}, scoring=scorer)\n    assert_raises(ValueError, gs.fit, X, y)\n", "def test_scoring_with_multiple_datasets():\n    X, y1 = make_classification(n_samples=100, random_state=0)\n    X, y2 = make_classification(n_samples=100, random_state=1)\n    X = np.concatenate([X, X], axis=0)\n\n        return accuracy_score(y_true, y_pred)\n\n    clf = SVC()\n    gs = GridSearchCV(clf, param_grid={'C': [0.1, 1, 10]},\n                      scoring=custom_scoring, cv=2)\n    gs.fit(X, [y1, y2])\n\n\n"], "sample_358": ["    def test_repr_and_str(self):\n        statement = Statement(\n            template=\"%(reference)s - %(non_reference)s\",\n            reference=MockReference('reference', {'table'}, {('table', 'column')}),\n            non_reference='non_reference',\n        )\n        self.assertEqual(repr(statement), \"<Statement 'reference - non_reference'>\")\n        self.assertEqual(str(statement), 'reference - non_reference')\n\n\n\n\n\n", "    def test_str_with_quoted_suffix(self):\n        compiler = Person.objects.all().query.get_compiler(connection.alias)\n        expressions = Expressions(\n            table=Person._meta.db_table,\n            expressions=ExpressionList(\n                IndexExpression(F('first_name')),\n                IndexExpression(F('last_name').desc()),\n            ).resolve_expression(compiler.query),\n            compiler=compiler,\n            quote_value=self.editor.quote_value,\n            col_suffixes=[\n                self.editor.quote_name(\"suffix\"),\n            ],\n        )\n        self.assertIn(\n            '{} AS first_name{}'.format(\n                self.editor.quote_name('first_name'),\n                self.editor.quote_name(\"suffix\"),\n            ),\n            str(expressions),\n        )\n        self.assertIn(\n            '{} AS last_name{}'.format(\n                self.editor.quote_name('last_name'),\n                self.editor.quote_name(\"suffix\"),\n            ),\n            str(expressions),\n        )\n", "    def test_str_with_col_suffixes(self):\n        compiler = Person.objects.all().query.get_compiler(connection.alias)\n        expressions = Expressions(\n            table=Person._meta.db_table,\n            expressions=ExpressionList(\n                IndexExpression(F('first_name').lower() + '_suffix'),\n                IndexExpression(F('last_name')),\n            ).resolve_expression(compiler.query),\n            compiler=compiler,\n            quote_value=self.editor.quote_value,\n            col_suffixes=['lower'],\n        )\n\n        self.assertEqual(\n            str(expressions),\n            '(LOWER(%s)lower), %s' % (\n                self.editor.quote_name('first_name'),\n                self.editor.quote_name('last_name'),\n            ),\n        )\n", "    def test_repr(self):\n        self.assertEqual(repr(self.expressions), \"<Expressions 'UPPER(last_name), first_name'>\")\n\n\n\n", "    def test_str(self):\n        self.assertIn(\n            '%s.%s' % (\n                self.editor.quote_name(Person._meta.db_table),\n                self.editor.quote_name('first_name'),\n            ),\n            str(self.expressions),\n        )\n        self.assertIn(\n            '%s.%s' % (\n                self.editor.quote_name(Person._meta.db_table),\n                self.editor.quote_name('last_name'),\n            ),\n            str(self.expressions),\n        )\n        self.assertIn(\n            '%s.%s' % (\n                self.editor.quote_name(Person._meta.db_table),\n                self.editor.quote_name('last_name'),\n            ),\n            str(self.expressions),\n        )\n\n"], "sample_144": ["    def test_m2m_related_queries_with_parent_pk(self):\n        r = Restaurant.objects.create(name='Guido\\'s House of Pasta')\n        s = Supplier.objects.create(name='John', restaurant=r)\n        with self.assertNumQueries(1):\n            supplier_set = r.supplier_set.all()\n            self.assertEqual(supplier_set.count(), 1)\n            self.assertEqual(supplier_set[0].name, 'John')\n        with self.assertNumQueries(1):\n            supps = Supplier.objects.filter(restaurant_ptr=r)\n            self.assertEqual(supps.count(), 1)\n            self.assertEqual(supps[0].name, 'John')\n", "    def test_field_on_both_models(self):\n        \"\"\"\n        Regression test for #9518: fields on both parent and child models should be properly accessible\n        \"\"\"\n        class WithName(models.Model):\n            name = models.CharField(max_length=100)\n\n        class Child(WithAName):\n            pass\n\n        withName = WithName.objects.create(name='Parent')\n        child = Child.objects.create(name='Child')\n\n        self.assertEqual(withName.name, 'Parent')\n        self.assertEqual(child.name, 'Child')\n\n        self.assertEqual(child.name, withName.name)\n", "    def test_m2m_related_name_remapping(self):\n        # Regression test for #27239: When a model has a ManyToManyField,\n        # ensure that related_name is remapped correctly, even if the\n        # intermediate model is abstract.\n\n        article_with_author = ArticleWithAuthor.objects.create(\n            headline=\"My Article\", author=\"John Doe\"\n        )\n        with self.assertNumQueries(2):\n            related_articles = article_with_author.article_set.all()\n\n        self.assertEqual(related_articles.count(), 1)\n\n\n\n", "    def test_related_manager_with_child_and_parent_fields(self):\n        # Regression test for #19327 - related managers should work with\n        # models that have both parent and child fields.\n\n        parent = Place.objects.create(name='Parent Place', address='Parent Address')\n        child = Child.objects.create(parent_ptr=parent, name='Child Name')\n\n        # Check that the child's related manager works correctly\n        self.assertEqual(child.parent.name, 'Parent Place')\n\n        # Check that the parent's related manager works correctly\n        self.assertEqual(parent.children.count(), 1)\n        self.assertEqual(parent.children.first().name, 'Child Name')\n", "    def test_create_new_instance_with_pk_equals_none_from_foreign_key(self):\n        p1 = Profile.objects.create(username='john')\n        user = User.objects.get(pk=p1.user_ptr_id)\n        # Create a new profile for a user by providing None for pk and user_ptr\n        p2 = Profile(pk=None, user_ptr=None, username='bill')\n        p2.full_clean()\n        p2.save()\n        self.assertEqual(Profile.objects.count(), 2)\n        self.assertEqual(User.objects.get(pk=p1.user_ptr_id).username, 'john')\n        self.assertEqual(User.objects.get(pk=p2.user_ptr_id).username, 'bill')\n\n\n\n"], "sample_171": ["    def test_squashmigrations_app_name_specified_as_label(self):\n        with self.assertRaisesMessage(CommandError, self.did_you_mean_auth_error):\n            call_command('squashmigrations', 'django.contrib.auth', '0002')\n", "    def test_squashmigrations_app_name_specified_as_label(self):\n        with self.assertRaisesMessage(CommandError, self.did_you_mean_auth_error):\n            call_command('squashmigrations', 'django.contrib.auth', '0002')\n", "    def test_squashmigrations_app_name_specified_as_label(self):\n        with self.assertRaisesMessage(CommandError, self.did_you_mean_auth_error):\n            call_command('squashmigrations', 'django.contrib.auth', '0002')\n", "    def test_squashmigrations_app_name_specified_as_label(self):\n        with self.assertRaisesMessage(CommandError, self.did_you_mean_auth_error):\n            call_command('squashmigrations', 'django.contrib.auth', '0002')\n", "    def test_squashmigrations_app_name_specified_as_label(self):\n        with self.assertRaisesMessage(CommandError, self.did_you_mean_auth_error):\n            call_command('squashmigrations', 'django.contrib.auth', '0002')   \n"], "sample_572": ["    def test_percentile_errorbars(self, long_df):\n\n        agg = EstimateAggregator(\"mean\", \"percentile\", low=25, high=75)\n        out = agg(long_df, \"y\")\n        assert out[\"ymin\"] == np.percentile(long_df[\"y\"], 25)\n        assert out[\"ymax\"] == np.percentile(long_df[\"y\"], 75)\n\n        agg = EstimateAggregator(\"mean\", (\"percentile\", 10, 90))\n        out = agg(long_df, \"y\")\n        assert out[\"ymin\"] == np.percentile(long_df[\"y\"], 10)\n        assert out[\"ymax\"] == np.percentile(long_df[\"y\"], 90)\n", "    def test_empty_dataframe(self):\n\n        agg = EstimateAggregator(\"mean\", \"ci\")\n        out = agg(pd.DataFrame(), \"y\")\n        assert pd.isna(out[\"y\"])\n        assert pd.isna(out[\"ymin\"])\n        assert pd.isna(out[\"ymax\"])\n", "    def test_bootstrapped_errorbars_with_weights(self, long_df):\n\n        agg = EstimateAggregator(\"mean\", \"ci\", n_boot=100000, seed=0, weights='w')\n        out = agg(long_df, \"y\", weights=long_df[\"w\"])\n        \n        agg_ref = EstimateAggregator(\"mean\", (\"se\", 1.96))\n        out_ref = agg_ref(long_df, \"y\", weights=long_df[\"w\"])\n\n        assert out[\"ymin\"] == pytest.approx(out_ref[\"ymin\"], abs=1e-2)\n        assert out[\"ymax\"] == pytest.approx(out_ref[\"ymax\"], abs=1e-2)\n", "    def test_nan_handling(self, long_df):\n\n        long_df[\"y\"] = long_df[\"y\"].astype(\"float\")\n        long_df[\"y\"].iloc[5] = np.nan\n        agg = EstimateAggregator(\"mean\")\n        out = agg(long_df, \"y\")\n        assert pd.isna(out[\"y\"]) \n\n        agg = EstimateAggregator(\"mean\", \"se\")\n        out = agg(long_df, \"y\")\n        assert pd.isna(out[\"y\"])\n        assert pd.isna(out[\"xmin\"])\n        assert pd.isna(out[\"xmax\"])\n\n        agg = EstimateAggregator(\"mean\", \"pi\")\n        out = agg(long_df, \"y\")\n        assert pd.isna(out[\"ymin\"])\n        assert pd.isna(out[\"ymax\"])\n\n\n", "    def test_multi_column_errorbars(self, long_df):\n\n        agg = EstimateAggregator(\"mean\", \"se\")\n        out = agg(long_df, [\"x\", \"y\"])\n        assert isinstance(out, pd.DataFrame)\n        assert set(out.columns) == {\"x\", \"xmin\", \"xmax\", \"y\", \"ymin\", \"ymax\"}\n\n\n\n"], "sample_791": ["def test_one_hot_encoder_categorical_features_index():\n    X = np.array([['abc', 2, 55], ['def', 1, 55], ['ghi', 3, 59]])\n    enc = OneHotEncoder(categorical_features=[0, 2], sparse=False)\n    X_tr = enc.fit_transform(X)\n    exp = np.array([\n        [1., 0., 0., 1., 0., 0.],\n        [0., 1., 0., 0., 1., 0.],\n        [0., 0., 1., 0., 0., 1.]\n    ])\n    assert_array_equal(X_tr, exp)\n\n    # Test with dtype\n    enc = OneHotEncoder(categorical_features=[0, 2], dtype=np.int32)\n    X_tr = enc.fit_transform(X)\n    assert_array_equal(X_tr.dtype, np.int32)\n\n    # Test with categorical_features out of range\n    with pytest.raises(ValueError, match=\"Index out of bounds\"):\n        enc = OneHotEncoder(categorical_features=[3, 4])\n        enc.fit_transform(X)\n\n    # Test with categorical_features None\n    enc = OneHotEncoder(categorical_features=None)\n    assert_array_equal(enc.fit_transform(X).toarray(),\n                       np.array(X.copy()))\n\n", "def test_one_hot_encoder_sparse_output_without_fit():\n    enc = OneHotEncoder(sparse=True)\n    with pytest.raises(ValueError, match=\"This OneHotEncoder instance is not fitted yet.\"):\n        enc.transform(np.array([[1, 2]]).T)\n", "compilation error", "def test_one_hot_encoder_sparse_output():\n    X = np.array([['abc', 2, 55], ['def', 1, 55], ['def', 3, 55]])\n    enc = OneHotEncoder(sparse=True)\n    Xtr = enc.fit_transform(X)\n    assert isinstance(Xtr, sparse.csr_matrix) or isinstance(Xtr, sparse.csc_matrix)\n\n    # test inverse_transform with sparse output\n    X_inverse = enc.inverse_transform(Xtr)\n    assert_array_equal(X_inverse, X)\n\n    # test fit_transform with sparse output\n    Xtr2 = enc.fit_transform(X)\n    assert isinstance(Xtr2, sparse.csr_matrix) or isinstance(Xtr2, sparse.csc_matrix)\n\n    assert_array_equal(Xtr.toarray(), Xtr2.toarray())\n", "def test_one_hot_encoder_sparse_output():\n    enc = OneHotEncoder(sparse=True)\n    X = [['abc', 2, 55], ['def', 1, 55]]\n    X_sparse = enc.fit_transform(X)\n    assert isinstance(X_sparse, sparse.csr_matrix)\n    assert_array_equal(X_sparse.toarray(),\n                       np.array([[1., 0., 1., 0.], [0., 1., 0., 1.]]))\n\n"], "sample_799": ["def test_cross_val_score_estimator_with_no_fit_method():\n    class NoFitEstimator:\n            return np.zeros_like(X)\n    X, y = make_classification(n_samples=10, random_state=0)\n    estimator = NoFitEstimator()\n    scores = cross_val_score(estimator, X, y, cv=3)\n    assert_array_almost_equal(scores, np.zeros(3))\n\n\n\n", "    def non_numeric_scorer(estimator, X_test):\n        return 'abc'", "    def check_scoring_array_output(X, y, estimator, scoring):\n        y_pred = estimator.predict(X)\n        result = _score(estimator, X, y, scoring=scoring)\n        assert isinstance(result, float)\n\n\n", "def test_scorer_with_extra_arguments():\n    scorer = lambda estimator, X, y, **kwargs: 42\n\n    X, y = make_classification(n_samples=30, random_state=0)\n    clf = SVC(kernel=\"linear\", random_state=0)\n    train, test = next(ShuffleSplit().split(X))\n    assert_equal(_score(clf, X, y, scorer), 42)\n\n\n\n", "def test_fit_and_score_error_score_with_invalid_scoring():\n    X, y = make_classification(n_samples=30, random_state=0)\n    clf = SVC(kernel=\"linear\", random_state=0)\n    train, test = next(ShuffleSplit().split(X))\n    \n    invalid_scoring = 'invalid_scoring'\n    \n    fit_and_score_args = [clf, X, y, invalid_scoring, train, test, 0]\n    fit_and_score_kwargs = {'error_score': 0}\n    assert_raise_message(ValueError,\n                         \"The 'error_score' parameter is only valid \"\n                         \"when scoring is a callable.\", _fit_and_score,\n                         *fit_and_score_args, **fit_and_score_kwargs)\n"], "sample_825": ["def test_pls_with_missing_values():\n    from sklearn.utils import check_random_state\n    rng = check_random_state(42)\n    \n    # Create a dataset with some missing values\n    n_samples = 100\n    n_features = 10\n    X = rng.rand(n_samples, n_features)\n    Y = rng.rand(n_samples, 1)\n    mask = rng.randint(0, 2, size=(n_samples, n_features))\n    X[mask == 0] = np.nan\n\n    # Test with various PLS variants\n    for cls in [pls_.PLSCanonical, pls_.PLSRegression, pls_.PLSSVD]:\n        clf = cls()\n        with pytest.raises(ValueError):\n            clf.fit(X, Y) \n\n        # Test with imputations\n        for imputation_method in ['mean', 'median', 'most_frequent']:\n            X_imputed = X.copy()\n            for col in range(n_features):\n                X_imputed[:, col] = X_imputed[:, col].fillna(\n                    getattr(X[:, col], imputation_method)())\n            clf.fit(X_imputed, Y)\n", "compilation error", "def test_pls_handling_different_input_shapes():\n    for n_components in [1, 2, 3]:\n        for clf in [pls_.PLSCanonical(), pls_.PLSRegression(), pls_.PLSSVD()]:\n            # Test with X and Y having different shapes \n            X_shape1 = (5, 4)\n            Y_shape1 = (5, 2) \n            X1 = np.random.rand(*X_shape1)\n            Y1 = np.random.rand(*Y_shape1)\n            clf.n_components = n_components\n            assert_raise_message(ValueError, \"Input dimensions mismatch\",\n                                 clf.fit, X1, Y1)\n\n            # Test with X and Y having compatible shapes \n            X_shape2 = (5, 4)\n            Y_shape2 = (5, 4)\n            X2 = np.random.rand(*X_shape2)\n            Y2 = np.random.rand(*Y_shape2)\n            clf.n_components = n_components\n            clf.fit(X2, Y2)\n", "def test_pls_fit_transform():\n    # test that fit_transform returns consistent results\n    d = load_linnerud()\n    X = d.data\n    Y = d.target\n\n    n_components = 2\n    clf = pls_.PLSCanonical(n_components=n_components)\n    X_score, Y_score = clf.fit_transform(X, Y)\n    clf.fit(X, Y)\n    X_score_2, Y_score_2 = clf.transform(X)\n    assert_array_equal(X_score, X_score_2)\n    assert_array_equal(Y_score, Y_score_2)\n\n\n", "def test_pls_with_dummy_y():\n    d = load_linnerud()\n    X = d.data\n    # Create dummy Y with shape (n_samples, 0)\n    dummy_Y = np.array([]) \n\n    for clf in [pls_.PLSCanonical(), pls_.PLSRegression(),\n                pls_.PLSSVD()]:\n        assert_raise_message(ValueError, \"Y must have at least one column\",\n                             clf.fit, X, dummy_Y)\n"], "sample_32": ["    def test_de_density_scale(self, cosmo_cls, cosmo):\n        \"\"\"Test :meth:`astropy.cosmology.w0wzCDM.de_density_scale`.\"\"\"\n\n        z = np.linspace(0, 10, 10)\n        scale_factor = cosmo.de_density_scale(z)\n\n        # Check that the scale factor is always positive\n        assert np.all(scale_factor > 0)\n", "    def test_de_density_scale(self, cosmo, z, expected_de_density_scale):\n        \"\"\"Test :meth:`astropy.cosmology.w0wzCDM.de_density_scale`.\"\"\"\n        assert u.allclose(\n            cosmo.de_density_scale(z), expected_de_density_scale\n        )\n", "    def test_de_density_scale(self, cosmo):\n        \"\"\"Test :meth:`astropy.cosmology.w0wzCDM.de_density_scale`.\"\"\"\n        z_values = np.array([0.0, 0.5, 1.0, 2.0]) * u.dimensionless_unscaled\n        I_values = cosmo.de_density_scale(z_values)\n        \n        # Check that the results are within the expected range\n        assert np.all(I_values >= 1)  \n\n        # Check that the results agree with expected behavior for z=0\n        assert np.isclose(I_values[0], 1.0) \n\n        # TODO: More sophisticated tests with different cosmological parameters are needed\n", "    def test_methods_return_units(self, cosmo_cls, method, z, expected_unit):\n        \"\"\"Test that methods return with the expected units.\"\"\"\n        cosmo = cosmo_cls(H0=70, Om0=0.3, w0=-1.0, wz=0.2)\n        getattr(cosmo, method)(z)\n        assert getattr(cosmo, method)(z).unit == expected_unit\n\n", "    def test_comoving_distance_Tcmb0(self, cosmo_cls, args, kwargs, expected):\n        \"\"\"Test :meth:`astropy.cosmology.w0wzCDM.comoving_distance` with different Tcmb0 values.\"\"\"\n        super().test_comoving_distance_example(\n            cosmo_cls, args, {**COMOVING_DISTANCE_EXAMPLE_KWARGS, **kwargs}, expected\n        )\n"], "sample_1018": ["compilation error", "def test_fcode_Matrix_Assignment_with_index():\n    m = MatrixSymbol('m', 2, 2)\n    i, j = symbols('i j', integer=True)\n    expr = m[i, j] + 1\n    assert fcode(expr, assign_to=m[i, j], source_format='free') == 'm(i, j) = m(i, j) + 1'\n", "def test_fcode_Matrix_multiplication():\n    x, y, z = symbols('x,y,z')\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n    expr = A * B\n    assert fcode(expr, C) == (\n        \"      C(1, 1) = A(1, 1)*B(1, 1) + A(1, 2)*B(2, 1)\\n\"\n        \"      C(1, 2) = A(1, 1)*B(1, 2) + A(1, 2)*B(2, 2)\\n\"\n        \"      C(2, 1) = A(2, 1)*B(1, 1) + A(2, 2)*B(2, 1)\\n\"\n        \"      C(2, 2) = A(2, 1)*B(1, 2) + A(2, 2)*B(2, 2)\")\n", "compilation error", "compilation error"], "sample_401": ["    def test_invalid(self):\n        data = {\n            \"choices-TOTAL_FORMS\": \"2\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"Zero\",\n            \"choices-0-votes\": \"\",\n            \"choices-1-choice\": \"One\",\n            \"choices-1-votes\": \"1\",\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [{'votes': [\"This field is required.\"] * 2}, {}]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n", "    def test_invalid(self):\n        data = {\n            \"choices-TOTAL_FORMS\": \"2\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"\",\n            \"choices-0-votes\": \"0\",\n            \"choices-1-choice\": \"One\",\n            \"choices-1-votes\": \"\",\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        self.assertIs(all_valid((formset1, formset1)), False)\n        expected_errors = [{\"choice\": [\"This field is required.\"] }, {\"votes\": [\"This field is required.\"]}]\n        self.assertEqual(formset1._errors, expected_errors)\n", "    def test_partially_valid(self):\n        data = {\n            \"choices-TOTAL_FORMS\": \"2\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"\",\n            \"choices-0-votes\": \"0\",\n            \"choices-1-choice\": \"One\",\n            \"choices-1-votes\": \"1\",\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        self.assertIs(all_valid((formset1, formset1)), False)\n        self.assertEqual(formset1._errors, [{\"choice\": [\"This field is required.\"]}, {}])\n\n\n\n", "    def test_invalid(self):\n        data = {\n            \"choices-TOTAL_FORMS\": \"2\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"Zero\",\n            \"choices-0-votes\": \"\",\n            \"choices-1-choice\": \"One\",\n            \"choices-1-votes\": \"1\",\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [\n            {\"choices-0-votes\": [\"This field is required.\"] },\n            {},\n        ]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n\n\n\n", "    def test_part_valid_part_invalid(self):\n        data = {\n            \"choices-TOTAL_FORMS\": \"2\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"Zero\",\n            \"choices-0-votes\": \"\",\n            \"choices-1-choice\": \"One\",\n            \"choices-1-votes\": \"1\",\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        self.assertIs(all_valid((formset,)), False)\n        self.assertEqual(formset._errors, [{\"votes\": [\"This field is required.\"] }, {}])\n\n\n\n"], "sample_373": ["    def test_docstring_rendering(self):\n        class PersonWithDocstring(Person):\n            class Meta:\n                app_label = 'admin_docs'\n                verbose_name = 'Person'\n                verbose_name_plural = 'People'\n\n                \"\"\"\n                Returns the full name of the person.\n                \"\"\"\n                return f\"{self.first_name} {self.last_name}\"\n\n        PersonWithDocstring.objects.create(first_name=\"Doc\", last_name=\"String\")\n        response = self.client.get(reverse('django-admindocs-models-detail', args=['admin_docs', 'PersonWithDocstring']))\n        self.assertContains(response, 'Returns the full name of the person.')\n", "    def test_model_detail_with_related_fields(self):\n        response = self.client.get(reverse('django-admindocs-models-detail', args=['admin_docs', 'person']))\n        self.assertContains(response, \"<td>company</td>\")\n        self.assertContains(response, \"<td>company_id</td>\")\n\n\n\n", "    def test_field_with_description(self):\n        class FieldWithDescription(models.CharField):\n            description = \"A description for this field\"\n        field = FieldWithDescription(max_length=100)\n        self.assertEqual(views.get_readable_field_data_type(field), 'A description for this field')\n", "    def test_field_descriptions_are_handled(self):\n        class MyModel(models.Model):\n            name = models.CharField(max_length=100, description='A descriptive name')\n\n        self.assertEqual(views.get_readable_field_data_type(MyModel.name), 'A descriptive name')\n", "    def test_method_arguments(self):\n        response = self.client.get(reverse('django-admindocs-models-detail', args=['admin_docs', 'Person']))\n        self.assertContains(response, '<td>suffix_company_name(self, suffix)</td>')\n        self.assertContains(response, '<td>dummy_function(self, arg1, arg2)</td>')\n        self.assertContains(response, '<td>rename_company(self, new_name)</td>')\n"], "sample_185": ["    def test_translation_file_changed_activation(self):\n        gettext_module._translations = {'foo': 'bar'}\n        trans_real._translations = {'foo': 'bar'}\n        with mock.patch('django.utils.translation.activate') as activate_mock:\n            path = Path('test.mo')\n            translation_file_changed(None, path)\n            activate_mock.assert_called_once()\n", "    def test_resets_cache_with_mo_files_multiple_calls(self):\n        gettext_module._translations = {'foo': 'bar'}\n        trans_real._translations = {'foo': 'bar'}\n        trans_real._default = 1\n        trans_real._active = False\n        path = Path('test.mo')\n        translation_file_changed(None, path)\n        translation_file_changed(None, path)\n        self.assertEqual(gettext_module._translations, {})\n        self.assertEqual(trans_real._translations, {})\n        self.assertIsNone(trans_real._default)\n        self.assertIsInstance(trans_real._active, Local)\n\n", "    def test_doesnt_reset_cache_with_existing_mo_files(self):\n        gettext_module._translations = {'foo': 'bar'}\n        trans_real._translations = {'foo': 'bar'}\n        path = Path('test.mo')\n        translation_file_changed(None, path)\n        self.assertEqual(gettext_module._translations, {'foo': 'bar'})\n        self.assertEqual(trans_real._translations, {'foo': 'bar'})\n", "    def test_does_not_reset_cache_with_non_mo_files(self):\n        gettext_module._translations = {'foo': 'bar'}\n        path = Path('test.py')\n        self.assertIsNone(translation_file_changed(None, path))\n        self.assertEqual(gettext_module._translations, {'foo': 'bar'})\n\n", "    def test_translation_file_not_changed(self):\n        gettext_module._translations = {'foo': 'bar'}\n        trans_real._translations = {'foo': 'bar'}\n        with mock.patch('os.path.exists') as mock_exists:\n            mock_exists.return_value = True\n            self.assertIs(translation_file_changed(None, Path('test.mo')), False)\n\n\n"], "sample_909": ["    def test_unicode_characters(self):\n        docstring = \"\"\"", "    def test_complex_ref(self):\n        docstring = \"\"\"", "    def test_restructuredtext_in_docstring(self):\n        docstring = \"\"\"\n        This is a function.\n\n        It takes some input, does some processing, and then returns the result.\n\n        Here's a breakdown of what the code does:\n\n        * **Step 1:**  It calculates the square of the input.\n        * **Step 2:** It adds 1 to the result.\n        * **Step 3:** It returns the final value.\n\n        .. code-block:: python\n                return x**2 + 1\n        \"\"\"\n\n        expected = \"\"\"\n        This is a function.\n\n        It takes some input, does some processing, and then returns the result.\n\n        Here's a breakdown of what the code does:\n\n        * Step 1:  It calculates the square of the input.\n        * Step 2: It adds 1 to the result.\n        * Step 3: It returns the final value.\n\n        .. code-block:: python\n                return x**2 + 1\n        \"\"\"\n        config = Config()\n        app = mock.Mock()\n        actual = str(NumpyDocstring(docstring, config, app, \"function\"))\n        self.assertEqual(expected, actual)\n", "    def test_reStructuredText(self):\n        docstring = \"\"\"", "    def test_restructuredtext_in_parameters(self):\n        docstring = \"\"\""], "sample_854": ["def test_svm_multiclass_fit_intercept_ovr():\n    # Test that fit_intercept works correctly with OneVsRest in multi-class\n    # OVR model\n\n    X, y = make_classification(n_samples=100, n_features=10,\n                               n_informative=5,\n                               n_classes=3, random_state=0)\n\n    for clf in [svm.SVC(kernel='linear', decision_function_shape='ovr',\n                         random_state=0)\n                for _ in range(2)]:\n        # Fit with and without intercept\n        clf1 = clf.copy().fit(X, y)\n        clf2 = clf.copy().set_params(fit_intercept=False).fit(X, y)\n\n        # Check classes and intercepts are consistent\n        assert set(clf1.classes_) == set(clf2.classes_)\n        assert clf1.intercept_.shape == clf2.intercept_.shape\n        assert_array_almost_equal(clf1.intercept_, clf2.intercept_)\n\n        # Check predictions are equal\n        assert (clf1.predict(X) == clf2.predict(X)).all()\n\n\n\n", "def test_dual_coef_is_none_for_non_dual_svm():\n    for estimator in [svm.SVC, svm.NuSVC]:\n        clf = estimator(kernel='linear', dual=False)\n        clf.fit(X, y)\n        assert clf.dual_coef_ is None\n\n\n\n", "def test_svc_probability_with_sparse_kernel():\n    X_sparse = sparse.csr_matrix([[1, 0], [0, 1], [1, 1]])\n    y = [0, 1, 1]\n\n    clf = svm.SVC(kernel='linear', probability=True, random_state=0)\n    clf.fit(X_sparse, y)\n    assert clf.predict_proba(X_sparse).shape == (3, 2)\n", "def test_precomputed_kernel():\n    from sklearn.metrics import accuracy_score\n\n    # Test with a precomputed kernel\n    X = np.array([[0, 1], [1, 1], [0, 0], [1, 0]])\n    y = np.array([0, 0, 1, 1])\n    kernel = np.dot(X, X.T)\n    clf = svm.SVC(kernel='precomputed')\n    clf.fit(kernel, y)\n    assert_array_equal(clf.predict(kernel), y)\n    # Test accuracy with precomputed kernel\n    X_test = np.array([[0.5, 0.5], [0.7, 0.3]])\n    kernel_test = np.dot(X_test, X.T)\n    y_pred = clf.predict(kernel_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    assert accuracy == 1.0\n", "def test_param_validation_with_precomputed_kernel(kernel):\n    X = np.random.random((100, 10))\n    if kernel not in ['precomputed']:\n        y = np.random.randint(0, 2, size=100)\n    else:\n        y = np.random.randint(0, 2, size=100)\n    with pytest.raises(ValueError):\n        svm.SVC(kernel=kernel, gamma='auto').fit(X, y)\n\n"], "sample_9": ["def test_write_table_html_fill_values_masked_multicol():\n    \"\"\"\n    Test that passing masked values in fill_values should only replace\n    masked columns or values for multidimensional tables\n    \"\"\"\n    buffer_output = StringIO()\n    t = Table([[1, 2], [np.nan, 'a']], names=('a', 'b'), masked=True)\n    t['a'] = np.ma.masked\n    ascii.write(t, buffer_output, fill_values=('MASKED', \n                                                'TEST'),\n                format='html')\n\n    t_expected = Table([[1, 'TEST'], ['MASKED', 'TEST']], names=('a', 'b'))\n    buffer_expected = StringIO()\n    ascii.write(t_expected, buffer_expected, format='html')\n\n    assert buffer_output.getvalue() == buffer_expected.getvalue()\n", "def test_write_table_html_preserve_null_values():\n    \"\"\"\n    Test that writing HTML preserves null values.\n    \"\"\"\n    t = Table([['a', None], ['b', 123]], names=('col1', 'col2'))\n    buffer_output = StringIO()\n    ascii.write(t, buffer_output, format='html')\n    buffer_expected = StringIO()\n    ascii.write(t, buffer_expected, format='html')\n\n    assert buffer_output.getvalue() == buffer_expected.getvalue()\n\n", "def test_write_table_html_float_formats():\n    \"\"\"\n    Test to make sure that the HTML writer handles float formatting correctly.\n    \"\"\"\n    col1 = [1.23456789, 2.34567890, 3.45678901]\n    col2 = ['1.23e+01', '-2.34e+02']\n    table = Table([col1, col2], names=('C1', 'C2'))\n\n    # Test various float formats\n    formats = {\"C1\": \".3f\", \"C2\": \"e\"}\n\n    with StringIO() as sp:\n        table.write(sp, format=\"html\", formats=formats)\n        out = sp.getvalue().strip()\n\n    expected = \"\"\"\\", "def test_read_html_with_header_row():\n    \"\"\"\n    Test that the HTML reader correctly identifies and uses the first row\n    as a header row.\n    \"\"\"\n    html_content = \"\"\"\n    <table>\n     <tr>\n      <th>Column 1</th>\n      <th>Column 2</th>\n      <th>Column 3</th>\n     </tr>\n     <tr>\n      <td>1</td>\n      <td>2</td>\n      <td>3</td>\n     </tr>\n    </table>\n    \"\"\"\n    table = Table.read(html_content, format='html')\n    assert table.colnames == ['Column 1', 'Column 2', 'Column 3']\n    assert np.all(table['Column 1'] == [1])\n\n\n\n", "    def test_write_table_html_with_unicode():\n        \"\"\"\n        Test to make sure that the HTML writer can handle Unicode data\n        correctly.\n        \"\"\"\n        buffer_output = StringIO()\n        t = Table([[u'\u00c9\u00e0\u00e7\u00fc'], [u'\u00de\u00f8\u00fc\u00c9']], names=('a', 'b'))\n        ascii.write(t, buffer_output, format='html')\n\n        t_expected = Table([[u'\u00c9\u00e0\u00e7\u00fc'], [u'\u00de\u00f8\u00fc\u00c9']], names=('a', 'b'))\n        buffer_expected = StringIO()\n        ascii.write(t_expected, buffer_expected, format='html')\n\n        assert buffer_output.getvalue() == buffer_expected.getvalue()\n"], "sample_1128": ["def test_point_vel_with_changing_frame_velocities():\n    q1, q2 = dynamicsymbols('q1 q2')\n    t = dynamicsymbols._t\n    N = ReferenceFrame('N')\n    B = N.orientnew('B', 'Axis', [q1, N.z])\n    O = Point('O')\n    P = O.locatenew('P', 10 * B.x)\n    O.set_vel(N, q2 * N.y)\n    P.set_vel(B, q1 * B.x)\n    assert P.vel(N) == (q1 * B.x).express(N) + O.vel(N)\n    assert P.vel(B) == q1 * B.x\n\n    B.set_ang_vel(N, q2 * B.y)\n    assert P.vel(N) == (q1 * B.x).express(N) + O.vel(N) + (O.vel(N) ^ P.pos_from(O))\n\n", "def test_point_acc():\n    q1, q2 = dynamicsymbols('q1 q2')\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    O = Point('O')\n    P = O.locatenew('P', q1 * B.x)\n    O.set_vel(N, q2 * N.y)\n    P.set_vel(B, q1 * B.x)\n    assert P.acc(N) == (-q2**2) * B.x + (2 * q1 * q2) * B.y + q1 * B.z\n", "def test_point_vel_multiple_stages():\n    q1, q2, q3 = dynamicsymbols('q1 q2 q3')\n    t = dynamicsymbols._t\n    N = ReferenceFrame('N')\n    P = Point('P')\n    P.set_vel(N, q1 * N.x)\n    P1 = Point('P1')\n    P1.set_pos(P, q2 * N.y)\n    P1.set_vel(N, q3 * N.z)\n    P2 = Point('P2')\n    P2.set_pos(P1, q1 * N.z)\n\n    assert P2.vel(N) == q1.diff(t) * N.z + q3 * N.z\n", "def test_point_vel_frame_change():\n    q1, q2 = dynamicsymbols('q1 q2')\n    N = ReferenceFrame('N')\n    B = N.orientnew('B', 'Axis', [q1, N.z])\n    O = Point('O')\n    P = O.locatenew('P', 10 * B.x)\n    O.set_vel(N, 5 * N.x)\n    assert P.vel(N) == 5 * N.x + 10 * q1.diff(t) * B.y\n\n\n", "def test_point_acc():\n    q, q2 = dynamicsymbols('q q2')\n    qd, q2d = dynamicsymbols('q q2', 1)\n    qdd, q2dd = dynamicsymbols('q q2', 2)\n    N = ReferenceFrame('N')\n    B = N.orientnew('B', 'Axis', [q, N.z])\n    O = Point('O')\n    P = O.locatenew('P', q * B.x)\n    O.set_vel(N, 0)\n    assert P.a1pt_theory(O, N, B) == (-qd**2) * B.x + qdd * B.y\n    P.set_vel(B, q2d * B.z)\n    assert P.a1pt_theory(O, N, B) == (-qd**2) * B.x + qdd * B.y + q2dd * B.z\n\n    P = O.locatenew('P', B.x)\n    assert P.a2pt_theory(O, N, B) == (-qd**2) * B.x + qdd * B.y\n"], "sample_263": ["    def test_circular_reference_with_many_to_many(self):\n        management.call_command('loaddata', 'circular_reference_m2m.json', verbosity=0)\n        obj_a = CircularA.objects.get()\n        obj_b = CircularB.objects.get()\n        self.assertEqual(obj_a.objects.get().key, 'x')\n        self.assertEqual(obj_b.objects.get().key, 'y')\n        self._dumpdata_assert(\n            ['fixtures'],\n            '[{\"model\": \"fixtures.circulara\", \"pk\": 1, \"fields\": {\"key\": \"x\", \"objects\": [1]}}, {\"model\": \"fixtures.circularb\", \"pk\": 1, \"fields\": {\"key\": \"y\", \"objects\": [1]}}, {\"model\": \"fixtures.circularobject\", \"pk\": 1, \"fields\": {\"key\": \"x\", \"circulara\": null}}]'\n        )\n", "    def test_circular_reference_with_depth(self):\n        with self.assertRaises(ValueError):\n            management.call_command(\n                'loaddata', 'circular_reference_with_depth.json', verbosity=0\n            )\n\n\n", "    def test_circular_reference_with_fk(self):\n        management.call_command('loaddata', 'circular_reference_with_fk.json', verbosity=0)\n        obj_a = CircularA.objects.get()\n        obj_b = CircularB.objects.get()\n        self.assertEqual(obj_a.obj, obj_b)\n        self.assertEqual(obj_b.obj, obj_a)\n        self._dumpdata_assert(\n            ['fixtures'],\n            '[{\"model\": \"fixtures.circulara\", \"pk\": 1, \"fields\": {\"key\": \"x\", \"obj_id\": 2}}, '\n            '{\"model\": \"fixtures.circularb\", \"pk\": 2, \"fields\": {\"key\": \"y\", \"obj_id\": 1}}]',\n        ) \n", "    def test_circular_reference_with_m2m(self):\n        management.call_command('loaddata', 'circular_reference_m2m.json', verbosity=0)\n        obj_a = CircularA.objects.get()\n        obj_b = CircularB.objects.get()\n        self.assertIn(obj_b, obj_a.objects)\n        self.assertIn(obj_a, obj_b.objects)\n        self._dumpdata_assert(\n            ['fixtures'],\n            '[{\"model\": \"fixtures.circulara\", \"pk\": 1, '\n            '\"fields\": {\"key\": \"x\", \"objects\": [1]},}, '\n            '{\"model\": \"fixtures.circularb\", \"pk\": 1, '\n            '\"fields\": {\"key\": \"y\", \"objects\": [1]}}]',\n            natural_foreign_keys=True,\n        )\n", "    def test_loading_app_label_inheritance(self):\n        management.call_command('loaddata', 'inheritance_fixture.json', verbosity=0)\n        super_obj = SuperModel.objects.get(name='super_instance')\n        sub_obj = SubModel.objects.get(name='sub_instance')\n        self.assertEqual(super_obj.name, 'super_instance')\n        self.assertEqual(sub_obj.name, 'sub_instance')\n        self.assertEqual(sub_obj.inherited_field, 'inherited value')\n        self._dumpdata_assert(\n            ['fixtures'],\n            '[{\"model\": \"fixtures.supermodel\", \"pk\": 1, \"fields\": {\"name\": \"super_instance\", \"inherited_field\": \"inherited value\"}}, {\"model\": \"fixtures.submodel\", \"pk\": 1, \"fields\": {\"name\": \"sub_instance\"}}]'\n        )\n\n\n"], "sample_677": ["def test_nested_parentheses(expr: str, expected: bool) -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "def test_ident_with_other_operators(expr: str, expected: bool) -> None:\n    matcher = {\"ident\": True}.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "def test_multiple_idents(expr: str, expected: bool) -> None:\n    matcher = {\"ident1\": False, \"ident2\": False, \"ident3\": True, \"ident4\": False}.__getitem__\n    assert evaluate(expr, matcher) is expected\n", "def test_complex_parentheses(expr: str) -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(expr, matcher) \n", "def test_multiple_idents(expr: str, expected: bool) -> None:\n    matcher = {\"ident1\": True, \"ident2\": True, \"ident3\": True}.__getitem__\n    assert evaluate(expr, matcher) is expected\n"], "sample_865": ["compilation error", "def test_prune_tree_raises_invalid_ccp_alpha_type():\n    clf = DecisionTreeClassifier()\n    msg = \"Invalid type for parameter 'ccp_alpha': 'str'\"\n\n    with pytest.raises(TypeError, match=msg):\n        clf.set_params(ccp_alpha=\"abc\")\n\n", "def test_sparse_matrix_csr_as_sample_weight():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([0, 1, 0])\n    sample_weight = csr_matrix((np.ones(3), (np.arange(3), np.arange(3))))\n    est = DecisionTreeClassifier(random_state=0)\n    est.fit(X, y, sample_weight=sample_weight)\n    assert_array_equal(est.tree_.sample_weight, np.array([1.0, 1.0, 1.0]))\n\n\n\n", "compilation error", "def test_prune_tree_raises_invalid_ccp_alpha_type():\n    clf = DecisionTreeClassifier()\n    with pytest.raises(TypeError, match=\"ccp_alpha must be a number\"):\n        clf.set_params(ccp_alpha=\"abc\")\n"], "sample_1033": ["compilation error", "compilation error", "def test_issue_17743():\n    from sympy import sympify\n    expr = sympify('3*x + 2 + 3/x')\n    assert expr.as_finite() == expr\n", "def test_issue_16380():\n    from sympy import symbols, sin, cos, asin, acos\n    x = symbols('x')\n    assert asin(sin(x)).rewrite(asin).equals(x)\n    assert acos(cos(x)).rewrite(acos).equals(x)\n", "compilation error"], "sample_1119": ["def test_transpose():\n    assert A.T.shape == (m, n)\n    assert (A.T).T == A\n    assert (A*B).T == B.T*A.T\n    assert (A.T + C.T) == (A + C).T\n    assert (A.T * C.T) == (C * A).T\n    assert (A.T.I).I == A.T\n    assert (A.I).T == (A.T).I\n    assert (A.transpose()).shape == (m, n)\n\n", "def test_LUdecomposition_Simple():\n    A = Matrix([[1, 2], [3, 4]])\n    L, U, P = A.LUdecomposition_Simple()\n    assert A == L * U * P\n    \n    B = Matrix([[1, 2, 3], [4, 5, 6]])\n    L, U, P = B.LUdecomposition_Simple()\n    assert B == L * U * P\n\n", "def test_matmul_identity():\n    assert A * Identity(A.cols) == A\n    assert Identity(A.cols) * A == A\n    assert A * Identity(A.rows) == A\n    assert Identity(A.rows) * A == A\n    assert A * Identity(n) != A if n != A.cols else A\n    assert Identity(n) * A != A if n != A.rows else A\n\n\n\n", "compilation error", "compilation error"], "sample_417": ["    def test_force_grouping_with_decimal(self):\n        with translation.override(\"en\"):\n            self.assertEqual(floatformat(66666.666, \"2g\"), \"66,666.67\")\n            self.assertEqual(floatformat(666666.666, \"2g\"), \"666,666.67\")\n            # Invalid suffix.\n            self.assertEqual(floatformat(66666.666, \"g2\"), \"66666.67\")\n        with translation.override(\"de\", deactivate=True):\n            self.assertEqual(floatformat(66666.666, \"2g\"), \"66.666,67\")\n            self.assertEqual(floatformat(666666.666, \"2g\"), \"666.666,67\")\n            # Invalid suffix.\n            self.assertEqual(floatformat(66666.666, \"g2\"), \"66666.67\")\n", "    def test_floatformat_with_decimal(self):\n        with localcontext().configure(prec=4):\n            self.assertEqual(floatformat(Decimal(\"1.23456789\"), 2), \"1.23\")\n            self.assertEqual(floatformat(Decimal(\"1.23456789\"), 3), \"1.235\")\n            self.assertEqual(floatformat(Decimal(\"1.23456789\"), 5), \"1.23457\")\n            self.assertEqual(floatformat(Decimal(\"1.23456789\"), 10), \"1.2345678900\")\n", "    def test_float_with_decimal_context(self):\n        with localcontext().configure(prec=5):\n            self.assertEqual(floatformat(1.23456789, 3), \"1.235\")\n\n\n\n\n", "compilation error", "    def test_floatformat_custom_grouping(self):\n        with self.settings(NUMBER_GROUPING=4):\n            self.assertEqual(floatformat(1000000, \"2\"), \"1.000.000\")\n            self.assertEqual(floatformat(66666.666, \"2\"), \"66.666,67\")\n        with self.settings(NUMBER_GROUPING=2):\n            self.assertEqual(floatformat(1000000, \"2\"), \"1.000.000\")\n            self.assertEqual(floatformat(66666.666, \"2\"), \"66,666.67\")\n\n\n"], "sample_913": ["def test_module_index_duplicates(app):\n    text = (\".. py:module:: docutils\\n\"\n            \".. py:module:: sphinx\\n\"\n            \".. py:module:: docutils\\n\"\n            \".. py:module:: sphinx\\n\")\n    restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    assert index.generate() == (\n        [('d', [IndexEntry('docutils', 0, 'index', 'module-docutils', '', '', '')]),\n         ('s', [IndexEntry('sphinx', 0, 'index', 'module-sphinx', '', '', '')])],\n        False\n    )\n", "def test_module_index_sorting(app):\n    text = (\".. py:module:: docutils\\n\"\n            \".. py:module:: sphinx\\n\"\n            \".. py:module:: sphinx.config\\n\"\n            \".. py:module:: sphinx.builders\\n\"\n            \".. py:module:: sphinx_intl\\n\"\n            \".. py:module:: sphinx.core\\n\")\n    restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    assert index.generate() == (\n        [('d', [IndexEntry('docutils', 0, 'index', 'module-docutils', '', '', '')]),\n         ('s', [IndexEntry('sphinx', 1, 'index', 'module-sphinx', '', '', ''),\n                IndexEntry('sphinx.builders', 2, 'index', 'module-sphinx.builders', '', '', ''),\n                IndexEntry('sphinx.core', 2, 'index', 'module-sphinx.core', '', '', ''),\n                IndexEntry('sphinx.config', 2, 'index', 'module-sphinx.config', '', '', ''),\n                IndexEntry('sphinx_intl', 0, 'index', 'module-sphinx_intl', '', '', '')])],\n        False\n    )\n", "def test_module_index_with_aliases(app):\n    text = (\".. py:module:: docutils\\n\"\n            \".. py:module:: sphinx\\n\"\n            \".. py:alias:: sphinx : module: sphinx.writers\\n\")\n    restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    assert index.generate() == (\n        [('d', [IndexEntry('docutils', 0, 'index', 'module-docutils', '', '', '')]),\n         ('s', [IndexEntry('sphinx', 1, 'index', 'module-sphinx', '', '', ''),\n                IndexEntry('sphinx.writers', 2, 'index', 'module-sphinx.writers', '', '', '')])],\n        False\n    ) \n", "def test_module_index_nested(app):\n    text = (\".. py:module:: docutils\\n\"\n            \"   .. py:module:: docutils.parsers\\n\"\n            \".. py:module:: sphinx\\n\")\n    restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    assert index.generate() == (\n        [('d', [IndexEntry('docutils', 0, 'index', 'module-docutils', '', '', ''),\n                IndexEntry('docutils.parsers', 1, 'index', 'module-docutils.parsers', '', '', '')]),\n         ('s', [IndexEntry('sphinx', 0, 'index', 'module-sphinx', '', '', '')])],\n        False\n    )\n", "def test_module_index_with_submodules(app):\n    text = (\".. py:module:: docutils\\n\"\n            \".. py:module:: sphinx\\n\"\n            \"    .. py:submodule:: sphinx.builders\\n\"\n            \"       .. py:submodule:: sphinx.builders.html\\n\"\n            \".. py:module:: sphinx_intl\\n\")\n    restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    assert index.generate() == (\n        [('d', [IndexEntry('docutils', 0, 'index', 'module-docutils', '', '', '')]),\n         ('s', [IndexEntry('sphinx', 1, 'index', 'module-sphinx', '', '', ''),\n                IndexEntry('sphinx.builders', 2, 'index', 'module-sphinx.builders', '', '', ''),\n                IndexEntry('sphinx.builders.html', 3, 'index', 'module-sphinx.builders.html', '', '', '')]),\n         ('s', [IndexEntry('sphinx_intl', 0, 'index', 'module-sphinx_intl', '', '', '')])],\n        False\n    )\n"], "sample_355": ["    def test_authenticate_invalid_user(self):\n        nonexistent_user = authenticate(username='nonexistent_user', password='test')\n        self.assertIsNone(nonexistent_user)\n\n\n\n\n", "    def test_authenticate_with_invalid_password(self):\n        self.assertFalse(self.user.is_active)\n        with self.assertRaises(AuthenticationFailed):\n            authenticate(username='test', password='wrongpassword')\n", "    def test_authenticate_inactive_user_with_correct_password(self):\n        self.user.is_active = False\n        self.user.save()\n        authenticate(**self.user_credentials)\n", "    def test_has_perm(self):\n        self.assertTrue(self.user.has_perm('auth.change_user'))\n\n\n\n", "    def test_check_password_inactive_user(self):\n        self.assertFalse(self.user.is_active)\n        self.assertTrue(check_password_hash(self.user.password, 'test'))\n"], "sample_525": ["def test_subplots_with_bbox_inches():\n    fig, axs = plt.subplots(2, 2)\n    fig.savefig(\"test.png\", bbox_inches='tight')\n", "def test_subplot_title():\n    fig, axs = plt.subplots(2, 2)\n    axs[0, 0].set_title('1')\n    axs[0, 1].set_title('2')\n    axs[1, 0].set_title('3')\n    axs[1, 1].set_title('4')\n\n    assert axs[0, 0].get_title() == '1'\n    assert axs[0, 1].get_title() == '2'\n    assert axs[1, 0].get_title() == '3'\n    assert axs[1, 1].get_title() == '4'\n\n\n\n", "def test_colorbar_shared_axis(recwarn):\n    warnings.filterwarnings(\"ignore\", \"cannot show the figure\")\n    fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n\n    im1 = ax1.imshow([[1, 2, 3], [4, 5, 6]])\n    im2 = ax2.imshow([[7, 8, 9], [10, 11, 12]])\n\n    fig.colorbar(im1, ax=ax1)\n    fig.colorbar(im2, ax=ax2)\n\n    # Test that the colorbars are independent\n    assert ax1.get_children()[1].get_ax().yaxis.get_label() != \\\n        ax2.get_children()[1].get_ax().yaxis.get_label()\n", "def test_subplots_adjust_with_subplots():\n    fig = plt.figure()\n    gs = fig.add_gridspec(2, 2)\n    ax1 = fig.add_subplot(gs[0, 0])\n    ax2 = fig.add_subplot(gs[0, 1])\n    ax3 = fig.add_subplot(gs[1, 0])\n    ax4 = fig.add_subplot(gs[1, 1])\n\n    fig.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1,\n                        wspace=0.2, hspace=0.2)\n\n    assert ax1.get_position().x0 == 0.1\n    assert ax1.get_position().x1 == 0.4\n    assert ax2.get_position().x0 == 0.42\n    assert ax2.get_position().x1 == 0.9\n    assert ax3.get_position().y0 == 0.1\n    assert ax3.get_position().y1 == 0.4\n    assert ax4.get_position().y0 == 0.42\n    assert ax4.get_position().y1 == 0.9\n", "def test_subplots_adjust_kwargs():\n    fig = plt.figure()\n    ax = fig.add_subplot(1, 1, 1)\n    fig.subplots_adjust(left=0.2, bottom=0.1, right=0.9, top=0.9, wspace=0.2, hspace=0.2)\n    assert fig.get_tight_layout('pad') == (0.1, 0.2, 0.05, 0.05)\n\n    try:\n        fig.subplots_adjust(wspace=np.nan)\n        assert False\n    except ValueError:\n        pass\n\n    try:\n        fig.subplots_adjust(hspace=-1)\n        assert False\n    except ValueError:\n        pass \n\n    plt.close(fig)\n"], "sample_1023": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_582": ["compilation error", "compilation error", "compilation error", "def index():\n    return \"Hello from custom!\"\n", "compilation error"], "sample_773": ["def test_logistic_regression_penalty_non_converge():\n    # Test if logistic regression throws exception or warnings when penalty \n    # is used with a non-convergent solver\n    X, y = make_classification(random_state=0)\n\n    for penalty in ['l1', 'l2', 'elasticnet']:\n        for solver in ['newton-cg', 'lbfgs']:  # These solvers can sometimes fail\n            lr = LogisticRegression(penalty=penalty, solver=solver,\n                                    random_state=0)\n            with pytest.raises(ConvergenceWarning):\n                lr.fit(X, y)\n\n", "def test_logistic_regression_penalty_l1_ratio(penalty, solver):\n    # Test the behavior of the l1_ratio parameter with different penalty types\n    # and solvers\n    X, y = make_classification(random_state=0)\n    Cs = np.logspace(-4, 4, 5)\n\n    if penalty == 'elasticnet':\n        l1_ratios = np.linspace(0, 1, 5)\n        for C in Cs:\n            for l1_ratio in l1_ratios:\n                lr = LogisticRegression(penalty=penalty, C=C, l1_ratio=l1_ratio,\n                                       solver=solver, random_state=0)\n                lr.fit(X, y)\n                assert lr.penalty_ == penalty\n                assert lr.l1_ratio_ == l1_ratio\n                assert lr.coef_.shape == (1,)\n\n    else:\n        lr = LogisticRegression(penalty=penalty, C=Cs[0], solver=solver,\n                                random_state=0)\n        lr.fit(X, y)\n        assert lr.penalty_ == penalty\n        assert lr.l1_ratio_ is None\n", "def test_logistic_regression_path_sparse_data(multi_class):\n    # Test logistic_regression_path with sparse data\n\n    n_samples = 1000\n    n_features = 1000\n    X = sp.csr_matrix((np.random.rand(n_samples, n_features),\n                       (np.random.randint(0, n_features, size=n_samples),\n                        np.random.randint(0, n_features, size=n_samples))))\n    y = np.random.randint(0, 2, size=n_samples)\n\n    Cs = [.01, 1, 100]\n    coefs, _, _ = _logistic_regression_path(X, y, penalty='l2', Cs=Cs,\n                                            solver='saga', random_state=0,\n                                            multi_class=multi_class)\n\n    # Check that coefs is a list of numpy arrays\n\n    for coef in coefs:\n        assert isinstance(coef, np.ndarray)\n\n", "def test_logistic_regression_path_coefs_sparse():\n    # Make sure coef returned by logistic_regression_path works with sparse matrices\n    X, y = make_classification(n_samples=200, n_clusters_per_class=1,\n                               n_informative=10, n_redundant=0, random_state=0)\n    X_sparse = sparse.csc_matrix(X)\n    Cs = [.00001, 1, 10000]\n    coefs, _, _ = _logistic_regression_path(X_sparse, y, penalty='l1', Cs=Cs,\n                                            solver='saga', random_state=0,\n                                            multi_class='ovr')\n\n    assert coefs.shape == (3, 10)  # 3 Cs, 10 coefficients\n    assert np.allclose(coefs[0].A1, coefs[0].reshape(-1))\n    assert np.allclose(coefs[1].A1, coefs[1].reshape(-1))\n    assert np.allclose(coefs[2].A1, coefs[2].reshape(-1))\n", "def test_logistic_regression_path_coefs_l1_and_l2():\n    # Make sure that the returned coefficients for logistic_regression_path\n    # are correct when penalty='l1' or 'l2'\n    X, y = make_classification(n_samples=200, n_classes=3, n_informative=2,\n                               n_redundant=0, n_clusters_per_class=1,\n                               random_state=0, n_features=2)\n    Cs = [.00001, 1, 10000]\n    coefs_l1, _, _ = _logistic_regression_path(X, y, penalty='l1', Cs=Cs,\n                                             solver='saga', random_state=0)\n    coefs_l2, _, _ = _logistic_regression_path(X, y, penalty='l2', Cs=Cs,\n                                             solver='saga', random_state=0)\n    assert_array_almost_equal(coefs_l1.min(), 0, decimal=1)\n    assert_array_almost_equal(coefs_l2.min(), 0, decimal=1)\n\n"], "sample_319": ["    def test_operation_with_custom_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    \"Person\",\n                    fields=[\n                        (\"id\", models.AutoField(primary_key=True)),\n                        (\"name\", models.CharField(max_length=255)),\n                    ],\n                    name=\"person_migration_test\",\n                )\n            ]\n\n        migration = Migration(\"0001_initial\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"person\")\n\n", "    def test_rename_field_in_index_together(self):\n        initial_author = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n                (\"age\", models.IntegerField()),\n            ],\n            {\n                \"index_together\": {(\"name\", \"age\")},\n            },\n        )\n        author_renamed_constraints = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"fullname\", models.CharField(max_length=200)),\n                (\"age\", models.IntegerField()),\n            ],\n            {\n                \"index_together\": {(\"fullname\", \"age\")},\n            },\n        )\n        changes = self.get_changes(\n            [initial_author], [author_renamed_constraints]\n        )\n\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(\n            changes, \"testapp\", 0, [\"RenameField\", \"AlterIndexTogether\"]\n        )\n\n\n\n", "    def test_add_field_with_default_from_expression(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.AddField(\n                    model_name=\"Person\",\n                    name=\"age\",\n                    field=models.IntegerField(default=models.F(\"dob\") // 10000),\n                ),\n            ]\n\n        migration = Migration(\"0001_initial\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"add_field_age\")\n", "    def test_operation_with_empty_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    \"Person\", fields=[], name=\"\"\n                ),\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"person\") \n", "    def test_operation_with_custom_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.AlterField(\n                    model_name=\"Person\",\n                    name=\"name\",\n                    field=models.CharField(max_length=100),\n                )\n            ]\n            name = \"change_person_name\"\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"change_person_name\")\n\n"], "sample_535": ["def test_bbox():\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.axis('off')\n    table = ax.table(\n        cellText=[[1, 2, 3], [4, 5, 6]],\n        bbox=Bbox([0.1, 0.1, 0.3, 0.2]),\n    )\n    plt.tight_layout()\n\n", "def test_bbox():\n    fig, ax = plt.subplots()\n\n    # Test that the bbox is set correctly\n    table = ax.table(\n        cellText=[['1'] * 3] * 2,\n        bbox=[0.1, 0.1, 0.5, 0.5],\n        colWidths=[0.1, 0.1, 0.1],\n    )\n    bbox = table.get_bbox()\n    assert bbox.bounds[0] == 0.1\n    assert bbox.bounds[1] == 0.1\n    assert bbox.bounds[2] == 0.6\n    assert bbox.bounds[3] == 0.6 \n\n\n\n", "def test_table_bboxes():\n    fig, ax = plt.subplots()\n    table = Table(ax)\n    cell1 = table.add_cell(0, 0, width=1, height=1)  \n    \n    bbox = cell1.get_window_extent(ax.figure.canvas.get_renderer())\n    assert bbox.bounds == (0, 0, 1, 1)\n", "def test_bbox():\n    data = [['A', 'B'], ['C', 'D']]\n    table = plt.table(cellText=data, loc='bottom', bbox=[0, 0, 1, 1])\n\n    plt.axis('off')\n\n", "def test_bbox():\n    fig, ax = plt.subplots()\n\n    # Create a table with a large bbox\n    cellText = [['A', 'B'], ['C', 'D']]\n    table = ax.table(cellText=cellText, bbox=[0, 0.2, 0.8, 0.8], loc='bottom')\n\n    # Verify that the table is within the specified bbox\n    bbox = table.get_window_extent(renderer=ax.figure.canvas.get_renderer())\n    assert bbox.xmin == 0\n    assert bbox.ymin == 0.2\n    assert bbox.xmax == 0.8\n    assert bbox.ymax == 0.8\n\n    plt.tight_layout()\n"], "sample_123": ["    def test_parsing_with_timezone(self):\n        parsed = parse_http_date('Sun, 06 Nov 1994 08:49:37 GMT+0100')\n        self.assertEqual(datetime.utcfromtimestamp(parsed), datetime(1994, 11, 6, 7, 49, 37))\n\n", "    def test_parsing_invalid_date(self):\n        with self.assertRaises(ValueError):\n            parse_http_date(\"invalid date\")\n", "    def test_parsing_rfc850_with_leading_zeros(self):\n        parsed = parse_http_date('06-Nov-94 08:49:37 GMT')\n        self.assertEqual(datetime.utcfromtimestamp(parsed), datetime(1994, 11, 6, 8, 49, 37))\n", "    def test_parsing_invalid_date(self):\n        with self.assertRaises(ValueError):\n            parse_http_date('invalid date')\n\n\n\n", "    def test_parsing_future_dates(self):\n        parsed = parse_http_date('Sun, 06 Nov 2050 08:49:37 GMT')\n        self.assertEqual(datetime.utcfromtimestamp(parsed), datetime(2050, 11, 6, 8, 49, 37))\n\n"], "sample_1038": ["def test_transpose_with_values():\n    x, y, z, w = symbols(\"x y z w\")\n    M = Matrix([[x, y], [z, w]])\n    i, j = symbols(\"i, j\")\n    assert M.T[i, j] == M[j, i]\n    assert M.T.subs({i: 0, j: 1}) == M[1, 0]\n    assert M.T.subs({i: 1, j: 0}) == M[0, 1]\n    assert M.T[i, j].subs(M, Matrix([[1, 2], [3, 4]])) == \\\n        Matrix([[1, 3], [2, 4]])[i, j]\n\n    A = MatrixSymbol(\"A\", 2, 2)\n    assert A.T.subs(A, M) == M.T\n    assert (A.T).T == A\n    assert (A.T).subs(A, M) == A\n\n\n\n\n", "def test_MatrixSymbol_expand():\n    A = MatrixSymbol(\"A\", 2, 2)\n    B = MatrixSymbol(\"B\", 2, 2)\n    expr = A* (B + A)\n\n    assert expr.expand() == A*B + A**2\n    expr = (A + B)*A\n    assert expr.expand() == A**2 + B*A\n", "def test_MatrixElement_subs_advanced():\n    A = MatrixSymbol(\"A\", 3, 3)\n    B = MatrixSymbol(\"B\", 3, 3)\n\n    # Test substitution with a list of matrices\n    expr = A[0, 0] + B[1, 1]\n    assert expr.subs({A: [[1, 2, 3], [4, 5, 6], [7, 8, 9]],\n                     B: [[10, 11, 12], [13, 14, 15], [16, 17, 18]]}) == 1 + 14\n\n\n\n", "def test_det():\n    A = MatrixSymbol('A', 2, 2)\n    assert A.det() == A[0, 0]*A[1, 1] - A[0, 1]*A[1, 0]\n    B = Matrix([[1, 2], [3, 4]])\n    assert B.det() == 1*4 - 2*3 == -2\n    assert (B*B).det() == B.det()**2\n    C = Matrix([[1, 0], [0, 0]])\n    assert C.det() == 0\n    D = Matrix([[0, 1], [1, 0]])\n    assert D.det() == -1\n    E = Matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    assert E.det() == 1*(5*9 - 6*8) - 2*(4*9 - 6*7) + 3*(4*8 - 5*7)\n\n\n\n", "def test_transpose_identity():\n    I = Identity(n)\n    assert I.T == I\n    assert transpose(I) == I\n\n"], "sample_533": ["def test_contour_label_formatting():\n    x = np.arange(10)\n    y = np.arange(10)\n    X, Y = np.meshgrid(x, y)\n    Z = np.sin(X * Y)\n\n    fig, ax = plt.subplots()\n    cs = ax.contour(X, Y, Z)\n    ax.clabel(cs, inline=True, fontsize=10,\n              fmt='%1.2f', labelpad=20)\n    for text in ax.texts:\n        assert text.get_text().startswith('0')\n        assert len(text.get_text()) == 4\n", "def test_contourf_binary_data():\n    # Test contourf with binary data (values are either 0 or 1).\n    data = np.array([[1, 0, 1, 1],\n                     [0, 1, 0, 0],\n                     [0, 0, 1, 0],\n                     [1, 0, 1, 1]])\n\n    fig, ax = plt.subplots()\n    cs = ax.contourf(data, cmap=plt.cm.binary)\n    assert isinstance(cs.levels, list)\n    assert len(cs.levels) == 3\n    assert all(x == 0 or x == 1 for x in cs.levels)\n\n\n", "    def test_contourf_uneven_levels():\n        # Test that contourf with uneven levels works as expected.\n        x = np.arange(0, 10, 0.5)\n        y = np.arange(0, 10, 0.5)\n        X, Y = np.meshgrid(x, y)\n        Z = np.sin(np.sqrt(X**2 + Y**2))\n\n        fig, ax = plt.subplots()\n        cs = ax.contourf(X, Y, Z, levels=[-1, 0, 0.5, 1, 1.5, 2])  # uneven levels\n        ax.set_title(\"contourf with uneven levels\")\n", "def test_contour_filled_lines():\n    x = np.arange(0, 10, 0.1)\n    y = np.arange(0, 10, 0.1)\n    X, Y = np.meshgrid(x, y)\n\n    Z = np.sin(np.sqrt(X**2 + Y**2))\n\n    fig, ax = plt.subplots()\n    cs = ax.contourf(X, Y, Z, levels=20)\n    ax.clabel(cs, inline=True, fontsize=8)\n\n    ax.contour(X, Y, Z, levels=20, colors='k', linewidths=0.5)\n\n", "def test_contour_interpolation():\n    x = np.arange(0, 10, 0.5)\n    y = np.arange(0, 10, 0.5)\n    X, Y = np.meshgrid(x, y)\n    Z = np.sin(np.sqrt(X**2 + Y**2))\n\n    fig, ax = plt.subplots()\n    cs = ax.contour(X, Y, Z, levels=10, interpolation='nearest')\n    assert cs._contour_generator.interpolation == 'nearest'\n\n    cs = ax.contour(X, Y, Z, levels=10, interpolation='linear')\n    assert cs._contour_generator.interpolation == 'linear'\n\n    cs = ax.contour(X, Y, Z, levels=10, interpolation='cubic')\n    assert cs._contour_generator.interpolation == 'cubic'\n\n    with pytest.raises(ValueError, match=\"Invalid interpolation method\"):\n        ax.contour(X, Y, Z, levels=10, interpolation='invalid')\n\n    # Test that interpolation is properly used in clabel\n    ax.clabel(cs, fontsize=8, inline=True, fmt='%1.1f')\n    \n    # Test that interpolation is properly used in contourf\n    csf = ax.contourf(X, Y, Z, levels=10, interpolation='nearest')\n    assert csf._contour_generator.interpolation == 'nearest'\n\n\n\n\n"], "sample_87": ["    def test_snapshot_files_keeps_timestamps(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n            snapshot1 = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot1)\n            self.increment_mtime(self.existing_file)\n            snapshot2 = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot2)\n            self.assertGreater(snapshot2[self.existing_file], snapshot1[self.existing_file])\n\n\n\n", "    def test_snapshot_files_deleted_files(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n            snapshot1 = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot1)\n            os.remove(self.existing_file)\n            snapshot2 = dict(self.reloader.snapshot_files())\n            self.assertNotIn(self.existing_file, snapshot2)\n", "    def test_snapshot_files_handles_non_file_entries(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file, self.tempdir]):\n            snapshot = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot)\n            self.assertIn(str(self.tempdir), snapshot)\n", "    def test_snapshot_files_deleted(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n            snapshot1 = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot1)\n            os.remove(self.existing_file)\n            snapshot2 = dict(self.reloader.snapshot_files())\n            self.assertNotIn(self.existing_file, snapshot2)\n", "    def test_snapshot_files_handles_creation(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.nonexistent_file]):\n            snapshot1 = dict(self.reloader.snapshot_files())\n            self.assertNotIn(self.nonexistent_file, snapshot1)\n            self.increment_mtime(self.nonexistent_file)\n            snapshot2 = dict(self.reloader.snapshot_files())\n            self.assertIn(self.nonexistent_file, snapshot2)\n"], "sample_283": ["    def test_empty_database_name(self):\n        self.assertEqual(\n            self.settings_to_cmd_args_env({}),\n            (['psql', 'postgres'], {}),\n        )\n", "    def test_defaults(self):\n        self.assertEqual(\n            self.settings_to_cmd_args_env({}),\n            (['psql', 'postgres'], {'PGPASSWORD': None}),\n        )\n", "compilation error", "compilation error", "    def test_sigint_handler_restoration(self):\n        \"\"\"SIGINT handler is restored after running psql.\"\"\"\n        original_handler = signal.getsignal(signal.SIGINT)\n        with mock.patch('subprocess.run', wraps=subprocess.run) as mock_subprocess_run:\n            DatabaseClient.runshell(self, [])\n        restored_handler = signal.getsignal(signal.SIGINT)\n        self.assertEqual(restored_handler, original_handler)\n"], "sample_217": ["    def test_merge_css_duplicate_order(self):\n        widget1 = Media(css={'screen': ['screen.css'], 'all': ['all.css']})\n        widget2 = Media(css={'screen': ['screen.css']})\n        widget3 = Media(css={'all': ['all.css']})\n        merged = widget1 + widget2 + widget3\n        self.assertEqual(merged._css, {'screen': ['screen.css'], 'all': ['all.css']})\n\n        merged = widget2 + widget1 + widget3 \n        self.assertEqual(merged._css, {'screen': ['screen.css'], 'all': ['all.css']})\n", "    def test_merge_css_duplicate_media(self):\n        widget1 = Media(css={'all': ['a.css', 'b.css']})\n        widget2 = Media(css={'all': ['b.css', 'a.css']})\n        merged = widget1 + widget2\n        self.assertEqual(merged._css, {'all': ['a.css', 'b.css']}) \n\n", "    def test_merge_css_three_way_with_media_types(self):\n        widget1 = Media(css={'screen': ['c.css'], 'print': ['d.css']})\n        widget2 = Media(css={'screen': ['a.css'], 'print': ['b.css']})\n        widget3 = Media(css={'screen': ['a.css', 'b.css', 'c.css'], 'print': ['e.css']})\n        merged = widget1 + widget2 + widget3\n        self.assertEqual(merged._css, {'screen': ['a.css', 'b.css', 'c.css'], 'print': ['b.css', 'd.css', 'e.css']})\n", "    def test_append_html_safe(self):\n        media = Media(css={'all': ['/path/to/css']}, js=['/path/to/js'])\n        self.assertTrue(hasattr(Media, '__html__'))\n        self.assertEqual(str(media), media.__html__())\n\n        another_media = Media(css={}, js=[])\n        media_combined = media + another_media\n\n        self.assertEqual(str(media_combined), \"\"\"<link href=\"/path/to/css\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">", "    def test_media_inheritance_with_override(self):\n        class MyWidget1(TextInput):\n            class Media:\n                css = {\n                    'all': ('path/to/css1', '/path/to/css2')\n                }\n                js = ('/path/to/js1', 'http://media.other.com/path/to/js2', 'https://secure.other.com/path/to/js3')\n\n        class MyWidget2(MyWidget1):\n            class Media:\n                extend = True\n                css = {\n                    'screen': ('/path/to/css3',),\n                }\n                js = ('/path/to/js4', )\n\n        w2 = MyWidget2()\n        self.assertEqual(\n            str(w2.media),\n            \"\"\"<link href=\"http://media.example.com/static/path/to/css1\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">"], "sample_1088": ["compilation error", "def test_viete():\n    assert viete(x**2 + 1) == [(0, 0), (1, 1)]\n    assert viete(x**2 - 1) == [(0, 0), (1, -1)]\n    assert viete(x**3 + 1) == [(0, 0), (0, -1), (1, 1)]\n    assert viete(x**3 - 1) == [(0, 0), (0, 1), (1, -1)]\n    assert viete(x**3 + 2*x**2 + x - 1, [r1, r2, r3]) == [(r1 + r2 + r3, -2), (r1*r2 + r1*r3 + r2*r3, 1), (r1*r2*r3, -1)]\n    assert viete(x**4 + x**3 - 2*x**2 - x + 1, [r1, r2, r3, r4]) == \\\n        [(r1 + r2 + r3 + r4, -1),\n         (r1*r2 + r1*r3 + r1*r4 + r2*r3 + r2*r4 + r3*r4, 1),\n         (r1*r2*r3 + r1*r2*r4 + r1*r3*r4 + r2*r3*r4, -2),\n         (r1*r2*r3*r4, 1)]\n\n\n\n", "def test_viete():\n    assert viete(x**2) == [(0, -0)]\n    assert viete(x**2 + 1) == [(0, -0), (1, 1)]\n\n    assert viete(x**3 - 3*x**2 + 2*x) == [(3, 3), (-3, -2)]\n    assert viete(x**3 - 3*x**2 + 2*x, r1, r2, r3) == [(r1 + r2 + r3, 3),\n    (r1*r2 + r1*r3 + r2*r3, -2)]\n    \n    assert viete(x**4 - 4*x**3 + 6*x**2 - 4*x + 1) == [(4, 4),\n    (-4, -6), (6, -4), (-1, 1)]\n    \n    U = symbols('u0,u1,u2,u3')\n    assert viete(x**4 - 4*x**3 + 6*x**2 - 4*x + 1, U) == [(U[0] + U[1] + U[2] + U[3], 4),\n    (U[0]*U[1] + U[0]*U[2] + U[0]*U[3] + U[1]*U[2] + U[1]*U[3] + U[2]*U[3], -6),\n    (U[0]*U[1]*U[2] + U[0]*U[1]*U[3] + U[0]*U[2]*U[3] + U[1]*U[2]*U[3], -4),\n    (U[0]*U[1]*U[2]*U[3], 1)]\n\n", "def test_viete():\n    assert viete(0, x) == [(0, 0)]\n    assert viete(1, x) == [(1, 0)]\n    assert viete(x, x) == [(x, 1)]\n    assert viete(x + 1, x) == [(x + 1, -1)]\n\n    s1 = x + y + z\n    s2 = x*y + x*z + y*z\n\n    assert viete(x**2, x, y) == [(x + y, -0), (x*y, 1)]\n\n    assert viete(x**2 + y**2, x, y) == [(x + y, 0), (x*y, 1)]\n\n    assert viete(x**3 + y**2 + a*x**2 + b*y**3, x, y) == \\\n      [(x + y, -a), (x*y + a*x + b*y**2, a), (x*y*z, 1)]\n\n    assert viete(x**3 + y**3 + z**3 - 3*x*y*z, x, y, z) == \\\n      [(x + y + z, 0), (x*y + x*z + y*z, 0), (x*y*z, 1)]\n\n\n\n", "def test_viete():\n    assert viete(x**2 + 2*x + 1, [r1], x) == [(r1 + r1, -2)]\n    assert viete(x**3 - 3*x + 2, [r1, r2, r3], x) == [(r1 + r2 + r3, 0), (r1*r2 + r1*r3 + r2*r3, -3)]\n    assert viete(x**3 - 3*x + 2, [r1, r2, r3], x) == [(r1 + r2 + r3, 0), (r1*r2 + r1*r3 + r2*r3, -3)]\n    assert viete(a*x**2 + b*x + c, [r1, r2], x) == [(r1 + r2, -b/a), (r1*r2, c/a)]\n    assert viete(a*x**3 + b*x**2 + c*x + d, [r1, r2, r3], x) == [(r1 + r2 + r3, -b/a),\n                                                             (r1*r2 + r1*r3 + r2*r3, c/a),\n                                                             (r1*r2*r3, -d/a)]\n\n\n\n"], "sample_963": ["def test_stringify_type_hints_TypeAlias():\n    from typing import TypeAlias\n\n    T = TypeAlias(\"T\", int)\n    assert stringify(T) == \"T\"\n    assert stringify(T) == \"tests.test_util_typing.T\"\n", "def test_stringify_typing_modules(annotation: Any, expected: str):\n    assert stringify(annotation) == expected\n\n\n\n", "def test_stringify_type_hints_origin_is_None():\n    from typing import TypeVar, Union\n    T = TypeVar('T')\n    assert stringify(Union[T, None]) == \"Union[T, None]\"  # type: ignore\n\n\n\n", "def test_stringify_type_hints_TypeAlias():\n    from typing import TypeAlias  # type: ignore\n\n    T = TypeAlias('T', int)  \n\n    assert stringify(T) == \"int\"\n", "def test_stringify_type_hints_TypeAlias():\n    from typing import TypeAlias\n    T = TypeAlias(\"T\", int)\n    assert stringify(T) == \"T\"  # type: ignore\n"], "sample_753": ["def test_penalty_l2_with_class_weight():\n    X, y = make_classification(n_samples=100, n_features=5, random_state=0)\n    class_weight = {'0': 1, '1': 2}\n    lr = LogisticRegression(penalty='l2', C=1.0, class_weight=class_weight,\n                            solver='liblinear')\n    lr.fit(X, y)\n    assert_array_almost_equal(lr.coef_, lr.coef_)\n", "def test_l1_with_class_weight():\n    # Test that class_weight works correctly with l1 penalty.\n    X, y = make_classification(n_samples=50, n_features=20, random_state=0,\n                               class_weight={0: 1, 1: 2},\n                               random_state=0)\n    lr1 = LogisticRegression(penalty=\"l1\", C=1.0, solver='liblinear',\n                             class_weight={0: 1, 1: 2})\n    lr1.fit(X, y)\n\n    lr2 = LogisticRegression(penalty=\"l1\", C=1.0, solver='liblinear',\n                             class_weight=None)\n    lr2.fit(X, y)\n    assert_array_almost_equal(lr1.coef_, lr2.coef_, decimal=3)\n", "def test_penalty_l1_coef_scaling():\n    # Test that the coefficient scaling is correct when penalty='l1'\n\n    X, y = make_classification(n_samples=100, n_features=10, random_state=0)\n    clf = LogisticRegression(penalty='l1', C=1.0, solver='saga')\n    clf.fit(X, y)\n\n    # Ensure that coefficients in the solution are not all zero\n    assert_greater(np.max(np.abs(clf.coef_)), 1e-6)\n\n    # Scale coefficients by the regularization parameter\n    scaled_coef = clf.coef_ * clf.C\n    assert_array_almost_equal(scaled_coef, clf.coef_)\n", "def test_penalty_l1_and_l2_with_solver_lbfgs():\n    # Test penalty=\"l1\" and penalty=\"l2\" with solver=\"lbfgs\"\n\n    for penalty in ['l1', 'l2']:\n        X, y = make_classification(n_samples=100, n_features=5, random_state=0)\n        lr_lbfgs = LogisticRegression(penalty=penalty, solver='lbfgs',\n                                       random_state=0)\n        lr_lbfgs.fit(X, y)\n        assert_array_almost_equal(lr_lbfgs.coef_, lr_lbfgs.coef_)\n\n", "compilation error"], "sample_77": ["    def test_urlize_trim_url(self):\n        items = (\n            (\n                'Visit http://www.example.com/long-url-that-should-be-truncated',\n                'Visit <a href=\"http://www.example.com/long-url-that-should-be-truncated\">http://www.example.com/long-url-that-should-be-truncated...</a>',\n            ),\n            (\n                lazystr('Visit http://www.example.com/long-url-that-should-be-truncated'),\n                'Visit <a href=\"http://www.example.com/long-url-that-should-be-truncated\">http://www.example.com/long-url-that-should-be-truncated...</a>',\n            ),\n        )\n        for value, output in items:\n            with self.subTest(value=value):\n                self.assertEqual(urlize(value, trim_url_limit=30), output)\n", "    def test_urlize_trim_url_limit(self):\n        items = (\n            (\n                'Visit http://www.example.com/this-is-a-very-long-url-that-needs-to-be-trimmed',\n                'Visit <a href=\"http://www.example.com/this-is-a-very-long-url-that-needs-to-be-trimmed\">http://www.example.com/this-is-a-very-long-url-that-needs-to-be-trimmed\u2026</a>',\n            ),\n            (\n                'Read more about the <a href=\"https://www.example.com/another-very-long-url-that-needs-to-be-trimmed\">long URL</a> here.',\n                'Read more about the <a href=\"https://www.example.com/another-very-long-url-that-needs-to-be-trimmed\">https://www.example.com/another-very-long-url-that-needs-to-be-trimmed\u2026</a> here.',\n            ),\n        )\n        for value, output in items:\n            with self.subTest(value=value):\n                self.assertEqual(urlize(value, trim_url_limit=50), output)\n", "    def test_urlize_with_trailing_punctuation(self):\n        tests = (\n            ('http://example.com/?q=stuff! ', 'http://example.com/?q=stuff!'),\n            ('http://example.com/?q=stuff, ', 'http://example.com/?q=stuff, '),\n            ('http://example.com/?q=stuff.', 'http://example.com/?q=stuff.'),\n        )\n        for value, output in tests:\n            with self.subTest(value=value):\n                self.assertEqual(urlize(value), output)\n\n", "    def test_urlize_with_trailing_punctuation(self):\n        items = (\n            ('http://example.com/foo?q=search.!!!',\n             'http://example.com/foo?q=search.<a href=\"http://example.com/foo?q=search.!!!\" rel=\"nofollow\">!!!</a>'),\n            ('https://www.google.com?q=django&hl=en.',\n             'https://www.google.com?q=django&hl=en.<a href=\"https://www.google.com?q=django&hl=en.!!!\" rel=\"nofollow\">!!!</a>'),\n        )\n        for value, output in items:\n            with self.subTest(value=value):\n                self.assertEqual(urlize(value, trim_url_limit=20), output)\n", "    def test_urlize_with_trim_url_limit(self):\n        items = (\n            ('This is a long long URL: http://www.example.com/long/long/long/long/long/long/?param1=value1&param2=value2',\n             'This is a long long URL: <a href=\"http://www.example.com/long/long/long/long/long/long/?param1=value1&param2=value2\"... </a>'),\n            ('This is a long URL: http://www.example.com/long/long/long/long/long/long/long/long/?param1=value1&param2=value2',\n             'This is a long URL: <a href=\"http://www.example.com/long/long/long/long/long/long/long/long/?param1=value1&param2=value2\"... </a>'),\n        )\n        for value, output in items:\n            with self.subTest(value=value):\n                self.assertEqual(urlize(value, trim_url_limit=15), output)\n"], "sample_160": ["    def test_empty_string(self):\n        self.assertEqual(nformat(''), '')\n        self.assertEqual(nformat('', '.'), '')\n        self.assertEqual(nformat('', '.', decimal_pos=2), '')\n", "    def test_thousands_grouping_with_zero_grouping(self):\n        self.assertEqual(nformat(1234567890, '.', grouping=0, thousand_sep=','), '1234567890')\n        self.assertEqual(nformat(1234567890, '.', grouping=0, thousand_sep=' ', force_grouping=True), '1234567890')\n        self.assertEqual(nformat(1234567890, '.', grouping=(0, 0), thousand_sep=','), '1234567890')\n", "    def test_non_numeric_input(self):\n        self.assertRaises(TypeError, nformat, 'abc', '.')\n        self.assertRaises(TypeError, nformat, [1, 2, 3], '.')\n        self.assertRaises(TypeError, nformat, True, '.')\n        self.assertRaises(TypeError, nformat, None, '.')\n", "    def test_zero_and_empty_strings(self):\n        self.assertEqual(nformat(0, '.', decimal_pos=2), '0.00')\n        self.assertEqual(nformat(0, '.', grouping=2, thousand_sep=','), '0')\n        self.assertEqual(nformat('', '.'), '')", "    def test_empty_string(self):\n        self.assertEqual(nformat(''), '')\n        self.assertEqual(nformat('', '.'), '')\n        self.assertEqual(nformat('', '.', decimal_pos=2), '0.00')\n        self.assertEqual(nformat('', '.', grouping=2, thousand_sep=','), '')\n        self.assertEqual(nformat('', '.', grouping=2, thousand_sep=',', force_grouping=True), '')\n"], "sample_6": ["def test_array_operations():\n    # basic addition and subtraction\n    c1 = ICRS([1, 2] * u.deg, [3, 4] * u.deg)\n    c2 = ICRS([0.5, 1.5] * u.deg, [0.5, 1.5] * u.deg)\n\n    c3 = c1 + c2\n    npt.assert_array_almost_equal(c3.ra.degree, [1.5, 3.5])\n    npt.assert_array_almost_equal(c3.dec.degree, [3.5, 5.5])\n\n    c4 = c1 - c2\n    npt.assert_array_almost_equal(c4.ra.degree, [0.5, 0.5])\n    npt.assert_array_almost_equal(c4.dec.degree, [2.5, 2.5])\n\n    # multiplication\n    c5 = c1 * 2\n    npt.assert_array_almost_equal(c5.ra.degree, [2, 4])\n    npt.assert_array_almost_equal(c5.dec.degree, [6, 8])\n\n    c6 = ICRS(np.array([1, 2])*u.deg, np.array([3, 4])*u.deg) * 2\n    npt.assert_array_almost_equal(c6.ra.degree, [2, 4])\n    npt.assert_array_almost_equal(c6.dec.degree, [6, 8])\n\n    # division\n    c7 = c1 / 2\n    npt.assert_array_almost_equal(c7.ra.degree, [0.5, 1])\n    npt.assert_array_almost_equal(c7.dec.degree, [1.5, 2])\n\n    # ensure that the unit is preserved\n\n    # addition with scalar\n    c8 = c1 + 1*u.deg\n    npt.assert_array_almost_equal(c8.ra.degree, [2, 3])\n    npt.assert_array_", "def test_array_output():\n    c1 = ICRS(np.array([1, 2, 3])*u.deg, np.array([4, 5, 6])*u.deg)\n\n    # Make sure to test various output formats\n    assert c1.to_string('hmsdms')\n    assert c1.to_string('decimal')\n    assert c1.to_string('equinox2000')\n    assert c1.to_string('fk5')\n    \n    # also test to_table\n    assert c1.to_table()\n\n    # test with custom unit names\n    c2 = ICRS(np.array([1, 2, 3])*u.degree, np.array([4, 5, 6])*u.deg)\n    assert c2.to_string('hmsdms', unit_name='arcseconds')\n", "def test_array_repr_latex():\n    \"\"\"\n    Test that array representations are correct in latex format.\n    \"\"\"\n    ra = np.array([0, 1, 2]) * u.degree\n    dec = np.array([0, 1, 2]) * u.degree\n    c = ICRS(ra, dec)\n\n    assert c._repr_latex_() == r'ICRS(RA = [0', r'$^\\circ$', '1', r'$^\\circ$', '2', r'$^\\circ$]', r'DEC = [0', r'$^\\circ$', '1', r'$^\\circ$',\n                                      '2', r'$^\\circ$])'\n    # Verify that the output string is correctly encoded as a LaTeX string.\n\n", "def test_array_errors():\n    with pytest.raises(ValueError):\n        ICRS(ra=np.array([1, 2, 3])*u.deg, dec=np.array([4, 5]) * u.deg)\n    with pytest.raises(ValueError):\n        ICRS(ra=np.array([1, 2])*u.deg, dec=np.array([4, 5, 6]) * u.deg)\n\n    with pytest.raises(TypeError):\n        ICRS(ra=np.array([1, 2])*u.deg, dec=np.array([4, 5]) * u.m)\n\n\n    # This test is here to make sure we are using the same logic\n    # for checking invalid input arrays that we do in other parts\n    # of the Astropy codebase.\n\n    with pytest.raises(TypeError):\n        ICRS(ra=np.array(1, 2), dec=np.array([3, 4]) * u.deg)\n\n    with pytest.raises(TypeError):\n        ICRS(ra=np.array([1, 2])*u.deg, dec=np.array([3, 4]) * \"deg\")\n", "def test_array_string_conversion():\n    c1 = ICRS([1, 2]*u.deg, [3, 4]*u.deg, distance=[5, 6]*u.kpc)\n\n    # Test to_string\n    \n    string_rep = c1.to_string() \n    assert isinstance(string_rep, str)\n\n    #  Test for all the elements\n    assert '1' in string_rep \n    assert '2' in string_rep \n    assert '3' in string_rep \n    assert '4' in string_rep \n    assert '5' in string_rep\n    assert '6' in string_rep \n\n    #  Test to_string with different formats\n    string_rep_rad = c1.to_string(format='rad')\n    assert 'rad' in string_rep_rad\n\n    string_rep_hms = c1.to_string(format='hms')\n    assert 'h' in string_rep_hms \n    assert 'm' in string_rep_hms \n    assert 's' in string_rep_hms \n\n    # Check that to_string() works with arrays of coordinates\n    c2 = ICRS([1, 2]*u.deg, [3, 4]*u.deg, distance=[5, 6]*u.kpc)\n\n    string_rep_array = c2.to_string() \n    assert isinstance(string_rep_array, str)\n    assert len(string_rep_array.split('\\n')) == 2\n\n    #  Test that the output string for each coordinate in the array is correct\n\n"], "sample_103": ["    def test_annotate_with_f_expressions_in_aggregation(self):\n        qs = Book.objects.annotate(\n            sum_of_rating_and_price=Sum(F('rating') + F('price'))\n        ).values('sum_of_rating_and_price')\n\n        self.assertEqual(list(qs) , [{'sum_of_rating_and_price': 614}])\n", "    def test_ordering_after_annotation(self):\n        books = Book.objects.annotate(\n            average_price=Avg('price')\n        ).order_by('average_price')\n        self.assertQuerysetEqual(\n            books, [\n                {'name': 'The Definitive Guide to Django: Web Development Done Right', 'pages': 604, 'rating': 4.0, 'price': Decimal('49.99'), 'average_price': Decimal('49.99')},\n                {'name': 'Artificial Intelligence: A Modern Approach', 'pages': 800, 'rating': 4.0, 'price': Decimal('50.00'), 'average_price': Decimal('50.00')},\n                # ... other books\n            ], lambda b: (b['name'], b['average_price'])\n        )\n\n\n\n", "    def test_value_functions_with_aggregation(self):\n        author_qs = Author.objects.values('name').annotate(\n            average_age=Avg('age'),\n            median_age=Median('age', output_field=IntegerField()),\n        )\n\n        authors = list(author_qs)\n        self.assertEqual(\n            {a.name: {\n                'average_age': 37,\n                'median_age': 37\n            } for a in authors}, {\n                'Adrian Holovaty': {'average_age': 37, 'median_age': 37},\n                'Brad Dayley': {'average_age': 37, 'median_age': 37},\n                'Jacob Kaplan-Moss': {'average_age': 37, 'median_age': 37},\n                'James Bennett': {'average_age': 37, 'median_age': 37},\n                'Jeffrey Forcier': {'average_age': 37, 'median_age': 37},\n                'Paul Bissex': {'average_age': 37, 'median_age': 37},\n                'Peter Norvig': {'average_age': 37, 'median_age': 37},\n                'Stuart Russell': {'average_age': 37, 'median_age': 37},\n                'Wesley J. Chun': {'average_age': 37, 'median_age': 37},\n            })\n", "    def test_subquery_no_output_field(self):\n        with self.assertRaisesMessage(\n            FieldError, 'Output field must be specified for Subquery annotation'\n        ):\n            Publisher.objects.annotate(\n                latest_book_pubdate=Subquery(Book.objects.all().order_by('-pubdate')[:1])\n            )\n        with self.assertRaisesMessage(\n            FieldError, 'Output field must be specified for Subquery annotation'\n        ):\n            Publisher.objects.annotate(\n                latest_book_pubdate=Subquery(Book.objects.all().order_by('-pubdate')[:1], output_field=Object)\n            )\n\n\n\n\n", "    def test_subquery_with_complex_expressions(self):\n        avg_rating_qs = Book.objects.values('publisher').annotate(\n            avg_rating=Avg('rating', output_field=DecimalField())\n        )\n        avg_rating_subquery = Subquery(avg_rating_qs)\n\n        publisher_qs = Publisher.objects.annotate(\n            avg_rating_above_threshold=\n            When(F('avg_rating') > 4.2, then=F('avg_rating'),\n                 otherwise=F('avg_rating').cast(DecimalField())),\n            filtered_avg_rating=Case(\n                When(\n                    avg_rating_subquery > 4.5, then='High Quality'\n                ),\n                default='Not High Quality',\n                output_field=CharField(),\n            )\n        ).values('name', 'avg_rating_above_threshold', 'filtered_avg_rating')\n\n        self.assertEqual(\n            list(publisher_qs), [\n                {\n                    \"name\": 'Apress',\n                    \"avg_rating_above_threshold\": Decimal(\"29.69\"),\n                    \"filtered_avg_rating\": 'Not High Quality'\n                },\n                {\n                    \"name\": 'Prentice Hall',\n                    \"avg_rating_above_threshold\": Decimal(\"23.09\"),\n                    \"filtered_avg_rating\": 'Not High Quality'\n                },\n                {\n                    \"name\": 'Morgan Kaufmann',\n                    \"avg_rating_above_threshold\": Decimal(\"30.0\"),\n                    \"filtered_avg_rating\": 'High Quality'\n                },\n                {\n                    \"name\": 'Sams',\n                    \"avg_rating_above_threshold\": Decimal(\"23.0\"),\n                    \"filtered_avg_rating\": 'Not High Quality'\n                }\n            ]\n        )\n"], "sample_709": ["def test_pytester_assert_outcomes_ignore_warnings(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import warnings\n\n            warnings.warn(UserWarning(\"some custom warning\"))\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--disable-warnings\")\n    result.assert_outcomes(passed=1)\n", "compilation error", "def test_pytester_assert_outcomes_mixed_outcomes(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            assert False\n\n            assert True\n\n            warnings.warn(UserWarning(\"Expected warning\"))\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1, failed=1, warnings=1)\n", "def test_pytester_makepyfile_invalid_path(pytester: Pytester) -> None:\n    with pytest.raises(ValueError, match=\"Invalid path provided\"):\n        pytester.makepyfile(\"/invalid/path/to/file\")\n", "compilation error"], "sample_439": ["    def test_legend_tag_with_custom_renderer(self):\n        class CustomRenderer(DjangoTemplates):\n            form_template_name = \"forms_tests/form_snippet.html\"\n\n        class CustomFrameworkForm(FrameworkForm):\n            template_name = \"forms_tests/legend_test.html\"\n            required_css_class = \"required\"\n\n        get_default_renderer.cache_clear()\n        f = CustomFrameworkForm(renderer=CustomRenderer())\n        self.assertHTMLEqual(\n            str(f),\n            '<div class=\"fieldWrapper\"><label for=\"id_name\" class=\"required\">Name:</label>'\n            '<input type=\"text\" name=\"name\" required id=\"id_name\"></div>'\n            '<div class=\"fieldWrapper\"><legend for=\"id_language\" class=\"required\">Language:</legend>'\n            '<input type=\"text\" name=\"language\" required id=\"id_language\"></div>',\n        )\n", "    def test_custom_error_messages(self):\n        class UserRegistration(Form):\n            username = CharField(\n                max_length=10,\n                widget=TextInput,\n                error_messages={\n                    \"required\": \"Username is required.\",\n                    \"max_length\": \"Username must be at most 10 characters.\",\n                },\n            )\n            password1 = CharField(\n                widget=PasswordInput,\n                error_messages={\"required\": \"Password is required.\"},\n            )\n            password2 = CharField(widget=PasswordInput)\n\n        f = UserRegistration({\"username\": \"\", \"password1\": \"\"})\n        self.assertHTMLEqual(\n            f.as_p(),\n            '<p><label for=\"id_username\">Username:</label>'\n            '<input type=\"text\" name=\"username\" maxlength=\"10\" required id=\"id_username\">'\n            '<ul class=\"errorlist\"><li>Username is required.</li></ul></p>'\n            '<p><label for=\"id_password1\">Password:</label>'\n            '<input type=\"password\" name=\"password1\" required id=\"id_password1\">'\n            '<ul class=\"errorlist\"><li>Password is required.</li></ul></p>'\n            '<p><label for=\"id_password2\">Password (again):</label>'\n            '<input type=\"password\" name=\"password2\" id=\"id_password2\"></p>',\n        )\n\n\n\n", "    def test_template_rendering_with_form_field_errors(self):\n        class UserRegistration(Form):\n            username = CharField(max_length=10)\n            password1 = CharField(widget=PasswordInput)\n            password2 = CharField(widget=PasswordInput)\n\n                if (\n                    self.cleaned_data.get(\"password1\")\n                    and self.cleaned_data.get(\"password2\")\n                    and self.cleaned_data[\"password1\"] != self.cleaned_data[\"password2\"]\n                ):\n                    raise ValidationError(\"Please make sure your passwords match.\")\n                return self.cleaned_data\n\n        t = Template(\n            \"<form>\"\n            \"{{ form.username.errors.as_ul }}\"\n            \"<p><label>Your username: {{ form.username }}</label></p>\"\n            \"{{ form.password1.errors.as_ul }}\"\n            \"<p><label>Password: {{ form.password1 }}</label></p>\"\n            \"{{ form.password2.errors.as_ul }}\"\n            \"<p><label>Password (again): {{ form.password2 }}</label></p>\"\n            '<input type=\"submit\" required>'\n            \"</form>\"\n        )\n        f = UserRegistration({\"username\": \"django\", \"password1\": \"foo\", \"password2\": \"bar\"}, auto_id=False)\n        self.assertHTMLEqual(\n            t.render(Context({\"form\": f})),\n            \"<form>\"\n            \"<p><label>Your username: \"\n            '<input type=\"text\" name=\"username\" value=\"django\" maxlength=\"10\" required></label></p>'\n            \"<p><label>Password: \"\n            '<input type=\"password\" name=\"password1\" required></label></p>'\n            \"<p><label>Password (again): \"\n            '<input type=\"password\" name=\"password2\" required></label></p>'\n            '<input type=\"submit\" required>'\n            \"</form>\",\n        )\n", "    def test_nested_formsets(self):\n        class ArticleForm(forms.Form):\n            title = forms.CharField(max_length=100)\n            authors = forms.ModelMultipleChoiceField(\n                queryset=Author.objects.all(),\n                widget=forms.CheckboxSelectMultiple,\n            )\n\n        class AuthorForm(forms.Form):\n            name = forms.CharField(max_length=50)\n\n        class ArticleWithAuthorsForm(forms.Form):\n            article = ArticleForm()\n            authors = forms.ModelMultipleChoiceField(\n                queryset=Author.objects.all(),\n                widget=forms.CheckboxSelectMultiple,\n            )\n\n        f = ArticleWithAuthorsForm()\n        self.assertHTMLEqual(f.as_p(), \n            \"<p>Title: <input type=\\\"text\\\" name=\\\"article-title\\\" maxlength=\\\"100\\\"></p>\"\n            \"<p>Authors: <input type=\\\"checkbox\\\" name=\\\"authors\\\" value=\\\"1\\\"> Author 1</p>\"\n            \"<p>Authors: <input type=\\\"checkbox\\\" name=\\\"authors\\\" value=\\\"2\\\"> Author 2</p>\")\n\n\n\n", "    def test_non_required_field_without_label(self):\n        class CommentForm(Form):\n            name = CharField(required=False)\n            email = EmailField()\n            comment = CharField()\n\n        f = CommentForm()\n        self.assertHTMLEqual(\n            f.as_p(),\n            '<p>Name: <input type=\"text\" name=\"name\" maxlength=\"50\"></p>'\n            '<p>Email: <input type=\"email\" name=\"email\" required></p>'\n            '<p>Comment: <input type=\"text\" name=\"comment\" required></p>',\n        )\n\n\n"], "sample_498": ["def test_legend_markers_from_pathcollection():\n    _markers = ['o', 's', 'D']\n    fig, ax = plt.subplots()\n    paths = [plt.PathCollection([plt.Path.unit_circle]),\n             plt.PathCollection([plt.Path.wedge(0, 0, 1, 0.5, 0.5, 0.5)]),\n             plt.PathCollection([plt.Path.offset(plt.Path.unit_circle(), (0.2, 0.2))])]\n    labels = [\"foo\", \"bar\", \"xyzzy\"]\n    markers = [path.get_paths()[0].get_verts()[0] for path in paths]\n    legend = ax.legend(paths, labels)\n\n    new_markers = [line.get_marker() for line in legend.get_lines()]\n    new_labels = [text.get_text() for text in legend.get_texts()]\n\n    assert markers == new_markers == _markers\n    assert labels == new_labels\n", "def test_legend_markers_from_patchcollection():\n    # Test that markers can be copied for legend patches (#17960)\n    _markers = ['.', '*', 'v']\n    fig, ax = plt.subplots()\n    patches = [mpatches.Patch(facecolor=f'C{i}', label=f'Patch {i}')\n               for i in range(len(_markers))]\n    legend = ax.legend(patches, ['Patch {}'.format(i) for i in range(len(_markers))])\n\n    new_markers = [line.get_marker() for line in legend.get_lines()]\n    new_labels = [text.get_text() for text in legend.get_texts()]\n\n    assert new_markers == []\n    assert new_labels == ['Patch 0', 'Patch 1', 'Patch 2']\n", "def test_legend_fontsize_from_figure():\n    fig = plt.figure(figsize=(8, 6))\n    fig.set_size_inches(8, 6)\n    ax = fig.add_subplot(111)\n    ax.plot([0, 1], [0, 1], label=\"Line\")\n    legend = ax.legend()\n    assert legend.get_fontsize() == mpl.rcParams['legend.fontsize']\n\n\n", "def test_legend_markers_from_patchcollection():\n    # Test that markers can be copied for legend patches (#17960)\n    patches = [mpatches.Patch(color='red', label=str(i)) for i in range(3)]\n    fig, ax = plt.subplots()\n    legend = ax.legend(patches)\n\n    new_patches = [patch for patch in legend.legendHandles]\n    assert [patch.get_facecolor() for patch in new_patches] == [patch.get_facecolor() for patch in patches]\n", "def test_legend_handles_order():\n    fig, axes = plt.subplots(nrows=2, ncols=1)\n    axes[0].plot([1, 2], [3, 4], label='line 1')\n    axes[1].plot([1, 2], [3, 4], label='line 2')\n\n    legend = fig.legend()\n\n    # Ensure the legend order matches the order of the plots\n    assert legend.get_texts()[0].get_text() == 'line 1'\n    assert legend.get_texts()[1].get_text() == 'line 2'\n"], "sample_430": ["    def test_ignore_auto_created_fields(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    \"Person\",\n                    fields=[\n                        (\n                            \"id\",\n                            models.AutoField(\n                                primary_key=True,\n                                verbose_name=\"ID\",\n                                auto_created=True,\n                            ),\n                        ),\n                        (\"name\", models.CharField(max_length=200)),\n                    ],\n                )\n            ]\n\n        migration = Migration(\"0001_initial\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"person\")\n", "    def test_rename_field_with_index_together(self):\n        initial_author = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n                (\"age\", models.IntegerField()),\n            ],\n            {\n                \"index_together\": {(\"name\",)},\n            },\n        )\n        updated_author = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"nickname\", models.CharField(max_length=200)),\n                (\"age\", models.IntegerField()),\n            ],\n            {\n                \"index_together\": {(\"nickname\",)},\n            },\n        )\n        changes = self.get_changes(\n            [initial_author], [updated_author], MigrationQuestioner({\"ask_rename\": True})\n        )\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(\n            changes,\n            \"testapp\",\n            0,\n            [\"RenameField\", \"AlterIndexTogether\"],\n        )\n        self.assertOperationAttributes(\n            changes,\n            \"testapp\",\n            0,\n            0,\n            name=\"Author\",\n            field_name=\"name\",\n            new_name=\"nickname\",\n        )\n        self.assertOperationAttributes(\n            changes,\n            \"testapp\",\n            0,\n            1,\n            name=\"Author\",\n            index_together=set([(\"nickname\",)]),\n        )\n", "    def test_rename_field_and_index_together_order(self):\n        initial_author = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n                (\"age\", models.IntegerField()),\n            ],\n            {\n                \"index_together\": {(\"name\", \"age\")},\n            },\n        )\n        author_new_constraints = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n                (\"new_age\", models.IntegerField()),\n            ],\n            {\n                \"index_together\": {(\"name\", \"new_age\")},\n            },\n        )\n        changes = self.get_changes(\n            [initial_author],\n            [author_new_constraints],\n            MigrationQuestioner({\"ask_rename\": True}),\n        )\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(\n            changes,\n            \"testapp\",\n            0,\n            [\"RenameField\", \"AlterIndexTogether\"],\n        )\n\n\n\n", "    def test_alter_index_together_with_field_addition(self):\n        changes = self.get_changes(\n            [AutodetectorTests.author_empty, self.book_index_together],\n            [AutodetectorTests.author_empty, self.book_index_together_3],\n        )\n        self.assertNumberMigrations(changes, \"otherapp\", 1)\n        self.assertOperationTypes(\n            changes,\n            \"otherapp\",\n            0,\n            [\"AlterIndexTogether\", \"AddField\"],\n        )\n        self.assertOperationAttributes(\n            changes,\n            \"otherapp\",\n            0,\n            0,\n            name=\"book\",\n            index_together={(\"title\", \"newfield\")},\n        )\n\n", "    def test_suggest_name_with_custom_operations(self):\n        class CustomOperation(migrations.MigrationOperation):\n                super().__init__(name, **kwargs)\n\n        class Migration(migrations.Migration):\n            operations = [\n                CustomOperation(\"my_custom_operation\", model_name=\"person\", field_name=\"name\")\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"person_my_custom_operation\")\n"], "sample_629": ["def test_expand_modules_basic():\n    files_or_modules = [\"mymodule.py\", \"subpackage.subpackage2.mymodule\"]\n    ignore_list = []\n    ignore_list_re = []\n    ignore_list_paths_re = []\n    expected_result = [\n        {\n            \"path\": \"mymodule.py\",\n            \"name\": \"mymodule\",\n            \"isarg\": True,\n            \"basepath\": \"mymodule.py\",\n            \"basename\": \"mymodule\",\n        },\n        {\n            \"path\": \"subpackage/subpackage2/mymodule.py\",\n            \"name\": \"subpackage.subpackage2.mymodule\",\n            \"isarg\": False,\n            \"basepath\": \"subpackage/subpackage2/__init__.py\",\n            \"basename\": \"subpackage.subpackage2\",\n        },\n    ]\n    result, errors = expand_modules(files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re)\n    assert result == expected_result\n    assert errors == []\n", "compilation error", "def test_expand_modules_simple():\n    files_or_modules = [\"module1.py\", \"module2/module3.py\"]\n    ignore_list = []\n    ignore_list_re = []\n    ignore_list_paths_re = []\n    expected_result = [\n        {\"path\": \"module1.py\", \"name\": \"module1\", \"isarg\": True, \"basepath\": \"module1.py\", \"basename\": \"module1\"},\n        {\"path\": \"module2/module3.py\", \"name\": \"module2.module3\", \"isarg\": True, \"basepath\": \"module2/module3.py\", \"basename\": \"module2.module3\"},\n    ]\n    result, errors = expand_modules(files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re)\n    assert result == expected_result\n    assert not errors\n", "    def test_expand_modules_no_init():\n        ignore_list = [\"test_utils.py\"]\n        ignore_list_re = [re.compile(\".*tests/.*\")]\n        files_or_modules = [\"test_utils.py\"]\n        result, errors = expand_modules(\n            files_or_modules, ignore_list, ignore_list_re, []\n        )\n        assert len(result) == 0\n        assert len(errors) == 1\n        assert errors[0][\"key\"] == \"fatal\"\n        assert errors[0][\"mod\"] == \"test_utils\"\n\n\n\n", "def test_expand_modules_recursive():\n    files_or_modules = [\"module_a\", \"module_b/module_c\", \"module_d/__init__.py\"]\n    ignore_list = []\n    ignore_list_re = []\n    ignore_list_paths_re = []\n    expected_result = [\n        {\"path\": \"module_a.py\", \"name\": \"module_a\", \"isarg\": True, \"basepath\": \"module_a\", \"basename\": \"module_a\"},\n        {\"path\": \"module_b/module_c.py\", \"name\": \"module_b.module_c\", \"isarg\": True, \"basepath\": \"module_b\", \"basename\": \"module_b\"},\n        {\"path\": \"module_d/__init__.py\", \"name\": \"module_d\", \"isarg\": True, \"basepath\": \"module_d\", \"basename\": \"module_d\"},\n    ]\n\n    result, errors = expand_modules(files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re)\n    assert result == expected_result\n    assert len(errors) == 0\n"], "sample_917": ["    def members(role, tag):\n        pattern = (r'{role}-role:.*?'\n                   r'<(?P<tag>{tag}) .*?member=[\"\\'](?P<members>.*?)[\"\\'].*?>'\n                   r'.*'\n                   r'</(?P=tag)>').format(role=role, tag=tag)\n        result = re.search(pattern, output)\n        expect = '''\\", "compilation error", "def test_xref_target_id_namespace(app, status, warning):\n    app.builder.build_all()\n\n    test = 'xref_target_id_namespace.html'\n    output = (app.outdir / test).read_text()\n\n        pattern = r'{name}.*?id=\"{expected_id}\"'.format(name=name, expected_id=expected_id)\n        result = re.search(pattern, output)\n        expect = '''\\", "compilation error", "    def check_xref_consistency(role, tag, expected_classes):\n        classes_found = classes(role, tag)\n        assert classes_found == expected_classes\n"], "sample_361": ["    def test_urlize_with_trim_url_limit(self):\n        items = (\n            (\n                'This is a long URL: http://www.example.com/this-is-a-super-long-url-that-needs-to-be-truncated',\n                'This is a long URL: http://www.example.com/this-is-a-super-l...',\n            ),\n            (\n                'This is another long URL: http://www.example.com/this-is-a-super-long-url-that-needs-to-be-truncated',\n                'This is another long URL: http://www.example.com/this-is-a-super-l...',\n            ),\n        )\n        for value, output in items:\n            with self.subTest(value=value):\n                self.assertEqual(urlize(value, trim_url_limit=25), output)\n\n\n", "    def test_urlize_special_chars(self):\n        items = (\n            (\"Search for google.com/example?q=%20and+see.\",\n             'Search for <a href=\"http://google.com/example?q=%20and%2Bsee.\">google.com/example?q=%20and%2Bsee.</a>'),\n            (\"Search for google.com/?q=foo\\u00E9bar\",\n             'Search for <a href=\"http://google.com/?q=foo\\u00E9bar\">google.com/?q=foo\\u00E9bar</a>'),\n            (\"Search for google.com/?q=! and see.\",\n             'Search for <a href=\"http://google.com/?q=!\">google.com/?q=!</a> and see.'),\n        )\n        for value, output in items:\n            with self.subTest(value=value):\n                self.assertEqual(urlize(value), output)\n\n\n", "    def test_urlize_with_trim_url_limit(self):\n        items = (\n            ('This is a long URL: http://example.com/this-is-a-long-url-that-will-be-trimmed',\n             'This is a long URL: http://example.com/this-...'),\n            ('This is another long URL: http://example.com/this-is-another-long-url-that-will-be-trimmed',\n             'This is another long URL: http://example.com/this-...'),\n        )\n        for value, output in items:\n            with self.subTest(value=value):\n                self.assertEqual(urlize(value, \n                                        trim_url_limit=20), output)\n", "    def test_urlize_with_trailing_punctuation(self):\n        items = (\n            (\n                'This is a URL: http://example.com/.com!',\n                'This is a URL: <a href=\"http://example.com/.com\">http://example.com/.com</a>!'\n            ),\n            (\n                'This is a URL: http://example.com/.com?q=1&lt!',\n                'This is a URL: <a href=\"http://example.com/.com?q=1%3C\">http://example.com/.com?q=1&lt</a>!'\n            ),\n            (\n                'This is a URL: http://example.com/.com(test).com',\n                'This is a URL: <a href=\"http://example.com/.com(test).com\">http://example.com/.com(test).com</a>'\n            ),\n        )\n        for value, output in items:\n            with self.subTest(value=value):\n                self.assertEqual(urlize(value), output)\n", "    def test_urlize_nofollow(self):\n        with self.subTest(nofollow=True):\n            result = urlize('Search for <a href=\"http://google.com/\">google.com</a>', nofollow=True)\n            self.assertEqual(result, 'Search for <a href=\"http://google.com/\" rel=\"nofollow\">google.com</a>')\n"], "sample_1124": ["def test_FracElement_compose():\n    F, x,y,z = field(\"x,y,z\", QQ)\n\n    f = (x**2 + 3*y)/(x*z)\n    g = (x + y)/z\n    h = f.compose(g)\n\n    assert h.numer == (\n        (F.ring.gens[0]**2 + 3*F.ring.gens[1]) * F.ring.gens[0] +\n        (F.ring.gens[0]**2 + 3*F.ring.gens[1]) * F.ring.gens[1]\n    )\n    assert h.denom == (F.ring.gens[0] * F.ring.gens[2]) * (F.ring.gens[0] + F.ring.gens[1])\n", "def test_FracElement_compose():\n    F, x,y,z = field(\"x,y,z\", ZZ)\n    f = (x**2 + 3*y)/z\n    g = (x*y + z) / (x*z + 1)\n\n    with raises(NotImplementedError):\n        f.compose(g)\n", "def test_FracElement_compose():\n    F, x,y,z = field(\"x,y,z\", ZZ)\n    f = (x**2 + y)/(z + 1)\n    g = x**2\n\n    with raises(NotImplementedError):\n        f.compose(g)\n", "def test_FracElement_compose():\n    F, x, y, z = field(\"x,y,z\", ZZ)\n\n    f = (x**2 + 3*y)/z\n    g = (y + 1)/x\n\n    assert f.compose(g) == ((g.numer*g.numer + 3*g.numer)/g.denom) / z\n", "def test_FracElement_compose():\n    F, x,y = field(\"x,y\", ZZ)\n    f = (x + y)/(x - y)\n\n    with raises(NotImplementedError):\n        f.compose(x, y)\n\n    Fxyzt, x,y,z,t = field(\"x,y,z,t\", ZZ)\n    f = (x*y + z)/(x*z - y*t)\n\n    with raises(NotImplementedError):\n        f.compose(x, y, z, t) \n\n\n"], "sample_1021": ["def test_quaternion_multiplication():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(5, 6, 7, 8)\n\n    assert q1 * q2 == Quaternion(-26, 43, 58, -10)\n    assert q2 * q1 == Quaternion(-26, 43, 58, -10)\n    assert q1 * (q2 * q1) == Quaternion(-536, -737, -972, -186)\n    assert (q1 * q2) * q1 == Quaternion(1278, -1495, -1970, -396)\n\n\n\n    q3 = Quaternion(0, 1, 0, 0)\n    assert q1 * q3 == Quaternion(0, 3, -4, 2)\n    assert q3 * q1 == Quaternion(0, 3, -4, 2)\n", "def test_quaternion_multiplication():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(5, 6, 7, 8)\n\n    assert q1 * q2 == Quaternion(-26, 4, 58, -2)\n    assert q2 * q1 == Quaternion(-26, 4, 58, -2)\n    assert (q1 * q2) * q3 == Quaternion(-1, 2, 3, 4)\n\n\n\n", "def test_quaternion_real_field():\n    q1 = Quaternion(1, 2, 3, 4, real_field = True)\n    assert q1.real_field is True\n    assert q1.inverse() == Quaternion(-1/30, -2/30, -3/30, 1/30) \n\n\n", "def test_quaternion_multiplication():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(5, 6, 7, 8)\n    q3 = Quaternion(9, 10, 11, 12)\n\n    assert q1 * q2 == Quaternion(-56, 40, 28, 0)\n    assert q2 * q1 == Quaternion(-56, 40, 28, 0)  \n\n    assert q1 * q3 == Quaternion(-36, -16, -54, -18) \n    assert q3 * q1 == Quaternion(-36, -16, -54, -18)\n\n    assert q1 * q1 == Quaternion(-23, 0, 0, 0)\n    assert q1 * Quaternion(0, 0, 0, 0) == Quaternion(0, 0, 0, 0)\n\n    assert Quaternion(0, 0, 0, 0) * q1 == Quaternion(0, 0, 0, 0)\n", "def test_quaternion_mul():\n    q = Quaternion(x, y, z, w)\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(5, 6, 7, 8)\n    q3 = Quaternion(0, 0, 0, 0)\n    \n    assert q * q1 == Quaternion(-w*q1.a + x*q1.b + y*q1.c + z*q1.d,\n                                  w*q1.b + x*q1.a - y*q1.d + z*q1.c,\n                                  w*q1.c - x*q1.d + y*q1.a + z*q1.b,\n                                  w*q1.d + x*q1.c - y*q1.b + z*q1.a)\n\n    assert q1 * q2 == Quaternion(-6w + 2*x*5 + 3*y*6 + 4*z*7,\n                                  6w + 2*x*6 - 3*y*7 + 4*z*8,\n                                  7w + 2*x*7 + 3*y*8 - 4*z*5,\n                                  8w + 2*x*8 - 3*y*5 + 4*z*6)\n\n    assert q * q3 == q3\n    assert q3 * q == q3\n    assert q1 * q3 == q3\n    assert q3 * q1 == q3\n\n\n\n"], "sample_490": ["    def test_expressions_with_include(self):\n        msg = \"UniqueConstraint.include cannot be used with expressions.\"\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                Lower(\"field_1\"), name=\"test_func_include\", include=[\"field_2\"]\n            )\n\n", "    def test_unique_constraint_with_nulls_distinct_and_condition(self):\n        with self.assertRaisesMessage(\n            ValueError,\n            \"UniqueConstraint with nulls_distinct and condition are mutually exclusive.\",\n        ):\n            models.UniqueConstraint(\n                fields=[\"name\"],\n                name=\"name_nulls_distinct_condition\",\n                nulls_distinct=True,\n                condition=models.Q(color__isnull=True),\n            )\n", "    def test_expressions_with_include(self):\n        msg = (\n            \"UniqueConstraint.include cannot be used with expressions. \"\n            \"Use include_fields='...' when defining a constraint on \"\n            \"a table without a model.\"\n        )\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                Lower(\"field\"),\n                name=\"test_func_include\",\n                include=[\"field2\"],\n            )\n", "    def test_expressions_with_include(self):\n        msg = (\n            \"UniqueConstraint.include cannot be used with expressions. Use \"\n            \"django.contrib.postgres.indexes.OpClass() instead.\"\n        )\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                Lower(\"field\"),\n                name=\"test_func_include\",\n                include=[\"field2\"],\n            )\n", "    def test_expressions_with_include(self):\n        msg = \"UniqueConstraint.include cannot be used with expressions.\"\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                Lower(\"field\"),\n                name=\"test_func_include\",\n                include=[\"extra_field\"],\n            )\n"], "sample_55": ["    def test_provided_pk_is_not_used_when_not_exists(self):\n        post_data = {'name': '1'}\n        response = self.client.post(reverse('admin:admin_views_explicitlyprovidedpk_add'), post_data)\n        self.assertEqual(response.status_code, 302)\n\n        response = self.client.post(reverse('admin:admin_views_explicitlyprovidedpk_change', args=(10000,)), post_data)\n        self.assertEqual(response.status_code, 200)\n\n\n", "    def test_new_object_instance(self):\n        response = self.client.get(reverse('admin:admin_views_getformsetswithinlines_add'))\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, '<input type=\"hidden\" name=\"next\" value=\"/admin/getformsetswithinlines/add/\">')\n\n\n\n", "    def test_pk_from_url(self):\n        response = self.client.get(reverse('admin:admin_views_explicitlyprovidedpk_change', args=(1,)))\n        self.assertEqual(response.status_code, 200)\n\n\n", "    def test_change_view_form_and_formsets_run_validation(self):\n        \"\"\"\n        Issue #20522\n        Verifying that if the parent form fails validation, the inlines also\n        run validation even if validation is contingent on parent form data\n        \"\"\"\n        pwdc = ParentWithDependentChildren.objects.create(some_required_info=6, family_name='Test1')\n        # The form validation should fail because 'some_required_info' is\n        # not included on the parent form, and the family_name of the parent\n        # does not match that of the child\n        post_data = {\n            'family_name': 'Test2',\n            'dependentchild_set-TOTAL_FORMS': '1',\n            'dependentchild_set-INITIAL_FORMS': '0',\n            'dependentchild_set-MAX_NUM_FORMS': '1',\n            'dependentchild_set-0-id': '',\n            'dependentchild_set-0-parent': str(pwdc.id),\n            'dependentchild_set-0-family_name': 'Test1',\n        }\n        response = self.client.post(\n            reverse('admin:admin_views_parentwithdependentchildren_change', args=(pwdc.id,)), post_data\n        )\n        self.assertFormError(response, 'adminform', 'some_required_info', ['This field is required.'])\n        self.assertFormsetError(\n            response, 'inline_admin_formset', 0, None,\n            ['Children must share a family name with their parents in this contrived test case']\n        )\n\n\n\n", "    def test_adding_object_with_inlines(self):\n        response = self.client.get(reverse('admin:admin_views_inlines_add'))\n        self.assertContains(response, '<input type=\"hidden\" name=\"next\" value=\"%s\">' % reverse('admin:admin_views_inlines_changelist'))\n        post_data = {\n            'inline_child_set-TOTAL_FORMS': '1',\n            'inline_child_set-INITIAL_FORMS': '0',\n            'inline_child_set-MAX_NUM_FORMS': '1',\n            'inline_child_set-0-child_field': 'Inline child data',\n        }\n        response = self.client.post(reverse('admin:admin_views_inlines_add'), post_data)\n        self.assertRedirects(response, reverse('admin:admin_views_inlines_changelist'))\n\n\n"], "sample_1090": ["compilation error", "compilation error", "def test_nan():\n    assert isinstance(S.NaN, float)\n    assert not S.NaN.is_real\n    assert not S.NaN.is_number\n    assert S.NaN != S.NaN\n    assert S.NaN == S.NaN is False\n    assert S.NaN < S.NaN is False\n    assert S.NaN > S.NaN is False\n    assert S.NaN <= S.NaN is False\n    assert S.NaN >= S.NaN is False\n", "compilation error", "compilation error"], "sample_699": ["    def test_is_setup_py_checks_filename(tmp_path: Path) -> None:\n        setup_py = tmp_path.joinpath(\"setup.py\")\n        setup_py.write_text('')\n        assert _is_setup_py(setup_py)\n", "def test_ignore_non_python_files(pytester: Pytester) -> None:\n    pytester.maketxtfile(\"not_a_python_file\", \"this is a test\")\n    pytester.maketxtfile(\"test_module.py\", \"\"\"\n        'This is a docstring'\n    \"\"\")\n    result = pytester.runpytest(\"--doctest-modules\")\n    result.stdout.fnmatch_lines([\"*test_module.py:*\"])\n\n\n\n", "    def test_is_setup_py_ignores_content(tmp_path: Path) -> None:\n        setup_py = tmp_path.joinpath(\"setup.py\")\n        setup_py.write_text(content, \"utf-8\")\n        assert _is_setup_py(setup_py)\n", "    def test_is_setup_py_is_a_setup_py_with_shebang(tmp_path: Path) -> None:\n        setup_py = tmp_path.joinpath(\"setup.py\")\n        setup_py.write_text(\"#!/usr/bin/env python\\nfrom setuptools import setup; setup(name=\\\"foo\\\")\", \"utf-8\")\n        assert _is_setup_py(setup_py)\n", "    def test_is_setup_py_is_a_relative_path(tmp_path: Path) -> None:\n        setup_py = tmp_path.joinpath(\"subfolder\", \"setup.py\")\n        setup_py.write_text(\n            f'from setuptools import setup; setup(name=\"foo\")', \"utf-8\"\n        )\n        assert _is_setup_py(setup_py)  \n"], "sample_264": ["    def test_encode_empty_cookie_data(self):\n        storage = self.get_storage()\n        response = self.get_response()\n        set_cookie_data(storage, [], encode_empty=True)\n        self.assertEqual(stored_cookie_messages_count(storage, response), 0)\n\n", "    def test_not_finished_sentinel(self):\n        storage = self.storage_class(self.get_request())\n        messages = ['test', 'me']\n        set_cookie_data(storage, messages[:-1], encode_empty=True)\n        self.assertEqual(list(storage), [])\n        self.assertTrue(storage.used)\n        storage.add(constants.INFO, 'test')\n        storage.update(self.get_response())\n        self.assertEqual(list(storage), ['test', 'me'])\n\n", "    def test_not_finished_sentinel(self):\n        storage = self.get_storage()\n        response = self.get_response()\n\n        # Store some messages, and then add the not_finished sentinel.\n        messages = ['message 1', 'message 2']\n        storage.add(constants.INFO, messages[0])\n        storage.add(constants.INFO, messages[1])\n        storage._store(messages + [CookieStorage.not_finished], response, remove_oldest=False)\n\n        # Retrieve the messages, expecting one at a time.\n        self.assertEqual(list(storage), messages[:1])\n        remaining_messages = storage._store(messages[1:], response, remove_oldest=False)\n        self.assertEqual(len(remaining_messages), 1)\n        self.assertEqual(remaining_messages[0].message, messages[1])\n        self.assertEqual(list(storage), messages)\n\n        # Check that the message count in the cookie is 0 after all messages are retrieved.\n        self.assertEqual(stored_cookie_messages_count(storage, response), 0)\n\n\n\n", "    def test_not_finished_sentinel(self):\n        storage = self.get_storage()\n        response = self.get_response()\n\n        # Add a few messages and save them to the cookie.\n        for i in range(5):\n            storage.add(constants.INFO, f'message {i}')\n\n        # The cookie should now contain all 5 messages.\n        self.assertEqual(stored_cookie_messages_count(storage, response), 5)\n\n        # Now, simulate adding more messages than will fit in the cookie\n        # and check that the not_finished sentinel is set.\n        for i in range(5):\n            storage.add(constants.INFO, f'message {i + 5}')\n\n        # The cookie should now contain only 5 messages (the ones that fit) +\n        # the not_finished sentinel.\n        self.assertEqual(stored_cookie_messages_count(storage, response), 5)\n", "    def test_encode_empty(self):\n        storage = self.get_storage()\n        # When storing as a cookie, the cookie has constant overhead.\n        # When there is nothing in the messages, it should not store anything\n        response = self.get_response()\n\n        set_cookie_data(storage, [], encode_empty=True)\n        self.assertEqual(stored_cookie_messages_count(storage, response), 0)\n        set_cookie_data(storage, [], encode_empty=False)\n        self.assertEqual(stored_cookie_messages_count(storage, response), 0)\n\n\n\n"], "sample_215": ["    def test_sensitive_variables_decorator_with_arguments(self):\n        @sensitive_variables(\n            ['username', 'secret_key'],\n            extra_sensitive_settings=['PASSWORD'],\n        )\n            return username, password, secret_key, kwargs\n\n        with self.settings(DEBUG=True, USERNAME='john.doe', PASSWORD='super_secret', SECRET_KEY='my_secret_key'):\n            result = test_func('john.doe', 'super_secret', 'my_secret_key', extra='value')\n            self.assertEqual(result, ('john.doe', 'XXXXXXXXXXXXXXXXXXXX', 'XXXXXXXXXXXXXXXXXXXX', {'extra': 'value'}))\n\n        with self.settings(DEBUG=False, USERNAME='john.doe', PASSWORD='super_secret', SECRET_KEY='my_secret_key'):\n            result = test_func('user', 'secret', 'key')\n            self.assertEqual(result, ('user', 'XXXXXXXXXXXXXXXXXXXX', 'XXXXXXXXXXXXXXXXXXXX', {}))\n", "    def test_sensitive_arguments_not_called(self):\n        msg = (\n            'sensitive_arguments() must be called to use it as a '\n            'decorator, e.g., use @sensitive_arguments(), not '\n            '@sensitive_arguments.'\n        )\n        with self.assertRaisesMessage(TypeError, msg):\n            @sensitive_arguments\n                return index_page(password, secret_key)\n", "    def test_sensitive_variables_decorator_with_args(self):\n        @sensitive_variables\n            return password + SECRET_KEY\n        request = self.rf.get('/test_view/')\n        with self.settings(DEBUG=True):\n            with self.settings(SECRET_KEY='secret_key'):\n                result = test_func('password', 'SECRET_KEY')\n                self.assertNotContains(str(result), 'passwordsecret_key')\n", "    def test_sensitive_variables_handles_nested_dictionaries(self):\n        @sensitive_variables\n            return data\n        data = {'key1': {'key2': 'secret'}}\n        result = test_func(data)\n        self.assertEqual(result, {'key1': {'key2': 'XXXXXXXXXXXXXXXXX'}})\n\n", "    def test_sensitive_variables_decorator_with_custom_filter(self):\n        class CustomSafeExceptionReporterFilter(SafeExceptionReporterFilter):\n            cleansed_substitute = 'CUSTOM_SENSITIVE_VALUE'\n\n        with self.settings(DEFAULT_EXCEPTION_REPORTER_FILTER='%s.CustomSafeExceptionReporterFilter' % __name__):\n            @sensitive_variables\n                return 'Success'\n\n            response = self.client.get('/test_view/', HTTP_FOO='bar')\n            self.assertIn(\n                'CUSTOM_SENSITIVE_VALUE',\n                response.content.decode('utf-8'),\n            )\n"], "sample_693": ["    def test_do_cleanups_on_teardown_failure(self):\n        testpath = pytester.makepyfile(\n            \"\"\"\n            import unittest\n            class MyTestCase(unittest.TestCase):\n                values = []\n                        self.values.append(1)\n                    self.addCleanup(cleanup)\n                    pass\n                    assert False\n                assert MyTestCase.values == [1]\n            \"\"\"\n        )\n        reprec = pytester.inline_run(testpath)\n        passed, skipped, failed = reprec.countoutcomes()\n        assert failed == 1\n        assert passed == 1\n\n\n\n", "    def test_cleanups_called_on_teardown_failure(self):\n        class MyTestCase(unittest.TestCase):\n            values = []\n                    self.values.append(1)\n                self.addCleanup(cleanup)\n                pass\n                assert False\n        reprec = self.inline_run()\n        assert reprec.countoutcomes()[2] == 2\n", "    def test_do_cleanups_on_teardown_failure(self):\n        testpath = pytester.makepyfile(\n            \"\"\"\n            import unittest\n            class MyTestCase(unittest.TestCase):\n                values = []\n                        self.values.append(1)\n                    self.addCleanup(cleanup)\n                    pass\n                    assert False\n                assert MyTestCase.values == [1]\n            \"\"\"\n        )\n        reprec = pytester.inline_run(testpath)\n        passed, skipped, failed = reprec.countoutcomes()\n        assert failed == 2\n        assert passed == 1\n", "    def test_do_cleanups_on_teardown_failure(self):\n        testpath = pytester.makepyfile(\n            \"\"\"\n            import unittest\n            class MyTestCase(unittest.TestCase):\n                values = []\n                        self.values.append(1)\n                    self.addCleanup(cleanup)\n                    pass\n                    assert False\n                assert MyTestCase.values == [1]\n            \"\"\"\n        )\n        reprec = pytester.inline_run(testpath)\n        passed, skipped, failed = reprec.countoutcomes()\n        assert failed == 2\n        assert passed == 1\n", "def test_do_cleanups_on_teardown_failure(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n        class MyTestCase(unittest.TestCase):\n            values = []\n                    self.values.append(1)\n                self.addCleanup(cleanup)\n                assert False\n                pass\n                pass\n            assert MyTestCase.values == [1, 1]\n    \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    passed, skipped, failed = reprec.countoutcomes()\n    assert passed == 2\n    assert failed == 1\n\n\n"], "sample_174": ["    def test_sql_flush_with_tables(self):\n        Author.objects.create(name='Test Author')\n        Book.objects.create(title='Test Book', author=Author.objects.get(name='Test Author'))\n\n\n        with self.subTest('single table'):\n            tables = [('auth_user', 'auth_group', 'auth_group_permissions', 'auth_permission', 'django_migrations')]  \n            sql = connection.ops.sql_flush(no_style(), tables)\n            self.assertGreater(len(sql), 0)\n\n\n        with self.subTest('all tables'):\n            tables = connection.ops.get_tables()\n            sql = connection.ops.sql_flush(no_style(), tables)\n            self.assertGreater(len(sql), 0)\n\n", "    def test_sql_flush_truncate_tables(self):\n        Author.objects.create(name='Author A')\n        Book.objects.create(title='Book A', author=Author.objects.get(name='Author A'))\n\n        with transaction.atomic():\n            sql_list = connection.ops.sql_flush(no_style(), [Author, Book], reset_sequences=True)\n\n        self.assertEqual(len(sql_list), 4)  # 2 truncate statements and 2 resequence statements\n\n\n", "    def test_sql_flush_with_tables(self):\n        with transaction.atomic(using=self.db):\n            Author.objects.create(name='Author 1')\n            Book.objects.create(title='Book 1', author=Author.objects.get(name='Author 1'))\n\n        tables = ['author', 'book']\n        sql = connection.ops.sql_flush(\n            no_style(),\n            tables=tables, \n            reset_sequences=True\n        )\n\n        self.assertIsNotEqual(sql, [])\n        self.assertEqual(len(sql), 2)\n\n\n", "    def test_sql_flush_with_tables(self):\n        with transaction.atomic(\n            using=self.connection.alias\n        ):\n            Author.objects.create(name='John Doe')\n            Book.objects.create(title='A Book', author=Author.objects.last())\n\n        sql_list = connection.ops.sql_flush(no_style(), [Author, Book])\n        self.assertEqual(len(sql_list), 2)\n        self.assertIn('TRUNCATE TABLE django_auth_group', sql_list)\n\n\n", "    def test_sql_flush_with_tables(self):\n        # Create a table\n        with transaction.atomic(using=self.connection.alias):\n            self.connection.cursor().execute(\n                'CREATE TABLE test_flush_table (id INTEGER PRIMARY KEY, name TEXT)'\n            )\n\n        # Test sql_flush with tables\n        flush_sql = connection.ops.sql_flush(no_style(), ['test_flush_table'])\n        self.assertTrue(len(flush_sql) > 0)\n        self.assertIn(\n            'DROP TABLE test_flush_table', flush_sql[0]\n        )\n\n        with transaction.atomic(using=self.connection.alias):\n            # Drop the table to cleanup\n            self.connection.cursor().execute('DROP TABLE test_flush_table')\n"], "sample_920": ["    def test_inheritance_docs(self):\n\n        docstring = \"\"\"\n        Base Class Example\n\n        This is a base class with some inherited methods.\n\n        Methods\n        -------\n        method_a :\n\n            This method does something.\n        method_b\n\n            This method does something else.\n        \"\"\"\n        config = Config()\n\n        expected = \"\"\"\n        Base Class Example\n\n        This is a base class with some inherited methods.\n\n        :Methods:\n            * :meth:`method_a`\n                This method does something.\n            * :meth:`method_b`\n                This method does something else.\n        \"\"\"\n\n        actual = str(NumpyDocstring(docstring, config))\n\n        self.assertEqual(expected, actual)\n", "    def test_parameter_ordering(self):\n        docstring = \"\"\"", "    def test_unicode_characters(self):\n        docstring = \"\"\"", "    def test_restructured_text_in_parameters(self):\n        docstring = \"\"\"", "    def test_example_code_block(self):\n        docstring = \"\"\""], "sample_104": ["    def test_staticfiles_storage_cache_invalidation(self):\n        \"\"\"\n        Ensure the cache is invalidated correctly when files change.\n        \"\"\"\n        temp_files = [\n            self._get_filename_path(\"cached/styles.css\"),\n            self._get_filename_path(\"cached/styles.css.map\"),\n            self._get_filename_path(\"cached/styles_insensitive.css\"),\n        ]\n\n        # Write some files\n        for file in temp_files:\n            with open(file, 'w') as f:\n                f.write(\"Content\\n\")\n\n        # Collect static files and ensure they are cached\n        self.run_collectstatic()\n\n        # Modify one of the files\n        with open(temp_files[0], 'w') as f:\n            f.write(\"Changed content\\n\")\n\n        # Collect static files again. This should re-hash the modified file\n        self.run_collectstatic()\n\n        # Check if the modified file is re-hashed\n        for file in temp_files:\n            hash_key = storage.staticfiles_storage.hash_key(file)\n            self.assertEqual(\n                storage.staticfiles_storage.hashed_files.get(hash_key),\n                file,\n            )\n\n\n\n\n", "    def test_manifest_hash_order_dependency(self):\n        self.run_collectstatic(clear=True)\n\n        file1_path = self._get_filename_path('file1.css')\n        file2_path = self._get_filename_path('file2.css')\n        with open(file1_path, 'w') as f:\n            f.write(\"url('file2.css')\")\n        with open(file2_path, 'w') as f:\n            f.write(\"/* placeholder */\")\n\n        self.run_collectstatic()\n        file1_hashed_path = self.hashed_file_path('test/file1.css')\n        file2_hashed_path = self.hashed_file_path('test/file2.css')\n        with storage.staticfiles_storage.open(file1_hashed_path) as f:\n            file1_content = f.read()\n        with storage.staticfiles_storage.open(file2_hashed_path) as f:\n            file2_content = f.read()\n\n        self.assertIn(file2_hashed_path, file1_content)\n        self.assertEqual(file2_content, \"/* placeholder */\")\n", "compilation error", "    def test_hashed_cache_invalidation(self):\n        \"\"\"\n        Test that the hashed cache is invalidated correctly when files are modified\n        and the `collectstatic` command is run again.\n        \"\"\"\n        initial_file_path = self._get_filename_path('initial.css')\n        with open(initial_file_path, 'w') as f:\n            f.write('/* Initial content */\\n')\n        self.run_collectstatic()\n\n        # Assert that the hashed file exists and has the expected content.\n        hashed_file_path = self.hashed_file_path('test/initial.css')\n        with open(hashed_file_path, 'r') as f:\n            initial_content = f.read()\n        self.assertIn('Initial content', initial_content)\n\n        # Modify the original file.\n        with open(initial_file_path, 'w') as f:\n            f.write('/* Modified content */\\n')\n\n        # Run collectstatic again.\n        self.run_collectstatic()\n\n        # Assert that the hashed file now has the modified content.\n        with open(hashed_file_path, 'r') as f:\n            modified_content = f.read()\n        self.assertIn('Modified content', modified_content)\n        self.assertNotEqual(initial_content, modified_content)\n", "    def test_manifest_with_modified_files(self):\n        \"\"\"\n        Test manifest generation with a file that has changed after initial hashing.\n        \"\"\"\n        relpath = self._get_filename_path(\"cached/styles.css\")\n        original_contents = '/* This is some CSS */'\n        with open(relpath, 'w') as f:\n            f.write(original_contents)\n        self.run_collectstatic()\n        first_hash = storage.staticfiles_storage.hashed_files[\n            relpath\n        ]\n\n        contents = '/* This is some updated CSS */'\n        with open(relpath, 'w') as f:\n            f.write(contents)\n        self.run_collectstatic()\n\n        second_hash = storage.staticfiles_storage.hashed_files[relpath]\n        self.assertNotEqual(first_hash, second_hash)\n\n\n"], "sample_23": ["def test_angle_preserve_unit(value, expected_value):\n    angle = Angle(value)\n    assert angle.unit == value.unit\n    assert angle.value == expected_value.value\n", "def test_latitude_out_of_limits_close_to_limits(value):\n    \"\"\"\n    Test that values very close to pi/2 are rejected for latitude.\n    \"\"\"\n    with pytest.raises(ValueError, match=r\"Latitude angle\\(s\\) must be within.*\"):\n        Latitude(value, u.rad, dtype=None)\n", "def test_latitude_too_large_value(value):\n    with pytest.raises(ValueError, match=r\"Latitude angle\\(s\\) must be within.*\"):\n        Latitude(value, u.rad)\n", "def test_longitude_wrap_at_multiples_of_2pi(value):\n    lon = Longitude(value, u.deg)\n    assert lon == Longitude(value % 360, u.deg)\n\n\n", "def test_wrap_at_with_various_wrap_angles(\n    value, expected_value, wrap_angle, expected_wrap_value"], "sample_892": ["def test_adaboost_with_weighted_samples():\n    # Check that sample weights are properly used in AdaBoost.\n    rng = np.random.RandomState(0)\n    X = rng.randn(100, 2)\n    y = rng.randint(0, 2, size=100)\n\n    # Generate some sample weights\n    sample_weights = rng.randn(100)\n\n    # Train AdaBoost with and without sample weights\n    boost_no_weights = AdaBoostClassifier().fit(X, y)\n    boost_with_weights = AdaBoostClassifier().fit(X, y, sample_weight=sample_weights)\n\n    # Check if the trained models have different prediction results\n    assert not np.array_equal(boost_no_weights.predict(X), boost_with_weights.predict(X))\n\n", "def test_adaboost_with_sample_weight_and_sparse_data(algorithm):\n    rng = np.random.RandomState(0)\n    X = rng.randn(100, 5)\n    y = rng.randint(0, 2, size=100)\n\n    # Create sparse data\n    X_sparse = sparse_matrix(X, dtype=np.float32)\n\n    # Create sample weights\n    sample_weight = rng.rand(100)\n\n    # Instantiate AdaBoostClassifier\n    clf = AdaBoostClassifier(\n        estimator=DecisionTreeClassifier(), algorithm=algorithm, random_state=0\n    )\n\n    # Train and predict with sparse data and sample weights\n    clf.fit(X_sparse, y, sample_weight=sample_weight)\n    predictions = clf.predict(X_sparse)\n\n\n", "def test_adaboost_with_categorical_features():\n    from sklearn.datasets import make_classification\n\n    X, y = make_classification(\n        n_samples=100,\n        n_features=5,\n        n_informative=2,\n        n_redundant=1,\n        random_state=42,\n        categorical_features=[1, 2],\n    )\n\n    # Convert categorical features to ordinal values\n    X = pd.DataFrame(X, columns=[f\"feature_{i}\" for i in range(X.shape[1])])\n    X = pd.get_dummies(X, columns=[X.columns[1], X.columns[2]])\n\n    # Test with AdaBoostClassifier\n    clf = AdaBoostClassifier(DecisionTreeClassifier(random_state=42))\n    clf.fit(X, y)\n    assert clf.score(X, y) > 0.8\n\n    # Test with AdaBoostRegressor\n    reg = AdaBoostRegressor(DecisionTreeRegressor(random_state=42))\n    reg.fit(X, y)\n    assert reg.score(X, y) > 0.8\n\n\n\n", "def test_adaboost_with_sparse_sample_weights():\n    # Check that sample_weights can be sparse matrices.\n\n    from scipy.sparse import csc_matrix\n\n    rng = np.random.RandomState(42)\n    X = rng.rand(100, 10)\n    y = rng.randint(2, size=100)\n\n    sample_weight = csc_matrix((np.ones(100), (np.arange(100), np.arange(100))))\n\n    clf = AdaBoostClassifier(\n        base_estimator=DecisionTreeClassifier(max_depth=1), random_state=42\n    )\n    clf.fit(X, y, sample_weight=sample_weight)\n\n    # Verify weights are correctly applied\n    assert (clf.estimator_weights_ != 0).any()\n", "def test_adaboost_estimator_with_predict_proba_error():\n    # Check that AdaBoost raises an error when base estimator doesn't have predict_proba\n\n    class NoPredictProbaEstimator(BaseEstimator):\n            pass\n\n            return np.zeros(X.shape[0])\n\n    with pytest.raises(ValueError, match=\"estimator must have predict_proba\"):\n        AdaBoostClassifier(estimator=NoPredictProbaEstimator())\n"], "sample_1152": ["def test_powsimp_with_gamma():\n    x, y = symbols('x y')\n    assert powsimp(gamma(x)*gamma(y)) == gamma(x)*gamma(y)\n    assert powsimp(gamma(x)**2) == gamma(x)**2\n    assert powsimp(gamma(x)*gamma(y)*x*y) == x*y*gamma(x)*gamma(y)\n    assert powsimp(gamma(x)*gamma(y)/(x*y)) == gamma(x)*gamma(y)/(x*y)\n\n\n", "def test_issue_19765():\n    x, y = symbols('x y', positive=True)\n    assert powsimp(x**(1/2)*y**(1/2)*(x**2 + y**2)**(1/2)) == \\\n        (x*y)**(1/2)\n", "def test_issue_17524_negative_base():\n    a = symbols(\"a\", negative=True)\n    e = (-1 - a**2)*sqrt(1 + a**2)\n    assert signsimp(powsimp(e)) == signsimp(e) ==  -(a**2 + 1)**(S(3)/2) \n", "compilation error", "def test_issue_18195():\n    z = symbols('z', complex=True)\n    assert powsimp(z**(-1)) == 1/z\n    assert powsimp(z**(-2)) == 1/z**2\n    assert powsimp(z.conjugate()**-1) == z.conjugate().real/z.real\n"], "sample_112": ["    def test_cell_count(self):\n        inline_admin_form = self.create_mock_inline_admin_form()\n        count = cell_count(inline_admin_form)\n        self.assertEqual(count, 6)\n", "    def test_cell_count(self):\n        inline_formset = Article.objects.create(title='Test Article')\n        inline_admin_form = self.mock_inline_formset(Article.objects.get(title='Test Article'))\n        count = cell_count(inline_admin_form)\n        self.assertEqual(count, 2)  \n", "    def test_cell_count(self):\n        inline_admin_form = ArticleAdmin._meta.get_field('questions').formset.form\n        context = {'inline_admin_form': inline_admin_form}\n        cell_count_result = cell_count(context['inline_admin_form'])\n        self.assertEqual(cell_count_result, 3)\n", "    def test_date_hierarchy(self):\n        article = Article.objects.create(title=\"Article One\", content=\"Content for Article One\", date=datetime.date(2023, 10, 26))\n        article2 = Article.objects.create(title=\"Article Two\", content=\"Content for Article Two\", date=datetime.date(2023, 10, 27))\n        request = self.factory.get(reverse('admin:admin_views_article_changelist'))\n        request.user = self.superuser\n        admin = ArticleAdmin(Article, site)\n        response = admin.changelist_view(request)\n        response.render()\n        self.assertContains(response, date_hierarchy(admin, 'date', request))\n        self.assertContains(response, 'override-date_hierarchy')\n", "    def test_prepopulated_fields_js(self):\n        article = Article.objects.create(\n            title='Test Article', content='Test content', date=datetime.date(2023, 10, 26)\n        )\n        request = self.factory.get(reverse('admin:admin_views_article_change', args=[article.pk]))\n        request.user = self.superuser\n        admin = ArticleAdmin(Article, site)\n        response = admin.change_view(request, str(article.pk))\n        response.render()\n        self.assertContains(response, 'override-prepopulated_fields_js')\n        self.assertContains(response, 'name=\"prepopulated_fields\"')\n        self.assertContains(response, 'name=\"prepopulated_fields_json\"')\n"], "sample_229": ["    def test_union_with_complex_expressions(self):\n        qs1 = Number.objects.filter(num__lte=1).annotate(count=Value(0, IntegerField()))\n        qs2 = Number.objects.filter(num__gte=2, num__lte=3).annotate(\n            count=F('num')\n        )\n        self.assertNumbersEqual(qs1.union(qs2).order_by('count', 'num'), [0, 2, 3])\n", "    def test_union_with_multiple_fields(self):\n        qs1 = Number.objects.filter(num__lte=1).values('num', 'other_num')\n        qs2 = Number.objects.filter(num__gte=2, num__lte=3).values('num', 'other_num')\n        self.assertCountEqual(list(qs1.union(qs2)), [\n            (0, 10), (1, 9), (2, 8), (3, 7)\n        ])\n", "    def test_union_with_subquery(self):\n        qs1 = Number.objects.filter(num__lt=5)\n        qs2 = Number.objects.filter(num__gt=5).union(\n            Number.objects.filter(num__lt=5).values_list('num')\n        )\n        self.assertNumbersEqual(qs1.union(qs2), [0, 1, 2, 3, 4, 6, 7, 8, 9])\n\n\n", "    def test_union_with_multiple_models(self):\n        Number.objects.bulk_create(Number(num=i, other_num=10 - i) for i in range(10))\n        ReservedName.objects.bulk_create([\n            ReservedName(name=f'name{i}', order=i) for i in range(10)\n        ])\n        qs1 = Number.objects.filter(num__lte=5).values('num')\n        qs2 = ReservedName.objects.filter(order__gte=5).values('order')\n        self.assertEqual(list(qs1.union(qs2).values_list('num', 'order')), [(0, None), (1, None), (2, None), (3, None), (4, None), (5, 5), (6, 6), (7, 7), (8, 8), (9, 9)])\n\n", "    def test_union_with_different_field_names(self):\n        ReservedName.objects.create(name='a', order=2)\n        qs1 = Number.objects.all()\n        qs2 = ReservedName.objects.all()\n        self.assertEqual(list(qs1.union(qs2).values('num', 'pk', 'name').order_by('num')),\n                         [(0, 0, None), (1, 1, None), (2, 2, None), (3, 3, None), (4, 4, None), (5, 5, None),\n                          (6, 6, None), (7, 7, None), (8, 8, None), (9, 9, None), (2, None, 'a')])\n"], "sample_70": ["    def test_fast_delete_related_m2m(self):\n        \"\"\"\n        Test that related M2M models are fast-deleted without extra queries.\n        \"\"\"\n        m = M.objects.create()\n        r = R.objects.create()\n        m.m2m.add(r)\n        # One to delete m, one to fast-delete r from m.m2m\n        self.assertNumQueries(2, m.delete)\n        self.assertFalse(M.objects.exists())  # Should be deleted\n        self.assertFalse(R.objects.exists())  # Should be deleted via fast delete\n\n\n\n", "    def test_fast_delete_related_m2m(self):\n        t = M2MTo.objects.create()\n        f1 = M2MFrom.objects.create()\n        f2 = M2MFrom.objects.create()\n        f1.m2m.add(t)\n        f2.m2m.add(t)\n        # 1 to delete f1, 1 to fast-delete m2m for f1 + 1 for f2\n        self.assertNumQueries(3, f1.delete)\n        self.assertEqual(M2MFrom.objects.count(), 1)\n        self.assertEqual(M2MTo.objects.count(), 0)\n\n\n\n", "    def test_fast_delete_m2m_with_custom_manager(self):\n        class M2MManager(models.Manager):\n                return super().get_queryset().filter(pk__gt=1)\n\n        class M2MModel(models.Model):\n            name = models.CharField(max_length=255)\n\n        class M2MFrom(models.Model):\n            m2m = models.ManyToManyField(M2MModel, through='M2MThrough', manager=M2MManager)\n\n        class M2MThrough(models.Model):\n            m2m_from = models.ForeignKey(M2MFrom, on_delete=models.CASCADE)\n            m2m_to = models.ForeignKey(M2MModel, on_delete=models.CASCADE)\n\n        m2m_to1 = M2MModel.objects.create(name='to1')\n        m2m_to2 = M2MModel.objects.create(name='to2')\n        m2m_to3 = M2MModel.objects.create(name='to3')\n\n        m2m_from = M2MFrom.objects.create()\n        m2m_from.m2m.add(m2m_to1, m2m_to2)\n        self.assertNumQueries(1, m2m_from.delete)  \n", "    def test_related_obj_exists_when_target_deleted(self):\n        u = User.objects.create()\n        a = Avatar.objects.create(avatar_user=u)\n        u.delete()\n        self.assertTrue(Avatar.objects.filter(pk=a.pk).exists())\n", "    def test_fast_delete_related_m2m_reverse(self):\n        t = M2MTo.objects.create()\n        f = M2MFrom.objects.create()\n        f.m2m.add(t)\n        # 1 to delete t, 1 to fast-delete t's m_set\n        self.assertNumQueries(2, t.delete)\n"], "sample_508": ["def test_set_transform():\n    ax = plt.axes()\n    p = mpatches.Ellipse((0, 0), 1, 1)\n    p.set_transform(ax.transData)\n\n    assert isinstance(p.get_transform(), mtransforms.Affine2D)\n    assert p.get_transform().inverse() == ax.transData.inverted()\n\n\n\n", "        def set(self, **kwargs):\n            \"\"\"Documented and overwritten.\"\"\"\n            pass\n", "        def set_myparam3(self, val):\n            pass\n", "def test_property_with_alias_is_listed_in_getp():\n\n    class MyArtist(martist.Artist):\n            return self.prop_value\n\n            self.prop_value = value\n\n        my_prop = property(get_my_prop, set_my_prop)\n\n        my_alias = property(get_my_prop, set_my_prop,\n                           doc=\"Alias for my_prop\")\n\n    art = MyArtist()\n    art.prop_value = 10\n    \n    expected_output = \"\"\"\n    my_prop: 10\n    my_alias: 10\n    \"\"\"\n    with io.StringIO() as sio:\n        print(martist.getp(art, 'my_prop', 'my_alias'), file=sio)\n    assert sio.getvalue().strip() == expected_output\n", "def test_set_with_property_and_standard_setter():\n    class MyArtist4(martist.Artist):\n            self._prop = 0\n\n            return self._prop\n\n        prop = property(get_prop)\n\n            self._prop = val\n\n    art = MyArtist4()\n    art.set_prop = lambda self, val : None  # Overwrite the standard setter\n    art.set('prop', 5)  # Should not raise an error\n    assert art.prop == 5\n"], "sample_746": ["def test_log_loss_with_multiclass_labels():\n    y_true = np.array([0, 1, 2, 0, 1])\n    y_pred = np.array([\n        [0.1, 0.8, 0.1],\n        [0.05, 0.9, 0.05],\n        [0.01, 0.005, 0.985],\n        [0.1, 0.8, 0.1],\n        [0.05, 0.9, 0.05]\n    ])\n    loss = log_loss(y_true, y_pred, normalize=True)\n    assert_almost_equal(loss, 0.8317582)\n", "def test_log_loss_with_invalid_y_true_pred_types():\n    y_true = [1, 0, 2]\n    y_pred = [[0.2, 0.7], [0.6, 0.5], [0.4, 0.1]]\n\n    for y_true_type, y_pred_type in [\n        (np.array(y_true), list(y_pred)),\n        (list(y_true), np.array(y_pred)),\n        (dict(zip(range(len(y_true)), y_true)), np.array(y_pred)),\n        (np.array(y_true), dict(zip(range(len(y_pred[0])), y_pred[0]))),\n    ]:\n        error_msg = (\"y_true and y_pred should be numpy arrays or lists, \"\n                     \"not {} and {}\".format(type(y_true_type),\n                                          type(y_pred_type)))\n        assert_raise_message(TypeError, error_msg, log_loss, y_true_type,\n                             y_pred_type)\n\n    # test that the function works with a single label with labels argument \n    y_true = [1]\n    y_pred = [[0.2, 0.7]]\n    with assert_warns(UserWarning,\n                      message=\"y_true contains only one label. Please \"\n                             \"provide the true labels explicitly through \"\n                             \"the labels argument.\"):\n        log_loss(y_true, y_pred, labels=[1])\n\n\n\n", "compilation error", "def test_log_loss_multiclass_with_binary_labels():\n    y_true = np.array([0, 1, 2])\n    y_pred = np.array([[0.1, 0.9], [0.6, 0.4], [0.2, 0.8]])\n    loss = log_loss(y_true, y_pred, labels=[0, 1, 2])\n    assert_almost_equal(loss, 0.4690917)\n\n\n    y_true = np.array(['spam', 'ham', 'spam'])\n    y_pred = np.array([[0.1, 0.9], [0.6, 0.4], [0.2, 0.8]])\n    loss = log_loss(y_true, y_pred, labels=['spam', 'ham'])\n    assert_almost_equal(loss, 0.4690917)\n\n", "def test_log_loss_multiclass_with_binary_labels():\n    y_true = np.array([0, 1, 2])\n    y_pred = np.array([[0.1, 0.9], [0.6, 0.4], [0.2, 0.8]])\n    loss = log_loss(y_true, y_pred)\n    assert_almost_equal(loss, 0.6904911)\n\n    y_true = np.array([0, 1, 2])\n    y_pred = np.array([[0.1, 0.9], [0.6, 0.4], [0.2, 0.8]])\n    loss = log_loss(y_true, y_pred, labels=[0, 1, 2])\n    assert_almost_equal(loss, 0.6904911)\n"], "sample_877": ["def test_isotonic_regression_memory_usage():\n    X = np.arange(100_000)\n    y = np.random.randn(100_000)\n    ir = IsotonicRegression()\n\n    # Check that the memory usage is reasonable\n    _ = ir.fit(X, y)\n", "def test_isotonic_regression_with_missing_values():\n    # Test that isotonic regression handles missing values correctly\n\n    # Create data with missing values\n    X = np.array([0, 1, 2, 3, 4, 5, 6], dtype=np.float64)\n    y = np.array([1, 2, np.nan, 4, 5, np.nan, 7], dtype=np.float64)\n\n    # Fit the model with and without handling missing values\n    model_no_handling = IsotonicRegression().fit(X, y)\n    model_with_handling = IsotonicRegression(fill_missing='ffill').fit(X, y)\n\n    # Check that the models behave differently\n    assert_array_almost_equal(model_no_handling.predict(X), np.array([1, 2, np.nan, 4, 5, np.nan, 7], dtype=np.float64))\n    assert_array_almost_equal(model_with_handling.predict(X), np.array([1, 2, 3, 4, 5, 6, 7], dtype=np.float64)) \n", "def test_isotonic_regression_with_missing_values():\n    # Test handling of missing values in the input data\n    rng = np.random.RandomState(42)\n\n    X = np.arange(10)\n    y = rng.randn(10)\n\n    # Introduce missing values\n    X[2] = np.nan\n    y[5] = np.nan\n\n    # Test with default behavior (remove missing values)\n    iso = IsotonicRegression().fit(X, y)\n    assert np.isnan(iso.X_thresholds_[2]).any()\n    assert np.isnan(iso.y_thresholds_[5]).any()\n\n    # Test with `fill_value` parameter\n    fill_value = -1\n    iso2 = IsotonicRegression(fill_value=fill_value).fit(X, y)\n    assert not np.isnan(iso2.X_thresholds_[2])\n    assert not np.isnan(iso2.y_thresholds_[5])\n    assert iso2.X_thresholds_[2] == fill_value\n    assert iso2.y_thresholds_[5] == fill_value\n", "def test_isotonic_regression_with_empty_data():\n    # Test handling of empty input data\n    X = []\n    y = []\n\n    with pytest.raises(ValueError) as excinfo:\n        isotonic_regression(X, y)\n\n    assert \"Input data cannot be empty\" in str(excinfo.value)\n\n    with pytest.raises(ValueError) as excinfo:\n        IsotonicRegression().fit(X, y)\n\n    assert \"Input data cannot be empty\" in str(excinfo.value)\n\n\n\n", "def test_isotonic_regression_fit_transform_with_sample_weight():\n    rng = np.random.RandomState(42)\n    n_samples = 30\n    X = rng.rand(n_samples)\n    y = rng.randn(n_samples)\n    sample_weight = rng.rand(n_samples)\n    \n    ir = IsotonicRegression()\n    \n    y_fitted = ir.fit_transform(X, y, sample_weight=sample_weight)\n    \n    # make sure fit_transform produces the same result as fitting then transforming\n    ir.fit(X, y, sample_weight=sample_weight)\n    y_fitted_2 = ir.transform(X)\n    \n    assert_array_almost_equal(y_fitted, y_fitted_2)\n"], "sample_698": ["def test_log_format_timestamp() -> None:\n    from _pytest.logging import ColoredLevelFormatter\n    import time\n\n    logfmt = \"%(asctime)s %(filename)-25s %(lineno)4d %(levelname)-8s %(message)s\"\n\n    record = logging.LogRecord(\n        name=\"dummy\",\n        level=logging.INFO,\n        pathname=\"dummypath\",\n        lineno=10,\n        msg=\"Test Message\",\n        args=(),\n        exc_info=None,\n    )\n    record.created = time.time()\n\n    tw = TerminalWriter()\n    tw.hasmarkup = True\n    formatter = ColoredLevelFormatter(tw, logfmt)\n    output = formatter.format(record)\n    assert output.startswith(time.ctime(record.created))\n\n\n", "def test_percentstylemultiline_with_complex_log_record() -> None:\n    from _pytest.logging import PercentStyleMultiline\n\n    logfmt = \"%(filename)-25s %(lineno)4d %(levelname)-8s %(message)s\"\n\n    record: Any = logging.LogRecord(\n        name=\"dummy\",\n        level=logging.WARNING,\n        pathname=\"dummypath\",\n        lineno=10,\n        msg=(\n            \"This is a warning message with \"\n            \"multiple lines and nested formatting: \\n\"\n            \"  - First line\\n\"\n            \"  - Second line\\n\"\n            \"    - Nested line\"\n        ),\n        args=(),\n        exc_info=None,\n    )\n    # this is called by logging.Formatter.format\n    record.message = record.getMessage()\n\n    ai_on_style = PercentStyleMultiline(logfmt, True)\n    output = ai_on_style.format(record)\n    assert output == (\n        \"dummypath                   10 \\x1b[33mWARNING   This is a warning message with \"\n        \"multiple lines and nested formatting: \\n\"\n        \"\\x1b[10;30m                          - First line\\n\"\n        \"\\x1b[10;30m                          - Second line\\n\"\n        \"\\x1b[10;30m                                 - Nested line\"\n    )\n\n    ai_off_style = PercentStyleMultiline(logfmt, False)\n    output = ai_off_style.format(record)\n    assert output == (\n        \"dummypath                   10 WARNING   This is a warning message with \"\n        \"multiple lines and nested formatting: \\n\"\n        \"  - First line\\n\"\n        \"  - Second line\\n\"\n        \"    - Nested line\"\n    )\n\n\n\n", "def test_log_format_with_unicode_characters() -> None:\n    logfmt = \"%(filename)-25s %(lineno)4d %(levelname)-8s %(message)s\"\n\n    record = logging.LogRecord(\n        name=\"dummy\",\n        level=logging.INFO,\n        pathname=\"dummypath\",\n        lineno=10,\n        msg=\"This message includes \\u2665 (heart) emoji.\",\n        args=(),\n        exc_info=None,\n    )\n\n    tw = TerminalWriter()\n    tw.hasmarkup = True\n    formatter = ColoredLevelFormatter(tw, logfmt)\n    output = formatter.format(record)\n    assert (\n        output\n        == \"dummypath                   10 \\x1b[32mINFO    This message includes \u2764\ufe0f emoji.\"\n    )\n    tw.hasmarkup = False\n    formatter = ColoredLevelFormatter(tw, logfmt)\n    output = formatter.format(record)\n    assert output == \"dummypath                   10 INFO     This message includes \u2764\ufe0f emoji.\"\n\n\n", "def test_auto_indent_with_kwargs() -> None:\n    from _pytest.logging import PercentStyleMultiline\n\n    logfmt = \"%(filename)-25s %(lineno)4d %(levelname)-8s %(message)s\"\n    record: Any = logging.LogRecord(\n        name=\"dummy\",\n        level=logging.INFO,\n        pathname=\"dummypath\",\n        lineno=10,\n        msg=\"Test Message line1\\nline2\",\n        args=(),\n        exc_info=None,\n    )\n    record.message = record.getMessage()\n\n    ai_on_style = PercentStyleMultiline(logfmt, auto_indent=True)\n    output = ai_on_style.format(record)\n    assert output == (\n        \"dummypath                   10 INFO     Test Message line1\\n\"\n        \"                                        line2\"\n    )\n\n    ai_off_style = PercentStyleMultiline(logfmt, auto_indent=False)\n    output = ai_off_style.format(record)\n    assert output == \"dummypath                   10 INFO     Test Message line1\\nline2\"\n\n    ai_none_style = PercentStyleMultiline(logfmt, auto_indent=None)\n    output = ai_none_style.format(record)\n    assert output == \"dummypath                   10 INFO     Test Message line1\\nline2\"\n", "def test_coloredlogformatter_with_custom_colors() -> None:\n    from _pytest.logging import ColoredLevelFormatter\n\n    logfmt = \"%(filename)-25s %(lineno)4d %(levelname)-8s %(message)s\"\n    custom_colors = {\n        logging.DEBUG: \"\\x1b[35m\",  # Magenta\n        logging.INFO: \"\\x1b[32m\",  # Green\n        logging.WARNING: \"\\x1b[33m\",  # Yellow\n        logging.ERROR: \"\\x1b[31m\",  # Red\n        logging.CRITICAL: \"\\x1b[35m\",  # Magenta\n    }\n\n    tw = TerminalWriter()\n    tw.hasmarkup = True\n\n    # Test with custom colors\n    formatter = ColoredLevelFormatter(tw, logfmt, custom_colors=custom_colors)\n    record = logging.LogRecord(\n        name=\"dummy\",\n        level=logging.INFO,\n        pathname=\"dummypath\",\n        lineno=10,\n        msg=\"Test Message\",\n        args=(),\n        exc_info=None,\n    )\n    output = formatter.format(record)\n    assert output == (\n        \"dummypath                   10 \\x1b[32mINFO    \\x1b[0m Test Message\"\n    )\n\n\n\n"], "sample_426": ["    def test_reversed_time_strings(self):\n        time_strings = {\n            \"minute\": npgettext_lazy(\n                \"naturaltime-past\",\n                \"%(num)d minute\",\n                \"%(num)d minutes\",\n                \"num\",\n            ),\n        }\n        with translation.override(\"cs\"):\n            for now in [self.t + self.oneminute, self.t + self.oneday, self.t + self.oneweek]:\n                with self.subTest(now):\n                    self.assertEqual(timeuntil(self.t, now, time_strings=time_strings), \"1\\xa0minut\")\n\n\n", "    def test_timezone_naive(self):\n        naive_datetime = datetime.datetime(2023, 1, 1, 12, 0, 0)\n        aware_datetime = timezone.make_aware(naive_datetime, timezone.utc)\n        self.assertEqual(timesince(naive_datetime, aware_datetime), \"0\\xa0minutes\")\n        self.assertEqual(timeuntil(aware_datetime, naive_datetime), \"0\\xa0minutes\")\n", "    def test_timeuntil_future_datetime(self):\n        \"\"\"Test timeuntil with a future datetime.\"\"\"\n        future_dt = self.t + self.onemonth\n        self.assertEqual(timeuntil(future_dt), \"1\\xa0month\")\n\n\n\n", "    def test_reversed_months(self):\n        t = datetime.datetime(2022, 1, 1)\n        self.assertEqual(timeuntil(t - self.onemonth, t), \"1\\xa0month\")\n        self.assertEqual(timeuntil(t - 2 * self.onemonth, t), \"2\\xa0months\")\n        self.assertEqual(timeuntil(t - 3 * self.onemonth, t), \"3\\xa0months\")\n        self.assertEqual(timeuntil(t - 4 * self.onemonth, t), \"4\\xa0months\")\n        self.assertEqual(timeuntil(t - 5 * self.onemonth, t), \"5\\xa0months\")\n", "    def test_future_date_with_time(self):\n        past = datetime.datetime(2023, 1, 1, 10, 0, 0)\n        future = datetime.datetime(2023, 1, 10, 18, 0, 0)\n        self.assertEqual(timesince(past, future), \"9\\xa0days, 8\\xa0hours\")\n        self.assertEqual(timeuntil(future, past), \"9\\xa0days, 8\\xa0hours\")\n"], "sample_611": ["def test_date_range_like_with_periods(\n    start, freq, periods, cal_src, cal_tgt, use_cftime, exp0, exp_pd", "    def test_date_range_like_empty_input():\n        with pytest.raises(ValueError):\n            date_range_like([], \"standard\", use_cftime=False)\n", "def test_cftime_range_with_calendar_specified(\n    start, end, periods, freq, closed, calendar", "def test_cftime_range_calendar_awareness(start, end, periods, freq, closed, calendar):\n    result = cftime_range(\n        start=start,\n        end=end,\n        periods=periods,\n        freq=freq,\n        closed=closed,\n        calendar=calendar,\n    )\n    assert isinstance(result, CFTimeIndex)\n    assert result.calendar.name == calendar\n", "def test_date_range_like_same_calendar():\n    # Test when the source calendar and target calendar are the same.\n    src = date_range(\n        \"2020-01-01\", periods=12, freq=\"6H\", calendar=\"standard\"\n    )\n    out = date_range_like(src, \"standard\", use_cftime=False)\n    assert src is out\n"], "sample_1149": ["def test_sympify_on_creation():\n    class MySingleton(Basic, metaclass=Singleton):\n            obj = super().__new__(cls, *args, **kwargs)\n            assert isinstance(obj, Basic)\n            return obj\n    S.MySingleton  \n    assert isinstance(S.MySingleton, Basic)   \n", "def test_sympify_in_singleton_registry():\n    from sympy.core.symbol import Symbol\n\n    class MySymbol(Basic, metaclass=Singleton):\n        pass\n\n    S.register(MySymbol)\n\n    assert S.MySymbol is MySymbol()\n    assert isinstance(S.MySymbol, Symbol)\n", " def test_sympify_and_singleton():\n    class MySingleton(Basic, metaclass=Singleton):\n        pass\n    S.register(MySingleton)\n    assert MySingleton() is S.MySingleton\n    assert S(MySingleton) is S.MySingleton\n    \n\n", "def test_sympify_in_singleton_registry():\n    class TestClass(Basic, metaclass=Singleton):\n        pass\n    assert S(TestClass).is_instance(TestClass)\n", "def test_sympify_in_S():\n    class MySingleton(Basic, metaclass=Singleton):\n        pass\n    \n    assert S(MySingleton()) is S.MySingleton \n"], "sample_251": ["    def test_window_function_alias(self):\n        qs = Book.objects.alias(\n            rank_alias=Window(ranking_sql='RANK() OVER (ORDER BY rating DESC)'),\n        ).annotate(rank=F('rank_alias')).order_by('rank')\n        self.assertCountEqual(\n            qs,\n            [\n                {'rank': 1}, {'rank': 2}, {'rank': 3}, {'rank': 4},\n                {'rank': 5}, {'rank': 6}, {'rank': 7}, {'rank': 8},\n            ],\n        )\n\n", "    def test_alias_with_f_expression_in_subquery(self):\n        qs = Book.objects.annotate(\n            top_rating=Subquery(\n                Book.objects.filter(\n                    pubdate__year=OuterRef('pubyear')\n                ).order_by('-rating').values('rating')[:1]\n            )\n        ).alias(pubyear_alias=F('pubyear'), top_rating_alias=F('top_rating'))\n        self.assertIs(hasattr(qs.first(), 'pubyear_alias'), False)\n        self.assertIs(hasattr(qs.first(), 'top_rating_alias'), False)\n        self.assertQuerysetEqual(\n            qs, [\n                {'pubyear': 1991, 'top_rating': 5.0},\n                {'pubyear': 1995, 'top_rating': 4.0},\n                {'pubyear': 2007, 'top_rating': 4.5},\n                {'pubyear': 2008, 'top_rating': 4.0},\n            ]\n        )\n", "    def test_complex_alias_annotate_chain(self):\n        qs = Book.objects.alias(\n            other_rating=F('rating') - 1,\n            other_pages=F('pages') * 2,\n        ).annotate(\n            final_rating=F('other_rating') + 1,\n            double_pages=F('other_pages') / 2,\n        )\n        for book in qs:\n            with self.subTest(book=book):\n                self.assertEqual(book.final_rating, book.rating)\n                self.assertEqual(book.double_pages, book.pages)\n", "    def test_alias_with_f_expression_in_lookup(self):\n        qs = Book.objects.alias(rating_diff=F('rating') - 4).filter(rating_diff__gt=0)\n        self.assertSequenceEqual(qs, [self.b1])\n\n", "    def test_alias_with_value_and_expression(self):\n        qs = Author.objects.alias(\n            age_plus_one=F('age') + Value(1),\n        ).annotate(age_with_value=F('age_plus_one') + Value(5))\n        self.assertIs(hasattr(qs.first(), 'age_plus_one'), False)\n        self.assertIs(hasattr(qs.first(), 'age_with_value'), False)\n        for author in qs:\n            with self.subTest(author=author):\n                self.assertEqual(author.age_with_value, author.age + 6) \n"], "sample_316": ["    def test_image_dimensions_with_bytesio(self):\n        with BytesIO(open(os.path.join(os.path.dirname(__file__), \"test.png\"), 'rb').read()) as fh:\n            size = images.get_image_dimensions(fh)\n        self.assertEqual(size, (540, 405))\n\n\n\n\n", "    def test_spooled_temp_write_read_content(self):\n        with tempfile.SpooledTemporaryFile(max_size=1024) as temp:\n            temp.write(b\"foo bar baz quux\\n\" * 100)\n            temp.seek(0)\n            content = temp.read()\n            self.assertEqual(content, b\"foo bar baz quux\\n\" * 100)\n", "    def test_webp_dimensions(self):\n        img_path = os.path.join(os.path.dirname(__file__), 'test.webp')\n        with open(img_path, 'rb') as fh:\n            size = images.get_image_dimensions(fh)\n        self.assertEqual(size, (540, 405))\n", "    def test_file_open_seek_rewind(self):\n        with tempfile.TemporaryFile() as temp, File(temp, name='something.txt') as test_file:\n            test_file.write(b'This is some text.')\n            test_file.seek(5)\n            self.assertEqual(test_file.read(), b' is some text.')\n            test_file.seek(0)\n            self.assertEqual(test_file.read(), b'This is some text.')\n", "    def test_empty_file(self):\n        with tempfile.SpooledTemporaryFile() as temp:\n            django_file = File(temp, name=\"something.txt\")\n            self.assertEqual(django_file.size, 0)\n"], "sample_464": ["    def test_response_with_error_on_invalid_file(self):\n        with self.assertRaises(FileNotFoundError):\n            FileResponse(open(\"nonexistent_file.txt\", \"rb\"))\n", "    def test_content_disposition_missing_filename(self):\n        response = FileResponse(io.BytesIO(b\"binary content\"), as_attachment=True)\n        self.assertEqual(response.headers.get(\"Content-Disposition\"), \"attachment\")\n\n\n\n", "    def test_content_type_encoding_override(self):\n        response = FileResponse(\n            io.BytesIO(b\"binary content\"),\n            content_type=\"image/jpeg\",\n            content_encoding=\"identity\",\n        )\n        self.assertEqual(response.headers[\"Content-Type\"], \"image/jpeg\")\n\n\n", "    def test_seekable_file_with_content_length(self):\n        with tempfile.NamedTemporaryFile(mode=\"w+b\") as tmp:\n            tmp.write(b\"binary content\")\n            tmp.seek(0)\n            response = FileResponse(tmp)\n            self.assertEqual(response.headers[\"Content-Length\"], \"14\")\n", "    def test_content_length_seekable_stream(self):\n        class SeekableStream:\n                self.content = content\n                self.position = 0\n\n                read_data = self.content[self.position : self.position + size]\n                self.position += len(read_data)\n                return read_data\n\n                if whence == 0:\n                    self.position = offset\n                elif whence == 1:\n                    self.position += offset\n                elif whence == 2:\n                    self.position = os.path.getsize(self.content) + offset\n                return self.position\n\n                return self.position\n\n        stream = SeekableStream(b\"binary content\")\n        response = FileResponse(stream)\n        response.close()\n        self.assertEqual(response.headers[\"Content-Length\"], \"14\")\n"], "sample_1": ["compilation error", "    def model_b(x):\n        return x * x\n", "    def model_b(x):\n        return x ** 2\n", "def test_separability_matrix_with_mapping():\n    model = (map2 & p2 & sh1)\n    expected_result = separability_matrix(model)\n    # Expected result, adjust based on the specific outputs\n    # Example:\n    expected_result = np.array([\n        [True, True, False, False],\n        [True, True, False, False],\n        [False, False, True, False],\n        [False, False, False, True]\n    ]) \n    assert_allclose(expected_result, separability_matrix(model))\n", "    def model_b(x):\n        return x * x"], "sample_356": ["    def test_name_collision(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Person', fields=[]),\n                migrations.CreateModel('Person', fields=[]),\n            ]\n\n        migration = Migration('0001_initial', 'test_app')\n\n        with self.assertRaises(ValueError):\n            migration.suggest_name()\n\n", "    def test_unique_together_operation(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.UniqueConstraint(\n                    model_name='person',\n                    fields=['name', 'age'],\n                    name='unique_name_age',\n                ),\n            ]\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'unique_name_age')\n", "    def test_rename_field_operation(self):\n        class Migration(migrations.Migration):\n            operations = [migrations.RenameField('model', 'old_field_name', 'new_field_name')]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'rename_field_model_old_field_name_to_new_field_name')\n", "    def test_order_of_operations_dependency(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Parent', fields=[]),\n                migrations.CreateModel('Child', fields=[], bases=('app.Parent',)),\n            ]\n\n        migration = Migration('0001_initial', 'app')\n        self.assertEqual(migration.suggest_name(), 'parent_child')\n", "    def test_rename_field_in_mti_hierarchy(self):\n        \"\"\"\n        #24186 - Ensure renaming a field in a base model\n        correctly affects inherited models.\n        \"\"\"\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.AlterField(\n                    model_name='app.Animal',\n                    old_name='name',\n                    new_name='species',\n                ),\n            ]\n\n        migration = Migration('0001_initial', 'app')\n        self.assertEqual(migration.suggest_name(), 'rename_animal_name_to_species')\n        \n        class Migration(migrations.Migration):\n            operations = [\n                migrations.AlterField(\n                    model_name='app.Dog',\n                    old_name='name',\n                    new_name='breed',\n                ),\n            ]\n        migration = Migration('0002_initial', 'app')\n        self.assertEqual(migration.suggest_name(), 'rename_dog_name_to_breed')\n\n"], "sample_78": ["    def test_normalize_path_patterns(self):\n        cases = (\n            (['foo', 'bar/baz'], ['foo', 'bar/baz']),\n            (['*', 'foo/bar', 'baz/*'], ['*', 'foo/bar', 'baz/*']),\n            (['*', 'foo/b*'], ['*', 'foo/b*']),\n            (['foo/', 'bar/baz'], ['foo', 'bar/baz']),\n            (['foo', '..'], ['foo', '..']),\n            (['foo/', '..'], ['foo', '..']),\n        )\n        for patterns, expected in cases:\n            with self.subTest(patterns=patterns):\n                self.assertEqual(normalize_path_patterns(patterns), expected)\n", "    def test_normalize_path_patterns(self):\n        self.assertEqual(normalize_path_patterns(['foo/bar', 'baz/*']), ['foo/bar', 'baz/*'])\n        self.assertEqual(normalize_path_patterns(['foo/bar', '*.baz']), ['foo/bar', '*.baz'])\n        self.assertEqual(normalize_path_patterns(['foo/bar', 'baz/*/**', 'qux/']), ['foo/bar', 'baz/*/**', 'qux/'])\n", "    def test_is_ignored_path_multiple_patterns(self):\n        patterns = ['*/baz', 'foo/bar/baz']\n        self.assertIs(is_ignored_path('foo/bar/baz', ignore_patterns=patterns), True)\n\n\n\n", "    def test_normalize_path_patterns(self):\n        self.assertEqual(normalize_path_patterns(['foo/bar/*.js', '*.css'],), ['foo/bar/**/*.js', '*.css'])\n        self.assertEqual(normalize_path_patterns(['**/*.js', 'foo/bar/*.css'],), ['**/*.js', 'foo/bar/*.css'])\n        self.assertEqual(normalize_path_patterns(['foo/bar/**', '*.css'],), ['foo/bar/**', '*.css'])\n", "    def test_normalize_path_patterns(self):\n        self.assertListEqual(normalize_path_patterns([]), [])\n        self.assertListEqual(normalize_path_patterns(['a/b', 'c/d']), ['a/b', 'c/d'])\n        self.assertListEqual(normalize_path_patterns(['a/b', 'C/D']), ['a/b', 'c/d'])\n        self.assertListEqual(normalize_path_patterns(['a/b', 'a/b/c,d']), ['a/b', 'a/b/c,d'])\n        self.assertListEqual(normalize_path_patterns(['a/b/', 'c/d']), ['a/b', 'c/d'])\n        self.assertListEqual(normalize_path_patterns(['a/b/..', 'c/d']), ['c/d'])\n        self.assertListEqual(normalize_path_patterns(['a/b/../c', 'c/d']), ['c/d'])\n"], "sample_34": ["def test_multiple_scales():\n    unit = u.Unit(\"10e0.5 * m\", format='cds')\n    assert unit.scale == 10.0 ** 0.5\n    assert unit.bases == [u.m]\n\n    unit = u.Unit(\"10**0.5 * m\", format='cds')\n    assert unit.scale == 10.0 ** 0.5\n    assert unit.bases == [u.m]\n\n\n", "def test_enable_unit_groupings_multiple():\n    with si.enable() as registry:\n        assert 'm' in registry.registry\n        assert 'km' in registry.registry\n    \n\n\n\n", "compilation error", "compilation error", "def test_non_standard_unit_comparison():\n    \"\"\"\n    Issue #4008\n    \"\"\"\n    with pytest.raises(u.UnitsError):\n        u.m > u.s\n"], "sample_385": ["    def test_render_options_multiple_selected_values(self):\n        beatles = Band.objects.create(name=\"The Beatles\", style=\"rock\")\n        who = Band.objects.create(name=\"The Who\", style=\"rock\")\n        form = AlbumForm(initial={\"featuring\": [beatles.pk, who.pk]})\n        output = form.as_table()\n        selected_option_beatles = (\n            '<option value=\"%s\" selected>The Beatles</option>' % beatles.pk\n        )\n        selected_option_who = (\n            '<option value=\"%s\" selected>The Who</option>' % who.pk\n        )\n        self.assertIn(selected_option_beatles, output)\n        self.assertIn(selected_option_who, output)\n", "    def test_render_options_with_multiple_selected(self):\n        beatles = Band.objects.create(name=\"The Beatles\", style=\"rock\")\n        who = Band.objects.create(name=\"The Who\", style=\"rock\")\n        # With 'featuring', a ManyToManyField.\n        form = AlbumForm(initial={\"featuring\": [beatles.pk, who.pk]})\n        output = form.as_table()\n        selected_option_beatles = (\n            '<option value=\"%s\" selected>The Beatles</option>' % beatles.pk\n        )\n        selected_option_who = (\n            '<option value=\"%s\" selected>The Who</option>' % who.pk\n        )\n        self.assertIn(selected_option_beatles, output)\n        self.assertIn(selected_option_who, output)\n", "    def test_i18n_language(self):\n        translation.activate(\"fr\")\n        form = AlbumForm()\n        widget = form[\"band\"].field.widget\n        self.assertEqual(widget.i18n_name, \"fr\")\n        translation.deactivate()\n\n\n\n        ", "    def test_language_specific_translations(self):\n        with translation.override(\"es\"):\n            form = AlbumForm()\n            attrs = form[\"band\"].field.widget.get_context(\n                name=\"my_field\", value=None, attrs={}\n            )[\"widget\"][\"attrs\"]\n            self.assertEqual(attrs[\"lang\"], \"es\")\n\n        with translation.override(\"fr\"):\n            form = AlbumForm()\n            attrs = form[\"band\"].field.widget.get_context(\n                name=\"my_field\", value=None, attrs={}\n            )[\"widget\"][\"attrs\"]\n            self.assertEqual(attrs[\"lang\"], \"fr\")\n\n        with translation.override(\"en\"):\n            form = AlbumForm()\n            attrs = form[\"band\"].field.widget.get_context(\n                name=\"my_field\", value=None, attrs={}\n            )[\"widget\"][\"attrs\"]\n            self.assertEqual(attrs[\"lang\"], \"en\")\n\n\n\n", "    def test_render_options_with_language_translation(self):\n        translation.activate('fr')\n        beatles = Band.objects.create(name=\"Les Beatles\", style=\"rock\")\n        who = Band.objects.create(name=\"The Who\", style=\"rock\")\n        form = AlbumForm(initial={\"band\": beatles.uuid})\n        output = form.as_table()\n        selected_option = (\n            '<option value=\"%s\" selected>Les Beatles</option>' % beatles.uuid\n        )\n        self.assertIn(selected_option, output)\n        translation.deactivate()\n\n"], "sample_97": ["    def test_snapshot_files_keeps_previous_metadata(self):\n        with self.tick_twice():\n            snapshot1 = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot1)\n            self.increment_mtime(self.existing_file)\n            snapshot2 = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot2)\n            self.assertNotEqual(snapshot1[self.existing_file], snapshot2[self.existing_file])\n", "    def test_snapshot_files_handles_creation_and_deletion(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n            snapshot1 = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot1)\n            self.increment_mtime(self.existing_file)\n            snapshot2 = dict(self.reloader.snapshot_files())\n            self.assertNotEqual(snapshot1[self.existing_file], snapshot2[self.existing_file])\n\n            # Simulate file deletion\n            os.remove(self.existing_file)\n            snapshot3 = dict(self.reloader.snapshot_files())\n            self.assertNotIn(self.existing_file, snapshot3)\n\n", "    def test_snapshot_files_updates_with_nonexistent_file(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file, self.nonexistent_file]):\n            snapshot1 = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot1)\n            self.assertNotIn(self.nonexistent_file, snapshot1)\n            self.increment_mtime(self.existing_file)\n            snapshot2 = dict(self.reloader.snapshot_files())\n            self.assertNotEqual(snapshot1[self.existing_file], snapshot2[self.existing_file])\n            self.assertNotIn(self.nonexistent_file, snapshot2) \n", "    def test_snapshot_files_ignores_non_python_files(self):\n        non_py_file = self.ensure_file(self.tempdir / 'non_py_file')\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file, non_py_file]):\n            snapshot = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot)\n            self.assertNotIn(non_py_file, snapshot)\n", "    def test_snapshot_files_removes_deleted_files(self):\n        snapshot1 = dict(self.reloader.snapshot_files())\n        self.assertIn(self.existing_file, snapshot1)\n        os.remove(self.existing_file)\n        snapshot2 = dict(self.reloader.snapshot_files())\n        self.assertNotIn(self.existing_file, snapshot2)\n"], "sample_446": ["    def test_custom_grouping_in_floatformat(self):\n        with self.settings(\n            NUMBER_GROUPING=2, THOUSAND_SEPARATOR=\",\"\n        ):\n            self.assertEqual(floatformat(1234567.89, \"2\"), \"1.234.567,89\")\n            self.assertEqual(floatformat(1234567.89, \"g\"), \"1.234,567.89\")\n            self.assertEqual(floatformat(1234567.89, \"2g\"), \"1.234.567,89\")\n", "    def test_float_precision(self):\n        with self.settings(USE_THOUSAND_SEPARATOR=True, NUMBER_GROUPING=3, THOUSAND_SEPARATOR=\"!\"):\n            self.assertEqual(floatformat(66666.666, \"2\", precision=0), \"66!667\")\n            self.assertEqual(floatformat(66666.666, \"2\", precision=1), \"66!666.7\")\n            self.assertEqual(floatformat(66666.666, \"2\", precision=2), \"66!666.67\")\n            self.assertEqual(floatformat(66666.666, \"2\", precision=3), \"66!666.666\")\n", "    def test_locale_aware_rounding(self):\n        with translation.override(\"fr-FR\"):\n            self.assertEqual(floatformat(1.23456789, 2), \"1.23\")  \n\n\n\n", "    def test_invalid_inputs(self):\n        self.assertEqual(floatformat(\"hello\"), \"\")\n        self.assertEqual(floatformat(True), \"1\")\n        self.assertEqual(floatformat(False), \"0\")\n        self.assertEqual(floatformat(None), \"\")\n        self.assertEqual(floatformat([1, 2, 3]), \"\")\n        self.assertEqual(floatformat({}), \"\")\n        self.assertEqual(floatformat(datetime.datetime.now()), \"\")\n\n\n", "    def test_float_format_with_sign(self):\n        self.assertEqual(floatformat(-1.23, 2), \"-1.23\")\n        self.assertEqual(floatformat(1.23, 2), \"1.23\")\n        self.assertEqual(floatformat(0.0, 2), \"0.00\")\n        self.assertEqual(floatformat(-0.0, 2), \"-0.00\")\n\n"], "sample_457": ["    def test_expressions_with_deferrable(self):\n        msg = (\n            \"UniqueConstraint.deferrable cannot be used with expressions. \"\n            \"Expressions are always deferred.\"\n        )\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                Lower(\"field\"),\n                name=\"test_func_deferrable\",\n                deferrable=models.Deferrable.DEFERRED,\n            )\n\n", "    def test_expressions_with_deferrable(self):\n        msg = (\n            \"UniqueConstraint.deferrable cannot be used with expressions. \"\n            \"Deferred unique constraints on expressions are not supported.\"\n        )\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                Lower(\"field\"),\n                name=\"deferred_expression_unique\",\n                deferrable=models.Deferrable.DEFERRED,\n            )\n", "    def test_expressions_with_include(self):\n        msg = \"UniqueConstraint.include cannot be used with expressions.\"\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                Lower(\"field\"),\n                name=\"test_func_include\",\n                include=[\"other_field\"],\n            )\n", "    def test_expressions_with_include(self):\n        msg = \"UniqueConstraint.include cannot be used with expressions.\"\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                Lower(\"field\"),\n                name=\"test_func_include\",\n                include=[\"other_field\"],\n            )\n", "    def test_opclasses_with_expressions_using_opclass(self):\n        msg = (\n            \"UniqueConstraint.opclasses can be used with expressions \"\n            \"if provided as OpClass instance.\"\n        )\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                Lower(\"field\"),\n                name=\"test_func_opclass\",\n                opclasses=models.OpClass(name=\"jsonb_path_ops\"),\n            )\n\n"], "sample_578": ["    def test_baseline_position(self, x, y):\n\n        baseline = 2\n        p = Plot(x, y).add(Bars(baseline=baseline)).plot()\n        ax = p._figure.axes[0]\n        for bar in ax.patches:\n            assert bar.get_height() == (y[np.where(x == bar.get_x())[0][0]] - baseline)\n", "    def test_baseline(self, x, y):\n        p = Plot(x, y).add(Bars(baseline=2)).plot()\n        ax = p._figure.axes[0]\n        paths = ax.collections[0].get_paths()\n        for i, path in enumerate(paths):\n            verts = path.vertices\n            assert verts[2, 1] == pytest.approx(y[i] + 2) \n", "    def test_baseline_offset(self, x, y):\n        p = Plot(x, y).add(Bars(baseline=1)).plot()\n        ax = p._figure.axes[0]\n        paths = ax.collections[0].get_paths()\n        for i, path in enumerate(paths):\n            verts = path.vertices\n            assert verts[0, 1] == pytest.approx(y[i] + 1)\n            assert verts[3, 1] == pytest.approx(y[i] + 1)\n", "    def test_baseline(self, x, y):\n\n        p = Plot(x, y).add(Bars(baseline=2)).plot()\n        ax = p._figure.axes[0]\n        paths = ax.collections[0].get_paths()\n        for i, path in enumerate(paths):\n            verts = path.vertices\n            assert verts[0, 1] == pytest.approx(y[i] + 2)\n            assert verts[3, 1] == pytest.approx(y[i] + 2)\n", "    def test_baseline_position(self, x, y):\n\n        p = Plot(x, y).add(Bars(baseline=2)).plot()\n        ax = p._figure.axes[0]\n        paths = ax.collections[0].get_paths()\n        for i, path in enumerate(paths):\n            verts = path.vertices\n            assert verts[0, 1] == pytest.approx(y[i] - 2)\n            assert verts[3, 1] == pytest.approx(y[i] - 2 + y[i])\n\n\n"], "sample_301": ["    def test_notify_file_changed_handles_os_errors(self, mock_os_stat):\n        mock_os_stat.side_effect = OSError(\"Fake error\")\n        with self.assertRaises(OSError) as cm:\n            self.reloader.notify_file_changed(self.existing_file)\n        self.assertEqual(cm.exception.args[0], \"Fake error\") \n\n", "    def test_snapshot_files_ignores_non_python_files(self):\n        non_py_file = self.ensure_file(self.tempdir / 'non_py_file')\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[non_py_file, self.existing_file]):\n            snapshot = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot)\n            self.assertNotIn(non_py_file, snapshot) \n", "    def test_snapshot_files_handles_new_files(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n            snapshot1 = dict(self.reloader.snapshot_files())\n        new_file = self.ensure_file(self.tempdir / 'new.py')\n        snapshot2 = dict(self.reloader.snapshot_files())\n        self.assertIn(new_file, snapshot2)\n", "    def test_snapshot_files_with_modified_files_returns_only_modified_files(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n            snapshot1 = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot1)\n            self.increment_mtime(self.existing_file)\n            snapshot2 = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot2)\n            self.assertNotEqual(snapshot1[self.existing_file], snapshot2[self.existing_file])\n\n\n\n", "    def test_snapshot_files_updates_with_glob(self):\n        self.reloader.watch_dir(self.tempdir, '*.py')\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file, self.nonexistent_file]):\n            snapshot1 = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot1)\n            self.assertNotIn(self.nonexistent_file, snapshot1)\n            self.increment_mtime(self.existing_file)\n            snapshot2 = dict(self.reloader.snapshot_files())\n            self.assertNotEqual(snapshot1[self.existing_file], snapshot2[self.existing_file])\n"], "sample_318": ["    def test_lookahead_negated_with_groups(self):\n        test_urls = [\n            ('/lookahead-negated-group', {'city': 'a-city'}, '/lookahead-negated-group/a-city/'),\n            ('/lookahead-negated-group2', {'city': 'a-city'}, '/lookahead-negated-group2/a-city/'),\n        ]\n        for name, kwargs, expected in test_urls:\n            with self.subTest(name=name, kwargs=kwargs):\n                self.assertEqual(reverse(name, kwargs=kwargs), expected)\n", "    def test_lookahead_nested_lookbehind(self):\n        test_urls = [\n            ('/nested-lookahead-lookbehind/a-city/', None),\n        ]\n        for test_url, kwargs in test_urls:\n            with self.subTest(url=test_url):\n                self.assertEqual(resolve(test_url).kwargs, {'city': 'a-city'})\n", "    def test_reverse_with_special_characters(self):\n        test_urls = [\n            ('lookahead-positive', {'city': 'a+city'}, '/lookahead+/a%2Bcity/'),\n            ('lookahead-negative', {'city': 'a-city'}, '/lookahead-/a-city/'),\n            ('lookbehind-positive', {'city': 'a+city'}, '/lookbehind+/a%2Bcity/'),\n            ('lookbehind-negative', {'city': 'a-city'}, '/lookbehind-/a-city/'),\n        ]\n        for name, kwargs, expected in test_urls:\n            with self.subTest(name=name, kwargs=kwargs):\n                self.assertEqual(reverse(name, kwargs=kwargs), expected)\n", "    def test_lookahead_and_lookbehind_in_same_urlpattern(self):\n        test_urls = [\n            ('/lookahead+/a-city/lookbehind-/b-town/', {'city': 'a-city', 'town': 'b-town'}),\n            ('/lookbehind-/b-town/lookahead+/a-city/', {'town': 'b-town', 'city': 'a-city'}),\n        ]\n        for test_url, kwargs in test_urls:\n            with self.subTest(url=test_url):\n                self.assertEqual(resolve(test_url).kwargs, kwargs)\n\n\n\n", "    def test_lookahead_regex_with_capture(self):\n        test_urls = [\n            ('/lookahead-capture/(?P<city>a-city)/', {'city': 'a-city'}),\n            ('/lookbehind-capture/(?P<city>a-city)/', {'city': 'a-city'}),\n        ]\n        for url, kwargs in test_urls:\n            with self.subTest(url=url):\n                match = resolve(url)\n                self.assertEqual(match.kwargs, kwargs)\n                self.assertEqual(match.url_name, 'lookahead-capture')\n\n"], "sample_1196": ["def test_contains_with_non_symbolic_sets():\n    assert Contains(1, set([1, 2, 3])) is S.true\n    assert Contains(4, set([1, 2, 3])) is S.false\n", "def test_contains_with_sympy_objects():\n    x = Symbol('x')\n    y = Symbol('y')\n    assert Contains(x, FiniteSet(x, y)) is S.true\n    assert Contains(x, FiniteSet(y)) is S.false\n    assert Contains(x, FiniteSet(x, Eq(y, x))) is S.true \n", "def test_contains_with_symbols():\n    x = Symbol('x')\n    y = Symbol('y')\n    assert Contains(x, FiniteSet(y)) == Contains(x, {y})\n", "def test_contains_with_booleans():\n    x = Symbol('x')\n    assert Contains(x, FiniteSet(x, S.true)) == S.true\n    assert Contains(S.false, FiniteSet(S.false, x)) == S.true\n", "def test_contains_with_symbols_in_set():\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n    assert Contains(x, FiniteSet(y, z)) == Contains(x, FiniteSet(y, z))\n    assert Contains(x, FiniteSet(Set(y), z)) == Contains(x, FiniteSet(y, z))\n"], "sample_608": ["    def test__mapping_repr_collapsed(display_max_rows, n_vars, n_attr) -> None:\n        long_name = \"long_name\"\n        a = defchararray.add(long_name, np.arange(0, n_vars).astype(str))\n        b = defchararray.add(\"attr_\", np.arange(0, n_attr).astype(str))\n        c = defchararray.add(\"coord\", np.arange(0, n_vars).astype(str))\n        attrs = {k: 2 for k in b}\n        coords = {_c: np.array([0, 1]) for _c in c}\n        data_vars = dict()\n        for (v, _c) in zip(a, coords.items()):\n            data_vars[v] = xr.DataArray(\n                name=v,\n                data=np.array([3, 4]),\n                dims=[_c[0]],\n                coords=dict([_c]),\n            )\n        ds = xr.Dataset(data_vars)\n        ds.attrs = attrs\n\n        with xr.set_options(display_max_rows=display_max_rows):  \n            actual = formatting.dataset_repr(ds)\n            assert \"\\n\".join(actual.split(\"\\n\")[:display_max_rows]) == (\n                formatting._join_dims_and_attrs(ds, display_max_rows)\n            )\n\n", "    def test_array_repr_with_formatting(array):\n        formatted = formatting.array_repr(array)\n        assert len(formatted.splitlines()) == 1\n", "    def test_repr_file_collapsed_with_netcdf3(tmp_path) -> None:\n        arr = xr.DataArray(np.random.randn(100, 5, 1))\n        arr.to_netcdf(tmp_path / \"test.nc\", engine=\"netcdf3\")\n\n        with xr.open_dataarray(tmp_path / \"test.nc\") as arr, xr.set_options(\n            display_expand_data=False\n        ):\n            actual = formatting.array_repr(arr)\n            expected = dedent(\n                \"\"\"\\\n        <xarray.DataArray (test: 100)>\n        array([[ ... ],\n               [ ... ],\n               [ ... ],\n               ...])\n        Dimensions without coordinates: test\"\"\"\n            )\n\n            assert actual == expected\n", "    def test_short_numpy_repr_with_custom_dtype(self) -> None:\n        class MyDType(np.dtype):\n            char_info = np.char.char_info_base\n            kind = 'O'\n\n        arr = np.array([1, 2, 3], dtype=MyDType())\n        short_repr = formatting.short_numpy_repr(arr)\n        assert \"MyDType\" in short_repr\n\n\n\n", "    def test_coord_repr_empty_dataset(self) -> None:\n        ds = xr.Dataset()\n        actual = formatting.coords_repr(ds.coords)\n        expected = \"Coordinates: ()\\n\"\n        assert actual == expected\n\n\n\n"], "sample_1147": ["    def test_issue_18122():\n        from sympy.vector import CoordSys, vector\n        c = CoordSys('c')\n        x, y = c.x, c.y\n        v = vector(x + y, c)\n        assert latex(v) == r'\\mathbf{x} + \\mathbf{y}'\n\n", "    def test_unicode_symbols():\n        from sympy import symbols, unicode\n        x, y = symbols('x y')\n        \n        assert latex(unicode(\"\u03c0\")) == r'\\pi'\n        assert latex(unicode(\"\u03b1\")) == r'\\alpha'\n        assert latex(unicode(\"\u2211\")) == r'\\sum'\n        assert latex(unicode(\"\u222b\")) == r'\\int'\n        assert latex(unicode(\"\u2208\")) == r'\\in'\n        assert latex(unicode(\"\u2208\")) == r'\\in'\n        assert latex(unicode(\"\u2260\")) == r'\\neq'\n        assert latex(unicode(\"\u2248\")) == r'\\approx'\n        assert latex(unicode(\"\u2264\")) == r'\\leq'\n\n", "def test_latex_unicode():\n    # Ensure unicode characters are printed correctly\n    a = Symbol('\u03b1')\n    b = Symbol('\u03b2')\n    c = Symbol('\u03b3')\n    d = Symbol('\u03b4')\n    e = Symbol('\u03b5')\n    f = Symbol('\u03b6')\n    g = Symbol('\u03b7')\n    h = Symbol('\u03b8')\n\n    assert latex(a) == r\"\\alpha\"\n    assert latex(b) == r\"\\beta\"\n    assert latex(c) == r\"\\gamma\"\n    assert latex(d) == r\"\\delta\"\n    assert latex(e) == r\"\\epsilon\"\n    assert latex(f) == r\"\\zeta\"\n    assert latex(g) == r\"\\eta\"\n    assert latex(h) == r\"\\theta\"\n\n    letters = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n    for letter in letters:\n        symbol = Symbol(letter)\n        assert latex(symbol) == f\"\\\\{letter}\"\n\n    assert latex(\"Hello world\") == r\"Hello world\"\n\n\n\n", "def test_latex_printing_of_matrices_with_non_numerical_entries():\n    from sympy import Matrix, symbols\n\n    x, y, z = symbols('x y z')\n    A = Matrix([[x, y, z], [1, 2, 3], [4, 5, 6]])\n    assert latex(A) == r\" \\begin{bmatrix} x & y & z \\\\ 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}\"\n\n    B = Matrix([[x**2, y**3, z**4], [1, 2, 3], [4, 5, 6]])\n    assert latex(B) == r\" \\begin{bmatrix} x^{2} & y^{3} & z^{4} \\\\ 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}\"\n", "def test_mathtext_fonts():\n    assert latex(1, mathtext_fonts=True) == r'1'\n    assert latex(2**3, mathtext_fonts=True) == r'2^{3}'\n    assert latex(sin(x), mathtext_fonts=True) == r'\\sin{\\left(x\\right)}'\n    assert latex(sqrt(x), mathtext_fonts=True) == r'\\sqrt{x}'\n"], "sample_1008": ["compilation error", "def test_dcm_derivatives():\n    N = ReferenceFrame('N')\n    A = N.orientnew('A', 'Axis', [q1, N.z])\n    B = A.orientnew('B', 'Axis', [q2, A.x])\n    C = B.orientnew('C', 'Axis', [q3, B.y])\n\n    # Test derivatives of DCM with respect to time\n    q1d = dynamicsymbols('q1', 1)\n    q2d = dynamicsymbols('q2', 1)\n    q3d = dynamicsymbols('q3', 1)\n    \n    dN_dB = A.dcm(B).diff(q1d)\n    assert dN_dB == Matrix([\n        [-cos(q1) * sin(q2) * sin(q3) * dN_dB[0, 0] - sin(q1) *\n        cos(q3) * q2d * dN_dB[0, 1] - sin(q1) * sin(q2) * cos(q3) * q2d,\n        - sin(q1) * sin(q2) * cos(q3) * dN_dB[1, 0] + sin(q1) *\n        cos(q3) * q2d * dN_dB[1, 1] + sin(q1) * sin(q2) * sin(q3) * q2d,\n        - sin(q3) * cos(q2) * dN_dB[2, 0] + sin(q2) * q2d * dN_dB[2, 1]],\n        [cos(q1) * cos(q3) * sin(q2) * dN_dB[0, 0] + sin(q1) *\n        cos(q2) * q2d * dN_dB[0, 1] + cos(q1) * sin(q2) * cos(q3) * q2d,\n        sin(q1) * cos(q3) * sin(q2) * dN_dB[1, 0] + cos(q1) *\n        cos(", "compilation error", "    def test_dcm_update():\n        N = ReferenceFrame('N')\n        A = N.orientnew('A', 'Axis', [pi/2, N.z])\n        B = N.orientnew('B', 'Axis', [pi/4, N.x])\n        assert A.dcm(N) != B.dcm(N)\n        A.orient(B, 'Axis', [pi/4, B.y])\n        assert A.dcm(N) == B.dcm(N)\n", "def test_dcm_with_time_derivatives():\n    N = ReferenceFrame('N')\n    A = N.orientnew('A', 'Axis', [q1, N.z])\n    B = A.orientnew('B', 'Axis', [q2, A.x])\n    q1, q2 = dynamicsymbols('q1 q2')\n    q1d, q2d = dynamicsymbols('q1 q2', 1)\n    assert N.dcm(B) == Matrix([\n        [-sin(q1) * sin(q2), -cos(q1), sin(q1) * sin(q2)],\n        [cos(q1) * sin(q2), cos(q1), -cos(q1) * sin(q2)],\n        [cos(q2), 0, sin(q2)]])\n    test_mat = time_derivative(N.dcm(B), A)\n    assert test_mat.expand() == Matrix(\n        [\n            [ -sin(q1) * sin(q2) * q1d - cos(q1) * cos(q2) * q2d, - q1d * cos(q1) * sin(q2)\n            - q2d * sin(q1) * cos(q2), sin(q1) * sin(q2) * q1d + cos(q1) * cos(q2) * q2d ],\n            [ cos(q1) * sin(q2) * q1d + cos(q1) * cos(q2) * q2d, cos(q1) * q1d * sin(q2)\n            + q2d * sin(q1) * cos(q2), -cos(q1) * sin(q2) * q1d - q2d * sin(q1) * cos(q2)],\n            [ -cos(q2) * q1d, -q2d * sin(q2), sin(q2) * q1d ]\n        ])\n"], "sample_218": ["    def test_trunc_func_with_timezone_none(self):\n        start_datetime = None\n        end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n        if settings.USE_TZ:\n            end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n        self.create_model(start_datetime, end_datetime)\n        self.assertIsNone(DTModel.objects.annotate(truncated=TruncYear('start_datetime')).first().truncated)\n        self.assertIsNone(DTModel.objects.annotate(truncated=TruncMonth('start_datetime')).first().truncated)\n        self.assertIsNone(DTModel.objects.annotate(truncated=TruncDay('start_datetime')).first().truncated)\n        self.assertIsNone(DTModel.objects.annotate(truncated=TruncHour('start_datetime')).first().truncated)\n        self.assertIsNone(DTModel.objects.annotate(truncated=TruncMinute('start_datetime')).first().truncated)\n        self.assertIsNone(DTModel.objects.annotate(truncated=TruncSecond('start_datetime')).first().truncated)\n\n\n\n", "    def test_trunc_func_with_timezone_and_non_aware_datetime(self):\n        start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n        end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n        melb = pytz.timezone('Australia/Melbourne')\n        self.create_model(start_datetime, end_datetime)\n        self.create_model(end_datetime, start_datetime)\n\n        with self.assertRaises(pytz.exceptions.NonExistentTimeError):\n            DTModel.objects.annotate(\n                truncated=Trunc('start_datetime', 'year', tzinfo=melb)\n            ).get()\n\n\n", "    def test_trunc_func_with_timezone_and_naive_datetime(self):\n        start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n        end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n        start_datetime = timezone.localize(start_datetime)\n        end_datetime = timezone.localize(end_datetime)\n        self.create_model(start_datetime, end_datetime)\n\n        melb = pytz.timezone('Australia/Melbourne')\n        naive_start_datetime = start_datetime.astimezone(None)  \n        naive_end_datetime = end_datetime.astimezone(None) \n\n        with self.assertRaises(pytz.exceptions.NonExistentTimeError):\n            DTModel.objects.annotate(\n                truncated=Trunc('start_datetime', 'day', tzinfo=melb)\n            ).filter(start_datetime=naive_start_datetime)\n\n\n", "    def test_trunc_with_datetime_field_input(self):\n        start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n        end_datetime = datetime(2016, 6, 15, 14, 10, 50, 123)\n        start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n        end_datetime = timezone.make_aware(end_datetime, is_dst=False)\n\n        self.create_model(start_datetime, end_datetime)\n        with self.assertRaises(ValueError):\n            list(DTModel.objects.annotate(truncated=TruncDay('start_date')).order_by('start_datetime'))\n", "    def test_trunc_func_with_timezone_and_null_datetime(self):\n        self.create_model(None, None)\n        self.assertIsNone(DTModel.objects.annotate(truncated=TruncHour('start_datetime')).first().truncated)\n\n\n\n"], "sample_977": ["def test_Sum():\n    assert mcode(Sum(x**2, (x, 1, 5))) == \"Hold[Sum[x^2, {x, 1, 5}]]\"\n    assert mcode(Sum(x**2 + y**2, (x, 1, 5), (y, 0, 3))) == \\\n        \"Hold[Sum[x^2 + y^2, {x, 1, 5}, {y, 0, 3}]]\"\n\n\n", "def test_Sum():\n    assert mcode(Sum(x*y, (x, 1, 5))) == \"Hold[Sum[x*y, {x, 1, 5}]]\" \n", "def test_Sum():\n    assert mcode(Sum(x, (x, 0, 5))) == \"Hold[Sum[x, {x, 0, 5}]]\"\n    assert mcode(Sum(x**2, (x, 0, 10))) == \"Hold[Sum[x^2, {x, 0, 10}]]\"\n", "def test_Sum():\n    assert mcode(Sum(i**2, (i, 1, 5))) == \"Hold[Sum[i^2, {i, 1, 5}]]\"\n", "def test_Sum():\n    assert mcode(Sum(x, (x, 1, 5))) == \"Hold[Sum[x, {x, 1, 5}]]\"\n    assert mcode(Sum(x**2, (x, 0, oo))) == \"Hold[Sum[x^2, {x, 0, Infinity}]]\" \n"], "sample_351": ["    def test_required_readonly_field(self):\n        class ModelChoiceForm(forms.ModelForm):\n            category = forms.ModelChoiceField(Category.objects.all(), required=True, read_only=True)\n\n            class Meta:\n                model = Book\n                fields = ['category']\n\n        book = Book.objects.create(author=Writer.objects.create(name='Test writer'))\n        book.category = self.c1\n        form = ModelChoiceForm(instance=book)\n\n        self.assertTrue(form.is_valid())\n", "    def test_empty_label_validation(self):\n        f = forms.ModelChoiceField(Category.objects.all(), empty_label=None)\n        self.assertIsNone(f.clean(''))\n        f = forms.ModelChoiceField(Category.objects.all(), empty_label='---------')\n        self.assertIsNone(f.clean('---------'))\n\n\n", "    def test_custom_label(self):\n        class CustomModelChoiceField(forms.ModelChoiceField):\n                return f\"Custom Label: {obj.name}\"\n\n        f = CustomModelChoiceField(Category.objects.all())\n        self.assertEqual(list(f.choices), [\n            ('', '---------'),\n            (self.c1.pk, 'Custom Label: Entertainment'),\n            (self.c2.pk, 'Custom Label: A test'),\n            (self.c3.pk, 'Custom Label: Third'),\n        ])\n", "    def test_queryset_filter(self):\n        f = forms.ModelChoiceField(\n            Category.objects.filter(name__startswith='T'),\n            required=False\n        )\n        self.assertEqual(len(f.choices), 2)\n        self.assertEqual(list(f.choices), [('', '---------'), (self.c2.pk, 'A test'), (self.c3.pk, 'Third')])\n\n        with self.assertRaises(ValidationError):\n            f.clean(self.c1.pk)\n\n        f = forms.ModelChoiceField(\n            Category.objects.filter(name__icontains='A'),\n            required=False\n        )\n        self.assertEqual(len(f.choices), 1)\n        self.assertEqual(list(f.choices), [('', '---------'), (self.c2.pk, 'A test')])\n\n\n\n        f = forms.ModelChoiceField(\n            Category.objects.filter(name__isnull=False),\n            required=False\n        )\n        self.assertEqual(len(f.choices), 4)\n        self.assertEqual(list(f.choices), [('', '---------'), (self.c1.pk, 'Entertainment'), (self.c2.pk, 'A test'), (self.c3.pk, 'Third')])\n\n\n\n  \n", "    def test_to_field_name_multiselect(self):\n        f = forms.ModelMultipleChoiceField(Category.objects.all(), to_field_name='slug')\n        data = [self.c1.slug, self.c2.slug]\n        self.assertEqual(f.clean(data), [self.c1, self.c2])\n\n"], "sample_526": ["compilation error", "def test_set_epoch_error():\n    with pytest.raises(ValueError, match=\"Epoch must be a string\"):\n        mdates.set_epoch(123)\n", "def test_tz_aware_dates():\n    tz = dateutil.tz.tzlocal()\n    dt = datetime.datetime(2023, 10, 26, 10, 30, tzinfo=tz)\n    dt_num = mdates.date2num(dt)\n    dt_back = mdates.num2date(dt_num, tz=tz)\n    assert dt_back.tzinfo is tz\n    assert dt_back.year == dt.year\n    assert dt_back.month == dt.month\n    assert dt_back.day == dt.day\n    assert dt_back.hour == dt.hour\n    assert dt_back.minute == dt.minute\n\n", "def test_WeekdayLocator_firstweekday_handling():\n    locator = mdates.WeekdayLocator(byweekday=mdates.FRIDAY,\n                                    interval=1)\n    assert locator.firstweekday == mdates.FRIDAY\n    assert locator.get_ticks(np.arange(3, 10), unit='D')[0] == 20220407\n\n\n", "def test_date2num_day_of_week():\n    # Test edge cases for day of the week handling\n    dates = [datetime.datetime(2023, 1, 1),\n             datetime.datetime(2023, 1, 2),\n             datetime.datetime(2023, 1, 3),\n             datetime.datetime(2023, 1, 4)]\n\n    for i, date in enumerate(dates):\n        num = mdates.date2num(date)\n        # Ensure date2num returns a consistent result even for days of the week\n        assert mdates.num2date(num) == date\n"], "sample_737": ["def test_vectorizer_ngram_range_validation():\n    for vectorizer_class in [CountVectorizer, TfidfVectorizer, HashingVectorizer]:\n        for ngram_range in [(-1, 1), (1, -1), (2, 0)]:\n            with assert_raises(ValueError):\n                vectorizer_class(ngram_range=ngram_range)\n\n        with assert_raises(ValueError):\n            vectorizer_class(ngram_range=(0, 0))\n\n\n\n", "def test_vectorizer_unicode_string_input():\n    vect = CountVectorizer()\n    txt = u\"\u00e4\u00f6\u00fc\u00df\"\n    assert_array_equal(vect.fit_transform([txt]).toarray(), [[1, 1, 1]])\n\n    vect = TfidfVectorizer()\n    assert_array_equal(vect.fit_transform([txt]).toarray(), [[1, 1, 1]])\n\n    vect = HashingVectorizer()\n    assert_array_equal(vect.fit_transform([txt]).toarray(), [[1, 1, 1]])\n\n\n\n\n", "def test_vectorizer_stop_words_list_and_string():\n    for vec_type in [CountVectorizer, TfidfVectorizer]:\n        for stop_words_type in [[\"the\", \"a\"], \"english\"]:\n            vect = vec_type(stop_words=stop_words_type)\n            vect.fit(JUNK_FOOD_DOCS)\n            \n            # Test that the stop words are correctly applied\n            test_doc = 'this is a test document with the word the'\n            transformed_doc = vect.transform([test_doc])\n            assert_array_equal(transformed_doc.getnnz(), 5)  # Should have 5 non-zero elements\n\n\n", "def test_vectorizer_custom_analyzer():\n        tokens = [token.upper() for token in text.split()]\n        return tokens\n\n    vect = CountVectorizer(analyzer=custom_analyzer)\n    X = vect.fit_transform(JUNK_FOOD_DOCS)\n    assert_array_equal(\n        X.toarray(),\n        [[1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0],\n         [1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0],\n         [1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0],\n         [0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0],\n         [0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1],\n         [1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0],\n         [1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1],\n         [0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0],\n         [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1],", "def test_vectorizer_unicode_and_strip_accents():\n    docs = [\n        u\"This is \\xd0\\xa1\\xd0\\xbe\\xd0\\xb1\\xd1\\x81\\xd0\\xb8\\xd0\\xb9 \\xd0\\xb8\\xd0\\xb7\\xd1\\x83\\xd1\\x87\\xd0\\xb0\\xd1\\x8e\\xd1\\x89 \\xd0\\xb0\\xd0\\xb5\\xd1\\x80\\xd1\\x82\\xd0\\xb8\\xd0\\xb5\\xd0\\xb9 \\xd0\\xb2 \\xd0\\xbe\\xd1\\x80\\xd0\\xb1\\xd0\\xb8\\xd0\\xb0\\xd0\\xb1\\xd0\\xb8 \\xd0\\xb2\\xd0\\xbe\\xd0\\xb4\\xd0\\xb0\\xd1\\x82\\xd0\\xb8. \",\n        u\"This is a \\xf4\\x90\\xf4\\x8f\\xf4\\x8e\\xf4\\x87 \\xd0\\xb8\\xd0\\xb7\\xd1\\x83\\xd1\\x87\\xd0\\xb0\\xd1\\x8e\\xd1\\x89 \\xd0\\xb2\\xd0\\xb5\\xd0\\xbe\\xd0\\xb1\\xd0\\xb0\\xd0\\xb4\\xd0\\xb8\\xd0\\xb3\\xd0\\xb5\\xd0\\xb9\\xd0\\xb5\\xd0\\xb9. \"\n    ]\n\n    for vectorizer in [CountVectorizer, TfidfVectorizer]:\n        vect = vectorizer()\n        vect.fit(docs)\n        X = vect.transform(docs)\n        assert_array_equal(X.toarray(), X.toarray())\n\n        vect = vectorizer(strip_accents=strip_accents_unicode)\n        vect.fit(docs)\n        X = vect.transform(docs)\n        assert_array_equal(X.toarray(), X.toarray"], "sample_738": ["def test_vectorizer_custom_analyzer():\n    custom_analyzer = lambda doc: [word.upper() for word in doc.split()]\n    vec = CountVectorizer(analyzer=custom_analyzer)\n    X = vec.fit_transform(JUNK_FOOD_DOCS)\n    assert_array_equal(X.toarray(),\n                       [[1, 0, 0, 0, 0],\n                        [0, 1, 0, 0, 0],\n                        [0, 0, 1, 0, 0]])  \n\n\n\n", "def test_vectorizer_empty_vocabulary():\n    for vec in [TfidfVectorizer(), CountVectorizer(), HashingVectorizer()]:\n        vect = vec()\n        with assert_raises(ValueError):\n            vect.fit_transform([])\n", "def test_vectorizer_with_unknown_tokens():\n    vect = TfidfVectorizer()\n    X = vect.fit_transform([\"This is a sentence with unknown words\"])\n    assert_equal(X.shape[1], vect.get_feature_names_out().size)\n    \n\n    # unknown words are not present in vocab\n\n    for word in [\"unknown1\", \"unknown2\", \"unknown3\"]:\n        assert_false(word in vect.vocabulary_)\n\n    \n    X2 = vect.transform([\"This is another sentence with unknown words\"])\n    assert_equal(X2.shape[1], vect.get_feature_names_out().size)\n    assert_array_equal(X2.toarray(), np.zeros((1, X2.shape[1])))\n", "    def test_vectorizer_empty_documents():\n        vect = CountVectorizer()\n        X = vect.fit_transform([''])\n        assert_array_equal(X.toarray(), np.zeros((1, 0)))\n\n        vect = TfidfVectorizer()\n        X = vect.fit_transform([''])\n        assert_array_equal(X.toarray(), np.zeros((1, 0)))\n\n        vect = HashingVectorizer()\n        X = vect.fit_transform([''])\n        assert_array_equal(X.toarray(), np.zeros((1, vect.n_features)))\n", "def test_vectorizer_unicode_with_custom_stop_words():\n    # Ensure unicode and custom stop words work together\n    document = (\n        \"\\xd0\\x9c\\xd0\\xb0\\xd1\\x88\\xd0\\xb8\\xd0\\xbd\\xd0\\xbd\\xd0\\xbe\\xd0\"\n        \"\\xb5 \\xd0\\xBE\\xd0\\xb1\\xd1\\x83\\xd1\\x87\\xd0\\xb5\\xd0\\xb0 \\xe2\\x80\\x94 \"\n        \"\\xd0\\xBE\\xd0\\xb1\\xd1\\x88\\xd0\\xb8\\xd1\\x80\\xd0\\xbd\\xd1\\x8b\\xd0\\xb9 \"\n        \"\\xd0\\xbf\\xd0\\xbe\\xd0\\xb4\\xd1\\x80\\xd0\\xb0\\xd0\\xb7\\xd0\\xb4\\xd0\\xb5\\xd0\"\n        \"\\xb9 \\xd0\\xb8\\xd1\\x81\\xd0\\xba\\xd1\\x83\\xd1\\x81\\xd1\\x81\\xd1\\x82\\xd0\\xb2\"\n        \"\\xd0\\xb5\\xd0\\xb6\\xd0\\xb5\\xd0\\xb7\\xd0\\xb8\\xd0\\xb0\\xd1\\x81\\xd1\\x8f.\"\n    )\n    stop_words = [\"the\", \"a\", \"is\", \"and\"]\n    vect = CountVectorizer(stop_words=stop_words)\n    X = vect.fit_transform([document])\n    assert_array_equal(X.toarray(), [[1, 1, 1, 1]])\n\n\n"], "sample_1041": ["def test_transpose_immutable_matrix():\n    A = ImmutableMatrix([[1, 2], [3, 4]])\n    assert Transpose(A) == A.T\n    assert (A.T).T == A\n    raises(TypeError, lambda: Transpose(A**-1))\n\n\n", "def test_matrix_addition_matrix_scalar():\n    A = MatrixSymbol('A', 2, 2)\n    scalar = symbols('s', integer=True)\n    assert (A + scalar).is_Matrix == True\n    assert (scalar + A).is_Matrix == True\n    assert isinstance(A + scalar, MatAdd)\n    assert isinstance(scalar + A, MatAdd)\n\n\n\n", "def test_matrix_symbol_subs():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n    expr = A*B + C\n\n    assert expr.subs({A: B}) == B*B + C\n    assert expr.subs({C: C*C}) == A*B + C*C\n    assert expr.subs({A: Identity(2)}) == Identity(2)*B + C\n    assert expr.subs({B:  ZeroMatrix(2, 2)}) == ZeroMatrix(2, 2)+ C\n\n\n\n", "compilation error", "def test_issue_5632():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    assert A * B == B * A\n\n\n\n"], "sample_772": ["    def test_parallel_execution(self):\n        from joblib import Parallel, delayed\n        forest = RandomForestClassifier(n_estimators=2, n_jobs=-1,\n                                        random_state=1)\n        # Use delayed to wrap the fit calls\n        results = Parallel(n_jobs=-1)(\n            delayed(forest.fit)(X, y) for i in range(5))\n        \n        # Assert all fit calls are completed\n        assert len(results) == 5\n\n        final_model = results[0]\n        # Assert all models are consistent\n\n", "    def test_n_jobs_with_custom_backend(self):\n        self.count = 0\n        backend = MyBackend()\n        # Use a custom backend if possible\n        if backend.available:\n            forest = RandomForestClassifier(n_jobs=2, verbose=0,\n                                            backend=backend)\n            forest.fit(X, y)\n            assert_equal(self.count, 2)\n        else:\n            forest = RandomForestClassifier(n_jobs=2, verbose=0)\n            forest.fit(X, y)\n", "    def test_backend(name):\n        X, y = datasets.make_classification(random_state=1)\n        ForestEstimator = FOREST_ESTIMATORS[name]\n        est = ForestEstimator(n_estimators=1, random_state=1, backend=MyBackend())\n        est.fit(X, y)\n        assert_equal(est.backend.count, len(est.estimators_))\n", "    def test_joblib_backend(self):\n        backend = MyBackend()\n        with backend:\n            RandomForestClassifier().fit(X, y)\n        assert_equal(backend.count, 1)\n", "    def test_joblib_backend(self):\n        est = RandomForestClassifier(n_estimators=10, random_state=0)\n        backend = MyBackend()\n        with joblib.Memory(location=\"tmp\", verbose=0) as cache:\n            result = cache.cache(est.fit, (X, y), backend=backend)\n\n        assert_equal(backend.count, 1)\n        assert_array_almost_equal(result, est.fit(X, y))\n"], "sample_727": ["def test_imputation_sparse_empty_columns():\n    # Test imputation with sparse matrices with empty columns\n    for strategy in [\"mean\", \"median\", \"most_frequent\"]:\n        X = sparse.csc_matrix((np.ones(10), (np.arange(10), np.arange(5))),\n                shape=(10, 5))\n        X[:, 2] = 0\n        imputer = Imputer(missing_values=0, strategy=strategy)\n        imputed_X = imputer.fit_transform(X)\n        assert_equal(imputed_X.shape, X.shape)\n        assert_array_almost_equal(imputed_X[:, 2], np.nan) \n\n\n\n", "def test_imputation_sparse_csc_axis_1():\n    # Test imputation with sparse CSC matrix and axis=1\n    X = sparse.csc_matrix([\n        [1, 2, np.nan],\n        [4, np.nan, 6],\n        [7, 8, 9],\n    ])\n    statistics = [np.nan, 4.0, 8.0]\n\n    imputer = Imputer(missing_values=0, strategy=\"mean\", axis=1)\n    X_trans = imputer.fit(X).transform(X)\n    assert_array_almost_equal(X_trans.toarray(),\n                              [[1, 3, 6], [4, 4, 6], [7, 8, 9]])\n    assert_array_almost_equal(imputer.statistics_, statistics)\n\n", "def test_imputation_sparse_csr_dense_axis_1():\n    \"\"\"Test imputation with sparse CSR matrix and axis=1.\"\"\"\n    X_sparse = sparse.csr_matrix([[1, 2, 3], [4, np.nan, 6], [7, 8, 9]])\n    X_dense = X_sparse.toarray()\n\n    for strategy in [\"mean\", \"median\", \"most_frequent\"]:\n        imputer = Imputer(missing_values=0, strategy=strategy, axis=1)\n        imputer.fit(X_sparse)\n        X_imputed_sparse = imputer.transform(X_sparse)\n        X_imputed_dense = imputer.transform(X_dense)\n\n        assert_array_almost_equal(X_imputed_sparse.toarray(),\n                                  X_imputed_dense,\n                                  err_msg=\n                                  \"Fail to transform data after converting \"\n                                  \"from sparse to dense (strategy = %s)\" %\n                                  (strategy))\n\n\n\n", "def test_imputation_sparse_data_with_NaN_missing_values():\n    # Test imputation with sparse data and NaN as missing values\n    X = sparse.csc_matrix([[1, 2, np.nan],\n                           [4, np.nan, 6],\n                           [7, 8, np.nan]])\n    X_true = sparse.csc_matrix([[1, 2, 3.5],\n                               [4, 5, 6],\n                               [7, 8, 7.5]])\n\n    imputer = Imputer(missing_values=np.nan, strategy=\"mean\", axis=0)\n    X_imputed = imputer.fit(X).transform(X)\n\n    assert_array_almost_equal(X_imputed.toarray(), X_true.toarray())\n", "def test_imputation_sparse_csc_axis_1():\n    # Test imputation with sparse CSC matrix and axis=1\n    X = sparse.csc_matrix(np.arange(12).reshape(3, 4))\n    missing_values = 0\n    imputer = Imputer(missing_values=missing_values, strategy=\"mean\", axis=1)\n    imputer.fit(X)\n    X_trans = imputer.transform(X)\n    assert_array_almost_equal(X_trans.mean(axis=0),\n                              imputer.statistics_, err_msg=\"Error with mean imputation in axis=1\")\n"], "sample_1069": ["def test_loggamma():\n    assert octave_code(loggamma(x)) == 'log(gamma(x))'\n    assert octave_code(loggamma(x, evaluate=False)) == 'log(gamma(x))' \n", "def test_floordiv():\n    assert mcode(x // y) == \"floor(x/y)\"\n    assert mcode(x // 3) == \"floor(x/3)\"\n", "def test_log_with_negative_argument():\n    assert mcode(log(x)) == 'log(x)'\n    assert mcode(log(-x)) == '% Not supported in Octave:\\n% log\\nlog(-x)'\n", "def test_polylog():\n    assert octave_code(polylog(2, x)) == 'polylog(2, x)'\n    assert octave_code(polylog(x, y)) == '% Not supported in Octave:\\n% polylog\\npolylog(x, y)'\n", "def test_logarithm_base():\n    assert mcode(log(x, 2)) == 'log2(x)'\n    assert mcode(log(x, 10)) == 'log10(x)'\n    assert mcode(log(x, pi)) == 'log(x, pi)'\n"], "sample_286": ["    def test_refresh_complex_lookup(self):\n        a1 = Article.objects.create(headline='Foo', pub_date=datetime(2023, 10, 26))\n        a2 = Article.objects.create(headline='Bar', pub_date=datetime(2023, 10, 26))\n\n        # Complex lookup to test refresh_from_db() on nested queries\n        a1_complex = Article.objects.annotate(\n            related_count=Count('relatedarticle_set')\n        ).filter(related_count__gte=0).get(headline='Foo')\n\n        with self.assertNumQueries(1):\n            a1_complex.refresh_from_db()  \n            self.assertEqual(a1_complex.related_count, 0)\n\n        rel = RelatedArticle.objects.create(article=a1)\n        a1_complex = Article.objects.annotate(\n            related_count=Count('relatedarticle_set')\n        ).filter(headline='Foo').get()\n\n        with self.assertNumQueries(1):\n            a1_complex.refresh_from_db()\n            self.assertEqual(a1_complex.related_count, 1)\n\n\n\n", "    def test_refresh_with_object_state_changed(self):\n        a = Article.objects.create(headline='foo', pub_date=datetime.now())\n        a.headline = 'bar'  # Modify the object's state\n        \n        # Simulate a scenario where the object's state is changed before refresh\n        with self.assertNumQueries(1):\n            a.save()\n\n        a.headline = 'baz'  # Modify the object's state again\n        \n        with self.assertNumQueries(1):\n            a.refresh_from_db()\n        \n        self.assertNotEqual(a.headline, 'bar')\n        self.assertEqual(a.headline, 'foo')  \n", "    def test_refresh_on_save(self):\n        a = Article.objects.create(headline='foo', pub_date=datetime.now())\n        with self.assertNumQueries(1):\n            a.save()\n        old_headline = a.headline\n\n        a.headline = 'bar'\n        a.save()\n        with self.assertNumQueries(1):\n            a.refresh_from_db()\n        self.assertEqual(a.headline, 'bar')\n", "    def test_refresh_multiple(self):\n        a = Article.objects.create(headline='foo', pub_date=datetime.now())\n        b = Article.objects.create(headline='bar', pub_date=datetime.now())\n        with self.assertNumQueries(1):\n            a.refresh_from_db()\n            b.refresh_from_db()\n        self.assertEqual(a.headline, 'foo')\n        self.assertEqual(b.headline, 'bar')\n", "    def test_refresh_with_related_object(self):\n        a = Article.objects.create(headline='Foo', pub_date=datetime.now())\n        s = SelfRef.objects.create(article=a)\n        with self.assertNumQueries(1):\n            s.refresh_from_db()\n        self.assertEqual(s.article.headline, 'Foo')\n        a.headline = 'Bar'\n        a.save()\n        with self.assertNumQueries(1):\n            s.refresh_from_db()\n        self.assertEqual(s.article.headline, 'Bar')\n"], "sample_236": ["    def test_fast_delete_with_signals(self):\n        \"\"\"\n        Deletion signals should be triggered even if fast-delete is used.\n        \"\"\"\n        called = False\n\n            nonlocal called\n            called = True\n\n        models.signals.pre_delete.connect(receiver, sender=R)\n        r = R.objects.create()\n        r.delete()\n        self.assertTrue(called)\n        models.signals.pre_delete.disconnect(receiver, sender=R)\n\n        called = False\n        models.signals.post_delete.connect(receiver, sender=R)\n        r = R.objects.create()\n        r.delete()\n        self.assertTrue(called)\n        models.signals.post_delete.disconnect(receiver, sender=R)\n\n", "    def test_fast_delete_with_keep_parents(self):\n        child = RChild.objects.create()\n        parent = child.r_ptr\n        deleted_child, deleted_rows = child.delete(keep_parents=True)\n        self.assertEqual(deleted_child, 1)\n        # self.assertEqual(deleted_rows, 0) # This is a bit tricky because the object deleted\n        # has no relations we are deleting, but we should\n        # not be adding a new query to find related orcas\n        self.assertTrue(RChild.objects.filter(pk=child.pk).exists())\n        self.assertTrue(R.objects.filter(pk=parent.pk).exists())\n\n", "    def test_fast_delete_related_with_default(self):\n            thing = Thing.objects.create(pk=1)\n            thing.related = RelatedThing.objects.create(thing=thing)\n            return thing\n\n        thing = create_thing()\n        self.assertNumQueries(2, thing.related.delete)\n        self.assertFalse(thing.related.exists())\n        self.assertEqual(thing.pk, 1)\n\n\n\n", "    def test_fast_delete_signals_for_related_objects(self):\n            return sender == RChild\n\n        models.signals.post_delete.connect(receiver, sender=R)\n        child = RChild.objects.create()\n        parent = child.r_ptr\n        # 1 for child delete, 1 for signal firing for parent\n        self.assertNumQueries(2, child.delete)\n        self.assertFalse(RChild.objects.exists())\n        self.assertTrue(R.objects.filter(pk=parent.pk).exists())\n        models.signals.post_delete.disconnect(receiver, sender=R)\n", "    def test_fast_delete_with_signals(self):\n        \"\"\"\n        #25685 - Deleting instances of a model with existing proxy\n        classes should not issue multiple queries during cascade\n        deletion of referring models when signals are connected.\n        \"\"\"\n        avatar = Avatar.objects.create()\n        origin = Origin.objects.create()\n        referer = Referrer.objects.create(origin=origin)\n\n            pass\n\n        models.signals.pre_delete.connect(receiver, sender=Referrer)\n        with self.assertNumQueries(2):\n            avatar.delete()\n        models.signals.pre_delete.disconnect(receiver, sender=Referrer)\n\n"], "sample_1197": ["def test_issue_25582():\n    from sympy.physics.units import Quantity, kilogram, meter, second, joule\n    q = Quantity(\"q\",  dimension=joule * kilogram, scale_factor=1)\n    assert q.is_dimensionless is False\n    assert q.unit == joule * kilogram\n    assert q.scale_factor == 1 * joule * kilogram\n    assert q.convert_to(q) == q\n\n    q2 = Quantity(\"q2\", dimension=joule)\n    assert q2.is_dimensionless is False\n    assert q2.unit == joule\n    assert q2.scale_factor == 1 * joule\n    assert q2.convert_to(q2) == q2\n\n    assert q.convert_to(q2) == q / kilogram\n", "compilation error", "def test_issue_24063():\n    from sympy.physics.units import Quantity, meter, second,  kilogram, joule\n\n    quantity_1 = Quantity(\"quantity_1\")\n    quantity_2 = Quantity(\"quantity_2\")\n\n    quantity_1.set_global_relative_scale_factor(1, meter)\n    quantity_2.set_global_relative_scale_factor(1, second)\n\n    expr = quantity_1 / quantity_2\n    assert expr.units == meter/second\n    \n\n\n\n", "def test_issue_24433():\n    from sympy.physics.units import Quantity, meter, kilogram, second, joule\n    from sympy.physics.units.systems.si import SI\n    M = Quantity(\"M\")\n    L = Quantity(\"L\")\n    T = Quantity(\"T\")\n\n    SI.set_quantity_dimension(M, kilogram)\n    SI.set_quantity_dimension(L, meter)\n    SI.set_quantity_dimension(T, second)\n\n    expr = (M * L**2) / (T**3)\n    assert SI._collect_factor_and_dimension(expr)[1] == (kilogram*meter**2)/second**3\n    assert SI._collect_factor_and_dimension(expr)[0] == 1\n\n\n\n", "def test_issue_25053():\n    from sympy.physics.units import electric_charge\n    q = Quantity(\"q\", electric_charge)\n    SI.set_quantity_dimension(q, electric_charge)\n    q.set_global_relative_scale_factor(1, coulomb)\n    assert q/2 == Quantity(\"q/2\", electric_charge)\n    assert q/q == Quantity(\"1\", electric_charge)\n    assert q * 2 == Quantity(\"2q\", electric_charge)\n    assert q * q == Quantity(\"q**2\", electric_charge)\n"], "sample_554": ["def test_text_antialiased_on_with_scale():\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.text(0.5, 0.5, 'foo', fontsize=48,\n            fontweight='bold', antialiased=True,\n            transform=ax.transAxes)\n    fig.savefig(io.BytesIO(), format='png')\n\n\n\n", "def test_text_bbox_clipping():\n    fig, ax = plt.subplots()\n    text = ax.text(0.9, 0.9, 'This text should be clipped',\n                   ha='right', va='top', clip_on=True,\n                   bbox={'facecolor': 'red'})\n    fig.canvas.draw()\n    bbox = text.get_window_extent()\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    fig.canvas.draw()\n    assert bbox.width < 0.1  \n    assert bbox.height < 0.1\n", "def test_text_as_path_bbox():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, 'abc', transform=ax.transAxes)\n\n    # Check that bbox is correct when text is drawn as path\n    bbox = text.get_bbox()\n    assert bbox is not None\n\n    # Draw text as path and check bbox again\n    text.set_rasterized(True)\n    bbox_path = text.get_bbox()\n    assert bbox_path is not None\n    assert bbox.x0 == bbox_path.x0\n    assert bbox.y0 == bbox_path.y0\n    assert bbox.x1 == bbox_path.x1\n    assert bbox.y1 == bbox_path.y1\n\n\n\n", "def test_text_get_bbox_with_rotation():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, 'foo', rotation=45)\n    bbox = text.get_bbox(renderer=fig.canvas.get_renderer())\n    assert bbox.width != 0\n    assert bbox.height != 0\n", "def test_text_alignment_with_bbox():\n    fig, ax = plt.subplots()\n    bbox = dict(boxstyle=\"round, pad=0.1\", facecolor='lightblue',\n                alpha=0.5)\n    text = ax.text(0.5, 0.5, 'foo', bbox=bbox, ha='center', va='center')\n    fig.canvas.draw()\n    # Ensure the text is centered within the bbox\n    bbox_rect = text.get_window_extent()\n    text_rect = text._get_text_extent()\n    assert bbox_rect.x0 <= text_rect.x0 <= bbox_rect.x1\n    assert bbox_rect.y0 <= text_rect.y0 <= bbox_rect.y1\n"], "sample_690": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1179": ["def test_issue_22821():\n    from sympy.physics.units import cm, m\n    assert str(1*cm) == '1*cm'\n    assert str(1.0*cm) == '1.0*cm'\n    assert str(2 + cm) == '2 + cm'\n", "def test_issue_23318():\n    from sympy.physics.units import cm, m\n    assert str(m * cm) == 'm*cm'\n\n\n\n", "def test_MatrixBase():\n    A = MatrixBase(3, 3)\n    assert str(A) == 'MatrixBase(3, 3)'\n\n", "def test_MatrixPow():\n    A = MatrixSymbol(\"A\", 2, 2)\n    assert str(A**0) == 'A**0'\n    assert str(A**1) == 'A**1'\n    assert str(A**-1) == 'A**(-1)'\n    assert str(A**(1/2)) == 'A**(1/2)'\n\n\n\n", "def test_Tuple():\n    t = Tuple(x, y, z)\n    assert str(t) == '(x, y, z)'\n    assert str(t.args) == '(x, y, z)'\n"], "sample_856": ["compilation error", "def test_shuffle_split_with_indices():\n    rng = np.random.RandomState(42)\n    X = rng.rand(10, 2)\n    y = rng.randint(0, 2, size=10)\n\n    cv = ShuffleSplit(n_splits=3, test_size=0.2, random_state=rng)\n    for train_index, test_index in cv.split(X, y):\n        assert set(train_index).isdisjoint(set(test_index))\n        assert len(train_index) + len(test_index) == len(X)\n\n\n", "def test_repeated_stratified_kfold_reproducibility():\n    rng = np.random.RandomState(42)\n    X = rng.rand(100, 20)\n    y = rng.randint(0, 2, 100)\n    rsf = RepeatedStratifiedKFold(n_splits=3, n_repeats=2, random_state=42)\n\n    for i in range(rsf.get_n_splits()):\n        train, test = next(rsf.split(X, y))\n        assert_array_equal(train, rsf.split(X, y))\n\n\n\n\n", "def test_time_series_cv_with_groups():\n    X = np.arange(10).reshape((10, 1))\n    groups = np.array([0, 0, 1, 1, 2, 2, 3, 3, 4, 4])\n\n    tscv = TimeSeriesSplit(n_splits=3)\n\n    splits = tscv.split(X, groups=groups)\n    for train, test in splits:\n        assert (np.all(groups[train] == groups[test]) ==\n                False)  \n", "def test_time_series_cv_multivariate():\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13, 14]])\n    X = X[:, np.newaxis, :]  \n    tscv = TimeSeriesSplit(2)\n\n    splits = tscv.split(X)\n\n    # Ensure that the Time Series CV preserves the data ordering for multivariate data\n    for train, test in splits:\n        assert np.all(np.sort(train) == np.sort(test)) \n"], "sample_362": ["    def test_rename_field(self):\n        changes = self.get_changes(\n            [self.author_empty],\n            [self.author_rename_field_name]\n        )\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['RenameField'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='first_name', new_name='given_name', model_name='Author')\n", "    def test_operation_with_custom_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Person', fields=[], name='MyPerson'),\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'myperson')\n", "    def test_operation_with_unicode_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Person', fields=[\n                    models.CharField(max_length=255, name='Name'),\n                ]),\n            ]\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'person')\n", "    def test_operation_with_non_alphanumeric_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Person_With-Special_Chars', fields=[]),\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'person_with_special_chars')\n", "    def test_suggest_name_with_custom_field_operations(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Person', fields=[\n                    migrations.TextField('bio', blank=True, null=True),\n                ]),\n                migrations.AlterField(\n                    model_name='Person',\n                    old_field='bio',\n                    new_field=models.TextField('bio', blank=True),\n                ),\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'person_bio')\n"], "sample_198": ["    def test_group_by_with_alias(self):\n        expr = ExpressionWrapper(Value(3), output_field=IntegerField())\n        self.assertEqual(expr.get_group_by_cols(alias='foo'), ['foo'])\n", "    def test_order_by_with_wrapper(self):\n        expr = ExpressionWrapper(F('age') + Value(1), output_field=IntegerField())\n        ordering = OrderBy(expr)\n        self.assertEqual(ordering.expression, expr)\n        self.assertEqual(ordering.field_name, None)\n        self.assertEqual(ordering.descending, False)\n", "    def test_group_by_with_alias(self):\n        expr = ExpressionWrapper(Value(3), output_field=IntegerField())\n        self.assertEqual(expr.get_group_by_cols(alias='my_alias'), ['my_alias'])\n ", "    def test_expression_wrapper_output_field(self):\n        expr = ExpressionWrapper(\n            ExpressionList(F('cost') + F('tax')), output_field=IntegerField()\n        )\n        self.assertEqual(expr.output_field, IntegerField())\n\n\n\n\n", "    def test_deconstruct(self):\n        expr = ExpressionWrapper(Value(3), output_field=IntegerField())\n        path, args, kwargs = expr.deconstruct()\n        self.assertEqual(path, 'django.db.models.expressions.ExpressionWrapper')\n        self.assertEqual(args, (Value(3),))\n        self.assertEqual(kwargs, {'output_field': IntegerField()})\n\n"], "sample_130": ["    def test_clone_with_deferred_fields(self):\n        query = Query(Item)\n        query.add_deferred_loading(['id'])\n        clone = query.clone()\n        clone.add_deferred_loading(['name', 'created'])\n        self.assertEqual(query.deferred_to_data, {})\n        self.assertEqual(clone.deferred_to_data, {'name': {'created'}, 'created': {'name'}})\n", "    def test_deferred_loading(self):\n        query = Query(Item)\n        query.add_deferred_loading(['creator__name', 'related__id'])\n        self.assertEqual(query.deferred_to_data, set([('Item', {'creator__name': None, 'related__id': None})]))\n\n\n\n", "    def test_deferred_loading(self):\n        query = Query(Item)\n        query.add_fields(['id', 'name'])\n        query.add_deferred_loading(['creator__name'])\n        self.assertEqual(query.deferred_loading, (frozenset(['id', 'name']), False))\n        self.assertEqual(query.get_loaded_field_names(), {'Item': {'creator__name': None}})\n\n        query = Query(Item)\n        query.add_fields(['id', 'name'])\n        query.add_deferred_loading(['creator__', 'creator__name'])\n        self.assertEqual(query.deferred_loading, (frozenset(['id', 'name']), False))\n        self.assertEqual(query.get_loaded_field_names(), {'Item': {'creator__': None, 'creator__name': None}})\n        \n        query = Query(Author)\n        query.add_fields(['id', 'name'])\n        query.add_deferred_loading(['books__title'])\n        self.assertEqual(query.deferred_loading, (frozenset(['id', 'name']), False))\n        self.assertEqual(query.get_loaded_field_names(), {'Author': {'books__title': None}})\n", "    def test_join_promotion(self):\n        query = Query(Item)\n        query.add_remote_join('authors', Author, 'item_authors')\n        query.add_remote_join('creators', Author, 'author_items')\n        query.set_where(Q(authors__num__gt=2) | Q(creators__name__iexact='john'))\n        promoter = JoinPromoter(OR, 2, False)\n        promoter.add_votes({'item_authors': 1, 'author_items': 1})\n        promoter.update_join_types(query)\n        self.assertEqual(query.alias_map['item_authors'].join_type, 'LEFT OUTER')\n        self.assertEqual(query.alias_map['author_items'].join_type, 'LEFT OUTER')\n", "    def test_add_deferred_loading(self):\n        query = Query(Item)\n        query.add_deferred_loading(['id', 'description'])\n        self.assertEqual(query.deferred_loading, (frozenset(['id', 'description']), True))\n        query.add_deferred_loading(['content'])\n        self.assertEqual(query.deferred_loading, (frozenset(['id', 'description', 'content']), True))\n\n\n"], "sample_242": ["    def test_contains_lookup_with_empty_rhs(self):\n        lookup = Contains(Value('hello'), Value(''))\n        params = lookup.as_sql(mock.Mock(), mock.Mock())\n        self.assertEqual(params[0], '%%')\n        self.assertEqual(params[1], '')\n", "    def test_year_lookup_exact(self):\n        now = datetime.now()\n        lookup = YearExact(Value(now), Value(now.year))\n        sql = lookup.as_sql(mock.Mock(), mock.Mock())\n        self.assertEqual(sql[0], 'field_name BETWEEN %s AND %s')\n", "    def test_relabeled_clone(self):\n        lookup = Lookup(Value(1), Value(2))\n        new_lookup = lookup.relabeled_clone({'a': 'b'})\n        self.assertNotEqual(lookup, new_lookup)\n        self.assertEqual(new_lookup.lhs.output_field.name, 'b')\n        self.assertEqual(new_lookup.rhs.output_field.name, 'b')\n\n\n", "    def test_year_lookup_bounds(self):\n        with self.settings(INSTALLED_APPS={'my_app': 'tests.apps.MyappConfig'}):\n            from my_app.models import MyModel\n\n            MyModel.objects.create(my_date=datetime(2023, 1, 1))\n            MyModel.objects.create(my_date=datetime(2024, 2, 2))\n\n            class TestYearLookup(YearLookup):\n                lookup_name = 'test_year'\n                    return (start,)\n            \n            lookup = TestYearLookup(MyModel.objects.get(pk=1).my_date, Value(2023))\n            self.assertEqual(lookup.year_lookup_bounds(self.connection, 2023), (2023, 2023))\n\n\n", "    def test_year_lookup(self):\n        lookup = YearExact(Value(1), Value(2023))\n        # Assuming connection_class is a mock object for testing.\n        with mock.patch('django.db.models.sql.compiler.Compiler.connection', new_callable=mock.MagicMock) as connection:\n            connection.ops.year_lookup_bounds_for_datetime_field.return_value = (Value(20230101), Value(20231231))\n            sql, params = lookup.as_sql(mock.Mock(), connection)\n            self.assertEqual(sql, 'col BETWEEN 20230101 AND 20231231')\n            self.assertEqual(params, [])\n\n"], "sample_57": ["    def test_password_length_validation(self):\n        user = User.objects.get(username='testclient')\n        data = {'password1': 'test', 'password2': 'test'}\n        form = AdminPasswordChangeForm(user, data)\n        with self.assertNumQueries(1):\n            self.assertFalse(form.is_valid())\n        self.assertIn(_('Enter a password with at least 8 characters.'), form.errors['password1'])\n        self.assertIn(_('Enter a password with at least 8 characters.'), form.errors['password2'])\n", "    def test_password_validation_errors(self):\n        user = User.objects.get(username='testclient')\n        data = {\n            'password1': 'abc',\n            'password2': 'abc',\n        }\n        # Simulate a password validation error\n        with patch('django.contrib.auth.password_validation.validate_password') as mock_validate:\n            mock_validate.side_effect = ValueError(\"Invalid password\")\n            form = AdminPasswordChangeForm(user, data)\n            self.assertFalse(form.is_valid())\n            self.assertEqual(form.errors['password1'], ['Invalid password'])\n            self.assertEqual(form.errors['password2'], ['Invalid password'])\n\n\n", "    def test_bug_23685_unicode_passwords(self):\n        user = User.objects.get(username='testclient')\n        decoded_password = 'p\\xa9ssword'.encode('utf-8').decode('utf-8')\n        data = {'password1': decoded_password, 'password2': decoded_password}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertTrue(form.is_valid())\n\n        user.password = form.cleaned_data['password1']\n        user.save()\n        self.assertEqual(user.password, form.cleaned_data['password1'])\n", "    def test_password_validation_errors(self):\n        user = User.objects.get(username='testclient')\n        data = {'password1': 'password', 'password2': 'password'}\n        form = AdminPasswordChangeForm(user, data)\n        with self.assertNumWarnings(1, 'Password validation using PBKDF2PasswordHasher is deprecated.'):\n            self.assertFalse(form.is_valid())\n        self.assertIn('password1', form.errors)\n        self.assertIn(\n            'Enter a strong password. It must contain at least 1 uppercase letter, 1 lowercase letter, 1 digit and 1 special character.',\n            form.errors['password1'][0]\n        )\n\n", "    def test_password_too_short(self):\n        user = User.objects.get(username='testclient')\n        data = {'password1': 'abc', 'password2': 'abc'}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(\n            form.errors['password1'],\n            ['Password is too short. It must contain at least 8 characters.']\n        )\n        self.assertEqual(\n            form.errors['password2'],\n            ['Password is too short. It must contain at least 8 characters.']\n        )\n"], "sample_847": ["def test_enet_positive_with_multi_output():\n    X, Y, _, _ = build_dataset(n_samples=50, n_features=50, n_targets=2)\n    with pytest.raises_regex(ValueError, \"The positive parameter is not supported for MultiTaskElasticNet\"):\n        MultiTaskElasticNet(positive=True).fit(X, Y)\n\n\n\n", "def test_enet_multioutput_validation_with_precompute():\n    X, y, X_test, y_test = build_dataset(n_samples=100, n_features=50, n_targets=2)\n\n    for precompute in [True, False]:\n        for normalize in [True, False]:\n            clf = MultiTaskElasticNet(\n                alpha=0.1, l1_ratio=0.5,\n                max_iter=100, fit_intercept=True, normalize=normalize,\n                precompute=precompute)\n            clf.fit(X, y)\n            assert clf.coef_.shape == (2, 50)\n            assert clf.intercept_.shape == (2,)\n\n            X_test = X_test.astype(np.float64)\n\n            y_pred = clf.predict(X_test)\n            assert y_pred.shape == (100, 2)\n\n            # Check that the predictions are valid\n            for i in range(2):\n                assert np.min(y_pred[:, i]) >= 0\n                assert np.max(y_pred[:, i]) <= 1\n\n\n\n", "def test_multitask_enet_with_sparse_input():\n    X, y, _, _ = build_dataset(n_samples=100, n_features=50)\n    y = np.c_[y, y * 2]  # Create a multi-target dataset\n\n    # Test with sparse input\n    X_sparse = sparse.csr_matrix(X)\n    clf = MultiTaskElasticNet(alpha=0.5, l1_ratio=0.5).fit(X_sparse, y)\n    assert isinstance(clf.coef_, list)\n    assert isinstance(clf.coef_[0], sparse.csr_matrix)\n\n\n\n", "def test_enet_tol_convergence(tol):\n    X, y, _, _ = build_dataset()\n\n    X_test = X.copy()\n    y_test = y.copy()\n    clf = ElasticNet(tol=tol, alpha=0.5, max_iter=100)\n    clf.fit(X, y)\n\n    # Check convergence\n    coef_start = clf.coef_.copy()\n    clf.fit(X_test, y_test, tol=tol, max_iter=100)\n    coef_end = clf.coef_\n\n    assert_array_almost_equal(coef_start, coef_end, decimal=10,\n                              err_msg=f\"Tolerance {tol} did not lead to convergence\")\n\n\n", "def test_enet_multi_output_with_sparse_input():\n    X, y, _, _ = build_dataset(n_samples=100, n_features=50)\n    y = y[:, np.newaxis]  # Convert to 2D array for multi-output\n    X_sparse = sparse.csr_matrix(X)\n\n    clf = ElasticNet(alpha=0.5, precompute=True)\n    clf.fit(X_sparse, y)\n    assert_array_almost_equal(clf.coef_, clf.coef_)\n"], "sample_811": ["def test_sparse_matrix_incompatibility():\n    # Check that sparse matrices cannot be mixed with dense matrices\n    X = np.array([[1, 2], [3, 4]])\n    Y = csr_matrix([[5, 6], [7, 8]])\n    with pytest.raises(TypeError):\n        pairwise_kernels(X, Y, metric='cosine')\n\n    with pytest.raises(TypeError):\n        pairwise_kernels(X, Y, metric='euclidean')\n\n    with pytest.raises(TypeError):\n        pairwise_kernels(X, Y, metric='linear')\n\n    with pytest.raises(TypeError):\n        check_pairwise_arrays(X, Y)\n\n\n", "def test_check_sparse_matrices_mismatch():\n    # Ensure an error is raised if inputs are sparse and dense.\n    XA = csr_matrix(np.arange(40).reshape(5, 8))\n    XB = np.arange(40).reshape(5, 8)\n\n    assert_raises(ValueError, check_pairwise_arrays, XA, XB) \n", "def test_additive_chi2_kernel_sparse_matrices():\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    X_sparse = csr_matrix(X)\n    K = additive_chi2_kernel(X_sparse, X_sparse)\n    assert issparse(K)\n\n\n", "def test_check_different_types():\n    # Ensures an error is raised if the input types are different\n    XA = np.resize(np.arange(40), (5, 8)).astype(np.float32)\n    XB = np.resize(np.arange(40), (5, 8)).astype(np.int32)\n    assert_raises(TypeError, check_pairwise_arrays, XA, XB)\n\n    XB = np.resize(np.arange(40), (5, 8)).astype(np.float64)\n    assert_raises(TypeError, check_pairwise_arrays, XA, XB)\n\n\n", "def test_check_pairwise_arrays_mixed_dtypes():\n    # Ensures that checks handle mixed dtypes gracefully.\n\n    XA = np.array([[1, 2.0, 3, 4], [5, 6.5, 7, 8]],\n                  dtype=[('a', np.int32), ('b', np.float64), ('c', np.int32), ('d', np.float32)])\n    XB = np.array([[9, 10, 11, 12], [13, 14.5, 15, 16]],\n                  dtype=[('a', np.int32), ('b', np.float64), ('c', np.int32), ('d', np.float32)])\n\n    XA_checked, XB_checked = check_pairwise_arrays(XA, XB)\n\n    # Type of elements should be same after conversion.\n    for i in range(XA.shape[0]):\n        for j in range(XA.shape[1]):\n            assert XA_checked[i, j].dtype == XA[i, j].dtype\n            assert XB_checked[i, j].dtype == XB[i, j].dtype\n"], "sample_396": ["    def test_ticket_24605_with_alias(self):\n        \"\"\"\n        Subquery table names should be quoted even when using aliases.\n        \"\"\"\n        i1 = Individual.objects.create(alive=True)\n        RelatedIndividual.objects.create(related=i1)\n        i2 = Individual.objects.create(alive=False)\n        RelatedIndividual.objects.create(related=i2)\n        i3 = Individual.objects.create(alive=True)\n        i4 = Individual.objects.create(alive=False)\n\n        self.assertSequenceEqual(\n            Individual.objects.filter(\n                Q(alive=False), Q(related_individual__isnull=True)\n            ).values(\"id\", \"alive\"),\n            [{\"id\": i4.pk, \"alive\": False}],\n        )\n", "    def test_ticket_24992(self):\n        parent = Parent.objects.create()\n        child1 = Child.objects.create(parent=parent)\n        child2 = Child.objects.create(parent=parent)\n        self.assertSequenceEqual(\n            Parent.objects.filter(children__isnull=False).all(), [parent]\n        )\n", "    def test_ticket_24605_complex_subquery(self):\n        \"\"\"\n        More complex subquery nesting should also quote tables correctly.\n        \"\"\"\n        i1 = Individual.objects.create(alive=True)\n        i2 = Individual.objects.create(alive=False)\n        r1 = RelatedIndividual.objects.create(related=i1)\n        r2 = RelatedIndividual.objects.create(related=i2)\n\n        self.assertSequenceEqual(\n            Individual.objects.filter(\n                Q(alive=True) & Q(related_individual__isnull=False),\n                Q(related_individual__id=r1.id),\n            ),\n            [i1],\n        )\n", "    def test_ticket_24605_relation_subquery(self):\n        i1 = Individual.objects.create(alive=True)\n        ri1 = RelatedIndividual.objects.create(related=i1)\n        i2 = Individual.objects.create(alive=False)\n        ri2 = RelatedIndividual.objects.create(related=i2)\n        qs = Individual.objects.filter(\n            Q(related_individual__isnull=False), Q(alive=True)\n        )\n        self.assertSequenceEqual(qs, [i1])\n", "    def test_ticket_24605_subquery_ordering(self):\n        \"\"\"\n        Subqueries should be ordered correctly even when used in an outer query\n        \"\"\"\n        Individual.objects.create(alive=True)\n        i2 = Individual.objects.create(alive=False)\n        i3 = Individual.objects.create(alive=True)\n        i4 = Individual.objects.create(alive=False)\n        RelatedIndividual.objects.create(related=i2)\n        RelatedIndividual.objects.create(related=i3)\n\n        self.assertSequenceEqual(\n            Individual.objects.filter(\n                alive=True\n            ).exclude(\n                related_individual__pk__in=Individual.objects.filter(alive=False)\n            ).order_by(\"pk\"),\n            [i3],\n        )\n"], "sample_675": ["compilation error", "        def test_emit_error():\n            logger.fatal(\"oops\", 'first', 2)", "    def test_log_level_override(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import logging\n\n            logging.basicConfig(level=logging.WARNING)\n\n                logging.warning(\"This message should be logged\")\n\n                logging.info(\"This message should not be logged\")\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines(\n            [\n                \"*::test_log_warning.py*\",\n                \"*WARNING*This message should be logged*\",\n            ]\n        )\n        result.stdout.no_fnmatch_line(\"*::test_log_info.py*\")\n", "        def test_bad_log():\n            logging.warning('oops', 'first', 2)", "    def test_log_level_inheritance(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import logging\n\n            logging.basicConfig(level=logging.WARNING)\n\n                logging.info(\"this should be ignored\")\n                logging.warning(\"this should be logged\")\n\n                logging.debug(\"this should be ignored\")\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines(\n            [\n                \"*test_log_level_inheritance.py PASSED\",\n            ]\n        )\n"], "sample_866": ["def test_affinity_propagation_preference_handling():\n    # Test preference handling with different values.\n\n    # Small preference, should result in all points being exemplars\n    X = np.array([[1, 1], [-1, -1], [1, -1]])\n    S = -euclidean_distances(X, squared=True)\n    preference = -10\n    cluster_centers_indices, labels = affinity_propagation(S, preference)\n    assert np.all(np.unique(labels) == np.arange(X.shape[0]))\n\n    # Large preference, should result in a single cluster\n    preference = 10\n    cluster_centers_indices, labels = affinity_propagation(S, preference)\n    assert len(np.unique(labels)) == 1\n    assert np.all(labels == 0)\n\n\n    # Test preference array\n    preference_array = np.array([1, -10, 1])\n    cluster_centers_indices, labels = affinity_propagation(S, preference=preference_array)\n    assert np.all(np.unique(labels) == np.arange(X.shape[0]))\n", "def test_affinity_propagation_preference_default():\n    # Test default preference handling\n    X = np.array([[1, 1], [-1, -1], [1, -1]])\n    S = -euclidean_distances(X, squared=True)\n\n    # Fit with default preference\n    af = AffinityPropagation().fit(X)\n\n    # Check if preference was set to the median\n    assert np.isclose(af.preference, np.median(S))\n\n    # Ensure results are consistent with expected median preference\n    # (this may require additional testing based on dataset specifics)\n    # ... (Add assertions for expected cluster assignments/centers here)\n\n\n\n", "def test_affinity_propagation_preference_scaling():\n    n_samples = 10\n    X = np.random.rand(n_samples, 2)\n    S = -euclidean_distances(X, squared=True)\n\n    # Test that different preference scales lead to different number\n    # of clusters.\n\n    # Small preference, should form many clusters\n    af_small = AffinityPropagation(preference=0.1, max_iter=100)\n    cluster_centers_indices_small = af_small.fit(X).cluster_centers_indices_\n\n    af_large = AffinityPropagation(preference=10, max_iter=100)\n    cluster_centers_indices_large = af_large.fit(X).cluster_centers_indices_\n\n    assert len(cluster_centers_indices_small) > len(cluster_centers_indices_large)\n\n    # Test with sparse data\n    sparse_X = csr_matrix(X)\n    af_sparse_small = AffinityPropagation(preference=0.1, max_iter=100)\n    cluster_centers_indices_sparse_small = af_sparse_small.fit(sparse_X).cluster_centers_indices_\n\n    af_sparse_large = AffinityPropagation(preference=10, max_iter=100)\n    cluster_centers_indices_sparse_large = af_sparse_large.fit(sparse_X).cluster_centers_indices_\n\n    assert len(cluster_centers_indices_sparse_small) > len(cluster_centers_indices_sparse_large)\n\n\n", "def test_affinity_propagation_preference_impact_on_clusters():\n    # Test the impact of preference on cluster formation\n    # With high preference, all samples become clusters\n    X = np.array([[1, 2], [1, 4], [1, 0], [4, 2], [4, 4], [4, 0]])\n    S = -euclidean_distances(X, squared=True)\n    labels_high_preference = affinity_propagation(S, preference=10)[1]\n    assert_array_equal(np.arange(len(X)), labels_high_preference)\n\n    # With low preference, a single cluster forms\n    labels_low_preference = affinity_propagation(S, preference=-10)[1]\n    assert_array_equal(np.zeros_like(labels_high_preference), labels_low_preference)\n\n    # Test with a preference close to the median similarity\n    median_similarity = np.median(S)\n    labels_median_preference = affinity_propagation(S, preference=median_similarity)[1]\n    assert_array_equal(np.unique(labels_median_preference),\n                       np.arange(len(X)))\n\n\n", "def test_affinity_propagation_preference_influence():\n    # Test the influence of preference on cluster number\n    X = np.array([[1, 1], [-1, -1], [0, 0]])\n\n    # Similarities should be set to a matrix with all values equal to 0\n    S = np.zeros((3, 3))\n\n    # Test with preference equal to min similarity\n    n_clusters_min = len(affinity_propagation(S,\n                                            preference=np.min(S))[0])\n    assert n_clusters_min == 3\n\n    # Test with preference larger than min similarity\n    preference = np.mean(S)\n    n_clusters = len(affinity_propagation(S, preference=preference)[0])\n    assert n_clusters == 1\n\n    # Test with preference significantly larger than min similarity\n    preference = np.max(S)\n    n_clusters = len(affinity_propagation(S, preference=preference)[0])\n    assert n_clusters == 3\n"], "sample_233": ["    def test_token_generation_with_legacy_argument(self):\n        user = User.objects.create_user('legacyuser', 'test2@example.com', 'testpw')\n        \n        with self.settings(DEFAULT_HASHING_ALGORITHM='sha256', PASSWORD_RESET_TIMEOUT=60 * 60):\n            p0 = PasswordResetTokenGenerator()\n            token = p0.make_token(user)\n            self.assertFalse(p0.check_token(user, token, legacy=True))  \n\n            p1 = PasswordResetTokenGenerator()\n            p1.algorithm = 'sha1' \n            token = p1.make_token(user)\n            self.assertTrue(p1.check_token(user, token, legacy=True))\n\n", "    def test_legacy_token(self):\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        now = datetime.now()\n        p0 = MockedPasswordResetTokenGenerator(now)\n        token_legacy = p0.make_token_with_timestamp(user, now.timestamp(), legacy=True)\n        p1 = PasswordResetTokenGenerator()\n        self.assertIs(p1.check_token(user, token_legacy), True)\n", "    def test_legacy_token_handling(self):\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        \n        with self.settings(PASSWORD_RESET_TIMEOUT=60 * 60):\n            # Generate a token with timestamp and legacy behavior enabled\n            p1 = MockedPasswordResetTokenGenerator(datetime.now() - timedelta(seconds=settings.PASSWORD_RESET_TIMEOUT))\n            tk2 = p1.make_token_with_timestamp(user, p1._num_seconds(p1._now()), legacy=True)\n            self.assertEqual(tk1, tk2)\n", "    def test_token_with_legacy_hashing_algorithm(self):\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        # Create and check a token using the legacy hashing algorithm.\n        generator = MockedPasswordResetTokenGenerator(datetime.now())\n        generator.algorithm = 'sha1'\n        token = generator.make_token(user)\n        self.assertIs(generator.check_token(user, token, legacy=True), True)\n\n        # Check that a token generated with the current hashing algorithm doesn't\n        # validate against the legacy check.\n        generator = PasswordResetTokenGenerator()\n        token = generator.make_token(user)\n        self.assertIs(generator.check_token(user, token, legacy=True), False)\n", "    def test_token_with_legacy_algorithm(self):\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        with self.settings(DEFAULT_HASHING_ALGORITHM='sha1'):\n            p1 = PasswordResetTokenGenerator()\n            self.assertEqual(p1.algorithm, 'sha1')\n            self.assertIs(p1.check_token(user, tk1), True)\n"], "sample_1159": ["def test_issue_18241():\n    x = Symbol('x', positive=True)\n    assert (2*x).is_positive is True\n    y = Symbol('y', negative=True)\n    assert (y - 1).is_nonpositive is  True\n    z = Symbol('z', zero=True)\n    assert (z - 1).is_negative is True\n    a = Symbol('a', real=True)\n    assert (a - 1).is_real is True\n\n", "def test_issue_18002():\n    x = Symbol('x', positive=True)\n    assert (x ** 2).is_positive is True\n    assert (x ** 3).is_positive is True\n    assert (-x ** 2).is_positive is False\n    assert abs(x).is_positive is True\n    assert (-abs(x)).is_positive is False\n", "def test_issue_17982():\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    assert (x**2 + y**2).is_nonnegative is True\n", "def test_issue_18330():\n    x = Symbol('x', real=True, nonnegative=True)\n    y = Symbol('y', real=True, positive=True)\n    assert (x + y).is_positive is True\n    z = Symbol('z', real=True, nonnegative=False)\n    assert (x + z).is_positive is None\n\n\n", "def test_issue_18051():\n    x = Symbol('x', real=True, nonzero=True)\n    assert (1/x).is_real is True\n"], "sample_53": ["    def test_render_no_results(self):\n        with translation.override('en'):\n            # Remove all bands so autocomplete has no results\n            Band.objects.all().delete()\n\n            form = AlbumForm(initial={'band': None})\n            output = form.as_table()\n            self.assertIn('No results found', output)\n\n\n\n", "    def test_render_options_with_translations(self):\n        translation.activate('es-es')\n        beatles = Band.objects.create(name='Los Beatles', style='rock')\n        who = Band.objects.create(name='The Who', style='rock')\n\n        form = AlbumForm(initial={'band': beatles.pk})\n        output = form.as_table()\n        self.assertIn('<option value=\"%s\" selected>Los Beatles</option>' % beatles.pk, output)\n        self.assertNotIn('<option value=\"%s\">The Who</option>' % who.pk, output)\n\n        translation.activate(None)\n\n\n", "    def test_language_aware_autocomplete(self):\n        # Set language to English (en) before the test\n        with translation.override('en'):\n            beatles = Band.objects.create(name='The Beatles', style='rock')\n            band_name_translated = beatles.name.translate({'The': 'Die'})\n            beatles = Band.objects.create(name=band_name_translated, style='rock')\n            form = AlbumForm()\n            output = form.as_table()\n            self.assertIn('<option value=\"%s\">Die Beatles</option>' % beatles.pk, output)\n", "    def test_render_options_with_translation(self):\n        with translation.override('en-us'):\n            beatles = Band.objects.create(name='The Beatles', style='rock')\n            who = Band.objects.create(name='The Who', style='rock')\n\n            form = AlbumForm(initial={'band': beatles.pk})\n            output = form.as_table()\n            self.assertIn('<option value=\"%s\" selected>The Beatles</option>' % beatles.pk, output)\n        with translation.override('es'):\n            form = AlbumForm(initial={'band': beatles.pk})\n            output = form.as_table()\n            self.assertIn('<option value=\"%s\" selected>Los Beatles</option>' % beatles.pk, output)\n", "    def test_render_options_with_language(self):\n        old_language, _ = translation.get_language()\n        translation.activate('es')\n\n        beatles = Band.objects.create(name='Los Beatles', style='rock')\n        who = Band.objects.create(name='The Who', style='rock')\n        with self.assertTranslation('es'):\n            form = AlbumForm(initial={'band': beatles.pk})\n            output = form.as_table()\n            self.assertIn(f'<option value=\"{beatles.pk}\" selected>Los Beatles</option>', output)\n        translation.activate(old_language)\n\n\n\n"], "sample_687": ["def test_caplog_handles_log_format(caplog):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_format = %(asctime)s - %(name)s - %(levelname)s - %(message)s\n        \"\"\"\n    )\n\n    caplog.set_level(logging.INFO)\n    logger.info(\"boo %s\", \"arg\")\n\n    assert caplog.text.split(\"\\n\")[0] == caplog.text.split(\"\\n\")[0]\n", "def test_clearing_live_logging_in_teardown(testdir, capsys):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n\n            logging.info(\"line 1\")\n            logging.info(\"line 2\")\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--log-cli-level=DEBUG\")\n    result.stdout.fnmatch_lines([\"*line 1*\"])\n    result.stdout.fnmatch_lines([\"*line 2*\"])\n\n    # Clear caplog in teardown\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n\n            logging.info(\"line 1\")\n            logging.info(\"line 2\")\n            @pytest.fixture(autouse=True)\n                caplog.clear()\n\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--log-cli-level=DEBUG\")\n    result.stdout.no_fnmatch_lines([\"*line 1*\"])\n    result.stdout.no_fnmatch_lines([\"*line 2*\"])\n", "def test_caplog_supports_null_formatter(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n            logger = logging.getLogger('catchlog')\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_level == logging.WARNING\n            logger.info(\"INFO message won't be shown\")\n            # test that NullFormatter doesn't raise errors\n            caplog.Logger.formatter = logging.NullFormatter()\n            assert caplog.records == []\n    \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_level=WARNING\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n    assert result.ret == 0\n", "def test_caplog_handles_colorized_log_messages(testdir, caplog):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import coloredlogs\n\n        coloredlogs.install(level='DEBUG', fmt='%(asctime)s - %(levelname)s - %(message)s')\n\n            logger = logging.getLogger(__name__)\n            logger.debug(\"DEBUG message\")\n            logger.info(\"INFO message\")\n            logger.warning(\"WARNING message\")\n            logger.error(\"ERROR message\")\n            logger.critical(\"CRITICAL message\")\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*DEBUG message*\",\n            \"*INFO message*\",\n            \"*WARNING message*\",\n            \"*ERROR message*\",\n            \"*CRITICAL message*\",\n        ]\n    )\n    assert caplog.text  # Make sure caplog captures something\n\n", "def test_caplog_handles_nested_calls(caplog):\n    caplog.set_level(logging.INFO)\n    logger.info(\"outer level\")\n\n        logger.info(\"inner level\")\n\n    nested_function()\n    assert [x.message for x in caplog.records] == [\n        \"outer level\",\n        \"inner level\",\n    ]\n\n\n"], "sample_883": ["def test_ard_lambda_initialization():\n    # Test the initialization of lambda_\n    X = np.random.rand(10, 5)\n    y = np.random.rand(10)\n    \n    # Test with different input lambda_1 and lambda_2 values\n    for lambda_1 in [1e-2, 1e-4, 1e-6]:\n        for lambda_2 in [1e-2, 1e-4, 1e-6]:\n            model = ARDRegression(lambda_1=lambda_1, lambda_2=lambda_2)\n            model.fit(X, y)\n            # Check that the shape of lambda_ is as expected\n            assert len(model.lambda_) == X.shape[1]\n            assert model.lambda_[0] > 0\n\n", "def test_bayesian_ridge_ard_compute_score(Estimator):\n    \"\"\"Test compute_score parameter for both Bayesian Ridge and ARDRegression.\"\"\"\n    X = np.random.randn(10, 5)\n    y = np.random.randn(10)\n\n    # Test with compute_score=True\n    model = Estimator(compute_score=True)\n    model.fit(X, y)\n    assert hasattr(model, \"scores_\")\n    assert len(model.scores_) > 0\n\n    # Test with compute_score=False (should not raise errors)\n    model = Estimator(compute_score=False)\n    model.fit(X, y)\n    assert not hasattr(model, \"scores_\")\n", "def test_sparse_data(Estimator):\n    # Test handling of sparse data\n    from sklearn.datasets import fetch_olivetti_faces\n    from scipy.sparse import csr_matrix\n\n    data = fetch_olivetti_faces(return_X_train_y=True)\n    X = data[0]\n    y = data[1]\n\n    X_sparse = csr_matrix(X)\n\n    model = Estimator()\n    model.fit(X_sparse, y)\n    # Check if the model returns a non-empty array for the coefficients\n    assert len(model.coef_) > 0\n", "def test_ard_regularization_strength(global_random_seed):\n    # Test the impact of lambda on regularization in ARDRegression\n    # (Github issue #14055)\n    rng = np.random.RandomState(global_random_seed)\n\n    X = rng.randn(100, 5)\n    y = rng.randn(100)\n\n    # Test different lambda values\n    for lambda_ in [1e-6, 1e-4, 1e-2, 1e0]:\n        model_lambda = ARDRegression(lambda_1=lambda_)\n        model_lambda.fit(X, y)\n\n        # Check if the regularization effect increases with lambda\n        assert np.mean(np.abs(model_lambda.coef_)) <= np.mean(\n            np.abs(model_lambda.coef_)\n        ) if lambda_ == 1e-6 else np.mean(np.abs(model_lambda.coef_)) > np.mean(\n            np.abs(model_lambda.coef_)\n        )\n", "def test_bayesian_ridge_ard_fit_intercept(Estimator):\n    # Test fit_intercept with both Bayesian Ridge and ARD\n    for estimator_class in [BayesianRidge, ARDRegression]:\n        X = np.array([[1, 1], [3, 4], [5, 7], [4, 1], [2, 6], [3, 10], [3, 2]])\n        y = np.array([1, 2, 3, 2, 0, 4, 5]).T\n\n        # Test with fit_intercept=True\n        model_with_intercept = estimator_class(fit_intercept=True)\n        model_with_intercept.fit(X, y)\n        # Check that intercept is not None\n        assert model_with_intercept.intercept_ is not None\n\n        # Test with fit_intercept=False\n        model_no_intercept = estimator_class(fit_intercept=False)\n        model_no_intercept.fit(X, y)\n        # Check that intercept is None\n        assert model_no_intercept.intercept_ is None \n"], "sample_326": ["    def test_urlize_special_chars(self):\n        tests = (\n            ('Search for http://www.google.com/search?q=\u00e4\u00f6\u00fc&!@#$%^&*()',\n             'Search for <a href=\"http://www.google.com/search?q=\u00e4\u00f6\u00fc&!@#$%^&*()\">'\n             'http://www.google.com/search?q=\u00e4\u00f6\u00fc&!@#$%^&*()</a>'\n             ),\n            ('Search for http://www.google.com/search?q=with+lots+of+spaces',\n             'Search for <a href=\"http://www.google.com/search?q=with+lots+of+spaces\">'\n             'http://www.google.com/search?q=with+lots+of+spaces</a>'\n             ),\n        )\n        for value, output in tests:\n            with self.subTest(value=value):\n                self.assertEqual(urlize(value), output)\n", "    def test_urlize_with_trim_url_limit(self):\n        long_url = 'https://www.example.com/this-is-a-very-long-url-that-should-be-trimmed-to-avoid-overwhelming-the-user-with-too-much-text'\n        trimmed_url = 'https://www.example.com/this-is-a-very-long-url-that-should-be-trimmed-to-avoid-overwhelming-the-user-with-too-much-text\u2026'\n        with self.subTest(trim_url_limit=30):\n            result = urlize(long_url, trim_url_limit=30)\n            self.assertEqual(result, f'<a href=\"{trimmed_url}\">https://www.example.com/this-is-a-very-long-url-that-should-be-trimmed-to-avoid-overwhelming-the-user-with-too-much-text\u2026</a>')\n", "    def test_urlize_trim_url_limit(self):\n        long_url = \"https://www.example.com/this-is-a-very-long-url-that-needs-to-be-trimmed-down-to-a-more-manageable-length\"\n        shortened_url = \"https://www.example.com/this-is-a-ver\"\n        for limit in [15, 20, None]:\n            with self.subTest(limit=limit):\n                result = urlize(long_url, trim_url_limit=limit)\n                if limit is None:\n                    self.assertEqual(result, f'<a href=\"{long_url}\">{long_url}</a>')\n                else:\n                    self.assertIn(shortened_url, result)\n\n\n\n", "    def test_urlize_trim_url_limit(self):\n        long_url = \"https://www.this-is-a-very-long-url-that-will-be-truncated.org\"\n        trimmed_url = \"https://www.this-is-a-very-long-url-that-will-be-truncated.org\u2026\"\n        \n        self.assertEqual(urlize(long_url, trim_url_limit=30),\n                         f'<a href=\"{long_url}\">{trimmed_url}</a>')\n\n\n\n", "    def test_urlize_with_trim_url_limit(self):\n        test_cases = (\n            ('https://www.longurl.com/this-is-a-really-long-url-that-needs-to-be-trimmed',\n             'https://www.longurl.com/this-is-a-really-long-url-that-needs-to-',\n             25),\n            ('http://example.com/path/to/verylongfilenamewithlotsofcharacters.txt',\n             'http://example.com/path/to/verylongfil...',\n             30),\n        )\n        for url, expected_output, limit in test_cases:\n            with self.subTest(url=url, expected_output=expected_output, limit=limit):\n                self.assertEqual(urlize(url, trim_url_limit=limit),\n                                 expected_output)\n\n\n\n"], "sample_556": ["compilation error", "def test_savefig_metadata_with_label():\n    fig = Figure(label='my figure')\n    fig.savefig(io.BytesIO(), format='png', metadata={'metadata_key': 'metadata_value'})\n    assert fig.get_label() == 'my figure'\n", "compilation error", "compilation error", "compilation error"], "sample_1004": ["def test_ConditionSet_subs_with_MultipleSymbols():\n    s = FiniteSet(z, y)\n    c = ConditionSet(x, x < 2, s)\n    assert c.subs({x: w, y: t}) == ConditionSet(w, w < 2, s.subs({y: t}))\n    assert c.subs({x: w, y: t}) == ConditionSet(w, w < 2, {w, t})\n    c = ConditionSet(x, x < y, s)\n    assert c.subs({x: w, y: t}) == ConditionSet(w, w < t, s.subs({y: t}))\n    assert c.subs({x: w, y: t}) == ConditionSet(w, w < t, {w, t})\n", "def test_CondSet_empty_base_set():\n    assert ConditionSet(x, x < 1, S.EmptySet()) == S.EmptySet()\n    assert ConditionSet(x, x > 1, S.EmptySet()) == S.EmptySet()\n    assert ConditionSet(x, S.true, S.EmptySet()) == S.EmptySet()\n    assert ConditionSet(x, S.false, S.EmptySet()) == S.EmptySet()\n", "def test_CondSet_union():\n    s = FiniteSet(1, 2, 3, 4)\n    c1 = ConditionSet(x, x < 3, s)\n    c2 = ConditionSet(x, x > 1, s)\n    assert c1 | c2 == ConditionSet(x, True, s)\n    assert (ConditionSet(x, x < 2, s) |\n            ConditionSet(x, x > 2, s)) == ConditionSet(x, True, s)\n    assert (ConditionSet(x, x < 2, s) |\n            ConditionSet(x, x < 2, s)) == ConditionSet(x, x < 2, s)\n    assert (ConditionSet(x, x < 1, s) |\n            ConditionSet(x, x > 1, s)) == ConditionSet(x, True, s)\n    assert (ConditionSet(x, x < 1, s) |\n            ConditionSet(x, x < 1, s)) == ConditionSet(x, x < 1, s)\n    assert (ConditionSet(x, x < 1, s) |\n            ConditionSet(x, x > 3, s)) == ConditionSet(x, True, s)\n\n", "def test_union_CondSet():\n    s = FiniteSet(z, y)\n    c1 = ConditionSet(x, x < 2, s)\n    c2 = ConditionSet(x, x > 1, s)\n    assert Union(c1, c2) == ConditionSet(x, (x < 2) | (x > 1), s)\n    c3 = ConditionSet(x, x < 0, s)\n    assert Union(c1, c3) == ConditionSet(x, (x < 2) | (x < 0), s)\n    assert Union(c3, c3) == c3\n\n\n\n", "compilation error"], "sample_636": ["    def test_duplicate_code_raw_strings_ignore_comments(self) -> None:\n        \"\"\"Tests the behaviour of ignoring comments when detecting duplicated code.\"\"\"\n        path = join(DATA, \"raw_strings_ignore_comments\")\n        expected_output = \"Similar lines in 2 files\"\n        self._test_output(\n            [path, \"--disable=all\", \"--enable=duplicate-code\"],\n            expected_output=expected_output,\n        )\n\n", "    def test_duplicate_code_raw_strings_single_file_with_imports(self) -> None:\n        \"\"\"Tests if duplicate code is detected when imports are present.\"\"\"\n        path = join(DATA, \"raw_strings_single_file_with_imports\")\n        expected_output = \"Similar lines in 1 files\"\n        self._test_output(\n            [path, \"--disable=all\", \"--enable=duplicate-code\"],\n            expected_output=expected_output,\n        )\n", "    def test_duplicate_code_raw_strings_ignore_comments(self) -> None:\n        \"\"\"Tests ignoring comments in duplicate code detection.\"\"\"\n        path = join(DATA, \"raw_strings_ignore_comments\")\n        expected_output = \"Similar lines in 2 files\"\n        self._test_output(\n            [path, \"--disable=all\", \"--enable=duplicate-code\"],\n            expected_output=expected_output,\n        )\n", "    def test_duplicate_code_raw_strings_disable_scope_nested(self) -> None:\n        \"\"\"Tests disabling duplicate-code at an inner scope level with nested scopes.\"\"\"\n        path = join(DATA, \"raw_strings_disable_scope_nested\")\n        expected_output = \"Similar lines in 2 files\"\n        self._test_output(\n            [path, \"--disable=all\", \"--enable=duplicate-code\"],\n            expected_output=expected_output,\n        )\n", "    def test_duplicate_code_raw_strings_disable_scope_multiple(self) -> None:\n        \"\"\"Tests disabling duplicate-code at multiple inner scope levels.\"\"\"\n        path = join(DATA, \"raw_strings_disable_scope_multiple\")\n        expected_output = \"Similar lines in 2 files\"\n        self._test_output(\n            [path, \"--disable=all\", \"--enable=duplicate-code\"],\n            expected_output=expected_output,\n        )\n"], "sample_73": ["    def test_hashed_files_cache_integrity(self):\n        manifest = storage.staticfiles_storage.load_manifest()\n        # Ensure that the manifest is populated when the files are first processed\n        self.assertEqual(len(manifest), 2)\n        # Modify one of the files and re-collect static files\n        with open(self._get_filename_path('cached/styles.css'), 'a') as f:\n            f.write(\"\\n/* Modified */\")\n        self.run_collectstatic()\n\n        # Verify that the manifest has been updated\n        self.assertEqual(len(storage.staticfiles_storage.hashed_files), 2)\n\n        # Check the updated manifest for the modified file\n        updated_manifest = storage.staticfiles_storage.load_manifest()\n        self.assertIn('cached/styles.css', updated_manifest)\n        self.assertNotEqual(updated_manifest[list(updated_manifest.keys())[0]], manifest[list(manifest.keys())[0]])\n        # Test the integrity of the manifest by clearing the cache\n        storage.staticfiles_storage.hashed_files.clear()\n        # Ensure the manifest is re-populated correctly after clearing\n        self.assertEqual(len(storage.staticfiles_storage.hashed_files), 2)\n", "    def test_manifest_update_on_file_change(self):\n        old_content = \"\"\"\n            /* This is old CSS */\n        \"\"\"\n        with open(self._get_filename_path('old.css'), 'w') as f:\n            f.write(old_content)\n        self.run_collectstatic()\n\n        # Ensure the manifest exists and is populated\n        self.assertTrue(os.path.exists(storage.staticfiles_storage.manifest_name))\n\n        new_content = \"\"\"\n            /* This is updated CSS */\n        \"\"\"\n        with open(self._get_filename_path('old.css'), 'w') as f:\n            f.write(new_content)\n        self.run_collectstatic()\n\n        # Check the manifest has been updated\n        manifest_content = storage.staticfiles_storage.load_manifest()\n        self.assertNotIn('old.css.deploy12345.css', manifest_content)\n        self.assertIn('old.css.another_hash.css', manifest_content)\n\n", "    def test_manifest_refresh(self):\n        with open(self._get_filename_path('cached/styles.css'), 'w') as f:\n            f.write('body { background: url(\"cached/image.png\") }')\n\n        with open(self._get_filename_path('cached/image.png'), 'w') as f:\n            pass\n\n        # Ensure the manifest is initially populated\n        self.run_collectstatic()\n        manifest = storage.staticfiles_storage.load_manifest()\n        self.assertIn('cached/styles.css', manifest)\n        self.assertIn('cached/image.png', manifest)\n\n        # Modify cached/image.png and then refresh the manifest\n        with open(self._get_filename_path('cached/image.png'), 'w') as f:\n            f.write(' ')\n\n        storage.staticfiles_storage.refresh_manifest()\n        # The image should now have a new hash in the manifest\n        manifest = storage.staticfiles_storage.load_manifest()\n        self.assertIn('cached/styles.css', manifest)\n        self.assertIn('cached/image.png', manifest)\n\n        # Ensure the old manifest is not used for the updated image file\n        with open(self._get_filename_path('cached/styles.css'), 'r') as f:\n            content = f.read()\n        self.assertNotIn(\n            f'url(\"cached/image.{storage.staticfiles_storage.hashed_files[\"cached/image.png\"]}\"),', content\n        )\n\n\n", "    def test_relative_imports_with_hashed_dependencies(self):\n        temp_files = {\n            'test/styles.css': '@import \"relative.css\";\\n',\n            'test/relative.css': 'body { background-image: url(\"image.png\"); }',\n            'test/image.png': b'dummy png data',\n        }\n        for path, content in temp_files.items():\n            with open(self._get_filename_path(path), 'w') as f:\n                f.write(content)\n\n        # Run collectstatic to hash the files\n        self.run_collectstatic()\n\n        # Assert that the final hashed styles.css file includes the correct\n        # hashed paths for relative.css and image.png\n        with storage.staticfiles_storage.open(self.hashed_file_path('test/styles.css')) as f:\n            content = f.read()\n            self.assertIn(self.hashed_file_path('test/relative.css'), content)\n            self.assertIn(self.hashed_file_path('test/image.png'), content)\n", "    def test_manifest_timestamp_updates(self):\n        cached_file = self._get_filename_path(\"cached/styles.css\")\n        with open(cached_file, \"w\") as f:\n            f.write(\"/* Some CSS code */\")\n\n        # Initial manifest should not have the file\n        self.run_collectstatic()\n        manifest = storage.staticfiles_storage.load_manifest()\n        self.assertNotIn(\"cached/styles.css\", manifest)\n\n        # Update the file content\n        with open(cached_file, \"a\") as f:\n            f.write(\"/* More CSS code */\")\n        os.utime(cached_file, (time.time(), time.time()))\n\n        self.run_collectstatic()\n        manifest = storage.staticfiles_storage.load_manifest()\n        self.assertIn(\"cached/styles.css\", manifest)\n"], "sample_56": ["    def test_check_for_invalid_fieldsets_names(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            fieldsets = [\n                ('', {'fields': ['title']}),\n                (None, {'fields': ['title']}),\n            ]\n\n        errors = MyModelAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"Fieldsets names in 'fieldsets' should be unique.\",\n                obj=MyModelAdmin,\n                id='admin.E011'\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_check_for_invalid_field_names(self):\n        class InvalidFieldNameAdmin(admin.ModelAdmin):\n            fields = ['invalid-field-name']\n\n        errors = InvalidFieldNameAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'fields[0]' contains invalid characters.\",\n                obj=InvalidFieldNameAdmin,\n                id='admin.E001'\n            )\n        ]\n        self.assertEqual(errors, expected)\n\n\n", "    def test_check_inline_sublists_for_duplicates(self):\n        class MyInline(admin.StackedInline):\n            model = Song\n            exclude = ('title',  'title')\n\n        class MyModelAdmin(admin.ModelAdmin):\n            inlines = [MyInline]\n\n        errors = MyModelAdmin(Album, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'exclude' contains duplicate field(s).\",\n                obj=MyInline,\n                id='admin.E014'\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_check_for_invalid_fieldsets_keys(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            fieldsets = [('', {'fields': ['title']}), (None, {'fields': ['title']})]\n\n        errors = MyModelAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"Fieldset keys must be unique. Duplicate key found: '\".',\n                obj=MyModelAdmin,\n                id='admin.E011',\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_check_readonly_fields_for_duplicates(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            readonly_fields = ('title', 'title')\n\n        errors = MyModelAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'readonly_fields' contains duplicate field(s).\",\n                obj=MyModelAdmin,\n                id='admin.E036'\n            )\n        ]\n        self.assertEqual(errors, expected)\n\n"], "sample_415": ["    def test_expressions_with_include(self):\n        msg = (\n            \"UniqueConstraint.include cannot be used with expressions. Use \"\n            \"django.contrib.postgres.indexes.OpClass() instead.\"\n        )\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                Lower(\"field\"),\n                name=\"test_func_include\",\n                include=[\"other_field\"],\n            )\n\n", "    def test_abstract_name(self):\n        constraints = get_constraints(ChildModel._meta.db_table)\n        self.assertIn(\"constraints_childmodel_adult\", constraints)\n", "    def test_expressions_with_include(self):\n        msg = (\n            \"UniqueConstraint.include cannot be used with expressions. \"\n            \"Use django.contrib.postgres.indexes.OpClass() instead.\"\n        )\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                Lower(\"field\"),\n                name=\"test_func_include\",\n                include=[\"other_field\"],\n            )\n", "    def test_expressions_with_deferrable(self):\n        msg = (\n            \"UniqueConstraint with expressions cannot be deferred. Use \"\n            \"django.contrib.postgres.indexes.OpClass() instead.\"\n        )\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                Lower(\"field\"), name=\"test_func_deferrable\", deferrable=models.Deferrable.DEFERRED\n            ) \n", "    def test_opclasses_with_expressions_and_include(self):\n        msg = (\n            \"UniqueConstraint.opclasses cannot be used with expressions \"\n            \"and include fields. Use django.contrib.postgres.indexes.OpClass() \"\n            \"instead.\"\n        )\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                Lower(\"field\"),\n                fields=[\"field_2\"],\n                include=[\"field_3\"],\n                opclasses=[\"jsonb_path_ops\"],\n                name=\"test_func_opclass_include\",\n            )\n"], "sample_619": ["def test_decode_cf_datetime_datetime64_out_of_range(calendar):\n    units = \"days since 1-1-1\"\n    num_dates = np.datetime64(\"9999-12-31 00:00:00\", \"D\")\n    result = decode_cf_datetime(num_dates, units, calendar)\n    expected_result = num2date(num_dates, units, calendar, only_use_cftime_datetimes=True)\n    assert isinstance(result, type(expected_result))\n", "def test_decode_cf_datetime_array_handling(time_array, expected_dtype) -> None:\n    units = \"days since 2000-01-01\"\n    result = decode_cf_datetime(time_array, units)\n    assert result.dtype == expected_dtype\n", "def test_decode_cf_datetime_with_large_numbers():\n    units = \"days since 1900-01-01\"\n    num_dates = np.array([2_000_000_000, 2_000_000_001])\n    result = decode_cf_datetime(num_dates, units)\n    expected = np.asarray([\n        np.datetime64(\"5412-04-19\", \"ns\"),\n        np.datetime64(\"5412-04-20\", \"ns\")\n    ])\n    np.testing.assert_equal(result, expected) \n\n\n", "def test_decode_cf_datetime_with_time_zone_offset():\n    units = \"seconds since 2000-01-01T00:00:00-05:00\"\n    num_dates = np.array([0, 1, 2])\n    result = decode_cf_datetime(num_dates, units)\n    expected = np.array([\n        np.datetime64(\"2000-01-01T00:00:00-05:00\"),\n        np.datetime64(\"2000-01-01T01:00:00-05:00\"),\n        np.datetime64(\"2000-01-01T02:00:00-05:00\"),\n    ])\n    np.testing.assert_equal(result, expected)\n\n\n\n", "def test_decode_cf_datetime_invalid_units():\n    with pytest.raises(ValueError, match=\"Invalid time units\"):\n        decode_cf_datetime([1, 2], \"days since naT\")\n"], "sample_691": ["def test_timeout_non_zero(pytester: Pytester, timeout: float) -> None:\n    \"\"\"Test that the timeout duration is correctly applied.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n    import os, time\n        time.sleep(timeout * 1.1)  # Sleep a bit longer than the timeout\n    \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        faulthandler_timeout = %s\n        \"\"\" % timeout\n    )\n    result = pytester.runpytest_subprocess()\n    result.stderr.fnmatch_lines([\"*most recent call first*\"])\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n    assert result.ret == 0\n\n", "def test_timeout_invalid_value_ignored(\n    pytester: Pytester, timeout: float, expected_ret: int", "def test_custom_stderr(pytester: Pytester) -> None:\n    \"\"\"Test that we can use a custom stderr file for faulthandler.\"\"\"\n    temp_file = io.StringIO()\n    pytester.makepyfile(\n        \"\"\"\n        import faulthandler\n            faulthandler.dump_traceback()\n    \"\"\"\n    )\n    result = pytester.runpytest_subprocess(\n        \"-p\", \"faulthandler\", \"--faulthandler-stderr=%s\" % temp_file.name\n    )\n    result.stderr.fnmatch_lines([\"*traceback information*\"])\n    assert temp_file.getvalue() == result.stderr.str()\n", "def test_timeout_non_zero_duration(pytester: Pytester) -> None:\n    \"\"\"Test that the timeout duration is respected.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n    import os, time\n        time.sleep(0.05)\n    \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        faulthandler_timeout = 0.04\n        \"\"\"\n    )\n    result = pytester.runpytest_subprocess()\n    result.stderr.fnmatch_lines([\"*Traceback*\"])  \n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n    assert result.ret != 0 \n", "def test_timeout_config_value(pytester: Pytester, timeout: float) -> None:\n    \"\"\"Test that timeout is correctly set.\"\"\"\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        faulthandler_timeout = %s\n        \"\"\" % timeout\n    )\n    pytester.makepyfile(\n        \"\"\"\n        import time\n        time.sleep(timeout)\n\n            pass\n    \"\"\"\n    )\n    result = pytester.runpytest_subprocess()\n    if timeout > 0:\n        result.stderr.fnmatch_lines([\"*%s*\" % (\"*Traceback*\")])\n    else:\n        assert \"*Traceback*\" not in result.stderr.str()\n    result.stdout.fnmatch_lines([\"*1 passed*\"])\n    assert result.ret == 0\n"], "sample_740": ["    def test_check_memory_with_cachedir_and_path():\n        memory = check_memory(cache_dir='my_cache')\n        assert_equal(memory.cachedir, 'my_cache')\n\n        memory = check_memory(cache_dir='my_cache', path='data.pkl')\n        assert_equal(memory.cachedir, 'my_cache')\n        assert_equal(memory.path, 'data.pkl')\n", "def test_check_array_sparse_format():\n    # Test that sparse formats are correctly detected and not transformed\n    # unnecessarily.\n\n    sparse_format = ['csr', 'csc', 'coo', 'lil', 'bsr']\n    for fmt in sparse_format:\n        X_sparse = sp.random(5, 5, format=fmt)\n        X_checked = check_array(X_sparse)\n        assert_equal(X_checked.format, fmt)\n\n    # Test that changing the sparse format is not desirable\n    X_csr = sp.csr_matrix([[1, 2], [3, 4]])\n    X_csc = X_csr.tocsc()\n    assert_raises(ValueError, check_array, X_csc, dtype=np.float64,\n                  accept_sparse=True,\n                  copy=True)\n\n    # Test that changing the sparse format is not desirable when force=True\n    X_csr = sp.csr_matrix([[1, 2], [3, 4]])\n    X_csc = X_csr.tocsc()\n    X_checked = check_array(X_csc, copy=True, force='csc')\n    assert_equal(X_checked.format, 'csc')\n", "def test_check_sparse_matrix():\n    X = sp.csr_matrix([[1, 2], [3, 4]])\n    check_array(X)\n    assert_equal(X.format, 'csr')\n\n    # Check that a sparse matrix with a dense representation is correctly\n    # handled.\n    X = sp.csc_matrix([[1, 2], [3, 4]])\n    check_array(X)\n    assert_equal(X.format, 'csc')\n\n    # Check that a list of sparse matrices is correctly handled.\n    X = [sp.csr_matrix([[1, 2], [3, 4]]),\n         sp.csc_matrix([[5, 6], [7, 8]])]\n    check_array(X)\n    assert_equal([m.format for m in X], ['csr', 'csc'])\n\n    ## Check that NoneType is accepted for sparse matrix\n    X = None\n    check_array(X, accept_sparse=True)\n\n\n", "def test_check_array_sparse_dtype_warning():\n    # Test that check_array properly warns when encountering a sparse matrix \n    # with a dtype that's incompatible with the requested dtype, but \n    # doesn't raise an error when warned_on_dtype is False.\n\n    X = sp.csr_matrix([[1, 2], [3, 4]], dtype='int8')\n\n    # Check that a warning is raised when dtype is not None and \n    # warn_on_dtype is True\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(\"always\")\n        check_array(X, dtype=np.float64, warn_on_dtype=True)\n        assert_equal(len(w), 1)\n        assert_true(\n            \"Sparse matrices with dtype\" in str(w[0].message))  \n\n    # Check that no warning is raised when warn_on_dtype is False\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(\"always\")\n        check_array(X, dtype=np.float64, warn_on_dtype=False)\n        assert_equal(len(w), 0)\n\n\n", "    def test_check_array_with_datetime64():\n        import pandas as pd\n        \n        # Tests with 'datetime64[ns]' dtype\n        X = pd.DataFrame({'feature': pd.to_datetime(['2023-01-01',\n                                                       '2023-01-02'])})\n        check_array(X)\n        \n        # Tests with 'datetime64[us]' dtype\n        X = pd.DataFrame({'feature': pd.to_datetime(['2023-01-01',\n                                                       '2023-01-02'],\n                                                      unit='us')})\n        check_array(X)\n        \n        # Tests with mixed dtypes, including datetime64\n        X = pd.DataFrame({'feature1': pd.to_datetime(['2023-01-01',\n                                                       '2023-01-02']),\n                          'feature2': [1, 2]})\n        check_array(X)\n"], "sample_588": ["    def test_auto_combine_with_explicit_fill_value(self):\n        datasets = [\n            Dataset({\"a\": (\"x\", [2, 3]), \"x\": [1, 2]}),\n            Dataset({\"a\": (\"x\", [1, nan]), \"x\": [0, 1]}),\n        ]\n        with pytest.warns(FutureWarning, match=\"`fill_value`\"):\n            auto_combine(datasets, concat_dim=\"t\", fill_value=0)\n\n\n", "def test_auto_combine_with_non_matching_coords(self):\n    with pytest.raises(ValueError, match=\"Coordinates must be present\"):\n        auto_combine(\n            [\n                Dataset({\"x\": [0], \"y\": [0]}),\n                Dataset({\"y\": [1], \"z\": [1]}),\n            ]\n        )\n", "    def test_auto_combine_with_identical_datasets(self):\n        objs = [Dataset({\"x\": [0]}), Dataset({\"x\": [0]})]\n        with pytest.warns(FutureWarning, match=\"identical datasets were passed\"):\n            auto_combine(objs) \n", "    def test_auto_combine_with_invalid_dims(self):\n        objs = [Dataset({\"x\": [0]}), Dataset({\"y\": [1]}), Dataset({\"z\": [2]})]\n        with pytest.warns(FutureWarning, match=\"Could not find any dimension coordinates\"):\n            auto_combine(objs)\n", "    def test_auto_combine_with_incompatible_coords(self):\n        datasets = [\n            Dataset({\"x\": (\"a\", [0, 1])}, coords={\"a\": (\"a\", [1, 2])}),\n            Dataset({\"x\": (\"a\", [1, 2])}, coords={\"a\": (\"a\", [3, 4])}),\n        ]\n        with pytest.warns(FutureWarning, match=\"dimensions are not\"):\n            auto_combine(datasets)\n\n"], "sample_241": ["    def test_group_by_with_alias(self):\n        expr = ExpressionWrapper(Value(3), output_field=IntegerField(), alias='my_alias')\n        self.assertEqual(expr.get_group_by_cols(alias=None), ['my_alias'])\n", "    def test_expression_wrapper_with_output_field_override(self):\n        value = Value('bar', output_field=CharField())\n        wrapped = ExpressionWrapper(value, output_field=TextField())\n        self.assertEqual(wrapped.output_field, TextField())\n", "    def test_group_by_with_alias(self):\n        expr = ExpressionWrapper(Value(3), output_field=IntegerField())\n        self.assertEqual(expr.get_group_by_cols(alias=\"my_alias\"), ['my_alias'])\n", "    def test_resolve_output_field_with_func(self):\n        with self.assertRaises(TypeError):\n            ExpressionWrapper(Func('a', function='NOW'), output_field=IntegerField())\n", "    def test_resolve_output_field_with_aggregate(self):\n        expr = ExpressionWrapper(Avg('cost'), output_field=DecimalField())\n        self.assertIsInstance(expr.output_field, DecimalField)\n        expr = ExpressionWrapper(Count('cost'), output_field=IntegerField())\n        self.assertIsInstance(expr.output_field, IntegerField)\n\n\n\n"], "sample_692": ["    def test_basetemp_with_read_only_dirs(pytester: Pytester) -> None:\n        \"\"\"Integration test for #5524\"\"\"\n        pytester.makepyfile(\n            \"\"\"\n            import os\n            import stat\n\n                dir = tmp_path / 'dir'\n                dir.mkdir()\n                (dir / 'foo.txt').touch()\n                mode = os.stat(str(dir)).st_mode\n                os.chmod(str(dir), mode & ~stat.S_IREAD)", "    def test_relative_basetemp(tmp_path, monkeypatch):\n        my_tmp = tmp_path.joinpath(\"my_tmp\")\n        monkeypatch.chdir(my_tmp)\n        pytester.makepyfile(\n            \"\"\"\n            import os\n                assert tmpdir.resolve().startswith(os.path.abspath(os.path.join(os.getcwd(), '..'))\n            \"\"\"\n        )\n        result = pytester.inline_run()\n        result.assertoutcome(passed=1)\n", "    def test_basetemp_with_read_only_directories(pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import os\n            import stat\n\n                d = tmp_path / 'dir'\n                d.mkdir()\n                (d / 'foo.txt').touch()\n                mode = os.stat(str(d)).st_mode\n                os.chmod(str(d), mode & ~stat.S_IRWXU)", "    def test_basetemp_with_read_only_directories(pytester: Pytester) -> None:\n        \"\"\"Integration test for #5524\"\"\"\n        pytester.makepyfile(\n            \"\"\"\n            import os\n            import stat\n\n                adir = tmp_path / 'mydir'\n                adir.mkdir()\n                (adir / 'foo.txt').touch()\n                mode = os.stat(str(adir)).st_mode\n                os.chmod(str(adir), mode & ~stat.S_IREAD)", "    def test_basetemp_with_read_only_directory(pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import os\n            import stat\n\n                dir = tmp_path / 'mydir'\n                dir.mkdir()\n                fn = dir / 'my_file.txt'\n                fn.write_text('hello')\n                mode = os.stat(str(fn)).st_mode\n                os.chmod(str(fn), mode & ~stat.S_IREAD)\n                os.chmod(str(dir), mode & ~stat.S_IREAD)"], "sample_849": ["compilation error", "def test_train_test_split_invalid_test_size():\n    with pytest.raises(ValueError, match=r\"test_size should be in the \\(0, 1\\) range\"):\n        train_test_split(X, test_size=1.1)\n\n    with pytest.raises(ValueError, match=r\"test_size should be in the \\(0, 1\\) range\"):\n        train_test_split(X, test_size=-0.1)\n", "def test_train_test_split_random_state_consistency():\n    rng = np.random.RandomState(42)\n    X = [[1], [2], [3], [4], [5]]\n    y = [0, 1, 0, 1, 0]\n    for i in range(5):\n        train, test = train_test_split(X, y, test_size=0.5, random_state=i)\n        assert len(train) == 2\n        assert len(test) == 3\n        train_y, test_y = train_test_split(y, y, test_size=0.5, random_state=i)\n        assert len(train_y) == 2\n        assert len(test_y) == 3\n\n\n\n", "compilation error", "compilation error"], "sample_465": ["    def test_permissions_with_custom_permissions_check(self):\n        class CustomPermissionsAdmin(ModelAdmin):\n                return app_label == \"modeladmin\"\n\n                return request.user.id == obj.id\n\n        ma = CustomPermissionsAdmin(Band, AdminSite())\n        request = MockRequest()\n        request.user = self.MockUser()\n        band = Band(name=\"The Doors\", bio=\"\", sign_date=date(1965, 1, 1))\n        band.save()\n        self.assertFalse(ma.has_view_permission(request))\n        request.user.id = band.id\n        self.assertTrue(ma.has_view_permission(request))\n\n        self.assertFalse(ma.has_add_permission(request))\n        self.assertFalse(ma.has_change_permission(request))\n        self.assertFalse(ma.has_delete_permission(request))\n        request.user.id = band.id\n        self.assertTrue(ma.has_change_permission(request))\n        self.assertTrue(ma.has_delete_permission(request))\n", "    def test_get_form_kwargs_includes_changed_object(self):\n        class ConcertAdmin(ModelAdmin):\n                kwargs[\"changed_object\"] = obj\n                return kwargs\n\n        ConcertAdmin(Concert, self.site)\n        form = ConcertAdmin().get_form(MockRequest(), Band())\n        self.assertIn(\"changed_object\", form.fields)\n\n\n\n", "    def test_get_urls(self):\n        ma = ModelAdmin(Band, self.site)\n        urls = ma.get_urls(self.site)\n        self.assertEqual(\n            urls,\n            [\n                url(r\"^$\", self.admin_view, name=\"admin_band_changelist\"),\n                url(\n                    r\"^(?P<pk>\\d+)/$\",\n                    self.admin_view,\n                    name=\"admin_band_change\",\n                ),\n                url(\n                    r\"^(?P<pk>\\d+)/delete$\",\n                    self.admin_delete_view,\n                    name=\"admin_band_delete\",\n                ),\n            ],\n        )\n", "    def test_can_export_queryset(self):\n        \"\"\"\n        ModelAdmin.can_export_queryset() returns True for users who can export\n        querysets of objects and False for users who can't.\n        \"\"\"\n        class MockExportUser(MockUser):\n                return perm == \"modeladmin.export_band\"\n\n        ma = ModelAdmin(Band, AdminSite())\n        request = MockRequest()\n        request.user = self.MockViewUser()\n        self.assertFalse(ma.can_export_queryset(request))\n        request.user = MockExportUser()\n        self.assertTrue(ma.can_export_queryset(request))\n        request.user = self.MockAddUser()\n        self.assertFalse(ma.can_export_queryset(request))\n        request.user = self.MockChangeUser()\n        self.assertFalse(ma.can_export_queryset(request))\n        request.user = self.MockDeleteUser()\n        self.assertFalse(ma.can_export_queryset(request))\n\n", "    def test_get_related_object_for_id(self):\n        \"\"\"\n        get_related_object_for_id should return the correct related object\n        from the given id.\n        \"\"\"\n        mock_request = MockRequest()\n        mock_request.user = User.objects.create_superuser(\n            username=\"bob\", email=\"bob@test.com\", password=\"test\"\n        )\n        self.site.register(Band, ModelAdmin)\n        ma = self.site._registry[Band]\n        other_band = Band.objects.create(name=\"The Beatles\", bio=\"\", sign_date=date(1962, 1, 1))\n\n        band = Band.objects.create(name=\"The Doors\", bio=\"\", sign_date=date(1965, 1, 1))\n        related_object = ma.get_related_object_for_id(\n            mock_request, other_band.id, None\n        )\n        self.assertEqual(related_object, other_band)\n        \n        related_object = ma.get_related_object_for_id(\n            mock_request, band.id, band\n        )\n        self.assertEqual(related_object, band)\n"], "sample_369": ["    def test_field_name_in_operation_attribute(self):\n        changes = self.get_changes(\n            [self.author_empty], [self.author_with_biography]\n        )\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['AddField'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='biography', model_name='Author')\n\n", "    def test_operation_with_custom_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    name='CustomModel', model_name='testapp.CustomModel', fields=[]\n                ),\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'custom_model')\n", "    def test_rename_field_with_same_name_in_base(self):\n        before = [\n            ModelState('app', 'readable', [\n                ('id', models.AutoField(primary_key=True)),\n                ('title', models.CharField(max_length=200)),\n            ]),\n        ]\n        after = [\n            ModelState('app', 'readable', [\n                ('id', models.AutoField(primary_key=True)),\n                ('new_title', models.CharField(max_length=200)),\n            ]),\n            ModelState('app', 'book', [\n                ('title', models.CharField(max_length=200)),\n            ], bases=('app.readable',)),\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['RenameField', 'CreateModel'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='title', model_name='readable', new_field='new_title')\n        self.assertOperationAttributes(changes, 'app', 0, 1, name='book')\n", "    def test_handle_field_type_change(self):\n        original_model = ModelState('testapp', 'Person', [\n            ('id', models.AutoField(primary_key=True)),\n            ('name', models.CharField(max_length=200)),\n        ])\n        changed_model = ModelState('testapp', 'Person', [\n            ('id', models.AutoField(primary_key=True)),\n            ('name', models.TextField()),\n        ])\n        changes = self.get_changes([original_model], [changed_model])\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='name', model_name='Person', old_type='CharField', new_type='TextField')\n", "    def test_operation_with_special_character_in_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Person!!!', fields=[]),\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'person_with_special_characters')\n"], "sample_1163": ["compilation error", "def test_issue_14775():\n    from sympy import Abs, symbols, I\n    x, y = symbols('x y')\n    assert Abs(I*x + y).rewrite(sqrt).subs(x, 2).subs(y, 3) == sqrt(5)\n", "def test_issue_19850():\n    from sympy import Symbol, piecewise, re, im\n    x = Symbol('x', real=True)\n    y = Symbol('y', complex=True)\n    assert re(piecewise((x, x > 0), (y, True))) == piecewise((x, x > 0), (re(y), True))\n    assert im(piecewise((x, x > 0), (y, True))) == piecewise((x, x > 0), (im(y), True))\n", "def test_issue_14238():\n    # doesn't cause recursion error\n    r = Symbol('r', real=True)\n    assert Abs(r + Piecewise((0, r > 0), (1 - r, True)))\n", "compilation error"], "sample_375": ["    def test_m2m_cycle(self):\n        A = self.create_model(\"A\", foreign_keys=[models.ManyToManyField('B')])\n        B = self.create_model(\"B\", foreign_keys=[models.ManyToManyField('A')])\n        self.assertRelated(A, [B, A.a_1.rel.through])\n        self.assertRelated(B, [A, B.b_1.rel.through])\n\n", "    def test_proxy_and_abstract_mix(self):\n        A = self.create_model(\"A\", abstract=True)\n        B = self.create_model(\"B\", bases=(A,), proxy=True)\n        C = self.create_model(\"C\", bases=(B,))\n        self.assertRelated(A, [B, C])\n        self.assertRelated(B, [C])\n        self.assertRelated(C, [])\n", "    def test_nested_circular_relations(self):\n        A = self.create_model(\"A\", foreign_keys=[models.ForeignKey('B', models.CASCADE)])\n        B = self.create_model(\"B\", foreign_keys=[models.ForeignKey('C', models.CASCADE)])\n        C = self.create_model(\"C\", foreign_keys=[models.ForeignKey('A', models.CASCADE)])\n        self.assertRelated(A, [B, C])\n        self.assertRelated(B, [A, C])\n        self.assertRelated(C, [A, B])\n\n\n", "    def test_proxy_m2m_relationship(self):\n        A = self.create_model(\"A\", foreign_keys=[models.ManyToManyField('B', through='T')])\n        B = self.create_model(\"B\")\n        T = self.create_model(\"T\", foreign_keys=[\n            models.ForeignKey('A', models.CASCADE),\n            models.ForeignKey('B', models.CASCADE),\n        ], proxy=True)\n        self.assertRelated(A, [B, T])\n", "    def test_proxy_with_constraints(self):\n        A = self.create_model(\"A\", foreign_keys=[models.ForeignKey('B', models.CASCADE)])\n        B = self.create_model(\"B\", proxy=True)\n        self.assertRelated(A, [B])\n        self.assertRelated(B, [])\n"], "sample_1045": ["def test_mod_inv_error_conditions():\n    assert mod_inverse(0, 5) == \n    assert mod_inverse(1, 1) == 1\n    assert mod_inverse(2, 1) ==\n    raises(ValueError, lambda: mod_inverse(2, 0)) \n", "def test_issue_11721():\n    assert is_real(pi) is True\n    assert is_real(E) is True\n    assert is_real(sqrt(4)) is True\n    assert is_real(Rational(3, 4)) is True\n    assert is_real(Integer(10)) is True\n    assert is_real(-oo) is True\n    assert is_real(Float(1.23)) is True\n    assert not is_real(I)\n    assert not is_real(I*pi)\n    assert not is_real(1+I)\n", "def test_NaN_arithmetic():\n    assert (nan + oo) is S.NaN\n    assert (nan - oo) is S.NaN\n    assert (nan * oo) is S.NaN\n    assert (nan / oo) is S.NaN\n    assert (nan ** oo) is S.NaN\n    assert (oo ** nan) is S.NaN\n    assert (nan // oo) is S.NaN\n    assert (nan % oo) is S.NaN\n\n", "def test_zero_power():\n    assert(0**0 == 1)\n    assert(0**S.NaN == S.NaN)\n", "def test_issue_10776():\n    x = Symbol('x')\n    assert (S(2) + x).as_real() == S(2) + x\n    assert (S(2) - x).as_real() == S(2) - x\n    assert (S(2) * x).as_real() == S(2) * x\n    assert (S(2) / x).as_real() == S(2) / x\n\n    assert (S(2) + x).as_imag() == S(0) + x\n    assert (S(2) - x).as_imag() == S(0) - x\n    assert (S(2) * x).as_imag() == S(0) * x\n    assert (S(2) / x).as_imag() == S(0) / x\n"], "sample_1082": ["def test_csch_series():\n    x = Symbol('x')\n    assert csch(x).series(x, 0, 8) == 1/x - x/6 + x**3/30 - x**5/120 + x**7/840 + O(x**8)\n\n\n\n", "compilation error", "compilation error", "compilation error", "def test_complex_numbers():\n    x = Symbol('x', complex=True)\n    assert sinh(x).expand(complex=True) == (exp(x) - exp(-x))/2\n    assert cosh(x).expand(complex=True) == (exp(x) + exp(-x))/2\n    assert tanh(x).expand(complex=True) == (exp(x) - exp(-x))/(exp(x) + exp(-x))\n    assert csch(x).expand(complex=True) == 1 / sinh(x)\n    assert sech(x).expand(complex=True) == 1 / cosh(x)\n    assert coth(x).expand(complex=True) == cosh(x) / sinh(x)\n    assert acosh(x).expand(complex=True) == log(x + sqrt(x**2 - 1))\n    assert acoth(x).expand(complex=True) == (1/2)*log((x + 1) / (x - 1))\n    assert asinh(x).expand(complex=True) == log(x + sqrt(x**2 + 1))\n    assert atanh(x).expand(complex=True) == (1/2)*log((1 + x)/(1 - x))\n"], "sample_1092": ["def test_issue_19926():\n    a, b, c = symbols('a b c')\n    expr = (a + b*c*sin(c))/(b*c)\n    assert cse(expr) == ([(x0, b*c)], [a/(b*c) + x0*sin(c)])\n", "def test_issue_18297():\n    eq = (x + y)*exp(x + y)\n    ans = cse(eq)\n    assert ans == ([(x0, x + y)], [x0*exp(x0)])\n", "def test_issue_18635():\n    expr = 2*x**2*sin(y) + 2*x*cos(y) + cos(y)\n    result = cse(expr)\n    assert result[0] == []\n    assert result[1] == [expr]\n\n\n\n", "def test_issue_19010():\n    from sympy import symbols, sin, cos, log, pi, exp, C\n    x, y, z = symbols('x y z')\n    expr = sin(exp(x)*y + z) + cos(exp(x)*y + z)\n    assert cse(expr) == ([], [sin(exp(x)*y + z) + cos(exp(x)*y + z)])\n    # test for  issue 19010: cse should handle nested functions\n    expr = log(1 + exp(x)*y + z)\n    assert cse(expr) == ([], [log(1 + exp(x)*y + z)])\n\n\n", "def test_issue_19058():\n    from sympy.stats import Normal, density\n    x = symbols('x')\n    prob = density(Normal('mu', 'sigma'), x)\n    assert cse(prob) == ([], [prob]) \n"], "sample_13": ["    def test_latitude_invalid_values(value):\n        with pytest.raises(ValueError):\n            Latitude(value, u.rad)\n", "def test_angle_from_string_with_nan():\n    \"\"\"\n    Regression test for issue #11473\n    \"\"\"\n    # this input causes the parser to raise a ValueError\n    test_str = 'nan, 5d, 0d'\n    with pytest.raises(ValueError) as excinfo:\n        Angle(test_str)\n    assert \"could not convert\" in str(excinfo.value)\n\n\n\n", "def test_angle_interpolation():\n    a1 = Angle([0, 1, 2, 3] * u.deg)\n    a2 = Angle([0, 2, 4, 6] * u.deg)\n\n    # Test linear interpolation with a scalar\n    interp_angle = a1.interpolate(0.5)\n    assert_allclose(interp_angle.value, 1.0)\n\n    # Test linear interpolation with an array\n    interp_angles = a1.interpolate([0.25, 0.75])\n    assert_allclose(interp_angles.value, [0.5, 1.5])\n\n    # Test extrapolation\n    interp_angle = a1.interpolate(1.5)\n    assert_allclose(interp_angle.value, 2.5)\n\n    # Test interpolation with radians\n    a1 = Angle([0, 1, 2, 3] * u.rad)\n    interp_angle = a1.interpolate(0.5 * u.rad)\n    assert_allclose(interp_angle.value, 0.5)\n", "def test_angle_errors():\n    with pytest.raises(ValueError) as excinfo:\n        Angle(\"1h10.5m59.1s\", unit=\"deg\")\n    assert \"Invalid time string\" in str(excinfo.value)\n\n    with pytest.raises(ValueError) as excinfo:\n        Angle('abc deg')\n    assert \"Invalid angle string\" in str(excinfo.value)\n\n    with pytest.raises(ValueError) as excinfo:\n        Angle(10, u.kpc)\n    assert \"Angle\" in str(excinfo.value)\n\n    with pytest.raises(ValueError) as excinfo:\n        Angle((1, 2, 3), unit=u.hour)\n    assert \"Degrees, minutes, seconds or hours required\" in str(excinfo.value)\n", "def test_angle_to_string_format_precision():\n    a1 = Angle(10.23456789, u.deg)\n    assert a1.to_string(precision=2) == '10d13m46.00s'\n    assert a1.to_string(precision=3) == '10d13m46.207s'\n    assert a1.to_string(precision=5) == '10d13m46.23456s'\n    assert a1.to_string(precision=10) == '10d13m46.23456789s'\n\n    # Ensure that precision is honored even for arrays\n    a2 = Angle([10.23456789, 20.3456789], u.deg)\n    assert a2.to_string(precision=2) == '[10d13m46.00s 20d20m46.00s]' \n\n    # Test edge cases\n    a3 = Angle(0.0, u.deg)\n    assert a3.to_string(precision=0) == '0d00m00s'\n    assert a3.to_string(precision=1) == '0d00m00s'\n    assert a3.to_string(precision=10) == '0d00m00.000000000s'\n\n    a4 = Angle(1.0, u.deg)\n    assert a4.to_string(precision=1) == '1d06m00s'\n\n\n\n"], "sample_400": ["    def test_operation_with_custom_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    \"Person\", fields=[], custom_name=\"my_custom_name\"\n                ),\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"my_custom_name\")\n", "    def test_operation_type_changes_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.RenameField(\n                    model_name=\"Person\", old_name=\"name\", new_name=\"full_name\"\n                ),\n            ]\n\n        migration = Migration(\"0001_initial\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"rename_person_name_to_full_name\")\n", "    def test_operation_with_custom_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    \"Person\", fields=[], name=\"new_person_table\"\n                ),\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"new_person_table\")\n", "    def test_suggest_name_with_empty_operation_list(self):\n        class Migration(migrations.Migration):\n            operations = []\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"auto_0001\")\n\n", "    def test_operation_with_string_args(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    \"Person\",\n                    fields=[\n                        migrations.CharField(\n                            max_length=50,\n                            default=\"Some default\",\n                        ),\n                    ],\n                ),\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        suggest_name = migration.suggest_name()\n        self.assertEqual(suggest_name, \"person\")\n\n\n\n"], "sample_555": ["def test_linecollection_custom_colors():\n    fig, ax = plt.subplots()\n    verts = [(0, 0), (1, 0), (1, 1), (0, 1), (0, 0)]\n    codes = [Path.MOVETO, Path.LINETO, Path.LINETO, Path.LINETO, Path.CLOSEPOLY]\n    path = Path(verts, codes)\n\n    line_collection = LineCollection(\n        [path],\n        colors=['r', 'g', 'b', 'y', 'c'],\n        lw=2,\n        alpha=0.5\n    )\n    ax.add_collection(line_collection)\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n", "def test_polygon_closed():\n    # Test that Polygon works correctly with both closed and open polygons\n    poly_closed = Polygon([(0, 0), (1, 0), (1, 1), (0, 1), (0, 0)])\n    poly_open = Polygon([(0, 0), (1, 0), (1, 1), (0, 1)])\n    assert poly_closed.closed\n    assert not poly_open.closed\n\n    with pytest.raises(ValueError):\n        Polygon([np.array([0, 0, 0]), np.array([1, 1, 1])])\n\n\n\n", "def test_polygon_with_hole():\n    fig, ax = plt.subplots()\n    poly = Polygon([(0, 0), (1, 0), (1, 1), (0, 1), (0, 0)],\n                   facecolor=\"none\", edgecolor='k')\n    hole = Polygon([(0.2, 0.2), (0.8, 0.2), (0.8, 0.8), (0.2, 0.8), \n                      (0.2, 0.2)], facecolor='red')\n    poly.patches.append(hole)\n    ax.add_patch(poly)\n    ax.set_xlim(-0.1, 1.1)\n    ax.set_ylim(-0.1, 1.1)\n", "def test_arc_clip_box():\n    fig, ax = plt.subplots()\n    ax.set_xlim(0, 2)\n    ax.set_ylim(0, 2)\n\n    arc = Arc((1, 1), 1, 1, angle=45, theta1=0, theta2=180)\n    ax.add_patch(arc)\n    bbox = Bbox([[0.2, 0.2], [1.8, 1.8]])\n    ax.set_clip_box(bbox)\n    ax.autoscale()\n    ani = FuncAnimation(fig, lambda x: arc.set_theta1(x * 15), frames=10, blit=False)\n    ani.save('test_arc_clip_box.gif', writer='imagemagick')\n\n\n", "def test_polygon_from_list_of_tuples():\n    points = [(1, 2), (3, 4), (5, 6)]\n    polygon = Polygon(points)\n    assert np.allclose(polygon.get_verts(), np.array(points))\n"], "sample_1036": ["compilation error", "def test_matmul_commutativity():\n    assert MatMul(A, B) == MatMul(B, A)\n    assert MatMul(A, 2*B) == MatMul(2*B, A)\n    assert MatMul(2*A, B) == MatMul(B, 2*A)\n    assert MatMul(A, B*C) == MatMul(A*B, C)\n\n\n\n", "def test_matmul_nested_symbols():\n    a, b = symbols('a b', integer=True)\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    expr = MatMul(a*A, b*B)\n    assert expr.args == (a*A, b*B)\n\n\n", "def test_matmul_non_commutative_symbols():\n    a, b, c = symbols('a b c', commutative=False)\n    assert MatMul(a, b, c).args_cnc() == [[], [a, b, c]]\n    assert MatMul(a, b, c).doit() == a*b*c\n\n\n\n", "def test_matmul_commutative_symbols():\n    x, y = symbols('x y', commutative=True)\n    a = MatrixSymbol('A', 2, 2)\n    assert (MatMul(x, a, y)).doit() == x*a*y \n    assert (MatMul(x, y, a)).doit() == x*y*a\n"], "sample_1034": ["def test_apply_grover():\n    numqubits = 2\n    v = OracleGate(numqubits, return_one_on_two)\n    expected = IntQubit(2, nqubits=numqubits)\n    assert qapply(apply_grover(return_one_on_two, numqubits)) == expected\n", "def test_apply_grover():\n    numqubits = 2\n    oracle = return_one_on_two\n    expected = IntQubit(2, nqubits=numqubits)\n    assert qapply(apply_grover(oracle, numqubits)) == expected\n", "def test_apply_grover():\n    v = apply_grover(return_one_on_two, 2)\n    assert qapply(v) == IntQubit(2, nqubits=2)\n", "def test_apply_grover():\n    numqubits = 2\n    result = apply_grover(return_one_on_two, numqubits)\n    assert qapply(result) == IntQubit(2, nqubits=numqubits) \n", "def test_apply_grover():\n    numqubits = 2\n    v = OracleGate(numqubits, return_one_on_two)\n    expected = IntQubit(2, numqubits=numqubits)\n    assert qapply(apply_grover(return_one_on_two, numqubits)) == expected\n"], "sample_133": ["    def test_i18n_language_fallback_plural_forms(self):\n        \"\"\"\n        The JavaScript i18n view should fallback to a language with\n        less plural forms while preserving the real language's number of\n        plural forms and correct translations.\n\n        This test specifically checks the behaviour of plural forms when\n        the fallback language has fewer plural forms than the selected\n        language.\n\n        \"\"\"\n        with self.settings(LANGUAGE_CODE='ru'), override('no'):\n            response = self.client.get('/jsi18n/')\n            self.assertEqual(\n                response.context['catalog']['{count} plural3'],\n                ['{count} plural3', '{count} plural3s']\n            )\n            self.assertEqual(\n                response.context['catalog']['{count} plural2'],\n                ['{count} plural2', '{count} plural2s']\n            )\n\n", "    def test_i18n_domain_catalog(self):\n        \"\"\"\n        Tests that the JavaScriptCatalog can load a translation catalog from a specific domain\n        \"\"\"\n        with self.settings(LANGUAGE_CODE='en-us'), override('fr'):\n            response = self.client.get('/jsi18n_domain/')\n            self.assertContains(response, 'This is a message from the \"domain_test\" domain')\n\n", "    def test_i18n_with_custom_domain(self):\n        with self.settings(LANGUAGE_CODE='en-us'), override('fr'):\n            response = self.selenium.get(self.live_server_url + '/jsi18n_custom_domain/')\n            self.assertContains(response, 'Bonjour, ce message vient de l\\'application custom_domain.')\n", "    def test_i18n_with_missing_locale_language(self):\n        with self.settings(LANGUAGE_CODE='en'), override('es-ar'):\n            response = self.client.get('/jsi18n/')\n            self.assertContains(response, 'Untranslated string')\n", "    def test_i18n_with_custom_domain(self):\n        with self.settings(LANGUAGE_CODE='en-us'), override('fr'):\n            response = self.client.get('/jsi18n_custom_domain/')\n            self.assertContains(response, 'Custom domain translation')\n"], "sample_284": ["    def test_manifest_file_cache(self):\n        manifest_path = storage.staticfiles_storage.path(storage.staticfiles_storage.manifest_name)\n        self.assertTrue(os.path.exists(manifest_path))\n        # modify content in one of the files\n        with open(self._get_filename_path('styles.css'), 'a') as f:\n            f.write('\\n/* Modified */')\n        self.run_collectstatic()\n        # re-run collectstatic which should use the existing manifest\n        self.run_collectstatic()\n        new_manifest_content = storage.staticfiles_storage.load_manifest()\n        # Manifest content should include the modification\n        self.assertIn('\"modified\": true', new_manifest_content)\n", "    def test_manifest_file_not_found(self):\n        temp_dir = tempfile.mkdtemp()\n        os.makedirs(os.path.join(temp_dir, 'test'))\n        with open(os.path.join(temp_dir, 'test', 'manifest.json'), 'w') as f:\n            f.write(\"{\\\"version\\\": \\\"1.0\\\"}\")\n        self.patched_settings = self.settings(STATICFILES_DIRS=[temp_dir])\n        self.patched_settings.enable()\n        self.addCleanup(shutil.rmtree, temp_dir)\n        storage = storage.staticfiles_storage\n        storage.manifest_name = \"missing_manifest.json\"\n        with self.assertRaises(FileNotFoundError):\n            storage.read_manifest()\n        self.patched_settings.disable()\n", "    def test_manifest_update_on_modification(self):\n        test_file = self._get_filename_path('test.css')\n        with open(test_file, 'w') as f:\n            f.write('/* some styles */\\n')\n        self.run_collectstatic()\n\n        manifest_content = storage.staticfiles_storage.load_manifest()\n        initial_manifest_version = manifest_content['version']\n\n        with open(test_file, 'a') as f:\n            f.write('/* more styles */\\n')\n        self.run_collectstatic()\n\n        manifest_content = storage.staticfiles_storage.load_manifest()\n        updated_manifest_version = manifest_content['version']\n        self.assertNotEqual(initial_manifest_version, updated_manifest_version)\n\n\n", "    def tearDown(self):\n        self.patched_settings.disable()\n\n        if os.path.exists(self._clear_filename):\n            os.unlink(self._clear_filename)\n\n        storage.staticfiles_storage.manifest_strict = self._manifest_strict\n        super().tearDown()\n", "    def test_manifest_caching(self):\n        with open(self._get_filename_path(\"test.css\"), \"w\") as f:\n            f.write(\"/* this is test.css */\\n\")\n\n        # First collectstatic:\n        self.run_collectstatic()\n        # Second collectstatic:\n        self.run_collectstatic()\n\n        # Check that the manifest is served\n        manifest_content = storage.staticfiles_storage.load_manifest()\n        self.assertIn('test.css', manifest_content)\n        self.assertIn('deploy12345', manifest_content['test.css'])\n\n        # Modify test.css\n        with open(self._get_filename_path(\"test.css\"), \"w\") as f:\n            f.write(\"/* this is updated test.css */\\n\")\n\n        # Collectstatic again:\n        self.run_collectstatic()\n\n        # Check that the manifest was updated\n        manifest_content = storage.staticfiles_storage.load_manifest()\n        self.assertIn('test.css', manifest_content)\n        self.assertIn('deploy12346', manifest_content['test.css'])\n\n"], "sample_707": ["def test_node_get_closest_marker_with_default() -> None:\n    class MyNode(nodes.Node):\n            super().__init__(name, parent, **kw)\n\n    node = MyNode(\"root\")\n    node.add_marker(\"passme\")\n    child = MyNode(\"child\", parent=node)\n    child.add_marker(\"skipme\")\n\n    assert node.get_closest_marker(\"passme\") is not None\n    assert node.get_closest_marker(\"skipme\", default=\"fallback\") == \"fallback\"\n    assert node.get_closest_marker(\"missing\", default=\"fallback\") == \"fallback\"\n\n", "def test_fscollector_getitempath(pytester: Pytester) -> None:\n    session = pytester.getsession()\n    fspath = legacy_path(pytester.tmpdir / \"my_file.py\")\n    collector = nodes.FSCollector(fspath, parent=None, session=session)\n    assert collector.path == pytester.tmpdir / \"my_file.py\"\n", "def test_getfslocation_from_item_with_obj() -> None:\n    import sys\n    class TestClass:\n            pass\n\n    t = TestClass()\n    node = nodes.Node(\n        name=\"test_method\", obj=t, parent=None, config=None, session=None\n    )\n    l, _ = nodes.getfslocation_from_item(node)\n    assert l == \"<module>\"\n", "def test_getfslineno_no_location(pytester: Pytester) -> None:\n    \"\"\"\n    Tests getfslineno() when it's called for an item with no known location.\n    \"\"\"\n    items = pytester.getitems(\n        \"\"\"def test_no_location():\n            pass\n    \"\"\"\n    )\n    assert items[0].getfslineno() == (-1,)\n", "def test___prunetraceback_inherited_from_base(pytester: Pytester) -> None:\n    class InheritingNode(nodes.Node):\n            super().__init__(*args, **kwargs)\n            self.traceback = [\n                nodes.Traceback(\n                    [(\"file\", \"line\", \"function\"), (\"file2\", \"line2\", \"function2\")]\n                )\n            ]\n\n    items = pytester.getitems(\n        \"\"\"\n        class InheritingNode(nodes.Node):\n                super().__init__(*args, **kwargs)\n                self.traceback = [\n                    nodes.Traceback(\n                        [(\"file\", \"line\", \"function\"), (\"file2\", \"line2\", \"function2\")]\n                    )\n                ]\n\n            pass\n    \"\"\"\n    )\n    \n    item = items[0]\n    item._prunetraceback(Exception())\n    assert isinstance(item.traceback, list) \n    assert len(item.traceback) == 1\n    \n    assert item.traceback[0].frames == [\n        (\"file\", \"line\", \"function\"),\n    ]  \n\n"], "sample_1077": ["def test_issue_16871c():\n    assert ImageSet(Lambda(x, x + I), S.Integers).is_subset(S.Complexes)\n", "def test_issue_17146():\n    assert Interval.open(2, 3).intersection(S.Integers) == FiniteSet()\n    assert Interval.closed(2, 3).intersection(S.Integers) == {2, 3}\n\n\n\n", "def test_issue_17801():\n    c1 = ComplexRegion(Interval(0, 1) * Interval(0, 2*pi), polar=True)\n    c2 = ComplexRegion(Interval(0, 1) * Interval(pi/2, 3*pi/2), polar=True)\n    assert c1.union(c2) == ComplexRegion(Interval(0, 1) * Interval(0, 2*pi), polar=True)\n\n\n\n", "def test_issue_16871c():\n    assert ImageSet(Lambda(x, x**2), S.Integers).intersection(S.Reals) == \\\n        ImageSet(Lambda(x, x**2), S.Integers)\n\n", "def test_issue_12643():\n    assert imageset(Lambda(x, x**2 + 1), S.ComplexRegion(Interval(-1, 1)*Interval(-1, 1))) == \\\n        ComplexRegion(Interval(0, 2)*Interval(-1, 1))\n"], "sample_468": ["    def test_context_processors_return_none(self):\n        request = self.request_factory.get(\"/\")\n        with override_settings(\n            TEMPLATES=[\n                {\n                    \"BACKEND\": \"django.template.backends.django.DjangoTemplates\",\n                    \"APP_DIRS\": True,\n                    \"OPTIONS\": {\"context_processors\": [context_process_returning_none]},\n                }\n            ]\n        ):\n            ctx = RequestContext(request, {})\n            self.assertEqual(len(ctx.dicts), 3)\n        \n\n\n", "    def test_context_with_exception_in_processor(self):\n            raise TypeError(\"Processor error!\")\n\n        request = self.request_factory.get(\"/\")\n        with self.assertRaises(TypeError):\n            RequestContext(request, {}, processors=[bad_processor])\n", "    def test_context_processors_return_none(self):\n        engine = Engine(\n            loaders=[\n                (\n                    \"django.template.loaders.locmem.Loader\",\n                    {\n                        \"template\": \"<h1>{{ foo }}</h1>\"\n                    },\n                ),\n            ]\n        )\n\n        request = self.request_factory.get(\"/\")\n\n        with override_settings(TEMPLATES=[\n            {\"BACKEND\": \"django.template.backends.django.DjangoTemplates\",\n            \"APP_DIRS\": True,\n            \"OPTIONS\": {\n                \"context_processors\": [context_process_returning_none]\n            }}\n        ]):\n            ctx = RequestContext(request, {})\n            rendered_output = engine.from_string(\"template\").render(ctx)\n\n            self.assertEqual(rendered_output, \"<h1></h1>\")\n", "    def test_context_pop_exception(self):\n        request = self.request_factory.get(\"/\")\n        ctx = RequestContext(request, {})\n        with self.assertRaises(ContextPopException):\n            ctx.pop()\n", "    def test_empty_context_processors(self):\n        engine = Engine(\n            loaders=[\n                (\n                    \"django.template.loaders.locmem.Loader\",\n                    {\n                        \"child\": '{{ var|default:\"none\" }}',\n                    },\n                ),\n            ]\n        )\n        request = self.request_factory.get(\"/\")\n        ctx = RequestContext(request, {}, processors=[context_process_returning_none])\n        self.assertEqual(\n            engine.from_string('{% include \"child\" %}').render(ctx),\n            \"none\",\n        )\n\n\n\n"], "sample_542": ["def test_text_bbox_clip():\n    fig, ax = plt.subplots()\n    text = ax.text(0.95, 0.95, 'This text is clipped',\n                   horizontalalignment='right',\n                   verticalalignment='top', clip_on=True)\n    fig.canvas.draw()\n    bbox = text.get_window_extent()\n    assert bbox.width < 0.05\n    assert bbox.height < 0.05\n\n\n\n", "def test_text_bbox_with_angle():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, \"test\", rotation=45,\n                   transform=ax.transAxes)\n    bbox = text.get_window_extent(renderer=fig.canvas.get_renderer())\n    assert bbox.width != 0.0\n    assert bbox.height != 0.0\n\n\n\n", "def test_bbox_clipping_with_whitespace():\n    fig, ax = plt.subplots()\n    ax.text(0.1, 0.1, \"test\\n\\n\", clip_on=True)\n    fig.canvas.draw()\n\n    bbox = ax.text(0.1, 0.1, \"test\\n\\n\", clip_on=True).get_window_extent()\n    assert bbox.width == 0\n    assert bbox.height == 0\n", "def test_text_get_font_properties():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, 'Test')\n    expected_fontprops = text.get_font_properties()\n\n    # Check that the default font properties are returned.\n    assert expected_fontprops.fname == plt.rcParams['font.family']\n    assert expected_fontprops.size == plt.rcParams['font.size']\n    assert expected_fontprops.weight == plt.rcParams['font.weight']\n\n    # Modify the text's font properties\n    text.set_fontsize(20)\n    text.set_fontweight('bold')\n    text.set_fontname('serif')\n\n    modified_fontprops = text.get_font_properties()\n\n    # Check that the modified font properties are returned.\n    assert modified_fontprops.fname == 'serif'\n    assert modified_fontprops.size == 20\n    assert modified_fontprops.weight == 'bold'\n\n\n\n\n", "def test_non_latin1_text():\n    plt.figure()\n    text = plt.text(0.5, 0.5, \"\u4f60\u597d\u4e16\u754c\", fontsize=20)\n    fig.canvas.draw()\n    assert text.get_window_extent().width > 0\n"], "sample_904": ["def test_label_target_in_body(app):\n    text = (\"This is some text with a label:\"\n            \".. _target:\\n\"\n            \"Now some more text.\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, ([nodes.paragraph,\n                          (\"This is some text with a label\",),\n                          [nodes.labeled_target, \"target\", \"target\"],\n                          (\"Now some more text.\",),]))\n\n    domain = app.env.get_domain(\"std\")\n    assert 'target' in domain.labels\n    assert domain.labels['target'] == ('index', 'target', 'index')\n", "compilation error", "def test_option_with_no_description(app):\n    text = (\".. program:: ls\\n\"\n            \"\\n\"\n            \".. option:: -l\\n\"\n            \"\\n\")\n    domain = app.env.get_domain('std')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"-l\"],\n                                                    [desc_addname, ()])],\n                                  [desc_content, ()])]))\n    assert_node(doctree[0], addnodes.index,\n                entries=[('pair', 'ls command line option; -l', 'cmdoption-ls-l', '', None)])\n    assert ('ls', '-l') in domain.progoptions\n    assert domain.progoptions[('ls', '-l')] == ('index', 'cmdoption-ls-l')\n", "compilation error", "def test_productionlist_with_colons(app, status, warning):\n    app.builder.build_all()\n\n    with (app.outdir / 'index.html').open('rb') as f:\n        etree = HTMLParser(namespaceHTMLElements=False).parse(f)\n    ul = list(etree.iter('ul'))[1]\n    for li in list(ul):\n        assert len(list(li)) == 1\n        p = list(li)[0]\n        assert p.tag == 'p'\n        text = str(p.text).strip(' :')\n        assert ':' in text\n\n\n\n"], "sample_255": ["    def test_close_connection(self):\n        request = WSGIRequest(self.request_factory.get('/').environ)\n        request.makefile = lambda *args, **kwargs: BytesIO()\n        handler = WSGIRequestHandler(request, '192.168.0.2', None)\n        handler.close_connection = True\n        with self.assertLogs('django.server', 'INFO') as cm:\n            handler.handle_one_request()\n        self.assertEqual(len(cm.records), 1)\n        self.assertEqual(cm.records[0].levelname, 'INFO')\n        self.assertIn('Connection closed', cm.records[0].getMessage())\n\n\n\n", "    def test_close_connection(self):\n        request = WSGIRequest(self.request_factory.get('/').environ)\n        request.makefile = lambda *args, **kwargs: BytesIO()\n        handler = WSGIRequestHandler(request, '192.168.0.2', None)\n\n        # Set close_connection to True and check if it's respected.\n        handler.close_connection = True\n        handler.handle_one_request()\n        self.assertTrue(handler.connection.closed)\n\n\n\n", "    def test_close_connection(self):\n        request = WSGIRequest(self.request_factory.get('/').environ)\n        request.makefile = lambda *args, **kwargs: BytesIO()\n        handler = WSGIRequestHandler(request, '192.168.0.2', None)\n        handler.close_connection = True\n        handler.handle_one_request()\n        self.assertTrue(handler.server.socket.closed)\n\n        # This test is based on the observation\n        # that the server's socket is closed after handle_one_request\n\n", "    def test_content_length_handling(self):\n        request = WSGIRequest(self.request_factory.get('/').environ)\n        request.makefile = lambda *args, **kwargs: BytesIO()\n        handler = WSGIRequestHandler(request, '192.168.0.2', None)\n\n        # Case 1: Content-Length is defined\n        request.environ['CONTENT_LENGTH'] = '1024'\n        handler.cleanup_headers()\n        self.assertEqual(handler.headers.get('Connection'), 'keep-alive')\n\n        # Case 2: Content-Length is not defined\n        del request.environ['CONTENT_LENGTH']\n        handler.cleanup_headers()\n        self.assertEqual(handler.headers.get('Connection'), 'close')\n\n        # Case 3: Content-Length is zero\n        request.environ['CONTENT_LENGTH'] = '0'\n        handler.cleanup_headers()\n        self.assertEqual(handler.headers.get('Connection'), 'close')\n", "    def test_close_connection(self):\n        request = WSGIRequest(self.request_factory.get('/').environ)\n        request.makefile = lambda *args, **kwargs: BytesIO()\n\n        handler = WSGIRequestHandler(request, '192.168.0.2', None)\n        handler.close_connection = True\n\n        # Ensure that the connection is closed\n        with self.assertRaises(EOFError):\n            handler.rfile.readline()\n"], "sample_1203": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_331": ["    def test_iso8601_duration(self):\n        test_values = (\n            ('P1D', timedelta(days=1)),\n            ('P1DT2H', timedelta(days=1, hours=2)),\n            ('P1D2H3M', timedelta(days=1, hours=2, minutes=3)),\n            ('PT2H3M', timedelta(hours=2, minutes=3)),\n            ('PT2H3M4S', timedelta(hours=2, minutes=3, seconds=4)),\n            ('PT2.5H', timedelta(hours=2, minutes=30)),\n            ('P10Y', timedelta(days=3650)),\n            ('P10Y2M', timedelta(days=3652, minutes=120)),\n            ('P10Y2M3D', timedelta(days=3655, minutes=120)),\n            ('P10Y2M3DT1H3M', timedelta(days=3658, hours=1, minutes=3)),\n            ('P10Y2M3DT1H3M4S', timedelta(days=3658, hours=1, minutes=3, seconds=4)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n\n", "    def test_iso8601_duration(self):\n        test_values = (\n            ('P3D', timedelta(days=3)),\n            ('P10H', timedelta(hours=10)),\n            ('P10H30M', timedelta(hours=10, minutes=30)),\n            ('P10H30M15S', timedelta(hours=10, minutes=30, seconds=15)),\n            ('P10H30M15S.123456', timedelta(hours=10, minutes=30, seconds=15, microseconds=123456)),\n            ('PT10H30M15S', timedelta(hours=10, minutes=30, seconds=15)),\n            ('PT10H30M15S.123456', timedelta(hours=10, minutes=30, seconds=15, microseconds=123456)),\n            ('P10Y', timedelta(days=10 * 365)),\n            ('P10Y20D', timedelta(days=10 * 365 + 20)),\n            ('PT10H30M15S.123456Z', timedelta(hours=10, minutes=30, seconds=15, microseconds=123456)),\n            ('P-10H30M15S', timedelta(hours=-10, minutes=-30, seconds=-15)),\n            ('P-10Y', timedelta(days=-10 * 365)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n", "    def test_iso8601_duration(self):\n        test_values = (\n            ('P1D', timedelta(days=1)),\n            ('P1DT2H', timedelta(days=1, hours=2)),\n            ('PT2H', timedelta(hours=2)),\n            ('PT2H30M', timedelta(hours=2, minutes=30)),\n            ('P1DT2H30M', timedelta(days=1, hours=2, minutes=30)),\n            ('P1DT2H30M1S', timedelta(days=1, hours=2, minutes=30, seconds=1)),\n            ('PT2H30M1.875S', timedelta(hours=2, minutes=30, seconds=1, microseconds=875000)),\n            ('P1DT2H30M1S.875', timedelta(days=1, hours=2, minutes=30, seconds=1, microseconds=875000)),\n            ('P-1D', timedelta(days=-1)),\n            ('P-1DT-2H', timedelta(days=-1, hours=-2)),\n            ('P-1DT-2H30M', timedelta(days=-1, hours=-2, minutes=-30)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n\n\n", "    def test_iso8601_duration(self):\n        test_values = (\n            ('P1D', timedelta(days=1)),\n            ('PT1H', timedelta(hours=1)),\n            ('PT1M', timedelta(minutes=1)),\n            ('PT1S', timedelta(seconds=1)),\n            ('P1DT1H', timedelta(days=1, hours=1)),\n            ('P1DT1H30M', timedelta(days=1, hours=1, minutes=30)),\n            ('P1D1H30M1S', timedelta(days=1, hours=1, minutes=30, seconds=1)),\n            ('P1DT10H30M1S.123', timedelta(days=1, hours=10, minutes=30, seconds=1, microseconds=123000)),\n            ('PT10H30M1S.123', timedelta(hours=10, minutes=30, seconds=1, microseconds=123000)),\n            ('-P1D', timedelta(days=-1)),\n            ('P1DT-1H', timedelta(days=1, hours=-1)),\n            ('P1D-10H30M1S', timedelta(days=1, hours=-10, minutes=-30, seconds=-1)),\n            ('P1DT10H30M1S.123', timedelta(days=1, hours=10, minutes=30, seconds=1, microseconds=123000)),\n            ('PT10H30M1S.123', timedelta(hours=10, minutes=30, seconds=1, microseconds=123000)),\n            ('P1Y', timedelta(days=365)),\n            ()\n            ('P1Y1M', timedelta(days=366,  microseconds=0)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n\n", "    def test_iso8601_duration(self):\n        test_values = (\n            ('P1D', timedelta(days=1)),\n            ('P1DT1H', timedelta(days=1, hours=1)),\n            ('PT1H', timedelta(hours=1)),\n            ('PT1M', timedelta(minutes=1)),\n            ('PT1S', timedelta(seconds=1)),\n            ('P1DT10H3M', timedelta(days=1, hours=10, minutes=3)),\n            ('P1DT10H3M10S', timedelta(days=1, hours=10, minutes=3, seconds=10)),\n            ('P1D', timedelta(days=1)),\n            ('P1Y', timedelta(days=365)),\n            ('P1Y1M', timedelta(days=365, days=30)),\n            ('P1Y1M10D', timedelta(days=365, days=30, days=10)),\n            ('P1Y1M10DT10H3M10S', timedelta(days=365, days=30, days=10, hours=10, minutes=3, seconds=10)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n"], "sample_269": ["    def test_javascript_gettext_translations_missing(self):\n        with override('xx'):\n            self.selenium.get(self.live_server_url + '/jsi18n_template/')\n\n            elem = self.selenium.find_element_by_id(\"gettext\")\n            self.assertEqual(elem.text, 'gettext')\n            elem = self.selenium.find_element_by_id(\"ngettext_sing\")\n            self.assertEqual(elem.text, 'ngettext_sing')\n            elem = self.selenium.find_element_by_id(\"ngettext_plur\")\n            self.assertEqual(elem.text, 'ngettext_plur')\n            elem = self.selenium.find_element_by_id(\"ngettext_onnonplural\")\n            self.assertEqual(elem.text, 'ngettext_onnonplural')\n            elem = self.selenium.find_element_by_id(\"pgettext\")\n            self.assertEqual(elem.text, 'pgettext')\n            elem = self.selenium.find_element_by_id(\"npgettext_sing\")\n            self.assertEqual(elem.text, 'npgettext_sing')\n            elem = self.selenium.find_element_by_id(\"npgettext_plur\")\n            self.assertEqual(elem.text, 'npgettext_plur')\n", "    def test_i18n_with_locale_inheritance(self):\n        \"\"\"\n        Tests if locale inheritance works as expected when using JavaScriptCatalog.\n        \"\"\"\n        with self.settings(LANGUAGE_CODE='en-us', LOCALE_PATHS=[\n            path.join(path.dirname(path.abspath(__file__)), 'locales'),\n        ]):\n            with override('de'):\n                response = self.client.get('/jsi18n_template/')\n                self.assertContains(response, 'Entfernen')\n\n            with override('fr'):\n                response = self.client.get('/jsi18n_template/')\n                self.assertContains(response, 'Supprimer')\n\n\n\n", "    def test_i18n_with_invalid_package_names(self):\n        view = JavaScriptCatalog.as_view()\n        request = RequestFactory().get('/')\n        invalid_package_names = [' ', '==', '123']\n        for pkg_name in invalid_package_names:\n            with self.assertRaises(ValueError):\n                view(request, packages=pkg_name)\n\n", "    def test_i18n_language_fallback_with_app_translation(self):\n        \"\"\"\n        Check if the JavaScript i18n view returns a complete language catalog\n        if the selected language has no translation for a specific package\n        and the fallback language has the translation. See #13388,\n        #3594 and #13726 for more details.\n        \"\"\"\n        with self.settings(LANGUAGE_CODE='fr'), override('en-us'):\n            response = self.client.get('/jsi18n_app6/')\n            self.assertContains(response, 'this app6 string is to be translated (en)')\n\n\n", "    def test_i18n_with_custom_domain(self):\n        with self.settings(LANGUAGE_CODE='en-us'), override('fr'):\n            response = self.client.get('/jsi18n_my_domain/')\n            self.assertContains(response, 'Translated string for my custom domain')\n"], "sample_779": ["def test_check_estimator_get_params_deep_copy(", "def test_check_estimator_handles_deprecated_params_gracefully():\n    class DeprecatedParamEstimator(BaseEstimator):\n            self.old_param = old_param\n            self.new_param = new_param\n\n        @deprecated(\"Use 'new_param' instead.\")\n            return self\n\n    deprecated_param_estimator = DeprecatedParamEstimator(old_param=1,\n                                                         new_param=2)\n    check_estimator(deprecated_param_estimator)\n", "def test_check_estimator_multioutput():\n    # Check that check_estimator handles estimators with the multi_output\n    # attribute correctly.\n\n    class MultiOutputClassifier(BaseEstimator):\n            self.multi_output = True\n\n            return self\n\n            return np.ones(X.shape[0])\n\n    check_estimator(MultiOutputClassifier())\n\n\n\n", "def test_check_classifier_with_multi_output_labels():\n    class MultiOutputClassifier(BaseEstimator):\n            return self\n\n            return np.zeros(X.shape[0] * 2)\n\n    check_estimators_unfitted(\"estimator\", MultiOutputClassifier())\n\n\n\n", "    def test_check_estimator_cross_validation_params(self):\n        from sklearn.linear_model import LogisticRegression\n        with ignore_warnings(category=(FutureWarning, DeprecationWarning)):\n            est = LogisticRegression()\n            set_checking_parameters(est)\n            set_random_state(est)\n            msg = r\"Estimator should not have 'cv' parameter while 'init_params' is set to True\"\n            assert_raises_regex(AssertionError, msg, check_estimator, est)\n"], "sample_378": ["    def test_related_model_creation(self):\n        parent = RelatedObject.objects.create()\n        child = SingleObject()\n        parent.single = child\n        parent.save()\n        child_instance = SingleObject.objects.create(name='test')\n        with self.assertNumQueries(1):\n            RelatedObject.objects.bulk_update(\n                [parent],\n                fields=['single'],\n                batch_size=1\n            )\n        self.assertEqual(parent.single, child_instance)\n\n\n\n", "    def test_select_related_field_with_bulk_update(self):\n        member = Member.objects.create(name='test')\n        note = Note.objects.create(note='test note', member=member)\n        notes = [Note.objects.get(pk=note.pk)]\n        Note.objects.select_related('member').bulk_update(notes, ['note'])\n        self.assertEqual(note.member, member)\n\n\n", "    def test_related_object_bulk_update_with_foreign_key(self):\n        parent_objs = [\n            RelatedObject.objects.create(name=f\"Parent-{i}\")\n            for i in range(10)\n        ]\n        child_objs = [\n            SingleObject.objects.create(name=f\"Child-{i}\")\n            for i in range(10)\n        ]\n        for parent, child in zip(parent_objs, child_objs):\n            parent.single = child\n        with self.assertNumQueries(20):\n            RelatedObject.objects.bulk_update(parent_objs, ['single'])\n        for parent in parent_objs:\n            self.assertEqual(parent.single, child_objs[parent_objs.index(parent)])\n\n\n\n", "    def test_prefetch_related(self):\n        tags = Tag.objects.create_batch(10)\n        notes = Note.objects.create_batch(10)\n        note_tag_relations = [\n            (note, tags[i])\n            for i, note in enumerate(notes)\n        ]\n        for note, tag in note_tag_relations:\n            note.tags.add(tag)\n\n        with self.assertNumQueries(1):\n            Note.objects.bulk_update(notes, ['note'])\n\n        for note, tag in note_tag_relations:\n            with self.subTest(note=note, tag=tag):\n                note.refresh_from_db()\n                self.assertEqual(note.note, 'test-%s' % note.id)\n                self.assertEqual(note.tags.count(), 1)\n                self.assertEqual(note.tags.first(), tag)\n", "    def test_non_related_fields_updating(self):\n        objects = [\n            Valid.objects.create(valid='test', parent=None)\n            for _ in range(10)\n        ]\n        for obj in objects:\n            obj.valid = 'updated'\n        Valid.objects.bulk_update(objects, ['valid'])\n        self.assertCountEqual(Valid.objects.values_list('valid', flat=True), ['updated'] * len(objects))\n\n\n\n"], "sample_134": ["    def test_serialize_django_field_defaults(self):\n        for field_class in [\n            models.AutoField,\n            models.AutoField,\n            models.BooleanField,\n            models.CharField,\n            models.DateField,\n            models.DateTimeField,\n            models.DecimalField,\n            models.EmailField,\n            models.FileField,\n            models.FilePathField,\n            models.FloatField,\n            models.ImageField,\n            models.IntegerField,\n            models.ManyToManyField,\n            models.OneToOneField,\n            models.SlugField,\n            models.TextField,\n            models.URLField,\n        ]:\n            with self.subTest(field_class=field_class):\n                field = field_class(max_length=255)\n                self.assertSerializedResultEqual(\n                    field,\n                    (\"models.%s(max_length=255)\" % field_class.__name__.lower(), {'from django.db import models'})\n                )\n", "    def test_serialize_deconstructible_object(self):\n        class DeconstructibleInstance:\n                self.value = value\n        deconstructed = deconstructible(path=\"migrations.test_writer.DeconstructibleInstance\")(DeconstructibleInstance)(123)\n\n        string = MigrationWriter.serialize(deconstructed)[0]\n        self.assertEqual(string, 'migrations.test_writer.DeconstructibleInstance(123)')\n", "    def test_serialize_custom_field(self):\n        class CustomField(models.Field):\n                super().__init__(**kwargs)\n\n                return 'CUSTOM_FIELD'\n\n                return 'CharField'\n\n                return value\n\n            return f'CustomField({value})'\n\n        class CustomFieldSerializer(BaseSerializer):\n                return to_string(self.value), {}\n\n        field = CustomField(max_length=255)\n        string, imports = MigrationWriter.serialize(field)\n        self.assertEqual(string, 'CustomField(max_length=255)')\n        self.assertEqual(imports, {'from migrations.test_writer import CustomField, CustomFieldSerializer'})\n\n\n\n", "    def test_serialize_custom_field(self):\n        class CustomField(models.Field):\n            pass\n\n        CustomField.register_serializer(CustomSerializer)\n        string = MigrationWriter.serialize(models.CharField(max_length=100, default='', field=CustomField()))[0]\n        self.assertEqual(string, \"models.CharField(max_length=100, default='', field=CustomField())\")\n", "    def test_serialize_custom_serializer(self):\n        class MyCustomSerializer(BaseSerializer):\n                return f'\"{obj}\"', set()\n\n        field = models.CharField(default='some_value', serialize_with=MyCustomSerializer)\n        string = MigrationWriter.serialize(field)[0]\n        self.assertEqual(\n            string,\n            \"models.CharField(default='some_value', serialize_with=migrations.test_writer.MyCustomSerializer)\"\n        )\n\n        \n"], "sample_211": ["    def test_use_queryset_from_args(self):\n        test_view = views.CustomMultipleObjectMixinView()\n        queryset = [{'name': 'Lennon'}, {'name': 'Ono'}]\n        context = test_view.get_context_data(object_list=queryset)\n        self.assertEqual(context['object_list'], queryset) \n", "    def test_queryset_with_template_args(self):\n        test_view = views.CustomMultipleObjectMixinView()\n        test_view.get(self.rf.get('/with/args/'))\n        queryset = [{'name': 'Lennon'}, {'name': 'Ono'}]\n        context = test_view.get_context_data(object_list=queryset)\n        self.assertEqual(context['object_list'], queryset)\n        self.assertEqual(context['args'], ('with', 'args'))\n", "    def test_custom_context_object_name(self):\n        test_view = views.CustomMultipleObjectMixinView()\n        test_view.context_object_name = 'my_objects'\n        context = test_view.get_context_data()\n        self.assertEqual(context['my_objects'], test_view.object_list)\n\n\n", "    def test_custom_context_object_name(self):\n        test_view = views.CustomMultipleObjectMixinView()\n        test_view.context_object_name = 'my_ponies'\n        context = test_view.get_context_data()\n        self.assertEqual(context['my_ponies'], test_view.object_list)\n\n", "    def test_custom_context_data_ordering(self):\n        test_view = views.CustomContextView()\n        context = test_view.get_context_data()\n        self.assertEqual(context['test_name'], 'test_value')\n        self.assertEqual(context['custom_key'], 'custom_value')\n        self.assertEqual(context['kwarg_test'], 'default_value')\n"], "sample_1001": ["def test_derivative():\n    from sympy import Derivative\n    x = symbols('x')\n    assert latex(Derivative(x**2, x)) == r'\\frac{d}{d x} x^{2}'\n    assert latex(Derivative(x**2, x, 2)) == r'\\frac{d^{2}}{d x^{2}} x^{2}'\n    assert latex(Derivative(sin(x), x)) == r'\\frac{d}{d x} \\sin{\\left (x \\right )}'\n    assert latex(Derivative(sin(x), x, 2)) == r'\\frac{d^{2}}{d x^{2}} \\sin{\\left (x \\right )}'\n\n\n\n", "def test_Function_printing():\n    from sympy.core.function import Function\n    f = Function('f')\n    assert latex(f(x)) == r'f{\\left (x \\right )}'\n    g = Function('g', nargs=2, dummy_args=True)\n    assert latex(g(x, y)) == r'g{\\left (x , y \\right )}'\n    h = Function('h', dummy_args=True)\n    assert latex(h()) == r'h{}'\n\n\n\n", "compilation error", "def test_issue_15704():\n    from sympy.polys import Poly\n    x = Symbol('x')\n    p = Poly(x**2 + x, x)\n    assert latex(p) == r\"{x}^{2} + x\"\n", "def test_issue_14912():\n    from sympy.physics.vector import Vector\n    v = Vector('v')\n    assert latex(v.cross(v)) == r'\\vec{v} \\times \\vec{v}'\n"], "sample_500": ["def test_colorbar_reversed():\n    fig, ax = plt.subplots()\n    im = ax.imshow([[1, 2, 3], [4, 5, 6]])\n    cbar = fig.colorbar(im, orientation='vertical')\n    for tick in cbar.ax.get_yticklabels():\n        assert tick.get_text() == str(round(float(tick.get_text()), 1))\n\n    cbar.set_orientation('horizontal')\n    for tick in cbar.ax.get_xticklabels():\n        assert tick.get_text() == str(round(float(tick.get_text()), 1))\n\n    cbar.set_ticks([])\n    assert len(cbar.ax.get_yticks()) == 0\n    assert len(cbar.ax.get_xticks()) == 0\n\n    cbar.set_ticks(np.linspace(0, 1, 5))\n    assert len(cbar.ax.get_yticks()) == 5\n\n\n\n", "def test_colorbar_fraction_alignment():\n    fig, ax = plt.subplots()\n    im = ax.imshow([[1, 2], [3, 4]])\n    cbar = fig.colorbar(im, fraction=0.1, pad=0.01, alignment='left')\n    ax.set_position([0.1, 0.1, 0.8, 0.8])\n    fig.canvas.draw()\n    # Check the position of the colorbar\n    pos = cbar.ax.get_position()\n    assert pos.x0 == 0.1\n    assert pos.y0 == 0.8\n\n", "def test_colorbar_label_rotation():\n    fig, ax = plt.subplots()\n    im = ax.imshow([[1, 2], [3, 4]])\n    cbar = fig.colorbar(im, label='My Label')\n    cbar.ax.tick_params(labelrotation=90)\n    fig.canvas.draw()\n    assert cbar.ax.tick_params(axis='y', labelrotation=90)\n\n\n\n", "def test_colorbar_fraction_pad_shrink_kwargs():\n    fig, ax = plt.subplots()\n    im = ax.imshow([[1, 2], [3, 4]])\n    cbar = fig.colorbar(im, fraction=0.1, pad=0.05, shrink=0.8)\n    fig.canvas.draw()\n    bounds = cbar.ax.get_position().bounds\n    assert abs(bounds[0] - 0.15) < 1e-3\n    assert abs(bounds[1] - 0.95) < 1e-3\n    assert abs(bounds[2] - 0.08) < 1e-3\n    assert abs(bounds[3] - 0.04) < 1e-3\n\n\n", "def test_colorbar_tick_locations_with_custom_cmap():\n    fig, ax = plt.subplots()\n    data = np.random.rand(5, 5)\n    pcm = ax.pcolormesh(data, cmap=ListedColormap(['blue', 'red']))\n    cbar = fig.colorbar(pcm)\n    ticks = cbar.get_ticks()\n    # Expected number of ticks depends on the number of colors in the cmap\n    # Check that the number of ticks isn't less than the number of colors\n    assert len(ticks) >= len(ListedColormap(['blue', 'red']))\n\n\n"], "sample_828": ["def test_pairwise_distances_chunked_with_kernel():\n    rng = np.random.RandomState(0)\n    X = rng.normal(size=(1000, 10), scale=1e10)\n    kernels = ['linear', 'cosine', 'rbf']\n    for kernel in kernels:\n        # Check pairwise distances calculation with chunking\n        # and kernels\n        distances = pairwise_distances_chunked(X, metric=kernel,\n                                              working_memory=1,\n                                              n_jobs=2)\n        assert isinstance(distances, GeneratorType)\n        distances = list(distances)\n        assert_array_almost_equal(distances, pairwise_distances(X, metric=kernel))\n\n", "def test_different_input_types():\n    # Ensure that ValueError is raised for different input types\n    XA = np.array([[1, 2], [3, 4]])\n    # Test for list, tuple, and dictionary\n    for incorrect_input in ([1, 2, 3], [(1, 2, 3)], {\"a\": 1, \"b\": 2}):\n        with pytest.raises(ValueError):\n            check_pairwise_arrays(XA, incorrect_input)\n", "def test_check_sparse_arrays_mixed():\n    # Ensure that checks handle mixed sparse and dense arrays.\n    rng = np.random.RandomState(0)\n    XA = rng.random_sample((5, 4))\n    XA_sparse = csr_matrix(XA)\n    XB = rng.random_sample((5, 4))\n    XA_checked, XB_checked = check_pairwise_arrays(XA_sparse, XB)\n    assert issparse(XA_checked)\n    assert_equal(abs(XA_sparse - XA_checked).sum(), 0)\n    assert_array_equal(XB, XB_checked)\n\n    XA_checked, XB_checked = check_pairwise_arrays(XA, XB_sparse)\n    assert_array_equal(XA, XA_checked)\n    assert issparse(XB_checked)\n    assert_equal(abs(XB_sparse - XB_checked).sum(), 0)\n\n", "def test_pairwise_similarity_sparse_input(metric, pairwise_func):\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    Y = rng.random_sample((3, 4))\n    X_sparse = csr_matrix(X)\n    Y_sparse = csr_matrix(Y)\n\n    # should be sparse\n    K1 = pairwise_func(X_sparse, Y_sparse)\n    assert issparse(K1)\n\n    # should be dense, and equal to K1\n    K2 = pairwise_func(X, Y)\n    assert_array_almost_equal(K1.todense(), K2)\n", "def test_check_pairwise_arrays_with_sparse_input():\n    # Ensure that check_pairwise_arrays handles sparse inputs correctly\n    rng = np.random.RandomState(0)\n    XA = rng.random_sample((5, 4)).astype(np.float32)\n    XA_sparse = csr_matrix(XA)\n    XB = rng.random_sample((5, 4)).astype(np.float32)\n    XB_sparse = csr_matrix(XB)\n\n    XA_checked, XB_checked = check_pairwise_arrays(XA_sparse, XB_sparse)\n    assert isinstance(XA_checked, csr_matrix)\n    assert isinstance(XB_checked, csr_matrix)\n    assert_array_almost_equal(XA_sparse.toarray(), XA_checked.toarray())\n    assert_array_almost_equal(XB_sparse.toarray(), XB_checked.toarray())\n\n\n"], "sample_1067": ["def test_issue_7135():\n    x, y, z = symbols('x y z')\n    a = Wild('a')\n    b = Wild('b')\n    e = x**2 + y**2 + z**2\n\n    assert e.match(a * x**2 + b * y**2 + z**2) == {a: 1, b: 1}\n    assert e.match(a * x**2 + b * y**2 + c*z**2) == {a: 1, b: 1, c: 1}\n", "def test_issue_6585():\n    x, y, z = symbols('x y z')\n    a = Wild('a')\n\n    assert (x*y*z).match(a*x*(y*z)).is_not_None\n", " def test_issue_4097():\n    x = Symbol('x')\n    a = Wild('a')\n    b = Wild('b')\n    e = x*a + b\n\n    assert e.match(a*x + b, b) == {a: 1}\n", "def test_issue_5415():\n    x = Symbol('x')\n    y = Symbol('y')\n    a = Wild('a')\n    b = Wild('b')\n    \n    expr = (a * x + b * y)**2\n    pattern = (a * x + b * y) * (a * x + b * y)\n\n    assert expr.match(pattern) == {}\n", "compilation error"], "sample_875": ["def test_balanced_accuracy_score_with_zero_samples():\n    for i in range(10):\n        y_true = np.array([0] * i)\n        y_pred = np.array([0] * i)\n        balanced = balanced_accuracy_score(y_true, y_pred)\n        assert balanced == 1.0\n        balanced = balanced_accuracy_score(y_true, y_pred, adjusted=True)\n        assert balanced == 1.0\n\n    for i in range(10):\n        y_true = np.array([1] * i)\n        y_pred = np.array([1] * i)\n        balanced = balanced_accuracy_score(y_true, y_pred)\n        assert balanced == 1.0\n        balanced = balanced_accuracy_score(y_true, y_pred, adjusted=True)\n        assert balanced == 1.0\n\n\n", "def test_confusion_matrix():\n    y_true = np.array([0, 0, 1, 1, 0, 1, 0, 1, 1, 0])\n    y_pred = np.array([0, 1, 1, 1, 0, 1, 0, 1, 1, 0])\n    cm = confusion_matrix(y_true, y_pred)\n    assert cm.shape == (2, 2)\n    assert np.array_equal(cm, np.array([[4, 3], [2, 2]]))\n", "def test_balanced_accuracy_score_multiclass():\n    y_true = np.array([3, 0, 1, 2, 3, 0, 1, 2])\n    y_pred = np.array([3, 0, 1, 2, 3, 0, 1, 2])\n    balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n    assert balanced_accuracy == 1.0\n\n\n", "def test_balanced_accuracy_score_ties():\n    y_true = np.array([0, 0, 1, 1, 0, 0])\n    y_pred = np.array([0, 0, 1, 1, 0, 0])\n    balanced = balanced_accuracy_score(y_true, y_pred)\n    assert balanced == 1.0\n    y_true = np.array([0, 0, 1, 1, 0, 0])\n    y_pred = np.array([0, 0, 1, 1, 0, 1])\n    balanced = balanced_accuracy_score(y_true, y_pred)\n    assert balanced == pytest.approx(0.8333)\n    y_true = np.array([0, 0, 1, 1, 0, 0])\n    y_pred = np.array([0, 1, 1, 0, 0, 0])\n    balanced = balanced_accuracy_score(y_true, y_pred)\n    assert balanced == pytest.approx(0.8333)\n", "    def test_roc_auc_score_multiclass_average():\n        y_true = np.array([0, 1, 2, 0, 1, 1, 2, 0, 1])\n        y_score = np.array([[0.1, 0.8, 0.1],\n                           [0.2, 0.7, 0.1],\n                           [0.3, 0.6, 0.1],\n                           [0.4, 0.5, 0.1],\n                           [0.5, 0.4, 0.1],\n                           [0.6, 0.3, 0.1],\n                           [0.7, 0.2, 0.1],\n                           [0.8, 0.1, 0.1],\n                           [0.9, 0.0, 0.1]])\n        average_methods = [\"micro\", \"macro\", \"weighted\", \"samples\"]\n        for average in average_methods:\n            roc_auc = roc_auc_score(y_true, y_score, average=average)\n            assert np.isfinite(roc_auc)\n\n        with pytest.raises(ValueError):\n            roc_auc_score(y_true, y_score, average=\"unknown\")\n\n\n"], "sample_943": ["def test_package_file_with_inheritance(tempdir):\n    outdir = path(tempdir)\n    (outdir / 'parent').makedirs()\n    (outdir / 'parent' / '__init__.py').write_text('')\n    (outdir / 'parent' / 'child.py').write_text(\n        \"class Parent:\\n\"\n        \"    pass\\n\"\n        \"\\n\"\n        \"class Child(Parent):\\n\"\n        \"    pass\"\n    )\n    apidoc_main(['-o', tempdir, tempdir / 'parent'])\n    assert (outdir / 'parent.rst').exists()\n\n    content = (outdir / 'parent.rst').read_text()\n    assert content == (\"parent package\\n\"\n                       \"===============\\n\"\n                       \"\\n\"\n                       \"Module contents\\n\"\n                       \"---------------\\n\"\n                       \"\\n\"\n                       \".. automodule:: parent\\n\"\n                       \"   :members:\\n\"\n                       \"   :undoc-members:\\n\"\n                       \"   :show-inheritance:\\n\"\n                       \"\\n\"\n                       \"Submodules\\n\"\n                       \"----------\\n\"\n                       \"\\n\"\n                       \"parent.child module\\n\"\n                       \"--------------------\\n\"\n                       \"\\n\"\n                       \".. automodule:: parent.child\\n\"\n                       \"   :members:\\n\"\n                       \"   :undoc-members:\\n\"\n                       \"   :show-inheritance:\\n\")\n\n", "compilation error", "compilation error", "def test_package_file_with_submodule_and_subpackage(tempdir):\n    outdir = path(tempdir)\n    (outdir / 'testpkg').makedirs()\n    (outdir / 'testpkg' / '__init__.py').write_text('')\n    (outdir / 'testpkg' / 'example.py').write_text('')\n    (outdir / 'testpkg' / 'subpkg').makedirs()\n    (outdir / 'testpkg' / 'subpkg' / '__init__.py').write_text('')\n    (outdir / 'testpkg' / 'subpkg' / 'subsubpkg').makedirs()\n    (outdir / 'testpkg' / 'subpkg' / 'subsubpkg' / '__init__.py').write_text('')\n    apidoc_main(['-o', tempdir, tempdir / 'testpkg'])\n    assert (outdir / 'testpkg.rst').exists()\n    assert (outdir / 'testpkg.subpkg.rst').exists()\n    assert (outdir / 'testpkg.subpkg.subsubpkg.rst').exists()\n\n    content = (outdir / 'testpkg.rst').read_text()\n    assert content == (\"testpkg package\\n\"\n                       \"===============\\n\"\n                       \"\\n\"\n                       \"Subpackages\\n\"\n                       \"-----------\\n\"\n                       \"\\n\"\n                       \".. toctree::\\n\"\n                       \"   :maxdepth: 4\\n\"\n                       \"\\n\"\n                       \"   testpkg.subpkg\\n\"\n                       \"\\n\"\n                       \"Module contents\\n\"\n                       \"---------------\\n\"\n                       \"\\n\"\n                       \".. automodule:: testpkg\\n\"\n                       \"   :members:\\n\"\n                       \"   :undoc-members:\\n\"\n                       \"   :show-inheritance:\\n\"\n                       \"\\n\"\n                       \"Submodules\\n\"\n                       \"----------\\n\"\n                       \"\\n\"\n                       \"testpkg.example module\\n\"\n                       \"----------------------\\n\"\n                       \"\\n\"\n                       \".. automodule:: testpkg.example\\n\"\n                       \"   :members:\\n\"\n                       \"   :undoc-members:\\n\"\n                       \"   :show-inheritance:\\n\")\n\n    content =", "def test_package_file_with_nested_subpackages(tempdir):\n    outdir = path(tempdir)\n    (outdir / 'testpkg').makedirs()\n    (outdir / 'testpkg' / '__init__.py').write_text('')\n    (outdir / 'testpkg' / 'subpkg1').makedirs()\n    (outdir / 'testpkg' / 'subpkg1' / '__init__.py').write_text('')\n    (outdir / 'testpkg' / 'subpkg1' / 'subpkg2').makedirs()\n    (outdir / 'testpkg' / 'subpkg1' / 'subpkg2' / '__init__.py').write_text('')\n    (outdir / 'testpkg' / 'example.py').write_text('')\n    apidoc_main(['-o', tempdir, tempdir / 'testpkg'])\n    assert (outdir / 'testpkg.rst').exists()\n    assert (outdir / 'testpkg.subpkg1.rst').exists()\n    assert (outdir / 'testpkg.subpkg1.subpkg2.rst').exists()\n\n    content = (outdir / 'testpkg.rst').read_text()\n    assert content == (\"testpkg package\\n\"\n                       \"===============\\n\"\n                       \"\\n\"\n                       \"Subpackages\\n\"\n                       \"-----------\\n\"\n                       \"\\n\"\n                       \".. toctree::\\n\"\n                       \"   :maxdepth: 4\\n\"\n                       \"\\n\"\n                       \"   testpkg.subpkg1\\n\"\n                       \"\\n\"\n                       \"Submodules\\n\"\n                       \"----------\\n\"\n                       \"\\n\"\n                       \"testpkg.example module\\n\"\n                       \"--------------------\\n\"\n                       \"\\n\"\n                       \".. automodule:: testpkg.example\\n\"\n                       \"   :members:\\n\"\n                       \"   :undoc-members:\\n\"\n                       \"   :show-inheritance:\\n\"\n                       \"\\n\"\n                       \"Module contents\\n\"\n                       \"---------------\\n\"\n                       \"\\n\"\n                       \".. automodule:: testpkg\\n\"\n                       \"   :members:\\n\"\n                       \"   :undoc-members:\\n\"\n                       \"   :show-inheritance:\\n"], "sample_293": ["    def test_lookahead_negation(self):\n        test_urls = [\n            ('/lookahead-negation', {'city': 'not-a-city'}, '/lookahead-negation/not-a-city/'),\n            ('/lookbehind-negation', {'city': 'not-a-city'}, '/lookbehind-negation/not-a-city/'),\n        ]\n        for name, kwargs, expected in test_urls:\n            with self.subTest(name=name, kwargs=kwargs):\n                self.assertEqual(reverse(name, kwargs=kwargs), expected)\n", "    def test_lookahead_and_lookbehind_with_other_patterns(self):\n        test_url = '/other/lookahead-/a-city/'\n        with self.subTest(url=test_url):\n            match = resolve(test_url)\n            self.assertEqual(match.kwargs, {'city': 'a-city'})\n", "    def test_multiple_lookahead_lookbehind(self):\n        test_urls = [\n            ('/lookahead_lookbehind_positive', {'city': 'a-city', 'region': 'us'}, '/lookahead/lookbehind+/a-city/us/'),\n            ('/lookahead_lookbehind_negative', {'city': 'a-city', 'region': 'us'}, '/lookahead/lookbehind-/a-city/us/'),\n        ]\n        for name, kwargs, expected in test_urls:\n            with self.subTest(name=name, kwargs=kwargs):\n                self.assertEqual(reverse(name, kwargs=kwargs), expected)\n\n", "    def test_empty_lookahead_lookbehind(self):\n        with self.subTest(url='/lookahead---'):\n            with self.assertRaises(Resolver404):\n                resolve('/lookahead---')\n        with self.subTest(url='/lookbehind---'):\n            with self.assertRaises(Resolver404):\n                resolve('/lookbehind---')\n\n\n", "    def test_regex_lookahead_no_match(self):\n        test_urls = [\n            '/lookahead-/not-a-city/',\n            '/lookbehind-/not-a-city/',\n            '/lookahead+/other-city/',\n            '/lookbehind+/other-city/',\n        ]\n        for test_url in test_urls:\n            with self.subTest(url=test_url):\n                with self.assertRaises(Resolver404):\n                    resolve(test_url)\n"], "sample_49": ["    def test_merge_css_priority(self):\n        \"\"\"\n        CSS files are merged in a way that prioritizes the last listed file\n        to be included if there are duplicates.\n        \"\"\"\n        widget1 = Media(css={'all': ['style1.css', 'style2.css']})\n        widget2 = Media(css={'all': ['style2.css', 'style3.css']})\n        form_media = widget1 + widget2\n        self.assertEqual(form_media._css, ['style1.css', 'style2.css', 'style3.css']) \n\n\n\n", "    def test_merge_css_three_way(self):\n        \"\"\"\n        The relative order of CSS stylesheets is preserved in a three-way merge.\n        \"\"\"\n        # custom_widget.css doesn't depend on jquery.css.\n        widget1 = Media(css={'all': ['custom_widget.css']})\n        widget2 = Media(css={'all': ['jquery.css', 'uses_jquery.css']})\n        form_media = widget1 + widget2\n        # The relative ordering of custom_widget.css and jquery.css has been\n        # established (but without a real need to).\n        self.assertEqual(form_media._css['all'], ['custom_widget.css', 'jquery.css', 'uses_jquery.css'])\n        # The inline also uses custom_widget.css. This time, it's at the end.\n        inline_media = Media(css={'all': ['jquery.css', 'also_jquery.css']}) + Media(css={'all': ['custom_widget.css']})\n        merged = form_media + inline_media\n        self.assertEqual(merged._css['all'], ['custom_widget.css', 'jquery.css', 'uses_jquery.css', 'also_jquery.css'])\n", "    def test_merge_with_different_media_types(self):\n        media1 = Media(css={'all': ['/path/to/css1']})\n        media2 = Media(js=['/path/to/js1'])\n        merged_media = media1 + media2\n        self.assertEqual(merged_media._css, ['/path/to/css1'])\n        self.assertEqual(merged_media._js, ['/path/to/js1'])\n", "    def test_merge_css_three_way(self):\n        widget1 = Media(css={'all': ['widget1.css']})\n        widget2 = Media(css={'all': ['widget2.css', 'widget3.css']})\n        form_media = widget1 + widget2\n        self.assertEqual(form_media._css, {\n            'all': ['widget1.css', 'widget2.css', 'widget3.css']\n        })\n        inline_media = Media(css={'all': ['inline.css']}) + Media(css={'all': ['widget2.css']})\n        merged = form_media + inline_media\n        self.assertEqual(merged._css, {\n            'all': ['widget1.css', 'widget2.css', 'widget3.css', 'inline.css']\n        })\n\n", "    def test_merge_css_three_way(self):\n        \"\"\"\n        The relative order of stylesheets is preserved in a three-way merge.\n        \"\"\"\n        # custom_widget.css doesn't depend on jquery.css.\n        widget1 = Media(css={'all': ['custom_widget.css']})\n        widget2 = Media(css={'all': ['jquery.css', 'uses_jquery.css']})\n        form_media = widget1 + widget2\n        # The relative ordering of custom_widget.css and jquery.css has been\n        # established (but without a real need to).\n        self.assertEqual(form_media._css, {'all': ['custom_widget.css', 'jquery.css', 'uses_jquery.css']})\n        # The inline also uses custom_widget.css. This time, it's at the end.\n        inline_media = Media(css={'all': ['jquery.css', 'also_jquery.css']}) + Media(css={'all': ['custom_widget.css']})\n        merged = form_media + inline_media\n        self.assertEqual(merged._css, {'all': ['custom_widget.css', 'jquery.css', 'uses_jquery.css', 'also_jquery.css']})\n"], "sample_679": ["def test_pytest_param_id_allows_whitespace(s):\n    assert pytest.param(id=s)\n", "def test_pytest_param_id_casts_to_string(id, expected):\n    assert pytest.param(id=id) is pytest.param(id=str(id))\n", "    def test_pytest_param_id_not_empty_string(s):\n        assert pytest.param(id=s)\n", "def test_pytest_param_id_raises_error_for_invalid_type() -> None:\n    with pytest.raises(TypeError) as excinfo:\n        pytest.param(id=123)  # type: ignore[arg-type]\n    (msg,) = excinfo.value.args\n    assert msg == \"Expected id to be a string, got <class 'int'>: 123\"\n\n\n\n", "def test_pytest_param_id_allows_empty_string() -> None:\n    assert pytest.param(id=\"\")\n\n\n\n"], "sample_669": ["compilation error", "def test_encodedfile_writelines_with_b_objects(tmpfile: BinaryIO) -> None:\n    ef = capture.EncodedFile(tmpfile, encoding=\"utf-8\")\n    with pytest.raises(TypeError):\n        ef.writelines([b\"line1\", b\"line2\"])\n    assert ef.writelines(['line3', 'line4']) is None  # type: ignore[func-returns-value]\n    ef.flush()\n    tmpfile.seek(0)\n    assert tmpfile.read() == b\"line3line4\"\n    tmpfile.close()\n", "def test_encodedfile_write_bytes(tmpfile: BinaryIO) -> None:\n    ef = capture.EncodedFile(tmpfile, encoding=\"utf-8\")\n    ef.write(b\"line1\")\n    ef.flush()\n    tmpfile.seek(0)\n    assert tmpfile.read() == b\"line1\"\n    tmpfile.close()\n", "    def test_encodedfile_seek_before_reading(tmpfile: BinaryIO) -> None:\n        ef = capture.EncodedFile(tmpfile, encoding=\"utf-8\")\n        ef.write(\"first line\\n\")\n        ef.seek(5)  \n        assert ef.read() == \" first line\\n\"\n\n\n", "def test_encodedfile_tell(tmpfile: BinaryIO) -> None:\n    ef = capture.EncodedFile(tmpfile, encoding=\"utf-8\")\n    ef.write(\"hello\")\n    assert ef.tell() == 5\n    ef.write(\" world\")\n    assert ef.tell() == 11\n    ef.seek(1)\n    assert ef.tell() == 2\n    ef.seek(0, 2)\n    assert ef.tell() == 11\n    ef.flush()\n    tmpfile.seek(0)\n    assert tmpfile.read() == b\"hello world\"\n    tmpfile.close()\n"], "sample_603": ["def test_repr_html_with_icon():\n    formatted = fh.dataset_repr(dataset)\n    assert \"<svg\" in formatted\n", "def test_summarize_attrs_with_html_in_values():\n    attrs = {\"<x>\": \"<b>bold</b>\", \"y\": \"<p><i>italic</i>\"}\n    formatted = fh.summarize_attrs(attrs)\n    assert \"<dt><span>&lt;x&gt; :</span></dt>\" in formatted\n    assert \"<dd><b>bold</b></dd>\" in formatted\n    assert \"<dt><span>y :</span></dt>\" in formatted\n    assert \"<dd><i>italic</i></dd>\" in formatted\n", "def test_summarize_attrs_with_empty_attrs(dataset):\n    dataset_empty_attrs = dataset.copy()\n    dataset_empty_attrs.attrs = {}\n    formatted = fh.summarize_attrs(dataset_empty_attrs.attrs)\n    assert formatted == \"\" \n", "def test_summarize_variables(dataset):\n    formatted = fh.summarize_vars(dataset.data_vars)\n    assert \"tmax\" in formatted\n    assert \"tmin\" in formatted\n", "def test_summarize_variable_with_multiindex(multiindex):\n    var = multiindex.x\n    formatted = fh.summarize_variable(\"x\", var, is_index=True)\n    assert \"MultiIndex\" in formatted\n    assert var.variable.to_index_variable().level_names in formatted\n\n\n\n\n"], "sample_767": ["def test_column_transformer_sparse_threshold_with_remainder():\n    X_array = np.array([[0, 1, 0],\n                        [2, 4, 6],\n                        [8, 6, 4]]).astype('float').T\n    ct = ColumnTransformer([('trans1', StandardScaler(), [0])],\n                           remainder=SparseMatrixTrans(),\n                           sparse_threshold=0.9)\n\n    X_trans = ct.fit_transform(X_array)\n    assert sparse.issparse(X_trans)\n    assert X_trans.shape == (3, 3)  # 3 rows, 3 cols\n    assert_array_equal(X_trans.toarray(), np.eye(3))\n    assert len(ct.transformers_) == 2\n    assert ct.transformers_[-1][0] == 'remainder'\n    assert isinstance(ct.transformers_[-1][1], SparseMatrixTrans)\n    assert_array_equal(ct.transformers_[-1][2], [1, 2])\n\n\n\n", "def test_column_transformer_sparse_input():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).astype('float').T\n    sparse_X = sparse.csr_matrix(X_array)\n\n    ct = ColumnTransformer([('trans1', StandardScaler(), [0])],\n                           sparse_threshold=0.5)\n\n    ct.fit(sparse_X)\n\n    # check transformations are applied correctly even with sparse input\n    assert isinstance(ct.transform(sparse_X), sparse.csr_matrix)\n    assert_array_almost_equal(ct.transform(sparse_X).toarray(),\n                             ct.transformers_[0][1].transform(X_array))\n\n\n\n\n", "def test_column_transformer_sparse_threshold_pandas():\n    pd = pytest.importorskip('pandas')\n    \n    X = pd.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6], 'col3': [7, 8, 9]})\n    ct = ColumnTransformer(\n        [('trans', DictVectorizer(), 'col1'), ('trans2', StandardScaler(), 'col2')],\n        sparse_threshold=0.8)\n\n    # Ensure sparse threshold works correctly for pandas DataFrames\n    X_trans = ct.fit_transform(X) \n    assert isinstance(X_trans, sparse.csr_matrix)\n    assert X_trans.getnnz(axis=1) == [2, 2, 2]\n\n\n\n", "def test_column_transformer_sparse_threshold():\n    X_array = np.array([[0, 1, 2],\n                        [0, 0, 3],\n                        [4, 0, 0]]).astype('float')\n\n    ct = ColumnTransformer([('trans', StandardScaler(), [0])],\n                           sparse_threshold=0.9)\n\n    X_trans = ct.fit_transform(X_array)\n    assert sparse.issparse(X_trans)\n    assert X_trans.shape[1] == 3  # All columns remain\n\n    ct = ColumnTransformer([('trans', StandardScaler(), [0])],\n                           sparse_threshold=0)\n\n    X_trans = ct.fit_transform(X_array)\n    assert not sparse.issparse(X_trans)\n    assert X_trans.shape[1] == 3  # All columns remain\n", "def test_column_transformer_sparse_threshold_rounding():\n    X_array = np.array([[0, 1, 2],\n                        [2, 4, 6],\n                        [8, 6, 4]]).astype('float').T\n    ct = ColumnTransformer([('trans1', StandardScaler(), [0])],\n                           sparse_threshold=0.99)\n\n    ct.fit(X_array)\n    assert not sparse.issparse(ct.transformers_[0][1].fit_transform(X_array))\n    ct = ColumnTransformer([('trans1', StandardScaler(), [0])],\n                           sparse_threshold=0.01)\n\n    ct.fit(X_array)\n    assert sparse.issparse(ct.transformers_[0][1].fit_transform(X_array)) \n"], "sample_297": ["    def test_ticket_25032(self):\n        class TestModel(models.Model):\n            a = models.IntegerField()\n            b = models.CharField(max_length=255)\n            c = models.ForeignKey(\n                'TestModel', on_delete=models.CASCADE, related_name='many_to_self'\n            )\n        try:\n            models.Model.objects.filter(a=1, c__in=TestModel.objects.none())\n            self.fail('Expected ValueError')\n        except ValueError:\n            pass\n\n\n\n", "compilation error", "    def test_ticket_24605_parent_relationship(self):\n        parent1 = ParentObject.objects.create()\n        child1 = ChildObject.objects.create(parent=parent1)\n        parent2 = ParentObject.objects.create()\n        child2 = ChildObject.objects.create(parent=parent2)\n        \n        qs = ParentObject.objects.filter(childobject__isnull=True).order_by('pk')\n        self.assertSequenceEqual(qs, [parent2])\n\n        qs = ParentObject.objects.exclude(childobject__isnull=False).order_by('pk')\n        self.assertSequenceEqual(qs, [parent1]) \n\n\n\n", "    def test_related_lookup_values_with_nulls(self):\n        oa1 = ObjectA.objects.create(name=\"oa1\")\n        oa2 = ObjectA.objects.create(name=\"oa2\")\n        ob1 = ObjectB.objects.create(objecta=oa1, num=1)\n        ob2 = ObjectB.objects.create(objecta=oa2, num=2)\n\n        # Test cases with null values.\n        qs1 = ObjectB.objects.filter(objecta__name='oa1').values_list('num')\n        self.assertSequenceEqual(qs1, [(1,)])\n\n        qs2 = ObjectB.objects.filter(objecta__name='oa3').values_list('num')\n        self.assertEqual(qs2, [])\n\n        qs3 = ObjectB.objects.filter(objecta__in=[oa1, oa2]).values_list('num')\n        self.assertSequenceEqual(qs3, [(1, 2)])\n", "    def test_ticket_24605_with_complex_subquery(self):\n        \"\"\"\n        Test quoting of subquery table names in a more complex scenario.\n        \"\"\"\n        i1 = Individual.objects.create(alive=True)\n        i2 = Individual.objects.create(alive=False)\n        RelatedIndividual.objects.create(related=i1)\n        RelatedIndividual.objects.create(related=i2)\n\n        # More complex subquery involving multiple relations and filtering.\n        qs = Individual.objects.filter(\n            Q(alive=True) &\n            Q(related_individual__pk__in=RelatedIndividual.objects.filter(related__alive=False).values_list('pk'))\n        )\n        self.assertSequenceEqual(qs, [i1])\n\n\n\n"], "sample_937": ["    def test_unparse_keywords():\n        source = \"func(arg1=1, arg2='two', arg3=3.14)\"\n        expected = \"func(arg1=1, arg2='two', arg3=3.14)\"\n        module = ast.parse(source)\n        assert ast.unparse(module.body[0].value) == expected\n", "    def test_unparse_starred_arguments():\n        source = \"\"\"\n            pass\n        \"\"\"\n        module = ast.parse(source)\n        expected = \"\"\"func(a, *, b, c)\"\"\"\n        assert ast.unparse(module.body[0].value.args) == expected\n", "    def test_unparse_with_keyword_arguments():\n        source = \"def my_func(a, b=1, *c, d=2):\\n    pass\"\n        module = ast.parse(source)\n        expected = \"def my_func(a, b=1, *c, d=2):\\n    pass\"\n        assert ast.unparse(module.body[0]) == expected\n        \n", "    def test_unparse_starred_expression():\n        source = \"(*args, **kwargs)\"\n        expected = \"(*args, **kwargs)\"\n        module = ast.parse(source)\n        assert ast.unparse(module.body[0].value) == expected \n", "    def test_unparse_starred_args():\n        source = \"\"\""], "sample_488": ["    def test_non_existent_namespace_package(self):\n        \"\"\"\n        A non-existent namespace package should trigger an error\n        when tried to be registered.\n        \"\"\"\n        with self.assertRaises(ImportError):\n            with self.settings(INSTALLED_APPS=[\"non_existent_nsapp\"]):\n                pass \n\n\n", "    def test_namespace_package_with_empty_path(self):\n        \"\"\"\n        A namespace package with an empty path attribute should still be\n        correctly loaded as an app. This covers the case where the\n        namespace package has no explicit __path__ attribute.\n        \"\"\"\n        for location in [self.base_location, self.other_location]:\n            with extend_sys_path(location):\n                with self.settings(INSTALLED_APPS=[\"nsapp\"]):\n                    app_config = apps.get_app_config(\"nsapp\")\n                    self.assertEqual(app_config.path, '')\n\n\n\n", "    def test_no_namespace_package(self):\n        \"\"\"\n        A regular Python package that's not a namespace package\n        should load correctly as an app.\n        \"\"\"\n        with extend_sys_path(os.path.join(HERE, \"regular_package\")):\n            with self.settings(INSTALLED_APPS=[\"regular_package\"]):\n                app_config = apps.get_app_config(\"regular_package\")\n                self.assertEqual(app_config.path, os.path.join(HERE, \"regular_package\"))\n", "    def test_app_with_namespace_package_as_dependency(self):\n        \"\"\"\n        An app with a namespace package as a dependency should be loaded correctly.\n        \"\"\"\n        with extend_sys_path([self.app_path]):\n            with self.settings(INSTALLED_APPS=[\"apps.apps.ModelPKAppsConfig\"]):\n                app_config = apps.get_app_config(\"modelpkappsconfig\")\n                self.assertEqual(app_config.path, os.path.join(HERE, \"apps\"))\n", "    def test_namespace_package_with_init_file_as_app(self):\n        \"\"\"\n        Tests namespace package with __init__.py in subdirectories as app.\n        \"\"\"\n        with extend_sys_path(os.path.join(self.base_location, \"sub\")):\n            with self.settings(INSTALLED_APPS=[\"nsapp\"]):\n                app_config = apps.get_app_config(\"nsapp\")\n                self.assertEqual(app_config.path, os.path.join(self.base_location, \"sub\", \"nsapp\"))\n"], "sample_769": ["def test_balanced_accuracy_score_with_multilabel():\n    from sklearn.metrics import balanced_accuracy_score\n    y_true = np.array([[0, 1, 1], [1, 0, 0], [0, 0, 1]])\n    y_pred = np.array([[0, 1, 1], [1, 0, 0], [0, 1, 1]])\n    assert_equal(balanced_accuracy_score(y_true, y_pred), 0.6666666666666666)\n", "def test_balanced_accuracy_score_with_multiclass_labels():\n    y_true = np.array([2, 0, 2, 1, 0, 2, 1, 0, 2])\n    y_pred = np.array([2, 0, 1, 1, 0, 2, 2, 0, 2])\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred), 0.7778)\n    \n    y_true = np.array([0, 1, 2, 0, 1, 2, 0, 1, 2])\n    y_pred = np.array([0, 1, 2, 0, 1, 2, 0, 1, 2])\n    assert_almost_equal(balanced_accuracy_score(y_true, y_pred), 1.0)\n\n\n", "def test_balanced_accuracy_score_empty_classes():\n    assert_equal(balanced_accuracy_score(y_true=None, y_pred=None), 0.0)\n\n    assert_equal(balanced_accuracy_score(y_true=[], y_pred=[]), 0.0)\n\n    y_true = [0, 1, 2]\n    y_pred = [0, 1, 2]\n\n    balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n    assert_almost_equal(balanced_accuracy, 1.0)\n", "compilation error", "def test_balanced_accuracy_score_no_samples():\n    # Test \n    assert_equal(balanced_accuracy_score(y_true=np.array([]), y_pred=np.array([])), 0.0)\n    assert_equal(balanced_accuracy_score(y_true=np.array([1, 2]), y_pred=np.array([1, 2])), 1.0)\n"], "sample_17": ["    def test_dispatch_by_dtype_bool_array(self):\n        # This checks if dispatching works correctly for bool arrays.\n        bool_array = np.array([True, False, True]) * u.one\n        with pytest.raises(ValueError, match=\"Quantity\"):\n            FUNCTION_HELPERS[\"test_dispatch\"](bool_array)\n        q = u.Quantity(bool_array, u.one)\n        assert FUNCTION_HELPERS[\"test_dispatch\"](q) is not None\n", "    def test_transcendental_functions(self, func):\n        x = np.array([1, 2, 3]) * u.m\n        y = func(x)\n        expected = func(x.value) << x.unit\n        assert_array_equal(y, expected)\n\n\n", "    def test_deprecated_functions(self):\n        for func_name in deprecated_functions:\n            with pytest.deprecated_call(match=r\"{func_name} is deprecated\".format(func_name=func_name)):\n                getattr(np, func_name)(np.array([1, 2, 3]))\n", "    def test_function_helper_types(self):\n        for fname in FUNCTION_HELPERS:\n            func = FUNCTION_HELPERS[fname]\n            assert callable(func)\n            assert isinstance(func, type)\n\n\n\n", "    def test_array_ufuncs_with_units(self):\n        # Test that array ufuncs work correctly with quantities.\n        a = np.array([1, 2, 3]) * u.m\n        b = np.array([4, 5, 6]) * u.m\n        assert_array_equal(np.add(a, b), (5, 7, 9) * u.m)\n        assert_array_equal(np.subtract(a, b), (-3, -3, -3) * u.m)\n        assert_array_equal(np.multiply(a, b), (4, 10, 18) * u.m**2)\n        assert_array_equal(np.divide(a, b), (0.25, 0.4, 0.5) * u.one / u.m)\n        assert_array_equal(np.power(a, 2), (1, 4, 9) * u.m**2)\n        assert_array_equal(np.sqrt(a), np.sqrt(a.value) * u.m**0.5)\n\n        # Test with non-numeric array\n        c = np.array([1, 2, 3]) * u.one\n        with pytest.raises(u.UnitsError):\n            np.add(a, c)\n\n"], "sample_974": ["compilation error", "def test_ccode_if_else():\n    expr = Piecewise((x, x > 0), (y, True))\n    assert ccode(expr) == (\n            \"((x > 0) ? (\\n\"\n            \"   x\\n\"\n            \")\\n\"\n            \": (\\n\"\n            \"   y\\n\"\n            \"))\")\n    assert ccode(expr, assign_to='c') == (\n            \"if (x > 0) {\\n\"\n            \"   c = x;\\n\"\n            \"}\\n\"\n            \"else {\\n\"\n            \"   c = y;\\n\"\n            \"}\")\n    expr = Piecewise((x**2, x > 1), (x, x < 1), (0, True))\n    assert ccode(expr) == (\n            \"((x > 1) ? (\\n\"\n            \"   pow(x, 2)\\n\"\n            \")\\n\"\n            \": ((x < 1) ? (\\n\"\n            \"   x\\n\"\n            \")\\n\"\n            \": (\\n\"\n            \"   0\\n\"\n            \")))\")\n    assert ccode(expr, assign_to='c') == (\n            \"if (x > 1) {\\n\"\n            \"   c = pow(x, 2);\\n\"\n            \"}\\n\"\n            \"else if (x < 1) {\\n\"\n            \"   c = x;\\n\"\n            \"}\\n\"\n            \"else {\\n\"\n            \"   c = 0;\\n\"\n            \"}\")\n\n", "def test_ccode_For_loops():\n    from sympy.tensor import IndexedBase, Idx\n    from sympy import symbols\n    n, m = symbols('n m', integer=True)\n    A = IndexedBase('A')\n    i = Idx('i', n)\n    j = Idx('j', m)\n\n    s = (\n        'for (int i=0; i<n; i++){\\n'\n        '   for (int j=0; j<m; j++){\\n'\n        '      A[%s] = x[i] * y[j];\\n' % (i*m + j) +\\\n        '   }\\n'\n        '}'\n    )\n    c = ccode(For(i(0, n), A[i*m + j] = x[i]*y[j] for j in range(m)),\n              assign_to=A)\n    assert c == s\n", "compilation error", "def test_ccode_For():\n    from sympy import symbols, For, IndexedBase\n\n    n = symbols('n', integer=True)\n    x = IndexedBase('x')\n    y = IndexedBase('y')\n    i = Idx('i', n)\n    c = ccode(For(i, 0, n, x[i] = y[i]**2), assign_to=x[i])\n    assert c == 'for (int i=0; i<n; i++){\\n' \\\n                '   x[i] = pow(y[i], 2);\\n' \\\n                '}'\n\n"], "sample_885": ["compilation error", "def test_validate_params_with_default_value_and_constraints():\n    \"\"\"Check that default value is used when the parameter is not provided.\"\"\"\n    @validate_params({\"a\": [int, 1], \"b\": [Real, 0.0], \"c\": [str, \"default_value\"]})\n        return a, b, c\n\n    with pytest.raises(InvalidParameterError, match=\"The 'a' parameter must\"):\n        f(c=\"wrong\")\n\n    assert f() == (1, 0.0, \"default_value\")\n\n    assert f(a=2) == (2, 0.0, \"default_value\")\n    assert f(b=3.14) == (1, 3.14, \"default_value\")\n\n\n", "compilation error", "compilation error", "compilation error"], "sample_676": ["    def test_line_with_reprcrash_long(monkeypatch):\n        import _pytest.terminal\n        from wcwidth import wcswidth\n\n        mocked_verbose_word = \"FAILED\"\n\n        mocked_pos = \"some::nodeid\"\n\n        class config(object):\n            pass\n\n        class rep(object):\n                return mocked_verbose_word\n                return \"some traceback\"\n\n            return mocked_pos\n\n        monkeypatch.setattr(_pytest.terminal, \"_get_pos\", mock_get_pos)\n        monkeypatch.setattr(_pytest.terminal.Report, \"_get_traceback\", lambda x: \"some traceback\")\n\n        result = _format_line(rep, config, \"some::nodeid\")\n        assert result != \"some::nodeid\"\n        assert \"some traceback\" in result\n", "    def test_reprcrash_width(monkeypatch):\n        import _pytest.terminal\n        from wcwidth import wcswidth\n\n        mocked_verbose_word = \"FAILED\"\n\n        mocked_pos = \"some::nodeid\"\n\n            return mocked_pos\n\n        monkeypatch.setattr(_pytest.terminal, \"_get_pos\", mock_get_pos)\n\n        class config(object):\n            pass\n        class rep(object):\n                return mocked_verbose_word\n                return wcswidth(\"bar\")  # Some arbitrary width\n\n        rep = rep()\n        result = _folded_skips([rep])\n\n        assert len(result) == 1\n\n", "    def test_color_on_tty(testdir, monkeypatch):\n        import os\n        monkeypatch.setattr(\"sys.stdout\", io.BytesIO())\n        monkeypatch.setattr(\"os.isatty\", lambda x: True)\n        result = testdir.runpytest()\n        result.stdout.seek(0)\n        assert b\"FAILED\" in result.stdout.read()\n\n        monkeypatch.setattr(\"os.isatty\", lambda x: False)\n        result = testdir.runpytest()\n        result.stdout.seek(0)\n        assert b\"FAILED\" not in result.stdout.read()\n", "    def test_reprcrash_short_tb_style(monkeypatch):\n        from _pytest.terminal import get_terminal_writer\n        from _pytest.outcomes import _reprcrash\n        get_terminal_writer = monkeypatch.setattr(\n            _pytest.terminal, \"get_terminal_writer\", lambda: (\"stdout\", None)\n        )\n        monkeypatch.setattr(_pytest.outcomes, \"_reprcrash\", _reprcrash)\n        report = _pytest.collect.CollectReport(\n            \"test_one.py\", None, \"passed\", None\n        )\n        with monkeypatch.context() as m:\n            m.setattr(report, \"traceback\", \"Test Failure\")\n            output = _format_result_summary(report, config=config())\n        assert output == \"  Test Failure\\n\"  \n", "    def test_line_with_reprcrash_colored(monkeypatch):\n        import _pytest.terminal\n        from wcwidth import wcswidth\n\n        mocked_verbose_word = \"FAILED\"\n        mocked_pos = \"some::nodeid\"\n\n            return mocked_pos\n\n        monkeypatch.setattr(_pytest.terminal, \"_get_pos\", mock_get_pos)\n\n        class config(object):\n            pass\n\n        class rep(object):\n                return mocked_verbose_word\n                return \"red\"\n        \n        result = _format_test_line(config(), rep(),  \"crash\", True)\n        assert result.startswith(\"\\033[31m\")\n\n"], "sample_987": ["def test_issue_10798():\n    from sympy import log, Abs, S, N\n    from sympy.functions import factorial\n    for x in [1, 2, 3, 4, 5]:\n        assert log(factorial(x), (e)).evalf() == S(log(x)).evalf()\n    for x in [1, 2, 3, 4, 5]:\n        assert Abs(log(factorial(x), (e)) - S(log(x))).evalf() < 1e-15\n    assert log(factorial(0), (e)).evalf() == 0\n    assert N(log(log(factorial(5))), 30) == 5.399074525429698 \n\n\n", "def test_issue_10649():\n    from sympy import exp, pi, S\n    assert exp(S('1.25')).n(50) == 3.490349674323556\n\n\n", "def test_issue_10571():\n    from sympy import log, Symbol\n\n    x = Symbol('x')\n    assert log(exp(x) - 1, exp(x)).evalf(chop=True) == x - 1\n    assert log(1 + x, 1 + x).evalf(chop=True) == 1\n", "def test_issue_11635():\n    from sympy.abc import x\n    assert (x**2).evalf(subs={x: 3}, n=10) == 9.0000000000\n    assert (x**2).evalf(subs={x: 3}, n=1) == 9.0\n\n\n\n", "def test_issue_10779():\n    f = Function('f')\n    f = implemented_function(f, lambda x: x**2 - 1)\n    assert f(x).evalf() == x**2 - 1\n"], "sample_102": ["    def test_filter_after_union(self):\n        qs1 = Number.objects.filter(num__lte=1)\n        qs2 = Number.objects.filter(num__gte=2, num__lte=3)\n        union_qs = qs1.union(qs2)\n        # Should filter out nums greater than 3, regardless of original source\n        self.assertNumbersEqual(union_qs.filter(num__lte=2), [0, 1, 2])\n", "    def test_ordering_with_none_values(self):\n        from django.db import connection\n\n        ReservedName.objects.create(name='rn1', order=1)\n        ReservedName.objects.create(name='rn2', order=None)\n        qs1 = ReservedName.objects.filter(order__isnull=True)\n        qs2 = ReservedName.objects.filter(order__gt=1)\n        self.assertEqual(list(qs1.union(qs2).values_list('order', flat=True)), [1, None])\n        with connection.cursor() as cursor:\n            cursor.execute(\"SELECT order FROM django_reservedname_reservedname ORDER BY order;\")\n            result = list(cursor.fetchall())\n        self.assertEqual(result, [(1,), (None,),])\n\n\n\n", "    def test_order_by_complex_values(self):\n        ReservedName.objects.bulk_create([\n            ReservedName(name='a', order=1),\n            ReservedName(name='b', order=1),\n            ReservedName(name='c', order=2),\n            ReservedName(name='d', order=3),\n        ])\n        qs = ReservedName.objects.all()\n        for ordering in [\n            'order',\n            'order',\n            'name',\n            'name',\n            'order',\n            'name',\n            F('order') * 2,\n            F('order') * 2,\n        ]:\n            with self.subTest(ordering=ordering):\n                ordered_qs = qs.order_by(ordering)\n                expected_result = sorted(qs, key=lambda x: getattr(x, ordering)) if ordering != F('order') * 2 else sorted(\n                    qs, key=lambda x: getattr(x, ordering) * 2\n                )\n                self.assertCountEqual(list(ordered_qs), [r for r in expected_result])\n", "    def test_distinct_union_order_by_pk(self):\n        qs1 = Number.objects.filter(num__lte=1).distinct('num')\n        qs2 = Number.objects.filter(num__gte=2, num__lte=3).distinct('num')\n        self.assertNumbersEqual(qs1.union(qs2).order_by('pk'), [0, 1, 2, 3])\n    ", "    def test_union_with_duplicates(self):\n        qs1 = Number.objects.all()\n        qs2 = Number.objects.all()\n        duplicated_qs = qs1.union(qs2)\n        self.assertEqual(len(list(duplicated_qs)), 10)  \n\n"], "sample_971": ["def test_prefixed_warnings(app, status, warning):\n    with prefixed_warnings(prefix):\n        logger = logging.getLogger(__name__)\n        logger.warning('message')\n    assert expected in warning.getvalue()\n", "def test_prefixed_warnings(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    with prefixed_warnings(\"prefix: \"):\n        logger.warning(\"Warning message!\")\n    assert \"prefix: WARNING: Warning message!\" in strip_escseq(warning.getvalue())\n", " def test_prefixed_warnings(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n    \n    with prefixed_warnings(\"TEST:\"):\n        logger.warning('message')\n        \n    assert \"TEST: WARNING: message\" in warning.getvalue()", "def test_colorize(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n    handler = logging.StreamHandler(logging.Formatter(ColorizeFormatter))\n\n    # Ensure colorization is enabled\n    assert handler.formatter.uses_colors()\n\n    with colorize('red', 'test'):\n        logger.info('message')\n    assert 'message' in status.getvalue()\n    assert colorize('red', 'message') in status.getvalue()\n\n    with colorize('green', 'test'):\n        logger.warning('message')\n    assert 'WARNING: message' in warning.getvalue()\n    assert colorize('green', 'WARNING: message') in warning.getvalue()\n\n    # Test default colors\n    logger.debug('message')\n    assert colorize('darkgray', 'message') in status.getvalue()\n\n    logger.warning('message')\n    assert colorize('red', 'WARNING: message') in warning.getvalue()\n", "def test_prefix_warning_log(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    with prefixed_warnings(\"PREFIX: \"):\n        logger.warning('message')\n\n    assert 'PREFIX: WARNING: message' in warning.getvalue()\n\n\n\n"], "sample_729": ["def test_multi_task_enet_path_with_multi_output_y():\n    X, y, _, _ = build_dataset(n_samples=50, n_features=20, n_targets=2)\n    \n    # Test that multi-task ElasticNet path works\n    # when y is provided as a 2D array with multiple outputs\n\n    for alpha in [0.1, 0.5, 1.0]:\n        for l1_ratio in [0.1, 0.5, 0.9]: \n            for path in [enet_path_multi_task]: \n                coefs = path(X, y, alpha=alpha, l1_ratio=l1_ratio)[1]\n                assert_array_almost_equal(coefs.shape, (2, 20))\n                assert_almost_equal(coefs[0, :].mean(), coefs[1, :].mean())\n\n\n", "def test_enet_sparse_warm_start():\n    X, y, _, _ = build_dataset(n_samples=50, n_features=50)\n    X_sparse = sparse.csr_matrix(X)\n\n    # Test warm_start with sparse data\n    enet = ElasticNet(alpha=0.1, max_iter=100, tol=1e-8,\n                      selection='cyclic', warm_start=True)\n    enet.fit(X_sparse, y)\n    enet.warm_start = True\n    enet.fit(X_sparse, y)\n\n    # Test warm_start with different types of sparse data\n    X_csc = sparse.csc_matrix(X)\n    enet = ElasticNet(alpha=0.1, max_iter=100, tol=1e-8,\n                      selection='cyclic', warm_start=True)\n    enet.fit(X_csc, y)\n    enet.warm_start = True\n    enet.fit(X_csc, y)\n\n\n", "def test_enet_multioutput_sparse_input():\n    X, y, _, _ = build_dataset(n_samples=50, n_features=20)\n    y = np.hstack((y[:, np.newaxis], y[:, np.newaxis]))\n\n    X_sparse = sparse.csr_matrix(X)\n\n    # Test MultiTaskElasticNet with sparse input\n    clf = MultiTaskElasticNet(alpha=0.1, tol=1e-8)\n    clf.fit(X_sparse, y)\n    coef = clf.coef_\n    assert_almost_equal(coef.shape, (2, X.shape[1]))\n\n    # Test that the sparse input is correctly handled\n    clf = MultiTaskElasticNet(alpha=0.1, tol=1e-8)\n    clf.fit(X_sparse, y)\n    assert_true(isinstance(clf.coef_, sparse.csr_matrix))\n    assert_almost_equal(coef.shape, clf.coef_.shape)\n\n    # Test that dense and sparse input produce the same results for multioutput\n    clf_dense = MultiTaskElasticNet(alpha=0.1, tol=1e-8)\n    clf_dense.fit(X, y)\n    assert_array_almost_equal(clf.coef_, clf_dense.coef_)\n", "def test_enet_multioutput_path():\n    # Test multi-target path\n    X, y, _, _ = build_dataset(n_samples=50, n_features=20, n_targets=3)\n    \n    for path in [enet_path, lasso_path]:\n        coefs, _ = path(X, y, n_alphas=10, fit_intercept=True)\n        assert_equal(coefs.shape, (10, X.shape[1], y.shape[1]))\n\n", "compilation error"], "sample_775": ["def test_changed_only_with_nesting():\n    # Make sure changed_only works correctly with nested objects\n    set_config(print_changed_only=True)\n\n    class NestedEstimator:\n            self.param1 = param1\n            self.param2 = param2\n\n    pipe = Pipeline([\n        ('nested', NestedEstimator(param1=1, param2=2)),\n        ('outer', LogisticRegression())\n    ])\n    pipe.set_params(nested__param1=3)  \n    \n    expected = \"\"\"", "def test_changed_only_with_n_features_changed():\n    set_config(print_changed_only=True)\n    from sklearn.decomposition import PCA\n\n    pca = PCA(n_components=10)\n    pca.fit(np.random.rand(100, 20))\n    \n    # Modify n_components attribute\n    pca.n_components = 5 \n    expected = \"\"\"PCA(n_components=5, copy=True, iterated_power='auto',\n                 svd_solver='auto', tol=0.0, whiten=False)\"\"\"\n    \n    assert pca.__repr__() == expected\n\n\n\n", "compilation error", "def test_handling_nested_list_like_objects():\n    from sklearn.utils.validation import check_is_fitted\n\n    class MyEstimator:\n            self.coefficients_ = [\n                [[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]\n            ]\n            return self\n\n    estimator = MyEstimator()\n    estimator.fit(X=[1, 2, 3], y=[4, 5, 6])\n\n    expected = \"\"\"", "        def __init__(self, a):\n            self.a = a\n"], "sample_54": ["    def test_file_content_length_is_set_for_filelike_object(self):\n        with tempfile.NamedTemporaryFile() as tmp:\n            tmp.write(b'content')\n            tmp.seek(0)\n            response = FileResponse(tmp)\n            self.assertEqual(response['Content-Length'], str(len(tmp.read())))\n", "    def test_file_with_closed_file(self):\n        with tempfile.NamedTemporaryFile('w+b') as tmp:\n            tmp.write(b'some content')\n            tmp.close()\n            response = FileResponse(tmp)\n            self.assertEqual(list(response), [b'some content'])\n            response.close()\n", "    def test_file_response_closed(self):\n        with tempfile.NamedTemporaryFile() as tmpfile:\n            response = FileResponse(tmpfile)\n            response.close()\n            with self.assertRaises(IOError) as cm:\n                next(iter(response))\n            self.assertEqual(cm.exception.args[0], \"File is closed\")\n", "    def test_file_not_found(self):\n        with self.assertRaises(FileNotFoundError):\n            FileResponse(open('nonexistent_file.txt', 'rb'))\n", "    def test_filename_with_spaces(self):\n        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt') as tmp:\n            tmp.write(\"This is a file with spaces in its name.\")\n            tmp.flush()\n            response = FileResponse(tmp)\n            self.assertEqual(response['Content-Disposition'], 'inline; filename=\"This is a file with spaces in its name.txt\"')\n            response.close()\n"], "sample_1085": ["def test_issue_10598():\n    assert Float(1.5).asin() == Float(0.5235987755982988)\n", "def test_issue_11295():\n    assert Float('1.23e-100').n(100) == '1.23e-100'\n    assert Float('1.23e+100').n(100) == '1.23e+100'\n", "def test_issue_11881():\n    assert Float(0.5, precision=5).as_thick_mpf() == mpf('0.50000')\n    assert Float(0.5, precision=10).as_thick_mpf() == mpf('0.5000000000')\n    assert Float(0.123456789, precision=5).as_thick_mpf() == mpf('0.12346')\n", "def test_issue_11707():\n    assert Float('1.1') != Rational(11, 10)\n    assert Rational(11, 10) != Float('1.1')\n", "def test_issue_11489():\n    assert Float(1.0e-99).as_mpf() == mpf('1E-99')\n    assert Float(1.0e99).as_mpf() == mpf('1E+99')\n\n\n"], "sample_715": ["def test_cross_val_predict_with_callable_scoring():\n    iris = load_iris()\n    X, y = iris.data, iris.target\n\n    class MockCallableScoring:\n            self.fun = fun\n\n            return self.fun(est, X, y)\n\n        return sklearn.metrics.accuracy_score(y, est.predict(X))\n\n    estimator = RandomForestClassifier()\n    scoring = MockCallableScoring(mean_accuracy)\n\n    predictions = cross_val_predict(estimator, X, y, scoring=scoring)\n    assert_not_none(predictions)\n\n", "def test_cross_val_score_multioutput_scoring():\n    X, y = make_classification(n_samples=10, n_features=2, n_informative=2,\n                               n_redundant=0, n_clusters_per_class=1,\n                               random_state=0)\n    y = np.array(y).T\n    # Test with multi output scoring\n    clf = OneVsRestClassifier(LogisticRegression())\n    scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n    assert_array_almost_equal(scores.mean(), 1.0)\n\n\n", "def test_cross_val_score_sparse_feature_weights():\n    X, y = make_classification(n_samples=10, n_features=5, random_state=0)\n    X_sparse = csr_matrix(X)\n    y = np.array(y, dtype=np.int32)\n\n    estimator = LogisticRegression()\n\n    # Test with default setting\n    scores_dense = cross_val_score(estimator, X, y, cv=5)\n    scores_sparse = cross_val_score(estimator, X_sparse, y, cv=5)\n    assert_array_almost_equal(scores_dense, scores_sparse)\n\n    # Test with feature weights\n    estimator.fit(X, y)\n    feature_weights = estimator.coef_[0]\n    X_sparse_weighted = X_sparse * feature_weights\n\n    scores_sparse_weighted = cross_val_score(estimator, X_sparse_weighted, y, cv=5)\n    assert_array_almost_equal(scores_sparse_weighted, scores_dense) \n\n\n\n", "def test_cross_val_predict_multioutput():\n    X, y = make_multilabel_classification(n_classes=3, n_labels=2,\n                allow_unlabeled=False,\n                return_indicator=True,\n                random_state=0)\n    X_sparse = csr_matrix(X)\n    y_sparse = csr_matrix(y)\n    classif = OneVsRestClassifier(LogisticRegression())\n    preds = cross_val_predict(classif, X, y, cv=3)\n    preds_sparse = cross_val_predict(classif, X_sparse, y_sparse, cv=3)\n    preds_sparse = preds_sparse.toarray()\n    assert_array_almost_equal(preds_sparse, preds)\n", "def test_permutation_test_score_n_jobs():\n    iris = load_iris()\n    X, y = iris.data, iris.target\n    clf = MockClassifier()\n    \n    # Test permutation_test_score with n_jobs\n    for n_jobs in [-1, 1, 10]:\n        score = permutation_test_score(clf, X, y, n_jobs=n_jobs)\n        assert_true(np.isfinite(score)) \n"], "sample_724": ["def test_imputation_axis_errors():\n    X = np.random.randn(10, 2)\n    X[::2] = np.nan\n\n    imputer = Imputer(missing_values=\"NaN\", strategy=\"median\", axis=2)\n\n    assert_raises(ValueError, imputer.fit, X)\n\n    imputer = Imputer(missing_values=\"NaN\", strategy=\"median\", axis=-1)\n    assert_raises(ValueError, imputer.fit, X)\n", "def test_imputation_sparse_data_with_missing_values():\n    # Test imputation on sparse data where missing values are not zeros.\n    rng = np.random.RandomState(1)\n\n    # Create a sparse matrix with some missing values\n    n_samples = 100\n    n_features = 50\n    data = rng.randn(n_samples, n_features)\n    data[::2, :] = np.nan\n    sparse_matrix = sparse.csr_matrix(data)\n\n    # Test different strategies\n    for strategy in ['mean', 'median', 'most_frequent']:\n\n        imputer = Imputer(missing_values='NaN', strategy=strategy, axis=0)\n        imputer.fit(sparse_matrix)\n\n        # Check that the imputed values match the expected values\n        imputed_matrix = imputer.transform(sparse_matrix)\n        expected_values = data.copy()\n        expected_values[np.isnan(expected_values)] = imputer.statistics_\n\n        assert_array_almost_equal(\n            imputed_matrix.toarray(), expected_values,\n            err_msg=\"Imputed values do not match expected values \"\n            \"(strategy = %s)\" % (strategy)\n        )\n\n", "def test_imputation_axis_sparse():\n    # Test imputation along axis=1 with sparse matrices.\n    X = sparse.csc_matrix([[1, 2, 3], [np.nan, 5, 6], [7, np.nan, 9]])\n\n    X_imputed_mean = sparse.csc_matrix(\n        [[1, 3.5, 6], [3, 5, 6], [7, 6.5, 9]]\n    )\n    statistics_mean = [np.nan, 3.5, 6]\n\n    imputer = Imputer(missing_values=0, strategy=\"mean\", axis=1)\n    X_trans = imputer.fit_transform(X)\n    assert_array_almost_equal(X_trans.toarray(), X_imputed_mean.toarray())\n    assert_array_equal(imputer.statistics_, statistics_mean)\n\n    X_imputed_median = sparse.csc_matrix(\n        [[1, 3, 6], [3, 5, 6], [7, 6, 9]]\n    )\n    statistics_median = [1, 5, 6]\n\n    imputer = Imputer(missing_values=0, strategy=\"median\", axis=1)\n    X_trans = imputer.fit_transform(X)\n    assert_array_almost_equal(X_trans.toarray(), X_imputed_median.toarray())\n    assert_array_equal(imputer.statistics_, statistics_median)\n\n\n\n", "def test_imputation_sparse_most_frequent():\n    # Test imputation using the most-frequent strategy with sparse data\n    X = sparse.csr_matrix([\n        [1, 2, np.nan],\n        [3, np.nan, 5],\n        [np.nan, 7, 8],\n    ])\n\n    X_imputed_most_frequent = sparse.csr_matrix([\n        [2, 2, 2],\n        [3, 5, 5],\n        [7, 7, 8],\n    ])\n\n    _check_statistics(X, X_imputed_most_frequent, \"most_frequent\",\n                      [2, 5, 8], np.nan)\n", "def test_imputation_axis_0_sparse():\n    # Test imputation along axis 0 with sparse data when missing_values != 0\n    rng = np.random.RandomState(0)\n\n    dim = 10\n    dec = 10\n    shape = (dim * dim, dim + dec)\n    values = np.arange(1, shape[0] + 1)\n    values[4::2] = - values[4::2]\n\n    X = np.empty(shape)\n    X_true = np.empty(shape)\n\n    for j in range(shape[1]):\n        nb_zeros = (j - dec + 1 > 0) * (j - dec + 1) * (j - dec + 1)\n        nb_missing_values = max(shape[0] + dec * dec - (j + dec) * (j + dec), 0)\n        nb_values = shape[0] - nb_zeros - nb_missing_values\n\n        p = np.repeat(\"NaN\" ,nb_missing_values)\n        z = np.zeros(nb_zeros)\n        v = values[rng.permutation(len(values))[:nb_values]]\n\n        X[:, j] = np.hstack((v, z, p))\n        X_true[:, j] = np.hstack((v,\n                                  np.repeat(\"NaN\" ,nb_missing_values + nb_zeros)))\n\n        # Shuffle them the same way\n        np.random.RandomState(j).shuffle(X[:, j])\n        np.random.RandomState(j).shuffle(X_true[:, j])\n\n    imputer = Imputer(missing_values=\"NaN\", strategy=\"mean\", axis=0)\n    imputer.fit(sparse.csc_matrix(X))\n\n    X_trans = imputer.transform(sparse.csc_matrix(X))\n    assert_array_almost_equal(X_trans.toarray(), X_true, err_msg=\"Fail to transform sparse matrix (axis=0)\")\n\n\n\n"], "sample_530": ["def test_offsetbox_transformation():\n    fig, ax = plt.subplots()\n    da = DrawingArea(100, 100)\n    anchored_box = AnchoredOffsetbox(loc='center', child=da,\n                                     bbox_to_anchor=(0.5, 0.5),\n                                     transform=ax.transAxes)\n    ax.add_artist(anchored_box)\n\n    # Test that transformation works correctly.\n    pts = [(-10, 0), (0, -10), (10, 0), (0, 10)]\n    transformed_pts = [anchored_box.transform.transform(pt) for pt in pts]\n    expected_transformed_pts = [\n        (0.0, 0.0), (0.0, 0.5), (0.5, 0.0), (0.0, 0.5)\n    ]\n    assert_allclose(transformed_pts, expected_transformed_pts, atol=1e-3)\n\n", "def test_offsetbox_bbox_alignment():\n    fig, ax = plt.subplots()\n\n    # Test left and right alignment\n    da = DrawingArea(50, 50)\n    ab = AnnotationBbox(da, (0.5, 0.5), xybox=(0.2, 0.2),\n                        xycoords='data',\n                        boxcoords=\"axes fraction\",\n                        box_alignment=(\"center\", \"left\"))\n    ax.add_artist(ab)\n\n    da = DrawingArea(50, 50)\n    ab = AnnotationBbox(da, (0.5, 0.5), xybox=(0.8, 0.2),\n                        xycoords='data',\n                        boxcoords=\"axes fraction\",\n                        box_alignment=(\"center\", \"right\"))\n    ax.add_artist(ab)\n\n    # Test top and bottom alignment\n    da = DrawingArea(50, 50)\n    ab = AnnotationBbox(da, (0.5, 0.5), xybox=(0.2, 0.8),\n                        xycoords='data',\n                        boxcoords=\"axes fraction\",\n                        box_alignment=(\"left\", \"center\"))\n    ax.add_artist(ab)\n\n    da = DrawingArea(50, 50)\n    ab = AnnotationBbox(da, (0.5, 0.5), xybox=(0.2, 0.2),\n                        xycoords='data',\n                        boxcoords=\"axes fraction\",\n                        box_alignment=(\"right\", \"center\"))\n    ax.add_artist(ab)\n\n    fig.canvas.draw()\n    renderer = fig.canvas.get_renderer()\n    for ab in ax.get_children():\n        if isinstance(ab, AnnotationBbox):\n            bb = ab.get_tightbbox(renderer)\n\n            # assert bbox alignment\n            if ab.box_alignment == (\"center\", \"left\"):\n                assert bb.extents[0] == 0.2\n            elif ab.box_alignment == (\"center\", \"right\"):\n                assert bb.extents[2] == 0.8\n            elif ab.box_alignment == (\"left\", \"center\"):\n                assert bb.extents[1", "compilation error", "def test_annotationbbox_transform():\n    fig, ax = plt.subplots()\n    trans = ax.transAxes\n    \n    # Create a simple offsetbox\n    bbox_box = AnchoredOffsetbox(loc=\"center left\", child=DrawingArea(50, 50))\n    bbox_box.set_bbox_to_anchor((0.5, 0.5))\n    ax.add_artist(bbox_box)\n\n\n    # Define a custom transformation to apply to the bbox\n        return xy * 2\n\n    bbox_box.set_transform(custom_transform)\n\n    fig.canvas.draw()\n\n\n\n\n", "    def test_drawingarea_clip():\n        # Test if the clip functionality of DrawingArea works as expected\n        fig, ax = plt.subplots()\n        da = DrawingArea(100, 100, clip=True)\n        bg = Rectangle((0, 0), 100, 100, facecolor='lightblue',\n                      edgecolor='None', linewidth=0)\n        line = Rectangle((20, 20), 60, 60, facecolor='none',\n                         edgecolor='red', linewidth=3)\n        da.add_artist(bg)\n        da.add_artist(line)\n\n        ax.add_artist(da)\n\n        # Ensure the line is clipped inside the DrawingArea\n        assert line.get_window_extent(fig.canvas.get_renderer()).extents[2] == 80\n        plt.show()\n\n"], "sample_786": ["def test_fit_transform_n_features(n_features):\n    X = np.random.RandomState(0).randn(10, n_features)\n    for n_bins in [2, 3, 4]:\n        kbd = KBinsDiscretizer(n_bins=n_bins, strategy='uniform', encode='ordinal')\n        kbd.fit(X)\n        assert kbd.n_bins_.shape[0] == n_features\n", "def test_transform_with_ignored_features(strategy):\n    X = np.random.RandomState(0).randn(10, 5)\n    ignored_features = [2, 3]\n    kbd = KBinsDiscretizer(n_bins=3, strategy=strategy, encode='ordinal')\n    Xt = kbd.fit_transform(X)\n    assert Xt.shape[1] == X.shape[1] - len(ignored_features)\n    X2 = kbd.inverse_transform(Xt)\n    assert X2.shape[1] == X.shape[1]\n", "def test_ignore_feature():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    kbd = KBinsDiscretizer(n_bins=2, strategy='uniform', encode='ordinal',\n                          ignore_features=[1])\n    Xt = kbd.fit_transform(X)\n    assert Xt.shape[1] == 1\n    assert_array_equal(Xt, np.array([[0, ], [0, ], [1, ]]))\n", "def test_ignore_features(strategy):\n    X = np.array([[1, 2, 3],\n                  [4, 5, 6],\n                  [7, 8, 9]])\n    kbd = KBinsDiscretizer(n_bins=3, strategy=strategy, encode='ordinal')\n    # Ignore the first feature\n    kbd.fit(X[:, 1:])\n    Xt = kbd.transform(X)\n    assert Xt.shape == (3, 2)\n    assert np.all(Xt[:, 0] == 0)\n\n\n\n", "def test_transform_ignored_features(strategy):\n    X = np.arange(12).reshape(3, 4)\n    kbd = KBinsDiscretizer(n_bins=3, strategy=strategy,\n                           encode='onehot-dense')\n    kbd.fit(X)\n    X_ignored = np.zeros((3, 2))\n    Xt = kbd.transform(np.hstack((X, X_ignored)))\n    assert Xt.shape[1] == 12  # 4 features transformed + 8 ignored\n"], "sample_127": ["    def test_ignore_conflicts_on_bulk_create_with_multiple_objects(self):\n        already_existing_data = [\n            TwoFields(f1=i, f2=i + 1) for i in range(0, 5)\n        ]\n        TwoFields.objects.bulk_create(already_existing_data)\n        additional_data = [\n            TwoFields(f1=i, f2=i + 1) for i in range(5, 10)\n        ]\n\n        with self.assertNumQueries(1):\n            TwoFields.objects.bulk_create(additional_data, ignore_conflicts=True)\n\n        self.assertEqual(TwoFields.objects.count(), 10)\n        for i in range(0, 10):\n            self.assertIn(\n                TwoFields(f1=i, f2=i + 1),\n                TwoFields.objects.all()\n            )\n", "    def test_ignore_conflicts(self):\n        # Test that bulk_create with ignore_conflicts=True raises ValueError\n        # when a conflict is encountered, but successfully creates the objects if\n        # there are no conflicts.\n        TwoFields.objects.bulk_create([\n            TwoFields(f1=1, f2=2),\n            TwoFields(f1=3, f2=4),\n        ])\n        with self.assertRaises(IntegrityError):\n            TwoFields.objects.bulk_create([\n                TwoFields(f1=1, f2=2),\n                TwoFields(f1=1, f2=2),\n            ], ignore_conflicts=True)\n        with self.assertRaises(IntegrityError):\n            TwoFields.objects.bulk_create([\n                TwoFields(f1=1, f2=2),\n                TwoFields(f1=3, f2=4),\n            ], ignore_conflicts=True)\n\n        # Test that ignore_conflicts=True works if there are no conflicts.\n        with self.assertNumQueries(2):\n            TwoFields.objects.bulk_create([\n                TwoFields(f1=5, f2=6),\n                TwoFields(f1=7, f2=8),\n            ], ignore_conflicts=True)\n        self.assertEqual(TwoFields.objects.count(), 4)\n\n\n\n", "    def test_ignore_conflicts_inserting_duplicates(self):\n        existing_data = [TwoFields(f1=0, f2=1)]\n        TwoFields.objects.bulk_create(existing_data)\n        new_data = [TwoFields(f1=0, f2=1)]\n\n        with self.assertNumQueries(1):\n            TwoFields.objects.bulk_create(new_data, ignore_conflicts=True)\n\n        self.assertEqual(TwoFields.objects.count(), 1) \n", "    def test_ignore_conflicts(self):\n        with self.assertNumQueries(1):\n            TwoFields.objects.bulk_create(self.data[:3], ignore_conflicts=True)\n        self.assertEqual(TwoFields.objects.count(), 3)\n        with self.assertRaises(IntegrityError):\n            TwoFields.objects.bulk_create(self.data[:3], ignore_conflicts=True)\n        self.assertEqual(TwoFields.objects.count(), 3)\n", "    def test_ignore_conflicts(self):\n        # This test checks if bulk create ignores the conflicts if the \n        # ignore_conflicts parameter is set to True.\n        objs = [TwoFields(id=i, f1=i, f2=i) for i in range(10)]\n        TwoFields.objects.bulk_create(objs, ignore_conflicts=True)\n        self.assertEqual(TwoFields.objects.count(), 10)\n\n"], "sample_330": ["    def test_ref_to_non_existent_raises_error(self):\n        obj_id = 12345\n        with self.assertRaises(Object.DoesNotExist):\n            ObjectReference.objects.get(obj_id=obj_id)\n", "    def test_cannot_reference_null(self):\n        with self.assertRaises(ValueError):\n            ObjectReference.objects.create(obj=None)\n", "    def test_fk_field_with_null_allowed(self):\n        obj = ObjectNullable.objects.create(name=\"Test\")\n        ref = ObjectReferenceNullable.objects.create(obj=obj) \n        self.assertEqual(ref.obj, obj)\n\n        ref_new = ObjectReferenceNullable.objects.get(obj_id=obj.id)\n        self.assertEqual(ref_new.obj, obj)\n\n        # Test that we can create a ObjectReferenceNullable without an ObjectNullable.\n        ref2 = ObjectReferenceNullable.objects.create(obj=None)\n        self.assertEqual(ref2.obj, None)\n\n", "    def test_cascade_on_delete_all(self):\n        obj = Object.objects.create()\n        ref1 = ObjectReference.objects.create(obj=obj)\n        ref2 = ObjectReference.objects.create(obj=obj)\n\n        self.assertEqual(ObjectReference.objects.count(), 2)\n        self.assertEqual(Object.objects.count(), 1)\n\n        obj.delete()\n\n        self.assertEqual(ObjectReference.objects.count(), 0)\n        with self.assertRaises(Object.DoesNotExist):\n            Object.objects.get(pk=obj.pk) \n", "    def test_delete_cascades_deletion(self):\n        obj = Object.objects.create()\n        ref = ObjectReference.objects.create(obj=obj)\n        ref_count = ObjectReference.objects.count()\n\n        obj.delete()\n        self.assertEqual(ObjectReference.objects.count(), ref_count - 1)\n"], "sample_1047": ["def test_issue_10748():\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    assert (sqrt(x**2)).is_positive is True\n    assert (sqrt(x**2)).is_nonnegative is True\n    assert (sqrt((x + y)**2)).is_positive is None\n    assert (sqrt((x - y)**2)).is_positive is True\n    assert (sqrt((x - y)**2)).is_nonnegative is True\n\n    z = Symbol('z', complex=True)\n    assert (abs(z)).is_positive is True\n    assert (abs(z)).is_nonnegative is True\n    assert (abs(-z)).is_positive is True\n\n", "def test_issue_10434():\n    x = Symbol('x', real=True)\n    assert (x - I*x).is_real is True \n    assert (I*x - x).is_real is False\n\n\n\n\n", "def test_issue_10302_cont():\n    i = Symbol('i', imaginary=True)\n    assert (1 / (4 + i)).is_imaginary\n    assert (1 / (4 + 2 * i)).is_imaginary\n    assert (1 / (4 + 3 * i)).is_imaginary\n    assert (1 / (4 - 3 * i)).is_imaginary\n\n    x = Symbol('x', real=True)\n    assert (1 / (4 + i*x)).is_imaginary is None\n", "def test_issue_10375():\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    assert (x + I*y).conjugate().is_real is None\n    assert (x + I*y).conjugate().is_imaginary is None\n\n\n\n", "def test_issue_10791():\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    assert (x + y/x).is_real is None\n\n"], "sample_60": ["    def test_get_related_model_for_mediainline_with_generic_foreignkey(self):\n        class MediaInlineWithGfk(GenericTabularInline):\n            model = Media\n            extra = 1\n\n        class EpisodeAdmin(admin.ModelAdmin):\n            inlines = [MediaInlineWithGfk]\n\n        ma = EpisodeAdmin(Episode, self.site)\n        self.assertEqual(ma.get_related_model_for_mediainline_with_generic_foreignkey(request), Media)\n\n\n", "    def test_get_inline_instances_empty_list(self):\n        class MediaInline(GenericTabularInline):\n            model = Media\n\n        class AlternateInline(GenericTabularInline):\n            model = Media\n\n        class EpisodeAdmin(admin.ModelAdmin):\n            inlines = (AlternateInline, MediaInline)\n\n                return []\n\n        ma = EpisodeAdmin(Episode, self.site)\n        self.assertEqual(ma.get_inlines(request, None), [])\n        self.assertEqual(ma.get_inline_instances(request), [])\n", "    def test_order_fields_for_inlines(self):\n        class MediaForm(ModelForm):\n            class Meta:\n                model = Media\n                fields = ['description', 'keywords', 'url']\n\n        class MediaInline(GenericTabularInline):\n            form = MediaForm\n            model = Media\n            ordering = ['description', 'url']\n\n        class EpisodeAdmin(admin.ModelAdmin):\n            inlines = [\n                MediaInline\n            ]\n        ma = EpisodeAdmin(Episode, self.site)\n\n        formset = ma.get_formsets_with_inlines(request)[0][0]\n\n        self.assertEqual(\n            list(formset.forms[0].fields),\n            ['description', 'keywords', 'url', 'id', 'DELETE'])\n\n\n\n", "    def test_filter_vertical_and_horizontal(self):\n        \"\"\"\n        Test handling of `filter_vertical` and `filter_horizontal` settings.\n        \"\"\"\n        class MediaInline(GenericTabularInline):\n            model = Media\n            filter_vertical = True\n            filter_horizontal = True\n\n        class MediaForm(ModelForm):\n            class Meta:\n                model = Media\n                fields = '__all__'\n\n        class EpisodeAdmin(admin.ModelAdmin):\n            inlines = [MediaInline]\n\n        media_inline = MediaInline(Media, self.site)\n        formset = media_inline.get_formset(None)\n        self.assertIn('formset_filters', formset.media)\n\n        horizontal_filters = [\n            'url',\n            'description'\n        ]\n        vertical_filters = [\n            'keywords'\n        ]\n\n        self.assertEqual(\n            list(formset.media['formset_filters'].js.splitlines()),\n            [\n                f'filters = {json.dumps({\"horizontal\": horizontal_filters, \"vertical\": vertical_filters})};',\n            ]\n        )\n\n\n\n", "    def test_inline_form_fields_are_defined_inside_get_formsets_with_inlines(self):\n        class MediaForm(ModelForm):\n            class Meta:\n                model = Media\n\n        class MediaInline(GenericTabularInline):\n            form = MediaForm\n            model = Media\n\n        class EpisodeAdmin(admin.ModelAdmin):\n            inlines = [MediaInline]\n\n        ma = EpisodeAdmin(Episode, self.site)\n        inlines = ma.get_formsets_with_inlines(request)\n        self.assertIn(('Media', MediaInline, ), inlines)\n        formset = inlines[0][0]\n        self.assertEqual(formset.forms[0].fields, ['id', 'url', 'description', 'DELETE'])\n\n\n"], "sample_25": ["    def test_invalid_keyword_len(self):\n        h = fits.Header()\n        with pytest.raises(ValueError):\n            h[\"foo\"] = \"bar\"\n        with pytest.raises(ValueError):\n            h[\" foo\"] = \"bar\"\n        with pytest.raises(ValueError):\n            h[\"foo bar\"] = \"bar\"\n        with pytest.raises(ValueError):\n            h[\"foo  \"] = \"bar\"\n", "    def test_compressed_image_ext(self):\n        \"\"\"Regression test for https://github.com/astropy/astropy/issues/7312\n\n        Checks if compressed image extensions can be correctly read and written.\"\"\"\n\n        data = np.arange(2 * 2, dtype=np.int8).reshape((2, 2))\n        phdu = fits.PrimaryHDU(data=data)\n        chdu = fits.CompImageHDU(data=phdu.data, header=phdu.header)\n        chdu.writeto(self.temp(\"tmp.fits\"), overwrite=True)\n        with fits.open(self.temp(\"tmp.fits\")) as hdul:\n            assert hdul[1].header.get(\"XTENSION\") == \"COMP\"\n            assert hdul[1].header.get(\"PCOUNT\") > 0\n            assert hdul[1].header.get(\"GCOUNT\") > 0\n        # Compare the original and decompressed data\n        original_data = phdu.data\n        compressed_data = hdul[1].data\n        assert np.allclose(original_data, compressed_data)\n        hdul.close()\n\n\n\n", "    def test_fitsheader_non_ascii_keywords(self):\n        \"\"\"Tests the handling of non-ASCII keywords in the fitsheader script.\"\"\"\n        from astropy.io.fits.scripts import fitsheader\n\n        test_data = self.data(\"utf_test.fits\")\n        formatter = fitsheader.HeaderFormatter(test_data)\n\n        output = formatter.parse(extensions=[1], keywords=[\"T_OBS\", \"T_OBS_UTC\"])\n        assert \"T_OBS  = \" in output\n        assert \"T_OBS_UTC = \" in output\n        formatter.close()\n\n", "    def test_header_comments_persist_after_update(self):\n        h = fits.Header()\n        h[\"NAME\"] = \"Bob\"\n        h.comments[\"NAME\"] = \"Bob the Builder\"\n        h[\"NAME\"] = \"Alice\"\n        assert h.comments[\"NAME\"] == \"Bob the Builder\"\n\n\n    \n", "    def test_insert_duplicate_card(self):\n        h = fits.Header()\n        h.add_card(\"HELLO\", \"WORLD\")\n        h.insert(0, \"HELLO\", \"NEWWORLD\")\n        assert h[\"HELLO\"] == \"NEWWORLD\"\n        assert len(h) == 2\n\n"], "sample_19": ["def test_time_axis_units():\n    w = wcs.WCS(naxis=3)\n    w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\", \"TIME\"]\n    w.wcs.cunit = [\"deg\", \"deg\", \"s\"]\n    w.wcs.set()\n    assert w.wcs.cunit[2] == \"s\"\n    assert w.temporal.wcs.cunit[0] == \"s\"\n", "def test_wcs_with_missing_header_keywords():\n    \"\"\"\n    Test for WCS initialization with missing header keywords.\n    \"\"\"\n    # Create a header with missing keywords, potentially causing issues.\n    header = fits.Header()\n    header[\"NAXIS\"] = 2\n    header[\"CTYPE1\"] = \"RA---TAN\"\n    header[\"CRVAL1\"] = 0\n    # Missing: CRPIX1, CDELT1, PC1_1, PC1_2\n\n    with pytest.warns(wcs.FITSFixedWarning):\n        w = wcs.WCS(header)\n\n    # Ensure initialization succeeded, though some values might be set to defaults.\n    assert w.naxis == 2\n    assert w.wcs.ctype == [\"RA---TAN\"]\n    assert w.wcs.crval == [0]\n\n    # Check for potential issues with defaults or error handling.\n\n    # Check if missing values are set to appropriate defaults.\n\n\n\n", "compilation error", "def test_time_axis_units():\n    w = wcs.WCS(naxis=3)\n    w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\", \"TIME\"]\n    w.wcs.cunit = [\"deg\", \"deg\", \"s\"]\n    w.wcs.set()\n    assert w.wcs.cunit[2] == \"s\"\n    assert w.wcs.time_unit == \"s\"\n\n", "compilation error"], "sample_231": ["    def test_sensitive_variables_decorator_with_kwargs(self):\n        @sensitive_variables\n            return password, kwargs\n\n        with self.settings(DEBUG=True):\n            response = self.client.post('/test_view/', {'password': 'super_secret'})\n            self.assertContains(response, 'super_secret', status_code=500)\n\n        with self.settings(DEBUG=False):\n            response = self.client.post('/test_view/', {'password': 'super_secret'})\n            self.assertNotContains(response, 'super_secret', status_code=500)\n\n\n", "    def test_sensitive_function_arguments(self):\n        @sensitive_variables\n            return password + secret_key\n\n        with self.settings(DEBUG=True):\n            response = self.client.get('/sensitive_function/',\n                                       HTTP_PASSWORD='cooper',\n                                       HTTP_SECRET_KEY='test')\n            self.assertNotContains(response, 'test', status_code=500)\n\n        with self.settings(DEBUG=False):\n            response = self.client.get('/sensitive_function/',\n                                       HTTP_PASSWORD='cooper',\n                                       HTTP_SECRET_KEY='test')\n            self.assertContains(response, 'XXXXXXXXXXXXXXXXXXXX', status_code=500)\n", "    def test_sensitive_variables_decorator_on_class_method(self):\n        class MyView:\n            @sensitive_variables\n                return password\n\n        view = MyView()\n        self.assertNotEqual(view.some_method(password='secret'), 'secret')\n", "    def test_sensitive_variables_decorator_with_function_arguments(self):\n        @sensitive_variables\n            return secret_value + password\n\n        with self.settings(DEBUG=True):\n            response = self.client.post('/some_url/', {'secret_value': 'super_secret', 'password': 'super_password'})\n            self.assertContains(response, 'XXXXXXXXXXXXXXXXXXXX', status_code=500)\n\n        with self.settings(DEBUG=False):\n            response = self.client.post('/some_url/', {'secret_value': 'super_secret', 'password': 'super_password'})\n            self.assertContains(response, 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX', status_code=500)\n\n\n\n", "    def test_sensitive_decorators_with_non_string_values(self):\n        @sensitive_variables\n            return password\n        self.assertEqual(test_func(), 42)  \n        \n        @sensitive_post_parameters\n            return request.POST['password']\n        self.assertEqual(test_func(self.rf.post('/','password=42')), '42')\n"], "sample_625": ["def test_polyfit_keep_attrs(use_dask: bool, x: xr.DataArray, y: xr.DataArray) -> None:\n        y.coords[\"x\"] = x\n        if use_dask:\n            if not has_dask:\n                pytest.skip(\"requires dask\")\n            y = y.chunk({\"x\": 2})\n\n        \n        with pytest.raises(ValueError):\n            y.polyfit(dim=\"x\", deg=2, keep_attrs=False)\n\n", "compilation error", "compilation error", "    def test_polyfit_polyval_integration_different_dims(use_dask: bool, x: xr.DataArray) -> None:\n        y = xr.DataArray([1, 6, 17], dims=(\"y\", \"x\")).transpose()\n        \n        if use_dask:\n            if not has_dask:\n                pytest.skip(\"requires dask\")\n            y = y.chunk({\"y\": 2})\n        \n        fit = y.polyfit(dim=\"y\", deg=2)\n        evaluated = xr.polyval(y.x, fit.polyfit_coefficients)\n        expected = y.transpose(*evaluated.dims) \n        xr.testing.assert_allclose(evaluated.variable, expected.variable)  \n", "    def test_polyfit_keep_attrs(\n        use_dask: bool, x: xr.DataArray"], "sample_1144": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1099": ["def test_eval_partial_derivative_nested_expr():\n    tau, alpha = symbols(\"tau alpha\")\n    expr1a = PartialDerivative(A(i)*H(-i, j) + B(j), A(k))\n    expr1b = PartialDerivative(PartialDerivative(A(i)*H(-i, j) + B(j), A(k)), A(l))\n    assert expr1b._perform_derivative().expand() == \\\n        PartialDerivative(PartialDerivative(A(i)*H(-i, j), A(k)), A(l)) + \\\n        PartialDerivative(PartialDerivative(B(j), A(k)), A(l))\n\n\n\n", "def test_eval_partial_derivative_higher_order_derivatives():\n    tau, mu = symbols(\"tau mu\")\n\n    expr1 = PartialDerivative(tau**mu, tau)\n    expr2 = PartialDerivative(expr1, tau)\n    assert expr2._perform_derivative() == mu * (mu - 1) * tau**(mu - 2)\n\n    expr3 = A(i) * H(j, j)\n    expr4 = PartialDerivative(expr3, A(k))\n    expr5 = PartialDerivative(expr4, A(k))\n    assert expr5._perform_derivative() - L.delta(i, -k) * H(j, j) == 0\n\n\n\n", "def test_eval_partial_derivative_expr2():\n    tau, alpha = symbols(\"tau alpha\")\n\n    expr2 = A(i)*H(j, k) + B(i)*C(j, k) + tau**alpha*A(i)*B(j)\n\n    tensor_derivative1 = PartialDerivative(expr2, H(m, n))._perform_derivative()\n    tensor_derivative2 = PartialDerivative(expr2, C(m, n))._perform_derivative()\n    vector_derivative = PartialDerivative(expr2, A(k))._perform_derivative()\n\n    assert tensor_derivative1 - A(i)*L.delta(j, m)*L.delta(k, n) == 0\n    assert tensor_derivative2 - B(i)*L.delta(j, m)*L.delta(k, n) == 0\n    assert vector_derivative - (tau**alpha*B(j) + tau**alpha*A(i)*B(j) + H(j, k) * L.delta(i, k) + B(i)*C(j, k)*L.delta(i, k)) == 0\n", "def test_eval_partial_derivative_mixed_derivatives():\n    tau, alpha = symbols(\"tau alpha\")\n\n    expr1 = tau**alpha * A(i) * B(j) * C(k)\n\n    expr1a = PartialDerivative(expr1, A(i), B(j))\n    assert expr1a._perform_derivative() == tau**alpha * C(k) * B(j)\n\n    expr1b = PartialDerivative(expr1, A(i), B(j), C(k))\n    assert expr1b._perform_derivative() == tau**alpha * A(i) * C(k)\n\n    expr2 = A(i) * H(j, k) + B(i) * C(j)\n\n    expr2a = PartialDerivative(expr2, A(i), H(j, k))\n    assert expr2a._perform_derivative() == H(j, k)\n\n    expr2b = PartialDerivative(expr2, A(i), H(j, k), B(i))\n    assert expr2b._perform_derivative() == 0\n\n    expr3 = A(i) * B(j)\n\n    expr3a = PartialDerivative(expr3, A(i), A(j))\n    assert expr3a._perform_derivative() == 0\n\n    expr3b = PartialDerivative(expr3, A(i), B(j))\n    assert expr3b._perform_derivative() == 0\n\n\n", "compilation error"], "sample_888": ["def test_iforest_contamination_auto():\n    \"\"\"Test Isolation Forest with 'auto' contamination.\"\"\"\n\n    rng = check_random_state(0)\n    X = rng.randn(100, 2)\n    y = rng.randint(0, 2, size=100)\n\n    # Train with auto contamination\n    iforest = IsolationForest(contamination=\"auto\")\n    iforest.fit(X)\n\n    # Assert contamination is estimated\n    assert 0 <= iforest.contamination <= 1\n\n    y_pred = iforest.predict(X)\n    assert np.sum(y_pred != y) < 10  # Check some samples were correctly classified\n\n\n\n", "def test_iforest_with_sparse_outliers():\n    \"\"\"Test Isolation Forest with sparse outlier input.\"\"\"\n    rng = check_random_state(0)\n    n_samples = 1000\n    n_features = 100\n    X = rng.randn(n_samples, n_features)\n    outliers = rng.uniform(-1, 1, size=(100, n_features)).astype(\"float32\")\n\n    # Create a sparse outlier matrix by setting some elements to 0\n    sparse_outliers = csr_matrix(outliers)\n\n    # Combine dense and sparse data\n    X_train = np.vstack((X, sparse_outliers))\n    y_train = np.concatenate((np.zeros(n_samples), np.ones(100)))\n\n    # Train IsolationForest\n    iforest = IsolationForest()\n    iforest.fit(X_train)\n\n    # Test prediction on sparse outliers\n    predictions = iforest.predict(sparse_outliers)\n    assert np.all(predictions == 1)\n\n", "def test_iforest_contamination_with_sparse_data():\n    \"\"\"Test contamination parameter with sparse data\"\"\"\n    rng = check_random_state(0)\n    n_samples = 1000\n    n_features = 10\n    X_train = rng.randn(n_samples, n_features)\n    X_train_sparse = csr_matrix(X_train)\n    n_outliers = int(n_samples * 0.1)\n    y_train = np.ones(n_samples, dtype=int)\n    y_train[:n_outliers] = -1\n\n    for contamination in [0.1, \"auto\"]:\n        clf = IsolationForest(contamination=contamination)\n        clf.fit(X_train_sparse)\n        scores = clf.decision_function(X_train_sparse)\n        assert np.all(scores < 0)\n        assert np.mean(scores[y_train == -1]) < np.mean(scores[y_train == 1])\n\n\n\n", "def test_iforest_fit_sparse_with_missing_values():\n    \"\"\"\n    Test that IsolationForest can fit sparse matrices with missing values\n    (NaN).\n    \"\"\"\n    X = sp.csc_matrix(np.random.rand(100, 50).astype(float))\n    # Introduce some missing values\n    X[20:30, 10:20] = np.nan\n    iforest = IsolationForest()\n    iforest.fit(X)\n    \n    # Check that the model was fitted successfully\n    assert len(iforest.estimators_) == iforest.n_estimators\n\n\n\n", "def test_iforest_contamination_auto(contamination, random_state, global_random_seed):\n\n    X, y = make_classification(\n        n_samples=1000, n_features=5, random_state=global_random_seed\n    )\n    X_train, X_test = train_test_split(X, test_size=0.2)\n    y_train, y_test = train_test_split(y, test_size=0.2)\n\n    if contamination is None:\n        contamination = \"auto\"\n\n    # Test with different contamination values\n    for contamination_value in [contamination, 0.25, 0.5]:\n        clf1 = IsolationForest(\n            contamination=contamination_value, random_state=random_state\n        )\n        clf1.fit(X_train)\n\n        clf2 = IsolationForest(\n            contamination=contamination_value, random_state=random_state\n        )\n        clf2.fit(X_train)\n\n        # Ensure that the predictions are consistent\n        assert_array_equal(clf1.predict(X_test), clf2.predict(X_test))\n"], "sample_632": ["def test_ignore_signatures_with_multiline_string():\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([SIMILAR7, SIMILAR8])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == (\n            '''", "compilation error", "def test_ignore_imports_and_signatures():\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run(\n            [\n                \"--ignore-imports\",\n                \"--ignore-signatures\",\n                SIMILAR5,\n                SIMILAR6,\n            ]\n        )\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == \"\"\"", "def test_empty_files():\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([EMPTY_FUNCTION_1, EMPTY_FUNCTION_2])\n    assert ex.value.code == 0\n    assert output.getvalue().strip() == \"\"\"", "def test_different_file_encoding():\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([SIMILAR1, SIMILAR2])\n    assert ex.value.code == 0\n    assert \"TOTAL lines=60 duplicates=10 percent=16.67\" in output.getvalue()\n"], "sample_332": ["    def test_not_valid_mixed_with_valid(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '',  # <-- This is invalid\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset, formset)), False)\n        expected_errors = [{'votes': ['This field is required.']}, {}]\n        self.assertEqual(formset._errors, expected_errors)\n\n\n\n", "    def test_invalid_formset_with_non_field_errors(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': '',\n            'choices-1-votes': 'invalid',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(formset.errors, {'choice': ['This field is required.']})\n        self.assertEqual(formset._errors, {'choices-1-choice': ['This field is required.'], 'choices-1-votes': ['Invalid value.']})\n\n\n", "    def test_invalid(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '',  # <-- This field is missing\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [\n            {'votes': ['This field is required.']},\n            {}\n        ]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n", "    def test_non_form_errors_with_multiple_formsets(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',  # the number of forms rendered\n            'choices-INITIAL_FORMS': '0',  # the number of forms with initial data\n            'choices-MIN_NUM_FORMS': '0',  # min number of forms\n            'choices-MAX_NUM_FORMS': '0',  # max number of forms\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet1 = formset_factory(Choice, extra=1, max_num=1, validate_max=True)\n        ChoiceFormSet2 = formset_factory(Choice, extra=1, max_num=1, validate_max=True)\n        formsets = (ChoiceFormSet1(data, auto_id=False, prefix='choices'), ChoiceFormSet2(data, auto_id=False, prefix='choices'))\n        formset1, formset2 = formsets\n\n        self.assertFalse(all_valid(formsets))\n        self.assertEqual(formset1.total_error_count(), 1)\n        self.assertEqual(formset2.total_error_count(), 1)\n        self.assertEqual(len([fs for fs in formsets if fs.is_valid()]), 0)\n\n\n\n", "    def test_non_form_errors_override_form_errors(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-MAX_NUM_FORMS': '2',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        class CustomForm(Form):\n            pass\n\n        class CustomFormSet(BaseFormSet):\n                raise ValidationError(\"This is a non-form error\")\n\n        CustomChoiceFormSet = formset_factory(CustomForm, formset=CustomFormSet, max_num=2)\n        formset = CustomChoiceFormSet(data, prefix='choices')\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(\n            formset.errors,\n            ['This is a non-form error']\n        )\n\n"], "sample_1166": ["compilation error", "    def test_monomial_mulpow():\n        assert monomial_mulpow((3, 4, 1), (1, 2, 0), 2) == (4, 6, 2)\n        assert monomial_mulpow((3, 4, 1), (0, 5, 1), 3) == (0, 15, 3)\n        assert monomial_mulpow((-3, 4, 1), (0, 5, 1), 2) == (0, 15, 2)\n", "def test_monomial_mulpow():\n    assert monomial_mulpow((3, 4, 1), (1, 2, 0), 2) == (4, 6, 2)\n    assert monomial_mulpow((3, 4, 1), (1, 2, 0), 0) == (3, 4, 1)\n    assert monomial_mulpow((3, 4, 1), (1, 2, 0), -1) == (1, 2, 0)\n\n\n\n", "def test_monomial_mulpow():\n    assert monomial_mulpow((1, 2, 3), (2, 3, 4), 3) == (6, 12, 27)\n\n    assert monomial_mulpow((0, 0, 0), (1, 2, 3), 4) == (0, 0, 0)\n", "def test_monomial_mulpow():\n    assert monomial_mulpow((3, 4, 1), (1, 2, 0), 2) == (4, 8, 2)\n    assert monomial_mulpow((3, 4, 1), (1, 2, 0), 0) == (1, 0, 0) \n"], "sample_165": ["    def test_field_validation_errors(self):\n        class MyForm(Form):\n            field = Field\n\n        class Field(CharField):\n                if value == \"error\":\n                    raise ValidationError(\"This value is invalid!\")\n                return super().clean(value)\n\n        form = MyForm({'field': 'error'})\n        with self.assertRaises(ValidationError):\n            form.is_valid()\n        self.assertEqual(form.errors['field'], ['This value is invalid!'])\n\n\n\n\n", "    def test_modelforeignkeyfield_empty_label(self):\n        class ModelWithForeignKey(models.Model):\n            name = models.CharField(max_length=100)\n            parent = models.ForeignKey(\n                'self', on_delete=models.CASCADE, null=True, blank=True\n            )\n\n        form = ModelForm(\n            model=ModelWithForeignKey,\n            fields=['name', 'parent'],\n            widgets={'parent': Select()}\n        )\n        field = form.fields['parent']\n        self.assertIsNone(field.label)\n        self.assertEqual(field.empty_label, '---------')\n\n\n\n", "    def test_list_error_message_with_empty_list(self):\n        f = ModelMultipleChoiceField(\n            queryset=ChoiceModel.objects.all(),\n            error_messages={'list': 'NOT A LIST OF VALUES'},\n        )\n        self.assertFormErrors(['NOT A LIST OF VALUES'], f.clean, '')\n", "    def test_deprecated_choicefield_error_messages(self):\n        e = {\n            'invalid_choice': 'INVALID CHOICE',\n            'required': 'REQUIRED',\n            'invalid_list': 'NOT A LIST',\n        }\n        f = ChoiceField(choices=[('a', 'aye')], error_messages=e)\n        form = Form({'choice': '4'},)\n        self.assertHTMLEqual(str(form.errors['choice']), '<ul class=\"errorlist\"><li>INVALID CHOICE</li></ul>')\n", "    def test_modelchoicefield_empty_label(self):\n        class MyModel(models.Model):\n            name = models.CharField(max_length=255)\n        ChoiceModel.objects.create(pk=1, name='a')\n        f = ModelChoiceField(queryset=MyModel.objects.all(), empty_label=None)\n        self.assertFormErrors(['INVALID CHOICE'], f.clean, '4')\n"], "sample_339": ["    def test_inlineformset_factory_queryset_overridden_by_instance(self):\n        author = Author.objects.create(name='Charles Baudelaire')\n        other_author = Author.objects.create(name='Walt Whitman')\n        BookFormSet = inlineformset_factory(\n            Author,\n            Book,\n            fields='__all__',\n            queryset=Author.objects.filter(name__startswith='C')\n        )\n\n        formset = BookFormSet(instance=author)\n        self.assertEqual(len(formset.forms), 1)\n        self.assertCountEqual(formset.forms[0].fields['author'].queryset.all(), [author])\n", "    def test_modelformset_factory_fieldsets(self):\n        class AuthorFormSet(forms.Form):\n            name = forms.CharField(max_length=100)\n            bio = forms.CharField(widget=forms.Textarea)\n\n        AuthorFormSet = modelformset_factory(\n            Author,\n            fields=('name', 'bio'),\n            fieldsets=[\n                (\n                    'Personal Information',\n                    {'fields': ('name',)},\n                ),\n                (\n                    'Biography',\n                    {'fields': ('bio',)},\n                    {'classes': ['collapse']},\n                ),\n            ],\n        )\n        formset = AuthorFormSet()\n        self.assertEqual(len(formset.fields), 2)\n        self.assertEqual(formset.fields['name'].widget.attrs['class'], '')\n        self.assertEqual(formset.fields['bio'].widget.attrs['class'], 'collapse')\n\n", "    def test_inlineformset_factory_custom_field_class(self):\n        class CustomCharField(forms.CharField):\n                return value.upper()\n\n        BookFormSet = inlineformset_factory(\n            Author,\n            Book,\n            fields=\"__all__\",\n            field_classes={'title': CustomCharField},\n        )\n        formset = BookFormSet()\n        form = formset.forms[0]\n        self.assertIsInstance(form.fields['title'], CustomCharField)\n        self.assertTrue(form.fields['title'].clean('foo').isupper())\n", "    def test_inlineformset_factory_with_many_to_many_field(self):\n        AuthorFormSet = inlineformset_factory(\n            Author,\n            Book,\n            fields='__all__',\n        )\n        author = Author.objects.create(name='Charles Baudelaire')\n        data = {\n            'book_set-TOTAL_FORMS': '1',\n            'book_set-INITIAL_FORMS': '0',\n            'book_set-MAX_NUM_FORMS': '0',\n            'book_set-0-title': 'Les Fleurs du Mal',\n            'book_set-0-authors': str(author.id),\n        }\n        formset = AuthorFormSet(data, instance=author)\n        self.assertTrue(formset.is_valid())\n        formset.save()\n        author.refresh_from_db()\n        self.assertEqual(author.book_set.count(), 1)  \n", "    def test_modelformset_factory_passes_form_kwargs(self):\n        class MyForm(forms.ModelForm):\n            class Meta:\n                model = Author\n                fields = '__all__'\n\n            kwargs['label_suffix'] = ''\n            return kwargs\n\n        AuthorFormSet = modelformset_factory(\n            Author,\n            form=MyForm,\n            extra=2,\n            form_kwargs=my_extra_form_kwargs,\n        )\n        formset = AuthorFormSet()\n        for field in formset.forms[0].fields.values():\n            self.assertEqual(field.label_suffix, '')\n"], "sample_925": ["compilation error", "compilation error", "compilation error", "compilation error", "def test_mock_invalid_module_name():\n    with pytest.raises(ValueError):\n        mock([])\n"], "sample_793": ["def test_contamination_string_auto():\n    X_train = [[1, 1], [1, 2], [2, 1]]\n    for contamination in [\"auto\", \"0.25\"]:\n        clf = IsolationForest(contamination=contamination).fit(X_train)\n        assert (\n            clf.contamination == 0.25 if contamination == \"auto\" else float(contamination)\n        )\n", "def test_iforest_n_jobs_on_single_core():\n    # Check that n_jobs=1 behaves correctly\n    X = iris.data\n    clf = IsolationForest(n_jobs=1).fit(X)\n    assert_equal(clf.n_jobs_, 1)\n\n", "def test_iforest_outlier_detection():\n    # Test outlier detection on small dataset\n    X = [[1, 1], [1, 2], [2, 1], [4, 4]]\n    y = [1, 1, 1, -1]\n    clf = IsolationForest().fit(X)\n    y_pred = clf.predict(X)\n\n    # Assert that the last data point is correctly classified as an outlier\n    assert y_pred[3] == -1\n", "def test_iforest_oob_score():\n    # This test checks if the OOB score is calculated correctly\n    X = iris.data\n    y = iris.target\n    clf = IsolationForest(random_state=0, n_estimators=100, oob_score=True)\n    clf.fit(X, y)\n    assert clf.oob_score_ is not None\n\n", "def test_iforest_contamination_auto():\n    \"\"\"Test that contamination='auto' works.\"\"\"\n    rng = check_random_state(0)\n    X = 0.3 * rng.randn(120, 2)\n    X_train = np.r_[X + 2, X - 2]\n    \n    # Generate an outlier dataset with 20% contamination\n    X_outliers = rng.uniform(low=-4, high=4, size=(24, 2))\n    X_test = np.r_[X[:30], X_outliers]\n    y_test = np.array([0] * 30 + [1] * 24)\n\n    # fit the model with contamination='auto'\n    clf = IsolationForest(random_state=rng, contamination=\"auto\")\n    clf.fit(X_train)\n\n    # predict scores (the lower, the more normal)\n    y_pred = -clf.decision_function(X_test)\n\n    # check that there is at most 6 errors (false positive or false negative)\n    assert_greater(roc_auc_score(y_test, y_pred), 0.98)\n\n\n\n"], "sample_47": ["    def test_cleanse_setting_multiple_words(self):\n        self.assertEqual(cleanse_setting('API_KEY', 'super secret key'), CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleanse_setting('API_KEY', 'my_secret_api_key'), CLEANSED_SUBSTITUTE)\n\n", "    def test_cleanse_setting_nested_dict(self):\n        self.assertEqual(cleanse_setting('PASSWORD', {'key1': 'value1', 'PASSWORD': 'super_secret'}), {'key1': 'value1', 'PASSWORD': CLEANSED_SUBSTITUTE})\n", "    def test_cleanse_setting_nested_dict(self):\n        data = {'user': {'password': 'secret'}}\n        cleaned_data = cleanse_setting(data, 'user.password')\n        self.assertEqual(cleaned_data, {'user': {'password': CLEANSED_SUBSTITUTE}})\n", "    def test_cleanse_setting_nested_dict(self):\n        config = {\n            'SECRET_KEY': 'my_secret_key',\n            'DATABASE': {\n                'NAME': 'my_database',\n                'PASSWORD': 'my_database_password'\n            }\n        }\n        cleaned_config = cleanse_settings(config)\n        self.assertEqual(cleaned_config['SECRET_KEY'], CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleaned_config['DATABASE']['NAME'], 'my_database')\n        self.assertEqual(cleaned_config['DATABASE']['PASSWORD'], CLEANSED_SUBSTITUTE)\n", "    def test_cleanse_setting_recursive(self):\n        data = {\n            'SECRET_KEY': 'dont_show_me',\n            'foo': 'bar',\n            'nested': {\n                'PASSWORD': 'secret',\n                'another_key': 'value'\n            }\n        }\n        cleaned_data = cleanse_setting_recursive(data)\n        self.assertEqual(cleaned_data['SECRET_KEY'], CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleaned_data['foo'], 'bar')\n        self.assertEqual(cleaned_data['nested']['PASSWORD'], CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleaned_data['nested']['another_key'], 'value') \n"], "sample_1125": ["def test_outer_product():\n    from sympy.physics.quantum.state import Ket, Bra\n    x = symbols('x')\n    ket = Ket('k')\n    bra = Bra('b')\n    op = OuterProduct(ket, bra)\n    assert op ==  ket*bra\n    assert op.ket == ket\n    assert op.bra == bra\n    assert Dagger(op) == Bra('b')*Ket('k')\n", "def test_differential_operator():\n    x = sympy.Symbol('x')\n    f = sympy.Function('f')\n    a = qop.DifferentialOperator(sympy.Derivative(f(x), x), f(x))\n\n    assert a.variables == (x,)\n    assert a.function == f(x)\n    assert a.expr == sympy.Derivative(f(x), x)\n    \n    w = sympy.physics.quantum.state.Wavefunction(x**2, x)\n\n    result = qop.qapply(a*w)\n    expected = sympy.physics.quantum.state.Wavefunction(2*x, x)\n    assert result == expected\n\n", "def test_apply_operator_wavefunction():\n    from sympy.physics.quantum.state import Wavefunction\n    x = symbols('x')\n    f = Function('f')\n    d = DifferentialOperator(Derivative(f(x), x), f(x))\n    w = Wavefunction(x**2, x)\n    result = d*w\n    expected = Wavefunction(2*x, x)\n    assert result.doit() == expected \n", "def test_differential_operator():\n    from sympy.physics.quantum.state import Wavefunction\n    from sympy.physics.quantum.operator import DifferentialOperator\n    from sympy import Derivative, Function, Symbol, sin\n\n    x = Symbol('x')\n    f = Function('f')\n    d = DifferentialOperator(Derivative(f(x), x), f(x))\n\n    w = Wavefunction(sin(x), x)\n    result = d*w\n    expected = Wavefunction(cos(x), x)\n\n    assert result == expected\n", "def test_differential_operator():\n    from sympy.physics.quantum.state import Wavefunction\n    from sympy.physics.quantum.qapply import qapply\n    from sympy import Derivative, Function, Symbol\n\n    x = Symbol('x')\n    f = Function('f')\n    \n    d = DifferentialOperator(Derivative(f(x), x), f(x))\n    w = Wavefunction(x**2, x)\n    \n    assert qapply(d*w) == Wavefunction(2*x, x)\n"], "sample_986": ["def test_issue_10954():\n    from sympy.functions import integrate, exp\n    result = integrate(exp(x*S(1.5)), (x, -1, 1)).evalf()\n    assert abs(result - 1.9691627993662168) < 1e-10\n    result = integrate(exp(x**2), (x, -1, 1)).evalf()\n    assert abs(result - 1.493648062861546) < 1e-10   \n", "def test_issue_10987():\n    from sympy import cos, pi\n    assert cos(pi/12).evalf(chop=True) == 0.965925826289068\n    assert cos(pi/12).evalf() == 0.965925826289068\n", "def test_issue_10800():\n    from sympy.functions import gamma\n    assert gamma(0.5).evalf() == 2*sqrt(pi)\n    assert gamma(1.5).evalf() == sqrt(pi)/2\n", "compilation error", "def test_issue_10975():\n    from sympy.functions import log\n    x = Symbol('x')\n    assert log(x, 2).evalf(subs={x: 8}) == 3.\n    assert log(x, 2).evalf(subs={x: 1/2}) == -1.\n    assert log(x, 2).evalf(subs={x: 1}) == 0.\n"], "sample_557": ["def test_savefig_transparent_background():\n    fig = plt.figure(figsize=(3, 3))\n    ax = fig.add_subplot(111)\n    ax.plot([1, 2], [3, 4])\n    \n    with io.BytesIO() as buffer:\n        fig.savefig(buffer, format='png', transparent=True)\n        png_data = buffer.getvalue()\n    \n    # Check for the presence of the alpha channel\n    assert b'\\x78\\x52\\x47' in png_data, \"PNG data does not contain alpha channel\"\n", "def test_savefig_transparent_images():\n    fig, ax = plt.subplots()\n    ax.imshow([[1, 2, 3], [4, 5, 6]], cmap='viridis', transparent=True)\n    fig.savefig(io.BytesIO(), format='png', bbox_inches='tight',\n                transparent=True)  # Explicitly set transparent=True\n    fig.savefig(io.BytesIO(), format='png', bbox_inches='tight')\n    \n    # Check that only the second save preserves transparency\n    \n", "def test_savefig_dpi_deprecation():\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter('always')\n        fig = plt.figure()\n        fig.savefig(io.BytesIO(), dpi=120)\n    assert len(w) == 1\n    assert 'dpi' in str(w[0].message)\n", "def test_savefig_transparent_background():\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.set_facecolor('none')\n    fig.savefig('test_transparent.png', transparent=True)\n    with open('test_transparent.png', 'rb') as f:\n        magic = f.read(12)\n        assert magic == b'\\x89PNG\\r\\n\\x1a\\n'\n\n\n\n", "def test_savefig_metadata_extra_kwarg():\n    with pytest.raises(TypeError, match=\"got an unexpected keyword argument 'extra'\"):\n        Figure().savefig(io.BytesIO(), format='png', metadata={}, extra='...')\n"]}, "raw_scored_generations": {"sample_2": [false, false, false, false, false], "sample_4": [false, false, false, false, false], "sample_3": [false, true, false, false, false], "sample_1": [false, false, false, false, false], "sample_0": [false, false, false, false, false], "sample_5": [false, false, false, false, false], "sample_6": [false, false, false, true, false], "sample_7": [false, false, false, false, false], "sample_8": [false, true, false, true, false], "sample_9": [false, true, false, false, true], "sample_10": [true, false, false, false, true], "sample_11": [false, false, false, false, false], "sample_12": [false, true, false, false, false], "sample_13": [true, false, false, false, false], "sample_14": [false, true, false, false, false], "sample_15": [true, true, true, true, true], "sample_16": [false, false, false, false, false], "sample_17": [false, false, false, false, false], "sample_18": [false, false, false, false, false], "sample_19": [true, false, false, false, false], "sample_20": [false, false, true, false, false], "sample_21": [false, false, false, false, false], "sample_22": [false, false, false, false, false], "sample_23": [false, false, false, false, false], "sample_24": [true, false, false, true, false], "sample_25": [true, true, true, true, true], "sample_26": [true, true, false, false, true], "sample_27": [false, false, true, false, true], "sample_28": [true, false, true, true, true], "sample_29": [false, false, false, false, true], "sample_30": [false, true, false, false, true], "sample_31": [false, false, false, false, false], "sample_32": [true, false, true, false, false], "sample_33": [false, false, false, false, false], "sample_34": [false, false, false, false, true], "sample_35": [false, false, false, false, false], "sample_36": [false, false, false, false, false], "sample_37": [false, false, false, false, false], "sample_38": [false, true, false, false, false], "sample_39": [false, false, false, false, false], "sample_40": [false, false, false, false, false], "sample_41": [false, false, false, false, false], "sample_42": [false, false, false, true, false], "sample_43": [false, false, true, false, false], "sample_44": [false, false, false, false, false], "sample_45": [false, false, false, false, false], "sample_46": [true, true, true, false, false], "sample_47": [true, false, false, false, false], "sample_48": [false, false, false, false, false], "sample_49": [false, true, false, true, true], "sample_50": [false, false, false, false, false], "sample_51": [false, false, false, false, false], "sample_52": [false, false, false, false, false], "sample_54": [true, false, false, true, false], "sample_53": [false, true, false, false, false], "sample_58": [false, false, false, false, true], "sample_55": [false, false, false, true, false], "sample_56": [false, false, false, false, false], "sample_57": [false, false, true, false, false], "sample_59": [false, false, false, false, false], "sample_60": [false, false, false, false, false], "sample_61": [true, true, true, true, true], "sample_62": [true, true, false, false, true], "sample_63": [false, false, false, false, false], "sample_64": [true, true, false, false, true], "sample_65": [true, true, true, true, true], "sample_67": [false, false, false, false, false], "sample_66": [false, false, true, true, true], "sample_68": [false, false, false, false, false], "sample_69": [false, false, true, true, false], "sample_70": [false, false, false, false, true], "sample_71": [false, true, false, false, false], "sample_72": [false, false, false, false, false], "sample_73": [false, false, false, false, false], "sample_75": [false, false, false, false, false], "sample_74": [false, false, false, false, false], "sample_77": [false, false, false, false, false], "sample_76": [false, false, false, false, false], "sample_78": [false, false, true, false, false], "sample_79": [false, false, true, true, false], "sample_80": [false, true, false, false, false], "sample_82": [false, false, false, false, false], "sample_81": [false, false, false, false, false], "sample_83": [false, false, false, false, false], "sample_85": [false, false, false, false, true], "sample_84": [true, true, false, true, false], "sample_86": [false, false, false, false, false], "sample_88": [false, false, false, false, false], "sample_87": [true, true, false, true, false], "sample_89": [true, true, false, false, false], "sample_90": [false, false, false, true, false], "sample_91": [false, false, false, false, false], "sample_92": [false, false, false, false, false], "sample_93": [false, false, false, false, false], "sample_94": [false, false, false, false, false], "sample_95": [false, false, false, false, false], "sample_98": [false, false, false, false, true], "sample_96": [false, false, false, false, false], "sample_99": [false, false, false, false, false], "sample_97": [false, true, true, false, false], "sample_100": [true, false, true, true, false], "sample_102": [false, false, false, false, true], "sample_101": [false, false, false, false, false], "sample_103": [false, false, false, false, false], "sample_104": [false, false, false, false, false], "sample_107": [false, false, false, true, false], "sample_105": [false, false, true, false, false], "sample_106": [false, false, false, false, false], "sample_108": [false, false, false, false, false], "sample_109": [false, false, false, false, false], "sample_111": [false, true, true, true, true], "sample_110": [false, false, false, false, true], "sample_112": [false, false, false, false, false], "sample_113": [false, false, false, false, false], "sample_114": [false, false, false, false, false], "sample_115": [false, false, false, false, false], "sample_116": [false, true, false, false, false], "sample_117": [false, false, false, false, false], "sample_118": [true, false, false, false, false], "sample_119": [false, false, false, false, false], "sample_120": [false, false, false, false, false], "sample_121": [true, true, true, true, true], "sample_122": [false, false, false, false, false], "sample_123": [false, true, false, true, true], "sample_124": [false, true, true, true, false], "sample_125": [true, true, false, true, true], "sample_126": [false, false, false, false, false], "sample_127": [false, false, true, false, true], "sample_128": [true, true, true, true, true], "sample_129": [false, false, true, true, true], "sample_130": [false, false, false, false, true], "sample_131": [true, true, false, false, true], "sample_132": [false, false, false, false, false], "sample_133": [true, true, true, true, true], "sample_135": [false, true, false, false, true], "sample_134": [false, false, false, false, false], "sample_136": [true, false, false, true, false], "sample_139": [true, true, false, false, true], "sample_137": [false, false, false, false, false], "sample_138": [false, false, false, false, false], "sample_140": [false, false, false, false, false], "sample_141": [true, true, false, true, false], "sample_142": [true, false, false, true, false], "sample_143": [false, true, false, false, false], "sample_144": [false, false, false, false, true], "sample_145": [false, false, false, false, false], "sample_146": [true, true, true, true, true], "sample_147": [false, false, false, false, false], "sample_148": [false, true, false, false, false], "sample_151": [false, false, false, false, false], "sample_149": [false, false, true, true, true], "sample_152": [true, false, false, false, false], "sample_150": [false, false, false, false, false], "sample_153": [false, false, false, false, false], "sample_154": [false, false, false, false, false], "sample_155": [false, false, true, true, false], "sample_156": [false, true, false, true, false], "sample_157": [false, false, false, true, false], "sample_158": [false, true, false, false, false], "sample_159": [true, false, false, false, true], "sample_160": [false, true, false, false, false], "sample_161": [false, false, false, false, true], "sample_162": [true, true, true, true, true], "sample_163": [false, false, false, true, false], "sample_164": [false, false, false, true, false], "sample_165": [false, false, false, false, false], "sample_166": [true, true, true, true, true], "sample_167": [false, false, false, false, false], "sample_168": [false, false, false, false, false], "sample_169": [false, false, false, true, false], "sample_171": [true, true, true, true, true], "sample_170": [false, false, false, false, false], "sample_172": [true, true, true, true, true], "sample_173": [false, false, false, false, false], "sample_174": [false, false, false, false, false], "sample_175": [false, false, true, false, false], "sample_176": [false, true, false, false, false], "sample_177": [true, false, false, false, false], "sample_178": [true, true, true, true, false], "sample_180": [false, false, false, false, false], "sample_179": [true, true, true, true, true], "sample_182": [true, false, true, false, false], "sample_181": [false, false, false, false, false], "sample_183": [false, false, false, false, false], "sample_184": [false, false, false, false, false], "sample_185": [false, true, false, true, false], "sample_186": [false, false, false, false, false], "sample_187": [false, false, false, false, false], "sample_188": [false, true, false, true, false], "sample_189": [false, false, false, false, false], "sample_190": [false, false, true, false, false], "sample_191": [false, false, true, false, true], "sample_192": [false, false, false, false, false], "sample_193": [true, false, false, false, false], "sample_194": [false, false, false, false, false], "sample_195": [false, false, false, true, false], "sample_196": [false, false, false, false, false], "sample_198": [false, false, false, false, false], "sample_197": [false, true, false, false, false], "sample_199": [false, false, false, false, false], "sample_200": [false, false, false, false, false], "sample_201": [false, false, false, false, false], "sample_202": [true, false, false, false, false], "sample_203": [false, false, false, false, false], "sample_204": [true, false, false, false, false], "sample_205": [false, false, false, false, false], "sample_206": [false, false, true, true, false], "sample_207": [false, false, false, false, false], "sample_208": [false, false, false, false, false], "sample_209": [false, false, false, false, false], "sample_210": [false, false, false, false, false], "sample_211": [true, false, false, false, false], "sample_213": [false, false, false, false, false], "sample_212": [false, false, false, false, true], "sample_214": [false, true, false, false, false], "sample_215": [false, false, false, false, false], "sample_216": [false, false, false, false, false], "sample_217": [true, false, false, false, false], "sample_218": [true, false, false, false, true], "sample_219": [false, false, false, false, false], "sample_220": [true, true, true, false, true], "sample_221": [false, true, false, false, false], "sample_222": [false, false, true, false, true], "sample_223": [false, false, false, false, false], "sample_224": [false, false, false, false, true], "sample_225": [false, false, true, false, true], "sample_226": [false, false, false, false, false], "sample_227": [false, false, false, false, false], "sample_228": [false, true, false, true, false], "sample_229": [false, false, false, false, false], "sample_230": [true, false, false, false, true], "sample_231": [false, false, false, false, false], "sample_232": [false, false, false, false, false], "sample_233": [false, false, false, false, false], "sample_234": [false, false, false, false, false], "sample_235": [false, false, false, false, false], "sample_236": [false, false, false, false, false], "sample_237": [false, false, true, false, false], "sample_238": [true, false, false, false, false], "sample_239": [true, false, false, true, false], "sample_240": [true, false, false, false, false], "sample_241": [false, false, false, false, true], "sample_242": [false, false, false, false, false], "sample_243": [false, false, false, false, false], "sample_244": [false, false, false, false, false], "sample_245": [true, true, true, true, true], "sample_246": [false, true, true, true, true], "sample_247": [false, false, false, false, false], "sample_248": [false, false, false, false, false], "sample_249": [false, false, false, false, false], "sample_250": [false, false, false, false, true], "sample_251": [false, false, true, false, false], "sample_252": [true, true, false, false, false], "sample_253": [true, true, true, false, true], "sample_254": [false, true, false, false, true], "sample_256": [false, false, false, false, false], "sample_255": [false, false, false, false, false], "sample_257": [false, false, false, false, false], "sample_258": [false, false, false, false, false], "sample_259": [true, false, false, false, false], "sample_260": [false, false, false, true, false], "sample_261": [false, false, false, false, false], "sample_262": [false, false, false, false, false], "sample_263": [false, false, false, false, false], "sample_264": [true, false, false, false, true], "sample_265": [false, false, false, false, false], "sample_266": [false, false, false, false, false], "sample_267": [false, false, false, false, false], "sample_268": [true, false, true, false, false], "sample_269": [true, true, true, true, true], "sample_270": [false, false, false, false, false], "sample_271": [true, true, false, true, true], "sample_272": [false, false, false, false, false], "sample_273": [true, true, false, false, true], "sample_274": [false, false, false, false, true], "sample_275": [false, false, false, false, false], "sample_276": [false, false, false, false, false], "sample_277": [false, false, false, false, false], "sample_278": [false, false, false, false, false], "sample_279": [false, true, false, true, true], "sample_280": [false, false, false, false, true], "sample_281": [true, true, true, true, false], "sample_282": [false, false, false, false, false], "sample_283": [true, false, false, false, false], "sample_284": [false, false, false, false, false], "sample_285": [true, false, false, false, false], "sample_286": [false, false, true, false, true], "sample_287": [false, true, false, false, true], "sample_288": [false, false, false, false, false], "sample_289": [false, false, false, false, false], "sample_290": [false, true, false, false, false], "sample_291": [false, false, true, false, false], "sample_292": [false, false, false, false, false], "sample_293": [false, false, false, true, true], "sample_294": [false, false, false, false, false], "sample_295": [false, false, false, false, false], "sample_296": [false, false, false, false, false], "sample_297": [false, false, false, false, false], "sample_298": [true, true, false, false, false], "sample_299": [true, true, true, true, true], "sample_300": [true, false, false, false, false], "sample_301": [false, false, false, true, true], "sample_302": [false, false, false, false, false], "sample_303": [false, false, false, false, false], "sample_304": [false, false, false, false, false], "sample_305": [false, false, true, false, true], "sample_306": [false, false, false, false, false], "sample_307": [true, false, true, true, false], "sample_308": [false, false, false, true, false], "sample_309": [false, false, false, true, false], "sample_310": [false, false, false, false, false], "sample_312": [false, false, false, true, true], "sample_311": [true, false, false, false, false], "sample_313": [false, false, false, false, false], "sample_314": [false, false, false, false, false], "sample_315": [false, false, false, false, false], "sample_316": [false, true, true, false, true], "sample_317": [false, true, false, true, true], "sample_318": [false, false, false, false, false], "sample_319": [false, false, false, false, false], "sample_320": [true, true, false, false, false], "sample_321": [false, false, false, false, false], "sample_322": [false, false, false, false, false], "sample_323": [false, false, false, false, false], "sample_324": [false, false, false, false, false], "sample_325": [false, false, true, false, true], "sample_326": [false, false, true, false, false], "sample_327": [true, false, false, false, false], "sample_328": [false, false, false, false, true], "sample_329": [false, false, false, false, false], "sample_330": [false, false, false, true, true], "sample_331": [false, false, false, false, false], "sample_332": [true, false, true, true, false], "sample_333": [false, true, false, false, false], "sample_334": [true, true, false, true, true], "sample_335": [false, true, false, false, false], "sample_336": [false, false, false, false, false], "sample_337": [false, false, false, false, false], "sample_338": [true, false, false, false, false], "sample_339": [false, false, false, true, false], "sample_340": [false, false, false, false, false], "sample_341": [true, false, true, true, false], "sample_342": [true, true, true, true, true], "sample_343": [false, false, false, false, false], "sample_344": [false, false, false, true, false], "sample_345": [true, true, false, true, true], "sample_346": [false, false, false, false, false], "sample_347": [false, false, false, false, false], "sample_348": [false, false, false, false, false], "sample_349": [false, false, false, false, false], "sample_350": [false, false, false, false, false], "sample_351": [false, false, false, false, false], "sample_352": [false, true, false, false, true], "sample_353": [false, false, false, false, false], "sample_354": [false, false, false, false, false], "sample_355": [true, false, false, false, false], "sample_356": [false, false, false, true, false], "sample_357": [false, false, false, false, false], "sample_358": [true, false, false, false, true], "sample_359": [true, true, true, true, false], "sample_360": [false, false, false, false, false], "sample_361": [false, false, false, true, false], "sample_362": [false, false, false, false, false], "sample_363": [true, true, true, true, true], "sample_364": [false, false, false, false, false], "sample_365": [false, false, false, false, false], "sample_366": [false, false, false, false, false], "sample_367": [false, false, false, false, false], "sample_368": [false, false, false, false, false], "sample_369": [false, false, false, false, false], "sample_370": [false, false, false, false, false], "sample_371": [false, false, false, false, false], "sample_372": [false, false, false, false, false], "sample_373": [false, false, true, false, false], "sample_374": [false, false, false, false, false], "sample_375": [false, false, true, false, false], "sample_376": [false, false, false, false, false], "sample_377": [false, false, false, false, false], "sample_378": [false, false, false, false, false], "sample_379": [true, true, false, false, false], "sample_380": [false, false, false, false, false], "sample_381": [false, false, false, false, false], "sample_382": [false, false, true, false, false], "sample_383": [false, true, true, false, false], "sample_384": [false, false, false, false, false], "sample_385": [true, true, false, false, true], "sample_386": [true, false, false, false, false], "sample_387": [true, true, true, true, true], "sample_388": [false, false, false, false, false], "sample_389": [false, false, false, false, false], "sample_390": [false, false, true, true, true], "sample_391": [false, false, false, false, false], "sample_392": [true, false, false, false, false], "sample_393": [true, true, true, true, true], "sample_394": [false, false, true, false, false], "sample_395": [false, false, false, false, false], "sample_396": [true, false, true, true, false], "sample_397": [false, false, false, false, false], "sample_398": [false, true, false, false, false], "sample_399": [false, false, false, true, false], "sample_400": [false, false, false, false, false], "sample_401": [false, true, true, false, true], "sample_403": [false, true, false, false, true], "sample_402": [false, false, false, false, false], "sample_404": [true, true, true, true, true], "sample_405": [false, false, true, false, true], "sample_406": [true, false, false, false, false], "sample_407": [false, false, false, false, true], "sample_408": [false, false, false, false, false], "sample_409": [false, true, false, false, false], "sample_410": [false, false, false, false, false], "sample_411": [false, false, false, false, true], "sample_412": [false, true, false, false, false], "sample_413": [false, false, false, true, true], "sample_414": [true, true, true, false, false], "sample_415": [false, true, false, false, false], "sample_416": [true, true, true, true, true], "sample_417": [false, false, false, false, false], "sample_418": [false, false, false, false, true], "sample_419": [false, false, true, false, false], "sample_420": [false, false, false, false, false], "sample_421": [false, false, false, false, false], "sample_422": [false, false, false, false, false], "sample_423": [false, false, true, true, false], "sample_424": [false, false, false, true, false], "sample_425": [false, false, false, false, false], "sample_426": [false, false, false, false, false], "sample_427": [false, false, false, false, false], "sample_428": [false, false, false, true, true], "sample_429": [false, false, false, false, false], "sample_430": [true, false, false, false, false], "sample_431": [false, false, false, false, false], "sample_432": [true, true, true, true, true], "sample_433": [false, false, false, false, false], "sample_434": [false, false, false, false, false], "sample_435": [false, false, false, false, false], "sample_436": [false, true, false, false, false], "sample_437": [false, false, false, false, false], "sample_438": [false, false, false, false, false], "sample_439": [false, false, false, false, false], "sample_440": [false, false, false, false, false], "sample_441": [false, false, false, false, false], "sample_442": [false, true, false, false, false], "sample_443": [false, false, false, false, false], "sample_444": [false, false, false, false, false], "sample_445": [true, false, false, false, true], "sample_446": [false, false, false, false, false], "sample_447": [true, false, true, false, false], "sample_448": [false, false, true, false, false], "sample_449": [false, false, false, false, false], "sample_450": [false, false, false, false, false], "sample_451": [false, false, false, false, false], "sample_453": [false, false, false, true, false], "sample_452": [true, false, true, false, false], "sample_454": [true, false, false, false, false], "sample_455": [false, false, false, false, false], "sample_456": [true, true, false, false, true], "sample_457": [false, false, false, false, false], "sample_458": [false, false, false, true, true], "sample_459": [true, true, true, true, true], "sample_460": [true, true, true, true, false], "sample_461": [false, false, false, true, false], "sample_462": [false, false, false, true, false], "sample_463": [false, false, false, false, false], "sample_464": [true, true, false, true, false], "sample_465": [false, false, false, false, false], "sample_466": [false, false, false, false, false], "sample_467": [true, false, false, false, false], "sample_469": [false, false, false, false, false], "sample_468": [false, false, false, false, false], "sample_470": [false, false, false, false, false], "sample_471": [true, true, true, false, false], "sample_472": [false, true, false, false, true], "sample_473": [true, true, false, false, false], "sample_474": [false, false, false, false, false], "sample_475": [false, false, false, false, false], "sample_476": [false, false, false, false, false], "sample_477": [false, false, false, false, false], "sample_478": [false, false, false, false, false], "sample_479": [false, false, false, false, false], "sample_480": [false, false, true, false, true], "sample_481": [false, false, false, false, false], "sample_482": [false, false, false, false, false], "sample_483": [true, false, false, false, false], "sample_484": [true, false, true, true, false], "sample_485": [false, false, false, false, false], "sample_486": [false, false, false, false, false], "sample_487": [false, false, false, false, false], "sample_488": [true, false, false, false, false], "sample_489": [false, false, false, false, false], "sample_490": [false, false, false, false, false], "sample_491": [true, false, false, false, false], "sample_492": [false, false, false, false, false], "sample_493": [false, false, false, false, false], "sample_494": [false, false, false, false, false], "sample_495": [false, true, false, false, false], "sample_496": [true, true, false, true, true], "sample_497": [true, false, true, true, true], "sample_498": [false, true, false, false, true], "sample_499": [true, false, false, false, false], "sample_500": [false, false, false, false, false], "sample_501": [true, false, false, false, false], "sample_502": [false, true, false, false, false], "sample_503": [false, false, false, false, false], "sample_504": [false, false, false, false, false], "sample_505": [true, false, false, true, false], "sample_506": [false, false, false, false, false], "sample_507": [false, false, false, false, false], "sample_508": [false, false, false, false, false], "sample_509": [true, false, true, false, true], "sample_510": [true, false, false, false, false], "sample_511": [false, false, true, false, false], "sample_512": [false, true, true, false, true], "sample_513": [false, false, false, false, false], "sample_514": [false, false, false, false, false], "sample_515": [false, false, false, false, false], "sample_516": [false, false, false, false, false], "sample_517": [false, false, false, false, false], "sample_518": [true, true, true, false, true], "sample_519": [false, true, false, false, false], "sample_520": [false, false, false, false, false], "sample_521": [false, false, false, false, true], "sample_522": [false, false, false, false, true], "sample_523": [false, true, false, false, false], "sample_524": [false, true, false, false, false], "sample_525": [false, true, false, false, false], "sample_526": [false, false, true, false, false], "sample_527": [false, false, false, false, false], "sample_528": [false, true, true, false, false], "sample_529": [false, false, true, false, false], "sample_530": [false, false, false, false, true], "sample_531": [false, false, false, false, false], "sample_532": [true, false, false, false, false], "sample_533": [false, false, false, false, false], "sample_534": [false, true, false, false, false], "sample_535": [false, false, false, false, false], "sample_536": [false, false, false, false, false], "sample_537": [false, true, true, true, true], "sample_538": [false, true, false, false, false], "sample_539": [false, false, false, false, false], "sample_540": [false, false, false, false, false], "sample_541": [false, false, false, false, false], "sample_542": [false, true, false, false, false], "sample_543": [false, false, false, false, false], "sample_544": [false, false, false, false, false], "sample_545": [false, false, false, false, false], "sample_546": [false, false, false, false, false], "sample_547": [false, false, false, false, false], "sample_548": [false, false, false, false, true], "sample_549": [false, false, true, true, false], "sample_550": [false, false, false, true, false], "sample_551": [false, false, false, false, false], "sample_552": [false, false, false, false, false], "sample_553": [false, false, false, false, false], "sample_554": [false, false, false, false, false], "sample_555": [false, false, false, false, false], "sample_556": [false, true, false, false, false], "sample_557": [false, false, false, false, true], "sample_558": [false, false, false, false, false], "sample_559": [false, false, false, false, false], "sample_560": [false, false, false, false, false], "sample_561": [false, false, false, false, false], "sample_562": [false, false, false, false, false], "sample_563": [false, false, false, false, false], "sample_564": [false, false, false, false, false], "sample_565": [false, false, true, false, false], "sample_566": [false, false, false, false, false], "sample_567": [false, false, false, false, false], "sample_568": [false, false, false, false, false], "sample_569": [false, false, false, false, false], "sample_570": [false, false, false, false, true], "sample_571": [false, false, false, false, false], "sample_572": [false, false, false, false, false], "sample_573": [true, false, false, false, false], "sample_574": [false, false, false, false, false], "sample_575": [false, false, false, false, false], "sample_576": [false, false, false, false, false], "sample_577": [false, false, false, false, false], "sample_578": [true, false, false, false, false], "sample_579": [false, false, false, false, false], "sample_580": [false, false, false, false, true], "sample_581": [false, false, false, false, false], "sample_582": [false, false, false, false, false], "sample_583": [false, false, false, false, false], "sample_584": [false, false, false, false, false], "sample_585": [true, false, false, false, false], "sample_586": [false, false, false, false, false], "sample_587": [false, false, true, true, false], "sample_588": [false, false, false, false, false], "sample_589": [false, false, false, false, false], "sample_590": [false, false, false, false, false], "sample_591": [false, false, false, false, false], "sample_592": [true, true, true, true, true], "sample_593": [false, true, false, false, false], "sample_594": [true, true, true, true, false], "sample_595": [true, false, false, false, false], "sample_596": [false, true, true, false, true], "sample_597": [true, true, false, false, false], "sample_598": [false, true, false, false, false], "sample_599": [false, false, false, false, false], "sample_600": [false, false, false, false, false], "sample_601": [false, false, true, false, false], "sample_602": [false, false, false, false, false], "sample_603": [false, false, false, true, false], "sample_604": [false, false, false, false, false], "sample_605": [false, false, true, false, true], "sample_606": [false, false, false, false, false], "sample_607": [false, false, false, false, false], "sample_608": [true, true, true, true, true], "sample_609": [false, false, false, false, false], "sample_610": [false, false, false, false, false], "sample_611": [false, true, false, false, true], "sample_612": [false, false, false, false, false], "sample_613": [false, false, false, false, false], "sample_614": [true, false, true, false, true], "sample_615": [false, true, false, false, false], "sample_616": [false, false, false, false, false], "sample_617": [false, false, false, false, false], "sample_618": [false, false, false, false, false], "sample_619": [false, false, false, false, false], "sample_620": [false, false, false, false, false], "sample_621": [false, false, false, false, false], "sample_622": [false, true, false, true, true], "sample_623": [false, false, false, false, false], "sample_624": [false, false, false, false, false], "sample_625": [false, false, false, true, false], "sample_626": [true, true, true, true, true], "sample_627": [false, false, false, false, false], "sample_628": [false, false, true, false, false], "sample_629": [false, false, false, true, false], "sample_630": [false, false, false, false, false], "sample_631": [false, true, true, false, false], "sample_632": [false, false, false, false, false], "sample_633": [false, false, false, false, false], "sample_634": [false, false, true, false, false], "sample_635": [false, false, false, false, false], "sample_636": [false, false, false, false, false], "sample_637": [false, false, false, true, false], "sample_638": [false, false, false, false, false], "sample_639": [false, false, false, false, false], "sample_640": [false, false, false, false, false], "sample_641": [false, false, false, false, false], "sample_642": [false, false, false, false, false], "sample_643": [false, false, false, false, false], "sample_644": [false, false, false, false, false], "sample_645": [false, false, false, false, true], "sample_646": [false, true, true, false, false], "sample_647": [false, false, false, false, false], "sample_648": [true, true, true, false, true], "sample_649": [false, false, false, false, false], "sample_650": [false, false, false, false, false], "sample_651": [true, false, false, false, false], "sample_652": [false, false, false, false, false], "sample_653": [false, false, false, false, true], "sample_654": [false, false, false, false, false], "sample_655": [false, true, false, false, false], "sample_656": [true, false, true, false, false], "sample_657": [true, true, true, true, true], "sample_658": [false, true, true, true, true], "sample_659": [false, false, false, true, false], "sample_660": [true, false, true, false, true], "sample_661": [true, false, false, true, false], "sample_662": [false, false, false, false, true], "sample_663": [false, false, false, false, false], "sample_664": [false, false, false, false, false], "sample_665": [false, false, false, false, false], "sample_666": [false, false, false, false, false], "sample_667": [true, false, true, false, true], "sample_668": [false, false, false, false, false], "sample_669": [false, true, false, true, false], "sample_670": [false, false, false, false, false], "sample_671": [false, false, false, false, false], "sample_672": [false, false, false, false, false], "sample_673": [true, true, false, true, true], "sample_674": [false, false, false, false, false], "sample_675": [false, false, false, false, false], "sample_676": [false, false, true, true, false], "sample_677": [false, false, false, false, false], "sample_678": [true, true, true, false, false], "sample_679": [false, false, true, true, true], "sample_680": [false, false, false, false, false], "sample_681": [false, false, false, false, true], "sample_682": [false, false, false, false, false], "sample_683": [false, false, false, false, false], "sample_684": [false, false, false, false, false], "sample_685": [false, false, false, false, false], "sample_686": [false, false, false, false, false], "sample_687": [false, false, false, false, false], "sample_688": [false, false, false, false, false], "sample_689": [false, false, false, false, false], "sample_690": [false, false, false, false, false], "sample_691": [false, false, false, false, false], "sample_692": [false, true, false, false, false], "sample_693": [true, false, true, true, false], "sample_694": [false, false, false, false, false], "sample_695": [false, false, false, false, false], "sample_696": [false, false, false, false, false], "sample_697": [false, false, true, false, false], "sample_698": [false, false, false, true, false], "sample_699": [true, false, true, true, true], "sample_700": [false, false, false, false, false], "sample_701": [false, false, false, false, false], "sample_702": [true, false, true, false, false], "sample_703": [false, false, true, false, false], "sample_704": [false, false, false, true, false], "sample_705": [false, false, false, false, false], "sample_706": [false, true, false, false, false], "sample_707": [false, false, false, false, false], "sample_708": [false, false, false, false, false], "sample_709": [false, false, false, false, false], "sample_710": [true, false, true, true, true], "sample_711": [false, false, false, false, false], "sample_712": [false, true, false, false, false], "sample_713": [false, false, false, false, true], "sample_714": [false, false, false, false, false], "sample_715": [false, false, false, true, false], "sample_716": [false, false, true, true, false], "sample_717": [true, true, true, true, true], "sample_718": [false, false, false, false, false], "sample_719": [false, false, false, true, true], "sample_720": [false, false, true, false, false], "sample_721": [true, true, true, true, true], "sample_722": [false, false, true, true, false], "sample_723": [true, false, false, false, true], "sample_724": [true, false, false, false, false], "sample_725": [true, false, true, true, true], "sample_726": [false, false, false, false, false], "sample_727": [false, false, false, false, false], "sample_728": [false, true, false, false, true], "sample_729": [false, false, false, false, false], "sample_730": [false, true, false, false, false], "sample_731": [false, true, false, false, false], "sample_732": [false, false, false, false, false], "sample_733": [false, true, true, false, false], "sample_734": [false, false, false, false, true], "sample_735": [false, false, true, false, false], "sample_736": [false, false, false, false, false], "sample_737": [false, false, false, false, false], "sample_738": [false, false, false, true, false], "sample_739": [false, false, false, false, false], "sample_740": [true, false, false, false, false], "sample_741": [false, false, false, true, false], "sample_742": [false, false, true, false, false], "sample_743": [false, false, false, false, true], "sample_744": [false, false, false, true, false], "sample_745": [false, false, true, true, false], "sample_746": [false, false, false, false, false], "sample_747": [true, false, false, false, false], "sample_748": [false, false, false, false, false], "sample_749": [true, false, false, false, false], "sample_750": [true, true, false, false, false], "sample_751": [true, false, true, true, false], "sample_752": [false, false, false, true, false], "sample_753": [false, false, true, false, false], "sample_754": [false, false, false, false, false], "sample_755": [false, false, false, false, false], "sample_756": [false, false, false, true, true], "sample_757": [true, true, false, true, false], "sample_758": [false, false, false, false, false], "sample_759": [false, true, false, false, false], "sample_760": [false, false, false, false, false], "sample_761": [false, false, false, false, false], "sample_762": [true, false, true, false, false], "sample_763": [false, false, false, false, false], "sample_764": [false, false, false, false, false], "sample_765": [false, false, false, false, false], "sample_766": [false, false, true, false, true], "sample_767": [false, false, false, false, false], "sample_768": [false, false, false, false, false], "sample_769": [false, false, false, false, false], "sample_770": [false, false, true, false, false], "sample_771": [false, true, false, false, false], "sample_772": [true, true, true, true, true], "sample_773": [false, false, false, false, false], "sample_774": [false, true, false, false, false], "sample_775": [false, false, false, false, false], "sample_776": [true, false, false, false, false], "sample_777": [false, false, true, false, false], "sample_778": [false, false, false, false, false], "sample_779": [false, false, false, false, true], "sample_780": [true, false, false, false, false], "sample_781": [false, false, false, false, false], "sample_782": [false, false, false, false, false], "sample_783": [false, false, false, false, true], "sample_784": [false, false, true, false, false], "sample_785": [false, false, false, false, false], "sample_786": [false, false, false, false, false], "sample_787": [false, false, false, false, false], "sample_788": [false, true, false, false, false], "sample_789": [false, false, false, false, false], "sample_790": [false, true, false, true, false], "sample_791": [false, true, false, true, false], "sample_792": [false, false, false, true, false], "sample_793": [false, false, true, false, false], "sample_794": [false, false, false, false, false], "sample_795": [false, false, false, false, true], "sample_796": [true, false, true, true, false], "sample_797": [false, false, false, false, false], "sample_798": [false, false, false, false, false], "sample_799": [false, false, true, true, false], "sample_800": [false, false, false, false, false], "sample_801": [false, false, true, false, false], "sample_802": [false, false, false, false, false], "sample_803": [false, false, false, false, false], "sample_804": [false, false, false, false, false], "sample_805": [false, false, false, false, false], "sample_806": [true, false, false, true, false], "sample_807": [false, false, false, false, false], "sample_808": [false, false, false, false, true], "sample_809": [false, false, false, false, false], "sample_810": [false, false, true, false, false], "sample_811": [false, false, false, false, false], "sample_812": [false, false, false, false, false], "sample_813": [false, false, false, false, false], "sample_814": [true, true, true, false, true], "sample_815": [false, false, false, true, true], "sample_816": [false, false, false, false, false], "sample_817": [false, false, false, false, false], "sample_818": [false, false, true, true, true], "sample_819": [false, false, false, false, false], "sample_820": [false, true, false, false, false], "sample_821": [false, false, false, false, false], "sample_822": [true, false, false, false, false], "sample_823": [false, false, false, false, false], "sample_824": [false, false, true, false, true], "sample_825": [false, false, false, false, false], "sample_826": [true, false, false, false, false], "sample_827": [false, false, false, false, false], "sample_828": [false, false, false, false, true], "sample_829": [false, false, false, false, false], "sample_830": [false, false, false, false, false], "sample_831": [false, false, false, false, false], "sample_832": [true, false, false, false, false], "sample_833": [false, false, false, false, false], "sample_834": [true, false, false, false, false], "sample_835": [false, false, true, true, false], "sample_836": [false, false, false, false, false], "sample_837": [false, false, false, false, false], "sample_838": [false, false, false, false, false], "sample_839": [false, false, false, false, false], "sample_840": [false, false, false, false, false], "sample_841": [true, false, false, false, false], "sample_842": [false, false, false, false, false], "sample_843": [false, true, false, false, false], "sample_844": [false, false, false, false, false], "sample_845": [false, false, false, false, false], "sample_846": [false, false, false, false, false], "sample_847": [false, false, false, false, true], "sample_848": [false, false, false, false, false], "sample_849": [false, false, false, false, false], "sample_850": [false, true, false, false, true], "sample_851": [false, false, false, true, false], "sample_852": [false, false, false, false, true], "sample_853": [false, false, false, false, false], "sample_854": [false, false, true, false, false], "sample_855": [true, false, false, true, false], "sample_856": [false, true, false, true, false], "sample_857": [true, false, true, false, false], "sample_858": [false, false, false, false, false], "sample_859": [false, true, true, true, false], "sample_860": [false, false, false, false, false], "sample_861": [false, false, false, false, false], "sample_862": [false, false, false, false, false], "sample_863": [true, false, false, true, false], "sample_864": [false, true, false, false, false], "sample_865": [false, false, false, false, false], "sample_866": [false, false, false, false, false], "sample_867": [false, false, false, false, false], "sample_868": [false, false, false, false, false], "sample_869": [false, true, false, false, false], "sample_870": [true, false, false, true, true], "sample_871": [false, false, false, false, false], "sample_872": [false, false, false, false, false], "sample_873": [true, false, true, false, true], "sample_874": [false, false, false, false, false], "sample_875": [false, false, true, false, true], "sample_876": [false, true, false, false, false], "sample_877": [false, false, false, false, true], "sample_878": [false, false, false, false, false], "sample_879": [false, false, false, false, false], "sample_880": [false, false, false, false, false], "sample_881": [false, false, false, false, false], "sample_882": [false, false, false, false, false], "sample_883": [true, false, false, false, false], "sample_884": [true, false, true, true, false], "sample_885": [false, false, false, false, false], "sample_886": [false, false, false, false, true], "sample_887": [false, false, false, false, false], "sample_888": [false, false, false, false, false], "sample_889": [false, false, false, false, false], "sample_890": [false, false, false, false, false], "sample_891": [false, false, false, false, false], "sample_892": [false, false, false, false, false], "sample_893": [false, false, false, false, false], "sample_894": [false, false, false, false, false], "sample_895": [false, false, false, false, false], "sample_896": [false, true, false, false, false], "sample_897": [false, false, false, false, false], "sample_898": [false, true, false, true, false], "sample_899": [false, false, true, false, false], "sample_900": [false, false, false, false, false], "sample_901": [false, false, false, false, false], "sample_902": [false, false, false, false, false], "sample_903": [false, false, true, false, false], "sample_904": [false, false, true, false, false], "sample_905": [false, false, false, false, false], "sample_906": [false, false, false, false, false], "sample_907": [false, false, false, false, false], "sample_908": [true, true, false, false, true], "sample_909": [false, false, false, false, false], "sample_910": [false, false, false, false, false], "sample_911": [true, false, false, false, false], "sample_912": [false, false, false, false, false], "sample_913": [false, false, false, false, false], "sample_914": [false, true, true, false, true], "sample_915": [false, false, false, false, false], "sample_916": [false, false, true, true, true], "sample_917": [false, false, false, false, true], "sample_918": [true, false, false, true, false], "sample_919": [true, false, false, true, true], "sample_920": [false, false, false, false, false], "sample_921": [false, false, false, false, false], "sample_922": [false, false, false, false, false], "sample_923": [false, false, false, true, false], "sample_924": [false, false, false, false, false], "sample_925": [false, false, false, false, false], "sample_926": [false, true, false, false, true], "sample_927": [true, true, false, true, false], "sample_928": [false, false, false, false, false], "sample_929": [false, false, false, false, false], "sample_930": [false, false, false, false, false], "sample_931": [false, false, false, false, false], "sample_932": [false, true, true, true, true], "sample_933": [false, false, false, false, false], "sample_934": [true, true, true, false, false], "sample_935": [true, false, false, false, false], "sample_936": [true, false, false, false, true], "sample_937": [true, true, true, true, false], "sample_938": [false, false, false, false, false], "sample_939": [true, false, true, false, true], "sample_940": [false, false, false, false, false], "sample_941": [true, true, true, true, false], "sample_942": [false, false, false, false, false], "sample_943": [false, false, false, false, false], "sample_944": [false, true, true, false, false], "sample_945": [false, false, false, false, false], "sample_946": [false, false, false, false, false], "sample_947": [false, false, false, false, false], "sample_948": [false, false, false, false, false], "sample_949": [false, false, false, false, false], "sample_950": [false, false, false, false, false], "sample_951": [false, false, false, false, false], "sample_952": [false, false, false, false, false], "sample_953": [true, true, true, false, false], "sample_954": [false, false, false, false, false], "sample_955": [true, false, false, true, true], "sample_956": [false, false, false, false, false], "sample_957": [false, false, false, false, false], "sample_958": [false, false, false, false, false], "sample_959": [false, false, false, false, false], "sample_960": [false, false, false, false, false], "sample_961": [true, false, false, false, false], "sample_962": [false, false, false, false, false], "sample_963": [false, false, false, false, false], "sample_964": [false, false, false, false, false], "sample_965": [false, false, false, false, false], "sample_966": [false, false, false, false, false], "sample_967": [false, false, false, true, false], "sample_968": [false, false, true, false, false], "sample_969": [false, false, false, false, true], "sample_970": [false, false, false, false, false], "sample_971": [false, false, false, false, false], "sample_972": [false, false, false, false, false], "sample_973": [false, false, false, false, false], "sample_974": [false, true, false, false, false], "sample_975": [false, false, true, true, false], "sample_976": [false, false, false, false, false], "sample_977": [true, true, true, false, true], "sample_978": [false, false, false, true, false], "sample_979": [false, false, false, true, true], "sample_980": [false, false, false, false, false], "sample_981": [false, false, false, false, false], "sample_982": [false, false, false, false, false], "sample_983": [false, false, false, false, false], "sample_984": [false, false, false, false, false], "sample_985": [false, false, true, false, false], "sample_986": [false, false, false, false, true], "sample_987": [false, false, false, true, false], "sample_988": [false, true, false, false, false], "sample_989": [false, false, false, false, true], "sample_990": [false, false, false, true, false], "sample_991": [false, false, false, false, false], "sample_992": [false, true, false, false, true], "sample_993": [true, false, false, false, false], "sample_994": [false, false, false, false, true], "sample_995": [false, false, false, false, false], "sample_996": [true, false, false, false, false], "sample_997": [false, false, false, false, false], "sample_998": [false, false, false, false, true], "sample_999": [false, false, false, false, false], "sample_1000": [false, false, true, false, false], "sample_1001": [true, false, false, false, false], "sample_1002": [false, true, false, true, false], "sample_1003": [false, false, false, false, false], "sample_1004": [false, false, false, false, false], "sample_1005": [false, true, false, false, false], "sample_1006": [false, false, false, false, false], "sample_1007": [false, false, false, false, false], "sample_1008": [false, false, false, true, false], "sample_1009": [false, false, false, false, false], "sample_1010": [false, false, true, false, false], "sample_1011": [false, true, false, false, false], "sample_1012": [false, false, true, false, false], "sample_1013": [false, true, true, false, false], "sample_1014": [false, false, false, false, false], "sample_1015": [false, false, false, false, false], "sample_1016": [false, false, false, false, false], "sample_1017": [false, false, false, false, true], "sample_1018": [false, false, false, false, false], "sample_1019": [false, true, false, false, false], "sample_1020": [true, false, true, false, false], "sample_1021": [false, false, false, false, false], "sample_1022": [false, false, false, false, false], "sample_1023": [false, false, false, false, false], "sample_1024": [false, false, false, false, false], "sample_1025": [false, false, false, false, false], "sample_1026": [true, true, false, false, true], "sample_1027": [false, false, false, false, false], "sample_1028": [false, true, false, false, false], "sample_1029": [false, false, false, false, false], "sample_1030": [false, false, true, false, false], "sample_1031": [false, false, false, false, false], "sample_1032": [true, false, true, false, true], "sample_1033": [false, false, false, false, false], "sample_1034": [true, true, true, true, false], "sample_1035": [false, false, false, false, false], "sample_1036": [false, false, true, false, true], "sample_1037": [false, false, false, false, false], "sample_1038": [false, true, false, false, true], "sample_1039": [false, false, false, false, false], "sample_1040": [false, false, false, false, false], "sample_1041": [false, false, false, false, false], "sample_1042": [false, false, false, false, false], "sample_1043": [false, false, false, false, false], "sample_1044": [true, false, true, true, false], "sample_1045": [false, false, true, true, false], "sample_1046": [false, false, false, false, false], "sample_1047": [false, false, false, true, false], "sample_1048": [false, false, false, false, false], "sample_1049": [false, false, false, false, false], "sample_1050": [true, false, true, true, false], "sample_1051": [false, false, false, false, false], "sample_1052": [false, false, false, false, false], "sample_1053": [false, false, false, false, false], "sample_1054": [false, true, false, false, false], "sample_1055": [false, false, false, false, false], "sample_1056": [false, false, false, false, true], "sample_1057": [false, false, false, false, false], "sample_1058": [false, false, false, true, false], "sample_1059": [false, false, false, false, false], "sample_1060": [true, false, false, true, false], "sample_1061": [false, false, true, true, false], "sample_1062": [false, false, false, false, false], "sample_1063": [true, false, false, true, true], "sample_1064": [true, false, true, false, false], "sample_1065": [false, false, false, false, false], "sample_1066": [false, false, false, false, false], "sample_1067": [false, false, false, true, false], "sample_1068": [false, false, false, false, false], "sample_1069": [false, false, false, false, false], "sample_1070": [false, false, false, false, false], "sample_1071": [false, false, false, false, false], "sample_1072": [false, false, false, true, false], "sample_1073": [false, false, false, false, false], "sample_1074": [false, false, false, false, false], "sample_1075": [false, false, false, false, false], "sample_1076": [false, false, false, false, false], "sample_1077": [false, false, true, true, false], "sample_1078": [false, false, false, false, false], "sample_1079": [false, true, false, false, false], "sample_1080": [false, false, false, false, false], "sample_1081": [false, false, false, false, false], "sample_1082": [false, false, false, false, false], "sample_1083": [false, false, false, false, true], "sample_1084": [false, false, false, true, false], "sample_1085": [false, false, false, true, false], "sample_1086": [false, false, false, false, true], "sample_1087": [true, false, true, false, true], "sample_1088": [false, false, false, false, false], "sample_1089": [false, false, false, false, false], "sample_1090": [false, false, false, false, false], "sample_1091": [false, false, false, false, false], "sample_1092": [false, true, false, false, false], "sample_1093": [true, false, false, false, false], "sample_1094": [false, false, false, false, false], "sample_1095": [false, false, false, false, false], "sample_1096": [false, false, false, false, true], "sample_1097": [true, false, false, false, false], "sample_1098": [false, false, false, false, false], "sample_1099": [false, false, false, false, false], "sample_1100": [false, false, false, false, true], "sample_1101": [false, false, false, false, false], "sample_1102": [false, false, false, false, true], "sample_1103": [false, false, false, false, true], "sample_1104": [false, false, false, false, false], "sample_1105": [false, false, false, true, false], "sample_1106": [false, false, false, true, false], "sample_1107": [false, false, false, false, false], "sample_1108": [false, false, false, false, false], "sample_1109": [true, false, true, false, false], "sample_1110": [false, false, false, false, false], "sample_1111": [false, false, false, false, false], "sample_1112": [true, true, false, false, false], "sample_1113": [false, false, false, false, false], "sample_1114": [false, false, false, false, false], "sample_1115": [false, false, false, false, false], "sample_1116": [false, false, false, false, false], "sample_1117": [false, false, false, false, false], "sample_1118": [false, false, false, false, false], "sample_1119": [false, false, false, false, false], "sample_1120": [false, false, false, false, false], "sample_1121": [false, false, false, false, false], "sample_1122": [false, false, false, false, false], "sample_1123": [false, false, true, false, false], "sample_1124": [false, true, true, false, false], "sample_1125": [false, false, false, false, false], "sample_1126": [false, false, false, true, true], "sample_1127": [false, false, false, false, false], "sample_1128": [false, false, true, false, false], "sample_1129": [false, false, false, false, false], "sample_1130": [false, false, false, false, false], "sample_1131": [false, false, false, false, false], "sample_1132": [false, false, false, false, false], "sample_1133": [false, false, false, false, true], "sample_1134": [false, false, false, true, false], "sample_1135": [false, false, false, false, false], "sample_1136": [false, false, false, true, false], "sample_1137": [false, true, true, false, false], "sample_1138": [false, false, false, false, false], "sample_1139": [false, false, false, false, false], "sample_1140": [false, false, false, false, false], "sample_1141": [false, false, true, false, false], "sample_1142": [true, false, false, false, true], "sample_1143": [false, false, false, true, false], "sample_1144": [false, false, false, false, false], "sample_1145": [false, false, true, false, false], "sample_1146": [true, false, false, false, true], "sample_1147": [true, true, false, false, false], "sample_1148": [false, true, false, false, false], "sample_1149": [false, false, false, false, true], "sample_1150": [false, false, false, false, false], "sample_1151": [false, true, false, true, true], "sample_1152": [true, false, true, false, false], "sample_1153": [false, false, false, false, false], "sample_1154": [true, false, false, true, false], "sample_1155": [false, false, false, false, false], "sample_1156": [false, false, false, false, false], "sample_1157": [true, false, true, false, false], "sample_1158": [false, false, false, false, false], "sample_1159": [true, true, true, true, true], "sample_1160": [false, false, false, false, false], "sample_1161": [false, true, false, false, false], "sample_1162": [false, true, false, false, false], "sample_1163": [false, false, false, true, false], "sample_1164": [false, false, false, false, false], "sample_1165": [false, false, true, true, false], "sample_1166": [false, true, false, false, false], "sample_1167": [false, false, false, false, false], "sample_1168": [false, false, false, false, false], "sample_1169": [false, true, false, false, false], "sample_1170": [false, false, false, true, false], "sample_1171": [false, false, false, false, false], "sample_1172": [false, false, false, false, false], "sample_1173": [false, false, false, false, false], "sample_1174": [false, true, false, false, false], "sample_1175": [false, false, false, false, false], "sample_1176": [false, false, false, false, false], "sample_1177": [false, true, true, true, false], "sample_1178": [false, false, false, false, false], "sample_1179": [false, false, false, false, true], "sample_1180": [false, false, false, false, false], "sample_1181": [true, true, true, true, true], "sample_1182": [false, false, false, false, false], "sample_1183": [false, false, false, true, false], "sample_1184": [false, false, false, false, false], "sample_1185": [false, false, false, false, false], "sample_1186": [true, false, true, false, false], "sample_1187": [false, true, false, false, false], "sample_1188": [false, false, false, false, false], "sample_1189": [true, true, true, true, true], "sample_1190": [false, false, false, false, false], "sample_1191": [true, false, false, false, false], "sample_1192": [false, false, false, false, false], "sample_1193": [false, false, false, false, false], "sample_1194": [false, false, false, false, false], "sample_1195": [false, false, false, false, false], "sample_1196": [true, false, true, true, false], "sample_1197": [false, false, false, false, false], "sample_1198": [false, false, false, false, false], "sample_1199": [false, false, false, false, false], "sample_1200": [false, false, false, false, false], "sample_1201": [false, false, false, false, false], "sample_1202": [false, false, false, false, false], "sample_1203": [false, false, false, false, false], "sample_1204": [false, false, false, true, false], "sample_1205": [false, false, false, false, false], "sample_1206": [false, false, false, false, false], "sample_1207": [false, false, false, false, false], "sample_1208": [false, true, true, false, false], "sample_1209": [false, false, false, false, false]}}