{"raw_generations": {"sample_67": ["def test_serialize_zoneinfo(self):\n    tz = zoneinfo.ZoneInfo(\"America/New_York\")\n    dt = datetime.datetime(2022, 1, 1, 12, 0, tzinfo=tz)\n    self.assertSerializedEqual(dt)\n    string, imports = MigrationWriter.serialize(dt)\n    self.assertEqual(\n        string,\n        \"datetime.datetime(2022, 1, 1, 12, 0, tzinfo=datetime.timezone.utc)\",\n    )\n    self.assertEqual(imports, {\"import datetime\"})\n", "def test_serialize_nested_list(self):\n    nested_list = [[1, 2], [3, 4]]\n    self.assertSerializedEqual(nested_list)\n    string, imports = MigrationWriter.serialize(nested_list)\n    self.assertEqual(string, \"[[1, 2], [3, 4]]\")\n    self.assertEqual(imports, set())\n", "def test_serialize_set_containing_unhashable_types(self):\n    value = {1, 2, [3, 4], {\"a\": \"b\"}}\n    with self.assertRaisesMessage(TypeError, \"unhashable type: 'list'\"):\n        MigrationWriter.serialize(value)\n", "def test_serialize_method_type(self):\n    class TestClass:\n            pass\n\n    self.assertSerializedEqual(TestClass.method)\n    string, imports = MigrationWriter.serialize(TestClass.method)\n    self.assertEqual(\n        string,\n        \"migrations.test_writer.WriterTests.test_serialize_method_type.<locals>.TestClass.method\",\n    )\n    self.assertEqual(imports, {\"import migrations.test_writer\"})\n", "def test_serialize_path_with_os_sep(self):\n    path = pathlib.PurePath(\"path\", \"to\", \"file.txt\")\n    expected = (\"pathlib.PurePath('path{}to{}file.txt')\".format(os.sep, os.sep), {\"import pathlib\"})\n    self.assertSerializedResultEqual(path, expected)\n"], "sample_97": ["def test_inverse_binarize_thresholding_binary():\n    y = np.array([[0, 1], [1, 0]])\n    classes = np.array([0, 1])\n    got = _inverse_binarize_thresholding(y, output_type=\"binary\",\n                                         classes=classes,\n                                         threshold=0.5)\n    assert_array_equal(got, np.array([1, 0]))\n", "def test_label_binarizer_empty_input():\n    lb = LabelBinarizer()\n    assert_raises(ValueError, lb.fit, [])\n    assert_raises(ValueError, lb.fit_transform, [])\n    assert_raises(ValueError, lb.transform, [])\n\n    # Check that fitting with empty input raises ValueError\n    assert_raises(ValueError, LabelBinarizer().fit, np.array([]))\n    assert_raises(ValueError, LabelBinarizer().fit, csr_matrix(np.array([])))\n", "def test_label_binarizer_empty_class_list():\n    lb = LabelBinarizer()\n    y = []\n    assert_raises(ValueError, lb.fit, y)\n", "def test_label_binarizer_sparse_input():\n    # Test LabelBinarizer with sparse input\n    X = csr_matrix([[1, 0, 2], [0, 3, 0], [4, 0, 0]])\n    lb = LabelBinarizer()\n    lb.fit(X)\n    assert_array_equal(lb.classes_, [1, 2, 3, 4])\n    assert_true(issparse(lb.transform(X)))\n    assert_array_equal(lb.transform(X).toarray(), [[1, 1, 0, 0],\n                                                   [0, 0, 1, 0],\n                                                   [0, 0, 0, 1]])\n", "def test_inverse_binarize_thresholding_binary():\n    y = np.array([[0, 1], [1, 0]])\n    classes = np.array([0, 1])\n    got = _inverse_binarize_thresholding(y, \"binary\", classes, threshold=0.5)\n    assert_array_equal(got, np.array([1, 0]))\n"], "sample_7": ["    def test_echo_on_called(self):\n        with mock.patch('django.utils.autoreload.termios') as mocked_termios:\n            autoreload.ensure_echo_on()\n            self.assertEqual(mocked_termios.tcgetattr.call_count, 1)\n            self.assertEqual(mocked_termios.tcsetattr.call_count, 1)\n", "def test_tick_sets_stop_condition(self):\n        yield\n        self.reloader.stop()\n\n    with mock.patch.object(self.reloader, 'tick', side_effect=mocked_tick) as tick:\n        self.assertFalse(self.reloader.should_stop)\n        next(tick())\n        self.assertTrue(self.reloader.should_stop)\n", "    def test_stop_condition_set(self):\n        reloader = autoreload.BaseReloader()\n        self.assertFalse(reloader.should_stop)\n        reloader.stop()\n        self.assertTrue(reloader.should_stop)\n", "    def test_no_exception(self):\n        # Should raise no exception if _exception is None\n        autoreload.raise_last_exception()\n", "def test_check_errors_wraps_function_metadata(self):\n    @autoreload.check_errors\n        \"\"\"Example docstring.\"\"\"\n        pass\n\n    self.assertEqual(example.__name__, 'example')\n    self.assertEqual(example.__doc__, 'Example docstring.')\n    self.assertIsInstance(example, types.FunctionType)\n"], "sample_44": ["def test_modelchoicefield_widget_context(self):\n    class CustomCheckboxSelectMultiple(CheckboxSelectMultiple):\n            context = super().get_context(name, value, attrs)\n            context['widget']['custom_data'] = 'test'\n            return context\n\n    class CustomModelMultipleChoiceField(forms.ModelMultipleChoiceField):\n        widget = CustomCheckboxSelectMultiple\n\n    field = CustomModelMultipleChoiceField(Category.objects.all())\n    self.assertHTMLEqual(\n        field.widget.render('name', []),\n        \"\"\"<div>", "def test_model_choice_iterator_value_repr(self):\n    value = ModelChoiceIteratorValue(self.c1.pk, self.c1)\n    self.assertEqual(repr(value), f\"ModelChoiceIteratorValue({self.c1.pk}, {self.c1!r})\")\n", "def test_modelchoicefield_choices_when_queryset_is_none(self):\n    f = forms.ModelChoiceField(queryset=None)\n    self.assertEqual(list(f.choices), [('', '---------')])\n    self.assertEqual(len(f.choices), 1)\n    self.assertIs(bool(f.choices), True)\n", "def test_model_choice_iterator_len(self):\n    f = forms.ModelChoiceField(Category.objects.all())\n    self.assertEqual(len(f.choices), 4)\n    Category.objects.create(name='Fourth', slug='4th', url='4th')\n    self.assertEqual(len(f.choices), 5)\n    Category.objects.get(url='4th').delete()\n    self.assertEqual(len(f.choices), 4)\n", "def test_model_choice_field_with_empty_queryset(self):\n    f = forms.ModelChoiceField(Category.objects.filter(name='Unknown'))\n    self.assertEqual(list(f.choices), [\n        ('', '---------'),\n    ])\n    with self.assertRaises(ValidationError):\n        f.clean('1')\n"], "sample_150": ["def test_solve_generic():\n    raises(NotImplementedError, lambda: solve_generic([x**2 + y], Options((x, y), {'domain': 'ZZ'})))\n    raises(NotImplementedError, lambda: solve_generic([x**3 - y**3], Options((x, y), {'domain': 'ZZ'})))\n\n    assert solve_generic([x - 1, y - 1], Options((x, y), {'domain': 'ZZ'})) == [(1, 1)]\n    assert solve_generic([x - 1, y - x], Options((x, y), {'domain': 'ZZ'})) == [(1, 1)]\n\n    f_1 = x**2 + y + z - 1\n    f_2 = x + y**2 + z - 1\n    f_3 = x + y + z**2 - 1\n\n    a, b = sqrt(2) - 1, -sqrt(2) - 1\n\n    assert solve_generic([f_1, f_2, f_3], Options((x, y, z), {'domain': 'ZZ'})) == \\\n        [(0, 0, 1), (0, 1, 0), (1, 0, 0)]\n\n    dom = QQ.algebraic_field(sqrt(2))\n\n    assert solve_generic([f_1, f_2, f_3], Options((x, y, z), {'domain': dom})) == \\\n        [(0, 0, 1), (0, 1, 0), (1, 0, 0), (a, a, a), (b, b, b)]\n", "def test_solve_generic():\n    assert solve_generic([x - 1], Options((x,), {'domain': 'ZZ'})) == [(1,)]\n    assert solve_generic([y - x, y - x - 1], Options((x, y), {'domain': 'ZZ'})) is None\n    assert solve_generic([y - x**2, y + x**2], Options((x, y), {'domain': 'ZZ'})) == [(0, 0)]\n    raises(NotImplementedError, lambda: solve_generic([x**3 - y**3], Options((x, y), {'domain': 'ZZ'})))\n    raises(PolynomialError, lambda: solve_generic([1/x], Options((x,), {'domain': 'ZZ'})))\n", "def test_solve_generic():\n    assert solve_generic([Poly(x - 1)], Options((x,), {'domain': 'ZZ'})) == [(1,)]\n    assert solve_generic([Poly(y - x), Poly(y - x - 1)], Options((x, y), {'domain': 'ZZ'})) is None\n    assert solve_generic([Poly(y - x**2), Poly(y + x**2)], Options((x, y), {'domain': 'ZZ'})) == [(0, 0)]\n\n    raises(NotImplementedError, lambda: solve_generic([Poly(x**3 - y**3)], Options((x, y), {'domain': 'ZZ'})))\n    raises(NotImplementedError, lambda: solve_generic(\n        [Poly(z), Poly(-2*x*y**2 + x + y**2*z), Poly(y**2*(-z - 4) + 2)],\n        Options((x, y, z), {'domain': 'ZZ'})))\n\n    raises(PolynomialError, lambda: solve_generic([Poly(1/x)], Options((x,), {'domain': 'ZZ'})))\n", "def test_solve_generic():\n    # Test case for a simple system of linear equations\n    f1 = Poly(x + y - 1, x, y)\n    f2 = Poly(x - y + 1, x, y)\n    assert solve_generic([f1, f2], Options((x, y), {'domain': 'ZZ'})) == [(-1, 0)]\n\n    # Test case for a system with a quadratic equation\n    f1 = Poly(x**2 + y - 1, x, y)\n    f2 = Poly(x + y - 1, x, y)\n    assert solve_generic([f1, f2], Options((x, y), {'domain': 'ZZ'})) == [(0, 1), (1, 0)]\n\n    # Test case for a system with no solutions\n    f1 = Poly(x + y + 1, x, y)\n    f2 = Poly(x + y + 2, x, y)\n    assert solve_generic([f1, f2], Options((x, y), {'domain': 'ZZ'})) is None\n\n    # Test case for a system with infinitely many solutions\n    f1 = Poly(x + y, x, y)\n    f2 = Poly(2*x + 2*y, x, y)\n    raises(NotImplementedError, lambda: solve_generic([f1, f2], Options((x, y), {'domain': 'ZZ'})))\n", "def test_solve_generic():\n    assert solve_generic([Poly(x - 1, x)], Options((x,), {'domain': 'ZZ'})) == [(1,)]\n    assert solve_generic([Poly(y - x, x, y), Poly(y - x - 1, x, y)], Options((x, y), {'domain': 'ZZ'})) is None\n\n    assert solve_generic([Poly(y - x**2, x, y), Poly(y + x**2, x, y)], Options((x, y), {'domain': 'ZZ'})) == [(0, 0)]\n\n    assert solve_generic([Poly(2*x - 3, x), Poly(y*Rational(3, 2) - 2*x, x, y), Poly(z - 5*y, x, y, z)], Options((x, y, z), {'domain': 'QQ'})) == \\\n        [(Rational(3, 2), Integer(2), Integer(10))]\n\n    raises(NotImplementedError, lambda: solve_generic([Poly(x**3 - y**3, x, y)], Options((x, y), {'domain': 'ZZ'})))\n    raises(NotImplementedError, lambda: solve_generic(\n        [Poly(z, x, y, z), Poly(-2*x*y**2 + x + y**2*z, x, y, z), Poly(y**2*(-z - 4) + 2, x, y, z)], Options((x, y, z), {'domain': 'ZZ'})))\n    raises(PolynomialError, lambda: solve_generic([Poly(1/x, x)], Options((x,), {'domain': 'ZZ'})))\n\n    raises(NotImplementedError, lambda: solve_generic([Poly(x-1, x)], Options((x, y), {'domain': 'ZZ'})))\n    raises(NotImplementedError, lambda: solve_generic([Poly(y-1, y)], Options((x, y), {'domain': 'ZZ'})))\n"], "sample_84": ["def test_getfuncargnames_class_with___init__(self):\n    class A(object):\n            pass\n\n            pass\n\n    assert fixtures.getfuncargnames(A().test_something) == (\"arg3\",)\n", "def test_fixture_requests_missing_dependency_issue_4270(testdir):\n    \"\"\"Test that the error message for a fixture that depends on a missing fixture is helpful (#4270).\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            pass\n\n            pass\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*The requested fixture has no parameter defined for test:*\",\n            \"    test_fixture_requests_missing_dependency_issue_4270.py::test_bar\",\n            \"\",\n            \"Requested fixture 'bar' defined in:\",\n            \"test_fixture_requests_missing_dependency_issue_4270.py:3\",\n            \"Requested here:\",\n            \"test_fixture_requests_missing_dependency_issue_4270.py:6\",\n            \"*1 error*\",\n        ]\n    )\n", "def test_fixtures_with_autouse_and_parametrize(testdir):\n    \"\"\"Test fixtures that use both autouse and parametrize.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(autouse=True, params=[1, 2])\n            return request.param\n\n            assert my_fixture in [1, 2]\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-v\")\n    result.stdout.fnmatch_lines([\"*test_my_test[1]*PASSED*\", \"*test_my_test[2]*PASSED*\"])\n", "def test_fixture_parametrize(request):\n    \"\"\"Check that parametrized fixtures work correctly.\"\"\"\n    values = []\n\n    @pytest.fixture(params=[1, 2])\n        values.append(request.param)\n        return request.param\n\n        pass\n\n    request.applymarker(pytest.mark.usefixtures(\"fix\"))\n    pytest._fillfuncargs(request._pyfuncitem)\n\n    assert values == [1, 2]\n", "def test_fixture_with_yield_before_setup(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            yield\n            assert 0, \"This should not be executed\"\n\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*RuntimeError: setup function 'my_fixture' yielded before setup*\"])\n"], "sample_134": ["def test_cbrt():\n    if not np:\n        skip(\"NumPy not installed\")\n    assert abs(lambdify((a,), Cbrt(a), 'numpy')(27) - 3) < 1e-16\n", "def test_codegen_array_contraction():\n    if not np:\n        skip(\"NumPy not installed\")\n\n    M = MatrixSymbol(\"M\", 2, 3)\n    N = MatrixSymbol(\"N\", 3, 4)\n    P = MatrixSymbol(\"P\", 4, 5)\n\n    cg = CodegenArrayContraction(CodegenArrayTensorProduct(M, N, P), (1, 2))\n    f = lambdify((M, N, P), cg, 'numpy')\n\n    ma = np.matrix([[1, 2, 3], [4, 5, 6]])\n    mb = np.matrix([[7, 8, 9, 10], [11, 12, 13, 14], [15, 16, 17, 18]])\n    mc = np.matrix([[19, 20, 21, 22, 23], [24, 25, 26, 27, 28], [29, 30, 31, 32, 33], [34, 35, 36, 37, 38]])\n\n    assert (f(ma, mb, mc) == np.einsum(ma, [0, 1], mb, [1, 2], mc, [2, 3])).all()\n", "def test_codegen_array_elementwise_add():\n    if not np:\n        skip(\"NumPy not installed\")\n\n    M = MatrixSymbol(\"M\", 2, 2)\n    N = MatrixSymbol(\"N\", 2, 2)\n    P = MatrixSymbol(\"P\", 2, 2)\n\n    ma = np.matrix([[1, 2], [3, 4]])\n    mb = np.matrix([[1,-2], [-1, 3]])\n    mc = np.matrix([[2, 0], [1, 2]])\n\n    cg = CodegenArrayElementwiseAdd(M, N)\n    f = lambdify((M, N), cg, 'numpy')\n    assert (f(ma, mb) == ma+mb).all()\n\n    cg = CodegenArrayElementwiseAdd(M, N, P)\n    f = lambdify((M, N, P), cg, 'numpy')\n    assert (f(ma, mb, mc) == ma+mb+mc).all()\n", "def test_codegen_array_diagonal():\n    if not np:\n        skip(\"NumPy not installed\")\n\n    M = MatrixSymbol(\"M\", 3, 4)\n    N = MatrixSymbol(\"N\", 4, 5)\n    ma = np.matrix([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\n    mb = np.matrix([[13, 14, 15, 16, 17], [18, 19, 20, 21, 22],\n                    [23, 24, 25, 26, 27], [28, 29, 30, 31, 32]])\n\n    cg = CodegenArrayDiagonal(CodegenArrayTensorProduct(M, N), (0, 2))\n    f = lambdify((M, N), cg, 'numpy')\n    assert (f(ma, mb) == np.einsum(ma, [0, 1], mb, [1, 2])[np.arange(3), np.arange(3), :]).all()\n", "def test_issue_cbrt():\n    if not np:\n        skip(\"NumPy not installed\")\n    assert abs(lambdify((a,), Cbrt(a), 'numpy')(27) - 3) < 1e-16\n"], "sample_60": ["def test_serialize_model_instance(self):\n    model = models.Model()\n    with self.assertRaisesMessage(ValueError, \"Cannot serialize: <Model object>\"):\n        self.assertSerializedEqual(model)\n", "def test_serialize_flags(self):\n    # Test serialization of re flags.\n    pattern = re.compile(r\"^foo$\", re.I | re.M)\n    string, imports = MigrationWriter.serialize(pattern)\n    self.assertEqual(\n        string,\n        \"re.compile('^foo$', re.RegexFlag['IGNORECASE'] | re.RegexFlag['MULTILINE'])\",\n    )\n    self.assertEqual(imports, {\"import re\"})\n", "def test_serialize_flag_enum_with_multiple_members(self):\n    self.assertSerializedResultEqual(\n        IntFlagEnum.A | IntFlagEnum.B,\n        (\n            \"migrations.test_writer.IntFlagEnum['B'] | \"\n            \"migrations.test_writer.IntFlagEnum['A']\",\n            {\"import migrations.test_writer\"},\n        ),\n    )\n", "def test_serialize_path(self):\n    self.assertSerializedResultEqual(\n        pathlib.PurePath,\n        (\"pathlib.PurePath\", {\"import pathlib\"}),\n    )\n    self.assertSerializedResultEqual(\n        os.PathLike,\n        (\"os.PathLike\", {\"import os\"}),\n    )\n    path = pathlib.Path(\"/path/to/file.txt\")\n    expected = (\"pathlib.PurePath('/path/to/file.txt')\", {\"import pathlib\"})\n    self.assertSerializedResultEqual(path, expected)\n", "def test_serialize_flags_enum_member(self):\n    class FlagsEnum(enum.Flag):\n        A = 1\n        B = 2\n\n    self.assertSerializedResultEqual(\n        FlagsEnum.A,\n        (\n            \"migrations.test_writer.WriterTests.test_serialize_flags_enum_member.<locals>.FlagsEnum['A']\",\n            {\"import migrations.test_writer\"},\n        ),\n    )\n    self.assertSerializedResultEqual(\n        FlagsEnum.B,\n        (\n            \"migrations.test_writer.WriterTests.test_serialize_flags_enum_member.<locals>.FlagsEnum['B']\",\n            {\"import migrations.test_writer\"},\n        ),\n    )\n    self.assertSerializedResultEqual(\n        FlagsEnum.A | FlagsEnum.B,\n        (\n            \"migrations.test_writer.WriterTests.test_serialize_flags_enum_member.<locals>.FlagsEnum['A'] | \"\n            \"migrations.test_writer.WriterTests.test_serialize_flags_enum_member.<locals>.FlagsEnum['B']\",\n            {\"import migrations.test_writer\"},\n        ),\n    )\n"], "sample_47": ["def test_migration_plan_with_replaced(self):\n    \"\"\"\n    Tests running a migration plan with replaced migrations.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    # Load the graph with replaced migrations\n    executor.loader.replace_migrations = True\n    executor.loader.build_graph()\n    # Get the migration plan\n    plan = executor.migration_plan([(\"migrations\", \"0002_second\")])\n    self.assertEqual(\n        plan,\n        [\n            (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], False),\n            (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], False),\n        ],\n    )\n    # Make sure the replaced migration is not in the plan\n    self.assertNotIn((\"migrations\", \"0001_replaced\"), [n[0] for n in plan])\n", "def test_custom_user_soft_applied(self):\n    \"\"\"\n    Tests detection of initial custom user migration already having been applied.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    self.assertTableNotExists('migrations_author')\n    self.assertTableNotExists('migrations_tribble')\n    # Run it normally\n    self.assertEqual(\n        executor.migration_plan([(\"migrations\", \"0001_initial\")]),\n        [\n            (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], False),\n        ],\n    )\n    executor.migrate([(\"migrations\", \"0001_initial\")])\n    # Are the tables there now?\n    self.assertTableExists('migrations_author')\n    self.assertTableExists('migrations_tribble')\n    # Fake-reverse that\n    executor.migrate([(\"migrations\", None)], fake=True)\n    # Make sure the soft-application detection works\n    migration = executor.loader.get_migration('migrations', '0001_initial')\n    self.assertIs(executor.detect_soft_applied(None, migration)[0], True)\n    # Finally, migrate forwards; this should fake-apply our initial migration\n    executor.loader.build_graph()\n    self.assertEqual(\n        executor.migration_plan([(\"migrations\", \"0001_initial\")]),\n        [\n            (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], False),\n        ],\n    )\n    executor.migrate([(\"migrations\", \"0001_initial\")], fake_initial=True)\n    # And migrate back to clean up the database\n    executor.loader.build_graph()\n    executor.migrate([(\"migrations\", None)])\n    self.assertTableNotExists('migrations_author')\n    self.assertTableNotExists('migrations_tribble')\n", "def test_migration_plan_with_replaced_migrations(self):\n    \"\"\"\n    Tests that replaced migrations are not included in the migration plan.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    # Let's look at the plan first and make sure it's up to scratch\n    plan = executor.migration_plan([(\"migrations\", \"0002_second\")])\n    self.assertEqual(\n        plan,\n        [\n            (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], False),\n            (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], False),\n        ],\n    )\n    # Now let's replace 0001_initial with a new migration\n    executor.loader.replace_migrations = True\n    executor.loader.build_graph()\n    plan = executor.migration_plan([(\"migrations\", \"0002_second\")])\n    self.assertEqual(\n        plan,\n        [\n            (executor.loader.graph.nodes[\"migrations\", \"0001_squashed_0002\"], False),\n        ],\n    )\n", "def test_migration_plan_with_replaced_migrations(self):\n    \"\"\"\n    Test migration plan with replaced migrations.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    # Load the graph without replacements\n    executor.loader.replace_migrations = False\n    executor.loader.build_graph()\n    # Create a fake migration history where 0001_initial is applied but not 0002_second\n    executor.recorder.record_applied(\"migrations\", \"0001_initial\")\n    # Now load the graph with replacements\n    executor.loader.replace_migrations = True\n    executor.loader.build_graph()\n    # The plan should only include the squashed migration\n    plan = executor.migration_plan([(\"migrations\", \"0002_second\")])\n    self.assertEqual(\n        plan,\n        [\n            (executor.loader.graph.nodes[\"migrations\", \"0001_squashed_0002\"], False),\n        ],\n    )\n", "def test_migrate_all_forwards(self):\n    \"\"\"\n    Test that _migrate_all_forwards() applies all migrations in the correct order.\n    \"\"\"\n    a1_impl = FakeMigration('a1')\n    a1 = ('a', '1')\n    a2_impl = FakeMigration('a2')\n    a2 = ('a', '2')\n    b1_impl = FakeMigration('b1')\n    b1 = ('b', '1')\n    graph = MigrationGraph()\n    graph.add_node(a1, a1_impl)\n    graph.add_node(a2, a2_impl)\n    graph.add_node(b1, b1_impl)\n    graph.add_dependency(None, a2, a1)\n    graph.add_dependency(None, b1, a1)\n\n    executor = MigrationExecutor(None)\n    executor.loader = FakeLoader(graph, {\n        a1: a1_impl,\n        b1: b1_impl,\n    })\n\n    state = ProjectState()\n    migrations_to_run = {a1_impl, a2_impl, b1_impl}\n    plan = [(a1_impl, False), (b1_impl, False), (a2_impl, False)]\n\n    executor._migrate_all_forwards(state, plan, [(a1_impl, False), (b1_impl, False), (a2_impl, False)], fake=False, fake_initial=False)\n\n    self.assertEqual(migrations_to_run, set())\n    self.assertEqual(executor.loader.applied_migrations, {a1: a1_impl, b1: b1_impl, a2: a2_impl})\n"], "sample_10": ["def test_nested_outerref_lhs_with_selected_columns(self):\n    tag = Tag.objects.create(name=self.au1.alias)\n    tag.articles.add(self.a1)\n    qs = Tag.objects.annotate(\n        has_author_alias_match=Exists(\n            Article.objects.values('author').annotate(\n                author_exists=Exists(\n                    Author.objects.filter(alias=OuterRef(OuterRef('name')))\n                ),\n            ).filter(author_exists=True)\n        ),\n    )\n    self.assertEqual(qs.get(has_author_alias_match=True), tag)\n", "def test_chain_date_time_lookups_with_invalid_lookup(self):\n    msg = \"Unsupported lookup 'blah' for DateTimeField or join on the field not permitted.\"\n    with self.assertRaisesMessage(FieldError, msg):\n        Article.objects.filter(pub_date__month__blah=7)\n", "def test_lookup_with_bilateral_transforms(self):\n    # Create a bilateral transform that squares a value.\n    class Square(Transform):\n        bilateral = True\n\n            lhs, lhs_params = compiler.compile(self.lhs)\n            return \"POWER(%s, 2)\" % lhs, lhs_params\n\n    # Register the bilateral transform on IntegerField.\n    IntegerField.register_lookup(Square)\n\n    # Use the bilateral transform in a lookup.\n    self.assertEqual(\n        Season.objects.filter(year__square=2012**2).count(),\n        1\n    )\n", "def test_lookup_with_transform_and_subqueries(self):\n    # Test that lookups with transforms work correctly with subqueries.\n    authors = Author.objects.filter(name='Author 1')\n    articles = Article.objects.filter(author__in=authors).values_list('author', flat=True)\n    self.assertEqual(list(articles), [self.au1.id, self.au1.id, self.au1.id, self.au1.id])\n", "def test_exists_with_subquery_aggregation(self):\n    max_pub_date = Article.objects.aggregate(Max('pub_date'))['pub_date__max']\n    latest_articles = Article.objects.filter(pub_date=max_pub_date)\n    authors_with_latest_articles = Author.objects.annotate(\n        has_latest_article=Exists(latest_articles.filter(author=OuterRef('pk')))\n    ).filter(has_latest_article=True)\n    self.assertCountEqual(authors_with_latest_articles, [self.au2])\n"], "sample_61": ["def test_non_uniform_digit_grouping(self):\n    self.assertEqual(\n        nformat(123456789, \".\", grouping=(3, 2, 0), thousand_sep=\",\"),\n        \"123,45,6789\"\n    )\n    self.assertEqual(\n        nformat(123456789, \".\", grouping=(3, 2, 0), thousand_sep=\",\", force_grouping=True),\n        \"123,45,6789\"\n    )\n    self.assertEqual(\n        nformat(123456789, \".\", grouping=(3, 3, 0), thousand_sep=\",\"),\n        \"123,456,789\"\n    )\n    self.assertEqual(\n        nformat(123456789, \".\", grouping=(3, 3, 0), thousand_sep=\",\", force_grouping=True),\n        \"123,456,789\"\n    )\n", "def test_non_uniform_digit_grouping(self):\n    self.assertEqual(\n        nformat(123456789, \".\", grouping=(3, 2, 0), thousand_sep=\",\", force_grouping=True),\n        \"123,45,6789\"\n    )\n    self.assertEqual(\n        nformat(123456789, \".\", grouping=(3, 2, 0), thousand_sep=\".\", force_grouping=True),\n        \"123.45.6789\"\n    )\n    self.assertEqual(\n        nformat(123456789, \".\", grouping=(4, 3, 0), thousand_sep=\",\", force_grouping=True),\n        \"1234,567,789\"\n    )\n    self.assertEqual(\n        nformat(123456789, \".\", grouping=(4, 3, 0), thousand_sep=\".\", force_grouping=True),\n        \"1234.567.789\"\n    )\n", "def test_non_uniform_grouping(self):\n    self.assertEqual(\n        nformat(1234567, \".\", grouping=(3, 2), thousand_sep=\",\", force_grouping=True),\n        \"12,34,567\",\n    )\n    self.assertEqual(\n        nformat(1234567890, \".\", grouping=(3, 2, 0), thousand_sep=\",\", force_grouping=True),\n        \"1,23,45,678,90\",\n    )\n    self.assertEqual(\n        nformat(\n            1234567890123456,\n            \".\",\n            grouping=(3, 2, 0),\n            thousand_sep=\",\",\n            force_grouping=True,\n        ),\n        \"1,23,45,678,901,234,56\",\n    )\n    self.assertEqual(\n        nformat(\n            -1234567890123456,\n            \".\",\n            grouping=(3, 2, 0),\n            thousand_sep=\",\",\n            force_grouping=True,\n        ),\n        \"-1,23,45,678,901,234,56\",\n    )\n    self.assertEqual(\n        nformat(1234, \".\", grouping=(3, 2), thousand_sep=\",\", force_grouping=False),\n        \"1234\",\n    )\n", "def test_non_uniform_grouping(self):\n    self.assertEqual(\n        nformat(123456789, \".\", grouping=(3, 2, 0), thousand_sep=\",\"),\n        \"123,45,6789\",\n    )\n    self.assertEqual(\n        nformat(123456789, \".\", grouping=(3, 2), thousand_sep=\",\", force_grouping=True),\n        \"123,45,67,89\",\n    )\n    self.assertEqual(\n        nformat(123456789, \".\", grouping=(3,), thousand_sep=\",\", force_grouping=True),\n        \"123,456,789\",\n    )\n", "def test_non_uniform_grouping(self):\n    self.assertEqual(\n        nformat(1234567, \".\", grouping=(3, 2, 0), thousand_sep=\",\", force_grouping=True),\n        \"12,34,567\"\n    )\n    self.assertEqual(\n        nformat(123456789, \".\", grouping=(3, 2, 0), thousand_sep=\",\", force_grouping=True),\n        \"123,45,6789\"\n    )\n    self.assertEqual(\n        nformat(123456789012, \".\", grouping=(3, 2, 0), thousand_sep=\",\", force_grouping=True),\n        \"1,234,56,789,012\"\n    )\n"], "sample_9": ["    def test_trigger_reload(self, mock_exit):\n        autoreload.trigger_reload('/path/to/file.py')\n        self.assertEqual(mock_exit.call_count, 1)\n        self.assertEqual(mock_exit.call_args[0][0], 3)\n", "    def test_ensure_echo_on_calls_tcsetattr(self, mocked_termios):\n        with mock.patch('sys.stdin.isatty', return_value=True):\n            autoreload.ensure_echo_on()\n        self.assertEqual(mocked_termios.tcsetattr.call_count, 1)\n", "    def test_tty(self):\n        with mock.patch('sys.stdin.isatty', return_value=True):\n            with mock.patch('termios.tcgetattr') as tcgetattr:\n                with mock.patch('termios.tcsetattr') as tcsetattr:\n                    autoreload.ensure_echo_on()\n                    self.assertTrue(tcgetattr.called)\n                    self.assertTrue(tcsetattr.called)\n", "    def test_autoreload_started_signal_sent(self, mocked_send):\n        reloader = autoreload.BaseReloader()\n        reloader.run(mock.MagicMock())\n        self.assertEqual(mocked_send.call_count, 1)\n        self.assertSequenceEqual(mocked_send.call_args[0], [reloader])\n", "    def test_ensure_echo_on_with_termios(self, mocked_termios):\n        attrs = [0, 0, 0, 0]\n        mocked_termios.tcgetattr.return_value = attrs\n        with mock.patch('sys.stdin.isatty', return_value=True):\n            autoreload.ensure_echo_on()\n        self.assertEqual(mocked_termios.tcgetattr.call_count, 1)\n        self.assertEqual(mocked_termios.tcsetattr.call_count, 1)\n        mocked_termios.tcsetattr.assert_called_once_with(sys.stdin, mocked_termios.TCSANOW, attrs)\n"], "sample_146": ["def test_MatrixBase_str():\n    M = Matrix([[x**+1, 1], [y, x + y]])\n    assert str(M) == \"Matrix([[x, 1], [y, x + y]])\"\n    assert sstr(M) == \"Matrix([\\n[x,     1],\\n[y, x + y]])\"\n    M = Matrix([[1]])\n    assert str(M) == sstr(M) == \"Matrix([[1]])\"\n    M = Matrix()\n    assert str(M) == sstr(M) == \"Matrix(0, 0, [])\"\n", "def test_printing_str_tensor():\n    from sympy.tensor import Tensor, IndexedBase\n    A = IndexedBase(\"A\")\n    x, y, z = symbols('x y z')\n    T = Tensor(A[x, y]*A[y, z])\n    assert sstr(T) == \"A[x, y]*A[y, z]\"\n", "def test_printing_str_Quaternion():\n    assert sstr(Quaternion(x, y, z, t)) == \"x + y*i + z*j + t*k\"\n    assert sstr(Quaternion(x, y, z, x*t)) == \"x + y*i + z*j + t*x*k\"\n    assert sstr(Quaternion(x, y, z, x+t)) == \"x + y*i + z*j + (t + x)*k\"\n", "def test_Dimension():\n    from sympy.physics.units import Dimension\n    assert str(Dimension(\"length\")) == \"length\"\n", "def test_MatrixBase():\n    from sympy.matrices import MatrixBase\n    A = MatrixBase([[1, 2], [3, 4]])\n    assert str(A) == \"Matrix([[1, 2], [3, 4]])\"\n"], "sample_3": ["def test_separable_single_input_multi_output():\n    # Test the case where a model has one input and multiple outputs\n    model = models.Polynomial2D(1)\n    assert_allclose(is_separable(model), np.array([False, False]))\n    assert_allclose(separability_matrix(model), np.ones((2, 1), dtype=np.bool_))\n", "def test_separable_single_input_multi_output():\n    # Test that a model with a single input and multiple outputs is not separable\n    model = models.Polynomial1D(1, n_outputs=2)\n    assert not np.any(is_separable(model))\n    assert np.all(separability_matrix(model) == np.ones((2, 1)))\n", "def test_separable_single_input_multi_output():\n    # Test that a model with 1 input and multiple outputs is not separable\n    model = models.Polynomial2D(1)\n    assert np.all(is_separable(model) == [False, False])\n    assert np.all(separability_matrix(model) == [[True], [True]])\n", "def test_separable_with_one_input_and_multiple_outputs():\n    model = models.Polynomial2D(1)\n    is_separable_result = is_separable(model)\n    assert np.all(is_separable_result == [False, False])\n    separability_matrix_result = separability_matrix(model)\n    assert np.all(separability_matrix_result == [[True], [True]])\n", "def test_separable_single_input_model():\n    # Test that a model with a single input and multiple outputs is not separable\n    model = custom_model(lambda x: (x, x))(name='single_input_multi_output')\n    assert not np.any(is_separable(model))\n"], "sample_8": ["    def test_get_safe_settings(self):\n        settings.DATABASES['default']['PASSWORD'] = 'secret'\n        safe_settings = get_safe_settings()\n        self.assertEqual(safe_settings['DATABASES']['default']['PASSWORD'], CLEANSED_SUBSTITUTE)\n", "    def test_cleanse_setting_for_callable(self):\n            return \"This should not be displayed\"\n        cleansed = cleanse_setting('SETTING_NAME', callable_setting)\n        self.assertIsInstance(cleansed, CallableSettingWrapper)\n        self.assertEqual(repr(cleansed), repr(callable_setting))\n", "    def test_cleanse_setting_dictionary_with_non_str_key(self):\n        initial = {42: 'foo', 'bar': 'baz'}\n        self.assertEqual(cleanse_setting('SETTING_NAME', initial), initial)\n", "    def test_cleanse_setting_dict_with_non_string_key(self):\n        initial = {1: 'value'}\n        self.assertEqual(cleanse_setting('SETTING_NAME', initial), initial)\n", "    def test_cleanse_setting_with_callable(self):\n        self.assertEqual(cleanse_setting('SETTING_NAME', lambda: 'test'), CallableSettingWrapper(lambda: 'test'))\n"], "sample_133": ["def test_octave_code():\n    x = symbols('x')\n    expr = x**3\n    expected = [\n        (\"file.m\",\n        \"% File generated by SymPy\\n\"\n        \"\\n\"\n        \"function result = test(x)\\n\"\n        \"  result = x.^3;\\n\"\n        \"end\\n\"),\n    ]\n    result = codegen((\"test\", expr), \"octave\",\"file\", header=False, empty=False)\n    assert result == expected\n", "def test_octave_code():\n    x, y, z = symbols('x,y,z')\n    expr = (x + y)*z\n    routine = make_routine(\"test\", expr)\n    code_gen = OctaveCodeGen()\n    source = get_string(code_gen.dump_m, [routine])\n    expected = (\n        \"function out = test(x, y, z)\\n\"\n        \"  out = z.*(x + y);\\n\"\n        \"end\\n\"\n    )\n    assert source == expected\n", "def test_octave_codegen():\n    x, y = symbols('x,y')\n    name_expr = (\"test\", (x + y)*y)\n    result = codegen(name_expr, \"octave\", header=False, empty=False)\n    expected = (\n        '# -*- octave -*-\\n'\n        'function test(x, y)\\n'\n        '  test = y.*(x + y);\\n'\n        'end\\n'\n    )\n    assert result[0][1] == expected\n", "def test_rust_code():\n    x, y, z = symbols('x,y,z')\n    expr = (x + y)*z\n    routine = make_routine(\"test\", expr)\n    code_gen = RustCodeGen()\n    source = get_string(code_gen.dump_rs, [routine])\n    expected = (\n        \"fn test(x: f64, y: f64, z: f64) -> f64 {\\n\"\n        \"   let test_result = z*(x + y);\\n\"\n        \"   return test_result;\\n\"\n        \"}\\n\"\n    )\n    assert source == expected\n", "def test_julia_code():\n    x, y, z = symbols('x,y,z')\n    expr = (x + y)*z\n    routine = make_routine(\"test\", expr)\n    code_gen = JuliaCodeGen()\n    source = get_string(code_gen.dump_jl, [routine])\n    expected = (\n        \"function test(x, y, z)\\n\"\n        \"   test_result = z*(x + y)\\n\"\n        \"   return test_result\\n\"\n        \"end\\n\"\n    )\n    assert source == expected\n"], "sample_105": ["def test_voting_regressor_get_params():\n    \"\"\"Test that VotingRegressor.get_params returns the correct estimators.\"\"\"\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor()\n    ereg = VotingRegressor([('lr', reg1), ('rf', reg2)])\n    params = ereg.get_params()\n    assert 'lr' in params\n    assert 'rf' in params\n    assert params['lr'] is reg1\n    assert params['rf'] is reg2\n", "def test_voting_regressor_get_params():\n    \"\"\"Check get_params of VotingRegressor returns the correct estimators.\"\"\"\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor()\n    ereg = VotingRegressor([('lr', reg1), ('rf', reg2)])\n    params = ereg.get_params()\n    assert 'estimators' in params\n    estimators = params['estimators']\n    assert isinstance(estimators, list)\n    assert len(estimators) == 2\n    assert estimators[0][0] == 'lr'\n    assert estimators[0][1] is reg1\n    assert estimators[1][0] == 'rf'\n    assert estimators[1][1] is reg2\n", "def test_voting_regressor_transform():\n    \"\"\"Check transform method of VotingRegressor on toy dataset.\"\"\"\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor(random_state=123)\n    X = np.array([[-1.1, -1.5], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])\n    y = np.array([1, 1, 2, 2])\n\n    ereg = VotingRegressor(estimators=[\n        ('lr', reg1), ('rf', reg2)],\n        ).fit(X, y)\n\n    assert_array_equal(ereg.transform(X).shape, (4, 2))\n    assert_array_almost_equal(ereg.transform(X),\n                              np.column_stack((reg1.fit(X, y).predict(X),\n                                              reg2.fit(X, y).predict(X))))\n", "def test_voting_regressor_predict_shape():\n    \"\"\"Check the shape of the prediction made by VotingRegressor.\"\"\"\n    X, y = datasets.make_regression(n_samples=100, n_features=5)\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor()\n    ereg = VotingRegressor([('lr', reg1), ('rf', reg2)])\n    ereg.fit(X, y)\n    y_pred = ereg.predict(X)\n    assert y_pred.shape == (100,)\n", "def test_voting_regressor_with_sample_weight():\n    \"\"\"Test VotingRegressor with sample weights.\"\"\"\n    X, y = datasets.make_regression(n_samples=100, n_features=5, random_state=42)\n    reg1 = LinearRegression()\n    reg2 = RandomForestRegressor(n_estimators=5)\n    voter = VotingRegressor([('lr', reg1), ('rf', reg2)])\n\n    sample_weight = np.random.RandomState(42).uniform(size=(len(y),))\n    voter.fit(X, y, sample_weight)\n\n    y_pred = voter.predict(X)\n    assert y_pred.shape == y.shape\n\n    # Check that the weights are used by the individual estimators\n    for est in voter.estimators_:\n        if hasattr(est, 'sample_weight'):\n            assert est.sample_weight is not None\n"], "sample_59": ["def test_absolute_max_with_min_num(self):\n    data = {\n        \"form-TOTAL_FORMS\": \"1001\",\n        \"form-INITIAL_FORMS\": \"0\",\n        \"form-MIN_NUM_FORMS\": \"1001\",\n        \"form-MAX_NUM_FORMS\": \"1001\",\n    }\n    LimitedFavoriteDrinksFormSet = formset_factory(\n        FavoriteDrinkForm,\n        min_num=1001,\n        max_num=1001,\n        absolute_max=1000,\n    )\n    formset = LimitedFavoriteDrinksFormSet(data)\n    self.assertIs(formset.is_valid(), False)\n    self.assertEqual(len(formset.forms), 1000)\n    self.assertEqual(\n        formset.non_form_errors(),\n        [\"Please submit at most 1000 forms.\"],\n    )\n", "def test_template_name_div(self):\n    \"\"\"Test template_name_div is used when rendering formset as div.\"\"\"\n    class CustomFormSet(BaseFormSet):\n        template_name_div = \"custom/formset_div.html\"\n\n    ChoiceFormSet = formset_factory(Choice, formset=CustomFormSet)\n    formset = ChoiceFormSet()\n    with self.assertTemplateUsed(\"custom/formset_div.html\"):\n        formset.as_div()\n", "def test_formset_media_with_empty_widget(self):\n    \"\"\"Media is correctly collected for a formset with a form that has an empty widget.\"\"\"\n\n    class EmptyWidget(forms.Widget):\n        pass\n\n    class MediaForm(Form):\n        field = forms.Field(widget=EmptyWidget)\n\n    self.assertEqual(formset_factory(MediaForm)().media, forms.Media())\n", "def test_add_fields_modifies_form_fields(self):\n    \"\"\"add_fields() can modify the fields of a form.\"\"\"\n\n    class AddFieldsFormSet(BaseFormSet):\n            super().add_fields(form, index)\n            if \"name\" in form.fields:\n                form.fields[\"name\"].required = False\n\n    FormSet = formset_factory(FavoriteDrinkForm, formset=AddFieldsFormSet)\n    formset = FormSet()\n    self.assertFalse(formset.forms[0].fields[\"name\"].required)\n", "    def test_template_names(self):\n        formset = ChoiceFormSet()\n        self.assertEqual(formset.template_name, \"django/forms/formsets/default.html\")\n        self.assertEqual(formset.template_name_div, \"django/forms/formsets/div.html\")\n        self.assertEqual(formset.template_name_p, \"django/forms/formsets/p.html\")\n        self.assertEqual(formset.template_name_table, \"django/forms/formsets/table.html\")\n        self.assertEqual(formset.template_name_ul, \"django/forms/formsets/ul.html\")\n"], "sample_141": ["def test_convert_to_with_symbolic_exponents():\n    expr = speed_of_light**Symbol('n')\n    conv = convert_to(expr, meter/second)\n    assert conv == 299792458**Symbol('n') * (meter/second)**Symbol('n')\n", "def test_check_dimensions():\n    u = Quantity(\"u\")\n    v = Quantity(\"v\")\n    w = Quantity(\"w\")\n\n    u.set_global_relative_scale_factor(S(10), meter)\n    v.set_global_relative_scale_factor(S(5), meter)\n    w.set_global_relative_scale_factor(S(2), second)\n\n    assert check_dimensions(u + v) == u + v\n    assert check_dimensions(u - v) == u - v\n    raises(ValueError, lambda: check_dimensions(u + w))\n    raises(ValueError, lambda: check_dimensions(u - w))\n    raises(ValueError, lambda: check_dimensions(u + 1))\n    raises(ValueError, lambda: check_dimensions(u - 1))\n", "def test_convert_to_multiple_units_with_powers():\n    q = Quantity(\"q1\")\n    q.set_global_relative_scale_factor(S(5000), meter)\n\n    assert convert_to(q**2, [meter, second]) == 25000000*meter**2\n    assert convert_to(q**-1, [meter, second]) == Rational(1, 5000)*meter**-1\n    assert convert_to(q**0.5, [meter, second]) == 10*sqrt(5)*meter**0.5\n", "def test_check_dimensions():\n    u = Quantity(\"u\")\n    v = Quantity(\"v\")\n    w = Quantity(\"w\")\n\n    u.set_global_relative_scale_factor(S(10), meter)\n    v.set_global_relative_scale_factor(S(5), meter)\n    w.set_global_relative_scale_factor(S(2), second)\n\n    assert check_dimensions(u + v) == u + v\n    raises(ValueError, lambda: check_dimensions(u + w))\n    raises(ValueError, lambda: check_dimensions(u - w))\n    raises(ValueError, lambda: check_dimensions(u + 1))\n    raises(ValueError, lambda: check_dimensions(u - 1))\n", "def test_convert_to_with_prefix():\n    q = Quantity(\"q1\")\n    q.set_global_relative_scale_factor(S(5000), meter)\n\n    assert q.convert_to(kilometer) == kilo*q.convert_to(meter)\n    assert convert_to(q, kilometer) == kilo*convert_to(q, meter)\n    assert q.convert_to(millimeter) == milli*q.convert_to(meter)\n    assert convert_to(q, millimeter) == milli*convert_to(q, meter)\n"], "sample_140": ["def test_auto_point_acc():\n    t = dynamicsymbols._t\n    q1, q2 = dynamicsymbols('q1 q2')\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    O = Point('O')\n    O.set_vel(N, q1 * N.x)\n    O.set_acc(N, q1.diff(t) * N.x)\n    P = Point('P')\n    P.set_pos(O, q2 * N.y)\n    assert P.acc(N) == q2.diff(t, 2) * N.y + q1.diff(t) * N.x\n", "def test_auto_point_vel_multiple_paths():\n    t = dynamicsymbols._t\n    q1, q2, u1, u2 = dynamicsymbols('q1 q2 u1 u2')\n    B = ReferenceFrame('B')\n    P = Point('P')\n    P.set_vel(B, u1 * B.x)\n    P1 = Point('P1')\n    P1.set_pos(P, q2 * B.y)\n    P1.set_vel(B, q1 * B.z)\n    P2 = Point('P2')\n    P2.set_pos(P1, q1 * B.z)\n    P3 = Point('P3')\n    P3.set_pos(P2, 10 * q1 * B.y)\n    O = Point('O')\n    O.set_vel(B, u2 * B.y)\n    O1 = Point('O1')\n    O1.set_pos(O, q2 * B.z)\n    P3.set_pos(O1, q1 * B.x + q2 * B.z)\n    P4 = Point('P4')\n    P4.set_pos(P3, q1 * B.x)\n    P4.set_pos(O, q2 * B.y)\n    assert P4.vel(B) == (u2 + q2.diff(t)) * B.y + 2 * q1.diff(t) * B.x + q1.diff(t) * B.z\n", "def test_auto_point_vel_multiple_frame_path():\n    t = dynamicsymbols._t\n    q1, q2, u1, u2 = dynamicsymbols('q1 q2 u1 u2')\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    S = ReferenceFrame('S')\n    O = Point('O')\n    O.set_vel(N, u1 * N.x)\n    P = Point('P')\n    P.set_pos(O, q2 * B.y)\n    P.set_vel(B, q1 * B.z)\n    Q = Point('Q')\n    Q.set_pos(P, q1 * S.z)\n    Q.set_vel(S, u2 * S.x)\n    raises(ValueError, lambda : Q.vel(N)) # B's orientation wrt N not defined\n    B.orient(N, 'Axis', (q1, N.z))\n    raises(ValueError, lambda : Q.vel(N)) # S's orientation wrt N/B not defined\n    S.orient(B, 'Axis', (q2, B.y))\n    assert Q.vel(N) == u1 * N.x + 2 * q1.diff(t) * B.z - q2.diff(t) * q1 * B.x + u2 * S.x\n", "def test_point_vel_with_acc():\n    q1, q2 = dynamicsymbols('q1 q2')\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    Q = Point('Q')\n    O = Point('O')\n    Q.set_pos(O, q1 * N.x)\n    O.set_vel(N, q2 * N.y)\n    Q.set_acc(N, 0)\n    assert Q.vel(N) == q1.diff(dynamicsymbols._t) * N.x + q2 * N.y\n", "def test_point_partial_velocity_multiple_frames():\n    N = ReferenceFrame('N')\n    A = ReferenceFrame('A')\n    B = ReferenceFrame('B')\n\n    p = Point('p')\n\n    u1, u2 = dynamicsymbols('u1, u2')\n\n    p.set_vel(N, u1 * A.x + u2 * B.y)\n\n    assert p.partial_velocity(N, u1) == A.x\n    assert p.partial_velocity(N, u2) == B.y\n    raises(ValueError, lambda: p.partial_velocity(A, u1))\n    raises(ValueError, lambda: p.partial_velocity(B, u2))\n"], "sample_28": ["    def setUpTestData(cls):\n        cls.u1 = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n", "    def setUp(self):\n        self.site = admin.AdminSite()\n", "    def setUpTestData(cls):\n        cls.u1 = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n", "    def setUp(self):\n        self.site = admin.AdminSite()\n", "    def setUpTestData(cls):\n        cls.u1 = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n"], "sample_20": ["    def test_model_name_clashes_with_field_accessor(self):\n        class Model(models.Model):\n            foo = models.ForeignKey('Foo', models.CASCADE)\n\n        class Foo(models.Model):\n            pass\n\n        self.assertEqual(Model.check(), [\n            Error(\n                \"The model name 'Model' clashes with a related field accessor \"\n                \"for 'foo'.\",\n                obj=Model,\n                id='models.E028',\n            )\n        ])\n", "    def test_model_with_invalid_attribute(self):\n        class Model(models.Model):\n            field = models.CharField(max_length=100)\n\n            class Meta:\n                db_table = 'myapp_mymodel'\n\n            invalid_attribute = 'invalid_value'\n\n        self.assertEqual(Model.check(), [\n            Error(\n                \"The model Model has an attribute 'invalid_attribute' which \"\n                \"is not a valid Python identifier or keyword.\",\n                obj=Model,\n                id='models.E029',\n            ),\n        ])\n", "    def test_abstract_base_classes(self):\n        class AbstractBase(models.Model):\n            name = models.CharField(max_length=100)\n\n            class Meta:\n                abstract = True\n\n        class ConcreteModel(AbstractBase):\n            pass\n\n        self.assertEqual(ConcreteModel.check(), [])\n", "    def test_property_name_clash_with_field_accessor(self):\n        class Model(models.Model):\n            foo = models.CharField(max_length=100)\n\n            @property\n                pass\n\n        self.assertEqual(Model.check(), [\n            Error(\n                \"The property 'foo_id' clashes with a related field accessor.\",\n                obj=Model,\n                id='models.E025',\n            )\n        ])\n", "    def test_meta_inheritance(self):\n        class AbstractModel(models.Model):\n            class Meta:\n                ordering = ['id']\n\n        class ConcreteModel(AbstractModel):\n            pass\n\n        self.assertEqual(ConcreteModel._meta.ordering, ('id',))\n"], "sample_98": ["def test_check_array_dtype_object():\n    # Test that check_array with dtype='object' doesn't convert to float\n    X = np.array([[1, 2], [3, 4]], dtype=np.object)\n    X_checked = check_array(X, dtype='object')\n    assert_equal(X_checked.dtype, np.object)\n    assert_array_equal(X_checked, X)\n\n    # Test that check_array with dtype='object' doesn't convert DataFrame to float\n    pd = importorskip(\"pandas\")\n    X_df = pd.DataFrame(X)\n    X_df_checked = check_array(X_df, dtype='object)\n    assert_equal(X_df_checked.dtype, np.object)\n    assert_array_equal(X_df_checked, X)\n", "def test_check_X_y():\n    X = [[1, 2], [3, 4]]\n    y = [0, 0]\n    X_csr = sp.csr_matrix(X)\n\n    # multi_output should raise a warning when y is 1D\n    assert_warns(UserWarning, check_X_y, X, y, multi_output=True)\n    assert_warns(UserWarning, check_X_y, X_csr, y, multi_output=True,\n                 accept_sparse=True)\n\n    # if y is 2D but still only one output it should not raise a warning\n    y_2d = [[0], [0]]\n    assert_no_warnings(check_X_y, X, y_2d, multi_output=True)\n    assert_no_warnings(check_X_y, X_csr, y_2d, multi_output=True,\n                       accept_sparse=True)\n\n    # if number of outputs in y doesn't match the number of labels in y\n    # a ValueError should be raised\n    y_2d_2outputs = [[0, 1], [0, 1]]\n    msg = (\"y should be a 1d array, got an array of shape {} instead.\"\n           .format(np.array(y_2d_2outputs).shape))\n    assert_raise_message(ValueError, msg, check_X_y, X, y_2d_2outputs)\n    assert_raise_message(ValueError, msg, check_X_y, X_csr, y_2d_2outputs,\n                         accept_sparse=True)\n\n    # multi_output=False with 2D y should raise a ValueError\n    msg = \"bad input shape (2, 2)\"\n    assert_raise_message(ValueError, msg, check_X_y, X, y_2d_2outputs,\n                         multi_output=False)\n    assert_raise_message(ValueError, msg, check_X_y, X_csr, y_2d_2outputs,\n                         multi_output=False, accept_sparse=True)\n", "def test_check_X_y():\n    # Test that check_X_y returns the original arrays if they are already\n    # valid (2D and 1D respectively) and raises ValueError otherwise.\n    X = np.arange(20).reshape(4, 5)\n    y = np.arange(4)\n    X_checked, y_checked = check_X_y(X, y)\n    assert_array_equal(X_checked, X)\n    assert_array_equal(y_checked, y)\n\n    X = np.arange(12).reshape(3, 4)\n    y = np.arange(12).reshape(3, 4)\n    assert_raises(ValueError, check_X_y, X, y)\n\n    # Test that y can be a 2D array with one column.\n    X = np.arange(12).reshape(3, 4)\n    y = np.arange(3).reshape(-1, 1)\n    X_checked, y_checked = check_X_y(X, y)\n    assert_array_equal(X_checked, X)\n    assert_array_equal(y_checked, y.ravel())\n", "def test_check_X_y():\n    X = [[1, 2], [3, 4]]\n    y = [0, 0]\n    X_csr = sp.csr_matrix(X)\n\n    # check_X_y with default values\n    X_checked, y_checked = check_X_y(X, y)\n    assert_array_equal(X_checked, X)\n    assert_array_equal(y_checked, y)\n\n    # multi_output=True and y.ndim=1\n    X_checked, y_checked = check_X_y(X, y, multi_output=True)\n    assert_array_equal(X_checked, X)\n    assert_array_equal(y_checked, y)\n\n    # multi_output=True and y.ndim=2\n    y_2d = [[0, 1], [0, 1]]\n    X_checked, y_checked = check_X_y(X, y_2d, multi_output=True)\n    assert_array_equal(X_checked, X)\n    assert_array_equal(y_checked, y_2d)\n\n    # ensure_2d=False\n    X_array = np.array([1, 2])\n    y_array = np.array([0])\n    X_checked, y_checked = check_X_y(X_array, y_array, ensure_2d=False)\n    assert_array_equal(X_checked, X_array)\n    assert_array_equal(y_checked, y_array)\n\n    # y should be 1d if multi_output is False\n    assert_raise_message(ValueError, 'bad input shape (2, 2)',\n                         check_X_y, X, y_2d, multi_output=False)\n\n    # check sparse matrix in CSR format\n    X_checked, y_checked = check_X_y(X_csr, y)\n    assert_true(sp.issparse(X_checked))\n    assert_equal(X_checked.format, 'csr')\n    assert_array_equal(y_checked, y)\n\n    # force_all_finite\n    X_invalid = [[1, 2], [np.nan, 4]]\n    assert_raises_regex(\n        ValueError,\n        \"Input contains NaN, infinity or a value too large for \"\n        \"dtype('float64').\",\n        check_X_y,\n        X_invalid,\n        y,\n        force_all_finite=True\n    )\n    assert_no_warnings(check_X_y, X_invalid, y, force_all_finite=False)\n", "def test_indexable():\n    # Tests for the indexable function.\n\n    # List of lists\n    X = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n    assert_true(indexable(X) is X)\n\n    # List of numpy arrays\n    X = [np.array([1, 2, 3]), np.array([4, 5, 6]), np.array([7, 8, 9])]\n    result = indexable(X)\n    assert_false(result is X)\n    assert_array_equal(result, X)\n\n    # Numpy array\n    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    assert_true(indexable(X) is X)\n\n    # Pandas DataFrame\n    pd = importorskip(\"pandas\")\n    X = pd.DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    assert_true(indexable(X) is X)\n\n    # List of sparse matrices\n    X = [sp.csr_matrix([1, 2, 3]), sp.csr_matrix([4, 5, 6]), sp.csr_matrix([7, 8, 9])]\n    result = indexable(X)\n    assert_false(result is X)\n    assert_sparse_equal(result[0], X[0])\n    assert_sparse_equal(result[1], X[1])\n    assert_sparse_equal(result[2], X[2])\n\n    # Sparse matrix\n    X = sp.csr_matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    result = indexable(X)\n    assert_false(result is X)\n    assert_sparse_equal(result, X)\n"], "sample_45": ["    def test_decorator_from_middleware_with_args(self):\n        class MyMiddleware:\n                self.get_response = get_response\n                self.arg1 = arg1\n                self.arg2 = arg2\n\n                return self.get_response(request)\n\n        decorator = decorator_from_middleware_with_args(MyMiddleware)\n        decorated_view = decorator('arg1_value', 'arg2_value')(lambda request: HttpResponse())\n        self.assertIsInstance(decorated_view, type(lambda: None))\n", "    def test_sync_and_async_middleware(self):\n        @sync_and_async_middleware\n            return get_response\n\n        self.assertTrue(my_middleware.sync_capable)\n        self.assertTrue(my_middleware.async_capable)\n", "    def test_sync_and_async_middleware(self):\n        @sync_and_async_middleware\n            return get_response\n\n        self.assertTrue(my_middleware.sync_capable)\n        self.assertTrue(my_middleware.async_capable)\n", "    def test_make_middleware_decorator(self):\n        class TestMiddleware:\n                self.get_response = get_response\n\n                return self.get_response(request)\n\n        decorator = make_middleware_decorator(TestMiddleware)\n        view_func = lambda request: HttpResponse()\n\n        wrapped_view = decorator(view_func)\n        response = wrapped_view(HttpRequest())\n\n        self.assertIsInstance(response, HttpResponse)\n", "    def test_sync_and_async_middleware(self):\n        @sync_and_async_middleware\n            return get_response\n\n        self.assertTrue(my_middleware.sync_capable)\n        self.assertTrue(my_middleware.async_capable)\n"], "sample_11": ["def test_serialize_lazy_object_with_nested_lazy_objects(self):\n    lazy_object = SimpleLazyObject(lambda: SimpleLazyObject(lambda: \"nested_lazy_object\"))\n    self.assertEqual(self.serialize_round_trip(lazy_object), \"nested_lazy_object\")\n", "def test_serialize_datetime_timezone(self):\n    self.assertSerializedEqual(datetime.datetime(2012, 1, 1, 1, 1, tzinfo=get_fixed_timezone(-300)))\n    self.assertSerializedResultEqual(\n        datetime.datetime(2012, 1, 1, 1, 1, tzinfo=get_fixed_timezone(-300)),\n        (\n            \"datetime.datetime(2012, 1, 1, 1, 1, tzinfo=datetime.timezone(datetime.timedelta(days=-1, seconds=68400)))\",\n            {'import datetime'}\n        )\n    )\n", "def test_serialize_datetime_timezone(self):\n    tz = get_fixed_timezone(180)\n    dt = datetime.datetime(2014, 1, 1, 1, 1, tzinfo=tz)\n    self.assertSerializedResultEqual(\n        dt,\n        (\n            \"datetime.datetime(2014, 1, 1, 1, 1, tzinfo=datetime.timezone(datetime.timedelta(0, 64800)))\",\n            {'import datetime'}\n        )\n    )\n", "def test_serialize_nested_partial(self):\n    value = functools.partial(datetime.timedelta, 1, seconds=2)\n    nested_value = functools.partial(value, minutes=3)\n    result = self.serialize_round_trip(nested_value)\n    self.assertIsInstance(result, functools.partial)\n    self.assertEqual(result.func, value)\n    self.assertEqual(result.args, tuple())\n    self.assertEqual(result.keywords, {'minutes': 3})\n", "def test_serialize_datetime_with_timezone(self):\n    dt = datetime.datetime(2014, 1, 1, 1, 1, tzinfo=get_fixed_timezone(180))\n    self.assertSerializedEqual(dt)\n    string, imports = MigrationWriter.serialize(dt)\n    self.assertEqual(\n        string,\n        \"datetime.datetime(2014, 1, 1, 1, 1, tzinfo=datetime.timezone(datetime.timedelta(0, 64800)))\"\n    )\n    self.assertEqual(imports, {'import datetime'})\n"], "sample_107": ["def test_logistic_regression_multiclass_warn():\n    # Make sure warning is raised when multi_class='warn' and solver is not\n    # liblinear.\n\n    X, y = make_classification(n_samples=200, n_features=20, random_state=0,\n                               n_classes=3, n_informative=10)\n\n    msg = (\"'multi_class' option will be changed to 'auto' in version 0.22. \"\n           \" 'ovr' trains n_class one-vs-rest classifiers. 'multinomial'\"\n           \" trains 1 classifier with n_class outputs.\")\n    for solver in ['lbfgs', 'newton-cg', 'sag', 'saga']:\n        lr = LogisticRegression(solver=solver, multi_class='warn')\n        assert_warns_message(DeprecationWarning, msg, lr.fit, X, y)\n", "def test_logistic_regression_multiclass_auto_solver_liblinear():\n    # check that when multi_class='auto' and solver='liblinear', then\n    # multi_class='ovr'\n\n    X, y = make_classification(n_samples=100, n_features=20, n_informative=10,\n                               n_classes=3, random_state=0)\n\n    lr_auto = LogisticRegression(multi_class='auto', solver='liblinear',\n                                 random_state=0)\n    lr_ovr = LogisticRegression(multi_class='ovr', solver='liblinear',\n                                random_state=0)\n\n    lr_auto.fit(X, y)\n    lr_ovr.fit(X, y)\n\n    assert_array_almost_equal(lr_auto.coef_, lr_ovr.coef_)\n", "def test_logistic_regression_path_multiclass_ill_conditioned_design_matrix():\n    # Make sure logistic_regression_path converges on an ill-conditioned design\n    # matrix, even when the multi_class parameter is set to 'multinomial'.\n    rng = np.random.RandomState(0)\n    n_samples, n_features = 100, 10\n    X = rng.randn(n_samples, n_features) * 1e5\n    y = rng.randint(0, 2, size=n_samples)\n\n    # LogisticRegression with solver='saga' handles the case where n_features >\n    # n_samples.\n    Cs = [1.0]\n    coefs, _, _ = _logistic_regression_path(X, y, Cs=Cs,\n                                            penalty='l2', max_iter=2000,\n                                            tol=1e-6, solver='saga',\n                                            random_state=0, multi_class='ovr')\n\n    # With multi_class='multinomial', the design matrix passed to the\n    # underlying solver has shape (n_samples, n_classes * n_features).\n    # We thus need to make sure the underlying solver can handle this case.\n    Cs = [1.0]\n    coefs, _, _ = _logistic_regression_path(X, y, Cs=Cs,\n                                            penalty='l2', max_iter=2000,\n                                            tol=1e-6, solver='saga',\n                                            random_state=0,\n                                            multi_class='multinomial')\n", "def test_logistic_regression_intercept_scaling():\n    # Test that the intercept_scaling parameter is correctly implemented\n\n    X, y = make_classification(n_samples=100, n_features=5, random_state=0)\n\n    lr = LogisticRegression(intercept_scaling=1, fit_intercept=True)\n    lr.fit(X, y)\n\n    # check that the intercept is equal to the last coefficient\n    assert_array_almost_equal(lr.intercept_, lr.coef_[:, -1])\n\n    # check that the coefficients are the same as when using default\n    # intercept_scaling and no intercept\n    lr2 = LogisticRegression(fit_intercept=False)\n    lr2.fit(np.hstack((X, np.ones((X.shape[0], 1)))), y)\n    assert_array_almost_equal(lr.coef_[:, :-1], lr2.coef_)\n", "def test_multinomial_logistic_regression_string_labels():\n    # Test with string labels for LogisticRegression\n    X, y = make_classification(n_samples=100, random_state=0, n_classes=3,\n                               n_informative=6)\n    train, test = np.arange(80), np.arange(80, 100)\n    y_str = LabelEncoder().fit(['bar', 'baz', 'foo']).inverse_transform(y)\n    # we use lbfgs to support multinomial\n    params = {'C': 1., 'n_jobs': -1, 'warm_start': False}\n    lr = LogisticRegression(multi_class='multinomial')\n    lr.fit(X[train], y[train])\n    lr_str = LogisticRegression(multi_class='multinomial')\n    lr_str.fit(X[train], y_str[train])\n    assert_array_almost_equal(lr.coef_, lr_str.coef_)\n    assert_equal(sorted(lr_str.classes_), ['bar', 'baz', 'foo'])\n    assert_array_almost_equal(lr.predict_proba(X[test]),\n                              lr_str.predict_proba(X[test]))\n"], "sample_49": ["def test_get_template_directories_to_path_called(self, mock_to_path):\n    autoreload.get_template_directories()\n    self.assertGreater(mock_to_path.call_count, 0)\n", "def test_get_template_directories_with_non_path_dir(self, mock_to_path):\n    mock_to_path.side_effect = lambda x: x\n    with override_settings(\n        TEMPLATES=[{\n            'DIRS': ['extra/templates'],\n            'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        }]\n    ):\n        dirs = autoreload.get_template_directories()\n        self.assertIn(Path.cwd() / 'extra/templates', dirs)\n", "def test_get_template_directories_with_loader_without_get_dirs(self):\n    self.assertSetEqual(\n        autoreload.get_template_directories(),\n        {\n            EXTRA_TEMPLATES_DIR,\n            ROOT / 'templates',\n        }\n    )\n", "def test_get_template_directories_with_loader_without_get_dirs(self):\n    self.assertSetEqual(\n        autoreload.get_template_directories(),\n        {\n            EXTRA_TEMPLATES_DIR,\n            ROOT / 'templates',\n        }\n    )\n", "def test_reset_loaders_with_locmem_loader(self, mock_reset):\n    autoreload.reset_loaders()\n    self.assertEqual(mock_reset.call_count, 2)\n"], "sample_5": ["def test_collector_sort(self):\n    # Create some objects to delete\n    r = R.objects.create()\n    s1 = S.objects.create(r=r)\n    s2 = S.objects.create(r=r)\n    t1 = T.objects.create(s=s1)\n    t2 = T.objects.create(s=s2)\n\n    # Create a collector and add the objects\n    collector = Collector(using='default')\n    collector.add([r])\n    collector.add([s1, s2])\n    collector.add([t1, t2])\n\n    # Sort the collector\n    collector.sort()\n\n    # Check that the objects are in the correct order for deletion\n    self.assertEqual(collector.data, {\n        T: {t1, t2},\n        S: {s1, s2},\n        R: {r},\n    })\n", "def test_collector_sort(self):\n    \"\"\"\n    #28244 - Collector.sort() should handle cyclic dependencies.\n    \"\"\"\n    u = User.objects.create()\n    a = Avatar.objects.create(user=u)\n    u.avatar = a\n    u.save()\n\n    collector = Collector(using='default')\n    collector.collect([u])\n    self.assertIsNone(collector.sort())\n", "def test_can_fast_delete_with_proxy_models(self):\n    # Create a proxy model instance\n    p = Parent.objects.create()\n    c = Child.objects.get(pk=p.pk)\n    \n    # Check if can_fast_delete works correctly with proxy models\n    collector = Collector(using='default')\n    self.assertTrue(collector.can_fast_delete(c))\n    self.assertTrue(collector.can_fast_delete(p))\n\n    # Check if fast deletion works correctly with proxy models\n    c.delete()\n    self.assertFalse(Child.objects.exists())\n    self.assertFalse(Parent.objects.exists())\n", "def test_collector_sort(self):\n    collector = Collector(using='default')\n    a1 = A.objects.create()\n    a2 = A.objects.create()\n    r1 = R.objects.create()\n    r2 = R.objects.create()\n    collector.add([a1, a2], source=R)\n    collector.add([r1, r2])\n    self.assertEqual(list(collector.data), [R, A])\n    collector.sort()\n    self.assertEqual(list(collector.data), [A, R])\n", "def test_collector_sort(self):\n    collector = Collector(using='default')\n    model_a = A.objects.create()\n    model_r = R.objects.create()\n    collector.add([model_a], source=model_r, nullable=False)\n    collector.add([model_r], source=None, nullable=False)\n\n    # Before sorting, the models should be in the order they were added.\n    self.assertEqual(list(collector.data.keys()), [A, R])\n\n    collector.sort()\n\n    # After sorting, the models should be in an order suitable for deletion.\n    self.assertEqual(list(collector.data.keys()), [R, A])\n"], "sample_158": ["def test_get_units_non_prefixed():\n    assert set(SI.get_units_non_prefixed()) == {\n        meter, second, joule, kilogram, volt, ohm,\n        # Add other non-prefixed units here\n    }\n", "def test_get_units_non_prefixed():\n    units = SI.get_units_non_prefixed()\n    assert meter in units\n    assert second in units\n    assert joule in units\n    assert day in units\n    assert volt in units\n    assert ohm in units\n    assert not centimeter in units\n    assert not kilometer in units\n    assert not kilogram in units\n    assert not pebibyte in units\n", "def test_get_units_non_prefixed():\n    assert set(SI.get_units_non_prefixed()) == {\n        meter, second, joule, kilogram, coulomb, kelvin, mole, candela,\n        gram, volt, ohm, farad, henry, siemens, weber, tesla, newton, pascal,\n        radian, steradian, lux, becquerel, gray, sievert, katal\n    }\n    assert meter in SI.get_units_non_prefixed()\n    assert centimeter not in SI.get_units_non_prefixed()\n", "def test_UnitSystem_get_default_unit_system():\n    assert UnitSystem.get_default_unit_system() == UnitSystem._unit_systems[\"SI\"]\n    assert isinstance(UnitSystem.get_default_unit_system(), UnitSystem)\n", "def test_get_units_non_prefixed():\n    assert meter in SI.get_units_non_prefixed()\n    assert second in SI.get_units_non_prefixed()\n    assert joule in SI.get_units_non_prefixed()\n    assert day in SI.get_units_non_prefixed()\n    assert volt in SI.get_units_non_prefixed()\n    assert ohm in SI.get_units_non_prefixed()\n\n    assert centimeter not in SI.get_units_non_prefixed()\n    assert kilometer not in SI.get_units_non_prefixed()\n    assert kilogram not in SI.get_units_non_prefixed()\n    assert pebibyte not in SI.get_units_non_prefixed()\n"], "sample_55": ["def test_command_parser_called_from_command_line(self):\n    parser = BaseCommand().create_parser(\"prog_name\", \"subcommand\")\n    self.assertIs(parser.called_from_command_line, False)\n    parser = BaseCommand(_called_from_command_line=True).create_parser(\n        \"prog_name\", \"subcommand\"\n    )\n    self.assertIs(parser.called_from_command_line, True)\n", "def test_outputwrapper_isatty(self):\n    out = StringIO()\n    with mock.patch.object(out, \"isatty\", return_value=True):\n        management.call_command(\"outputwrapper\", stdout=out)\n    self.assertIn(\"\\n\" + self.style.SUCCESS(\"Done!\"), out.getvalue())\n    out.seek(0)\n    out.truncate()\n    with mock.patch.object(out, \"isatty\", return_value=False):\n        management.call_command(\"outputwrapper\", stdout=out)\n    self.assertNotIn(\"\\n\" + self.style.SUCCESS(\"Done!\"), out.getvalue())\n", "def test_outputwrapper_style(self):\n    out = StringIO()\n    management.call_command(\"outputwrapper\", stdout=out)\n    self.assertIn(\"\\033[1mWorking...\\033[0m\", out.getvalue())\n    out = StringIO()\n    management.call_command(\"outputwrapper\", no_color=True, stdout=out)\n    self.assertIn(\"Working...\", out.getvalue())\n    out = StringIO()\n    management.call_command(\"outputwrapper\", force_color=True, stdout=out)\n    self.assertIn(\"\\033[1mWorking...\\033[0m\", out.getvalue())\n", "def test_base_command_suppressed_base_arguments(self):\n    class Command(BaseCommand):\n        suppressed_base_arguments = {\"--help\"}\n\n    parser = Command().create_parser(\"prog_name\", \"subcommand\")\n    self.assertEqual(parser._actions[0].help, argparse.SUPPRESS)\n", "def test_command_error_returncode(self):\n    \"\"\"Test that CommandError takes an optional returncode parameter.\"\"\"\n    e = CommandError(\"An error occurred\", returncode=42)\n    self.assertEqual(e.returncode, 42)\n    e = CommandError(\"An error occurred\")\n    self.assertEqual(e.returncode, 1)\n"], "sample_95": ["def test_xfail_strict_reporting(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.xfail(strict=True)\n            assert True\n\n        @pytest.mark.xfail(strict=True)\n            assert False\n\n        @pytest.mark.xfail\n            assert True\n\n        @pytest.mark.xfail\n            assert False\n    \"\"\"\n    )\n    result = pytester.runpytest(\"-rxXs\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*XPASS (strict)*test_passing_strict*\",\n            \"*XFAIL*test_failing_strict*\",\n            \"*XPASS*test_passing_not_strict*\",\n            \"*XFAIL*test_failing_not_strict*\",\n        ]\n    )\n", "def test_pytest_runtest_setup_item(pytester: Pytester) -> None:\n    \"\"\"Test pytest_runtest_setup item argument is an instance of Node.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            assert isinstance(item, pytest.nodes.Node)\n\n            pass\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_mark_xfail_strict_item(pytester: Pytester) -> None:\n    # Ensure pytest.mark.xfail(strict=True) works with non-Python Item\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n        class MyItem(pytest.Item):\n            nodeid = 'foo'\n                marker = pytest.mark.xfail(True, reason=\"Expected failure\", strict=True)\n                self.add_marker(marker)\n                assert True\n\n            return MyItem.from_parent(name=\"foo\", parent=parent)\n    \"\"\"\n    )\n    result = pytester.inline_run()\n    passed, skipped, failed = result.listoutcomes()\n    assert not passed and not skipped\n    assert len(failed) == 1\n    assert \"XPASS(strict)\" in str(result.stdout)\n", "def test_function_definition_node_repr(pytester: Pytester) -> None:\n    item = pytester.getitem(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail\n            pass\n    \"\"\"\n    )\n    assert \"FunctionDefinition\" in repr(item)\n", "def test_bestrelpath_with_rootdir(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        **{\n            \"tests/test_1.py\": \"\"\"\n        import pytest\n        @pytest.mark.skip()\n            pass\n            \"\"\",\n        }\n    )\n    result = pytester.runpytest(\"-rs\", \"tests/test_1.py\", \"--rootdir=.\")\n    result.stdout.fnmatch_lines(\n        [\"SKIPPED [[]1[]] tests/test_1.py:2: unconditional skip\"]\n    )\n"], "sample_106": ["def test_nca_components_shape():\n    # Test that the shape of the components_ attribute is as expected\n    nca = NeighborhoodComponentsAnalysis(n_components=2)\n    X, y = make_classification()\n    nca.fit(X, y)\n    assert nca.components_.shape == (2, X.shape[1])\n", "def test_components_shape():\n    \"\"\"Test that the learned components has the expected shape.\"\"\"\n    X = iris_data\n    y = iris_target\n\n    nca = NeighborhoodComponentsAnalysis(n_components=X.shape[1])\n    nca.fit(X, y)\n    assert_equal(nca.components_.shape, (X.shape[1], X.shape[1]))\n\n    nca = NeighborhoodComponentsAnalysis(n_components=2)\n    nca.fit(X, y)\n    assert_equal(nca.components_.shape, (2, X.shape[1]))\n", "def test_components_readonly():\n    X = iris_data\n    y = iris_target\n\n    nca = NeighborhoodComponentsAnalysis()\n    nca.fit(X, y)\n\n    with pytest.raises(AttributeError):\n        nca.components_ = np.random.rand(1, 1)\n", "def test_components_init():\n    # Test that the components are initialized correctly\n    rng = np.random.RandomState(42)\n    X, y = make_classification(n_samples=30, n_features=5, n_classes=4,\n                               n_redundant=0, n_informative=5, random_state=0)\n\n    # Initialize with PCA\n    nca_pca = NeighborhoodComponentsAnalysis(init='pca')\n    nca_pca.fit(X, y)\n    assert_array_almost_equal(nca_pca.components_, PCA(n_components=X.shape[1]).fit(X).components_)\n\n    # Initialize with LDA\n    nca_lda = NeighborhoodComponentsAnalysis(init='lda')\n    nca_lda.fit(X, y)\n    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n    lda = LinearDiscriminantAnalysis(n_components=X.shape[1])\n    lda.fit(X, y)\n    assert_array_almost_equal(nca_lda.components_, lda.scalings_.T)\n\n    # Initialize with identity\n    nca_identity = NeighborhoodComponentsAnalysis(init='identity')\n    nca_identity.fit(X, y)\n    assert_array_almost_equal(nca_identity.components_, np.eye(X.shape[1]))\n\n    # Initialize with random\n    nca_random = NeighborhoodComponentsAnalysis(init='random', random_state=rng)\n    nca_random.fit(X, y)\n    assert not np.allclose(nca_random.components_, np.eye(X.shape[1]))\n\n    # Initialize with numpy array\n    init = rng.rand(X.shape[1], X.shape[1])\n    nca_array = NeighborhoodComponentsAnalysis(init=init)\n    nca_array.fit(X, y)\n    assert_array_almost_equal(nca_array.components_, init)\n", "def test_components_shape():\n    # Test that the shape of the components is as expected\n    X = iris_data\n    y = iris_target\n\n    nca = NeighborhoodComponentsAnalysis(n_components=2)\n    nca.fit(X, y)\n\n    assert nca.components_.shape == (2, X.shape[1])\n"], "sample_90": ["def test_mark_evaluator_istrue_with_reason(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.foo(reason=\"bar\")\n            pass\n        \"\"\"\n    )\n    items, _ = testdir.inline_genitems()\n    item = items[0]\n    evaluator = MarkEvaluator(item, \"foo\")\n    assert evaluator.istrue()\n    assert evaluator.getexplanation() == \"bar\"\n", "def test_mark_evaluator_with_condition(testdir):\n    item = testdir.getitem(\"def test_func(): pass\")\n    item.add_marker(pytest.mark.skipif(\"True\"))\n    evaluator = MarkEvaluator(item, \"skipif\")\n    assert not evaluator.istrue()\n    assert evaluator.getexplanation() == \"condition: True\"\n", "def test_mark_evaluator_istrue():\n    item = mock.Mock()\n    item.keywords = [\"mark1\"]\n    item.iter_markers.return_value = [Mark(\"mark1\", (), {})]\n    evaluator = MarkEvaluator(item, \"mark1\")\n    assert evaluator.istrue()\n", "def test_compiled_eval_with_invalid_syntax(testdir):\n    \"\"\"Test that compiled_eval raises a SyntaxError when given invalid syntax.\"\"\"\n    with pytest.raises(SyntaxError):\n        compiled_eval(\"invalid syntax\", {})\n", "def test_mark_evaluator_with_condition(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.skipif(\"1 + 1 == 2\")\n            assert False\n\n        @pytest.mark.xfail(\"1 + 1 != 2\")\n            assert False\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(skipped=1, xfailed=1)\n"], "sample_74": ["def test_colorbar_lognorm_autoscale():\n    fig, ax = plt.subplots()\n    im = ax.imshow([[0.1, 10], [1000, 10000]])\n    im.set_norm(LogNorm())\n    cb = fig.colorbar(im)\n    assert cb.vmin == 0.1\n    assert cb.vmax == 10000\n    np.testing.assert_allclose(cb.ax.yaxis.get_ticklocs(),\n                               np.logspace(-1, 4, 6))\n", "def test_colorbar_with_no_mappable():\n    fig, ax = plt.subplots()\n    cmap = mpl.colormaps[\"viridis\"]\n    norm = mcolors.Normalize(vmin=0, vmax=1)\n    cbar = fig.colorbar(cm.ScalarMappable(cmap=cmap, norm=norm), ax=ax)\n    assert cbar.mappable is not None\n", "def test_colorbar_alpha(fig_ref, fig_test):\n    ax_ref = fig_ref.add_subplot()\n    im = ax_ref.imshow([[0, 1], [2, 3]], alpha=0.5)\n    fig_ref.colorbar(im)\n\n    ax_test = fig_test.add_subplot()\n    im = ax_test.imshow([[0, 1], [2, 3]])\n    cb = fig_test.colorbar(im)\n    cb.set_alpha(0.5)\n", "def test_colorbar_no_warning_on_repeated_remove():\n    # github issue #20549 - Repeatedly removing the same colorbar\n    # should not raise a warning.\n    fig, ax = plt.subplots()\n    im = ax.imshow([[0, 1], [2, 3]])\n    cb = fig.colorbar(im)\n    for _ in range(3):\n        cb.remove()\n", "def test_colorbar_ax_attribute(fig_ref, fig_test):\n    ax_ref = fig_ref.add_subplot()\n    im = ax_ref.imshow([[0, 1], [2, 3]])\n    cb_ref = ax_ref.figure.colorbar(im, cax=ax_ref.inset_axes([0, 1.05, 1, 0.05]),\n                                    orientation=\"horizontal\", ticklocation=\"top\")\n    assert cb_ref.ax is not None\n\n    ax_test = fig_test.add_subplot()\n    im = ax_test.imshow([[0, 1], [2, 3]])\n    cb_test = ax_test.figure.colorbar(im, cax=ax_test.inset_axes([0, 1.05, 1, 0.05]),\n                                      orientation=\"horizontal\", ticklocation=\"top\")\n    assert cb_test.ax is not None\n    cb_test.ax = None\n    assert cb_test.ax is None\n"], "sample_85": ["def test_log_format_with_timestamp(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        logger = logging.getLogger()\n\n            logger.info('text going to logger')\n            assert False\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--log-level=INFO\", \"--log-format=%(asctime)s %(message)s\")\n    assert result.ret == 1\n    result.stdout.fnmatch_lines([\"*- Captured *log call -*\", \"*text going to logger*\"])\n    log_text = result.stdout.str()\n    assert \"text going to logger\" in log_text\n    # Check that timestamp is present in the log message\n    assert re.search(r\"\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2},\\d{3}\", log_text)\n", "def test_log_file_format(testdir):\n    log_file = testdir.tmpdir.join(\"pytest.log\").strpath\n\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file={}\n        log_file_level = INFO\n        log_file_format = %(asctime)s %(levelname)s %(message)s\n        log_file_date_format = %Y-%m-%d %H:%M:%S\n        \"\"\".format(\n            log_file\n        )\n    )\n\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logging.getLogger().info(\"Normal message\")\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n\n    assert result.ret == 0\n    assert os.path.isfile(log_file)\n    with open(log_file, encoding=\"utf-8\") as rfh:\n        contents = rfh.read()\n        assert re.match(r\"\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2} INFO Normal message\", contents)\n", "def test_get_actual_log_level(testdir):\n    \"\"\"Test that get_actual_log_level returns the correct log level.\"\"\"\n    config = testdir.parseconfig()\n    assert get_actual_log_level(config, \"log_level\") is None\n\n    config = testdir.parseconfig(\"--log-level=INFO\")\n    assert get_actual_log_level(config, \"log_level\") == logging.INFO\n\n    config = testdir.parseconfig(\"--log-level=DEBUG\", \"--log-cli-level=INFO\")\n    assert get_actual_log_level(config, \"log_level\") == logging.DEBUG\n    assert get_actual_log_level(config, \"log_cli_level\") == logging.INFO\n\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_level=WARNING\n        \"\"\"\n    )\n    config = testdir.parseconfig()\n    assert get_actual_log_level(config, \"log_level\") == logging.WARNING\n\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_level=ERROR\n        \"\"\"\n    )\n    config = testdir.parseconfig(\"--log-level=INFO\")\n    assert get_actual_log_level(config, \"log_level\") == logging.INFO\n", "def test_log_format_with_color(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        logger = logging.getLogger()\n\n            logger.info('text going to logger')\n            assert False\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--log-level=INFO\", \"--color=yes\")\n    assert result.ret == 1\n    result.stdout.fnmatch_lines([\"*- Captured *log call -*\", \"*text going to logger*\"])\n", "def test_get_option_ini_default(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_level=DEBUG\n        \"\"\"\n    )\n    config = testdir.parseconfigure()\n    assert get_option_ini(config, \"log_level\") == \"DEBUG\"\n    assert get_option_ini(config, \"log_format\") == DEFAULT_LOG_FORMAT\n    assert get_option_ini(config, \"log_date_format\") == DEFAULT_LOG_DATE_FORMAT\n"], "sample_27": ["def test_make_hash_value(self):\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    timestamp = 1643723900\n    login_timestamp = datetime(2022, 2, 1, 12, 0, 0)\n    user.last_login = login_timestamp\n    user.save()\n    p0 = PasswordResetTokenGenerator()\n    hash_value = p0._make_hash_value(user, timestamp)\n    expected_hash_value = f'{user.pk}{user.password}{login_timestamp.replace(microsecond=0, tzinfo=None)}{timestamp}{user.email}'\n    self.assertEqual(hash_value, expected_hash_value)\n", "def test_token_with_invalid_timestamp(self):\n    \"\"\"A token with an invalid timestamp is rejected.\"\"\"\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n    ts_b36, _ = tk1.split(\"-\")\n    # Attempt to parse the timestamp as base36\n    try:\n        ts = base36_to_int(ts_b36)\n    except ValueError:\n        pass  # This should not happen, but just in case\n    else:\n        # Check that the timestamp/uid has not been tampered with\n        # Make a new token with an invalid timestamp\n        invalid_ts = ts + 1000000\n        invalid_ts_b36 = int_to_base36(invalid_ts)\n        invalid_tk = \"%s-%s\" % (invalid_ts_b36, tk1.split(\"-\")[1])\n        self.assertIs(p0.check_token(user, invalid_tk), False)\n", "def test_token_with_invalid_timestamp(self):\n    \"\"\"A token with an invalid timestamp is rejected.\"\"\"\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n    ts_b36, _ = tk1.split(\"-\")\n    # Replace the valid timestamp with an invalid one\n    invalid_ts_b36 = \" invalid\"\n    invalid_tk1 = f\"{invalid_ts_b36}-{tk1.split('-')[1]}\"\n    self.assertIs(p0.check_token(user, invalid_tk1), False)\n\n    # Try with a non-base36 digit in the timestamp\n    invalid_ts_b36 = ts_b36 + \"G\"\n    invalid_tk1 = f\"{invalid_ts_b36}-{tk1.split('-')[1]}\"\n    self.assertIs(p0.check_token(user, invalid_tk1), False)\n", "def test_make_token_with_timestamp(self):\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    timestamp = self._num_seconds(datetime.now())\n    token = p0._make_token_with_timestamp(user, timestamp)\n    self.assertIsNotNone(token)\n    self.assertGreater(len(token), 0)\n    # check the token is correctly split into timestamp and hash\n    ts_b36, _ = token.split(\"-\")\n    self.assertEqual(base36_to_int(ts_b36), timestamp)\n", "def test_token_with_invalid_timestamp(self):\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n    ts_b36, _ = tk1.split(\"-\")\n    ts = base36_to_int(ts_b36)\n    # Check with a timestamp that's not a valid base36 value.\n    invalid_ts_b36 = ' invalid'\n    invalid_tk = f'{invalid_ts_b36}-{tk1.split(\"-\")[1]}'\n    self.assertIs(p0.check_token(user, invalid_tk), False)\n    # Check with a timestamp that's not an integer.\n    invalid_ts = '123abc'\n    invalid_ts_b36 = int_to_base36(ts) + invalid_ts\n    invalid_tk = f'{invalid_ts_b36}-{tk1.split(\"-\")[1]}'\n    self.assertIs(p0.check_token(user, invalid_tk), False)\n"], "sample_39": ["    def test_check_url(self):\n        resolver = get_resolver()\n        messages = check_resolver(resolver)\n        self.assertEqual(messages, [])\n", "    def test_warning_on_invalid_pattern(self):\n        msg = (\n            \"Your URL pattern '^%' [name='invalid-pattern'] has a route that \"\n            \"contains '(?P<', begins with a '^', or ends with a '$'. This was \"\n            \"likely an oversight when migrating to django.urls.path().\"\n        )\n        warnings = get_resolver().check()\n        self.assertEqual(len(warnings), 1)\n        self.assertEqual(str(warnings[0]), msg)\n", "    def test_warning_on_missing_slash(self):\n        class WarnOnMissingSlash(URLPattern):\n                super().__init__(pattern, callback)\n                self.regex = re.compile(r'^%s$' % pattern)\n\n                return [Warning(\"Regex pattern doesn't end with a '/'.\")]\n\n        resolver = URLResolver(RegexPattern(r'^/'), 'urlpatterns_reverse.urls')\n        resolver.url_patterns = [\n            WarnOnMissingSlash('example', views.empty_view),\n        ]\n\n        warnings = resolver.check()\n        self.assertEqual(len(warnings), 1)\n        warning = warnings[0]\n        self.assertIsInstance(warning, Warning)\n        self.assertEqual(\n            str(warning),\n            \"Regex pattern doesn't end with a '/'.\",\n        )\n", "    def test_current_app(self):\n        resolver = get_resolver('urlpatterns_reverse.namespace_urls')\n        test_urls = [\n            ('test-ns1:urlobject-view', [], {}, 'test-ns1', '/test1/inner/'),\n            ('test-ns1:urlobject-view', [37, 42], {}, 'test-ns1', '/test1/inner/37/42/'),\n            ('test-ns1:urlobject-view', [], {'arg1': 42, 'arg2': 37}, 'test-ns1', '/test1/inner/42/37/'),\n            ('test-ns1:urlobject-special-view', [], {}, 'test-ns1', '/test1/inner/+%5C$*/'),\n        ]\n        for name, args, kwargs, current_app, expected in test_urls:\n            with self.subTest(name=name, args=args, kwargs=kwargs, current_app=current_app):\n                self.assertEqual(\n                    resolver.reverse(name, *args, **kwargs, current_app=current_app),\n                    expected,\n                )\n", "    def test_warning_for_missing_trailing_slash(self):\n        msg = (\n            \"Your URL pattern '<URLPattern '^missing-slash/$' [name='missing-slash']>' \"\n            \"has a route ending with a '/'. Remove this slash as it is unnecessary. \"\n            \"If this pattern is targeted in an include(), ensure the include() \"\n            \"pattern has a trailing '/'\"\n        )\n        with self.assertWarnsMessage(Warning, msg):\n            check_resolver(get_resolver('urlpatterns_reverse.urls'))\n"], "sample_144": ["def test_refine_Pow_with_non_integer_exponent():\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    z = Symbol('z', real=True)\n\n    assert refine(x**y, Q.real(x)) == Abs(x)**y\n    assert refine(x**y, Q.positive(x)) == x**y\n    assert refine((x*y)**z, Q.real(x) & Q.real(y)) == Abs(x*y)**z\n    assert refine((x*y)**z, Q.positive(x) & Q.positive(y)) == (x*y)**z\n", "def test_refine_abs():\n    assert refine(Abs(x), Q.real(x)) == Abs(x)\n    assert refine(Abs(x), Q.positive(x)) == x\n    assert refine(Abs(x), Q.negative(x)) == -x\n    assert refine(Abs(x*y), Q.real(x) & Q.real(y)) == Abs(x*y)\n    assert refine(Abs(x*y), Q.positive(x) & Q.real(y)) == x*Abs(y)\n    assert refine(Abs(x*y), Q.negative(x) & Q.real(y)) == -x*Abs(y)\n", "def test_refine_with_multiple_assumptions():\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    assert refine(sign(x*y), Q.positive(x) & Q.positive(y)) == 1\n    assert refine(sign(x*y), Q.positive(x) & Q.negative(y)) == -1\n    assert refine(sign(x*y), Q.negative(x) & Q.positive(y)) == -1\n    assert refine(sign(x*y), Q.negative(x) & Q.negative(y)) == 1\n", "def test_refine_Pow_with_non_integer_exponent():\n    assert refine((-1)**(x/2), Q.even(x)) == 1\n    assert refine((-1)**(x/2), Q.odd(x)) == -I\n    assert refine((-1)**(x/3), Q.integer(x)) == (-1)**(x/3)\n    assert refine((-2)**(x/2), Q.even(x)) == 2**(x/2)\n", "def test_refine_mul():\n    assert refine(Abs(x * y), Q.positive(x) & Q.real(y)) == x * Abs(y)\n    assert refine(Abs(x * y * z), Q.positive(x) & Q.real(y) & Q.real(z)) == \\\n        x * Abs(y * z)\n    assert refine(Abs(x * y * z), Q.positive(x) & Q.positive(y) & Q.real(z)) == \\\n        x * y * Abs(z)\n    assert refine(Abs(x**2 * y * z), Q.real(x) & Q.positive(y) & Q.real(z)) == \\\n        x**2 * y * Abs(z)\n"], "sample_31": ["def test_shell_with_no_interfaces_installed(self, select):\n    select.return_value = ([], [], [])\n    with self.assertRaisesMessage(CommandError, \"Couldn't import python interface.\"):\n        call_command('shell')\n", "def test_shell_with_no_optional_shells_installed(self, select):\n    select.return_value = ([], [], [])\n    with self.assertRaisesMessage(CommandError, \"Couldn't import python interface.\"):\n        call_command('shell')\n", "def test_shell_with_no_interfaces_installed(self, select):\n    select.return_value = ([], [], [])\n    with self.assertRaisesMessage(CommandError, \"Couldn't import python interface.\"):\n        call_command('shell')\n", "def test_shell_with_python_interface(self, select):\n    select.return_value = ([], [], [])\n    with captured_stdout() as stdout:\n        call_command('shell', interface='python', command='print(\"Hello World\")')\n    self.assertEqual(stdout.getvalue().strip(), 'Hello World')\n", "def test_shell_with_no_interfaces_installed(self, select):\n    select.return_value = ([], [], [])\n    with self.assertRaisesMessage(CommandError, \"Couldn't import python interface.\"):\n        call_command('shell')\n"], "sample_64": ["def test_cell_count(self):\n    \"\"\"\n    The cell_count filter returns the correct number of cells for a tabular inline.\n    \"\"\"\n    modeladmin = ModelAdmin(Question, site)\n    inline_admin_formset = modeladmin.get_inline_formsets(None)[0]\n    inline_admin_form = inline_admin_formset.formset.forms[0]\n    self.assertEqual(cell_count(inline_admin_form), 2)\n\n    # Add a field to the inline form\n    inline_admin_form.fields['new_field'] = 'new field'\n    self.assertEqual(cell_count(inline_admin_form), 3)\n\n    # Make the field hidden\n    inline_admin_form.fields['new_field'].is_hidden = True\n    self.assertEqual(cell_count(inline_admin_form), 2)\n", "def test_cell_count(self):\n    \"\"\"\n    The cell_count filter should return the correct number of cells used in a tabular inline.\n    \"\"\"\n    modeladmin = ModelAdmin(Question, site)\n    inline_admin_formset = modeladmin.get_inline_formsets()[0]\n    inline_admin_form = inline_admin_formset.formset.forms[0]\n\n    # Test with no fields and no delete checkbox\n    self.assertEqual(cell_count(inline_admin_form), 1)\n\n    # Test with one field and no delete checkbox\n    field = inline_admin_form.fields[\"question\"]\n    inline_admin_form.fields = {\"question\": field}\n    self.assertEqual(cell_count(inline_admin_form), 2)\n\n    # Test with one field and delete checkbox\n    inline_admin_form.formset.can_delete = True\n    self.assertEqual(cell_count(inline_admin_form), 3)\n\n    # Test with multiple fields and delete checkbox\n    inline_admin_form.fields = {\"question\": field, \"posted\": field}\n    self.assertEqual(cell_count(inline_admin_form), 4)\n", "def test_prepopulated_fields_js(self):\n    \"\"\"\n    prepopulated_fields_js template tag should pass whole context and \n    include prepopulated fields.\n    \"\"\"\n    request = self.request_factory.get(\n        reverse(\"admin:auth_user_change\", args=[self.superuser.pk])\n    )\n    request.user = self.superuser\n    admin = UserAdmin(User, site)\n    response = admin.change_view(request, str(self.superuser.pk))\n    template_context = prepopulated_fields_js(response.context_data)\n    self.assertIn('prepopulated_fields', template_context)\n    self.assertIn('prepopulated_fields_json', template_context)\n    self.assertIsInstance(template_context['prepopulated_fields'], list)\n    self.assertIsInstance(template_context['prepopulated_fields_json'], str)\n", "def test_cell_count(self):\n    \"\"\"\n    Test cell_count filter for tabular inlines.\n    \"\"\"\n    modeladmin = ModelAdmin(Question, site)\n    inline_admin_formset = modeladmin.get_inline_formsets(None, Question())[0]\n    inline_admin_form = inline_admin_formset.formset.forms[0]\n    self.assertEqual(cell_count(inline_admin_form), 3)  # id, question, and delete checkbox\n\n    # Add a visible field to the form\n    class QuestionInlineForm(forms.ModelForm):\n        extra_field = forms.CharField()\n\n        class Meta:\n            model = Question\n            fields = (\"question\", \"extra_field\")\n\n    inline_admin_formset.form = QuestionInlineForm\n    inline_admin_form = inline_admin_formset.formset.forms[0]\n    self.assertEqual(cell_count(inline_admin_form), 4)  # id, question, extra_field, and delete checkbox\n", "def test_cell_count(self):\n    \"\"\"\n    Test the cell_count filter for tabular inlines.\n    \"\"\"\n    modeladmin = ModelAdmin(Question, site)\n    inline_admin_formset = modeladmin.get_inline_formsets(\n        self.request_factory.get(\"/\"), obj=None\n    )\n    inline_admin_form = next(iter(inline_admin_formset))\n    self.assertEqual(cell_count(inline_admin_form), 2)  # Hidden cell + 1 visible field\n\n    # Add a field to the formset and check again\n    inline_admin_form.fields[\"new_field\"] = \"test\"\n    self.assertEqual(cell_count(inline_admin_form), 3)\n\n    # Make one of the fields hidden and check again\n    inline_admin_form.fields[\"new_field\"].is_hidden = True\n    self.assertEqual(cell_count(inline_admin_form), 2)\n"], "sample_86": ["def test_add_stats(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            pass\n            assert 0\n            pytest.skip(\"\")\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    node = dom.find_first_by_tag(\"testsuite\")\n    node.assert_attr(errors=0, failures=1, skipped=1, tests=3)\n    log = LogXML(str(testdir.tmpdir.join(\"test.xml\")), None)\n    log.stats[\"error\"] += 1\n    log.stats[\"failure\"] += 2\n    log.stats[\"skipped\"] += 3\n    log.pytest_sessionfinish()\n    dom = minidom.parse(str(testdir.tmpdir.join(\"test.xml\")))\n    node = dom.find_first_by_tag(\"testsuite\")\n    node.assert_attr(errors=1, failures=2, skipped=3, tests=0)\n", "def test_escaped_xfail_reason_issue3533(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail(reason='1 <> 2')\n            assert False\n    \"\"\"\n    )\n    _, dom = runandparse(testdir)\n    node = dom.find_first_by_tag(\"testcase\")\n    snode = node.find_first_by_tag(\"skipped\")\n    assert \"1 <> 2\" in snode.text\n    snode.assert_attr(message=\"1 <> 2\")\n", "def test_add_stats(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            pass\n            assert 0\n            pytest.skip(\"\")\n        @pytest.mark.xfail\n            assert 0\n        @pytest.mark.xfail\n            assert 1\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    assert result.ret\n    node = dom.find_first_by_tag(\"testsuite\")\n    node.assert_attr(errors=0, failures=1, skipped=2, tests=5)\n\n    log = LogXML(str(testdir.tmpdir.join(\"test.xml\")), None)\n    log.add_stats(\"passed\")\n    log.add_stats(\"failure\")\n    log.add_stats(\"skipped\")\n    log.add_stats(\"error\")\n\n    assert log.stats == {\"error\": 1, \"passed\": 1, \"failure\": 1, \"skipped\": 1}\n", "def test_add_stats(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            pass\n            assert 0\n            pytest.skip(\"\")\n    \"\"\"\n    )\n    log = LogXML(str(testdir.tmpdir.join(\"test.xml\")), None)\n    log.add_stats(\"passed\")\n    log.add_stats(\"failure\")\n    log.add_stats(\"skipped\")\n    assert log.stats == {\"error\": 0, \"passed\": 1, \"failure\": 1, \"skipped\": 1}\n", "def test_invalid_test_case_name(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    node = dom.find_first_by_tag(\"testcase\")\n    assert node[\"name\"] == \"test_foo\"\n    node[\"name\"] = \"\"  # set name to invalid value\n    with pytest.raises(AssertionError):\n        node.assert_attr(name=\"test_foo\")\n"], "sample_76": ["def test_insufficient_data(self, df):\n\n    groupby = GroupBy([\"group\"])\n    df = df.head(PolyFit().order)  # Not enough data for a 2nd order fit\n    res = PolyFit()(df[[\"x\", \"y\"]], groupby, \"x\", {})\n\n    assert_array_equal(res.columns, [\"x\", \"y\"])\n    assert res.empty\n", "def test_insufficient_data(self, df):\n\n    groupby = GroupBy([\"group\"])\n    df = pd.DataFrame(dict(x=[1, 2], y=[3, 4]))\n    res = PolyFit(order=2)(df, groupby, \"x\", {})\n\n    assert_array_equal(res.columns, [\"x\", \"y\"])\n    assert res.empty\n", "def test_insufficient_data(self, df):\n\n    groupby = GroupBy([\"group\"])\n    df = df.head(PolyFit().order)  # Not enough data to fit the polynomial\n\n    res = PolyFit()(df[[\"x\", \"y\"]], groupby, \"x\", {})\n    assert_frame_equal(res, pd.DataFrame(columns=[\"x\", \"y\"]))\n", "def test_insufficient_data(self, df):\n\n    groupby = GroupBy([\"group\"])\n    df = pd.DataFrame(dict(x=[1], y=[1]))\n    res = PolyFit(order=2)(df, groupby, \"x\", {})\n\n    assert_array_equal(res.columns, [\"x\", \"y\"])\n    assert res.empty\n", "def test_too_few_unique_x(self, df):\n\n    groupby = GroupBy([\"group\"])\n    df[\"x\"] = np.repeat([1, 2], len(df) // 2)\n    res = PolyFit(order=2)(df, groupby, \"x\", {})\n\n    assert res.columns.to_list() == [\"x\", \"y\", \"group\"]\n\n    for _, part in res.groupby(\"group\"):\n        assert part.empty\n"], "sample_19": ["    def test_404_response(self):\n        request = self.rf.get('/test_view/')\n        exception = Http404('Not found')\n        response = technical_404_response(request, exception)\n        self.assertEqual(response.status_code, 404)\n        self.assertContains(response, 'Page not found <span>(404)</span>', status_code=404)\n        self.assertContains(response, 'Raised by:', status_code=404)\n", "    def test_technical_404_response_with_empty_path_info(self):\n        exception = Http404()\n        request = self.rf.get('/non-existent-url')\n        response = technical_404_response(request, exception)\n        self.assertEqual(response.status_code, 404)\n        self.assertContains(response, 'non-existent-url', status_code=404)\n", "    def test_get_safe_settings(self):\n        reporter_filter = SafeExceptionReporterFilter()\n        with self.settings(DEBUG=True, SECRET_KEY='super_secret'):\n            safe_settings = reporter_filter.get_safe_settings()\n            self.assertIn('SECRET_KEY', safe_settings)\n            self.assertEqual(safe_settings['SECRET_KEY'], reporter_filter.cleansed_substitute)\n", "    def test_postmortem_includes_all_frames(self):\n        exception = Http404('Test 404')\n        exception.__cause__ = Exception('Inner exception')\n        response = technical_404_response(self.rf.get('/test_view/'), exception)\n        self.assertContains(response, 'Inner exception', status_code=404)\n", "    def test_cleansed_substitute_length(self):\n        reporter_filter = SafeExceptionReporterFilter()\n        self.assertEqual(len(reporter_filter.cleansed_substitute), 20)\n"], "sample_118": ["def test_ccode_AugmentedAssignment():\n    assert ccode(aug_assign(x, '+', y + z)) == 'x += y + z;'\n    assert ccode(aug_assign(x, '-', y + z)) == 'x -= y + z;'\n    assert ccode(aug_assign(x, '*', y + z)) == 'x *= y + z;'\n    assert ccode(aug_assign(x, '/', y + z)) == 'x /= y + z;'\n    assert ccode(aug_assign(x, '%', y + z)) == 'x %= y + z;'\n", "def test_ccode_For_nested():\n    f = For(x, Range(0, 10, 2), [For(y, Range(0, 5, 1), [aug_assign(z, '+', x + y)])])\n    sol = ccode(f)\n    assert sol == (\"for (x = 0; x < 10; x += 2) {\\n\"\n                   \"   for (y = 0; y < 5; y += 1) {\\n\"\n                   \"      z += x + y;\\n\"\n                   \"   }\\n\"\n                   \"}\")\n", "def test_ccode_AugmentedAssignment():\n    expr = AugmentedAssignment(x, '+', y + z)\n    assert ccode(expr) == 'x += y + z;'\n    expr = AugmentedAssignment(x, '-', y + z)\n    assert ccode(expr) == 'x -= y + z;'\n    expr = AugmentedAssignment(x, '*', y + z)\n    assert ccode(expr) == 'x *= y + z;'\n    expr = AugmentedAssignment(x, '/', y + z)\n    assert ccode(expr) == 'x /= y + z;'\n", "def test_ccode_AugmentedAssignment():\n    assert ccode(AugmentedAssignment(x, '+', y + z)) == 'x += y + z;'\n    assert ccode(AugmentedAssignment(x, '-', y + z)) == 'x -= y + z;'\n    assert ccode(AugmentedAssignment(x, '*', y + z)) == 'x *= y + z;'\n    assert ccode(AugmentedAssignment(x, '/', y + z)) == 'x /= y + z;'\n", "def test_ccode_Idx():\n    from sympy.tensor import IndexedBase, Idx\n    from sympy import symbols\n    n, m = symbols('n m', integer=True)\n    i, j = Idx('i', n), Idx('j', m)\n    x = IndexedBase('x')\n    assert ccode(i) == 'i'\n    assert ccode(j) == 'j'\n    assert ccode(x[i]) == 'x[i]'\n    assert ccode(x[j]) == 'x[j]'\n"], "sample_152": ["def test_ndim_array_diff():\n    from sympy.tensor.array import ImmutableDenseNDimArray\n\n    x = ImmutableDenseNDimArray([[x, y], [1, x*y]])\n    assert x.diff(x) == ImmutableDenseNDimArray([[1, 0], [0, y]])\n\n    # Test diff with other var\n    assert x.diff(y) == ImmutableDenseNDimArray([[0, 1], [0, x]])\n", "def test_ndim_array_iter():\n    for ArrayType in array_types:\n        test_array = ArrayType([[1, 2], [3, 4]])\n        result = []\n        for elem in test_array:\n            result.append(elem)\n        assert result == [ArrayType([1, 2]), ArrayType([3, 4])]\n        \n        test_array = ArrayType([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n        result = []\n        for elem in test_array:\n            result.append(elem)\n        assert result == [ArrayType([[1, 2], [3, 4]]), ArrayType([[5, 6], [7, 8]])]\n", "def test_ndim_array_conjugate_and_adjoint():\n    for ArrayType in array_types:\n        A = ArrayType([[1 + 2j, 3 - 4j], [5 + 6j, 7 - 8j]])\n        A_conj = A.conjugate()\n        assert A_conj == ArrayType([[1 - 2j, 3 + 4j], [5 - 6j, 7 + 8j]])\n\n        A_adj = A.adjoint()\n        assert A_adj == ArrayType([[1 - 2j, 5 - 6j], [3 + 4j, 7 + 8j]])\n", "def test_array_rank():\n    for ArrayType in array_types:\n        test_array = ArrayType([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n        assert test_array.rank() == 3\n\n        test_array = ArrayType([[1, 2], [3, 4]])\n        assert test_array.rank() == 2\n\n        test_array = ArrayType([1, 2, 3, 4])\n        assert test_array.rank() == 1\n\n        test_array = ArrayType(5)\n        assert test_array.rank() == 0\n", "def test_ndim_array_repr():\n    array = ImmutableDenseNDimArray([1, 2, 3], (3,))\n    assert repr(array) == \"[1, 2, 3]\"\n    array = ImmutableDenseNDimArray([[1, 2], [3, 4]], (2, 2))\n    assert repr(array) == \"[[1, 2], [3, 4]]\"\n    array = ImmutableDenseNDimArray([[[1, 2], [3, 4]], [[5, 6], [7, 8]]], (2, 2, 2))\n    assert repr(array) == \"[[[1, 2], [3, 4]], [[5, 6], [7, 8]]]\"\n"], "sample_154": ["def test_lambdify_matrix_slice():\n    if not numpy:\n        skip(\"numpy not installed\")\n    M = MatrixSymbol('M', 3, 3)\n    f = lambdify(M, M[1, :])\n    A = numpy.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    assert numpy.array_equal(f(A), A[1, :])\n", "def test_lambdify_strict():\n    raises(Exception, lambda: lambdify(x, x + y, modules='scipy', printer=NumPyPrinter({})))\n", "def test_lambdify_issue_23256():\n    f = lambdify(x, Array([x**2, x**3]))\n    assert f(2) == [4, 8]\n", "def test_lambdastr_nested_args():\n    expr = x + y + z\n    assert lambdastr((x, (y, z)), expr) == 'lambda _0,_1: (lambda x,y,z: (x + y + z))(_0,_1[0],_1[1])'\n    assert lambdastr(((x, y), z), expr) == 'lambda _0,_1: (lambda x,y,z: (x + y + z))(_0[0],_0[1],_1)'\n", "def test_lambdify_with_cse():\n    # Test that lambdify works with common subexpressions eliminated.\n    from sympy import symbols, sin, cos\n    x, y = symbols('x y')\n    expr1 = sin(x) + cos(x)\n    expr2 = sin(x) - cos(x)\n    f = lambdify((x, y), (expr1, expr2), cse=True)\n    assert f(1, 2) == (sin(1) + cos(1), sin(1) - cos(1))\n"], "sample_51": ["def test_was_modified_since_overflow_error(self):\n    \"\"\"\n    was_modified_since should handle OverflowError when parsing If-Modified-Since header.\n    \"\"\"\n    header = \"Mon, 18 Jan 2038 05:14:07 GMT\"\n    mtime = 2147483647.1  # a timestamp that will cause an OverflowError\n    self.assertTrue(was_modified_since(header, mtime))\n", "def test_was_modified_since_invalid_header(self):\n    \"\"\"\n    Test that was_modified_since returns True for an invalid header value.\n    \"\"\"\n    mtime = 1343416141\n    header = \"Invalid Header Value\"\n    self.assertTrue(was_modified_since(header, mtime))\n", "def test_was_modified_since_invalid_header_format(self):\n    \"\"\"\n    Test that was_modified_since returns True for an invalid header format.\n    \"\"\"\n    mtime = 1343416141\n    header = \"Invalid header format\"\n    self.assertTrue(was_modified_since(header, mtime))\n", "def test_was_modified_since_invalid_header_format(self):\n    \"\"\"\n    was_modified_since returns True for an invalid header format.\n    \"\"\"\n    header = \"Invalid Header Format\"\n    mtime = 1343416141\n    self.assertTrue(was_modified_since(header, mtime))\n", "def test_was_modified_since_invalid_header(self):\n    \"\"\"\n    Test was_modified_since with an invalid header value.\n    \"\"\"\n    mtime = 1343416141\n    self.assertTrue(was_modified_since(header=\"Invalid Header\", mtime=mtime))\n    self.assertTrue(was_modified_since(header=\"Thu, 01 Jan 1970 00:00:00\", mtime=mtime))\n    self.assertTrue(was_modified_since(header=\"Thu, 01 Jan 1970 00:00:00 GMT; length=\", mtime=mtime))\n    self.assertTrue(was_modified_since(header=\"Thu, 01 Jan 1970 00:00:00 GMT; length=abc\", mtime=mtime))\n"], "sample_17": ["    def test_clone_test_db(self):\n        # Clone a test database.\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db'):\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n            with mock.patch.object(creation, '_clone_test_db') as mocked_clone:\n                creation.clone_test_db(suffix='clone', verbosity=0)\n            mocked_clone.assert_called_once()\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0)\n", "    def test_create_test_db_with_keepdb(self, mocked_migrate, mocked_ensure_connection):\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db'):\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False, keepdb=True)\n            self.assertEqual(test_connection.settings_dict['NAME'], creation._get_test_db_name())\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0)\n", "    def test_get_test_db_name_default(self):\n        # A test db name isn't set.\n        prod_name = 'hodor'\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['NAME'] = prod_name\n        test_connection.settings_dict['TEST'] = {'NAME': None}\n        creation = BaseDatabaseCreation(test_connection)\n        self.assertEqual(creation._get_test_db_name(), TEST_DATABASE_PREFIX + prod_name)\n", "    def test_create_test_db_with_keepdb(self, mocked_migrate, mocked_ensure_connection):\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db'):\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False, keepdb=True)\n            self.assertEqual(test_connection.settings_dict['NAME'], creation._get_test_db_name())\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0)\n", "    def test_create_test_db_with_keepdb(self, mocked_execute_create_test_db):\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            creation.create_test_db(verbosity=0, autoclobber=True, serialize=False, keepdb=True)\n            self.assertEqual(mocked_execute_create_test_db.call_count, 1)\n            # If keepdb is True, _execute_create_test_db should not raise an exception\n            # even if the database exists.\n            mocked_execute_create_test_db.assert_called_once()\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0)\n"], "sample_149": ["def test_MonomialOps():\n    ops = MonomialOps(3)\n    \n    assert ops.mul()( (1, 2, 3), (4, 5, 6) ) == (5, 7, 9)\n    assert ops.pow()( (1, 2, 3), 2 ) == (2, 4, 6)\n    assert ops.mulpow()( (1, 2, 3), (4, 5, 6), 2 ) == (9, 12, 15)\n    assert ops.ldiv()( (1, 2, 3), (4, 5, 6) ) == (-3, -3, -3)\n    assert ops.div()( (1, 2, 3), (1, 1, 1) ) == (0, 1, 2)\n    assert ops.lcm()( (1, 2, 3), (4, 5, 6) ) == (4, 5, 6)\n    assert ops.gcd()( (1, 2, 3), (4, 5, 6) ) == (1, 2, 3)\n", "def test_MonomialOps():\n    m = MonomialOps(3)\n\n    assert m.mul()((1, 2, 3), (4, 5, 6)) == (5, 7, 9)\n    assert m.pow()((1, 2, 3), 2) == (2, 4, 6)\n    assert m.mulpow()((1, 2, 3), (4, 5, 6), 2) == (9, 12, 15)\n    assert m.ldiv()((4, 5, 6), (1, 2, 3)) == (3, 3, 3)\n    assert m.div()((4, 5, 6), (1, 2, 3)) == (3, 3, 3)\n    assert m.lcm()((1, 2, 3), (4, 5, 6)) == (4, 5, 6)\n    assert m.gcd()((4, 5, 6), (1, 2, 3)) == (1, 2, 3)\n", "def test_monomial_ldiv():\n    assert monomial_ldiv((3, 4, 1), (1, 2, 0)) == (2, 2, 1)\n    assert monomial_ldiv((1, 2, 3), (4, 5, 6)) == (-3, -3, -3)\n", "def test_MonomialOps():\n    m = MonomialOps(3)\n\n    assert m.mul()( (1, 2, 3), (4, 5, 6) ) == (5, 7, 9)\n    assert m.pow()( (1, 2, 3), 2 ) == (2, 4, 6)\n    assert m.mulpow()( (1, 2, 3), (4, 5, 6), 2 ) == (9, 12, 15)\n    assert m.ldiv()( (1, 2, 3), (4, 5, 6) ) == (-3, -3, -3)\n    assert m.div()( (1, 2, 3), (1, 1, 1) ) == (0, 1, 2)\n    assert m.lcm()( (1, 2, 3), (4, 5, 6) ) == (4, 5, 6)\n    assert m.gcd()( (1, 2, 3), (4, 5, 6) ) == (1, 2, 3)\n", "def test_MonomialOps():\n    mo = MonomialOps(3)\n\n    assert mo.mul()((1, 2, 3), (4, 5, 6)) == (5, 7, 9)\n    assert mo.pow()((1, 2, 3), 3) == (3, 6, 9)\n    assert mo.mulpow()((1, 2, 3), (4, 5, 6), 3) == (13, 17, 21)\n    assert mo.ldiv()((4, 5, 6), (1, 2, 3)) == (3, 3, 3)\n    assert mo.div()((4, 5, 6), (1, 2, 3)) == (3, 3, 3)\n    assert mo.lcm()((1, 2, 3), (4, 5, 6)) == (4, 5, 6)\n    assert mo.gcd()((4, 5, 6), (1, 2, 3)) == (1, 2, 3)\n"], "sample_130": ["def test_lambdify_multiple_statements():\n    # Test that lambdify can handle multiple statements in the lambda function.\n    f = lambdify(x, (x**2, x**3))\n    result = f(2)\n    assert result == (4, 8)\n", "def test_lambdify_tensorflow_with_indexed():\n    if not tensorflow:\n        skip(\"tensorflow not installed.\")\n\n    i, j = symbols('i j')\n    A = IndexedBase('A')\n\n    f = lambdify(A, A[i, j], 'tensorflow')\n\n    tensor_A = tensorflow.constant([[1, 2], [3, 4]])\n\n    s = tensorflow.Session()\n    assert s.run(f(tensor_A)) == tensor_A\n", "def test_lambdastr():\n    # Test that lambdastr works with dummify=True\n    assert lambdastr(x, x**2, dummify=True) == 'lambda _0: (_0**2)'\n    assert lambdastr((x, y), x + y, dummify=True) == 'lambda _0,_1: (_0 + _1)'\n    assert lambdastr([x, y], x - y, dummify=True) == 'lambda _0,_1: (_0 - _1)'\n    assert lambdastr([[x, y]], x + y, dummify=True) == 'lambda _0: (lambda _0,_1: (_0 + _1))(_0[0],_0[1])'\n", "def test_lambdify_with_sum():\n    # Test that lambdify works correctly with Sum\n    if not numpy:\n        skip(\"numpy not installed\")\n\n    i = symbols('i')\n    expr = Sum(x**i, (i, 0, 3))\n    f = lambdify(x, expr, 'numpy')\n\n    assert f(2) == 15\n", "def test_lambdify_lambda():\n    f = lambdify(x, Lambda(x, x**2))\n    assert f(2) == 4\n    assert f(3) == 9\n"], "sample_113": ["def test_column_transformer_feature_names_out_dtype():\n    \"\"\"Check that feature names out is an object array.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    df = pd.DataFrame([[1, 2, 3, 4]], columns=[\"a\", \"b\", \"c\", \"d\"])\n    ct = ColumnTransformer(\n        [\n            (\"bycol1\", TransWithNames(), [\"a\"]),\n            (\"bycol2\", TransWithNames(), [\"b\"]),\n        ],\n        remainder=\"passthrough\",\n    )\n    ct.fit(df)\n    names = ct.get_feature_names_out()\n    assert names.dtype == object\n    assert isinstance(names[0], str)\n", "def test_column_transformer_n_features_in_none():\n    # Test that n_features_in_ is not set when no data has been fitted\n    ct = ColumnTransformer([(\"trans\", StandardScaler(), [0])])\n    assert ct.n_features_in_ is None\n", "def test_column_transformer_get_feature_names_out_with_function_transformer():\n    \"\"\"Check get_feature_names_out with FunctionTransformer\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    X_df = pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]})\n\n        return np.hstack((X[:, 0:1], np.log(X[:, 1:])))\n\n    ct = ColumnTransformer(\n        [(\"identity\", FunctionTransformer(func=func), [\"A\", \"B\"])],\n        remainder=\"drop\",\n        verbose_feature_names_out=False,\n    )\n    ct.fit(X_df)\n    assert_array_equal(ct.get_feature_names_out(), [\"A\", \"B\"])\n", "def test_column_transformer_get_feature_names_out_with_callable_specifier():\n    \"\"\"Check feature_names_out with callable specifier\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    df = pd.DataFrame([[1, 2, 3, 4]], columns=[\"a\", \"b\", \"c\", \"d\"])\n\n        return [\"a\", \"c\"]\n\n    ct = ColumnTransformer(\n        [(\"selector\", TransWithNames([\"x\", \"y\"]), column_selector)], remainder=\"drop\"\n    )\n    ct.fit(df)\n    names = ct.get_feature_names_out()\n    assert isinstance(names, np.ndarray)\n    assert names.dtype == object\n    assert_array_equal(names, [\"selector__x\", \"selector__y\"])\n", "def test_column_transformer_get_feature_names_out_with_drop():\n    \"\"\"Check get_feature_names_out when using 'drop'.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    X_df = pd.DataFrame(\n        {\n            \"col_int\": np.array([0, 1, 2], dtype=int),\n            \"col_float\": np.array([0.0, 1.0, 2.0], dtype=float),\n            \"col_str\": [\"one\", \"two\", \"three\"],\n        },\n        columns=[\"col_int\", \"col_float\", \"col_str\"],\n    )\n\n    ct = ColumnTransformer(\n        [(\"trans\", StandardScaler(), [\"col_int\", \"col_float\"]), (\"drop\", \"drop\", [\"col_str\"])],\n        remainder=\"passthrough\",\n    )\n    ct.fit(X_df)\n\n    names = ct.get_feature_names_out()\n    assert isinstance(names, np.ndarray)\n    assert names.dtype == object\n    assert_array_equal(names, [\"trans__col_int\", \"trans__col_float\"])\n"], "sample_116": ["def test_create_index_with_group_entries(app):\n    text = (\".. index:: docutils\\n\"\n            \".. index:: Python\\n\"\n            \".. index:: pip; install\\n\"\n            \".. index:: pip; upgrade\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder, group_entries=False)\n    assert len(index) == 4\n    assert index[0] == ('D', [('docutils', [[('', '#index-0')], [], None])])\n    assert index[1] == ('P', [('pip', [[], [('install', [('', '#index-2')])], None]),\n                              ('pip', [[], [('upgrade', [('', '#index-3')])], None]),\n                              ('Python', [[('', '#index-1')], [], None])])\n", "def test_create_index_group_entries(app):\n    text = (\".. index:: single: docutils\\n\"\n            \".. index:: single: Python\\n\"\n            \".. index:: single: pip; install\\n\"\n            \".. index:: single: pip; upgrade\\n\"\n            \".. index:: single: Sphinx\\n\"\n            \".. index:: single: Sphinx (in module foo)\\n\"\n            \".. index:: single: Sphinx (in module bar)\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder, group_entries=True)\n    assert len(index) == 4\n    assert index[0] == ('D', [('docutils', [[('', '#index-0')], [], None])])\n    assert index[1] == ('P', [('pip', [[], [('install', [('', '#index-2')]),\n                                            ('upgrade', [('', '#index-3')])], None]),\n                              ('Python', [[('', '#index-1')], [], None])])\n    assert index[2] == ('S', [('Sphinx', [[('', '#index-4')],\n                                          [(' (in module bar)', [('', '#index-6')]),\n                                           (' (in module foo)', [('', '#index-5')])],\n                                         None])])\n", "def test_create_index_with_group_entries(app):\n    text = (\".. index:: single: docutils\\n\"\n            \".. index:: pair: Python; interpreter\\n\"\n            \".. index:: Sphinx\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder, group_entries=False)\n    assert len(index) == 3\n    assert index[0] == ('D', [('docutils', [[('', '#index-0')], [], None])])\n    assert index[1] == ('P', [('Python', [[], [('interpreter', [('', '#index-1')])], None])])\n    assert index[2] == ('S', [('Sphinx', [[('', '#index-2')], [], None])])\n", "def test_create_index_with_group_entries(app):\n    text = (\".. index:: single: docutils\\n\"\n            \".. index:: single: Python\\n\"\n            \".. index:: pair: pip; install\\n\"\n            \".. index:: pair: pip; upgrade\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder, group_entries=False)\n    assert len(index) == 3\n    assert index[0] == ('D', [('docutils', [[('', '#index-0')], [], None])])\n    assert index[1] == ('P', [('pip', [[], [('install', [('', '#index-2')]),\n                                            ('upgrade', [('', '#index-3')])], None]),\n                              ('Python', [[('', '#index-1')], [], None])])\n    # Check that group_entries=False doesn't group subitems\n    assert index[2] == ('P', [('pip (install)', [[('', '#index-2')], [], None]),\n                               ('pip (upgrade)', [[('', '#index-3')], [], None])])\n", "def test_create_index_with_group_entries(app):\n    text = (\".. index:: single: docutils\\n\"\n            \".. index:: pair: Python; interpreter\\n\"\n            \".. index:: triple: Sphinx; documentation tool; reST\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder, group_entries=False)\n    assert len(index) == 5\n    assert index[0] == ('D', [('docutils', [[('', '#index-0')], [], None]),\n                              ('documentation tool', [[(), ('Sphinx reST', [('', '#index-2')])], None])])\n    assert index[1] == ('P', [('Python', [[], [('interpreter', [('', '#index-1')])], None])])\n    assert index[2] == ('R', [('reST', [[], [('Python Sphinx', [('', '#index-2')])], None])])\n    assert index[3] == ('S', [('Sphinx', [[], [('reST, Python', [('', '#index-2')])], None])])\n"], "sample_109": ["def test_validate_shuffle_split():\n    n_samples = 10\n    train_size, test_size = _validate_shuffle_split(n_samples, test_size=0.2)\n    assert train_size == 8 and test_size == 2\n\n    train_size, test_size = _validate_shuffle_split(n_samples, train_size=0.7)\n    assert train_size == 7 and test_size == 3\n\n    train_size, test_size = _validate_shuffle_split(n_samples, test_size=5)\n    assert train_size == 5 and test_size == 5\n\n    with pytest.raises(ValueError):\n        _validate_shuffle_split(n_samples, test_size=1.1)\n\n    with pytest.raises(ValueError):\n        _validate_shuffle_split(n_samples, train_size=-1)\n\n    with pytest.raises(ValueError):\n        _validate_shuffle_split(n_samples, test_size=11)\n\n    with pytest.raises(ValueError):\n        _validate_shuffle_split(n_samples, test_size=5, train_size=6)\n", "def test_validate_shuffle_split():\n    # Test _validate_shuffle_split function\n    n_samples = 10\n\n    # test_size and train_size as int\n    test_size, train_size = 3, 7\n    n_test, n_train = _validate_shuffle_split(n_samples, test_size, train_size)\n    assert n_test == test_size\n    assert n_train == train_size\n\n    # test_size as float and train_size as int\n    test_size, train_size = 0.3, 7\n    n_test, n_train = _validate_shuffle_split(n_samples, test_size, train_size)\n    assert n_test == int(test_size * n_samples)\n    assert n_train == train_size\n\n    # test_size as int and train_size as float\n    test_size, train_size = 3, 0.7\n    n_test, n_train = _validate_shuffle_split(n_samples, test_size, train_size)\n    assert n_test == test_size\n    assert n_train == int(train_size * n_samples)\n\n    # test_size and train_size as float\n    test_size, train_size = 0.3, 0.7\n    n_test, n_train = _validate_shuffle_split(n_samples, test_size, train_size)\n    assert n_test == int(test_size * n_samples)\n    assert n_train == int(train_size * n_samples)\n\n    # Test if test_size + train_size > n_samples\n    test_size, train_size = 8, 7\n    with pytest.raises(ValueError):\n        _validate_shuffle_split(n_samples, test_size, train_size)\n\n    # Test if neither test_size nor train_size is specified\n    test_size, train_size = None, None\n    with pytest.raises(ValueError):\n        _validate_shuffle_split(n_samples, test_size, train_size)\n", "def test_validate_shuffle_split():\n    # Test if _validate_shuffle_split returns a valid train_size and test_size\n    n_samples = 10\n    for test_size, exp_train, exp_test in [(2, 8, 2),\n                                           (0.2, 8, 2),\n                                           (None, 9, 1)]:\n        train_size, test_size = _validate_shuffle_split(n_samples,\n                                                         test_size=test_size)\n        assert train_size == exp_train\n        assert test_size == exp_test\n\n    for train_size, exp_train, exp_test in [(2, 2, 8),\n                                            (0.2, 2, 8),\n                                            (None, 9, 1)]:\n        train_size, test_size = _validate_shuffle_split(n_samples,\n                                                         train_size=train_size)\n        assert train_size == exp_train\n        assert test_size == exp_test\n\n    # Check error when neither train_size nor test_size is provided\n    assert_raises(ValueError, _validate_shuffle_split, n_samples)\n\n    # Check error when both train_size and test_size are provided\n    assert_raises(ValueError, _validate_shuffle_split, n_samples,\n                  test_size=0.5, train_size=0.5)\n", "def test_validate_shuffle_split():\n    # Test _validate_shuffle_split function\n    n_samples = 10\n\n    # Should raise an error when test_size is greater than 1 and train_size is 0\n    assert_raises(ValueError, _validate_shuffle_split, n_samples,\n                  test_size=2, train_size=0)\n\n    # Should raise an error when test_size is 0.5 and train_size is 0.6\n    assert_raises(ValueError, _validate_shuffle_split, n_samples,\n                  test_size=0.5, train_size=0.6)\n\n    # Should not raise an error when test_size is None and train_size is valid\n    _validate_shuffle_split(n_samples, test_size=None, train_size=0.5)\n\n    # Should not raise an error when train_size is None and test_size is valid\n    _validate_shuffle_split(n_samples, test_size=0.5, train_size=None)\n", "def test_validate_shuffle_split():\n    # Check the validation of test_size and train_size\n    assert_raises(ValueError, _validate_shuffle_split, 10, 0.6, 0.6)\n    assert_raises(ValueError, _validate_shuffle_split, 10, 11, None)\n    assert_raises(ValueError, _validate_shuffle_split, 10, None, 11)\n    assert_raises(ValueError, _validate_shuffle_split, 10, 'a', None)\n    assert_raises(ValueError, _validate_shuffle_split, 10, None, 'a')\n    assert_raises(ValueError, _validate_shuffle_split, 10, 0.5, 'a')\n    assert_raises(ValueError, _validate_shuffle_split, 10, 'a', 0.5)\n    assert_raises(ValueError, _validate_shuffle_split, 10, 0, 0.6)\n    assert_raises(ValueError, _validate_shuffle_split, 10, 0.6, 0)\n    assert_raises(ValueError, _validate_shuffle_split, 10, -1, None)\n    assert_raises(ValueError, _validate_shuffle_split, 10, None, -1)\n    assert_raises(ValueError, _validate_shuffle_split, 10, -1, 0.6)\n    assert_raises(ValueError, _validate_shuffle_split, 10, 0.6, -1)\n    assert_raises(ValueError, _validate_shuffle_split, 10, 10, 0.6)\n    assert_raises(ValueError, _validate_shuffle_split, 10, 0.6, 10)\n    assert_raises(ValueError, _validate_shuffle_split, 10, 10, None)\n    assert_raises(ValueError, _validate_shuffle_split, 10, None, 10)\n\n    # Check that it doesn't raise on valid inputs\n    assert _validate_shuffle_split(10, 0.6, None) == (4, 6)\n    assert _validate_shuffle_split(10, None, 0.4) == (4, 6)\n    assert _validate_shuffle_split(10, 4, None) == (4, 6)\n    assert _validate_shuffle_split(10, None, 6) == (4, 6)\n    assert _validate_shuffle_split(10, 4, 6) == (4, 6)\n    assert _validate_shuffle_split(10, 0.4, 0.6) == (4, 6)\n"], "sample_40": ["def test_renderer_instance_attribute(self):\n    class CustomForm(Form):\n        renderer = DjangoTemplates()\n\n    form = CustomForm()\n    self.assertEqual(form.renderer, CustomForm.renderer)\n", "def test_field_required_false_with_use_required_attribute(self):\n    class MyForm(Form):\n        use_required_attribute = True\n        f1 = CharField(max_length=30, required=False)\n        f2 = CharField(max_length=30, required=False, widget=Textarea)\n\n    form = MyForm()\n    self.assertHTMLEqual(\n        form.as_p(),\n        '<p><label for=\"id_f1\">F1:</label> <input id=\"id_f1\" maxlength=\"30\" name=\"f1\" type=\"text\"></p>'\n        '<p><label for=\"id_f2\">F2:</label> <textarea cols=\"40\" id=\"id_f2\" name=\"f2\" rows=\"10\">'\n        '</textarea></p>',\n    )\n    self.assertHTMLEqual(\n        form.as_ul(),\n        '<li><label for=\"id_f1\">F1:</label> <input id=\"id_f1\" maxlength=\"30\" name=\"f1\" type=\"text\"></li>'\n        '<li><label for=\"id_f2\">F2:</label> <textarea cols=\"40\" id=\"id_f2\" name=\"f2\" rows=\"10\">'\n        '</textarea></li>',\n    )\n    self.assertHTMLEqual(\n        form.as_table(),\n        '<tr><th><label for=\"id_f1\">F1:</label></th>'\n        '<td><input id=\"id_f1\" maxlength=\"30\" name=\"f1\" type=\"text\"></td></tr>'\n        '<tr><th><label for=\"id_f2\">F2:</label></th><td><textarea cols=\"40\" id=\"id_f2\" name=\"f2\" rows=\"10\">'\n        '</textarea></td></tr>',\n    )\n", "def test_boundfield_disabled(self):\n    class MyForm(Form):\n        field1 = CharField(disabled=True)\n        field2 = CharField()\n\n    form = MyForm({'field1': 'Initial value', 'field2': 'value'})\n\n    self.assertHTMLEqual(\n        str(form['field1']),\n        '<input type=\"text\" name=\"field1\" value=\"Initial value\" disabled>'\n    )\n    self.assertHTMLEqual(\n        str(form['field2']),\n        '<input type=\"text\" name=\"field2\" value=\"value\" required>'\n    )\n\n    # A disabled field's value is displayed, but it's not used in the form's data.\n    self.assertEqual(form.data, {'field1': 'Initial value', 'field2': 'value'})\n    self.assertTrue(form.is_valid())\n    self.assertEqual(form.cleaned_data, {'field1': 'Initial value', 'field2': 'value'})\n\n    # If a disabled field isn't provided in the form's data, its initial value is used.\n    form = MyForm({'field2': 'value'}, initial={'field1': 'Initial value'})\n\n    self.assertHTMLEqual(\n        str(form['field1']),\n        '<input type=\"text\" name=\"field1\" value=\"Initial value\" disabled>'\n    )\n    self.assertTrue(form.is_valid())\n    self.assertEqual(form.cleaned_data, {'field1': 'Initial value', 'field2': 'value'})\n", "def test_boundfield_with_custom_renderer(self):\n    class CustomRenderer(DjangoTemplates):\n            if template_name == 'django/forms/widgets/text.html':\n                return 'Custom text widget'\n            return super().render(template_name, context, request)\n\n    class SimpleForm(Form):\n        field = CharField()\n\n    form = SimpleForm(renderer=CustomRenderer())\n    self.assertHTMLEqual(form.as_p(), '<p>Field: Custom text widget</p>')\n", "def test_subwidgets(self):\n    class MyForm(Form):\n        f1 = CharField(widget=RadioSelect(choices=[('a', 'A'), ('b', 'B')]))\n        f2 = CharField(widget=CheckboxSelectMultiple(choices=[('c', 'C'), ('d', 'D')]))\n\n    form = MyForm()\n    self.assertEqual(len(form['f1'].subwidgets), 2)\n    self.assertEqual(len(form['f2'].subwidgets), 2)\n    self.assertIsInstance(form['f1'].subwidgets[0], BoundWidget)\n    self.assertIsInstance(form['f2'].subwidgets[0], BoundWidget)\n\n    # Check that subwidgets are cached.\n    self.assertIs(form['f1'].subwidgets[0], form['f1'].subwidgets[0])\n"], "sample_128": ["def test_build_options():\n    opt = build_options((x, y, z), {'domain': 'ZZ'})\n    assert opt.gens == (x, y, z)\n    assert opt.domain == ZZ\n\n    opt = build_options({'domain': 'ZZ'})\n    assert opt.gens == ()\n    assert opt.domain == ZZ\n\n    raises(OptionError, lambda: build_options((x, y, z), {'gens': (x, y)}))\n\n    opt = build_options((x, y, z), {'opt': Options(gens=(x, y), args={'domain': 'QQ'})})\n    assert opt.gens == (x, y)\n    assert opt.domain == QQ\n", "def test_Options___init__():\n    opt = Options((x, y, z), {'domain': 'ZZ'})\n    assert opt.domain == ZZ\n\n    raises(OptionError, lambda: Options((x, y, z), {'domain': 'abc'}))\n\n    opt = Options((x, y, z), {'wrt': x})\n    assert opt.wrt == ['x']\n\n    raises(OptionError, lambda: Options((x, y, z), {'wrt': 1}))\n\n    opt = Options((x, y, z), {'order': 'lex'})\n    assert opt.order == lex\n\n    raises(OptionError, lambda: Options((x, y, z), {'order': 1}))\n", "compilation error", "def test_build_options():\n    opt = build_options((x, y, z), {'domain': 'ZZ'})\n    assert opt['gens'] == (x, y, z)\n    assert opt['domain'] == ZZ\n\n    opt = build_options({'domain': 'ZZ'})\n    assert opt['gens'] == ()\n    assert opt['domain'] == ZZ\n\n    raises(OptionError, lambda: build_options((x, y, z), {'gens': (x, y)}))\n\n    opt2 = Options((x, y, z), {'domain': 'ZZ'})\n    opt = build_options((x, y, z), {'opt': opt2})\n    assert opt is opt2\n", "def test_build_options():\n    assert build_options((x, y, z), {'domain': 'ZZ'}) == Options((x, y, z), {'domain': 'ZZ'})\n    assert build_options({'opt': Options((x, y, z), {'domain': 'ZZ'})}) == Options((x, y, z), {'domain': 'ZZ'})\n\n    raises(OptionError, lambda: build_options((x, y, z), {'opt': Options((x, y, z), {'domain': 'ZZ'})}))\n"], "sample_73": ["def test_annotationbbox_set_xybox(fig_test, fig_ref):\n    ax_ref = fig_ref.add_subplot()\n    ab_ref = AnnotationBbox(DrawingArea(20, 20, 0, 0, clip=True), (0.5, 0.5),\n                            xycoords='data', boxcoords='data')\n    ax_ref.add_artist(ab_ref)\n\n    ax_test = fig_test.add_subplot()\n    ab_test = AnnotationBbox(DrawingArea(20, 20, 0, 0, clip=True), (0.3, 0.3),\n                             xycoords='data', boxcoords='data')\n    ax_test.add_artist(ab_test)\n    ab_test.xybox = (0.5, 0.5)\n", "def test_auxtransformbox():\n    fig, ax = plt.subplots()\n    atb = AuxTransformBox(mtransforms.Affine2D().rotate_deg(45))\n    text = TextArea(\"foo\")\n    atb.add_artist(text)\n    ax.add_artist(atb)\n    fig.canvas.draw()\n    assert not fig.stale\n    atb.remove()\n    assert not fig.artists\n", "def test_anchoredtext_vertical_alignment():\n    fig, ax = plt.subplots()\n    text = AnchoredText(\"test\\ntest long text\", loc=\"center\",\n                        pad=0.2, prop={\"ha\": \"center\"})\n    ax.add_artist(text)\n    fig.canvas.draw()\n    assert not fig.stale\n    text.txt.set_multilinebaseline(True)\n    assert fig.stale\n", "def test_AnnotationBbox_update_positions():\n    fig, ax = plt.subplots()\n    an = AnnotationBbox(TextArea(\"Test\"), (0.5, 0.5), xycoords='data')\n    ax.add_artist(an)\n    fig.canvas.draw()\n    renderer = fig.canvas.get_renderer()\n\n    # update_positions should not error when called without draw first\n    an.update_positions(renderer)\n\n    # check that update_positions updates the box position correctly\n    an.xybox = (0.7, 0.7)\n    an.update_positions(renderer)\n    assert an.offsetbox.get_offset() != (0, 0)\n\n    # check that update_positions updates the arrow position correctly\n    an.arrowprops = dict(arrowstyle=\"->\")\n    an.update_positions(renderer)\n    assert an.arrow_patch.get_positions() != ((0, 0), (1, 1))\n", "def test_anchoredtext_set_text(fig_test, fig_ref):\n    ax_ref = fig_ref.add_subplot()\n    text0 = AnchoredText(\"Foo\", \"upper left\")\n    ax_ref.add_artist(text0)\n\n    ax_test = fig_test.add_subplot()\n    text1 = AnchoredText(\"Bar\", \"upper left\")\n    ax_test.add_artist(text1)\n    text1.txt.set_text(\"Foo\")\n"], "sample_111": ["def test_empty_input(metric_name, metric_func):\n    # Test that an empty input raises a ValueError for all supervised metrics\n    with pytest.raises(ValueError, match='labels_true and labels_pred must not be empty'):\n        metric_func([], [])\n", "def test_empty_input(metric_name, metric_func):\n    # Test that supervised metrics handle empty inputs correctly\n    with pytest.raises(ValueError, match='true and predicted labels must be 1D arrays'):\n        metric_func([], [])\n    with pytest.raises(ValueError, match='true and predicted labels must be 1D arrays'):\n        metric_func(np.array([]), np.array([]))\n    with pytest.raises(ValueError, match='true and predicted labels must be 1D arrays'):\n        metric_func([0, 1, 2], [])\n", "def test_empty_input(metric_name, metric_func):\n    # Test that empty input arrays raise a ValueError\n    with pytest.raises(ValueError, match='contains only 0 sample'):\n        metric_func([], [])\n", "def test_empty_input(metric_name, metric_func):\n    # Test that all supervised metrics handle empty input correctly\n    assert metric_func([], []) == 1.0\n    with pytest.raises(ValueError, match='labels_true and labels_pred must have'):\n        metric_func([0], [])\n    with pytest.raises(ValueError, match='labels_true and labels_pred must have'):\n        metric_func([], [0])\n", "def test_empty_input(metric_name, metric_func):\n    # Test that empty inputs raise a ValueError\n    with pytest.raises(ValueError, match='true and predicted labels must not be empty'):\n        metric_func([], [])\n"], "sample_151": ["def test_orthogonal_direction():\n    p1 = Point(1, 2)\n    p2 = Point(3, 4, 0)\n    assert p1.orthogonal_direction == Point(-2, 1)\n    assert p2.orthogonal_direction == Point(-2, 1, 0)\n\n    # test that orthogonal direction is not normalized\n    p3 = Point(2, 2)\n    assert p3.orthogonal_direction == Point(-2, 2)\n\n    # test in higher dimensions\n    p4 = Point(1, 2, 3, 4)\n    assert p4.orthogonal_direction == Point(-2, 1, 0, 0)\n", "def test_affine_rank():\n    assert Point.affine_rank() == -1\n    assert Point.affine_rank(Point(1, 2)) == 0\n    assert Point.affine_rank(Point(1, 2), Point(3, 4)) == 1\n    assert Point.affine_rank(Point(1, 2), Point(3, 4), Point(5, 6)) == 1\n    assert Point.affine_rank(Point(1, 2, 3), Point(4, 5, 6)) == 1\n    assert Point.affine_rank(Point(1, 2, 3), Point(4, 5, 6), Point(7, 8, 9)) == 1\n    assert Point.affine_rank(Point(1, 2), Point(3, 4), Point(5, 6), Point(7, 8)) == 2\n", "def test_orthogonal_direction():\n    p1 = Point(1, 2)\n    p2 = Point(3, 4, 5)\n\n    assert p1.orthogonal_direction == Point(-2, 1)\n    assert p2.orthogonal_direction == Point(-2, 1, 0)\n    assert Point(0, 0).orthogonal_direction == Point(1, 0)\n    assert Point(0, 0, 0).orthogonal_direction == Point(1, 0, 0)\n", "def test_direction_ratio():\n    p1 = Point3D(0, 0, 0)\n    p2 = Point3D(1, 1, 1)\n\n    assert p1.direction_ratio(Point3D(1, 0, 0)) == [1, 0, 0]\n    assert p1.direction_ratio(Point3D(0, 1, 0)) == [0, 1, 0]\n    assert p1.direction_ratio(Point3D(0, 0, pi)) == [0, 0, pi]\n\n    assert p1.direction_ratio(Point3D(5, 0, 0)) == [5, 0, 0]\n    assert p1.direction_ratio(Point3D(0, sqrt(3), 0)) == [0, sqrt(3), 0]\n    assert p1.direction_ratio(Point3D(0, 0, 5)) == [0, 0, 5]\n\n    assert p1.direction_ratio(Point3D(2.4, 2.4, 0)) == [2.4, 2.4, 0]\n    assert p1.direction_ratio(Point3D(1, 1, 1)) == [1, 1, 1]\n    assert p1.direction_ratio(Point3D(-12, 0 -15)) == [-12, -15, 0]\n\n    assert p2.direction_ratio(Point3D(0, 0, 0)) == [-1, -1, -1]\n    assert p2.direction_ratio(Point3D(1, 1, 12)) == [0, 0, 11]\n    assert p2.direction_ratio(Point3D(12, 1, 12)) == [11, 0, 11]\n", "def test_direction_ratio():\n    p1 = Point3D(0, 0, 0)\n    p2 = Point3D(1, 1, 1)\n\n    assert p1.direction_ratio(Point3D(1, 0, 0)) == [1, 0, 0]\n    assert p1.direction_ratio(Point3D(0, 1, 0)) == [0, 1, 0]\n    assert p1.direction_ratio(Point3D(0, 0, pi)) == [0, 0, pi]\n\n    assert p1.direction_ratio(Point3D(5, 0, 0)) == [5, 0, 0]\n    assert p1.direction_ratio(Point3D(0, sqrt(3), 0)) == [0, sqrt(3), 0]\n    assert p1.direction_ratio(Point3D(0, 0, 5)) == [0, 0, 5]\n\n    assert p1.direction_ratio(Point3D(2.4, 2.4, 0)) == [2.4, 2.4, 0]\n    assert p1.direction_ratio(Point3D(1, 1, 1)) == [1, 1, 1]\n    assert p1.direction_ratio(Point3D(-12, 0 -15)) == [-12, -15, 0]\n\n    assert p2.direction_ratio(Point3D(0, 0, 0)) == [-1, -1, -1]\n    assert p2.direction_ratio(Point3D(1, 1, 12)) == [0, 0, 11]\n    assert p2.direction_ratio(Point3D(12, 1, 12)) == [11, 0, 11]\n"], "sample_54": ["def test_urlize_autoescape(self):\n    tests = (\n        (\"Search for google.com/?q=! and see.\", 'Search for <a href=\"http://google.com/?q=\">google.com/?q=</a>! and see.'),\n        (\"Search for google.com/?q=1&lt! and see.\", 'Search for <a href=\"http://google.com/?q=1%3C\">google.com/?q=1&lt</a>! and see.'),\n        (lazystr(\"Search for google.com/?q=!\"), 'Search for <a href=\"http://google.com/?q=\">google.com/?q=</a>!'),\n        (\"foo@example.com\", '<a href=\"mailto:foo@example.com\">foo@example.com</a>'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value, autoescape=True), mark_safe(output))\n", "def test_urlize_trailing_punctuation(self):\n    tests = (\n        (\"Check out www.google.com.\", 'Check out <a href=\"http://www.google.com\">www.google.com</a>.'),\n        (\"What is your email? foo@example.com!\", 'What is your email? <a href=\"mailto:foo@example.com\">foo@example.com</a>!'),\n        (\"The URL is http://example.com.\", 'The URL is <a href=\"http://example.com\">http://example.com</a>.'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n", "def test_urlize_with_trailing_punctuation(self):\n    tests = (\n        (\"Check out www.google.com.\", 'Check out <a href=\"http://www.google.com\">www.google.com</a>.'),\n        (\"I love example.com!\", 'I love <a href=\"http://example.com\">example.com</a>!'),\n        (\"Visit https://www.djangoproject.com/\", 'Visit <a href=\"https://www.djangoproject.com/\">https://www.djangoproject.com/</a>'),\n        (\"My email is test@example.com.\", 'My email is <a href=\"mailto:test@example.com\">test@example.com</a>.'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n", "def test_urlize_with_trailing_punctuation(self):\n    tests = (\n        (\"Check out www.google.com.\", 'Check out <a href=\"http://www.google.com\">www.google.com</a>.'),\n        (\"Check out www.google.com?\", 'Check out <a href=\"http://www.google.com\">www.google.com</a>?'),\n        (\"Check out www.google.com!\", 'Check out <a href=\"http://www.google.com\">www.google.com</a>!'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value), output)\n", "def test_urlize_no_follow(self):\n    tests = (\n        (\n            \"Search for google.com/?q=! and see.\",\n            'Search for <a href=\"http://google.com/?q=\" rel=\"nofollow\">'\n            \"google.com/?q=</a>! and see.\",\n        ),\n        (\n            lazystr(\"Search for google.com/?q=!\"),\n            'Search for <a href=\"http://google.com/?q=\" rel=\"nofollow\">'\n            \"google.com/?q=</a>!\",\n        ),\n        (\n            \"foo@example.com\",\n            '<a href=\"mailto:foo@example.com\" rel=\"nofollow\">foo@example.com</a>',\n        ),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlize(value, nofollow=True), output)\n"], "sample_148": ["def test_periodic_argument_nonpolar():\n    from sympy import periodic_argument, pi\n    assert periodic_argument(1 + 1j, 2*pi).doit() == pi/4\n", "def test_polar_lift():\n    from sympy import polar_lift, exp_polar, pi, I\n    x = Symbol('x')\n    assert polar_lift(2 + I) == 2*exp_polar(I*pi/4)*sqrt(2)/2 + exp_polar(I*pi/4)*I*sqrt(2)/2\n    assert polar_lift(x).is_polar is True\n    assert polar_lift(exp_polar(I)).as_real_imag() == (-1, 0)\n    assert polar_lift(2).as_real_imag() == (2, 0)\n    assert polar_lift(I).as_real_imag() == (0, 1)\n", "def test_issue_20680():\n    from sympy import Symbol, sin, cos, exp_polar\n    x = Symbol('x')\n    assert re(exp_polar(x)) == exp(re(x))*cos(im(x))\n    assert im(exp_polar(x)) == exp(re(x))*sin(im(x))\n", "def test_polar_lift():\n    from sympy import polar_lift, exp_polar, pi, I\n    x = Symbol('x')\n    assert polar_lift(x) == x*exp_polar(0)\n    assert polar_lift(2 + 3*I) == (2 + 3*I)*exp_polar(0)\n    assert polar_lift(-2 - 3*I) == (-2 - 3*I)*exp_polar(I*pi)\n    assert polar_lift((2 + 3*I)*(4 + 5*I)) == ((2 + 3*I)*(4 + 5*I))*exp_polar(0)\n    assert polar_lift((2 + 3*I)/(4 + 5*I)) == ((2 + 3*I)/(4 + 5*I))*exp_polar(0)\n", "def test_issue_19462():\n    from sympy import MatrixSymbol, im, re\n    A = MatrixSymbol(\"A\", 2, 2)\n    assert im(A[0, 0]).is_real is True\n    assert re(A[0, 0]).is_real is True\n"], "sample_155": ["def test_get_units_non_prefixed():\n    assert set(SI.get_units_non_prefixed()) == {\n        meter, second, joule, coulomb, gram, mole, kelvin, ampere, katal,\n        radian, steradian, hertz, newton, pascal, watt, volt, farad,\n        ohm, siemens, weber, tesla, henry, celsius, gray, sievert, lux,\n        becquerel, dimensionless\n    }\n", "def test_derived_units():\n    assert isinstance(SI.derived_units, dict)\n    for dim, unit in SI.derived_units.items():\n        assert isinstance(dim, Dimension)\n        assert isinstance(unit, Quantity)\n    # Add more assertions to check the contents of derived_units\n    # For example:\n    assert length in SI.derived_units\n    assert SI.derived_units[length] == meter\n    # Add more assertions here...\n", "def test_extend():\n    extended_si = SI.extend(base=(meter,), units=(kilometer,), name=\"Extended SI\", description=\"An extension of the SI system\")\n    assert extended_si.name == \"Extended SI\"\n    assert extended_si.descr == \"An extension of the SI system\"\n    assert extended_si._base_units == SI._base_units + (meter,)\n    assert extended_si._units == SI._units + (kilometer,)\n    assert extended_si.get_dimension_system() == SI.get_dimension_system()\n", "def test_get_units_non_prefixed():\n    units = SI.get_units_non_prefixed()\n    assert meter in units\n    assert joule in units\n    assert day in units\n    assert second in units\n    assert volt in units\n    assert ohm in units\n    assert centimeter not in units\n    assert kilometer not in units\n    assert kilogram not in units\n    assert pebibyte not in units\n", "def test_unit_system_get_default_unit_system():\n    assert UnitSystem.get_default_unit_system() == SI\n    assert UnitSystem.get_unit_system(\"SI\") == SI\n    with raises(ValueError):\n        UnitSystem.get_unit_system(\"NonExistentUnitSystem\")\n"], "sample_83": ["def test_colorized_output() -> None:\n    \"\"\"Test colorized output.\"\"\"\n    output = StringIO()\n    reporter = ColorizedTextReporter(output)\n    msg_style = reporter._get_decoration(\"C\")\n    assert msg_style.color is None\n    assert msg_style.style == (\"bold\",)\n\n    msg = Message(\n        symbol=\"missing-docstring\",\n        msg_id=\"C0123\",\n        location=MessageLocationTuple(\"abspath\", \"path\", \"module\", \"obj\", 1, 2, 1, 3),\n        msg=\"Missing docstring\",\n        confidence=HIGH,\n    )\n    reporter.handle_message(msg)\n    out_lines = output.getvalue().split(\"\\n\")\n    assert len(out_lines) == 2\n    assert out_lines[0].startswith(\"\\033\")  # ANSI escape code\n    assert \"Missing docstring\" in out_lines[1]\n", "def test_colorized_text_reporter_deprecated_color_mapping() -> None:\n    \"\"\"TODO remove in 3.0.\"\"\"\n    color_mapping = {\"I\": (\"green\", \"bold\")}\n    reporter = ColorizedTextReporter(color_mapping=color_mapping)\n    warning = reporter.color_mapping[\"I\"]\n    assert isinstance(warning, MessageStyle)\n    assert warning.color == \"green\"\n    assert warning.style == (\"bold\",)\n", "def test_colorized_text_reporter_color_mapping_deprecated():\n    with warnings.catch_warnings(record=True) as cm:\n        warnings.simplefilter(\"always\")\n        ColorizedTextReporter(color_mapping={\"I\": (\"green\", \"bold\")})\n\n    assert len(cm) == 1\n    assert isinstance(cm[0].message, DeprecationWarning)\n", "def test_colorized_text_reporter(linter: PyLinter) -> None:\n    \"\"\"Test the ColorizedTextReporter.\"\"\"\n    output = StringIO()\n    reporter = ColorizedTextReporter(output)\n    linter.reporter = reporter\n    linter.open()\n    linter.set_current_module(\"my_mod\")\n\n    # Test that message is colorized correctly\n    linter.add_message(\"C0301\", line=1, args=(1, 2))\n    out_lines = output.getvalue().split(\"\\n\")\n    assert \"\\033[\" in out_lines[1]  # Check for ANSI escape code\n\n    # Test that invalid color does not raise exception\n    reporter.color_mapping[\"I\"] = MessageStyle(\" invalid_color\")\n    linter.add_message(\"I0011\", line=2)\n    out_lines = output.getvalue().split(\"\\n\")\n    assert \"invalid_color\" not in out_lines[2]\n\n    # Test that message is not colorized if color is None\n    reporter.color_mapping[\"I\"] = MessageStyle(None)\n    linter.add_message(\"I0011\", line=3)\n    out_lines = output.getvalue().split(\"\\n\")\n    assert \"\\033[\" not in out_lines[3]\n", "def test_colorized_text_reporter_output() -> None:\n    \"\"\"Test ColorizedTextReporter's output.\"\"\"\n    output = StringIO()\n    reporter = ColorizedTextReporter(output)\n    msg = Message(\n        symbol=\"missing-docstring\",\n        msg_id=\"C0123\",\n        location=MessageLocationTuple(\"abspath\", \"path\", \"module\", \"obj\", 1, 2, 1, 3),\n        msg=\"Missing docstring\",\n        confidence=HIGH,\n    )\n    reporter.handle_message(msg)\n    assert \"\\033[\" in output.getvalue()\n    assert \"Missing docstring\" in output.getvalue()\n"], "sample_43": ["def test_get_context_data(self):\n    q = Question.objects.create(question='Is this a question?')\n    request = self.factory.get(self.url, {'term': 'is', **self.opts})\n    request.user = self.superuser\n    view = AutocompleteJsonView.as_view(**self.as_view_args)\n    response = view(request)\n    context = response.context_data\n    self.assertEqual(context['object_list'].count(), 1)\n    self.assertEqual(context['object_list'][0], q)\n    self.assertIsInstance(context['page_obj'], type(response.paginator.page(1)))\n", "def test_to_field_allowed(self):\n    class RestrictedQuestionAdmin(QuestionAdmin):\n            return False\n\n    with model_admin(Question, RestrictedQuestionAdmin):\n        request = self.factory.get(self.url, {'term': 'is', **self.opts})\n        request.user = self.superuser\n        with self.assertRaises(PermissionDenied):\n            AutocompleteJsonView.as_view(**self.as_view_args)(request)\n", "def test_get_context_data(self):\n    request = self.factory.get(self.url, {'term': 'is', **self.opts})\n    request.user = self.superuser\n    view = AutocompleteJsonView.as_view(**self.as_view_args)\n    response = view(request)\n    context = response.context_data\n    self.assertIn('object_list', context)\n    self.assertIn('page_obj', context)\n    self.assertEqual(context['page_obj'].number, 1)\n", "def test_process_request_invalid_app_label(self):\n    request = self.factory.get(self.url, {'term': 'is', 'app_label': 'invalid', 'model_name': Answer._meta.model_name, 'field_name': 'question'})\n    request.user = self.superuser\n    with self.assertRaises(PermissionDenied):\n        AutocompleteJsonView.as_view(**self.as_view_args)(request)\n", "def test_process_request_invalid_app_label(self):\n    request = self.factory.get(self.url, {'term': 'is', 'app_label': ' invalid', 'model_name': Answer._meta.model_name, 'field_name': 'question'})\n    request.user = self.superuser\n    with self.assertRaises(PermissionDenied):\n        AutocompleteJsonView.as_view(**self.as_view_args)(request)\n"], "sample_137": ["def test_roundrobin():\n    gen = roundrobin('ABC', 'D', 'EF')\n    assert list(gen) == ['A', 'D', 'E', 'B', 'F', 'C']\n", "def test_roundrobin():\n    from itertools import islice\n    assert list(roundrobin('ABC', 'D', 'EF')) == [\n        'A', 'D', 'E',\n        'B', 'F',\n        'C'\n    ]\n    assert ''.join(islice(roundrobin('ABC', 'D', 'EF'), 3)) == 'ADE'\n", "def test_roundrobin():\n    gen = roundrobin('ABC', 'D', 'EF')\n    assert list(gen) == ['A', 'D', 'E', 'B', 'F', 'C']\n    gen = roundrobin([1, 2], [3, 4, 5], [6, 7])\n    assert list(gen) == [1, 3, 6, 2, 4, 7, 5]\n", "def test_roundrobin():\n    from itertools import tee\n    its = [iter([1, 2]), iter([3, 4]), iter([5, 6])]\n    expected = [1, 3, 5, 2, 4, 6]\n    actual = list(roundrobin(*its))\n    assert actual == expected\n\n    its = [iter([1, 2, 3]), iter([]), iter([4])]\n    expected = [1, 4, 2, 3]\n    actual = list(roundrobin(*its))\n    assert actual == expected\n\n    its = []\n    expected = []\n    actual = list(roundrobin(*its))\n    assert actual == expected\n\n    its = [iter([1])]\n    expected = [1]\n    actual = list(roundrobin(*its))\n    assert actual == expected\n\n    # Test that roundrobin works with infinite iterators\n    its = [iter([1, 2] * 1000), iter([3, 4] * 1000)]\n    result = list(islice(roundrobin(*its), 8))\n    assert result == [1, 3, 2, 4, 1, 3, 2, 4]\n", "def test_roundrobin():\n    gen = roundrobin('ABC', 'D', 'EF')\n    assert list(gen) == ['A', 'D', 'E', 'B', 'F', 'C']\n"], "sample_68": ["def test_update_conflicts_non_unique_field(self):\n    with self.assertRaisesMessage(ValueError, \"bulk_create() can only be used with concrete fields in update_fields.\"):\n        TwoFields.objects.bulk_create(\n            [TwoFields(f1=1, f2=1)],\n            update_conflicts=True,\n            update_fields=[\"nonexistent\"],\n            unique_fields=[\"f1\"],\n        )\n", "def test_update_conflicts_unique_fields_update_fields_db_column_with_pk(self):\n    FieldsWithDbColumns.objects.bulk_create(\n        [\n            FieldsWithDbColumns(id=1, rank=1, name=\"a\"),\n            FieldsWithDbColumns(id=2, rank=2, name=\"b\"),\n        ]\n    )\n    self.assertEqual(FieldsWithDbColumns.objects.count(), 2)\n\n    conflicting_objects = [\n        FieldsWithDbColumns(id=1, rank=1, name=\"c\"),\n        FieldsWithDbColumns(id=2, rank=2, name=\"d\"),\n    ]\n    results = FieldsWithDbColumns.objects.bulk_create(\n        conflicting_objects,\n        update_conflicts=True,\n        unique_fields=[\"id\"],\n        update_fields=[\"name\"],\n    )\n    self.assertEqual(len(results), len(conflicting_objects))\n    if connection.features.can_return_rows_from_bulk_insert:\n        for instance in results:\n            self.assertIsNotNone(instance.pk)\n    self.assertEqual(FieldsWithDbColumns.objects.count(), 2)\n    self.assertCountEqual(\n        FieldsWithDbColumns.objects.values(\"id\", \"rank\", \"name\"),\n        [\n            {\"id\": 1, \"rank\": 1, \"name\": \"c\"},\n            {\"id\": 2, \"rank\": 2, \"name\": \"d\"},\n        ],\n    )\n", "def test_update_conflicts_update_fields_non_existent_field(self):\n    unique_fields = None\n    if connection.features.supports_update_conflicts_with_target:\n        unique_fields = [\"number\"]\n    msg = \"UpsertConflict has no field named 'nonexistent'\"\n    with self.assertRaisesMessage(FieldDoesNotExist, msg):\n        UpsertConflict.objects.bulk_create(\n            [\n                UpsertConflict(number=1, rank=4, name=\"Steve\"),\n                UpsertConflict(number=2, rank=2, name=\"Olivia\"),\n            ],\n            update_conflicts=True,\n            update_fields=[\"rank\", \"nonexistent\"],\n            unique_fields=unique_fields,\n        )\n", "def test_update_conflicts_unique_fields_update_fields_with_db_column(self):\n    FieldsWithDbColumns.objects.bulk_create(\n        [\n            FieldsWithDbColumns(rank=1, name=\"a\"),\n            FieldsWithDbColumns(rank=2, name=\"b\"),\n        ]\n    )\n    self.assertEqual(FieldsWithDbColumns.objects.count(), 2)\n\n    conflicting_objects = [\n        FieldsWithDbColumns(rank=1, name=\"c\"),\n        FieldsWithDbColumns(rank=2, name=\"d\"),\n    ]\n    results = FieldsWithDbColumns.objects.bulk_create(\n        conflicting_objects,\n        update_conflicts=True,\n        unique_fields=[\"rank\"],\n        update_fields=[FieldsWithDbColumns.name.field.db_column],\n    )\n    self.assertEqual(len(results), len(conflicting_objects))\n    if connection.features.can_return_rows_from_bulk_insert:\n        for instance in results:\n            self.assertIsNotNone(instance.pk)\n    self.assertEqual(FieldsWithDbColumns.objects.count(), 2)\n    self.assertCountEqual(\n        FieldsWithDbColumns.objects.values(\"rank\", \"name\"),\n        [\n            {\"rank\": 1, \"name\": \"c\"},\n            {\"rank\": 2, \"name\": \"d\"},\n        ],\n    )\n", "def test_update_conflicts_floating_point_fields(self):\n    FloatingPointFields.objects.bulk_create(\n        [\n            FloatingPointFields(float_field=1.0, double_field=2.0),\n            FloatingPointFields(float_field=3.0, double_field=4.0),\n        ]\n    )\n    self.assertEqual(FloatingPointFields.objects.count(), 2)\n\n    conflicting_objects = [\n        FloatingPointFields(float_field=1.0, double_field=10.0),\n        FloatingPointFields(float_field=3.0, double_field=12.0),\n    ]\n    results = FloatingPointFields.objects.bulk_create(\n        conflicting_objects,\n        update_conflicts=True,\n        update_fields=[\"double_field\"],\n        unique_fields=[\"float_field\"],\n    )\n    self.assertEqual(len(results), len(conflicting_objects))\n    if connection.features.can_return_rows_from_bulk_insert:\n        for instance in results:\n            self.assertIsNotNone(instance.pk)\n    self.assertEqual(FloatingPointFields.objects.count(), 2)\n    self.assertCountEqual(\n        FloatingPointFields.objects.values(\"float_field\", \"double_field\"),\n        [\n            {\"float_field\": 1.0, \"double_field\": 10.0},\n            {\"float_field\": 3.0, \"double_field\": 12.0},\n        ],\n    )\n"], "sample_119": ["def test_user_functions():\n    assert mcode(sin(x), user_functions={'sin': 'mySin'}) == \"mySin[x]\"\n    assert mcode(cos(x), user_functions={'cos': 'myCos'}) == \"myCos[x]\"\n    assert mcode(exp(x), user_functions={'exp': 'myExp'}) == \"myExp[x]\"\n    assert mcode(sin(x) + cos(x), user_functions={'sin': 'mySin', 'cos': 'myCos'}) == \"mySin[x] + myCos[x]\"\n", "def test_user_functions():\n    assert mcode(sin(x), user_functions={'sin': 'MySin'}) == \"MySin[x]\"\n    assert mcode(cos(x), user_functions={'cos': 'MyCos'}) == \"MyCos[x]\"\n    assert mcode(exp(x), user_functions={'exp': 'MyExp'}) == \"MyExp[x]\"\n    assert mcode(sin(x) + cos(x), user_functions={'sin': 'MySin', 'cos': 'MyCos'}) == \"MySin[x] + MyCos[x]\"\n", "def test_user_functions():\n    assert mcode(sin(x), user_functions={'sin': 'mySin'}) == \"mySin[x]\"\n    assert mcode(cos(x), user_functions={'cos': 'myCos'}) == \"myCos[x]\"\n    assert mcode(exp(x), user_functions={'exp': 'myExp'}) == \"myExp[x]\"\n    assert mcode(sin(x) + cos(x), user_functions={'sin': 'mySin', 'cos': 'myCos'}) == \"mySin[x] + myCos[x]\"\n", "def test_known_functions():\n    assert mcode(exp(x)) == \"Exp[x]\"\n    assert mcode(log(x)) == \"Log[x]\"\n    assert mcode(sin(x)) == \"Sin[x]\"\n    assert mcode(cos(x)) == \"Cos[x]\"\n    assert mcode(asin(x)) == \"ArcSin[x]\"\n    assert mcode(acos(x)) == \"ArcCos[x]\"\n    assert mcode(atan(x)) == \"ArcTan[x]\"\n    assert mcode(sinh(x)) == \"Sinh[x]\"\n    assert mcode(cosh(x)) == \"Cosh[x]\"\n    assert mcode(tanh(x)) == \"Tanh[x]\"\n    assert mcode(coth(x)) == \"Coth[x]\"\n    assert mcode(sech(x)) == \"Sech[x]\"\n    assert mcode(csch(x)) == \"Csch[x]\"\n    assert mcode(asinh(x)) == \"ArcSinh[x]\"\n    assert mcode(acosh(x)) == \"ArcCosh[x]\"\n    assert mcode(atanh(x)) == \"ArcTanh[x]\"\n    assert mcode(acoth(x)) == \"ArcCoth[x]\"\n    assert mcode(asech(x)) == \"ArcSech[x]\"\n    assert mcode(acsch(x)) == \"ArcCsch[x]\"\n", "def test_known_functions():\n    assert mcode(exp(x)) == \"Exp[x]\"\n    assert mcode(log(x)) == \"Log[x]\"\n    assert mcode(sin(x)) == \"Sin[x]\"\n    assert mcode(cos(x)) == \"Cos[x]\"\n    assert mcode(asin(x)) == \"ArcSin[x]\"\n    assert mcode(acos(x)) == \"ArcCos[x]\"\n    assert mcode(atan(x)) == \"ArcTan[x]\"\n    assert mcode(sinh(x)) == \"Sinh[x]\"\n    assert mcode(cosh(x)) == \"Cosh[x]\"\n    assert mcode(tanh(x)) == \"Tanh[x]\"\n    assert mcode(coth(x)) == \"Coth[x]\"\n    assert mcode(sech(x)) == \"Sech[x]\"\n    assert mcode(csch(x)) == \"Csch[x]\"\n    assert mcode(asinh(x)) == \"ArcSinh[x]\"\n    assert mcode(acosh(x)) == \"ArcCosh[x]\"\n    assert mcode(atanh(x)) == \"ArcTanh[x]\"\n    assert mcode(acoth(x)) == \"ArcCoth[x]\"\n    assert mcode(asech(x)) == \"ArcSech[x]\"\n    assert mcode(acsch(x)) == \"ArcCsch[x]\"\n"], "sample_79": ["def test_concat_positions(self):\n    # Test that the positions argument is correctly used when concatenating\n    # along a new dimension.\n    datasets = [\n        Dataset({\"a\": (\"x\", [1, 2])}, coords={\"x\": [0, 1]}),\n        Dataset({\"a\": (\"x\", [3, 4])}, coords={\"x\": [0, 1]}),\n        Dataset({\"a\": (\"x\", [5, 6])}, coords={\"x\": [0, 1]}),\n    ]\n    positions = [0, 2, 1]\n    actual = concat(datasets, dim=\"y\", positions=positions)\n    expected = Dataset(\n        {\"a\": ((\"y\", \"x\"), [[1, 2], [5, 6], [3, 4]])},\n        coords={\"x\": [0, 1], \"y\": [0, 2, 1]},\n    )\n    assert_identical(actual, expected)\n", "def test_concat_positions(self):\n    # Test that the positions argument works as expected\n    foo = DataArray([1, 2], coords=[(\"x\", [1, 2])])\n    bar = DataArray([3, 4], coords=[(\"x\", [3, 4])])\n\n    # Concatenate along a new dimension\n    actual = concat((foo, bar), dim=\"y\", positions=[0, 1])\n    expected = DataArray(\n        [[1, 2], [3, 4]], dims=[\"y\", \"x\"], coords={\"x\": [1, 2], \"y\": [0, 1]}\n    )\n    assert_identical(actual, expected)\n\n    # Concatenate along an existing dimension\n    actual = concat((foo, bar), dim=\"x\", positions=[0, 2])\n    expected = DataArray([1, 2, np.nan, 3, 4], coords=[(\"x\", [1, 2, 3, 4, 5])])\n    assert_identical(actual, expected)\n", "def test_concat_dim_with_coords(self):\n    ds1 = Dataset({\"x\": 0})\n    ds2 = Dataset({\"x\": 1})\n    coord = Variable(\"y\", [3, 4])\n    expected = Dataset({\"x\": (\"y\", [0, 1]), \"y\": [3, 4]})\n    actual = concat([ds1, ds2], coord)\n    assert_identical(actual, expected)\n\n    # Check that error is raised when coordinate lengths don't match\n    coord = Variable(\"y\", [3, 4, 5])\n    with raises_regex(ValueError, \"Length of concat dimension\"):\n        concat([ds1, ds2], coord)\n", "def test_concat_with_mismatched_coords(self):\n    ds1 = Dataset({\"a\": ((\"x\", \"y\"), [[0]])}, coords={\"x\": [0], \"y\": [0]})\n    ds2 = Dataset({\"a\": ((\"x\", \"y\"), [[0]])}, coords={\"x\": [1], \"z\": [0]})\n\n    with raises_regex(ValueError, \"coordinate in some datasets but not others\"):\n        concat([ds1, ds2], dim=\"x\")\n", "def test_concat_positions(self):\n    # Test that the positions argument works as expected\n    ds1 = Dataset({\"x\": [1, 2]})\n    ds2 = Dataset({\"x\": [3, 4]})\n    ds3 = Dataset({\"x\": [5, 6]})\n\n    # Specify positions for each dataset\n    positions = [[0, 1], [2, 3], [4, 5]]\n    actual = concat([ds1, ds2, ds3], dim=\"y\", positions=positions)\n    expected = Dataset({\"x\": (\"y\", [1, 2, 3, 4, 5, 6])})\n    assert_identical(actual, expected)\n\n    # Specify positions with a different length than the datasets\n    positions = [[0, 1], [2, 3]]\n    with raises_regex(ValueError, \"Length of positions does not match length of datasets\"):\n        concat([ds1, ds2, ds3], dim=\"y\", positions=positions)\n"], "sample_88": ["def test_saferepr_recursive():\n    \"\"\"Test saferepr() with recursive objects.\"\"\"\n\n    class Recursive:\n            self.ref = None\n\n            return \"Recursive({})\".format(self.ref)\n\n    a = Recursive()\n    b = Recursive()\n    a.ref = b\n    b.ref = a\n\n    # The repr should indicate recursion\n    assert \"...\" in saferepr(a)\n", "def test_saferepr_with_recursive_object():\n    \"\"\"Test saferepr() with recursive objects.\"\"\"\n    class RecursiveObject:\n            self.name = name\n            self.children = []\n\n            return f\"RecursiveObject({self.name})\"\n\n    obj1 = RecursiveObject(\"obj1\")\n    obj2 = RecursiveObject(\"obj2\")\n    obj1.children.append(obj2)\n    obj2.children.append(obj1)\n\n    assert saferepr(obj1).startswith(\"RecursiveObject(obj1)\")\n    assert \"...\" in saferepr(obj1)\n", "def test_saferepr_on_recursive_objects():\n    class Node:\n            self.value = value\n            self.children = []\n\n    node1 = Node(1)\n    node2 = Node(2)\n    node3 = Node(3)\n    node1.children.append(node2)\n    node2.children.append(node3)\n    node3.children.append(node1)  # create a cycle\n\n    assert saferepr(node1).startswith(\"Node(1)\")\n    assert \"...\" in saferepr(node1)  # check for ellipsis due to recursion\n", "def test_saferepr_recursive():\n    class Recursive:\n            return saferepr(self)\n\n    recursive_obj = Recursive()\n    assert saferepr(recursive_obj).startswith(\n        \"<[RecursionError('maximum recursion depth exceeded') raised in repr()]\"\n    )\n", "def test_saferepr_recursive():\n    class Recursive:\n            return saferepr(self)\n\n    assert saferepr(Recursive()).startswith(\n        \"<[RecursionError('maximum recursion depth exceeded') raised in repr()]\"\n    )\n"], "sample_21": ["def test_can_fast_delete_with_m2m_through_model(self):\n    m = M.objects.create()\n    r = R.objects.create()\n    MR.objects.create(m=m, r=r)\n    collector = Collector(using='default')\n    self.assertFalse(collector.can_fast_delete(m))\n    self.assertFalse(collector.can_fast_delete(r))\n    # Deleting an instance of the through model should be fast deletable.\n    mr = MR.objects.get(m=m, r=r)\n    self.assertTrue(collector.can_fast_delete(mr))\n", "def test_collector_sort(self):\n    # Create a collector and add some models to it.\n    collector = Collector(using='default')\n    collector.add([RChild.objects.create()], source=R, nullable=False)\n    collector.add([R.objects.create()], source=None, nullable=False)\n    collector.add([S.objects.create()], source=R, nullable=False)\n\n    # Check that the collector is sorted correctly.\n    self.assertEqual(collector.data, {R: {collector.data[R]}, RChild: {collector.data[RChild]}, S: {collector.data[S]}})\n    collector.sort()\n    self.assertEqual(collector.data, {RChild: {collector.data[RChild]}, R: {collector.data[R]}, S: {collector.data[S]}})\n", "def test_fast_delete_parent_before_child(self):\n    parent = Parent.objects.create()\n    child = Child.objects.create(parent_ptr=parent)\n    # 1 for self delete, 1 for fast delete of empty \"child\" qs.\n    with self.assertNumQueries(2):\n        parent.delete()\n    self.assertFalse(Parent.objects.exists())\n    self.assertFalse(Child.objects.exists())\n", "def test_fast_delete_with_existing_query(self):\n    u = User.objects.create()\n    qs = User.objects.filter(pk=u.pk)\n    # 1 query to delete the user, no additional query to delete related objects\n    self.assertNumQueries(1, qs.delete)\n    self.assertFalse(User.objects.exists())\n", "def test_fast_delete_m2m_through(self):\n    m = M.objects.create()\n    r = R.objects.create()\n    MR.objects.create(m=m, r=r)\n    # 1 query to delete r, 1 query to fast-delete MR\n    self.assertNumQueries(2, r.delete)\n    self.assertFalse(R.objects.exists())\n    self.assertFalse(MR.objects.exists())\n"], "sample_69": ["def test_artist_stale_callback():\n    class MockArtist(martist.Artist):\n            super().__init__()\n            self._stale_callback_called = False\n\n            self._stale_callback_called = True\n\n    artist = MockArtist()\n    artist.stale_callback = artist.stale_callback\n\n    # Test that the stale callback is called when the artist's stale state changes.\n    artist.stale = True\n    assert artist._stale_callback_called\n\n    # Reset the callback flag for the next test.\n    artist._stale_callback_called = False\n\n    # Test that the stale callback is not called when the artist's stale state does not change.\n    artist.stale = True\n    assert not artist._stale_callback_called\n\n    # Reset the callback flag and the artist's stale state for the next test.\n    artist._stale_callback_called = False\n    artist.stale = False\n\n    # Test that the stale callback is called when the artist's stale state changes.\n    artist.stale = True\n    assert artist._stale_callback_called\n", "def test_findobj():\n    fig, ax = plt.subplots()\n    ln, = ax.plot(range(5))\n\n    # find all lines\n    assert fig.findobj(mlines.Line2D) == [ln]\n\n    # find all artists with a specific label\n    ln.set_label('test')\n    assert fig.findobj(lambda x: x.get_label() == 'test') == [ln]\n", "def test_artist_sticky_edges():\n    artist = martist.Artist()\n    assert isinstance(artist.sticky_edges.x, list)\n    assert isinstance(artist.sticky_edges.y, list)\n\n    artist.sticky_edges.x[:] = [1, 2]\n    artist.sticky_edges.y[:] = [3, 4]\n\n    assert artist.sticky_edges.x == [1, 2]\n    assert artist.sticky_edges.y == [3, 4]\n", "def test_artist_stale_callback():\n    fig, ax = plt.subplots()\n    artist = martist.Artist()\n    ax.add_artist(artist)\n\n    stale_callback_called = False\n\n        nonlocal stale_callback_called\n        stale_callback_called = True\n\n    artist.stale_callback = stale_callback\n\n    artist.stale = True\n    assert stale_callback_called\n\n    stale_callback_called = False\n    artist.stale = False\n    assert not stale_callback_called\n", "def test_get_cursor_data():\n    class TestArtist(martist.Artist):\n            return \"Test cursor data\"\n\n    artist = TestArtist()\n    event = None  # mock event object\n    assert artist.get_cursor_data(event) == \"Test cursor data\"\n    assert artist.format_cursor_data(\"Test cursor data\") == \"[Test cursor data]\"\n"], "sample_121": ["def test_cycle_structure():\n    p = Permutation([1, 0, 2, 4, 3])\n    assert p.cycle_structure == {2: 1, 1: 3}\n    q = Permutation(0, 6)(1, 7)(2, 8)\n    assert q.cycle_structure == {2: 3, 1: 6}\n    r = Permutation(list(range(10)))\n    assert r.cycle_structure == {1: 10}\n    s = Permutation(0, 5, 3, 9, 8)(1, 7, 4, 6)(2)\n    assert s.cycle_structure == {5: 1, 4: 1, 1: 1}\n", "def test_cycle_structure():\n    p = Permutation([0, 1, 2])\n    assert p.cycle_structure == {1: 3}\n    p = Permutation([0, 2, 1])\n    assert p.cycle_structure == {1: 1, 2: 1}\n    p = Permutation([1, 2, 0])\n    assert p.cycle_structure == {3: 1}\n    p = Permutation([1, 0, 3, 2])\n    assert p.cycle_structure == {2: 2}\n    p = Permutation([0, 3, 1, 2])\n    assert p.cycle_structure == {1: 2, 2: 1}\n", "def test_cycle_structure():\n    p = Permutation(0, 1, 3)(2, 4)\n    assert p.cycle_structure == {2: 2}\n    q = Permutation(0, 3)(1, 2, 4)\n    assert q.cycle_structure == {2: 1, 3: 1}\n    r = Permutation(0, 1, 2, 3, 4)\n    assert r.cycle_structure == {5: 1}\n", "def test_cycle_structure():\n    p = Permutation([0, 1, 2])\n    assert p.cycle_structure == {1: 3}\n    p = Permutation([0, 2, 1])\n    assert p.cycle_structure == {1: 1, 2: 1}\n    p = Permutation([1, 2, 0])\n    assert p.cycle_structure == {3: 1}\n    p = Permutation([0, 1, 2, 3])\n    assert p.cycle_structure == {1: 4}\n    p = Permutation([3, 2, 1, 0])\n    assert p.cycle_structure == {2: 2}\n    p = Permutation([1, 0, 3, 2])\n    assert p.cycle_structure == {2: 2}\n", "def test_cycle_structure():\n    p = Permutation(0, 1, 2)\n    assert p.cycle_structure == {3: 1}\n    p = Permutation(0, 2, 1)\n    assert p.cycle_structure == {1: 1, 2: 1}\n    p = Permutation(4)(0, 3, 2, 1)\n    assert p.cycle_structure == {1: 1, 4: 1}\n    p = Permutation(2, 5, 6)(1, 4)\n    assert p.cycle_structure == {2: 1, 3: 1}\n"], "sample_58": ["def test_settings_to_cmd_args_env_with_empty_options(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env(\n            {\n                \"NAME\": \"dbname\",\n                \"USER\": \"someuser\",\n                \"HOST\": \"somehost\",\n                \"PORT\": \"444\",\n                \"OPTIONS\": {},\n            }\n        ),\n        (\n            [\"psql\", \"-U\", \"someuser\", \"-h\", \"somehost\", \"-p\", \"444\", \"dbname\"],\n            None,\n        ),\n    )\n", "def test_no_dbname_and_no_service(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env({}),\n        ([\"psql\", \"postgres\"], None),\n    )\n", "def test_settings_to_cmd_args_env_with_default_dbname(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env({}),\n        ([\"psql\", \"postgres\"], None),\n    )\n", "def test_default_database(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env({}),\n        ([\"psql\", \"postgres\"], None),\n    )\n", "def test_default_dbname(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env({}),\n        ([\"psql\", \"postgres\"], None),\n    )\n"], "sample_41": ["def test_formset_non_field_error(self):\n    \"\"\"A formset's non-form error is an instance of ErrorList.\"\"\"\n    data = {\n        'choices-TOTAL_FORMS': '2',  # the number of forms rendered\n        'choices-INITIAL_FORMS': '0',  # the number of forms with initial data\n        'choices-MIN_NUM_FORMS': '0',  # min number of forms\n        'choices-MAX_NUM_FORMS': '0',  # max number of forms\n        'choices-0-choice': 'Zero',\n        'choices-0-votes': '0',\n        'choices-1-choice': 'One',\n        'choices-1-votes': '1',\n    }\n    ChoiceFormSet = formset_factory(Choice, extra=1, max_num=1, validate_max=True)\n    formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n    self.assertFalse(formset.is_valid())\n    self.assertIsInstance(formset.non_form_errors(), ErrorList)\n    self.assertEqual(len(formset.non_form_errors()), 1)\n    self.assertEqual(str(formset.non_form_errors()[0]), 'Please submit at most 1 form.')\n", "def test_formset_absolute_max_deletion(self):\n    \"\"\"absolute_max limits the number of forms, including deleted ones.\"\"\"\n    ChoiceFormSet = formset_factory(Choice, extra=1, absolute_max=2, can_delete=True)\n    data = {\n        'choices-TOTAL_FORMS': '3',  # one more than absolute_max\n        'choices-INITIAL_FORMS': '3',\n        'choices-MIN_NUM_FORMS': '0',\n        'choices-MAX_NUM_FORMS': '3',\n        'choices-0-choice': 'Zero',\n        'choices-0-votes': '0',\n        'choices-0-DELETE': '',\n        'choices-1-choice': 'One',\n        'choices-1-votes': '1',\n        'choices-1-DELETE': 'on',  # deletion shouldn't affect absolute_max\n        'choices-2-choice': 'Two',\n        'choices-2-votes': '2',\n        'choices-2-DELETE': '',\n    }\n    formset = ChoiceFormSet(data, prefix='choices')\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(len(formset.forms), 2)\n", "def test_management_form_cleaned_data(self):\n    \"\"\"ManagementForm's cleaned_data contains all the fields.\"\"\"\n    formset = self.make_choiceformset([('Calexico', '100')])\n    management_form = formset.management_form\n    self.assertEqual(\n        management_form.cleaned_data,\n        {\n            'TOTAL_FORMS': 1,\n            'INITIAL_FORMS': 0,\n            'MIN_NUM_FORMS': 0,\n            'MAX_NUM_FORMS': 1000,\n        },\n    )\n", "def test_management_form_is_invalid(self):\n    \"\"\"A ManagementForm is invalid if it has non-integer data.\"\"\"\n    form = ManagementForm({\n        TOTAL_FORM_COUNT: 'abc',\n        INITIAL_FORM_COUNT: '1',\n    })\n    self.assertFalse(form.is_valid())\n    self.assertEqual(form.errors, {\n        TOTAL_FORM_COUNT: ['Enter a whole number.'],\n    })\n", "def test_formset_empty_management_form_error(self):\n    \"\"\"An empty management form raises an error.\"\"\"\n    data = {\n        'choices-TOTAL_FORMS': '',\n        'choices-INITIAL_FORMS': '',\n        'choices-MIN_NUM_FORMS': '',\n        'choices-MAX_NUM_FORMS': '',\n    }\n    formset = self.make_choiceformset(data)\n    self.assertFalse(formset.is_valid())\n    self.assertEqual(\n        formset.non_form_errors(),\n        [\n            'ManagementForm data is missing or has been tampered with. '\n            'Missing fields: choices-TOTAL_FORMS, choices-INITIAL_FORMS. '\n            'You may need to file a bug report if the issue persists.',\n        ],\n    )\n"], "sample_94": ["def test_indent() -> None:\n    source = Source(\n        \"\"\"\\\n            pass\n        \"\"\"\n    )\n    indented_source = source.indent(indent=\" \" * 8)\n    assert str(indented_source) == \"        def f(x):\\n            pass\"\n", "def test_getstatementrange_with_nested_blocks() -> None:\n    source = Source(\n        \"\"\"\\", "def test_getstatementrange_ast_with_nested_blocks() -> None:\n    source = Source(\n        \"\"\"\\", "def test_getstatementrange_with_empty_lines() -> None:\n    source = Source(\n        \"\"\"\\", "def test_getstatement_with_nested_blocks() -> None:\n    source = \"\"\"\\"], "sample_65": ["    def test_join_with_non_ascii(self):\n        output = self.engine.render_to_string(\"join09\", {\"a\": [\"alpha\", \"beta & me\"]})\n        self.assertEqual(output, \"alpha&nbsp;beta &amp; me\")\n", "    def test_truncatechars01(self):\n        output = self.engine.render_to_string(\"truncatechars01\", {\"a\": \"abcdefghij\"})\n        self.assertEqual(output, \"abcde...\")\n", "    def test_stringformat01(self):\n        output = self.engine.render_to_string(\"stringformat01\", {\"a\": 12345})\n        self.assertEqual(output, \"1.2345E+04\")\n", "    def test_add01(self):\n        output = self.engine.render_to_string(\"add01\", {\"a\": 1, \"b\": 2})\n        self.assertEqual(output, \"3\")\n", "    def test_add01(self):\n        output = self.engine.render_to_string(\"add01\", {\"a\": 5})\n        self.assertEqual(output, \"10\")\n"], "sample_72": ["def test_figure_repr():\n    fig = Figure(figsize=(6, 4))\n    assert repr(fig) == \"<Figure size 6x4 with 0 Axes>\"\n", "def test_subplots_adjust():\n    fig = plt.figure()\n    fig.subplots(1, 2)\n    fig.subplots_adjust(left=0.2, right=0.8, bottom=0.1, top=0.9, wspace=0.5,\n                        hspace=0.2)\n    assert fig.subplotpars.left == 0.2\n    assert fig.subplotpars.right == 0.8\n    assert fig.subplotpars.bottom == 0.1\n    assert fig.subplotpars.top == 0.9\n    assert fig.subplotpars.wspace == 0.5\n    assert fig.subplotpars.hspace == 0.2\n", "def test_subfigure_add_subplot():\n    fig = plt.figure()\n    subfig = fig.subfigures(1)\n    ax = subfig[0].add_subplot(111)\n    assert len(subfig[0].axes) == 1\n    assert len(fig.axes) == 1\n    assert ax in fig.axes\n    assert ax.get_subplotspec().get_gridspec() is not None\n", "def test_subplots_adjust():\n    fig = plt.figure()\n    fig.subplots(1, 2)\n    fig.subplots_adjust(wspace=0.5)\n    assert fig.subplotpars.wspace == 0.5\n    assert fig.get_layout_engine() is None\n", "def test_figure_add_subplot():\n    fig = Figure()\n    ax1 = fig.add_subplot(121)\n    ax2 = fig.add_subplot(122, sharex=ax1)\n    assert ax1 is not ax2\n    assert ax1.get_shared_x_axes().joined(ax1, ax2)\n"], "sample_122": ["def test_row_structure_symbolic_cholesky():\n    S = SparseMatrix([\n        [1, 0, 3, 2],\n        [0, 0, 1, 0],\n        [4, 0, 0, 5],\n        [0, 6, 7, 0]])\n    assert S.row_structure_symbolic_cholesky() == [[0], [], [0], [1, 2]]\n", "def test_sparse_matrix_multiplication_with_zero():\n    a = SparseMatrix(((1, 2), (3, 4)))\n    b = SparseMatrix(2, 2, {})\n    assert a*b == SparseMatrix(2, 2, {})\n    assert b*a == SparseMatrix(2, 2, {})\n    c = SparseMatrix(((0, 0), (0, 0)))\n    assert a*c == SparseMatrix(2, 2, {})\n    assert c*a == SparseMatrix(2, 2, {})\n", "def test_sparse_matrix_liupc():\n    # Liu's algorithm, for pre-determination of the Elimination Tree of\n    # the given matrix, used in row-based symbolic Cholesky factorization.\n    S = SparseMatrix([\n        [1, 0, 3, 2],\n        [0, 0, 1, 0],\n        [4, 0, 0, 5],\n        [0, 6, 7, 0]])\n    R, parent = S.liupc()\n    assert R == [[0], [], [0], [1, 2]]\n    assert parent == [4, 3, 4, 4]\n", "def test_fill():\n    A = SparseMatrix(3, 3, {})\n    A.fill(1)\n    assert A == Matrix([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n    A.fill(0)\n    assert A == Matrix([[0, 0, 0], [0, 0, 0], [0, 0, 0]])\n    A.fill(2)\n    assert A == Matrix([[2, 2, 2], [2, 2, 2], [2, 2, 2]])\n", "def test_has():\n    assert SparseMatrix([[0, 1], [0, 0]]).has(1)\n    assert not SparseMatrix([[0, 1], [0, 0]]).has(2)\n    x = Symbol('x')\n    assert SparseMatrix([[0, x], [0, 0]]).has(x)\n    assert SparseMatrix([[0, x], [0, 0]]).has(S.Zero) is None\n    assert SparseMatrix([[0, 0], [0, 0]]).has(S.Zero)\n"], "sample_91": ["def test_xfail_strict_ini(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        xfail_strict = True\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail\n            assert 1\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*XPASS(strict)*test_func*\"])\n", "def test_skipif_with_multiple_conditions(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.skipif(\"True or False\")\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-rs\")\n    result.stdout.fnmatch_lines([\"*SKIP*condition: True or False*\"])\n", "def test_xfail_strict_ini(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        xfail_strict = True\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail\n            assert 1\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*1 failed*\"])\n    assert result.ret == 1\n", "def test_xfail_mark_with_no_args(testdir):\n    item = testdir.getitem(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail()\n            assert 0\n    \"\"\"\n    )\n    xfailed = evaluate_xfail_marks(item)\n    assert xfailed\n    assert xfailed.reason == \"\"\n    assert xfailed.run\n    assert xfailed.strict is False\n    assert xfailed.raises is None\n", "def test_xfail_strict_ini(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        xfail_strict = True\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*1 failed*\"])\n"], "sample_115": ["def test_wrap_data_with_container_sparse_input():\n    \"\"\"Check _wrap_data_with_container with sparse input.\"\"\"\n    from scipy.sparse import csr_matrix\n\n    X = csr_matrix(np.asarray([[1, 0, 3], [0, 0, 1]]))\n    est = EstimatorWithSetOutput().fit(X.toarray())\n\n    # Sparse input should not be wrapped as pandas DataFrame\n    X_wrapped = _wrap_data_with_container(\"transform\", X, X, est)\n    assert isinstance(X_wrapped, csr_matrix)\n", "def test_set_output_pandas_columns_callable():\n    \"\"\"Check that set_output works with a callable for columns.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    X = np.asarray([[1, 0, 3], [0, 0, 1]])\n    est = EstimatorWithSetOutput().fit(X)\n\n        return [\"feat1\", \"feat2\", \"feat3\"]\n\n    est.set_output(transform=\"pandas\")\n    X_trans_pd = _wrap_in_pandas_container(X, columns=get_columns)\n    assert_array_equal(X_trans_pd.columns, [\"feat1\", \"feat2\", \"feat3\"])\n", "def test__wrap_in_pandas_container_none_columns():\n    \"\"\"Check _wrap_in_pandas_container with None columns.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    X = np.asarray([[1, 0, 3], [0, 0, 1]])\n    X_df = pd.DataFrame(X)\n\n    # Check that None columns does not change the DataFrame\n    wrapped_df = _wrap_in_pandas_container(X_df, columns=None)\n    assert_array_equal(wrapped_df.columns, X_df.columns)\n\n    # Check that None columns uses range for ndarray\n    wrapped_df = _wrap_in_pandas_container(X, columns=None)\n    assert_array_equal(wrapped_df.columns, range(X.shape[1]))\n", "def test_set_output_pandas_override_index():\n    \"\"\"Check that set_output overrides index when input is not a DataFrame.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    X = np.asarray([[1, 2, 3], [4, 5, 6]])\n    est = EstimatorWithSetOutput().set_output(transform=\"pandas\")\n    est.fit(X)\n\n    X_trans = est.transform(X)\n    assert_array_equal(X_trans.index, range(X.shape[0]))\n", "def test__wrap_in_pandas_container_columns_none():\n    \"\"\"Check _wrap_in_pandas_container with columns=None.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    X = np.asarray([[1, 0, 3], [0, 0, 1]])\n\n    # If data_to_wrap is a ndarray and columns is None, then use range(n_features)\n    dense_named = _wrap_in_pandas_container(X, columns=None)\n    assert isinstance(dense_named, pd.DataFrame)\n    assert_array_equal(dense_named.columns, range(X.shape[1]))\n\n    # If data_to_wrap is a DataFrame and columns is None, then do not change the columns\n    X_df = pd.DataFrame(X, columns=[\"a\", \"b\", \"c\"])\n    new_df = _wrap_in_pandas_container(X_df, columns=None)\n    assert_array_equal(new_df.columns, X_df.columns)\n"], "sample_15": ["def test_invalid_variant_consistent_language_settings(self):\n    tests = [\n        # language + region that doesn't exist.\n        'fr-ABC',\n        # language + script.\n        'zh-Hans',\n        # language + invalid variant.\n        'ca-ES-invalid',\n    ]\n    msg = (\n        'You have provided a value for the LANGUAGE_CODE setting that is '\n        'not in the LANGUAGES setting.'\n    )\n    for tag in tests:\n        with self.subTest(tag), self.settings(LANGUAGE_CODE=tag):\n            self.assertEqual(check_language_settings_consistent(None), [\n                Error(msg, id='translation.E004'),\n            ])\n", "def test_valid_languages_bidi_subset_of_languages(self):\n    self.assertEqual(check_language_settings_consistent(None), [])\n", "def test_language_code_script_variant_consistent_language_settings(self):\n    tests = [\n        # language + script.\n        'zh-Hans',\n    ]\n    for tag in tests:\n        with self.subTest(tag), self.settings(LANGUAGE_CODE=tag):\n            self.assertEqual(check_language_settings_consistent(None), [])\n", "def test_invalid_variant_consistent_language_settings(self):\n    tests = [\n        # language + region that doesn't exist.\n        'fr-ABC',\n        # language + script.\n        'zh-Hans',\n        # language + invalid variant.\n        'ca-ES-invalid',\n    ]\n    msg = (\n        'You have provided a value for the LANGUAGE_CODE setting that is '\n        'not in the LANGUAGES setting.'\n    )\n    for tag in tests:\n        with self.subTest(tag), self.settings(LANGUAGE_CODE=tag):\n            self.assertEqual(check_language_settings_consistent(None), [\n                Error(msg, id='translation.E004'),\n            ])\n", "def test_invalid_variant_consistent_language_settings(self):\n    tests = [\n        # language + region (region not in LANGUAGES).\n        'fr-UK',\n        'es-420',\n        # language + script.\n        'zh-Hans',\n        # language + script + region.\n        'zh-Hans-CN',\n    ]\n    msg = (\n        'You have provided a value for the LANGUAGE_CODE setting that is '\n        'not in the LANGUAGES setting.'\n    )\n    for tag in tests:\n        with self.subTest(tag), self.settings(LANGUAGE_CODE=tag):\n            self.assertEqual(check_language_settings_consistent(None), [\n                Error(msg, id='translation.E004'),\n            ])\n"], "sample_12": ["def test_alter_field_type_with_choices(self):\n    \"\"\"#27825 - Changing a field's type with choices should be allowed.\"\"\"\n    changes = self.get_changes(\n        [ModelState(\"testapp\", \"Author\", [(\"id\", models.AutoField(primary_key=True)), (\"name\", models.IntegerField(choices=[(1, 'a'), (2, 'b')]))])],\n        [ModelState(\"testapp\", \"Author\", [(\"id\", models.AutoField(primary_key=True)), (\"name\", models.CharField(max_length=10, choices=[('a', 'a'), ('b', 'b')]))])]\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"name\")\n", "def test_mti_inheritance_field_removal(self):\n    Animal = ModelState('app', 'Animal', [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"name\", models.CharField(max_length=255)),\n    ])\n    Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n    changes = self.get_changes([Animal, Dog], [ModelState('app', 'Animal', [\n        (\"id\", models.AutoField(primary_key=True)),\n    ]), Dog])\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['RemoveField'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, model_name='animal', name='name')\n", "def test_deep_deconstruct_partial(self):\n    \"\"\"\n    The deep_deconstruct method should be able to handle partial objects.\n    \"\"\"\n    from functools import partial\n\n    class MyField(models.Field):\n            super().__init__()\n            self.arg = arg\n            self.kwarg = kwarg\n\n            return (\n                'tests.test_migrations.test_autodetector.MyField',\n                [self.arg],\n                {'kwarg': self.kwarg},\n            )\n\n    field1 = MyField(1)\n    field2 = MyField(1, 'keyword')\n    field3 = MyField(partial(int, '1010', base=2), 'partial')\n\n    self.assertEqual(\n        MigrationAutodetector.deep_deconstruct(field1),\n        (\n            'tests.test_migrations.test_autodetector.MyField',\n            [1],\n            {},\n        )\n    )\n    self.assertEqual(\n        MigrationAutodetector.deep_deconstruct(field2),\n        (\n            'tests.test_migrations.test_autodetector.MyField',\n            [1],\n            {'kwarg': 'keyword'},\n        )\n    )\n    self.assertEqual(\n        MigrationAutodetector.deep_deconstruct(field3),\n        (\n            'tests.test_migrations.test_autodetector.MyField',\n            [(int, ('1010',), {'base': 2})],\n            {'kwarg': 'partial'},\n        )\n    )\n", "def test_alter_unique_together_with_field_removal(self):\n    changes = self.get_changes(\n        [self.author_empty, self.book_foo_together], [self.author_empty, self.book_with_no_author]\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    self.assertOperationTypes(changes, 'otherapp', 0, [\n        \"AlterUniqueTogether\", \"AlterIndexTogether\", \"RemoveField\"\n    ])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, name=\"book\", unique_together=set())\n    self.assertOperationAttributes(changes, 'otherapp', 0, 1, name=\"book\", index_together=set())\n    self.assertOperationAttributes(changes, 'otherapp', 0, 2, model_name=\"book\", name=\"author\")\n", "def test_mti_inheritance_model_removal_with_fk(self):\n    Animal = ModelState('app', 'Animal', [\n        (\"id\", models.AutoField(primary_key=True)),\n    ])\n    Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n    Kennel = ModelState('app', 'Kennel', [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"dog\", models.ForeignKey(\"app.Dog\", models.CASCADE)),\n    ])\n    changes = self.get_changes([Animal, Dog, Kennel], [Animal])\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['RemoveField', 'DeleteModel', 'DeleteModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, model_name='kennel', name='dog')\n    self.assertOperationAttributes(changes, 'app', 0, 1, name='Dog')\n    self.assertOperationAttributes(changes, 'app', 0, 2, name='Kennel')\n"], "sample_100": ["def test_ordinal_encoder_not_fitted():\n    X = np.array([['a'], ['b']])\n    enc = OrdinalEncoder()\n    msg = (\"This OrdinalEncoder instance is not fitted yet. \"\n           \"Call 'fit' with appropriate arguments before using this method.\")\n    with pytest.raises(NotFittedError, match=msg):\n        enc.transform(X)\n", "def test_one_hot_encoder_get_feature_names_empty():\n    enc = OneHotEncoder()\n    X = np.array([])\n    with pytest.raises(ValueError, match=\"no feature names\"):\n        enc.fit(X).get_feature_names()\n", "def test_ordinal_encoder_dtype():\n    # check for correct output dtype\n    X = np.array([[1, 2], [3, 4]], dtype='int64')\n    enc = OrdinalEncoder(dtype='float64')\n    exp = np.array([[0, 0], [1, 1]], dtype='float64')\n    assert_array_equal(enc.fit_transform(X), exp)\n\n    X = np.array([['a', 'b'], ['c', 'd']])\n    enc = OrdinalEncoder(dtype='int64')\n    exp = np.array([[0, 0], [1, 1]], dtype='int64')\n    assert_array_equal(enc.fit_transform(X), exp)\n", "def test_ordinal_encoder_set_params():\n    X = np.array([[1, 2]]).T\n    enc = OrdinalEncoder()\n    # set params on not yet fitted object\n    enc.set_params(categories=[[0, 1, 2, 3]])\n    assert enc.get_params()['categories'] == [[0, 1, 2, 3]]\n    assert enc.fit_transform(X).shape == (2, 1)\n    # set params on already fitted object\n    enc.set_params(categories=[[0, 1, 2, 3, 4]])\n    assert enc.fit_transform(X).shape == (2, 1)\n", "def test_ordinal_encoder_dtype():\n    # check for correct output dtype\n    X = np.array([['a', 'b']], dtype=object).T\n\n    enc = OrdinalEncoder(dtype='int32')\n    assert enc.fit_transform(X).dtype == 'int32'\n\n    enc = OrdinalEncoder(dtype='float64')\n    assert enc.fit_transform(X).dtype == 'float64'\n\n    enc = OrdinalEncoder(dtype='int64')\n    assert enc.fit_transform(X).dtype == 'int64'\n"], "sample_81": ["def test_fixme_pattern_with_custom_notes_rgx(self) -> None:\n    code = \"\"\"a = 1\n            # CUSTOM\n            \"\"\"\n    set_config(notes_rgx=\"CUSTOM\", notes=[])()\n    with self.assertAddsMessages(\n        MessageTest(msg_id=\"fixme\", line=2, args=\"CUSTOM\", col_offset=17)\n    ):\n        self.checker.open()\n        self.checker.process_tokens(_tokenize_str(code))\n", "def test_notes_rgx(self) -> None:\n    code = \"\"\"a = 1\n            # MYNOTE message\n            \"\"\"\n    set_config(notes=[], notes_rgx=\"MYNOTE\")\n    with self.assertAddsMessages(\n        MessageTest(msg_id=\"fixme\", line=2, args=\"MYNOTE message\", col_offset=17)\n    ):\n        self.checker.process_tokens(_tokenize_str(code))\n", "def test_disable_option_with_fixme_codetag(self) -> None:\n    code = \"\"\"a = 1\n            # pylint: disable=fixme\n            # FIXME\n            \"\"\"\n    with self.assertNoMessages():\n        self.checker.process_tokens(_tokenize_str(code))\n", "def test_fixme_pattern_with_custom_regex(self) -> None:\n    code = \"\"\"a = 1\n            # MYNOTE: message\n            \"\"\"\n    set_config(notes_rgx=\"MYNOTE\")\n    with self.assertAddsMessages(\n        MessageTest(msg_id=\"fixme\", line=2, args=\"MYNOTE: message\", col_offset=17)\n    ):\n        self.checker.process_tokens(_tokenize_str(code))\n", "def test_regex_notes(self) -> None:\n    code = \"\"\"a = 1\n            # BUG1234\n            \"\"\"\n    with set_config(notes_rgx=\"BUG\\\\d+\"):\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"fixme\", line=2, args=\"BUG1234\", col_offset=17)\n        ):\n            self.checker.process_tokens(_tokenize_str(code))\n"], "sample_1": ["def test_get_type_from_list_of_lines():\n    lines = [\"! Comment\", \"READ SERR 1\", \"1.0 2.0\", \"3.0 NO\", \"NO NO\"]\n    types, ncol = _get_type_from_list_of_lines(lines)\n    assert types == [\"comment\", \"command\", \"data,2\", \"data,2\", \"new\"]\n    assert ncol == 2\n\n    lines = [\"1.0, 2.0\", \"3.0, NO\", \"NO, NO\"]\n    types, ncol = _get_type_from_list_of_lines(lines, delimiter=\",\")\n    assert types == [\"data,2\", \"data,2\", \"new\"]\n    assert ncol == 2\n\n    lines = [\"1.0 2.0\", \"3.0 4.0 5.0\"]\n    with pytest.raises(ValueError):\n        _get_type_from_list_of_lines(lines)\n", "def test_line_type():\n    assert _line_type(\"READ SERR 3\") == \"command\"\n    assert _line_type(\" \\\\n    !some gibberish\") == \"comment\"\n    assert _line_type(\"   \") == \"comment\"\n    assert _line_type(\" 21345.45\") == \"data,1\"\n    assert _line_type(\" 21345.45 1.53e-3 1e-3 .04 NO nan\") == \"data,6\"\n    assert _line_type(\" 21345.45,1.53e-3,1e-3,.04,NO,nan\", delimiter=',') == \"data,6\"\n    assert _line_type(\" 21345.45 ! a comment to disturb\") == \"data,1\"\n    assert _line_type(\"NO NO NO NO NO\") == \"new\"\n    assert _line_type(\"NO,NO,NO,NO,NO\", delimiter=',') == \"new\"\n\n    with pytest.raises(ValueError):\n        _line_type(\" some non-comment gibberish\")\n\n    with pytest.raises(ValueError):\n        _line_type(\"N O N NOON OON O\")\n", "def test_line_type():\n    assert _line_type(\"READ SERR 3\") == \"command\"\n    assert _line_type(\" \\\\n    !some gibberish\") == \"comment\"\n    assert _line_type(\"   \") == \"comment\"\n    assert _line_type(\" 21345.45\") == \"data,1\"\n    assert _line_type(\" 21345.45 1.53e-3 1e-3 .04 NO nan\") == \"data,6\"\n    assert _line_type(\" 21345.45,1.53e-3,1e-3,.04,NO,nan\", delimiter=',') == \"data,6\"\n    assert _line_type(\" 21345.45 ! a comment to disturb\") == \"data,1\"\n    assert _line_type(\"NO NO NO NO NO\") == \"new\"\n    assert _line_type(\"NO,NO,NO,NO,NO\", delimiter=',') == \"new\"\n    with pytest.raises(ValueError):\n        _line_type(\"N O N NOON OON O\")\n    with pytest.raises(ValueError):\n        _line_type(\" some non-comment gibberish\")\n", "def test_line_type():\n    assert _line_type(\"READ SERR 3\") == 'command'\n    assert _line_type(\" \\\\n    !some gibberish\") == 'comment'\n    assert _line_type(\"   \") == 'comment'\n    assert _line_type(\" 21345.45\") == 'data,1'\n    assert _line_type(\" 21345.45 1.53e-3 1e-3 .04 NO nan\") == 'data,6'\n    assert _line_type(\" 21345.45,1.53e-3,1e-3,.04,NO,nan\", delimiter=',') == 'data,6'\n    assert _line_type(\" 21345.45 ! a comment to disturb\") == 'data,1'\n    assert _line_type(\"NO NO NO NO NO\") == 'new'\n    assert _line_type(\"NO,NO,NO,NO,NO\", delimiter=',') == 'new'\n    with pytest.raises(ValueError):\n        _line_type(\"N O N NOON OON O\")\n    with pytest.raises(ValueError):\n        _line_type(\" some non-comment gibberish\")\n", "def test_read_write_error_columns(tmp_path):\n    test_file = tmp_path / \"test.qdp\"\n    t1 = Table()\n    t1.add_column(Column(name=\"a\", data=[1, 2, 3]))\n    t1.add_column(Column(name=\"a_err\", data=[0.1, 0.2, 0.3]))\n    t1.add_column(Column(name=\"b\", data=[4, 5, 6]))\n    t1.add_column(Column(name=\"b_perr\", data=[0.4, 0.5, 0.6]))\n    t1.add_column(Column(name=\"b_nerr\", data=[-0.4, -0.5, -0.6]))\n\n    t1.write(test_file, format=\"ascii.qdp\", err_specs={'serr': [1], 'terr': [2]})\n\n    t2 = Table.read(test_file, table_id=0, format=\"ascii.qdp\", names=[\"a\", \"b\"])\n\n    assert np.allclose(t2[\"a\"], t1[\"a\"])\n    assert np.allclose(t2[\"a_err\"], t1[\"a_err\"])\n    assert np.allclose(t2[\"b\"], t1[\"b\"])\n    assert np.allclose(t2[\"b_perr\"], t1[\"b_perr\"])\n    assert np.allclose(t2[\"b_nerr\"], t1[\"b_nerr\"])\n"], "sample_102": ["def test_iforest_n_features_in():\n    # Test that n_features_in_ is set.\n    X = iris.data\n    clf = IsolationForest().fit(X)\n    assert clf.n_features_in_ == X.shape[1]\n", "def test_iforest_offset_():\n    X_train = [[1, 1], [1, 2], [2, 1]]\n    clf1 = IsolationForest(contamination=0.1).fit(X_train)\n    clf2 = IsolationForest().fit(X_train)\n    assert_array_equal(clf1.offset_, -0.5)\n    assert_array_equal(clf2.offset_, -0.5)\n\n    clf3 = IsolationForest(behaviour='new', contamination='auto').fit(X_train)\n    assert_array_equal(clf3.offset_, -0.5)\n\n    clf4 = IsolationForest(behaviour='new', contamination=0.1).fit(X_train)\n    assert clf4.offset_ != -0.5\n", "def test_iforest_fit_predict_sparse_csr():\n    \"\"\"Check fit and predict with csr sparse matrix.\"\"\"\n    X_train = csr_matrix(np.array([[0, 1], [1, 2]]))\n    X_test = csr_matrix(np.array([[2, 1], [1, 1]]))\n\n    clf = IsolationForest(random_state=rng)\n    clf.fit(X_train)\n    assert clf.predict(X_test).shape == (2,)\n", "def test_iforest_feature_importances():\n    \"\"\"Test feature importances.\"\"\"\n    X_train = np.array([[0, 1], [1, 2]])\n    clf = IsolationForest(n_estimators=10, random_state=42).fit(X_train)\n\n    assert_array_almost_equal(clf.feature_importances_.sum(), 1)\n    assert_array_equal(clf.feature_importances_.shape, (X_train.shape[1],))\n", "def test_iforest_fit_with_sample_weight():\n    \"\"\"Test fit method with sample weights.\"\"\"\n    X = np.array([[0, 1], [1, 2]])\n    sample_weight = np.array([0.5, 1.5])\n    clf = IsolationForest(random_state=rng)\n    clf.fit(X, sample_weight=sample_weight)\n    assert hasattr(clf, 'estimators_')\n    assert hasattr(clf, 'max_samples_')\n"], "sample_139": ["def test_issue_16130():\n    from sympy import simplify\n    z = Symbol('z')\n    assert simplify(sign(conjugate(z))) == sign(conjugate(z))\n    assert simplify(sign(conjugate(z)).conjugate()) == sign(z)\n", "def test_issue_16458():\n    from sympy import Matrix, I\n    M = Matrix([[1 + I, 1 - I], [1 - I, 1 + I]])\n    assert Abs(M).simplify() == Matrix([[sqrt(2), sqrt(2)], [sqrt(2), sqrt(2)]])\n", "def test_issue_16221():\n    from sympy import conjugate, im, re\n    x = Symbol('x')\n    assert re(conjugate(x)) == re(x)\n    assert im(conjugate(x)) == -im(x)\n    assert conjugate(re(x)) == re(x)\n    assert conjugate(im(x)) == -im(x)\n", "def test_issue_16255():\n    from sympy import FunctionMatrix, Lambda\n    x = Symbol('x')\n    n, m = symbols('n m')\n    A = FunctionMatrix(3, 3, Lambda((n, m), x))\n    assert re(A) == FunctionMatrix(3, 3, Lambda((n, m), re(x)))\n    assert im(A) == FunctionMatrix(3, 3, Lambda((n, m), im(x)))\n", "def test_issue_17821():\n    x = Symbol('x', real=True)\n    assert abs(1/(1 + x**2)) == 1/(1 + x**2)\n    assert Abs(1/(1 + x**2)) == 1/(1 + x**2)\n"], "sample_131": ["def test_user_functions():\n    assert mcode(sin(x), user_functions={'sin': 'MySin'}) == \"MySin[x]\"\n    assert mcode(cos(x), user_functions={'cos': 'MyCos'}) == \"MyCos[x]\"\n    assert mcode(exp(x), user_functions={'exp': 'MyExp'}) == \"MyExp[x]\"\n", "def test_user_functions():\n    assert mcode(sin(x), user_functions={'sin': 'mySin'}) == \"mySin[x]\"\n    assert mcode(cos(x), user_functions={'cos': 'myCos'}) == \"myCos[x]\"\n    assert mcode(exp(x), user_functions={'exp': 'myExp'}) == \"myExp[x]\"\n", "def test_user_functions():\n    assert mcode(sin(x), user_functions={'sin': 'MySin'}) == \"MySin[x]\"\n    assert mcode(cos(x), user_functions={'cos': 'MyCos'}) == \"MyCos[x]\"\n    assert mcode(exp(x), user_functions={'exp': 'MyExp'}) == \"MyExp[x]\"\n", "def test_user_functions():\n    assert mcode(sin(x), user_functions={'sin': 'MySin'}) == \"MySin[x]\"\n    assert mcode(cos(x), user_functions={'cos': 'MyCos'}) == \"MyCos[x]\"\n    assert mcode(exp(x), user_functions={'exp': 'MyExp'}) == \"MyExp[x]\"\n    assert mcode(f(x), user_functions={f: 'MyF'}) == \"MyF[x]\"\n", "def test_user_functions():\n    assert mcode(sin(x), user_functions={'sin': 'MySin'}) == \"MySin[x]\"\n    assert mcode(cos(x), user_functions={'cos': 'MyCos'}) == \"MyCos[x]\"\n    assert mcode(sin(x) + cos(x), user_functions={'sin': 'MySin', 'cos': 'MyCos'}) == \"MySin[x] + MyCos[x]\"\n"], "sample_29": ["    def test_clone_with_annotations(self):\n        qs = Company.objects.annotate(employees=Count('employees'))\n        qs_cloned = qs._clone()\n        self.assertEqual(qs.query.annotations, qs_cloned.query.annotations)\n", "    def test_raw_sql(self):\n        company = Company.objects.create(name='RawSQLTest')\n        qs = Company.objects.filter(pk=RawSQL(\"SELECT id FROM %s WHERE name = 'RawSQLTest'\" % Company._meta.db_table, ()))\n        self.assertEqual(list(qs), [company])\n", "    def test_resolve_expression(self):\n        company = Company.objects.annotate(\n            count=Count('employee_set'),\n            exists=Exists(Employee.objects.filter(company=OuterRef('pk'))),\n        ).get()\n        self.assertEqual(company.count, 0)\n        self.assertFalse(company.exists)\n", "    def test_resolve_output_field(self):\n        class MyExpression(Expression):\n            pass\n\n        msg = 'Cannot resolve expression type, unknown output_field'\n        with self.assertRaisesMessage(FieldError, msg):\n            MyExpression().output_field\n", "    def test_repr(self):\n        raw_sql = RawSQL('table.col', [])\n        self.assertEqual(repr(raw_sql), \"RawSQL(table.col, [])\")\n"], "sample_32": ["    def setUpTestData(cls):\n        cls.objs = [\n            NullableJSONModel.objects.create(value={'a': 1, 'b': 2.3}),\n            NullableJSONModel.objects.create(value={'a': 4, 'b': 5.6}),\n            NullableJSONModel.objects.create(value={'a': 7, 'b': 8.9}),\n        ]\n", "    def test_subquery_key_transform(self):\n        obj = NullableJSONModel.objects.create(value={'d': ['e', {'f': 'g'}]})\n        subquery = NullableJSONModel.objects.filter(value__d__1__f='g').values('value__d__1')\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__d__1__in=subquery),\n            [obj],\n        )\n", "def test_key_transform_on_annotation_with_params(self):\n    self.objs[4].value['new_key'] = 'new_value'\n    self.objs[4].save()\n    qs = NullableJSONModel.objects.annotate(\n        new_key=KeyTransform('new_key', 'value'),\n    ).filter(new_key='new_value')\n    self.assertSequenceEqual(qs, [self.objs[4]])\n", "    def test_text_lookup_key_transform_mixin(self):\n        mixin = KeyTransformTextLookupMixin(KeyTransform('test', 'value'))\n        self.assertIsInstance(mixin, KeyTransformTextLookupMixin)\n", "def test_key_transform_with_none(self):\n    obj = NullableJSONModel.objects.create(value={'a': None})\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__a=None),\n        [obj],\n    )\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__a__isnull=True),\n        self.objs[:3] + self.objs[5:] + [obj],\n    )\n    self.assertSequenceEqual(\n        NullableJSONModel.objects.filter(value__a__isnull=False),\n        [self.objs[3], self.objs[4]],\n    )\n"], "sample_62": ["    def test_thread_safety(self):\n        cache_file = \"/path/to/cache/default/1234567890abcdef0123456789abcdef\"\n\n            with open(cache_file, \"w\") as f:\n                f.write(\"Hello, World!\")\n\n        with mock.patch(\"django.core.cache.backends.filebased.os.path.exists\", return_value=False):\n            with mock.patch(\"django.core.cache.backends.filebased.open\", side_effect=thread_func):\n                t1 = threading.Thread(target=cache.set, args=(\"key\", \"value\"))\n                t2 = threading.Thread(target=cache.set, args=(\"key\", \"value\"))\n\n                t1.start()\n                t2.start()\n\n                t1.join()\n                t2.join()\n\n                self.assertEqual(cache.get(\"key\"), \"value\")\n", "    def test_permission_error(self):\n        with self.assertRaises(PermissionError):\n            cache.set(\"key\", \"value\")\n", "    def test_cull(self):\n        cache = caches[\"default\"]\n        for i in range(150):\n            cache.set(f\"key{i}\", f\"value{i}\")\n        self.assertLess(len(cache._list_cache_files()), 120)\n", "    def test_path_traversal(self):\n        cache = caches[\"default\"]\n        key = \"../../../etc/passwd\"\n        cache.set(key, \"value\")\n        self.assertIsNone(cache.get(key))\n", "    def setUp(self):\n        self.dirname = tempfile.mkdtemp()\n"], "sample_33": ["def test_connect_with_weak_ref_false(self):\n    receiver = Callable()\n    a_signal.connect(receiver, weak=False)\n    del receiver\n    garbage_collect()\n    self.assertEqual(len(a_signal.receivers), 1)\n    # Disconnect to avoid \"RuntimeError: dictionary changed size during iteration\"\n    # in assertTestIsClean.\n    a_signal.disconnect(a_signal.receivers[0][1])\n    self.assertTestIsClean(a_signal)\n", "def test_sender_receivers_cache(self):\n    receiver_1 = Callable()\n    receiver_2 = Callable()\n\n    # Connect receivers to signal with caching\n    d_signal.connect(receiver_1)\n    d_signal.connect(receiver_2)\n\n    # Send signal and populate cache\n    d_signal.send(sender=self, val='test')\n\n    # Check that cache contains correct receivers\n    self.assertIn(self, d_signal.sender_receivers_cache)\n    self.assertEqual(len(d_signal.sender_receivers_cache[self]), 2)\n\n    # Disconnect one receiver and check cache is cleared\n    d_signal.disconnect(receiver_1)\n    self.assertNotIn(self, d_signal.sender_receivers_cache)\n\n    # Reconnect receiver and check cache is repopulated\n    d_signal.connect(receiver_1)\n    d_signal.send(sender=self, val='test')\n    self.assertIn(self, d_signal.sender_receivers_cache)\n    self.assertEqual(len(d_signal.sender_receivers_cache[self]), 2)\n", "def test_weakref_finalization(self):\n    receiver = Callable()\n    ref = weakref.ref(receiver)\n    a_signal.connect(receiver)\n    del receiver\n    garbage_collect()\n    self.assertIsNone(ref())\n    self.assertTestIsClean(a_signal)\n", "def test_receiver_weakref(self):\n    class Receiver:\n            self.called = False\n\n            self.called = True\n\n    receiver_instance = Receiver()\n    weak_receiver_instance = weakref.ref(receiver_instance)\n\n    a_signal.connect(receiver_instance.receiver)\n    del receiver_instance\n    garbage_collect()\n\n    self.assertIsNone(weak_receiver_instance())\n    result = a_signal.send(sender=self)\n    self.assertEqual(result, [])\n    self.assertTestIsClean(a_signal)\n", "def test_weakref_finalization(self):\n    receiver = Callable()\n    ref = weakref.ref(receiver)\n    a_signal.connect(receiver.a, sender=self)\n    del receiver\n    garbage_collect()\n    self.assertIsNone(ref())\n    result = a_signal.send(sender=self, val=\"test\")\n    self.assertEqual(result, [])\n    self.assertTestIsClean(a_signal)\n"], "sample_93": ["def test_tmp_path_factory_handles_lock_timeout(\n    tmp_path_factory: TempPathFactory, monkeypatch: MonkeyPatch", "def test_temp_path_factory_with_cleanup_lock(tmp_path_factory: TempPathFactory, tmp_path: Path) -> None:\n    # Create a cleanup lock in the temporary directory\n    lockfile = create_cleanup_lock(tmp_path)\n\n    # Try to remove the temporary directory with the lock file present\n    assert not pathlib.ensure_deletable(\n        tmp_path, consider_lock_dead_if_created_before=lockfile.stat().st_mtime - 1\n    )\n\n    # Remove the lock file and try again\n    lockfile.unlink()\n    assert pathlib.ensure_deletable(tmp_path)\n", "def test_tmp_path_factory_handles_absolute_given_basetemp(\n    tmp_path_factory: TempPathFactory, monkeypatch: MonkeyPatch", "def test_tmp_path_factory_uses_given_basetemp(tmp_path_factory: TempPathFactory, tmp_path: Path) -> None:\n    given_basetemp = tmp_path / \"given_basetemp\"\n    tmp_path_factory = TempPathFactory(given_basetemp, trace=None)\n    assert tmp_path_factory.getbasetemp() == given_basetemp\n", "def test_tmp_path_factory_handles_lock_timeout(\n    tmp_path_factory: TempPathFactory, monkeypatch: MonkeyPatch"], "sample_42": ["def test_serialize_type_with_module(self):\n    self.assertSerializedResultEqual(\n        MigrationWriter.serialize(type),\n        (\"type\", set()),\n    )\n    self.assertSerializedResultEqual(\n        MigrationWriter.serialize(models.Field),\n        (\"models.Field\", {'from django.db import models'}),\n    )\n    self.assertSerializedResultEqual(\n        MigrationWriter.serialize(models.Field.__class__),\n        (\"type\", set()),\n    )\n", "def test_serialize_operation(self):\n    operation = migrations.RunPython(lambda apps, schema_editor: None)\n    writer = MigrationWriter(operation)\n    output = writer.as_string()\n    self.assertIn(\"migrations.RunPython\", output)\n    self.assertSerializedResultEqual(\n        operation,\n        (\n            \"migrations.RunPython(code=migrations.RunPython.noop)\",\n            {'from django.db import migrations'},\n        ),\n    )\n", "def test_serialize_operation_with_nested_args(self):\n    operation = custom_migration_operations.operations.TestOperation(\n        arg1=custom_migration_operations.operations.KwargsOperation(kwarg1=1, kwarg2=2),\n        arg2=[custom_migration_operations.operations.ArgsOperation(3, 4)]\n    )\n    writer = MigrationWriter(migration=migrations.Migration(\"0001_initial\", \"testapp\"))\n    output = writer.serialize(operation)[0]\n    self.assertEqual(\n        output,\n        \"custom_migration_operations.operations.TestOperation(\\n\"\n        \"    arg1=custom_migration_operations.operations.KwargsOperation(\\n\"\n        \"        kwarg1=1,\\n\"\n        \"        kwarg2=2,\\n\"\n        \"    ),\\n\"\n        \"    arg2=[\\n\"\n        \"        custom_migration_operations.operations.ArgsOperation(\\n\"\n        \"            arg1=3,\\n\"\n        \"            arg2=4,\\n\"\n        \"        ),\\n\"\n        \"    ],\\n\"\n        \")\"\n    )\n", "def test_serialize_operation_with_nested_list(self):\n    operation = custom_migration_operations.operations.TestOperation([1, 2, [3, 4]])\n    buff, imports = OperationWriter(operation, indentation=0).serialize()\n    self.assertEqual(imports, {'import custom_migration_operations.operations'})\n    self.assertEqual(\n        buff,\n        'custom_migration_operations.operations.TestOperation(\\n'\n        '    arg=[\\n'\n        '        1,\\n'\n        '        2,\\n'\n        '        [\\n'\n        '            3,\\n'\n        '            4,\\n'\n        '        ],\\n'\n        '    ],\\n'\n        '),'\n    )\n", "def test_serialize_deconstructible(self):\n    value = DeconstructibleInstances()\n    result = self.serialize_round_trip(value)\n    self.assertIsInstance(result, DeconstructibleInstances)\n\n    string, imports = MigrationWriter.serialize(value)\n    self.assertEqual(string, \"migrations.test_writer.DeconstructibleInstances()\")\n    self.assertEqual(imports, {'import migrations.test_writer'})\n"], "sample_142": ["def test_roundrobin():\n    gen = roundrobin('ABC', 'D', 'EF')\n    assert next(gen) == 'A'\n    assert next(gen) == 'D'\n    assert next(gen) == 'E'\n    assert next(gen) == 'B'\n    assert next(gen) == 'F'\n    assert next(gen) == 'C'\n", "def test_roundrobin():\n    assert list(roundrobin('ABC', 'D', 'EF')) == ['A', 'D', 'E', 'B', 'F', 'C']\n    assert list(roundrobin([1, 2], [3, 4, 5])) == [1, 3, 2, 4, 5]\n    assert list(roundrobin([1, 2, 3], [4, 5], [6, 7, 8, 9])) == [\n        1, 4, 6, 2, 5, 7, 3, 8, 9]\n", "def test_has_variety():\n    assert has_variety((1, 2, 1)) is True\n    assert has_variety((1, 1, 1)) is False\n    assert has_variety([]) is False\n    assert has_variety([1]) is False\n    assert has_variety([1, 1]) is False\n    assert has_variety([1, 2]) is True\n", "def test_roundrobin():\n    assert list(roundrobin('ABC', 'D', 'EF')) == ['A', 'D', 'E', 'B', 'F', 'C']\n    assert list(roundrobin([1, 2], [3, 4, 5])) == [1, 3, 2, 4, 5]\n    assert list(roundrobin([1, 2, 3], [], [4, 5])) == [1, 4, 2, 5, 3]\n", "def test_roundrobin():\n    assert list(roundrobin('ABC', 'D', 'EF')) == ['A', 'D', 'E', 'B', 'F', 'C']\n    assert list(roundrobin([1, 2], [3, 4, 5], [6])) == [1, 3, 6, 2, 4, 5]\n    assert list(roundrobin([1], [2], [3])) == [1, 2, 3]\n    assert list(roundrobin([1, 2], [], [3])) == [1, 3, 2]\n    assert list(roundrobin([], [], [])) == []\n"], "sample_120": ["def test_matrixelement_as_coeff_Mul():\n    A = MatrixSymbol('A', 2, 2)\n    assert A[0, 0].as_coeff_Mul()[0] == 1\n    assert A[0, 0].as_coeff_Mul()[1] == A[0, 0]\n", "def test_MatrixElement_as_real_imag():\n    A = MatrixSymbol('A', 2, 2)\n    x = symbols('x', real=True)\n    M = Matrix([[x, 1], [2, 3]])\n    assert (A[0, 0]).as_real_imag() == (re(A[0, 0]), im(A[0, 0]))\n    assert M[0, 0].as_real_imag() == (M[0, 0], 0)\n    assert (M[0, 0] + I).as_real_imag() == (M[0, 0], 1)\n", "def test_MatrixElement_as_real_imag():\n    n = symbols('n', integer=True)\n    A = MatrixSymbol('A', n, n)\n    i, j = symbols('i j', integer=True)\n    expr = A[i, j]\n    real, imag = expr.as_real_imag()\n    assert real == (expr + expr.conjugate())/2\n    assert imag == (expr - expr.conjugate())/(2*S.I)\n", "def test_matrixelement_subs():\n    A = MatrixSymbol('A', 2, 2)\n    B = ImmutableMatrix([[1, 2], [3, 4]])\n    assert A[0, 0].subs(A, B) == 1\n    assert A[1, 1].subs(A, B) == 4\n    C = MatrixSymbol('C', 2, 2)\n    assert A[0, 0].subs(A, C) == C[0, 0]\n    assert A[1, 1].subs(A, C) == C[1, 1]\n", "def test_matrixelement_simplify():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    i, j = symbols('i j')\n    assert simplify(A[i, j] + B[j, i]) == A[i, j] + B[j, i]\n    assert simplify(A[i, j] + A[i, j]) == 2*A[i, j]\n    assert simplify(A[i, j] - A[i, j]) == 0\n"], "sample_14": ["def test_serialize_functools_partial_with_keyword_only_args(self):\n    value = functools.partial(datetime.timedelta, days=1, seconds=2)\n    result = self.serialize_round_trip(value)\n    self.assertEqual(result.func, value.func)\n    self.assertEqual(result.args, value.args)\n    self.assertEqual(result.keywords, value.keywords)\n", "def test_serialize_datetime_timezone(self):\n    dt = datetime.datetime(2012, 1, 1, tzinfo=get_default_timezone())\n    self.assertSerializedEqual(dt)\n    dt = datetime.datetime(2012, 1, 1, tzinfo=get_fixed_timezone(-300))\n    self.assertSerializedEqual(dt)\n    dt = datetime.datetime(2012, 1, 1, tzinfo=utc)\n    self.assertSerializedEqual(dt)\n", "def test_serialize_model_manager_with_queryset(self):\n    manager = FoodManager.from_queryset(FoodQuerySet)()\n    string, imports = MigrationWriter.serialize(manager)\n    self.assertEqual(\n        string,\n        \"migrations.models.FoodManager.from_queryset(migrations.models.FoodQuerySet)()\"\n    )\n    self.assertEqual(imports, {'import migrations.models'})\n", "def test_serialize_regex_pattern(self):\n    pattern = re.compile(r'^foo$').pattern\n    self.assertSerializedEqual(pattern)\n    string, imports = MigrationWriter.serialize(pattern)\n    self.assertEqual(string, \"'^foo$'\")\n    self.assertEqual(imports, set())\n", "def test_serialize_lazy_strings(self):\n    lazy_string = _('Hello, world!')\n    self.assertSerializedEqual(lazy_string)\n    self.assertSerializedResultEqual(\n        lazy_string,\n        (\"'Hello, world!'\", set())\n    )\n"], "sample_157": ["def test_tensor_product_simp_Mul_with_composite_TensorProduct():\n    # Test that tensor_product_simp_Mul can handle Muls with composite TensorProducts\n    A, B, C, D = symbols('A B C D', commutative=False)\n    t1 = TensorProduct(A, B)\n    t2 = TensorProduct(C, D)\n    e = t1 * (t2 * 2)\n    assert tensor_product_simp(e) == 2 * TensorProduct(A*C, B*D)\n", "def test_tensor_product_simp_with_commutator():\n    assert tensor_product_simp(TP(A, B)*TP(Comm(B, C), D)) == \\\n        TP(A*Comm(B, C), B*D)\n    assert tensor_product_simp(TP(Comm(A, B), C)*TP(D, Comm(B, C))) == \\\n        TP(Comm(A, B)*D, C*Comm(B, C))\n", "def test_tensor_product_simp_density():\n    A, B, C, D = symbols('A B C D', commutative=False)\n    t1 = TensorProduct(A, B)\n    t2 = TensorProduct(C, D)\n    d = Density([t1, 0.5], [t2, 0.5])\n    assert tensor_product_simp(d) == Density([TensorProduct(A, B), 0.5], [TensorProduct(C, D), 0.5])\n", "def test_tensor_product_simp_Mul():\n    assert tensor_product_simp(TP(A, B)*TP(C, D)*TP(E, F)) == TP(A*C*E, B*D*F)\n    assert tensor_product_simp(x*TP(A, B)*TP(C, D)*TP(E, F)) == x*TP(A*C*E, B*D*F)\n    assert tensor_product_simp(TP(A, B)*x*TP(C, D)*TP(E, F)) == x*TP(A*C*E, B*D*F)\n    assert tensor_product_simp(TP(A, B)*TP(C, D)*x*TP(E, F)) == x*TP(A*C*E, B*D*F)\n    assert tensor_product_simp(TP(A, B)*TP(C, D)*TP(E, F)**x) == TP(A*C*E**x, B*D*F**x)\n", "def test_tensor_product_simp_Mul():\n    # Test simplification of Muls with TensorProducts\n    assert tensor_product_simp(TP(A, B)*TP(C, D)) == TP(A*C, B*D)\n    assert tensor_product_simp(x*TP(A, B)*TP(C, D)) == x*TP(A*C, B*D)\n    assert tensor_product_simp(TP(A, B)*TP(C, D)*TP(E, F)) == TP(A*C*E, B*D*F)\n"], "sample_110": ["def test_affinity_propagation_verbose_output():\n    # Test verbose output for AffinityPropagation\n    import io\n    import sys\n\n    X = np.array([[1, 2], [1, 4], [1, 0],\n                  [4, 2], [4, 4], [4, 0]])\n\n    af = AffinityPropagation(verbose=True)\n\n    capturedOutput = io.StringIO()                  # Create StringIO object\n    sys.stdout = capturedOutput                     # Redirect stdout\n    af.fit(X)                                       # Call the function\n    sys.stdout = sys.__stdout__                     # Reset stdout\n\n    assert \"Converged after\" in capturedOutput.getvalue()\n", "def test_affinity_propagation_fit_predict():\n    # Test AffinityPropagation.fit_predict\n    af = AffinityPropagation(affinity=\"euclidean\")\n    labels = af.fit_predict(X)\n    assert_array_equal(labels, af.labels_)\n", "def test_affinity_propagation_labels_assignment():\n    # Test labels assignment when all training samples have equal similarities\n    # and equal preferences.\n    X = np.array([[1, 2], [1, 4], [1, 0]])\n    S = -euclidean_distances(X, squared=True)\n    preference = np.median(S)\n\n    # When preference is smaller than the similarities, a single cluster center\n    # and label '0' for every sample should be returned.\n    cluster_centers_indices, labels = affinity_propagation(\n        S, preference=preference - 1)\n\n    assert len(cluster_centers_indices) == 1\n    assert_array_equal(labels, np.zeros(X.shape[0]))\n\n    # When preference is not smaller than the similarities, every training\n    # sample becomes its own cluster center and is assigned a unique label.\n    cluster_centers_indices, labels = affinity_propagation(\n        S, preference=preference + 1)\n\n    assert len(cluster_centers_indices) == X.shape[0]\n    assert_array_equal(labels, np.arange(X.shape[0]))\n", "def test_affinity_propagation_verbose_output():\n    # Test verbose output in AffinityPropagation\n    af = AffinityPropagation(verbose=True)\n    with pytest.raises(ValueError):\n        af.fit(np.array([[1, 2], [3, 4]]))\n    af.fit(np.array([[1, 2], [1, 4], [1, 0],\n                     [4, 2], [4, 4], [4, 0]]))\n", "def test_affinity_propagation_sparse_input():\n    # Test that AffinityPropagation works with sparse input\n    from scipy.sparse import csr_matrix\n    X_sparse = csr_matrix(X)\n    af = AffinityPropagation()\n    labels_sparse = af.fit_predict(X_sparse)\n    af = AffinityPropagation()\n    labels_dense = af.fit_predict(X)\n    assert_array_equal(labels_sparse, labels_dense)\n"], "sample_136": ["def test_block_collapse_trace():\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', m, m)\n    C = MatrixSymbol('C', l, l)\n    X = BlockDiagMatrix(A, B, C)\n\n    assert block_collapse(Trace(X)) == Trace(A) + Trace(B) + Trace(C)\n", "def test_block_collapse_determinant():\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, m)\n    C = MatrixSymbol('C', m, n)\n    D = MatrixSymbol('D', m, m)\n    X = BlockMatrix([[A, B], [C, D]])\n\n    assert block_collapse(det(X)) == det(A)*det(D - C*A.I*B)\n", "def test_BlockMatrix_structurally_equal():\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, n)\n    C = MatrixSymbol('C', m, m)\n    D = MatrixSymbol('D', m, m)\n\n    X = BlockMatrix([[A, B], [C, D]])\n    Y = BlockMatrix([[A, B], [C, D]])\n\n    assert X.structurally_equal(Y)\n\n    Z = BlockMatrix([[A, B], [C, A]])\n\n    assert not X.structurally_equal(Z)\n\n    W = BlockMatrix([[A, B], [C]])\n\n    assert not X.structurally_equal(W)\n", "def test_block_collapse_inverse_non_block_diagonal():\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, m)\n    C = MatrixSymbol('C', m, n)\n    D = MatrixSymbol('D', m, m)\n    X = BlockMatrix([[A, B], [C, D]])\n\n    assert block_collapse(Inverse(X)).equals(\n        BlockMatrix([[(A - B*D.I*C).I, -A.I*B*(D - C*A.I*B).I],\n                     [-(D - C*A.I*B).I*C*A.I, (D - C*A.I*B).I]]))\n", "def test_block_collapse_structurally_symmetric():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', n, n)\n    C = MatrixSymbol('C', m, m)\n    X = BlockMatrix([[B, A], [A.T, C]])\n    assert block_collapse(X).is_structurally_symmetric\n    assert X.is_structurally_symmetric\n"], "sample_80": ["def test_wrap_indent():\n    text = \"This is a very long string that needs to be wrapped and indented.\"\n    expected = \"\"\"\\\n    This is a very long string that needs to be\n    wrapped and indented.\"\"\"\n    actual = formatting.wrap_indent(text, start=\"    \")\n    assert actual == expected\n\n    text = \"Short string\"\n    expected = \"    Short string\"\n    actual = formatting.wrap_indent(text, start=\"    \")\n    assert actual == expected\n", "def test_short_data_repr_for_empty_arrays():\n    array = xr.DataArray(np.array([]))\n    actual = formatting.short_data_repr(array)\n    expected = \"[]\"\n    assert actual == expected\n\n    array = xr.DataArray(np.array([[]]))\n    actual = formatting.short_data_repr(array)\n    expected = \"[[]]\"\n    assert actual == expected\n\n    array = xr.DataArray(np.array([[], []]))\n    actual = formatting.short_data_repr(array)\n    expected = \"[[]\\n []]\"\n    assert actual == expected\n", "def test_inline_variable_array_repr_sparse_array():\n    sparse_array = xr.DataArray(\n        np.array([1, 0, 2, 0, 3, 0]), dims=\"x\", attrs={\"sparse\": True}\n    ).to_sparse(fill_value=0)\n\n    actual = formatting.inline_variable_array_repr(sparse_array.variable, max_width=80)\n    expected = \"<SparseArray: shape=(6,), dtype=int64, fill_value=0, nnz=3>\"\n    assert actual == expected\n", "def test_short_data_repr_for_sparse_array():\n    import sparse\n\n    array = sparse.COO(np.random.randn(1000))\n    actual = formatting.short_data_repr(array)\n    assert len(actual) < 100\n", "def test_short_data_repr_for_non_numpy_array():\n    data = [1, 2, 3, 4, 5]\n    actual = formatting.short_data_repr(data)\n    expected = \"[1, 2, 3, 4, 5]\"\n    assert actual == expected\n\n    data = \"hello\"\n    actual = formatting.short_data_repr(data)\n    expected = \"'hello'\"\n    assert actual == expected\n\n    data = 123\n    actual = formatting.short_data_repr(data)\n    expected = \"123\"\n    assert actual == expected\n"], "sample_99": ["def test_kneighbors_sparse_output():\n    # Test output of kneighbors when input is sparse\n    X = csr_matrix(np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]))\n    nn = neighbors.NearestNeighbors(n_neighbors=2, algorithm='brute').fit(X)\n    dist, ind = nn.kneighbors(X)\n    assert issparse(dist)\n    assert issparse(ind)\n", "def test_radius_neighbors_regressor_multioutput_zero_distance():\n    # Test radius-based regressor, when distance to a sample is zero.\n    X = np.array([[1.0, 1.0], [1.0, 1.0], [2.0, 2.0], [2.5, 2.5]])\n    y = np.array([[1.0, 1.5], [1.0, 1.5], [2.0, 0.0], [0.0, 0.0]])\n    radius = 0.2\n    z = np.array([[1.0, 1.0], [2.0, 2.0]])\n\n    rnn_correct_labels = np.array([[1.0, 1.5], [2.0, 0.0]])\n\n    for algorithm in ALGORITHMS:\n        for weights in ['uniform', 'distance']:\n            rnn = neighbors.RadiusNeighborsRegressor(radius=radius,\n                                                     weights=weights,\n                                                     algorithm=algorithm)\n            rnn.fit(X, y)\n            assert_array_almost_equal(rnn_correct_labels, rnn.predict(z))\n", "def test_radius_neighbors_regressor_multioutput_with_distance_weight():\n    # Test radius neighbors in multi-output regression (distance weight)\n\n    rng = check_random_state(0)\n    n_features = 5\n    n_samples = 40\n    n_output = 4\n\n    X = rng.rand(n_samples, n_features)\n    y = rng.rand(n_samples, n_output)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n    for algorithm in ALGORITHMS:\n\n        rnn = neighbors.RadiusNeighborsRegressor(weights='distance',\n                                                 algorithm=algorithm)\n        rnn.fit(X_train, y_train)\n\n        neigh_idx = rnn.radius_neighbors(X_test, return_distance=False)\n        weights = _get_weights(\n            pairwise_distances(X_test, X_train), 'distance')\n        y_pred_idx = np.array([np.average(y_train[idx], axis=0,\n                                          weights=weights[i, idx])\n                               for i, idx in enumerate(neigh_idx)])\n\n        y_pred = rnn.predict(X_test)\n\n        assert_equal(y_pred_idx.shape, y_test.shape)\n        assert_equal(y_pred.shape, y_test.shape)\n        assert_array_almost_equal(y_pred, y_pred_idx)\n", "def test_radius_neighbors_with_duplicates():\n    # Test radius neighbors with duplicates in the input data.\n    X = np.array([[1.0, 1.0], [1.0, 1.0], [2.0, 2.0]])\n    nbrs = neighbors.NearestNeighbors(radius=1.5).fit(X)\n\n    # Test that the output is correct when there are duplicates.\n    dist, ind = nbrs.radius_neighbors(X)\n    check_object_arrays(dist, [[0.0, 0.0], [0.0, 0.0], []])\n    check_object_arrays(ind, [[0, 1], [0, 1], []])\n\n    # Test that the output is correct when there are duplicates and we ask for\n    # the neighbors of a different set of points.\n    X2 = np.array([[1.0, 1.0], [2.0, 2.0]])\n    dist, ind = nbrs.radius_neighbors(X2)\n    check_object_arrays(dist, [[0.0, 0.0], []])\n    check_object_arrays(ind, [[0, 1], []])\n", "def test_neighbors_n_features_in_X():\n    # Test that the number of features in X is checked.\n    X = np.random.rand(10, 5)\n    y = np.random.randint(0, 2, 10)\n\n    for Est in (neighbors.KNeighborsClassifier,\n                neighbors.RadiusNeighborsClassifier,\n                neighbors.KNeighborsRegressor,\n                neighbors.RadiusNeighborsRegressor):\n        est = Est()\n        est.fit(X, y)\n\n        # One less feature in X\n        X_less_features = X[:, :-1]\n        msg = (\"X has {} features, but {} is expecting {} features as input.\"\n               .format(X_less_features.shape[1], Est.__name__, X.shape[1]))\n        assert_raises_regex(ValueError, msg, est.predict, X_less_features)\n\n        # One more feature in X\n        X_more_features = np.hstack((X, X[:, -1].reshape(-1, 1)))\n        msg = (\"X has {} features, but {} is expecting {} features as input.\"\n               .format(X_more_features.shape[1], Est.__name__, X.shape[1]))\n        assert_raises_regex(ValueError, msg, est.predict, X_more_features)\n"], "sample_6": ["    def test_unicode_validator_regex(self):\n        v = UnicodeUsernameValidator()\n        self.assertRegex('Ren\u00e9', v.regex)\n        self.assertRegex('\u1d2e\u1d35\u1d33\u1d2e\u1d35\u1d3f\u1d30', v.regex)\n        self.assertRegex('\u0623\u062d\u0645\u062f', v.regex)\n        self.assertNotRegex(\"o'connell\", v.regex)\n        self.assertNotRegex(\"zerowidth\\u200Bspace\", v.regex)\n", "    def test_ascii_validator_regex(self):\n        v = validators.ASCIIUsernameValidator()\n        self.assertEqual(v.regex, r'^[\\w.@+-]+\\Z')\n        self.assertEqual(v.flags, re.ASCII)\n", "    def test_regex(self):\n        v = ASCIIUsernameValidator()\n        self.assertRegex('hello.world', v.regex)\n        self.assertRegex('hello-world', v.regex)\n        self.assertRegex('hello_world', v.regex)\n        self.assertRegex('hello@world', v.regex)\n        self.assertRegex('helloworld123', v.regex)\n        self.assertNotRegex('hello world', v.regex)\n        self.assertNotRegex('hello!world', v.regex)\n", "    def test_custom_validator(self):\n        class CustomValidator(validators.RegexValidator):\n            regex = r'^[a-z]+$'\n            message = 'Only lowercase letters are allowed.'\n            flags = re.ASCII\n\n        v = CustomValidator()\n        valid_usernames = ['hello', 'world']\n        invalid_usernames = ['Hello', 'WORLD', '123', '!@#']\n\n        for valid in valid_usernames:\n            with self.subTest(valid=valid):\n                v(valid)\n\n        for invalid in invalid_usernames:\n            with self.subTest(invalid=invalid):\n                with self.assertRaises(ValidationError):\n                    v(invalid)\n", "    def test_regex_validator_flags(self):\n        # Test that flags are passed to re.compile.\n        v = validators.RegexValidator(regex=r'^.*$', flags=re.IGNORECASE)\n        self.assertIsNone(v('lowercase'))\n        self.assertIsNone(v('CamelCase'))\n        self.assertIsNone(v('UPPERCASE'))\n\n        v = validators.RegexValidator(regex=r'^[a-z]*$')\n        with self.assertRaises(ValidationError):\n            v('CamelCase')\n"], "sample_25": ["def test_alter_model_table_with_database_table(self):\n    \"\"\"\n    Tests when model changes and db_table also changes, autodetector must create \n    two operations.\n    \"\"\"\n    changes = self.get_changes(\n        [self.author_with_db_table_options],\n        [self.author_renamed_with_new_db_table_options],\n        MigrationQuestioner({\"ask_rename_model\": True}),\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"RenameModel\", \"AlterModelTable\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, old_name=\"Author\", new_name=\"NewAuthor\")\n    self.assertOperationAttributes(changes, \"testapp\", 0, 1, name=\"newauthor\", table=\"author_three\")\n", "def test_create_model_with_base_class(self):\n    \"\"\"\n    Test creation of new model with base classes already defined.\n    \"\"\"\n    animal = ModelState('testapp', 'Animal', [\n        ('id', models.AutoField(primary_key=True)),\n    ])\n    dog = ModelState('testapp', 'Dog', [], bases=('testapp.Animal',))\n    changes = self.get_changes([animal], [animal, dog])\n    # Right number of migrations?\n    self.assertEqual(len(changes['testapp']), 1)\n    # Right number of actions?\n    migration = changes['testapp'][0]\n    self.assertEqual(len(migration.operations), 1)\n    # Right actions order?\n    self.assertOperationTypes(changes, 'testapp', 0, ['CreateModel'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='Dog')\n", "def test_deep_deconstruct_partial_with_args(self):\n        return '{}/{}'.format(instance, filename)\n\n    before = [ModelState(\"testapp\", \"Author\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"file\", models.FileField(max_length=200, upload_to=content_file_name)),\n    ])]\n    after = [ModelState(\"testapp\", \"Author\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"file\", models.FileField(max_length=200, upload_to=functools.partial(content_file_name))),\n    ])]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'testapp', 0)\n", "def test_deep_deconstruct_with_functools_partial(self):\n        return '{}/{}'.format(instance, filename)\n\n    partial_func = functools.partial(content_file_name, 'file')\n    deconstructed = MigrationAutodetector.deep_deconstruct(partial_func)\n    self.assertEqual(deconstructed, (\n        'functools.partial',\n        (content_file_name, 'file'),\n        {}\n    ))\n", "def test_add_model_with_field_removed_from_base_model_in_different_app(self):\n    \"\"\"\n    Removing a base field takes place before adding a new inherited model\n    that has a field with the same name, even if they are in different apps.\n    \"\"\"\n    before = [\n        ModelState('app1', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200)),\n        ]),\n    ]\n    after = [\n        ModelState('app1', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n        ]),\n        ModelState('app2', 'book', [\n            ('title', models.CharField(max_length=200)),\n        ], bases=('app1.readable',)),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'app1', 1)\n    self.assertOperationTypes(changes, 'app1', 0, ['RemoveField'])\n    self.assertOperationAttributes(changes, 'app1', 0, 0, name='title', model_name='readable')\n    self.assertNumberMigrations(changes, 'app2', 1)\n    self.assertOperationTypes(changes, 'app2', 0, ['CreateModel'])\n    self.assertOperationAttributes(changes, 'app2', 0, 0, name='book')\n"], "sample_63": ["    def test_render_unusable_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = \"unusable_password\"\n        html = widget.render(\"name\", value, attrs={})\n        self.assertIn(_(\"No password set.\"), html)\n", "    def test_render_with_unusable_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = UNUSABLE_PASSWORD_PREFIX + \"unusable\"\n        html = widget.render(name=\"password\", value=value, attrs={})\n        self.assertIn(_(\"No password set.\"), html)\n", "    def test_read_only_password_hash_widget_context(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = (\n            \"pbkdf2_sha256$100000$a6Pucb1qSFcD$WmCkn9Hqidj48NVe5x0FEM6A9YiOqQcl/83m2Z5u\"\n            \"dm0=\"\n        )\n        context = widget.get_context(name=\"password\", value=value, attrs={})\n        summary = context[\"summary\"]\n        self.assertEqual(len(summary), 4)\n        self.assertEqual(summary[0][\"label\"], \"algorithm\")\n        self.assertEqual(summary[1][\"label\"], \"iterations\")\n        self.assertEqual(summary[2][\"label\"], \"salt\")\n        self.assertEqual(summary[3][\"label\"], \"hash\")\n", "    def test_render_with_unusable_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = \"unusable password\"\n        html = widget.render(name=\"password\", value=value, attrs={})\n        self.assertIn(_(\"No password set.\"), html)\n", "    def test_custom_username_field(self):\n        class CustomUserCreationForm(UserCreationForm):\n            class Meta(UserCreationForm.Meta):\n                model = CustomUser\n                fields = (\"email\",)\n\n        data = {\n            \"email\": \"test@example.com\",\n            \"password1\": \"test123\",\n            \"password2\": \"test123\",\n        }\n        form = CustomUserCreationForm(data)\n        self.assertTrue(form.is_valid())\n        user = form.save()\n        self.assertEqual(user.email, data[\"email\"])\n"], "sample_96": ["def test_ridge_saga_object_dtype():\n    # Check the dtype of the coef_ and intercept_ attributes\n    # when using the 'saga' solver.\n    X, y = make_regression(n_samples=1000, n_features=10,\n                           random_state=42)\n    ridge_32 = Ridge(solver='saga', random_state=42)\n    ridge_64 = Ridge(solver='saga', random_state=42)\n\n    X_32, y_32 = X.astype(np.float32), y.astype(np.float32)\n    X_64, y_64 = X.astype(np.float64), y.astype(np.float64)\n\n    ridge_32.fit(X_32, y_32)\n    ridge_64.fit(X_64, y_64)\n\n    assert ridge_32.coef_.dtype == X_32.dtype\n    assert ridge_64.coef_.dtype == X_64.dtype\n\n    assert ridge_32.intercept_.dtype == X_32.dtype\n    assert ridge_64.intercept_.dtype == X_64.dtype\n", "def test_ridge_saga_solver_convergence():\n    # Test that the SAGA solver converges to the same solution as the other\n    # solvers for a few different values of alpha.\n\n    n_samples, n_features = 100, 10\n    X, y = make_regression(n_samples=n_samples, n_features=n_features,\n                           random_state=42)\n\n    alphas = [1e-3, 1e-2, 1e-1]\n    for alpha in alphas:\n        saga = Ridge(solver='saga', alpha=alpha, max_iter=1000, tol=1e-8,\n                     random_state=42)\n        saga.fit(X, y)\n\n        cholesky = Ridge(solver='cholesky', alpha=alpha)\n        cholesky.fit(X, y)\n\n        assert_array_almost_equal(saga.coef_, cholesky.coef_)\n", "def test_ridge_classifier_multiclass():\n    # Test multiclass classification with RidgeClassifier\n\n    X, y = make_classification(n_samples=100, n_features=20, n_informative=10,\n                               n_redundant=0, n_repeated=0, n_classes=3,\n                               random_state=42)\n\n    clf = RidgeClassifier()\n    clf.fit(X, y)\n    assert_equal(clf.coef_.shape, (3, 20))\n    assert_equal(clf.intercept_.shape, (3,))\n    assert_almost_equal(clf.decision_function(X).shape, (100, 3))\n    assert_almost_equal(clf.predict_proba(X).shape, (100, 3))\n    assert_almost_equal(clf.predict(X).shape, (100,))\n", "def test_ridge_saga_solver():\n    # Test that the 'saga' solver is working as expected\n    X, y = make_regression(n_samples=1000, n_features=10, random_state=42)\n    ridge_saga = Ridge(solver='saga', max_iter=1000, tol=1e-8)\n    ridge_saga.fit(X, y)\n\n    ridge_lsqr = Ridge(solver='lsqr')\n    ridge_lsqr.fit(X, y)\n\n    assert_almost_equal(ridge_saga.coef_, ridge_lsqr.coef_, decimal=3)\n", "def test_ridge_multitarget():\n    # Test that Ridge can handle multi-target regression\n    X, y = make_regression(n_samples=100, n_features=10, n_targets=3,\n                           random_state=0)\n    ridge = Ridge()\n    ridge.fit(X, y)\n    assert_equal(ridge.coef_.shape, (3, 10))\n    assert_equal(ridge.intercept_.shape, (3,))\n"], "sample_36": ["def test_resolve_expression(self):\n    q = Q(x=1)\n    query = q.resolve_expression()\n    self.assertIsNotNone(query)\n    # Not testing the actual query construction here, that's done elsewhere.\n\n    q2 = ~Q(y=2)\n    query2 = q2.resolve_expression()\n    self.assertIsNotNone(query2)\n\n    combined_q = q & q2\n    combined_query = combined_q.resolve_expression()\n    self.assertIsNotNone(combined_query)\n", "def test_register_lookup(self):\n    class TestLookup:\n        lookup_name = 'test'\n\n    Q.register_lookup(TestLookup)\n    self.assertIn('test', Q.get_lookups())\n    Q._unregister_lookup(TestLookup)  # Cleanup for next tests\n", "def test_deconstruct_multiple_levels(self):\n    q1 = Q(price__gt=F('discounted_price'))\n    q2 = Q(price=F('discounted_price'))\n    q3 = Q(tax__lt=F('price'))\n    q = (q1 & q2) | q3\n    path, args, kwargs = q.deconstruct()\n    self.assertEqual(args, (\n        ('price', F('discounted_price')),\n        ('price__gt', F('discounted_price')),\n        ('tax__lt', F('price')),\n    ))\n    self.assertEqual(kwargs, {'_connector': 'OR'})\n", "def test_register_lookup(self):\n    class MyLookup:\n        lookup_name = 'mylookup'\n\n    Q.register_lookup(MyLookup)\n    self.assertEqual(Q._get_lookup('mylookup'), MyLookup)\n\n    # Test registering a lookup with the same name.\n    class MyLookup2:\n        lookup_name = 'mylookup'\n\n    with self.assertRaises(AttributeError):\n        Q.register_lookup(MyLookup2)\n\n    # Test unregistering a lookup.\n    Q._unregister_lookup(MyLookup)\n    self.assertIsNone(Q._get_lookup('mylookup'))\n", "def test_register_lookup(self):\n    class TestLookup:\n        lookup_name = 'test'\n\n    class TestClass(RegisterLookupMixin):\n        pass\n\n    TestClass.register_lookup(TestLookup)\n    self.assertIn('test', TestClass.get_lookups())\n\n    TestClass._unregister_lookup(TestLookup)\n    self.assertNotIn('test', TestClass.get_lookups())\n"], "sample_78": ["def test_cli_custom_command_name(app):\n    \"\"\"Test that the CLI command name can be customized\"\"\"\n    bp = Blueprint(\"custom\", __name__, cli_group=\"customized\")\n    app.register_blueprint(bp)\n\n    @click.command(\"custom_command\")\n        click.echo(\"custom_result\")\n\n    bp.cli.add_command(custom_command, name=\"custom_name\")\n\n    app_runner = app.test_cli_runner()\n\n    result = app_runner.invoke(args=[\"customized\", \"custom_name\"])\n    assert \"custom_result\" in result.output\n", "def test_cli_no_app(runner):\n    \"\"\"Test the CLI when no app is given\"\"\"\n    cli = FlaskGroup()\n    result = runner.invoke(cli, [\"--help\"])\n    assert \"FLASK_APP\" in result.stderr\n    assert \"Usage:\" in result.stdout\n\n    result = runner.invoke(cli, [\"run\", \"--help\"])\n    assert \"FLASK_APP\" in result.stderr\n    assert \"Usage:\" in result.stdout\n", "def test_cli_group_name_conflict(app):\n    \"\"\"Test that blueprints with conflicting CLI group names raise an error\"\"\"\n    bp1 = Blueprint(\"bp1\", __name__, cli_group=\"conflict\")\n    bp2 = Blueprint(\"bp2\", __name__, cli_group=\"conflict\")\n\n    app.register_blueprint(bp1)\n\n    with pytest.raises(ValueError):\n        app.register_blueprint(bp2)\n", "def test_cli_with_appcontext(app):\n    \"\"\"Test that the app context is pushed when running a command with\n    @with_appcontext.\"\"\"\n    called = []\n\n    @app.cli.command()\n    @with_appcontext\n        assert current_app._get_current_object() is app\n        called.append(True)\n\n    runner = app.test_cli_runner()\n    result = runner.invoke(args=[\"test\"])\n    assert result.exit_code == 0\n    assert called\n", "def test_cli_help_text(app):\n    \"\"\"Test help text for CLI commands\"\"\"\n    @app.cli.command(\"test\")\n        \"\"\"Test command help text.\"\"\"\n        click.echo(\"test\")\n\n    result = app.test_cli_runner().invoke(args=[\"--help\"])\n    assert \"Test command help text.\" in result.output\n\n    result = app.test_cli_runner().invoke(args=[\"test\", \"--help\"])\n    assert \"Test command help text.\" in result.output\n"], "sample_2": ["def test_get_axis_types():\n    with fits.open(get_pkg_data_filename('data/sip.fits')) as f:\n        w = wcs.WCS(f[0].header)\n\n    result = w.get_axis_types()\n    assert len(result) == 2\n    for item in result:\n        assert 'coordinate_type' in item\n        assert 'scale' in item\n        assert 'group' in item\n        assert 'number' in item\n", "def test_celestial():\n    w = wcs.WCS(naxis=3)\n    w.wcs.ctype = ['RA---TAN', 'DEC--TAN', 'FREQ']\n    w.wcs.crval = [1, 2, 3]\n    w.wcs.crpix = [4, 5, 6]\n    w.wcs.cdelt = [7, 8, 9]\n\n    celestial_wcs = w.celestial\n\n    assert celestial_wcs.naxis == 2\n    assert np.all(celestial_wcs.wcs.crval == w.wcs.crval[:2])\n    assert np.all(celestial_wcs.wcs.crpix == w.wcs.crpix[:2])\n    assert np.all(celestial_wcs.wcs.cdelt == w.wcs.cdelt[:2])\n", "def test_axis_type_names():\n    w = wcs.WCS(naxis=2)\n    w.wcs.ctype = ['RA---TAN', 'DEC--TAN']\n    w.wcs.cname = ['', '']\n    assert w.axis_type_names == ['RA', 'DEC']\n\n    w.wcs.cname = ['Longitude', 'Latitude']\n    assert w.axis_type_names == ['Longitude', 'Latitude']\n", "def test_axis_type_names():\n    w = wcs.WCS(naxis=2)\n    w.wcs.ctype = [\"RA---CAR\", \"DEC--CAR\"]\n    w.wcs.cname = [\"Right Ascension\", \"Declination\"]\n    assert w.axis_type_names == [\"Right Ascension\", \"Declination\"]\n\n    w.wcs.cname = [\"\", \"\"]\n    assert w.axis_type_names == [\"RA\", \"DEC\"]\n", "def test_axis_type_names():\n    w = wcs.WCS(naxis=2)\n    w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\"]\n    assert w.axis_type_names == ['RA', 'DEC']\n\n    w = wcs.WCS(naxis=3)\n    w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\", \"FREQ\"]\n    w.wcs.cname = [\"RA\", \"DEC\", \"\"]\n    assert w.axis_type_names == ['RA', 'DEC', 'FREQ']\n"], "sample_71": ["def test_reload_library():\n    original_library = style.library.copy()\n    with temp_style('test', DUMMY_SETTINGS):\n        assert 'test' in style.library\n    style.reload_library()\n    assert style.library == original_library\n", "def test_reload_library_with_empty_directory(tmpdir):\n    empty_dir = tmpdir.mkdir(\"empty_stylelib\")\n    USER_LIBRARY_PATHS.append(str(empty_dir))\n    try:\n        style.reload_library()\n    except Exception as e:\n        pytest.fail(f\"reload_library failed with empty directory: {e}\")\n    finally:\n        USER_LIBRARY_PATHS.pop()\n", "def test_reload_library():\n    original_library = style.library.copy()\n    with temp_style('test', DUMMY_SETTINGS):\n        assert 'test' in style.library\n    style.reload_library()\n    assert style.library == original_library\n", "def test_reload_library():\n    original_library = style.library.copy()\n    with temp_style('test', DUMMY_SETTINGS):\n        assert 'test' in style.library\n    style.reload_library()\n    assert style.library == original_library\n", "def test_reload_library():\n    original_value = 'gray'\n    other_value = 'blue'\n    mpl.rcParams[PARAM] = original_value\n    with temp_style('test', {PARAM: other_value}):\n        style.reload_library()\n        with style.context('test'):\n            assert mpl.rcParams[PARAM] == other_value\n    assert mpl.rcParams[PARAM] == original_value\n"], "sample_26": ["    def test_clone_test_db(self):\n        # Create a test database.\n        connection.creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n        old_database_name = connection.settings_dict['NAME']\n        try:\n            # Clone the test database.\n            connection.creation.clone_test_db(suffix='clone', verbosity=0, autoclobber=True, keepdb=False)\n            clone_settings_dict = connection.creation.get_test_db_clone_settings('clone')\n            # Check that the cloned database has the same settings as the original.\n            self.assertEqual(clone_settings_dict['ENGINE'], connection.settings_dict['ENGINE'])\n            self.assertEqual(clone_settings_dict['HOST'], connection.settings_dict['HOST'])\n            self.assertEqual(clone_settings_dict['PORT'], connection.settings_dict['PORT'])\n            self.assertNotEqual(clone_settings_dict['NAME'], connection.settings_dict['NAME'])\n        finally:\n            # Destroy the test databases.\n            connection.creation.destroy_test_db(old_database_name, verbosity=0, suffix='clone')\n            connection.creation.destroy_test_db(old_database_name, verbosity=0)\n", "    def test_create_test_db_keepdb(self):\n        # create_test_db with keepdb=True doesn't destroy the database.\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db'):\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False, keepdb=True)\n            self.assertEqual(test_connection.settings_dict['NAME'], creation._get_test_db_name())\n            # The database isn't destroyed.\n            with mock.patch.object(creation, '_destroy_test_db') as mocked_destroy:\n                creation.destroy_test_db(old_database_name, verbosity=0, keepdb=True)\n            mocked_destroy.assert_not_called()\n        finally:\n            # Clean up the test database.\n            test_connection.settings_dict['NAME'] = creation._get_test_db_name()\n            creation.destroy_test_db(old_database_name, verbosity=0)\n", "    def test_create_test_db_with_keepdb(self):\n        # create_test_db() with keepdb=True doesn't destroy the database.\n        creation = connection.creation\n        old_database_name = creation.connection.settings_dict['NAME']\n        try:\n            creation.create_test_db(verbosity=0, autoclobber=True, keepdb=True)\n            self.assertEqual(creation.connection.settings_dict['NAME'], creation._get_test_db_name())\n            creation.destroy_test_db(old_database_name, verbosity=0, keepdb=True)\n            self.assertEqual(creation.connection.settings_dict['NAME'], creation._get_test_db_name())\n        finally:\n            creation.destroy_test_db(old_database_name, verbosity=0)\n", "    def test_create_test_db_with_keepdb(self, mocked_execute_create_test_db):\n        # If keepdb is True, _execute_create_test_db should be called with keepdb=True.\n        connection.creation.create_test_db(verbosity=0, autoclobber=True, serialize=False, keepdb=True)\n        mocked_execute_create_test_db.assert_called_once()\n        self.assertEqual(mocked_execute_create_test_db.call_args[1]['keepdb'], True)\n", "    def test_clone_test_db(self):\n        # Create a test database.\n        creation = connection.creation\n        old_database_name = connection.settings_dict['NAME']\n        test_database_name = creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n\n        # Clone the test database.\n        clone_database_name = creation.get_test_db_clone_settings('clone')['NAME']\n        creation.clone_test_db(suffix='clone', verbosity=0, autoclobber=True, keepdb=False)\n\n        # Check that the clone database exists.\n        with creation._nodb_cursor() as cursor:\n            cursor.execute(\"SELECT 1 FROM information_schema.schemata WHERE schema_name = %s\", [clone_database_name])\n            self.assertEqual(cursor.fetchone()[0], 1)\n\n        # Destroy the test databases.\n        creation.destroy_test_db(old_database_name, verbosity=0)\n        creation.destroy_test_db(clone_database_name, verbosity=0)\n"], "sample_23": ["def test_union_with_queryset_none(self):\n    qs1 = Number.objects.all()\n    qs2 = Number.objects.none()\n    self.assertQuerysetEqual(qs1.union(qs2), qs1, ordered=False)\n    self.assertQuerysetEqual(qs2.union(qs1), qs1, ordered=False)\n", "def test_union_with_subqueries(self):\n    subquery1 = Number.objects.filter(num__lte=5)\n    subquery2 = Number.objects.filter(num__gt=5)\n    qs1 = Number.objects.filter(num__in=subquery1)\n    qs2 = Number.objects.filter(num__in=subquery2)\n    self.assertNumbersEqual(qs1.union(qs2), range(10))\n", "def test_union_with_f_expression_in_annotation(self):\n    qs1 = Number.objects.annotate(\n        annotation=Value(0, IntegerField()),\n    )\n    qs2 = Number.objects.annotate(\n        annotation=F('num'),\n    )\n    self.assertEqual(len(qs1.union(qs2)), 10)\n", "def test_union_with_subqueries(self):\n    subquery = Number.objects.filter(num__lte=5)\n    qs1 = Number.objects.filter(num__in=subquery).values_list('num', flat=True)\n    qs2 = Number.objects.filter(num__gt=5).values_list('num', flat=True)\n    self.assertCountEqual(qs1.union(qs2), list(range(10)))\n", "def test_union_with_filter(self):\n    qs1 = Number.objects.filter(num__lte=5)\n    qs2 = Number.objects.filter(num__gte=5)\n    self.assertNumbersEqual(qs1.union(qs2).filter(num__lte=3), [0, 1, 2, 3])\n"], "sample_117": ["def test_stringify_type_hints_Ellipsis():\n    assert stringify(Ellipsis) == '...'\n", "def test_stringify_type_hints_ForwardRef():\n    if sys.version_info >= (3, 7):\n        from typing import ForwardRef  # type: ignore\n        assert stringify(ForwardRef(\"myint\")) == \"myint\"\n    else:\n        from sphinx.util.typing import ForwardRef  # type: ignore\n        assert stringify(ForwardRef(\"myint\")) == \"myint\"\n", "def test_stringify_type_hints_Ellipsis():\n    assert stringify(Ellipsis) == \"...\"\n    assert stringify(Tuple[str, Ellipsis]) == \"Tuple[str, ...]\"\n", "def test_stringify_type_hints_Ellipsis():\n    assert stringify(Ellipsis) == \"...\"\n", "def test_stringify_type_hints_ForwardRef():\n    from typing import ForwardRef  # type: ignore\n    assert stringify(ForwardRef(\"myint\")) == \"myint\"\n"], "sample_87": ["def test_collect_report_on_conftest_files(testdir):\n    \"\"\"\n    Test that conftest files are reported when collecting tests.\n    \"\"\"\n    testdir.makeconftest(\n        \"\"\"\n        import pytest\n            return pytest.File(path, parent)\n    \"\"\"\n    )\n    testdir.makepyfile(\"def test_foo(): pass\")\n    result = testdir.runpytest(\"--collect-only\")\n    result.stdout.fnmatch_lines([\"*<Module test_*.py>*\", \"*<Conftest conftest.py>*\"])\n", "def test_collect_pkg_init_with_broken_import(testdir):\n    subdir = testdir.mkdir(\"sub\")\n    init = subdir.ensure(\"__init__.py\")\n    init.write(\"import unknown_module\")\n\n    result = testdir.runpytest(str(init))\n    result.stdout.fnmatch_lines([\"*ImportError*\"])\n\n    result = testdir.runpytest(\"-v\", \"-o\", \"python_files=*.py\", str(init))\n    result.stdout.fnmatch_lines([\"*ImportError*\"])\n", "def test_collect_ignore_collect_path_with_conftest(testdir):\n    testdir.makeconftest(\n        \"\"\"\n            return path.basename == \"test_ignore.py\"\n    \"\"\"\n    )\n    testdir.makepyfile(test_ignore=\"def test_ignore(): pass\")\n    testdir.makepyfile(test_other=\"def test_other(): pass\")\n\n    result = testdir.runpytest()\n    assert result.ret == 0\n    result.stdout.fnmatch_lines([\"*1 passed in*\"])\n", "def test_collect_error_on_broken_package(testdir):\n    sub = testdir.mkdir(\"sub\")\n    sub.ensure(\"__init__.py\").write(\"syntax error\")\n    sub.ensure(\"test_file.py\").write(\"def test_file(): pass\")\n\n    result = testdir.runpytest(str(sub))\n    result.stdout.fnmatch_lines(\n        [\n            \"*ERROR collecting sub*\",\n            \"  *SyntaxError: invalid syntax*\",\n            \"*! Interrupted: 1 error during collection !*\",\n            \"*= 1 error in *\",\n        ]\n    )\n    assert result.ret == ExitCode.INTERRUPTED\n", "def test_collect_file_ignores_broken_symlinks(testdir):\n    \"\"\"Test that collecting a directory ignores broken symlinks.\"\"\"\n    sub = testdir.mkdir(\"sub\")\n    sub.join(\"test_broken.py\").mksymlinkto(\"test_doesnotexist.py\")\n\n    result = testdir.runpytest(\"-v\", str(sub))\n    result.stdout.fnmatch_lines([\"*collected 0 items*\"])\n    assert result.ret == ExitCode.NO_TESTS_COLLECTED\n"], "sample_153": ["def test_pretty_printing_product():\n    from sympy.abc import x, y\n    from sympy import Product\n    expr = Product(x + y, (x, 1, 3))\n    expected = \"\"\"\\\n 3   ", "def test_pretty_printing_dict():\n    from sympy import Symbol\n\n    d = {1: 2, 'a': Symbol('b')}\n    expected_ascii = '{1: 2, a: b}'\n    expected_unicode = '{1: 2, a: b}'\n\n    assert pretty(d) == expected_ascii\n    assert upretty(d) == expected_unicode\n", "def test_pretty_printing_add():\n    from sympy import symbols, Add\n    x, y = symbols('x y')\n    assert pretty(Add(x, y)) == 'x + y'\n    assert pretty(Add(x, -y)) == 'x - y'\n    assert pretty(Add(-x, y)) == '-x + y'\n    assert pretty(Add(-x, -y)) == '-x - y'\n", "def test_pretty_printing_custom_symbols():\n    assert pretty(v[1], mat_symbol_style='bold') == 'i_N'\n    assert upretty(v[1], mat_symbol_style='bold') == 'i_N'\n\n    assert pretty(v[5], mat_symbol_style='bold') == '(a) i_N + (-b) j_N'\n    assert upretty(v[5], mat_symbol_style='bold') == '(a) i_N + (-b) j_N'\n", "def test_pretty_printing_Mod():\n    from sympy import symbols\n    x, y = symbols('x y')\n    assert upretty(x % y) == 'x mod y'\n    assert pretty(x % y) == 'x mod y'\n"], "sample_82": ["def test_groupby_quantile_interpolation():\n    array = xr.DataArray([1, 2, 3, 4, 5, 6], [(\"x\", [1, 1, 1, 2, 2, 2])])\n\n    # Linear interpolation (default)\n    expected = xr.DataArray(\n        data=[2.0, 5.0], coords={\"x\": [1, 2], \"quantile\": 0.5}, dims=\"x\"\n    )\n    actual = array.groupby(\"x\").quantile(0.5)\n    assert_identical(expected, actual)\n\n    # Lower interpolation\n    expected = xr.DataArray(\n        data=[1, 4], coords={\"x\": [1, 2], \"quantile\": 0.5}, dims=\"x\"\n    )\n    actual = array.groupby(\"x\").quantile(0.5, interpolation=\"lower\")\n    assert_identical(expected, actual)\n\n    # Higher interpolation\n    expected = xr.DataArray(\n        data=[3, 6], coords={\"x\": [1, 2], \"quantile\": 0.5}, dims=\"x\"\n    )\n    actual = array.groupby(\"x\").quantile(0.5, interpolation=\"higher\")\n    assert_identical(expected, actual)\n\n    # Midpoint interpolation\n    expected = xr.DataArray(\n        data=[2.0, 5.0], coords={\"x\": [1, 2], \"quantile\": 0.5}, dims=\"x\"\n    )\n    actual = array.groupby(\"x\").quantile(0.5, interpolation=\"midpoint\")\n    assert_identical(expected, actual)\n\n    # Nearest interpolation\n    expected = xr.DataArray(\n        data=[2, 5], coords={\"x\": [1, 2], \"quantile\": 0.5}, dims=\"x\"\n    )\n    actual = array.groupby(\"x\").quantile(0.5, interpolation=\"nearest\")\n    assert_identical(expected, actual)\n", "def test_groupby_map_type_promotion():\n    # GH: 4090\n    ds = xr.Dataset(\n        {\"a\": ((\"x\", \"y\"), np.random.randn(3, 4))},\n        coords={\"x\": [1, 2, 3], \"y\": [1, 2, 3, 4]},\n    )\n\n        return x.sum(\"y\")\n\n    expected = func(ds)\n    actual = ds.groupby(\"x\").map(func)\n\n    assert_identical(expected, actual)\n", "def test_groupby_bins_dtype():\n    # GH2232\n    array = xr.DataArray([1.1, 2.2, 3.3], dims=\"x\")\n    bins = [0, 2, 4]\n    actual = array.groupby_bins(\"x\", bins).sum()\n    expected = xr.DataArray(\n        [3.3, np.nan],\n        dims=[\"x_bins\"],\n        coords={\"x_bins\": pd.cut(array, bins).categories},\n    )\n    assert_identical(actual, expected)\n", "def test_groupby_iterate_groups(dataset):\n    for _, group in dataset.groupby(\"x\"):\n        assert isinstance(group, type(dataset))\n        assert group.sizes[\"x\"] == 1\n\n    for _, group in dataset.foo.groupby(\"x\"):\n        assert isinstance(group, type(dataset.foo))\n        assert group.sizes[\"x\"] == 1\n", "def test_groupby_bins_nonuniform():\n    # GH4195\n    da = xr.DataArray([1, 2, 3], dims=\"x\", coords={\"x\": [0.1, 0.4, 0.9]})\n    bins = [0, 0.3, 0.6, 1]\n    expected = xr.DataArray([1, 2, 3], dims=\"x_bins\", coords={\"x_bins\": pd.cut(da.x, bins).categories})\n    actual = da.groupby_bins(\"x\", bins=bins).sum()\n    assert_identical(actual, expected)\n"], "sample_70": ["def test_legend_set_title_fontproperties():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='line')\n    leg = ax.legend()\n    title_fontproperties = FontProperties(family='serif', size=20)\n    leg.set_title('Aardvark', prop=title_fontproperties)\n    assert leg.get_title().get_fontname() == 'serif'\n    assert leg.get_title().get_fontsize() == 20\n", "def test_legend_remove_duplicate_labels():\n    fig, ax = plt.subplots()\n    line1, = ax.plot([1, 2, 3], label='line')\n    line2, = ax.plot([2, 3, 4], label='line')\n    leg = ax.legend()\n    assert len(leg.get_texts()) == 1\n    assert leg.get_texts()[0].get_text() == 'line'\n", "def test_legend_bbox_to_anchor_tuple():\n    # Test that bbox_to_anchor can be a tuple of 2 or 4 floats.\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='line')\n    ax.legend(bbox_to_anchor=(0.5, 0.5))\n    ax.legend(bbox_to_anchor=(0.5, 0.5, 0.2, 0.2))\n", "def test_get_default_handler_map():\n    # Test that Legend.get_default_handler_map() returns a dictionary.\n    assert isinstance(mlegend.Legend.get_default_handler_map(), dict)\n", "def test_legend_bbox_transform():\n    # Test that bbox_to_anchor and bbox_transform work correctly.\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1], label='line')\n    leg = ax.legend(bbox_to_anchor=(0.5, 0.5), bbox_transform=ax.transAxes)\n    assert leg.get_bbox_to_anchor().x0 == 0.5\n    assert leg.get_bbox_to_anchor().y0 == 0.5\n    leg = ax.legend(bbox_to_anchor=(0.5, 0.5), bbox_transform=fig.transFigure)\n    assert leg.get_bbox_to_anchor().x0 == 0.5\n    assert leg.get_bbox_to_anchor().y0 == 0.5\n"], "sample_56": ["    def setUpClass(cls):\n        super().setUpClass()\n        cls.error_same_tags = Error(\n            E003.msg.format(\n                \"'same_tags'\",\n                \"'check_framework.template_test_apps.same_tags_app_1.\"\n                \"templatetags.same_tags', \"\n                \"'check_framework.template_test_apps.same_tags_app_2.\"\n                \"templatetags.same_tags', \"\n                \"'check_framework.template_test_apps.same_tags_app_3.\"\n                \"templatetags.same_tags'\",\n            ),\n            id=E003.id,\n        )\n", "    def setUpClass(cls):\n        super().setUpClass()\n        cls.error_same_tags = Error(\n            E003.msg.format(\n                \"'same_tags'\",\n                \"'check_framework.template_test_apps.same_tags_app_1.\"\n                \"templatetags.same_tags', \"\n                \"'check_framework.template_test_apps.same_tags_app_2.\"\n                \"templatetags.same_tags'\",\n            ),\n            id=E003.id,\n        )\n", "    def setUpClass(cls):\n        super().setUpClass()\n        cls.error_same_tags = Error(\n            E003.msg.format(\n                \"'static'\",\n                \"'django.templatetags.static', 'django.contrib.staticfiles.templatetags.staticfiles.static'\",\n            ),\n            id=E003.id,\n        )\n", "    def test_empty_libraries(self):\n        self.assertEqual(check_for_template_tags_with_the_same_name(None), [])\n", "    def setUpClass(cls):\n        super().setUpClass()\n        cls.error_same_tags = Error(\n            E003.msg.format(\n                \"'same_tags'\",\n                \"'check_framework.template_test_apps.same_tags_app_1.\"\n                \"templatetags.same_tags', \"\n                \"'check_framework.template_test_apps.same_tags_app_2.\"\n                \"templatetags.same_tags'\",\n            ),\n            id=E003.id,\n        )\n"], "sample_18": ["    def test_db_table_clash(self):\n        class Person(models.Model):\n            pass\n\n        class Group(models.Model):\n            members = models.ManyToManyField(Person, through='Membership')\n\n        class Membership(models.Model):\n            person = models.ForeignKey(Person, models.CASCADE)\n            group = models.ForeignKey(Group, models.CASCADE)\n\n        # Force the m2m table name to clash with an existing model.\n        m2m_table_name = Group._meta.get_field('members').m2m_db_table()\n        with mock.patch.object(Group._meta.get_field('members'), 'm2m_db_table', return_value=m2m_table_name):\n            with override_settings(DATABASE_ROUTERS=[]):\n                self.assertEqual(Group.check(), [\n                    Error(\n                        \"The field's intermediary table '%s' clashes with the table name of '%s'.\" % (\n                            m2m_table_name, Person._meta.label,\n                        ),\n                        obj=Group._meta.get_field('members'),\n                        id='fields.E340',\n                    ),\n                ])\n", "    def test_recursive_swappable_model(self):\n        class SwappableModel(models.Model):\n            class Meta:\n                swappable = 'TEST_SWAPPABLE_MODEL'\n\n        class Model(models.Model):\n            fk = models.ForeignKey('SwappableModel', models.CASCADE)\n\n        with override_settings(TEST_SWAPPABLE_MODEL='invalid_models_tests.SwappableModel'):\n            field = Model._meta.get_field('fk')\n            self.assertEqual(field.check(from_model=Model), [])\n", "def test_unique_foreign_object(self):\n    class Parent(models.Model):\n        a = models.PositiveIntegerField(unique=True)\n        b = models.PositiveIntegerField()\n\n    class Child(models.Model):\n        a = models.PositiveIntegerField()\n        b = models.PositiveIntegerField()\n        parent = models.ForeignObject(\n            Parent,\n            on_delete=models.SET_NULL,\n            from_fields=('a', 'b'),\n            to_fields=('a', 'b'),\n        )\n\n    field = Child._meta.get_field('parent')\n    self.assertEqual(field.check(from_model=Child), [\n        Error(\n            \"No subset of the fields 'a', 'b' on model 'Parent' is unique.\",\n            hint=(\n                'Mark a single field as unique=True or add a set of '\n                'fields to a unique constraint (via unique_together or a '\n                'UniqueConstraint (without condition) in the model '\n                'Meta.constraints).'\n            ),\n            obj=field,\n            id='fields.E310',\n        ),\n    ])\n", "    def test_foreign_key_to_model_with_inheritance(self):\n        class Parent(models.Model):\n            pass\n\n        class Child(Parent):\n            pass\n\n        class Model(models.Model):\n            foreign_key = models.ForeignKey(Parent, models.CASCADE)\n\n        field = Model._meta.get_field('foreign_key')\n        self.assertEqual(field.check(), [])\n", "    def test_m2m_through_fields_with_swappable_model(self):\n        class Replacement(models.Model):\n            pass\n\n        class SwappedModel(models.Model):\n            class Meta:\n                swappable = 'TEST_SWAPPABLE_MODEL'\n\n        class Event(models.Model):\n            invitees = models.ManyToManyField(\n                SwappedModel,\n                through='Invitation',\n                through_fields=('event', 'invitee'),\n            )\n\n        class Invitation(models.Model):\n            event = models.ForeignKey(Event, models.CASCADE)\n            invitee = models.ForeignKey(SwappedModel, models.CASCADE)\n\n        field = Event._meta.get_field('invitees')\n        self.assertEqual(field.check(from_model=Event), [\n            Error(\n                \"Field defines a relation with the model \"\n                \"'invalid_models_tests.SwappedModel', which has been swapped out.\",\n                hint=\"Update the relation to point at 'settings.TEST_SWAPPABLE_MODEL'.\",\n                obj=field,\n                id='fields.E301',\n            ),\n        ])\n"], "sample_75": ["def test_grid_label_mode():\n    fig = plt.figure()\n    grid = Grid(fig, 111, (2, 3))\n    for mode in [\"L\", \"1\", \"all\"]:\n        grid.set_label_mode(mode)\n        fig.canvas.draw()\n    with pytest.warns(UserWarning):\n        grid.set_label_mode(\"foo\")\n", "def test_cbar_axes_base_toggle_label():\n    fig = plt.figure()\n    cax = CbarAxesBase(fig, [0.1, 0.1, 0.8, 0.8], orientation='vertical')\n    with pytest.warns(mpl.MatplotlibDeprecationWarning):\n        cax.toggle_label(False)\n    assert not cax.axis['left'].get_visible()\n    assert not cax.axis['left'].major_ticklabels.get_visible()\n    with pytest.warns(mpl.MatplotlibDeprecationWarning):\n        cax.toggle_label(True)\n    assert cax.axis['left'].get_visible()\n    assert cax.axis['left'].major_ticklabels.get_visible()\n", "def test_grid_with_share_all():\n    fig = plt.figure()\n    grid = Grid(fig, 111, (2, 2), share_all=True)\n    for ax in grid:\n        ax.plot([1, 2, 3])\n    fig.canvas.draw()\n    assert len(grid[0].get_shared_x_axes().get_siblings(grid[0])) == 4\n    assert len(grid[0].get_shared_y_axes().get_siblings(grid[0])) == 4\n", "def test_grid_with_axes_pad():\n    fig = plt.figure(figsize=(8, 6))\n    grid = Grid(fig, 111, nrows_ncols=(2, 2), axes_pad=(0.5, 0.2))\n    for ax in grid:\n        ax.imshow(np.arange(100).reshape((10, 10)), interpolation='nearest')\n", "def test_cbar_axes_base_toggle_label_deprecation():\n    fig, ax = plt.subplots()\n    cax = CbarAxesBase(fig, [0.1, 0.1, 0.8, 0.05], orientation='horizontal')\n    with pytest.warns(mpl.MatplotlibDeprecationWarning):\n        cax.toggle_label(True)\n"], "sample_114": ["def test_class_distribution_sparse_empty_columns():\n    # Test class_distribution with sparse matrix containing empty columns\n\n    # Create a sparse matrix with two empty columns\n    data = np.array([1, 2, 1, 4, 2, 1])\n    indices = np.array([0, 1, 2, 3, 4, 5])\n    indptr = np.array([0, 3, 6, 6, 6])\n    y_sp = sp.csc_matrix((data, indices, indptr), shape=(6, 4))\n\n    classes, n_classes, class_prior = class_distribution(y_sp)\n\n    classes_expected = [[1, 2, 4], [0], [], []]\n    n_classes_expected = [3, 1, 0, 0]\n    class_prior_expected = [[3 / 6, 2 / 6, 1 / 6], [1.0], [], []]\n\n    for k in range(y_sp.shape[1]):\n        assert_array_almost_equal(classes[k], classes_expected[k])\n        assert_array_almost_equal(n_classes[k], n_classes_expected[k])\n        if n_classes[k] > 0:\n            assert_array_almost_equal(class_prior[k], class_prior_expected[k])\n", "def test_type_of_target_sparse_matrix_with_explicit_zeros():\n    # Test that type_of_target correctly handles sparse matrices with explicit zeros.\n    y = csr_matrix(np.array([[0, 1], [1, 0]]))\n    assert type_of_target(y) == \"multilabel-indicator\"\n\n    y = csc_matrix(np.array([[0, 1], [1, 0]]))\n    assert type_of_target(y) == \"multilabel-indicator\"\n\n    y = coo_matrix(np.array([[0, 1], [1, 0]]))\n    assert type_of_target(y) == \"multilabel-indicator\"\n\n    y = dok_matrix(np.array([[0, 1], [1, 0]]))\n    assert type_of_target(y) == \"multilabel-indicator\"\n\n    y = lil_matrix(np.array([[0, 1], [1, 0]]))\n    assert type_of_target(y) == \"multilabel-indicator\"\n", "def test_class_distribution_sparse_explicit_zeros():\n    # Test class_distribution function with sparse matrices containing explicit zeros.\n\n    # Define the sparse matrix with explicit zeros\n    data = np.array([1, 2, 0, 4, 2, 1, 0, 2, 3, 2, 3, 1, 1, 1, 1, 1, 1])\n    indices = np.array([0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 5, 0, 1, 2, 3, 4, 5])\n    indptr = np.array([0, 6, 11, 11, 17])\n    y_sp = sp.csc_matrix((data, indices, indptr), shape=(6, 4))\n\n    classes, n_classes, class_prior = class_distribution(y_sp)\n\n    # Expected results for the sparse matrix\n    classes_expected = [[0, 1, 2, 4], [0, 2, 3], [0], [1]]\n    n_classes_expected = [4, 3, 1, 1]\n    class_prior_expected = [\n        [1 / 6, 2 / 6, 1 / 6, 2 / 6],\n        [1 / 3, 1 / 3, 1 / 3],\n        [1.0],\n        [1.0],\n    ]\n\n    for k in range(y_sp.shape[1]):\n        assert_array_almost_equal(classes[k], classes_expected[k])\n        assert_array_almost_equal(n_classes[k], n_classes_expected[k])\n        assert_array_almost_equal(class_prior[k], class_prior_expected[k])\n", "def test_type_of_target_sparse_empty_row():\n    # Test type_of_target with sparse matrix that has empty row\n\n    X = sp.csr_matrix(np.array([[1, 2], [0, 0]]))\n    assert type_of_target(X) == \"multiclass\"\n\n    X = sp.csc_matrix(np.array([[1, 2], [0, 0]]))\n    assert type_of_target(X) == \"multiclass\"\n\n    X = sp.coo_matrix(np.array([[1, 2], [0, 0]]))\n    assert type_of_target(X) == \"multiclass\"\n", "def test_check_partial_fit_first_call():\n    clf = SVC()\n    classes = np.array([0, 1, 2])\n\n    # First call to partial_fit should set classes_\n    assert _check_partial_fit_first_call(clf, classes)\n    assert_array_equal(clf.classes_, classes)\n\n    # Second call to partial_fit with same classes should not raise an error\n    _check_partial_fit_first_call(clf, classes)\n\n    # Second call to partial_fit with different classes should raise an error\n    different_classes = np.array([0, 1, 3])\n    msg = \"`classes=%r` is not the same as on last call to partial_fit, was: %r\" % (\n        different_classes,\n        clf.classes_,\n    )\n    with pytest.raises(ValueError, match=msg):\n        _check_partial_fit_first_call(clf, different_classes)\n"], "sample_112": ["def test_isotonic_regression_output_dtype():\n    \"\"\"Check that `predict` does return the expected output dtype.\n\n    We need to check that the output dtype is the same as the input dtype.\n\n    Non-regression test for:\n    https://github.com/scikit-learn/scikit-learn/issues/25499\n    \"\"\"\n    X, y = make_regression(n_samples=10, n_features=1, random_state=42)\n    X = X.astype(np.float32)\n    y = y.astype(np.float32)\n    regressor = IsotonicRegression()\n    regressor.fit(X, y)\n    y_pred = regressor.predict(X)\n\n    assert y_pred.dtype == np.float32\n", "def test_isotonic_regression_output_dtype():\n    \"\"\"Check that `predict` and `transform` return the correct output dtype.\n\n    Non-regression test for:\n    https://github.com/scikit-learn/scikit-learn/issues/25499\n    \"\"\"\n    X, y = make_regression(n_samples=10, n_features=1, random_state=42)\n    regressor = IsotonicRegression()\n    regressor.fit(X, y)\n\n    X_test = np.array([0.5], dtype=np.float32)\n    y_pred_32 = regressor.predict(X_test)\n    assert y_pred_32.dtype == np.float32\n\n    X_test = np.array([0.5], dtype=np.float64)\n    y_pred_64 = regressor.predict(X_test)\n    assert y_pred_64.dtype == np.float64\n\n    X_test = np.array([0.5], dtype=np.float32)\n    X_trans_32 = regressor.transform(X_test)\n    assert X_trans_32.dtype == np.float32\n\n    X_test = np.array([0.5], dtype=np.float64)\n    X_trans_64 = regressor.transform(X_test)\n    assert X_trans_64.dtype == np.float64\n", "def test_isotonic_regression_with_ties_in_x_and_zero_weights():\n    # Non-regression test to handle issue where ties in X and zero weights\n    # would cause an error\n    x = [1, 1, 2, 3, 4, 5]\n    y = [1, 2, 3, 4, 5, 6]\n    sample_weight = [1, 0, 1, 1, 1, 1]\n    ir = IsotonicRegression()\n    ir.fit(x, y, sample_weight=sample_weight)\n    assert_array_equal(ir.predict(x), ir.fit_transform(x, y, sample_weight=sample_weight))\n", "def test_isotonic_regression_edge_cases():\n    \"\"\"Check that isotonic regression can handle edge cases.\"\"\"\n    # Single data point\n    X = np.array([1])\n    y = np.array([2])\n    ir = IsotonicRegression()\n    ir.fit(X, y)\n    assert_array_equal(ir.predict(X), y)\n\n    # Two data points with same x value\n    X = np.array([1, 1])\n    y = np.array([2, 3])\n    ir = IsotonicRegression()\n    ir.fit(X, y)\n    assert_array_equal(ir.predict(X), np.array([2.5, 2.5]))\n\n    # Two data points with same y value\n    X = np.array([1, 2])\n    y = np.array([2, 2])\n    ir = IsotonicRegression()\n    ir.fit(X, y)\n    assert_array_equal(ir.predict(X), y)\n\n    # Constant y values\n    X = np.array([1, 2, 3, 4, 5])\n    y = np.array([2, 2, 2, 2, 2])\n    ir = IsotonicRegression()\n    ir.fit(X, y)\n    assert_array_equal(ir.predict(X), y)\n", "def test_isotonic_regression_with_zero_sample_weight():\n    \"\"\"Check that isotonic regression can handle zero sample weights.\"\"\"\n    X = np.arange(10)\n    y = np.arange(10)\n    sample_weight = np.ones_like(y)\n    sample_weight[5] = 0\n\n    iso_reg = IsotonicRegression()\n    iso_reg.fit(X, y, sample_weight=sample_weight)\n\n    assert_array_equal(iso_reg.X_thresholds_, np.array([0, 1, 2, 3, 4, 6, 7, 8, 9]))\n    assert_array_equal(iso_reg.y_thresholds_, np.array([0, 1, 2, 3, 4, 6, 7, 8, 9]))\n\n    y_pred = iso_reg.predict(X)\n    assert_allclose(y_pred, np.array([0, 1, 2, 3, 4, 5.5, 6, 7, 8, 9]))\n"], "sample_138": ["def test_BlockMatrix_as_real_imag():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', n, m)\n    X = BlockMatrix([[A, B]])\n\n    A_re, A_im = symbols('A_re A_im')\n    B_re, B_im = symbols('B_re B_im')\n    A.as_real_imag = lambda: (A_re, A_im)\n    B.as_real_imag = lambda: (B_re, B_im)\n\n    X_re, X_im = X.as_real_imag()\n    assert X_re == BlockMatrix([[A_re, B_re]])\n    assert X_im == BlockMatrix([[A_im, B_im]])\n", "def test_block_collapse_with_inverse():\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, m)\n    C = MatrixSymbol('C', m, n)\n    D = MatrixSymbol('D', m, m)\n    X = BlockMatrix([[A, B], [C, D]])\n\n    assert isinstance(block_collapse(X.I), Inverse)\n\n    # Test that block_collapse works with inverse of a block matrix\n    X_inv = block_collapse(X.I)\n    assert X_inv.blocks.shape == (2, 2)\n    assert X_inv.blocks[0, 0] == (-B*D.I*C + A).I\n    assert X_inv.blocks[0, 1] == -A.I*B*(D + -C*A.I*B).I\n    assert X_inv.blocks[1, 0] == -(D - C*A.I*B).I*C*A.I\n    assert X_inv.blocks[1, 1] == (D - C*A.I*B).I\n", "def test_blockcollapse_blockdiag():\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', m, m)\n    X = BlockDiagMatrix(A, B)\n\n    assert block_collapse(X).equals(BlockDiagMatrix(A, B))\n    assert block_collapse(X.I).equals(BlockDiagMatrix(A.I, B.I))\n    assert block_collapse(X.T).equals(BlockDiagMatrix(A.T, B.T))\n", "def test_bc_inverse_blockdiag():\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', m, m)\n    X = BlockDiagMatrix(A, B)\n\n    assert bc_inverse(X.inverse()) == BlockDiagMatrix(A.inverse(), B.inverse())\n", "def test_BlockMatrix_inverse():\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, m)\n    C = MatrixSymbol('C', m, n)\n    D = MatrixSymbol('D', m, m)\n    X = BlockMatrix([[A, B], [C, D]])\n    assert X.inverse() == BlockMatrix([\n        [(A - B*D.I*C).I, -A.I*B*(D - C*A.I*B).I],\n        [-(D - C*A.I*B).I*C*A.I, (D - C*A.I*B).I]])\n    assert isinstance(X.inverse(), Inverse)\n\n    # Test inverse of a block matrix with a zero block\n    Z = BlockMatrix([[A, ZeroMatrix(n, m)], [ZeroMatrix(m, n), D]])\n    assert Z.inverse() == BlockMatrix([[A.I, ZeroMatrix(n, m)], [ZeroMatrix(m, n), D.I]])\n\n    # Test inverse of a block matrix with an identity block\n    I = Identity(m)\n    W = BlockMatrix([[A, ZeroMatrix(n, m)], [ZeroMatrix(m, n), I]])\n    assert W.inverse() == BlockMatrix([[A.I, ZeroMatrix(n, m)], [ZeroMatrix(m, n), I]])\n"], "sample_16": ["def test_unquote(self):\n    self.assertEqual(unquote('something_0Aor_0Aother'), 'something\\nor\\nother')\n", "def test_unquote(self):\n    self.assertEqual(unquote('something_0Aor_0Aother'), 'something\\nor\\nother')\n    self.assertEqual(unquote('something'), 'something')\n    self.assertEqual(unquote('something_2F_or_5C_other'), 'something/_or\\\\_other')\n", "def test_reverse_field_path(self):\n    model = Event\n    path = \"location\"\n    reversed_model, reversed_path = reverse_field_path(model, path)\n    self.assertEqual(reversed_model, Location)\n    self.assertEqual(reversed_path, \"event\")\n\n    model = Location\n    path = \"event__guest\"\n    reversed_model, reversed_path = reverse_field_path(model, path)\n    self.assertEqual(reversed_model, EventGuide)\n    self.assertEqual(reversed_path, \"event__location\")\n", "def test_unquote(self):\n    self.assertEqual(unquote('something_0Aor_0Aother'), 'something\\nor\\nother')\n    self.assertEqual(unquote('nothing'), 'nothing')\n    self.assertEqual(unquote(None), None)\n", "def test_unquote(self):\n    self.assertEqual(unquote('something_0Aor_0Aother'), 'something\\nor\\nother')\n"], "sample_89": ["def test_add_marker_twice(item):\n    \"\"\"Ensure that adding a marker twice does not raise an error.\"\"\"\n    item.add_marker(\"skip\")\n    item.add_marker(\"skip\")\n    assert len(item.own_markers) == 1\n", "def test_node_repr_failure_no_traceback():\n    class MockNode(nodes.Node):\n            return \"Mock failure\"\n\n    node = MockNode.from_parent(None, name=\"Mock Node\")\n    excinfo = pytest._code.ExceptionInfo.from_current()\n    assert node.repr_failure(excinfo) == \"Mock failure\"\n", "def test_get_fslocation_from_item():\n    class FakeItem:\n            self.fspath = fspath\n            self.location = location\n\n    item = FakeItem(py.path.local(\"path/to/item.py\"))\n    assert nodes.get_fslocation_from_item(item) == (\"path/to/item.py\", -1)\n\n    item = FakeItem(py.path.local(\"path/to/item.py\"), location=(\"custom/location.py\", 10))\n    assert nodes.get_fslocation_from_item(item) == (\"custom/location.py\", 10)\n", "def test_add_marker():\n    node = nodes.Node.from_parent(None, name=\"node\")\n    marker = pytest.mark.skip(reason=\"skip reason\")\n    node.add_marker(marker)\n    assert node.own_markers == [marker.mark]\n    assert node.keywords[\"skip\"] == marker\n\n    node.add_marker(pytest.mark.xfail(reason=\"xfail reason\"), append=False)\n    assert node.own_markers == [pytest.mark.xfail(reason=\"xfail reason\").mark, marker.mark]\n", "def test_item_location(testdir):\n    \"\"\"\n    Test that the location of an item is correctly determined, including when it's a file.\n    \"\"\"\n    p = testdir.makepyfile(\n        \"\"\"\n            assert False\n    \"\"\"\n    )\n    items = testdir.getitems(p)\n    item = items[0]\n    assert item.location[0] == p.relto(testdir.tmpdir)\n    assert item.location[1] is None\n    assert item.location[2] == \"\"\n"], "sample_13": ["    def test_fields_limit(self):\n        query_string = 'a=1&b=2&c=3'\n        self.assertEqual(limited_parse_qsl(query_string, fields_limit=2), [('a', '1'), ('b', '2')])\n        with self.assertRaises(TooManyFieldsSent):\n            limited_parse_qsl(query_string, fields_limit=1)\n", "    def test_fields_limit(self):\n        query = 'a=1&b=2&c=3'\n        self.assertEqual(limited_parse_qsl(query, fields_limit=2), [('a', '1'), ('b', '2')])\n        with self.assertRaisesMessage(TooManyFieldsSent, 'The number of GET/POST parameters exceeded settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.'):\n            limited_parse_qsl(query, fields_limit=1)\n", "    def test_fields_limit(self):\n        qs = 'a=1&b=2&c=3'\n        self.assertEqual(limited_parse_qsl(qs, fields_limit=2), [('a', '1'), ('b', '2')])\n", "    def test_limited_parse_qsl(self):\n        qs = 'a=1&b=2&c=3'\n        self.assertEqual(limited_parse_qsl(qs), [('a', '1'), ('b', '2'), ('c', '3')])\n", "    def test_parse_qsl(self):\n        tests = (\n            ('a=1&b=2', [('a', '1'), ('b', '2')]),\n            ('a=1&a=2', [('a', '1'), ('a', '2')]),\n            ('a=1;a=2', [('a', '1'), ('a', '2')]),\n            ('a=1;a=2;a=3;a=4;a=5', [('a', '1'), ('a', '2'), ('a', '3'), ('a', '4'), ('a', '5')]),\n        )\n        for qsl, expected in tests:\n            with self.subTest(qsl=qsl):\n                self.assertEqual(limited_parse_qsl(qsl), expected)\n"], "sample_50": ["def test_update_cookie_empty_value(self):\n    \"\"\"\n    CookieStorage._update_cookie correctly deletes the cookie when given an empty value.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n    storage._update_cookie(\"\", response)\n    self.assertEqual(response.cookies[CookieStorage.cookie_name]['max-age'], 0)\n    self.assertEqual(response.cookies[CookieStorage.cookie_name]['expires'], 'Thu, 01 Jan 1970 00:00:00 GMT')\n", "def test_not_finished(self):\n    \"\"\"\n    A not_finished sentinel value is appended to the messages when some of them\n    cannot fit in the cookie, and the sentinel value is removed when retrieving\n    the messages.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n\n    # Generate enough messages to fill the cookie\n    for i in range(5):\n        msg = get_random_string(400)\n        storage.add(constants.INFO, msg)\n\n    storage.update(response)\n\n    # The not_finished sentinel value should be present in the cookie\n    data = storage._decode(response.cookies['messages'].value)\n    self.assertEqual(data[-1], CookieStorage.not_finished)\n\n    # But it should not be present when retrieving the messages\n    messages = list(storage)\n    self.assertNotIn(CookieStorage.not_finished, messages)\n", "def test_not_finished(self):\n    \"\"\"\n    A message containing the not_finished sentinel value is correctly handled.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n\n    messages = [Message(constants.INFO, 'Test message 1'), Message(constants.INFO, 'Test message 2')]\n    encoded_data = storage._encode(messages + [CookieStorage.not_finished])\n    set_cookie_data(storage, messages, encode_empty=True)\n\n    # The message contains what's expected.\n    self.assertEqual(list(storage), messages)\n    self.assertFalse(storage.used)\n\n    # Update the cookie with more messages.\n    storage.add(constants.INFO, 'Test message 3')\n    storage.update(response)\n\n    # All messages are now stored.\n    all_messages = messages + [Message(constants.INFO, 'Test message 3')]\n    self.assertEqual(self.stored_messages_count(storage, response), len(all_messages))\n", "def test_cookie_update_with_empty_value(self):\n    \"\"\"\n    Test that updating the cookie with an empty value results in a 'delete'\n    cookie being set.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n    storage.update(response)\n    self.assertEqual(response.cookies['messages'].value, '')\n    self.assertEqual(response.cookies['messages']['max-age'], 0)\n    self.assertEqual(\n        response.cookies['messages']['expires'],\n        'Thu, 01 Jan 1970 00:00:00 GMT',\n    )\n", "def test_not_finished_sentinel(self):\n    \"\"\"\n    The not_finished sentinel value is correctly added and removed from the\n    messages list when storing and retrieving messages.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n\n    # Add some messages to the storage\n    for i in range(5):\n        storage.add(constants.INFO, f'message {i}')\n\n    # Update the response with the messages\n    unstored_messages = storage.update(response)\n\n    # Check that the not_finished sentinel value was added\n    cookie_storing = self.stored_messages_count(storage, response)\n    self.assertEqual(cookie_storing, 4)  # Not all messages fit in the cookie\n\n    # Check that the not_finished sentinel value is removed when retrieving messages\n    request = self.get_request()\n    request.COOKIES = response.cookies\n    storage = self.storage_class(request)\n    messages = list(storage)\n    self.assertEqual(len(messages), 4)  # The not_finished sentinel value is removed\n"], "sample_92": ["def test_xfail_strict_with_multiple_conditions(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.xfail(\"True or False\", strict=True, reason=\"Expected failure\")\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*1 failed*\"])\n", "def test_xfail_strict_with_multiple_conditions(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail(strict=True, condition=\"True or False\")\n            assert True\n    \"\"\"\n    )\n    result = testdir.runpytest(p)\n    result.stdout.fnmatch_lines([\"*1 failed*\"])\n", "def test_xfail_mark_evaluation_order(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.xfail(True, reason='outer')\n        @pytest.mark.xfail(False, reason='inner')\n            assert 0\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-rx\")\n    result.stdout.fnmatch_lines([\"*XFAIL*test_func*\", \"*outer*\"])\n", "def test_xfail_mark_evaluated_before_test_execution(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n            pytest.mark.xfail(reason=\"Expected failure\")\n\n            assert 0\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-rxs\")\n    result.stdout.fnmatch_lines([\"*XFAIL*test_func*\", \"*reason:*Expected failure*\"])\n    result.stdout.fnmatch_lines([\"*FAIL*test_func2*\"])\n", "def test_xfail_strict_with_multiple_conditions(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.xfail(\"True or False\", reason=\"Expected failure\", strict=True)\n            assert True\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-rxX\")\n    result.stdout.fnmatch_lines([\"*XPASS(strict)*Expected failure*\"])\n    assert result.ret == 1\n"], "sample_135": ["def test_replace_issue_15262():\n    from sympy import Wild, symbols, sin\n    x = symbols('x')\n    p = Wild('p')\n    e = sin(x)\n    assert e.replace(sin(p), p) == x\n", "def test_replace_lambda():\n    x, y = symbols('x y')\n    e = (x**2 + x*y)\n    assert e.replace(lambda expr: expr.is_Mul, lambda expr: 2*expr) == 2*x**2 + 2*x*y\n", "def test_constructor_postprocessors():\n    from sympy.core.function import FunctionClass\n    class Postprocessor1:\n            return obj + 1\n\n    class Postprocessor2:\n            return obj * 2\n\n    Postprocessor3 = lambda x: x ** 2\n\n        return x - 1\n\n    Basic._constructor_postprocessor_mapping[Basic] = [Postprocessor1(), Postprocessor2()]\n    Basic._constructor_postprocessor_mapping[type(Basic)] = [postprocessor4]\n    FunctionClass._constructor_postprocessor_mapping[type(Basic)] = [Postprocessor3]\n\n    assert Basic._exec_constructor_postprocessors(1) == (((1 + 1) * 2) ** 2) - 1\n\n    del Basic._constructor_postprocessor_mapping[Basic]\n    del Basic._constructor_postprocessor_mapping[type(Basic)]\n    del FunctionClass._constructor_postprocessor_mapping[type(Basic)]\n", "def test_replace_is_commutative():\n    from sympy import Wild\n    x, y = symbols('x y')\n    e = (x**2 + x*y)\n    a = Wild('a', commutative=False)\n    b = Wild('b')\n    assert e.replace(a*b, b*a) == e\n    assert e.replace(b*a, a*b) == e\n    c = Wild('c', commutative=False)\n    assert e.replace(c*x, x*c) == e\n    assert e.replace(x*c, c*x) == e\n", "def test_compare_pretty():\n    from sympy import Symbol, sin, cos\n    x = Symbol('x')\n    assert Basic._compare_pretty(sin(x), cos(x)) == -1\n    assert Basic._compare_pretty(cos(x), sin(x)) == 1\n    assert Basic._compare_pretty(sin(x), sin(x)) == 0\n"], "sample_46": ["    def setUp(self):\n        self.reference = IndexColumns(\n            'table', ['first_column', 'second_column'], lambda column: column.upper(),\n            col_suffixes=['ASC', 'DESC'], opclasses=['varchar_ops', 'int4_ops']\n        )\n", "    def setUp(self):\n        self.reference = IndexColumns(\n            'table', ['first_column', 'second_column'], lambda column: column.upper(),\n            col_suffixes=('ASC', 'DESC'), opclasses=('varchar_pattern_ops', 'text_pattern_ops')\n        )\n", "    def setUp(self):\n        self.reference = IndexColumns(\n            'table', ['first_column', 'second_column'], lambda column: column.upper(),\n            col_suffixes=('ASC', 'DESC'), opclasses=('varchar_pattern_ops', 'int4_ops')\n        )\n", "    def setUp(self):\n        self.reference = IndexColumns(\n            'table', ['first_column', 'second_column'], lambda column: column.upper(),\n            opclasses=['varchar_ops', 'text_ops']\n        )\n", "    def setUp(self):\n        self.reference = IndexColumns(\n            'table', ['first_column', 'second_column'], lambda column: column.upper(),\n            col_suffixes=['ASC', 'DESC'], opclasses=['varchar_ops', 'text_ops']\n        )\n"], "sample_159": ["def test_prefix_properties():\n    m = PREFIXES['m']\n\n    assert m.name == 'milli'\n    assert m.abbrev == 'm'\n    assert m.scale_factor == S.One / 1000\n    assert m.base == 10\n\n    kibi_name = kibi.name\n    kibi_abbrev = kibi.abbrev\n    kibi_scale_factor = kibi.scale_factor\n    kibi_base = kibi.base\n\n    assert kibi_name == 'kibi'\n    assert kibi_abbrev == 'Y'\n    assert kibi_scale_factor == 2 ** 10\n    assert kibi_base == 2\n", "def test_prefixes():\n    assert PREFIXES['Y'].scale_factor == 10**24\n    assert PREFIXES['y'].scale_factor == 10**-24\n    assert BIN_PREFIXES['Ki'].scale_factor == 2**10\n    assert BIN_PREFIXES['Ei'].scale_factor == 2**60\n\n    for prefix in PREFIXES.values():\n        assert isinstance(prefix, Prefix)\n        assert prefix.base == 10\n\n    for prefix in BIN_PREFIXES.values():\n        assert isinstance(prefix, Prefix)\n        assert prefix.base == 2\n", "def test_prefix_creation():\n    p = Prefix('test', 't', 3)\n    assert p.name == 'test'\n    assert p.abbrev == 't'\n    assert p.scale_factor == 1000\n    assert p.base == 10\n    assert p._exponent == 3\n\n    p2 = Prefix('test2', 't2', -3)\n    assert p2.scale_factor == 1/1000\n    assert p2._exponent == -3\n\n    p3 = Prefix('test3', 't3', 3, base=2)\n    assert p3.scale_factor == 8\n    assert p3.base == 2\n    assert p3._exponent == 3\n\n    p4 = Prefix('test4', 't4', 3, latex_repr='\\\\text{t4}')\n    assert p4._latex_repr == '\\\\text{t4}'\n", "def test_prefix_properties():\n    k = PREFIXES['k']\n    assert k.name == 'kilo'\n    assert k.abbrev == 'k'\n    assert k.scale_factor == 1000\n    assert k.base == 10\n\n    kibi = BIN_PREFIXES['Ki']\n    assert kibi.name == 'kibi'\n    assert kibi.abbrev == 'Y'  # Note: This seems like a bug in the code, it should be 'Ki'\n    assert kibi.scale_factor == 1024\n    assert kibi.base == 2\n", "def test_prefix_properties():\n    p = Prefix('test', 't', 3, base=10)\n\n    assert p.name == 'test'\n    assert p.abbrev == 't'\n    assert p.scale_factor == 1000\n    assert p.base == 10\n\n    p = Prefix('test', 't', -2, base=2)\n\n    assert p.name == 'test'\n    assert p.abbrev == 't'\n    assert p.scale_factor == 1/4\n    assert p.base == 2\n\n    assert str(p) == 't'\n    assert repr(p) == \"Prefix('test', 't', -2, 2)\"\n"], "sample_24": ["def test_update_error_dict(self):\n    error_dict = {'field1': ['error1']}\n    validation_error = ValidationError({'field2': ['error2']})\n    updated_error_dict = validation_error.update_error_dict(error_dict)\n    self.assertEqual(updated_error_dict, {'field1': ['error1'], 'field2': ['error2']})\n\n    error_dict = {'field1': ['error1']}\n    validation_error = ValidationError(['error2'])\n    updated_error_dict = validation_error.update_error_dict(error_dict)\n    self.assertEqual(updated_error_dict, {'field1': ['error1'], '__all__': ['error2']})\n\n    error_dict = {}\n    validation_error = ValidationError({'field1': ['error1'], 'field2': ['error2']})\n    updated_error_dict = validation_error.update_error_dict(error_dict)\n    self.assertEqual(updated_error_dict, {'field1': ['error1'], 'field2': ['error2']})\n\n    error_dict = {}\n    validation_error = ValidationError(['error1', 'error2'])\n    updated_error_dict = validation_error.update_error_dict(error_dict)\n    self.assertEqual(updated_error_dict, {'__all__': ['error1', 'error2']})\n", "def test_update_error_dict(self):\n    error_dict = {}\n    exception = ValidationError('message')\n    updated_dict = exception.update_error_dict(error_dict)\n    self.assertEqual(updated_dict, {NON_FIELD_ERRORS: ['message']})\n\n    error_dict = {'field1': ['error1']}\n    exception = ValidationError('message')\n    updated_dict = exception.update_error_dict(error_dict)\n    self.assertEqual(updated_dict, {'field1': ['error1'], NON_FIELD_ERRORS: ['message']})\n\n    error_dict = {}\n    exception = ValidationError({'field1': 'message'})\n    updated_dict = exception.update_error_dict(error_dict)\n    self.assertEqual(updated_dict, {'field1': ['message']})\n\n    error_dict = {'field1': ['error1']}\n    exception = ValidationError({'field1': 'message', 'field2': 'error2'})\n    updated_dict = exception.update_error_dict(error_dict)\n    self.assertEqual(updated_dict, {'field1': ['error1', 'message'], 'field2': ['error2']})\n", "def test_update_error_dict(self):\n    error_dict = {'field1': ['error1']}\n    exception = ValidationError({'field2': ['error2']})\n    updated_dict = exception.update_error_dict(error_dict)\n    self.assertEqual(updated_dict, {'field1': ['error1'], 'field2': ['error2']})\n\n    error_dict = {'field1': ['error1']}\n    exception = ValidationError('error2')\n    updated_dict = exception.update_error_dict(error_dict)\n    self.assertEqual(updated_dict, {'field1': ['error1'], NON_FIELD_ERRORS: ['error2']})\n\n    error_dict = {}\n    exception = ValidationError({'field2': ['error2']})\n    updated_dict = exception.update_error_dict(error_dict)\n    self.assertEqual(updated_dict, {'field2': ['error2']})\n\n    error_dict = {}\n    exception = ValidationError('error2')\n    updated_dict = exception.update_error_dict(error_dict)\n    self.assertEqual(updated_dict, {NON_FIELD_ERRORS: ['error2']})\n", "def test_update_error_dict(self):\n    error_dict = {}\n    exception = ValidationError('message')\n    updated_dict = exception.update_error_dict(error_dict)\n    self.assertEqual(updated_dict, {'__all__': ['message']})\n\n    error_dict = {'field1': ['error1']}\n    exception = ValidationError('message')\n    updated_dict = exception.update_error_dict(error_dict)\n    self.assertEqual(updated_dict, {'field1': ['error1'], '__all__': ['message']})\n\n    error_dict = {}\n    exception = ValidationError({'field1': 'message'})\n    updated_dict = exception.update_error_dict(error_dict)\n    self.assertEqual(updated_dict, {'field1': ['message']})\n\n    error_dict = {'field2': ['error2']}\n    exception = ValidationError({'field1': 'message'})\n    updated_dict = exception.update_error_dict(error_dict)\n    self.assertEqual(updated_dict, {'field2': ['error2'], 'field1': ['message']})\n", "def test_update_error_dict(self):\n    error1 = ValidationError('message')\n    error2 = ValidationError({'field1': 'error1', 'field2': 'error2'})\n    error_dict = {}\n\n    error1.update_error_dict(error_dict)\n    self.assertEqual(error_dict, {'__all__': ['message']})\n\n    error_dict = {}\n    error2.update_error_dict(error_dict)\n    self.assertEqual(error_dict, {'field1': ['error1'], 'field2': ['error2']})\n\n    error_dict = {'field1': ['existing error']}\n    error2.update_error_dict(error_dict)\n    self.assertEqual(error_dict, {'field1': ['existing error', 'error1'], 'field2': ['error2']})\n"], "sample_147": ["def test_MatrixSymbol_kind():\n    A = MatrixSymbol('A', 2,2)\n    assert A.kind is MatrixKind(UndefinedKind)\n", "def test_Function_kind():\n    from sympy.core.function import Function\n    f = Function('f')\n    assert f(comm_x).kind is NumberKind\n    assert f(noncomm_x).kind is UndefinedKind\n", "def test_Function_kind():\n    from sympy.core.function import Function\n    f = Function('f')\n    assert f(comm_x).kind is UndefinedKind\n    assert f(noncomm_x).kind is UndefinedKind\n", "def test_Function_kind():\n    from sympy.core.function import Function\n    f = Function('f')\n    assert f(comm_x).kind is NumberKind\n    assert f(noncomm_x).kind is UndefinedKind\n", "def test_Function_kind():\n    from sympy.core.function import Function, WildFunction\n    f = Function('f')\n    wf = WildFunction('wf')\n    assert f(comm_x).kind is NumberKind\n    assert wf(comm_x).kind is NumberKind\n    assert f(noncomm_x).kind is UndefinedKind\n    assert wf(noncomm_x).kind is UndefinedKind\n"], "sample_57": ["def test_all_valid_with_non_form_errors(self):\n    \"\"\"all_valid() validates all forms, even when some have non-form errors.\"\"\"\n    data = {\n        \"choices-TOTAL_FORMS\": \"2\",\n        \"choices-INITIAL_FORMS\": \"0\",\n        \"choices-MIN_NUM_FORMS\": \"0\",\n        \"choices-0-choice\": \"Zero\",\n        \"choices-0-votes\": \"0\",\n        \"choices-1-choice\": \"One\",\n        \"choices-1-votes\": \"1\",\n    }\n    ChoiceFormSet = formset_factory(Choice)\n    formset1 = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n    formset2 = ChoiceFormSetWithNonFormError(data, auto_id=False, prefix=\"choices\")\n    self.assertIs(all_valid((formset1, formset2)), False)\n    expected_errors = [{}, {}]\n    self.assertEqual(formset1._errors, expected_errors)\n    self.assertEqual(formset2._non_form_errors, [\"non-form error\"])\n", "def test_add_fields(self):\n    \"\"\"The add_fields method is called for each form in the formset.\"\"\"\n\n    class BaseAddFieldsFormSet(BaseFormSet):\n            super().add_fields(form, index)\n            form.fields[\"custom\"] = CharField()\n\n    AddFieldsFormSet = formset_factory(Choice, formset=BaseAddFieldsFormSet)\n    formset = AddFieldsFormSet()\n    for form in formset:\n        self.assertIn(\"custom\", form.fields)\n\n    # If the form has a limited number of fields, add_fields() should not try to\n    # access the form's fields by index.\n    class BaseLimitedFormSet(BaseFormSet):\n            if index is not None:\n                form.fields[\"limited\"] = CharField()\n\n    LimitedFormSet = formset_factory(Choice, extra=2, formset=BaseLimitedFormSet)\n    formset = LimitedFormSet(initial=[{\"choice\": \"Calexico\", \"votes\": 100}])\n    self.assertIn(\"limited\", formset.forms[0].fields)\n    self.assertIn(\"limited\", formset.forms[1].fields)\n    self.assertNotIn(\"limited\", formset.empty_form.fields)\n", "def test_get_context(self):\n    \"\"\"get_context() returns a dictionary with the formset instance.\"\"\"\n    ChoiceFormSet = formset_factory(Choice)\n    formset = ChoiceFormSet()\n    context = formset.get_context()\n    self.assertEqual(context, {\"formset\": formset})\n", "    def test_default_prefix(self):\n        ChoiceFormSet = formset_factory(Choice)\n        formset = ChoiceFormSet()\n        self.assertEqual(formset.prefix, \"form\")\n", "def test_empty_management_form_template_name(self):\n    \"\"\"Management form template name is set to div.html if not specified.\"\"\"\n    class CustomFormSet(BaseFormSet):\n        template_name_div = \"custom_div.html\"\n\n    ChoiceFormSet = formset_factory(Choice, formset=CustomFormSet)\n    formset = ChoiceFormSet()\n    self.assertEqual(formset.management_form.template_name, \"custom_div.html\")\n\n    class CustomFormSet(BaseFormSet):\n        template_name_table = \"custom_table.html\"\n\n    ChoiceFormSet = formset_factory(Choice, formset=CustomFormSet)\n    formset = ChoiceFormSet()\n    self.assertEqual(formset.management_form.template_name, \"div.html\")\n"], "sample_103": ["def test_mutual_info_classif_sparse():\n    # Test that MI is correctly computed when X is sparse.\n    X = csr_matrix(np.array([[0, 1, 1],\n                             [1, 0, 1],\n                             [1, 1, 0]]))\n    y = np.array([0, 1, 2])\n\n    mi = mutual_info_classif(X, y, discrete_features='auto')\n    assert_array_equal(mi.shape, (X.shape[1],))\n\n    # Check that the MI values are non-negative.\n    assert_greater(mi, 0).all()\n", "def test_mutual_info_classif_sparse():\n    # Test that mutual_info_classif works with sparse matrices.\n    rng = check_random_state(0)\n    X = csr_matrix(rng.rand(100, 5))\n    y = rng.randint(0, 2, size=100)\n\n    mi_dense = mutual_info_classif(X.toarray(), y, random_state=0)\n    mi_sparse = mutual_info_classif(X, y, random_state=0)\n\n    assert_array_equal(mi_dense, mi_sparse)\n", "def test_mutual_info_classif_sparse():\n    # Test that sparse and dense inputs give the same results for\n    # mutual_info_classif.\n    X = np.array([[0, 0, 1],\n                  [1, 1, 0],\n                  [2, 0, 1],\n                  [2, 0, 1],\n                  [2, 0, 1]])\n    y = np.array([0, 1, 2, 2, 1])\n\n    X_sparse = csr_matrix(X)\n\n    mi_dense = mutual_info_classif(X, y, discrete_features='auto')\n    mi_sparse = mutual_info_classif(X_sparse, y, discrete_features='auto')\n\n    assert_array_equal(mi_dense, mi_sparse)\n", "def test_mutual_info_classif_sparse():\n    # Test that mutual_info_classif works with sparse matrices.\n    X = np.array([[0, 0, 1],\n                  [1, 0, 0],\n                  [0, 1, 0],\n                  [0, 1, 0],\n                  [0, 0, 1]])\n    y = np.array([0, 1, 2, 2, 1])\n\n    X_csr = csr_matrix(X)\n\n    mi_dense = mutual_info_classif(X, y, discrete_features=True)\n    mi_sparse = mutual_info_classif(X_csr, y, discrete_features=True)\n\n    assert_array_equal(mi_dense, mi_sparse)\n", "def test_mutual_info_classif_sparse_input():\n    # Test that sparse input is handled correctly for classification.\n    X = csr_matrix(np.array([[0, 1], [1, 0]]))\n    y = np.array([0, 1])\n\n    mi = mutual_info_classif(X, y, discrete_features='auto', random_state=0)\n    assert_array_equal(mi, np.array([1., 1.]))\n\n    # Test that discrete_features='auto' works as expected with sparse input.\n    X_sparse = csr_matrix(np.array([[0, 1], [1, 0]]))\n    mi_sparse = mutual_info_classif(X_sparse, y, discrete_features='auto',\n                                    random_state=0)\n    assert_array_equal(mi_sparse, np.array([1., 1.]))\n\n    # Test that discrete_features=True works as expected with sparse input.\n    mi_true = mutual_info_classif(X_sparse, y, discrete_features=True,\n                                  random_state=0)\n    assert_array_equal(mi_true, np.array([1., 1.]))\n\n    # Test that discrete_features=False raises an error with sparse input.\n    assert_raises(ValueError, mutual_info_classif, X_sparse, y,\n                  discrete_features=False)\n"], "sample_108": ["def test_sparse_kernel():\n    # Test sparse matrix support for kernels in libsvm\n    X_train = sparse.csr_matrix(np.array([[0, 1], [1, 0]]))\n    y_train = np.array([0, 1])\n\n    X_test = sparse.csr_matrix(np.array([[1, 1], [1, 1]]))\n\n    clf = svm.SVC(kernel='linear')\n    clf.fit(X_train, y_train)\n    assert clf.score(X_test, np.array([0, 0])) == 0.5\n\n    clf = svm.SVR(kernel='linear')\n    clf.fit(X_train, y_train)\n    assert clf.score(X_test, np.array([0, 0])) == -1.0\n", "def test_liblinear_multiclass_iris():\n    # Test liblinear with multiclass dataset (iris) and\n    # check that decision_function agrees with predict\n    X, y = iris.data, iris.target\n\n    clf = svm.LinearSVC(max_iter=1000).fit(X, y)\n    assert_array_equal(clf.predict(X), np.argmax(clf.decision_function(X), axis=1))\n\n    clf = svm.LinearSVC(multi_class='crammer_singer', max_iter=1000).fit(X, y)\n    assert_array_equal(clf.predict(X), np.argmax(clf.decision_function(X), axis=1))\n", "def test_sparse_support_vectors():\n    # Test that support vectors are returned as a sparse matrix when input is\n    # sparse and kernel is not 'precomputed'.\n    X = sparse.csr_matrix([[0, 1], [1, 0]])\n    y = [0, 1]\n    clf = svm.SVC(kernel='linear')\n    clf.fit(X, y)\n    assert sparse.issparse(clf.support_vectors_)\n", "def test_base_libsvm_sparse_input():\n    # Test that sparse input to BaseLibSVM is handled correctly.\n    X_sparse = sp.csr_matrix(X)\n    clf = svm.SVC(kernel='linear').fit(X_sparse, Y)\n    assert_array_equal(clf.predict(T), true_result)\n    assert clf.support_vectors_.shape[0] == 2\n\n    # check decision_function with sparse data\n    dec_func_sparse = clf.decision_function(sp.csr_matrix(T))\n    dec_func_dense = clf.decision_function(T)\n    assert_array_almost_equal(dec_func_sparse, dec_func_dense)\n", "def test_sparse_liblinear_auto_weighting():\n    # Test that sparse liblinear gives same result with and without auto weighting\n    X, y = make_classification(n_samples=100, n_features=10, random_state=0)\n\n    # create unbalanced classes\n    y[:90] = 0\n    y[90:] = 1\n\n    clf = svm.LinearSVC(class_weight='auto')\n    clf.fit(X, y)\n    auto_weighted_coef = clf.coef_\n\n    sample_weight = np.ones(100)\n    sample_weight[:90] = 10\n    sample_weight[90:] = 90\n    clf = svm.LinearSVC()\n    clf.fit(X, y, sample_weight=sample_weight)\n    manually_weighted_coef = clf.coef_\n\n    assert_array_almost_equal(auto_weighted_coef, manually_weighted_coef)\n"], "sample_22": ["def test_camel_case_to_spaces(self):\n    items = [\n        ('camelCase', 'camel case'),\n        ('CamelCase', 'camel case'),\n        ('CAMELCase', 'camel case'),\n        ('_camelCase', '_camel case'),\n        ('Camel_Case', 'camel case'),\n    ]\n    for value, output in items:\n        self.assertEqual(text.camel_case_to_spaces(value), output)\n        self.assertEqual(text.camel_case_to_spaces(lazystr(value)), output)\n", "def test_camel_case_to_spaces(self):\n    items = [\n        ('CamelCase', 'camel case'),\n        ('camelCase', 'camel case'),\n        ('Camel case', 'camel case'),\n        ('multiple  spaces in Value', 'multiple  spaces in value'),\n        ('_value with underscore', '_value with underscore'),\n    ]\n    for value, output in items:\n        self.assertEqual(text.camel_case_to_spaces(value), output)\n        self.assertEqual(text.camel_case_to_spaces(lazystr(value)), output)\n", "def test_camel_case_to_spaces(self):\n    self.assertEqual(text.camel_case_to_spaces(\"camelCase\"), \"camel case\")\n    self.assertEqual(text.camel_case_to_spaces(\"CamelCase\"), \"camel case\")\n    self.assertEqual(text.camel_case_to_spaces(\"CAMELCase\"), \"camel case\")\n    self.assertEqual(text.camel_case_to_spaces(\"camelCASE\"), \"camel case\")\n    self.assertEqual(text.camel_case_to_spaces(\"cameLcaSe\"), \"came lca se\")\n", "def test_camel_case_to_spaces(self):\n    items = [\n        ('camelCase', 'camel case'),\n        ('CamelCase', 'camel case'),\n        ('multipleWordsInCamelCase', 'multiple words in camel case'),\n    ]\n    for value, output in items:\n        self.assertEqual(text.camel_case_to_spaces(value), output)\n        self.assertEqual(text.camel_case_to_spaces(lazystr(value)), output)\n", "def test_camel_case_to_spaces(self):\n    items = [\n        ('camelCase', 'camel case'),\n        ('CamelCase', 'camel case'),\n        ('CAMELCase', 'camel case'),\n        ('_camelCase', '_camel case'),\n        ('camel Case', 'camel  case'),\n        (' camelCase', ' camel case'),\n        ('camelcase', 'camelcase'),\n    ]\n    for value, output in items:\n        self.assertEqual(text.camel_case_to_spaces(value), output)\n        self.assertEqual(text.camel_case_to_spaces(lazystr(value)), output)\n"], "sample_38": ["    def test_get_context(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = 'pbkdf2_sha256$100000$a6Pucb1qSFcD$WmCkn9Hqidj48NVe5x0FEM6A9YiOqQcl/83m2Z5udm0='\n        context = widget.get_context(name='password', value=value, attrs={})\n        self.assertEqual(context['summary'], [\n            {'label': _('algorithm'), 'value': 'pbkdf2_sha256'},\n            {'label': _('iterations'), 'value': 100000},\n            {'label': _('salt'), 'value': 'a6Pucb******'},\n            {'label': _('hash'), 'value': 'WmCkn9**************************************'},\n        ])\n", "    def test_render_with_unusable_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = '!'\n        html = widget.render(name='password', value=value, attrs={})\n        self.assertIn(_(\"No password set.\"), html)\n", "    def test_render_with_unusable_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = '!'\n        html = widget.render(name='password', value=value, attrs={})\n        self.assertIn(_(\"No password set.\"), html)\n        self.assertNotIn(_(\"Invalid password format or unknown hashing algorithm.\"), html)\n", "    def test_render_with_unusable_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = 'unusable'\n        html = widget.render(name='password', value=value, attrs={})\n        self.assertIn(_(\"No password set.\"), html)\n", "    def test_unique_email(self):\n        User.objects.create_user(username='testclient', email='testclient@example.com', password='password')\n        data = {\n            'username': 'testclient2',\n            'email': 'testclient@example.com',\n            'password1': 'test123',\n            'password2': 'test123',\n        }\n        form = UserCreationForm(data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form[\"email\"].errors, [str(form.error_messages['duplicate_email'])])\n"], "sample_34": ["    def test_invalid_app_label(self):\n        class Model(models.Model):\n            class Meta:\n                app_label = 'invalid.app.label'\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [\n            Error(\n                \"Model class check_framework.Model doesn't declare an explicit \"\n                \"app_label and isn't in an application in INSTALLED_APPS.\",\n                id='models.E023',\n            )\n        ])\n", "    def test_unique_error_message(self):\n        class Model(models.Model):\n            unique_field = models.CharField(max_length=10, unique=True)\n\n        model_instance = Model(unique_field='some_value')\n        error_message = model_instance.unique_error_message(Model, ('unique_field',))\n        self.assertEqual(\n            error_message,\n            Error(\n                'Model with this Unique field already exists.',\n                code='unique',\n                params={\n                    'model': model_instance,\n                    'model_class': Model,\n                    'model_name': 'Model',\n                    'unique_check': ('unique_field',),\n                },\n            ),\n        )\n", "    def test_field_name_length(self):\n        class Model(models.Model):\n            field = models.CharField(max_length=255)\n\n        field.name = 'a' * 256\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [\n            Error(\n                \"Field names must be less than 256 characters (got a\").\" * 256 + \").\",\n                id='models.E034',\n            )\n        ])\n", "    def test_field_name_clash_with_related_field_accessor(self):\n        class Parent(models.Model):\n            pass\n\n        class Child(Parent):\n            parent = models.CharField(max_length=10)\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [\n            Error(\n                \"The field 'parent' clashes with the field 'parent' from model \"\n                \"'check_framework.Parent'.\",\n                obj=Child._meta.get_field('parent'),\n                id='models.E006',\n            ),\n        ])\n", "    def test_check_indexes_with_expressions(self):\n        class Model(models.Model):\n            field = models.CharField(max_length=10)\n\n            class Meta:\n                indexes = [\n                    models.Index(F('field')),\n                ]\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [\n            Warning(\n                'db.backends.base.DatabaseWrapper does not support indexes on expressions.',\n                hint=(\n                    \"An index won't be created. Silence this warning if you don't care about it.\"\n                ),\n                obj=Model,\n                id='models.W043',\n            )\n        ])\n"], "sample_35": ["    def test_modelformset_factory(self):\n        ChoiceModel.objects.create(pk=1, name='a')\n        ChoiceModel.objects.create(pk=2, name='b')\n\n        FormSet = modelformset_factory(ChoiceModel, fields=('name',))\n        formset = FormSet()\n        self.assertEqual(len(formset.forms), 2)\n\n        # Check that the forms are initialized correctly\n        for i, form in enumerate(formset.forms):\n            self.assertEqual(form.initial['name'], ChoiceModel.objects.get(pk=i+1).name)\n\n        # Test saving the formset\n        data = {\n            'form-0-name': 'new_a',\n            'form-1-name': 'new_b',\n            'form-TOTAL_FORMS': '2',\n            'form-INITIAL_FORMS': '2',\n        }\n        formset = FormSet(data)\n        self.assertTrue(formset.is_valid())\n        formset.save()\n        self.assertEqual(ChoiceModel.objects.get(pk=1).name, 'new_a')\n        self.assertEqual(ChoiceModel.objects.get(pk=2).name, 'new_b')\n\n        # Test that extra forms can be added and saved\n        FormSet = modelformset_factory(ChoiceModel, fields=('name',), extra=1)\n        data = {\n            'form-0-name': 'new_a',\n            'form-1-name': 'new_b',\n            'form-2-name': 'c',\n            'form-TOTAL_FORMS': '3',\n            'form-INITIAL_FORMS': '2',\n        }\n        formset = FormSet(data)\n        self.assertTrue(formset.is_valid())\n        formset.save()\n        self.assertEqual(ChoiceModel.objects.count(), 3)\n\n        # Test that extra forms with no data don't cause any errors\n        data = {\n            'form-0-name': 'new_a',\n            'form-1-name': 'new_b',\n            'form-2-name': '',\n            'form-TOTAL_FORMS': '3',\n            'form-INITIAL_FORMS': '2',\n        }\n        formset = FormSet(data)\n        self.assertTrue(formset.is_valid())\n        formset.save()\n        self.assertEqual(ChoiceModel.objects.count(), 3)\n", "    def test_unique_fields(self):\n        # Create a model with unique fields\n        class TestModel(models.Model):\n            name = models.CharField(max_length=10, unique=True)\n            code = models.IntegerField(unique=True)\n\n        # Create a form for the model\n        class TestForm(forms.ModelForm):\n            class Meta:\n                model = TestModel\n                fields = ('name', 'code')\n\n        # Create two instances of the model with the same unique field values\n        instance1 = TestModel(name='test', code=1)\n        instance2 = TestModel(name='test', code=1)\n\n        # Create a formset for the model\n        TestFormSet = forms.modelformset_factory(TestModel, form=TestForm)\n\n        # Test that the formset raises a ValidationError when trying to save the two instances\n        with self.assertRaises(ValidationError):\n            TestFormSet([{'name': 'test', 'code': 1}, {'name': 'test', 'code': 1}])\n", "    def test_modelform_unique_error_messages(self):\n        # Create a model form with custom unique error messages.\n        from .models import UniqueErrorModel\n\n        class UniqueErrorForm(forms.ModelForm):\n            class Meta:\n                model = UniqueErrorModel\n                fields = ('name',)\n                error_messages = {\n                    'name': {\n                        'unique': 'Custom unique name error.',\n                    }\n                }\n\n        # Try to create two instances with the same name.\n        UniqueErrorModel.objects.create(name='a')\n        f = UniqueErrorForm({'name': 'a'})\n        self.assertFormErrors(['Custom unique name error.'], f.clean)\n", "    def test_modelchoiceiterator(self):\n        ChoiceModel.objects.create(pk=1, name='a')\n        ChoiceModel.objects.create(pk=2, name='b')\n        ChoiceModel.objects.create(pk=3, name='c')\n\n        f = ModelChoiceField(queryset=ChoiceModel.objects.all())\n        iterator = f.iterator(f)\n\n        # check that the iterator returns the correct number of choices\n        self.assertEqual(len(list(iterator)), 4)  # includes the empty choice\n\n        # check that the iterator returns the correct choices\n        self.assertEqual(\n            list(iterator),\n            [\n                ('', '---------'),\n                (1, 'a'),\n                (2, 'b'),\n                (3, 'c'),\n            ]\n        )\n", "    def test_modelform_unique_error_messages(self):\n        # Create a model with unique fields for the model form tests below.\n        class UniqueModel(models.Model):\n            name = models.CharField(max_length=10, unique=True)\n            age = models.IntegerField(unique=True)\n\n        # Create a model form with custom error messages for unique fields.\n        class UniqueModelForm(forms.ModelForm):\n            class Meta:\n                model = UniqueModel\n                fields = ('name', 'age')\n                error_messages = {\n                    'name': {'unique': 'Custom unique name error message.'},\n                    'age': {'unique': 'Custom unique age error message.'},\n                }\n\n        # Test that the custom error messages are used when the unique constraint is violated.\n        UniqueModel.objects.create(name='test', age=20)\n        form = UniqueModelForm({'name': 'test', 'age': 20})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['name'], ['Custom unique name error message.'])\n        self.assertEqual(form.errors['age'], ['Custom unique age error message.'])\n"], "sample_77": ["    def test_scale_identity(self):\n\n        s = Scale._identity()\n        x = pd.Series([1, 2, 3], name=\"x\")\n        assert_array_equal(s(x), x)\n", "    def x(self):\n        return pd.Series([1, 3, 9], name=\"x\", dtype=float)\n", "    def test_coordinate(self):\n\n        scale = Continuous()\n        axis = PseudoAxis(scale._setup(pd.Series([1, 2, 3]), Coordinate())._matplotlib_scale)\n        assert axis.get_scale() is not None\n", "    def test_smoke(self):\n\n        scale = Continuous()._setup(pd.Series([1, 2, 3]), Coordinate())\n        axis = PseudoAxis(scale._matplotlib_scale)\n        axis.set_view_interval(0, 10)\n        assert axis.get_view_interval() == (0, 10)\n", "    def test_scale_with_object_property(self):\n\n        x = pd.Series([\"a\", \"b\", \"c\"], name=\"x\")\n        s = Nominal()._setup(x, ObjectProperty())\n        assert_array_equal(s(x), [\"a\", \"b\", \"c\"])\n"], "sample_4": ["    def test_file_response_sets_content_type(self):\n        file_path = os.path.join(os.path.dirname(__file__), 'test.txt')\n        with open(file_path, 'w') as f:\n            f.write('Hello, world!')\n\n        response = FileResponse(open(file_path, 'rb'))\n        self.assertEqual(response['Content-Type'], 'text/plain')\n\n        os.remove(file_path)\n", "    def test_serialize_headers(self):\n        response = HttpResponse()\n        response['Content-Type'] = 'text/plain'\n        response['X-Custom-Header'] = 'value'\n        expected = b'Content-Type: text/plain\\r\\nX-Custom-Header: value\\r\\n'\n        self.assertEqual(response.serialize_headers(), expected)\n", "    def test_file_response(self):\n        response = FileResponse(open(__file__, 'rb'))\n        self.assertEqual(response.status_code, 200)\n        self.assertIsInstance(response.file_to_stream, file)\n        self.assertTrue(hasattr(response, 'set_headers'))\n", "    def test_file_response_sets_proper_headers(self):\n        response = FileResponse(open(__file__, 'rb'))\n        self.assertEqual(response['Content-Type'], 'text/x-python; charset=utf-8')\n        self.assertIn('Content-Length', response)\n", "    def test_response_with_json_content(self):\n        response = HttpResponse(content='{\"key\": \"value\"}', content_type='application/json')\n        self.assertEqual(response.json(), {'key': 'value'})\n"], "sample_66": ["    def test_not_callable_or_boolean(self):\n        class TestModelAdmin(ModelAdmin):\n            view_on_site = \"hello\"\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'view_on_site' must be a callable or a boolean value.\",\n            \"admin.E025\",\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n", "    def test_readonly_fields_is_not_list_or_tuple(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = \"name\"\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            id=\"admin.E034\",\n        )\n", "    def test_not_boolean_or_callable(self):\n        class TestModelAdmin(ModelAdmin):\n            view_on_site = \"string\"\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'view_on_site' must be a callable or a boolean value.\",\n            id=\"admin.E025\",\n        )\n", "    def test_readonly_fields_validation(self):\n        class TestModelAdmin(ModelAdmin):\n            readonly_fields = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'readonly_fields' must be a list or tuple.\",\n            \"admin.E034\",\n        )\n"], "sample_101": ["def test_pipeline_get_params():\n    # Test that get_params returns all the parameters of the pipeline\n    # and its components.\n    steps = [('a', LogisticRegression()), ('b', StandardScaler())]\n    pipeline = Pipeline(steps)\n    params = pipeline.get_params()\n\n    # Check that we have all the parameters of the pipeline\n    assert 'steps' in params\n    assert 'memory' in params\n\n    # Check that we have all the parameters of the components\n    assert 'a__C' in params\n    assert 'a__penalty' in params\n    assert 'a__max_iter' in params\n    assert 'b__with_mean' in params\n    assert 'b__with_std' in params\n\n    # Check that we don't have any other parameters\n    expected_params = ['steps', 'memory', 'a__C', 'a__class_weight',\n                       'a__dual', 'a__fit_intercept', 'a__intercept_scaling',\n                       'a__l1_ratio', 'a__max_iter', 'a__multi_class',\n                       'a__n_jobs', 'a__penalty', 'a__random_state',\n                       'a__solver', 'a__tol', 'a__verbose', 'a__warm_start',\n                       'b__copy', 'b__with_mean', 'b__with_std']\n    assert set(params.keys()) == set(expected_params)\n", "def test_pipeline_get_params_with_empty_string():\n    # Test that get_params works with empty string as parameter name\n    pipe = Pipeline([('clf', SVC())])\n    assert_raises_regex(ValueError, \"Invalid parameter name: ''\",\n                        pipe.get_params, [''])\n", "def test_pipeline_inverse_transform():\n    X = np.array([[1]])\n    mult2 = Mult(mult=2)\n    mult3 = Mult(mult=3)\n    pipeline = Pipeline([('m1', mult2), ('m2', mult3)])\n    Xt = pipeline.fit_transform(X)\n    assert_array_equal(X, pipeline.inverse_transform(Xt))\n", "def test_pipeline_transform_with_feature_names():\n    iris = load_iris()\n    X = iris.data\n    pca = PCA(n_components=2, svd_solver='full')\n    pipeline = Pipeline([('pca', pca)])\n\n    # Test transform with feature names\n    pipeline.fit(X)\n    X_trans = pipeline.transform(X)\n    assert_equal(X_trans.shape[1], 2)\n\n    # Test get_feature_names_out\n    feature_names_out = pipeline.get_feature_names_out()\n    assert_array_equal(feature_names_out, ['pca0', 'pca1'])\n", "def test_pipeline_memory_fit_params():\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    cachedir = mkdtemp()\n    try:\n        if LooseVersion(joblib_version) < LooseVersion('0.12'):\n            # Deal with change of API in joblib\n            memory = Memory(cachedir=cachedir, verbose=10)\n        else:\n            memory = Memory(location=cachedir, verbose=10)\n\n        # Test with Transformer + SVC\n        clf = SVC(gamma='scale', probability=True, random_state=0)\n        transf = DummyTransf()\n        pipe = Pipeline([('transf', clone(transf)), ('svc', clf)])\n        cached_pipe = Pipeline([('transf', transf), ('svc', clf)],\n                               memory=memory)\n\n        # Memoize the transformer at the first fit\n        cached_pipe.fit(X, y)\n        pipe.fit(X, y)\n\n        # Check that passing fit_params doesn't affect cache hit\n        cached_pipe.fit(X, y, svc__probability=False)\n\n        # Check that cached_pipe and pipe yield identical results\n        assert_array_equal(pipe.predict(X), cached_pipe.predict(X))\n        assert_array_equal(pipe.predict_proba(X), cached_pipe.predict_proba(X))\n        assert_array_equal(pipe.predict_log_proba(X),\n                           cached_pipe.predict_log_proba(X))\n        assert_array_equal(pipe.score(X, y), cached_pipe.score(X, y))\n        assert_array_equal(pipe.named_steps['transf'].means_,\n                           cached_pipe.named_steps['transf'].means_)\n    finally:\n        shutil.rmtree(cachedir)\n"], "sample_37": ["    def test_equal(self):\n        q1 = Q(a=1)\n        q2 = Q(a=1)\n        self.assertEqual(q1, q2)\n", "    def test_combine_and(self):\n        q1 = Q(x=1)\n        q2 = Q(y=2)\n        combined = q1 & q2\n        self.assertEqual(combined.children, [q1, q2])\n        self.assertEqual(combined.connector, Q.AND)\n", "    def test_deconstruct(self):\n        q = Q(a=1)\n        path, args, kwargs = q.deconstruct()\n        self.assertEqual(path, 'django.db.models.Q')\n        self.assertEqual(args, ())\n        self.assertEqual(kwargs, {'a': 1})\n", "    def test_getattr(self):\n        obj = Employee.objects.create(firstname='John', lastname='Doe')\n        deferred_attr = DeferredAttribute(Employee._meta.get_field('firstname'))\n        self.assertEqual(deferred_attr.__get__(obj), 'John')\n", "    def test_negation(self):\n        q = ~Q(a=1)\n        self.assertEqual(q.children, [~Q(a=1)])\n        self.assertEqual(~q, Q(a=1))\n"], "sample_104": ["def test_format_params_or_dict_items():\n    # Test that _format_params_or_dict_items handles long dictionaries and parameters\n    pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True)\n    long_dict = {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8}\n    expected = \"\"\"{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8}\"\"\"\n    assert pp.pformat(long_dict) == expected\n\n    long_param = {'param_a': 1, 'param_b': 2, 'param_c': 3, 'param_d': 4, 'param_e': 5}\n    expected = \"\"\"param_a=1, param_b=2, param_c=3, param_d=4, param_e=5\"\"\"\n    assert pp._format_params(list(long_param.items()), None, 0, 100, {}, 0) is None\n    # check output directly as it's written to a stream\n    import io\n    stream = io.StringIO()\n    pp._format_params(list(long_param.items()), stream, 0, 100, {}, 0)\n    assert stream.getvalue().strip() == expected\n", "def test_invalid_n_max_elements_to_show():\n    # Test that an invalid value for n_max_elements_to_show raises a ValueError\n\n    pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True)\n\n    with pytest.raises(ValueError):\n        pp.n_max_elements_to_show = -1\n\n    with pytest.raises(ValueError):\n        pp.n_max_elements_to_show = 0\n\n    with pytest.raises(ValueError):\n        pp.n_max_elements_to_show = 'a'\n\n    with pytest.raises(ValueError):\n        pp.n_max_elements_to_show = None\n", "def test__changed_params():\n    # Test that _changed_params correctly identifies the changed parameters\n\n    class MyEstimator(BaseEstimator):\n            self.param1 = param1\n            self.param2 = param2\n\n    estimator = MyEstimator(param1='new_value')\n    changed_params = _changed_params(estimator)\n    assert changed_params == {'param1': 'new_value'}\n\n    estimator = MyEstimator(param2='new_value')\n    changed_params = _changed_params(estimator)\n    assert changed_params == {'param2': 'new_value'}\n\n    estimator = MyEstimator()\n    changed_params = _changed_params(estimator)\n    assert changed_params == {}\n", "def test_indent_at_name():\n    # Test the indent_at_name parameter of _EstimatorPrettyPrinter\n    pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=False)\n    lr = LogisticRegression()\n    expected = \"\"\"LogisticRegression(\n  C=1.0, class_weight=None, dual=False, fit_intercept=True,\n  intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class='warn',\n  n_jobs=None, penalty='l2', random_state=None, solver='warn', tol=0.0001,\n  verbose=0, warm_start=False)\"\"\"\n    assert pp.pformat(lr) == expected\n\n    pp = _EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True)\n    expected = \"\"\"", "def test_params_with_array():\n    # Test that params with array are correctly rendered\n    lr = LogisticRegression(multi_class=np.array(['ovr', 'multinomial']))\n    expected = \"\"\""], "sample_156": ["def test_parser_mathematica_edge_cases():\n    parser = MathematicaParser()\n\n    chain = lambda expr: parser._from_tokens_to_fullformlist(parser._from_mathematica_to_tokens(expr))\n\n    # Test edge cases with multiple consecutive operators\n    assert chain(\"a*b*c*d*e\") == [\"Times\", \"a\", \"b\", \"c\", \"d\", \"e\"]\n    assert chain(\"a+b+c+d+e\") == [\"Plus\", \"a\", \"b\", \"c\", \"d\", \"e\"]\n    assert chain(\"a-b-c-d-e\") == [\"Plus\", \"a\", [\"Times\", \"-1\", \"b\"], [\"Times\", \"-1\", \"c\"], [\"Times\", \"-1\", \"d\"], [\"Times\", \"-1\", \"e\"]]\n    assert chain(\"a/b/c/d/e\") == [\"Times\", \"a\", [\"Power\", \"b\", \"-1\"], [\"Power\", \"c\", \"-1\"], [\"Power\", \"d\", \"-1\"], [\"Power\", \"e\", \"-1\"]]\n\n    # Test edge cases with parentheses and operators\n    assert chain(\"(a+b)*c\") == [\"Times\", [\"Plus\", \"a\", \"b\"], \"c\"]\n    assert chain(\"(a+b)*(c+d)\") == [\"Times\", [\"Plus\", \"a\", \"b\"], [\"Plus\", \"c\", \"d\"]]\n    assert chain(\"(a+b)/(c+d)\") == [\"Times\", [\"Plus\", \"a\", \"b\"], [\"Power\", [\"Plus\", \"c\", \"d\"], \"-1\"]]\n    assert chain(\"(a+b)-(c+d)\") == [\"Plus\", [\"Plus\", \"a\", \"b\"], [\"Times\", \"-1\", [\"Plus\", \"c\", \"d\"]]]\n\n    # Test edge cases with invalid input\n    raises(SyntaxError, lambda: chain(\"(\"))\n    raises(SyntaxError, lambda: chain(\")\"))\n    raises(SyntaxError, lambda: chain(\"[\"))\n    raises(SyntaxError, lambda: chain(\"]\"))\n    raises(SyntaxError, lambda: chain(\"{\"))\n    raises(SyntaxError, lambda: chain(\"}\"))\n    raises(SyntaxError, lambda: chain(\"(a\"))\n    raises(SyntaxError, lambda: chain(\"a)\"))\n    raises(SyntaxError, lambda: chain(\"[a\"))\n    raises(SyntaxError, lambda: chain(\"a]\"))\n    raises(SyntaxError, lambda: chain(\"{a\"))\n    raises(SyntaxError, lambda: chain(\"a}\"))\n", "def test_parser_mathematica_function():\n    parser = MathematicaParser()\n\n    convert_chain = lambda expr: parser._from_tokens_to_fullformlist(parser._from_mathematica_to_tokens(expr))\n\n    # Function with no arguments\n    assert convert_chain(\"f[]\") == [\"f\"]\n\n    # Function with one argument\n    assert convert_chain(\"f[x]\") == [\"f\", \"x\"]\n\n    # Function with multiple arguments\n    assert convert_chain(\"f[x, y, z]\") == [\"f\", \"x\", \"y\", \"z\"]\n\n    # Nested functions\n    assert convert_chain(\"f[g[x]]\") == [\"f\", [\"g\", \"x\"]]\n    assert convert_chain(\"f[g[x], h[y]]\") == [\"f\", [\"g\", \"x\"], [\"h\", \"y\"]]\n\n    # Function with a list as an argument\n    assert convert_chain(\"f[{x, y, z}]\") == [\"f\", [\"List\", \"x\", \"y\", \"z\"]]\n\n    # Function with a matrix as an argument\n    assert convert_chain(\"f[{{x, y}, {z, w}}]\") == [\"f\", [\"List\", [\"List\", \"x\", \"y\"], [\"List\", \"z\", \"w\"]]]\n", "def test_parser_mathematica_pattern():\n    parser = MathematicaParser()\n\n    convert_chain2 = lambda expr: parser._from_fullformlist_to_fullformsympy(parser._from_mathematica_to_tokens(expr))\n\n    x, y, a, b, c = symbols(\"x y a b c\")\n    Pattern, Blank, BlankSequence, BlankNullSequence = symbols(\"Pattern Blank BlankSequence BlankNullSequence\", cls=Function)\n\n    assert convert_chain2(\"f[x_]\") == Function('f')(Pattern(x, Blank()))\n    assert convert_chain2(\"f[x__]\") == Function('f')(Pattern(x, BlankSequence()))\n    assert convert_chain2(\"f[x___]\") == Function('f')(Pattern(x, BlankNullSequence()))\n    assert convert_chain2(\"f[a_,b_,c_]\") == Function('f')(Pattern(a, Blank()), Pattern(b, Blank()), Pattern(c, Blank()))\n    assert convert_chain2(\"f[x_,y__]\") == Function('f')(Pattern(x, Blank()), Pattern(y, BlankSequence()))\n    assert convert_chain2(\"f[x_,y___]\") == Function('f')(Pattern(x, Blank()), Pattern(y, BlankNullSequence()))\n    assert convert_chain2(\"f[x_:1]\") == Function('f')(Pattern(x, Blank(), 1))\n    assert convert_chain2(\"f[x_:1,y_:2]\") == Function('f')(Pattern(x, Blank(), 1), Pattern(y, Blank(), 2))\n", "def test_parser_mathematica_fullformlist_to_sympy():\n    parser = MathematicaParser()\n\n    convert_chain2 = lambda expr: parser._from_fullformlist_to_fullformsympy(expr)\n\n    Sin, Times, Plus, Power = symbols(\"Sin Times Plus Power\", cls=Function)\n\n    full_form_list1 = [\"Sin\", [\"Times\", \"x\", \"y\"]]\n    full_form_list2 = [\"Plus\", [\"Times\", \"x\", \"y\"], \"z\"]\n    full_form_list3 = [\"Sin\", [\"Times\", \"x\", [\"Plus\", \"y\", \"z\"], [\"Power\", \"w\", \"n\"]]]\n\n    assert convert_chain2(full_form_list1) == sin(x*y)\n    assert convert_chain2(full_form_list2) == x*y + z\n    assert convert_chain2(full_form_list3) == sin(x*(y + z)*w**n)\n\n    # Test some more functions\n    assert convert_chain2([\"Cos\", \"x\"]) == cos(x)\n    assert convert_chain2([\"Tan\", \"x\"]) == tan(x)\n    assert convert_chain2([\"Cot\", \"x\"]) == 1/tan(x)\n    assert convert_chain2([\"Sec\", \"x\"]) == 1/cos(x)\n    assert convert_chain2([\"Csc\", \"x\"]) == 1/sin(x)\n\n    # Test some more complex expressions\n    assert convert_chain2([\"Plus\", \"x\", [\"Times\", \"y\", \"z\"]]) == x + y*z\n    assert convert_chain2([\"Times\", \"x\", [\"Plus\", \"y\", \"z\"]]) == x*(y + z)\n    assert convert_chain2([\"Power\", \"x\", [\"Plus\", \"y\", \"z\"]]) == x**(y + z)\n", "def test_parser_mathematica_trig():\n    parser = MathematicaParser()\n    convert_chain = lambda expr: parser._from_tokens_to_fullformlist(parser._from_mathematica_to_tokens(expr))\n\n    assert convert_chain(\"Sin[x]\") == [\"Sin\", \"x\"]\n    assert convert_chain(\"Cos[x]\") == [\"Cos\", \"x\"]\n    assert convert_chain(\"Tan[x]\") == [\"Tan\", \"x\"]\n    assert convert_chain(\"Cot[x]\") == [\"Cot\", \"x\"]\n    assert convert_chain(\"Sec[x]\") == [\"Sec\", \"x\"]\n    assert convert_chain(\"Csc[x]\") == [\"Csc\", \"x\"]\n\n    assert convert_chain(\"ArcSin[x]\") == [\"ArcSin\", \"x\"]\n    assert convert_chain(\"ArcCos[x]\") == [\"ArcCos\", \"x\"]\n    assert convert_chain(\"ArcTan[x]\") == [\"ArcTan\", \"x\"]\n    assert convert_chain(\"ArcCot[x]\") == [\"ArcCot\", \"x\"]\n    assert convert_chain(\"ArcSec[x]\") == [\"ArcSec\", \"x\"]\n    assert convert_chain(\"ArcCsc[x]\") == [\"ArcCsc\", \"x\"]\n\n    assert convert_chain(\"Sinh[x]\") == [\"Sinh\", \"x\"]\n    assert convert_chain(\"Cosh[x]\") == [\"Cosh\", \"x\"]\n    assert convert_chain(\"Tanh[x]\") == [\"Tanh\", \"x\"]\n    assert convert_chain(\"Coth[x]\") == [\"Coth\", \"x\"]\n    assert convert_chain(\"Sech[x]\") == [\"Sech\", \"x\"]\n    assert convert_chain(\"Csch[x]\") == [\"Csch\", \"x\"]\n\n    assert convert_chain(\"ArcSinh[x]\") == [\"ArcSinh\", \"x\"]\n    assert convert_chain(\"ArcCosh[x]\") == [\"ArcCosh\", \"x\"]\n    assert convert_chain(\"ArcTanh[x]\") == [\"ArcTanh\", \"x\"]\n    assert convert_chain(\"ArcCoth[x]\") == [\"ArcCoth\", \"x\"]\n    assert convert_chain(\"ArcSech[x]\") == [\"ArcSech\", \"x\"]\n    assert convert_chain(\"ArcCsch[x]\") == [\"ArcCsch\", \"x\"]\n"], "sample_30": ["    def setUp(self):\n        self.client.force_login(self.superuser)\n", "    def test_custom_pk_inline(self):\n        parent = ParentModelWithCustomPk.objects.create(my_own_pk=\"foo\", name=\"Foo\")\n        ChildModel1.objects.create(my_own_pk=\"bar\", name=\"Bar\", parent=parent)\n        ChildModel2.objects.create(my_own_pk=\"baz\", name=\"Baz\", parent=parent)\n        response = self.client.get(reverse('admin:admin_inlines_parentmodelwithcustompk_change', args=('foo',)))\n        self.assertContains(response, '<input type=\"hidden\" name=\"childmodel1_set-0-my_own_pk\" value=\"bar\">')\n        self.assertContains(response, '<input type=\"hidden\" name=\"childmodel2_set-0-my_own_pk\" value=\"baz\">')\n", "    def test_validation(self):\n        class MyModelAdmin(ModelAdmin):\n            inlines = [InnerInline]\n\n        class MyTabularInline(TabularInline):\n            model = Inner\n\n        class MyStackedInline(StackedInline):\n            model = Inner\n\n        ma = MyModelAdmin(Holder, admin_site)\n        with self.assertRaisesMessage(\n            ValueError,\n            \"No value for 'forms' supplied\"\n        ):\n            ma._create_formsets(request=None, obj=None)\n\n        class MyModelAdmin(ModelAdmin):\n            inlines = [MyTabularInline, MyStackedInline]\n            inline_instances = ['bad']\n\n        ma = MyModelAdmin(Holder, admin_site)\n        with self.assertRaisesMessage(\n            TypeError,\n            \"'bad' is not a valid InlineModelAdmin instance\"\n        ):\n            ma.get_inline_formsets(None, None)\n\n        class MyModelAdmin(ModelAdmin):\n            inlines = ['bad']\n\n        ma = MyModelAdmin(Holder, admin_site)\n        with self.assertRaisesMessage(\n            ImportError,\n            \"Module 'bad' not found\"\n        ):\n            ma.get_inline_instances(request=None, obj=None)\n", "    def test_get_extra(self):\n        class ExtraInline(TabularInline):\n            model = Inner\n\n                return 5\n\n        modeladmin = ModelAdmin(Holder, admin_site)\n        modeladmin.inlines = [ExtraInline]\n        obj = Holder.objects.create()\n        url = reverse('admin:admin_inlines_holder_change', args=(obj.pk,))\n        request = self.factory.get(url)\n        request.user = self.superuser\n        response = modeladmin.changeform_view(request)\n        self.assertContains(response, 'id=\"id_inner_set-TOTAL_FORMS\" value=\"5\"')\n", "    def test_inline_formset_save_as_new(self):\n        modeladmin = ModelAdmin(Parent, admin_site)\n        modeladmin.inlines = [ChildInline]\n        obj = Parent.objects.create(name='Parent')\n        Child.objects.create(parent=obj, name='Child')\n        request = self.factory.post('/path/to/url/', data={'_saveasnew': 'Save as new'})\n        request.user = self.superuser\n        formsets, inline_instances = modeladmin._create_formsets(request, obj, change=True)\n        for formset in formsets:\n            for form in formset.forms:\n                self.assertTrue(form.save_as_new)\n"], "sample_132": ["def test_are_coplanar():\n    from sympy import Point3D, Line3D, Plane\n\n    # All points on a plane are coplanar\n    p1 = Point3D(0, 0, 0)\n    p2 = Point3D(1, 1, 1)\n    p3 = Point3D(2, 2, 2)\n    assert are_coplanar(p1, p2, p3)\n\n    # Not all points are on the same plane\n    p4 = Point3D(3, 3, 4)\n    assert not are_coplanar(p1, p2, p3, p4)\n\n    # Lines intersecting at a point are coplanar\n    l1 = Line3D(p1, p2)\n    l2 = Line3D(p2, p3)\n    assert are_coplanar(l1, l2)\n\n    # Lines not intersecting are not coplanar\n    l3 = Line3D(p1, p4)\n    assert not are_coplanar(l1, l3)\n\n    # A line and a plane containing that line are coplanar\n    pl = Plane(p1, p2, p3)\n    assert are_coplanar(l1, pl)\n\n    # A line and a plane not containing that line are not coplanar\n    pl2 = Plane(p1, p2, p4)\n    assert not are_coplanar(l1, pl2)\n\n    # Two planes intersecting in a line are not coplanar\n    pl3 = Plane(p1, p3, p4)\n    assert not are_coplanar(pl, pl3)\n", "def test_are_coplanar():\n    from sympy.geometry import Point3D, Line3D, Plane\n    a = Line3D(Point3D(5, 0, 0), Point3D(1, -1, 1))\n    b = Line3D(Point3D(0, -2, 0), Point3D(3, 1, 1))\n    c = Line3D(Point3D(0, -1, 0), Point3D(5, -1, 9))\n    assert are_coplanar(a, b, c) is False\n\n    p = Plane(Point3D(0, 0, 0), Point3D(1, 1, 1), Point3D(2, 2, 2))\n    assert are_coplanar(p, a) is False\n    assert are_coplanar(p, p) is True\n\n    a = Line3D(Point3D(0, 0, 0), Point3D(1, 1, 1))\n    b = Line3D(Point3D(0, 0, 0), Point3D(2, 2, 2))\n    assert are_coplanar(a, b) is True\n", "def test_are_coplanar():\n    from sympy import Point3D, Line3D, Plane\n    a = Line3D(Point3D(5, 0, 0), Point3D(1, -1, 1))\n    b = Line3D(Point3D(0, -2, 0), Point3D(3, 1, 1))\n    c = Line3D(Point3D(0, -1, 0), Point3D(5, -1, 9))\n    assert are_coplanar(a, b, c) is False\n\n    p1 = Plane(Point3D(0, 0, 0), Point3D(1, 1, 1), Point3D(1, 1, -1))\n    p2 = Plane(Point3D(1, 1, 1), Point3D(1, 1, -1), Point3D(1, -1, 0))\n    assert are_coplanar(p1, p2) is False\n\n    l1 = Line3D(Point3D(0, 0, 0), Point3D(1, 1, 1))\n    l2 = Line3D(Point3D(0, 0, 0), Point3D(1, 1, -1))\n    l3 = Line3D(Point3D(0, 0, 0), Point3D(1, -1, 0))\n    assert are_coplanar(l1, l2, l3) is True\n\n    p1 = Point3D(1, 1, 1)\n    p2 = Point3D(1, 1, -1)\n    p3 = Point3D(1, -1, 0)\n    p4 = Point3D(2, 2, 2)\n    assert are_coplanar(p1, p2, p3) is True\n    assert are_coplanar(p1, p2, p3, p4) is False\n", "def test_are_coplanar():\n    from sympy import Point3D, Line3D, Plane\n    a = Line3D(Point3D(5, 0, 0), Point3D(1, -1, 1))\n    b = Line3D(Point3D(0, -2, 0), Point3D(3, 1, 1))\n    c = Line3D(Point3D(0, -1, 0), Point3D(5, -1, 9))\n    assert are_coplanar(a, b, c) is False\n    p = Plane(Point3D(0, 0, 0), Point3D(1, 1, 1), Point3D(1, 1, 2))\n    assert are_coplanar(p, p.point) is True\n    assert are_coplanar(p, Point3D(0, 1, 0)) is False\n    assert are_coplanar(p, Line3D(p.point, p.normal_vector)) is False\n    assert are_coplanar(p, Line3D(p.point, p.point + p.normal_vector*2)) is False\n", "def test_are_coplanar():\n    from sympy.geometry import Point3D, Line3D, Plane\n    a = Line3D(Point3D(5, 0, 0), Point3D(1, -1, 1))\n    b = Line3D(Point3D(0, -2, 0), Point3D(3, 1, 1))\n    c = Line3D(Point3D(0, -1, 0), Point3D(5, -1, 9))\n    assert are_coplanar(a, b, c) is False\n    assert are_coplanar(a, a) is True\n    p = Plane(Point3D(0, 0, 0), Point3D(1, 1, 1), Point3D(2, 2, 2))\n    assert are_coplanar(p, p) is True\n    assert are_coplanar(a, p) is False\n    assert are_coplanar(p, a) is False\n    raises(ValueError, lambda: are_coplanar(Point3D(1, 2, 3)))\n"], "sample_48": ["def test_alter_model_options_with_proxy(self):\n    \"\"\"\n    AlterModelOptions should update the options on a proxy model.\n    \"\"\"\n    project_state = self.set_up_test_model(\"test_almoopwp\", options=True, proxy_model=True)\n    operation = migrations.AlterModelOptions(\"ProxyPony\", {\"verbose_name\": \"My little pwny\"})\n    new_state = project_state.clone()\n    operation.state_forwards(\"test_almoopwp\", new_state)\n    self.assertEqual(new_state.models[\"test_almoopwp\", \"proxypony\"].options[\"verbose_name\"], \"My little pwny\")\n", "def test_reduce_references_model(self):\n    class RefModelOperation(migrations.operations.base.Operation):\n            return name == 'mymodel'\n\n    operation = FieldOperation('MoDel', 'field', models.ForeignKey('Other', models.CASCADE))\n    reduced_operation = operation.reduce(RefModelOperation(), 'migrations')\n    self.assertEqual(reduced_operation, [operation])\n    reduced_operation = operation.reduce(RefModelOperation(), 'other_app')\n    self.assertIsNone(reduced_operation)\n", "def test_alter_field_with_unique_constraint(self):\n    app_label = 'test_alter_field_with_unique_constraint'\n    project_state = self.apply_operations(app_label, ProjectState(), operations=[\n        migrations.CreateModel('Pony', fields=[\n            ('id', models.AutoField(primary_key=True)),\n            ('pink', models.IntegerField(unique=True)),\n        ]),\n    ])\n    operation = migrations.AlterField(\n        'Pony',\n        'pink',\n        models.CharField(max_length=100, unique=True),\n    )\n    new_state = project_state.clone()\n    operation.state_forwards(app_label, new_state)\n    with connection.schema_editor() as editor:\n        operation.database_forwards(app_label, editor, project_state, new_state)\n    Pony = new_state.apps.get_model(app_label, 'Pony')\n    with self.assertRaises(IntegrityError), transaction.atomic():\n        Pony.objects.create(pink='same')\n    Pony.objects.create(pink='different').delete()\n    # Reversal.\n    with connection.schema_editor() as editor:\n        operation.database_backwards(app_label, editor, new_state, project_state)\n    Pony = project_state.apps.get_model(app_label, 'Pony')\n    with self.assertRaises(IntegrityError), transaction.atomic():\n        Pony.objects.create(pink=1)\n    Pony.objects.create(pink=2).delete()\n", "def test_rename_field_references_model(self):\n    operation = migrations.RenameField('MoDel', 'old_name', 'new_name')\n    # Model name match.\n    self.assertIs(operation.references_model('mOdEl', 'migrations'), True)\n    # Doesn't reference.\n    self.assertIs(operation.references_model('Whatever', 'migrations'), False)\n", "def test_rename_field_with_index(self):\n    project_state = self.apply_operations('test_rfwidx', ProjectState(), operations=[\n        migrations.CreateModel('Pony', fields=[\n            ('id', models.AutoField(primary_key=True)),\n            ('field', models.IntegerField(db_index=True)),\n        ]),\n    ])\n    new_state = project_state.clone()\n    operation = migrations.RenameField('Pony', 'field', 'renamed_field')\n    operation.state_forwards('test_rfwidx', new_state)\n    self.assertIn('renamed_field', new_state.models['test_rfwidx', 'pony'].fields)\n    self.assertNotIn('field', new_state.models['test_rfwidx', 'pony'].fields)\n    self.assertIndexExists('test_rfwidx_pony', ['field'])\n    with connection.schema_editor() as editor:\n        with self.assertNumQueries(2):\n            operation.database_forwards('test_rfwidx', editor, project_state, new_state)\n    self.assertIndexNotExists('test_rfwidx_pony', ['field'])\n    self.assertIndexExists(\n        'test_rfwidx_pony',\n        [connection.introspection.identifier_converter('renamed_field')],\n    )\n    with connection.schema_editor() as editor:\n        with self.assertNumQueries(2):\n            operation.database_backwards('test_rfwidx', editor, new_state, project_state)\n    self.assertIndexExists('test_rfwidx_pony', ['field'])\n    self.assertIndexNotExists(\n        'test_rfwidx_pony',\n        [connection.introspection.identifier_converter('renamed_field')],\n    )\n"], "sample_124": ["def test_acsch_series():\n    x = Symbol('x')\n    t6 = acsch(x).expansion_term(6, x)\n    assert t6 == -5*x**6/96\n    assert acsch(x).expansion_term(8, x, t6, 0) == -35*x**8/1024\n", "def test_issue_21472():\n    x = Symbol('x')\n    assert tanh(x).rewrite(csch) == 1/csch(x)*csch(2*x)\n    assert coth(x).rewrite(sech) == 1/sech(x)*sech(2*x)\n    assert sinh(x).rewrite(coth) == 2*coth(x/2)/(coth(x/2)**2 - 1)\n    assert cosh(x).rewrite(coth) == (coth(x/2)**2 + 1)/(coth(x/2)**2 - 1)\n", "def test_real_imag():\n    x = Symbol('x', real=True)\n    z = Symbol('z')\n    for func in [sinh, cosh, tanh, coth, sech, csch]:\n        assert func(x).as_real_imag()[1] == 0\n    for func in [sinh, tanh, csch]:\n        assert func(I*x).as_real_imag()[0] == 0\n    for func in [cosh, sech]:\n        assert func(I*x).as_real_imag()[1] == 0\n    for func in [coth]:\n        assert coth(I*x).as_real_imag()[0] == 0\n    for func in [sinh, cosh, tanh, coth, sech, csch]:\n        assert func(z).as_real_imag()[1].subs(z, x) == 0\n", "def test_asinh_real():\n    x = Symbol('x', real=True)\n    assert asinh(x).is_real\n", "def test_csch_expansion():\n    x, y = symbols('x,y')\n    assert csch(x+y).expand(trig=True) == (csch(x)*csch(y)*cosh(x)*cosh(y) - \n                                           csch(x)*csch(y)*sinh(x)*sinh(y))\n    assert csch(2*x).expand(trig=True) == csch(x)*csch(x)*cosh(x)*cosh(x) - csch(x)*csch(x)*sinh(x)*sinh(x)\n    assert csch(3*x).expand(trig=True).expand() == (csch(x)**3*cosh(x)**3 - 3*csch(x)**3*cosh(x)*sinh(x)**2)\n"], "sample_0": ["def test_split_datetime_widget_render(self):\n    widget = SplitDateTimeWidget()\n    output = widget.render('datetime_field', datetime.datetime(2022, 12, 25, 14, 30))\n    self.assertIn('value=\"2022-12-25\"', output)\n    self.assertIn('value=\"14:30\"', output)\n", "def test_use_required_attribute(self):\n    # With use_required_attribute.\n    w = forms.Select(choices=[('1', 'a'), ('2', 'b')])\n    self.assertTrue(w.use_required_attribute(None))\n    w = forms.Select(choices=[('', 'a'), ('2', 'b')])\n    self.assertFalse(w.use_required_attribute(None))\n\n    # Without use_required_attribute.\n    w = forms.CheckboxSelectMultiple(choices=[('1', 'a'), ('2', 'b')])\n    self.assertFalse(w.use_required_attribute(None))\n", "def test_format_value(self):\n    widget = SelectDateWidget()\n    self.assertEqual(widget.format_value(None), {'year': None, 'month': None, 'day': None})\n    self.assertEqual(widget.format_value('2017-12-25'), {'year': 2017, 'month': 12, 'day': 25})\n    self.assertEqual(widget.format_value(datetime.date(2017, 12, 25)), {'year': 2017, 'month': 12, 'day': 25})\n    self.assertEqual(widget.format_value(datetime.datetime(2017, 12, 25)), {'year': 2017, 'month': 12, 'day': 25})\n", "def test_format_value(self):\n    beatles = Band.objects.create(name='The Beatles', style='rock')\n    form = AlbumForm(initial={'band': beatles.pk})\n    value = form['band'].field.widget.format_value(beatles.pk)\n    self.assertEqual(value, str(beatles.pk))\n", "def test_widget_media_order_conflict_warning(self):\n    class WarningWidget(forms.Select):\n        class Media:\n            js = ('1.js', '2.js')\n\n    class TestForm(forms.Form):\n        field1 = forms.ChoiceField(widget=WarningWidget())\n        field2 = forms.ChoiceField(widget=forms.Select())\n\n    form = TestForm()\n    with self.assertWarnsMessage(MediaOrderConflictWarning, 'Detected duplicate Media files in an opposite order'):\n        str(form.media)\n"], "sample_129": ["def test_issue_15669():\n    from sympy.printing.latex import LatexPrinter\n    from sympy.core.expr import Expr\n    class MyExpr(Expr):\n            return \"test\"\n    e = MyExpr()\n    assert latex(e) == \"test\"\n", "def test_latex_MatPow():\n    from sympy import MatrixSymbol\n    A = MatrixSymbol(\"A\", 3, 3)\n    n = Symbol(\"n\")\n    assert latex(A**n) == r\"A^{n}\"\n", "def test_latex_Transpose():\n    from sympy.matrices import MatrixSymbol\n    A = MatrixSymbol(\"A\", 3, 3)\n    assert latex(Transpose(A)) == r\"A^T\"\n    assert latex(Transpose(A).T) == r\"\\left(A^T\\right)^T\"\n", "def test_RootedArray_printing():\n    # test cases for issue #18884\n    from sympy.tensor.array import (ImmutableDenseNDimArray, ImmutableSparseNDimArray,\n                                    MutableSparseNDimArray, MutableDenseNDimArray)\n    for ArrayType in (ImmutableDenseNDimArray, ImmutableSparseNDimArray, MutableDenseNDimArray, MutableSparseNDimArray):\n        M = ArrayType([[1 / x, y], [y, x - 1]])\n        assert latex(M) == '\\\\left[\\\\begin{matrix}\\\\frac{1}{x} & y\\\\\\\\y & x - 1\\\\end{matrix}\\\\right]'\n", "def test_latex_MatPow(self):\n    from sympy import MatrixSymbol\n    A = MatrixSymbol(\"A\", 3, 3)\n    assert latex(A**2) == r\"A^{2}\"\n    assert latex(A**0.5) == r\"A^{\\frac{1}{2}}\"\n    assert latex(A**-1) == r\"A^{-1}\"\n"], "sample_145": ["def test_tensor_array_element():\n    from sympy.tensor.array.expressions.array_expressions import ArrayElement\n    assert latex(ArrayElement(\"A\", (2, 1/(1-x), 0))) == \"{{A}_{2, \\\\frac{1}{1 - x}, 0}}\"\n", "def test_printing_latex_NDimArray():\n    x, y = symbols('x y')\n    m = ImmutableDenseNDimArray([x, 1, x/y])\n    assert latex(m) == r'\\begin{pmatrix} x \\\\ 1 \\\\ \\frac{x}{y} \\end{pmatrix}'\n    m = ImmutableSparseNDimArray([x, 0, x/y])\n    assert latex(m) == r'\\begin{pmatrix} x \\\\ 0 \\\\ \\frac{x}{y} \\end{pmatrix}'\n", "def test_printing_latex_array_expressions_with_indices():\n    from sympy.tensor.array.expressions.array_expressions import Array, ArrayElement\n    i, j, k = symbols('i j k')\n    A = Array(ArraySymbol(\"A\", 2, 3, 4), (i, j, k))\n    assert latex(A) == r\"A\"\n    assert latex(A[i, j, k]) == r\"A\"\n    assert latex(ArrayElement(\"A\", (i, j, k))) == r\"A_{i, j, k}\"\n", "def test_partial_derivative():\n    from sympy import Function\n    f = Function('f')\n    x, y, z = symbols('x y z')\n    pd = PartialDerivative(f(x, y, z), x, y, z)\n    assert latex(pd) == r\"\\frac{\\partial^{3}}{\\partial z\\partial y\\partial x} f{\\left(x, y, z \\right)}\"\n    pd = PartialDerivative(f(x, y, z), x, x, x)\n    assert latex(pd) == r\"\\frac{\\partial^{3}}{\\partial x^{3}} f{\\left(x, y, z \\right)}\"\n    pd = PartialDerivative(f(x, y, z), x, y)\n    assert latex(pd) == r\"\\frac{\\partial^{2}}{\\partial y\\partial x} f{\\left(x, y, z \\right)}\"\n", "def test_latex_decimal_separator_with_parenthesize_super():\n    x_star = Symbol('x^*')\n    assert latex(x_star**2, decimal_separator='comma', parenthesize_super=False) == r\"{x^{*}}^{2}\"\n    assert latex(x_star**2, decimal_separator='period', parenthesize_super=False) == r\"{x^{*}}^{2}\"\n"], "sample_53": ["def test_alter_model_options_populate(self):\n    \"\"\"#31378 - AlterModelOptions doesn't create duplicate model options.\"\"\"\n    before = [\n        ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n            ],\n            options={\n                \"permissions\": [(\"can_hire\", \"Can hire\")],\n                \"verbose_name\": \"Authi\",\n            },\n        ),\n    ]\n    after = [\n        ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n            ],\n            options={\n                \"verbose_name\": \"Authi\",\n            },\n        ),\n    ]\n    changes = self.get_changes(before, after)\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelOptions\"])\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        options={\"permissions\": []},\n    )\n", "def test_add_model_with_existing_table_name(self):\n    \"\"\"\n    Test adding a new model with an existing table name. This would happen in the\n    case where a model was deleted and then a new model with the same name was\n    created.\n    \"\"\"\n    author = ModelState(\n        \"otherapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n        ],\n    )\n    changes = self.get_changes([self.author_empty], [self.author_empty, author])\n    # Right number of migrations?\n    self.assertEqual(len(changes[\"otherapp\"]), 1)\n    # Right number of actions?\n    migration = changes[\"otherapp\"][0]\n    self.assertEqual(len(migration.operations), 1)\n    # Right actions order?\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"CreateModel\"])\n    self.assertOperationAttributes(changes, \"otherapp\", 0, 0, name=\"Author\")\n", "def test_alter_index_together_m2m(self):\n    before = [\n        ModelState(\n            \"app\",\n            \"Foo\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n            ],\n        ),\n        ModelState(\n            \"app\",\n            \"Bar\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n            ],\n            options={\n                \"unique_together\": {(\"foo\",)},\n                \"index_together\": {(\"foo\",)},\n            },\n        ),\n    ]\n    after = [\n        ModelState(\n            \"app\",\n            \"Foo\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n            ],\n        ),\n        ModelState(\n            \"app\",\n            \"Bar\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n            ],\n            options={\n                \"unique_together\": {(\"foo\",)},\n                \"index_together\": {(\"foo\",)},\n            },\n        ),\n        ModelState(\n            \"app\",\n            \"BarFoo\",\n            [\n                (\"foo\", models.ForeignKey(\"app.Foo\", models.CASCADE)),\n                (\"bar\", models.ForeignKey(\"app.Bar\", models.CASCADE)),\n            ],\n            options={\n                \"unique_together\": {(\"foo\", \"bar\")},\n                \"index_together\": {(\"foo\", \"bar\")},\n            },\n        ),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, \"app\", 1)\n    self.assertOperationTypes(changes, \"app\", 0, [\"CreateModel\", \"AddField\"])\n    self.assertOperationAttributes(changes, \"app\", 0, 0, name=\"BarFoo\")\n    self.assertOperationAttributes(\n        changes, \"app\", 0, 1, model_name=\"barfoo\", name=\"bar\"\n    )\n    self.assertOperationFieldAttributes(\n        changes, \"app\", 0, 1, related_model=\"app.Bar\"\n    )\n", "def test_alter_model_table_with_m2m(self):\n    \"\"\"Tests detection for changing db_table in model's options with m2m field.\"\"\"\n    before = [\n        ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n            ],\n            options={\"db_table\": \"author_one\"},\n        ),\n        ModelState(\n            \"testapp\",\n            \"Book\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\n                    \"authors\",\n                    models.ManyToManyField(\"testapp.Author\", through=\"testapp.AuthorBook\"),\n                ),\n            ],\n        ),\n        ModelState(\n            \"testapp\",\n            \"AuthorBook\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n                (\"book\", models.ForeignKey(\"testapp.Book\", models.CASCADE)),\n            ],\n        ),\n    ]\n    after = [\n        ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n            ],\n            options={\"db_table\": \"author_two\"},\n        ),\n        ModelState(\n            \"testapp\",\n            \"Book\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\n                    \"authors\",\n                    models.ManyToManyField(\"testapp.Author\", through=\"testapp.AuthorBook\"),\n                ),\n            ],\n        ),\n        ModelState(\n            \"testapp\",\n            \"AuthorBook\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n                (\"book\", models.ForeignKey(\"testapp.Book\", models.CASCADE)),\n            ],\n        ),\n    ]\n    changes = self.get_changes(before, after)\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelTable\"])\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, name=\"author\", table=\"author_two\"\n    )\n", "def test_alter_unique_together_add_and_remove(self):\n    initial_author = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n            (\"age\", models.IntegerField()),\n        ],\n        {\n            \"unique_together\": {(\"name\",)},\n        },\n    )\n    author_reversed_constraints = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n            (\"age\", models.IntegerField()),\n        ],\n        {\n            \"unique_together\": {(\"age\",), (\"name\", \"age\")},\n        },\n    )\n    changes = self.get_changes([initial_author], [author_reversed_constraints])\n\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(\n        changes,\n        \"testapp\",\n        0,\n        [\n            \"AlterUniqueTogether\",\n        ],\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        name=\"author\",\n        unique_together={(\"age\",), (\"name\", \"age\")},\n    )\n"], "sample_123": ["def test_issue_with_rational_powers():\n    from sympy import Rational\n    assert Rational(-1, 2)**Rational(3, 4) == -sqrt(2)/4\n    assert Rational(-1, 2)**Rational(7, 4) == -sqrt(8)/8\n", "def test_mpfr_precision():\n    assert Float('0.1', dps=100)._mpf_[3] == 332\n    assert Float('0.1', precision=100)._mpf_[3] == 100\n", "compilation error", "def test_Rational_from_mpq():\n    from gmpy2 import mpq\n    r = mpq(1, 2)\n    assert Rational(r) == Rational(1, 2)\n", "def test_Float_eq_mixed_precision():\n    x = Float(.12345678901234567890123456789, '')\n    y = Float(.12345678901234567890123456789, 15)\n    assert x != y\n    assert x != .12345678901234567890123456789\n"], "sample_143": ["def test_pretty_HadamardProduct():\n    from sympy.matrices.expressions import HadamardProduct, MatrixSymbol\n\n    A = MatrixSymbol(\"A\", 2, 2)\n    B = MatrixSymbol(\"B\", 2, 2)\n    expr = HadamardProduct(A, B)\n    ascii_str = \\", "def test_pretty_print_Quantity():\n    assert upretty(degree) == '\u00b0'\n    assert pretty(degree) == 'degree'\n", "compilation error", "def test_pretty_int_mod():\n    from sympy import Mod\n    assert pretty(Mod(3, 7)) == \"3 mod 7\"\n    assert upretty(Mod(3, 7)) == \"3 mod 7\"\n    assert pretty(Mod(3*x, 7)) == \"3*x mod 7\"\n    assert upretty(Mod(3*x, 7)) == \"3\u22c5x mod 7\"\n    assert pretty(Mod(x + 3, 7)) == \"(x + 3) mod 7\"\n    assert upretty(Mod(x + 3, 7)) == \"(x + 3) mod 7\"\n", "def test_issue_19167():\n    from sympy.tensor.tensor import TensorIndexType, tensor_indices\n    Lorentz = TensorIndexType('Lorentz', metric_name='metric')\n    i, j, k = tensor_indices('i j k', Lorentz)\n    metric = Lorentz.metric\n\n    expr = metric(i, j) * metric(i, k)\n    assert upretty(expr) == '\u03b7(i, j)*\u03b7(i, k)'\n    assert pretty(expr) == 'metric(i, j)*metric(i, k)'\n"], "sample_52": ["def test_reduce_references_model(self):\n    operation = FieldOperation(\n        \"MoDel\", \"field\", models.ForeignKey(\"Other\", models.CASCADE)\n    )\n    # Model name match.\n    self.assertIs(operation.reduce(operation, [\"mOdEl\"]), False)\n    # Referenced field.\n    self.assertIs(operation.reduce(operation, [\"oTher\"]), False)\n    # Doesn't reference.\n    self.assertIs(operation.reduce(operation, [\"Whatever\"]), True)\n", "def test_rename_model_options_change(self):\n    \"\"\"\n    Tests the RenameModel operation if model_options change.\n    \"\"\"\n    app_label = \"test_rmoptc\"\n    project_state = self.apply_operations(\n        app_label,\n        ProjectState(),\n        operations=[\n            migrations.CreateModel(\n                \"Rider\",\n                fields=[\n                    (\"id\", models.AutoField(primary_key=True)),\n                ],\n                options={\"verbose_name\": \"Rider (old)\"},\n            ),\n        ],\n    )\n    new_state = project_state.clone()\n    operation = migrations.RenameModel(\"Rider\", \"RiderNew\")\n    operation.state_forwards(app_label, new_state)\n    operation = migrations.AlterModelOptions(\"RiderNew\", {\"verbose_name\": \"Rider (new)\"})\n    operation.state_forwards(app_label, new_state)\n    RiderNew = new_state.apps.get_model(app_label, \"RiderNew\")\n    self.assertEqual(RiderNew._meta.verbose_name, \"Rider (new)\")\n", "def test_can_reduce_with_separate_database_and_state(self):\n    \"\"\"\n    The operations in the SeparateDatabaseAndState operation can be reduced.\n    \"\"\"\n    create_operation = migrations.CreateModel(\n        \"ILovePonies\",\n        [(\"id\", models.AutoField(primary_key=True))],\n    )\n    delete_operation = migrations.DeleteModel(\"ILovePonies\")\n    operation = migrations.SeparateDatabaseAndState(\n        state_operations=[create_operation, delete_operation],\n        database_operations=[create_operation, delete_operation],\n    )\n    reduced_operation = operation.reduce(operation, None)\n    self.assertIs(reduced_operation, [])\n", "def test_reduce_references_model(self):\n    operation = migrations.CreateModel(\n        \"Model\", fields=[(\"field\", models.BooleanField(default=False))]\n    )\n    related_operation = migrations.CreateModel(\n        \"Related\", fields=[(\"field\", models.ForeignKey(\"migrations.Model\", models.CASCADE))]\n    )\n    self.assertIs(\n        operation.reduce(related_operation, \"migrations\"), [related_operation, operation]\n    )\n    self.assertIs(\n        related_operation.reduce(operation, \"migrations\"), [operation, related_operation]\n    )\n", "def test_references_field_by_remote_field_parent_model(self):\n    operation = FieldOperation(\n        \"Model\", \"field\", models.ForeignKey(\"Other\", models.CASCADE)\n    )\n    # Remote field model match.\n    self.assertIs(\n        operation.references_field(\"Other\", \"field\", \"migrations\"), True\n    )\n    # Parent model of the remote field model match.\n    self.assertIs(\n        operation.references_field(\"OtherParent\", \"field\", \"migrations\"), True\n    )\n    # No match.\n    self.assertIs(\n        operation.references_field(\"OtherParentParent\", \"field\", \"migrations\"), False\n    )\n\n    operation = FieldOperation(\n        \"Model\",\n        \"field\",\n        models.ForeignKey(\"OtherChild\", models.CASCADE),\n    )\n    # Remote field model match.\n    self.assertIs(\n        operation.references_field(\"OtherChild\", \"field\", \"migrations\"), True\n    )\n    # Parent model of the remote field model match.\n    self.assertIs(\n        operation.references_field(\"Other\", \"field\", \"migrations\"), True\n    )\n    # Parent model of the parent model of the remote field model match.\n    self.assertIs(\n        operation.references_field(\"OtherParent\", \"field\", \"migrations\"), True\n    )\n    # No match.\n    self.assertIs(\n        operation.references_field(\"OtherParentParent\", \"field\", \"migrations\"), False\n    )\n"], "sample_126": ["def test_Float_division_for_wide_range():\n    # Ensure no overflow when dividing large numbers\n    a = Float('1e308')\n    assert 1 / a == Float('1e-308')\n    a = Float('1e-323')\n    assert 1 / a == Float('1e+323')\n    assert 1 / (-a) == Float('-1e+323')\n    assert (-1) / (-a) == Float('1e+323')\n", "def test_mpf_log():\n    assert mpf_log(10**100, 10, 100) == 100\n    assert mpf_log(10**-100, 10, 100) == -100\n    assert mpf_log(2**-100, 2, 100) == -100\n    assert mpf_log(2**-100, 2, 100) == -100\n    assert mpf_log(16**-100, 16, 100) == -100\n", "def test_Rational_precision():\n    # Make sure Rational inputs for keyword args work\n    assert Float('1.0', dps=Rational(15, 1))._prec == 53\n    assert Float('1.0', precision=Rational(15, 1))._prec == 15\n    assert type(Float('1.0', precision=Rational(15, 1))._prec) == int\n    assert sympify(srepr(Float('1.0', precision=15))) == Float('1.0', precision=15)\n", "def test_issue_16863():\n    assert Float(1.0)**(1/S.Half) == 1.0\n    assert Float(0.5)**(1/S.Half) == 0.7071067811865475\n    assert Float(-1.0)**(1/S.Half) == I\n    assert Float(-0.5)**(1/S.Half) == 0.7071067811865475*I\n    assert Float(1.0)**(1/S(3)) == 1.0\n    assert Float(0.5)**(1/S(3)) == 0.7937005259840998\n    assert Float(-1.0)**(1/S(3)) == -1.0\n    assert Float(-0.5)**(1/S(3)) == -0.7937005259840998\n", "def test_AlgebraicNumber_inverse():\n    from sympy import AlgebraicNumber\n    a = AlgebraicNumber(sqrt(2) + sqrt(3))\n    assert a**-1 == AlgebraicNumber(1/(sqrt(2) + sqrt(3)))\n    b = AlgebraicNumber(sqrt(2) - sqrt(3))\n    assert b**-1 == AlgebraicNumber(1/(sqrt(2) - sqrt(3)))\n"], "sample_125": ["def test_mpf_norm_minimal_precision():\n    # Verify mpf_norm's precision, particularly in cases where the minimal \n    # precision is used\n    assert mpf_norm((1, 0, 1, 0), 1) == mpf('0')._mpf_\n    assert Float._new((1, 0, 1, 0), 1)._mpf_ == mpf('0')._mpf_\n    assert mpf_norm((0, long(123), -20, 1), 1) == mpf('0')._mpf_\n    assert Float._new((0, long(123), -20, 1), 1)._mpf_ == mpf('0')._mpf_\n    assert mpf_norm((0, long(123), -20, 2), 1) == mpf('0.3')._mpf_\n    assert Float._new((0, long(123), -20, 2), 1)._mpf_ == mpf('0.3')._mpf_\n    assert mpf_norm((0, long(123), -20, 2), 2) == mpf('0.33')._mpf_\n    assert Float._new((0, long(123), -20, 2), 2)._mpf_ == mpf('0.33')._mpf_\n    assert mpf_norm((0, long(123), -20, 2), 3) == mpf('0.327')._mpf_\n    assert Float._new((0, long(123), -20, 2), 3)._mpf_ == mpf('0.327')._mpf_\n", "def test_mpf_log2():\n    x = _LOG2\n    y = mpmath.log(2)\n    assert abs(x - y) < 1e-15\n", "def test_mpfloat_comparison():\n    x = Float(mpmath.mpf(1.234))\n    y = Float(mpmath.mpf(1.235))\n    assert (x == y) is False\n    assert (x != y) is True\n    assert (x < y) is True\n    assert (x <= y) is True\n    assert (x > y) is False\n    assert (x >= y) is False\n", "def test_issue_18362():\n    assert (S.Pi**2).is_irrational is True\n    assert (S.Pi**2).is_rational is False\n    assert (S.Pi**2).is_algebraic is False\n    assert (S.Pi**2).is_transcendental is True\n    assert (S.E**2).is_irrational is True\n    assert (S.E**2).is_rational is False\n    assert (S.E**2).is_algebraic is False\n    assert (S.E**2).is_transcendental is True\n", "def test_mod_inverse_AlgebraicNumber():\n    a = AlgebraicNumber(sqrt(2) + 3)\n    assert mod_inverse(a, 11) == 7\n    assert mod_inverse(a, 13) == 11\n    assert mod_inverse(sqrt(2) + 3, 17) == 10\n    assert mod_inverse(sqrt(2) + 3, 19) == 16\n    assert mod_inverse(sqrt(2) + 3, 23) == 13\n"], "sample_127": ["def test_latex_radian():\n    expr1 = 90*radian\n    assert latex(expr1) == r\"90 \\detokenize {radian}\"\n    expr2 = x*radian\n    assert latex(expr2) == r\"x \\detokenize {radian}\"\n    expr3 = cos(x*radian + 90*radian)\n    assert latex(expr3) == r'\\cos{\\left (x \\detokenize {radian} + 90 \\detokenize {radian} \\right )}'\n", "def test_latex_radian():\n    expr1 = pi*radian\n    assert latex(expr1) == r\"\\pi \\detokenize {radian}\"\n    expr2 = x*radian\n    assert latex(expr2) == r\"x \\detokenize {radian}\"\n    expr3 = sin(x*radian + pi*radian)\n    assert latex(expr3) == r'\\sin{\\left (x \\detokenize {radian} + \\pi \\detokenize {radian} \\right )}'\n", "def test_latex_radian():\n    expr1 = 90*radian\n    assert latex(expr1) == r\"90 \\detokenize {radian}\"\n    expr2 = x*radian\n    assert latex(expr2) == r\"x \\detokenize {radian}\"\n    expr3 = cos(x*radian + 90*radian)\n    assert latex(expr3) == r'\\cos{\\left (x \\detokenize {radian} + 90 \\detokenize {radian} \\right )}'\n", "def test_latex_UnsignedInfinite():\n    from sympy.core.numbers import UnsignedInfinite\n    assert latex(UnsignedInfinite()) == r'\\infty'\n", "def test_latex_radian():\n    expr1 = 90*radian\n    assert latex(expr1) == r\"90 \\detokenize {radian}\"\n    expr2 = x*radian\n    assert latex(expr2) == r\"x \\detokenize {radian}\"\n    expr3 = cos(x*radian + 90*radian)\n    assert latex(expr3) == r'\\cos{\\left (x \\detokenize {radian} + 90 \\detokenize {radian} \\right )}'\n"]}, "raw_scored_generations": {"sample_1": [false, false, false, false, true], "sample_2": [true, true, true, true, true], "sample_4": [false, false, false, false, false], "sample_0": [true, true, true, true, true], "sample_3": [false, false, true, true, false], "sample_6": [false, false, false, false, false], "sample_5": [true, true, true, true, true], "sample_7": [false, false, true, false, false], "sample_9": [false, false, false, false, false], "sample_8": [false, false, true, true, false], "sample_10": [true, true, false, true, true], "sample_11": [true, true, true, true, true], "sample_13": [false, false, false, false, false], "sample_14": [true, true, true, true, true], "sample_12": [true, true, false, true, true], "sample_15": [true, true, true, true, true], "sample_16": [true, true, true, true, true], "sample_17": [true, false, true, false, false], "sample_18": [false, true, true, true, false], "sample_19": [false, false, true, false, true], "sample_21": [true, true, true, true, true], "sample_20": [false, false, true, false, false], "sample_23": [true, true, true, true, true], "sample_22": [true, true, true, true, true], "sample_24": [true, true, true, true, true], "sample_25": [true, true, false, false, true], "sample_28": [false, false, false, false, false], "sample_26": [false, true, true, false, false], "sample_27": [true, true, true, true, true], "sample_29": [false, false, false, true, true], "sample_31": [true, true, true, true, true], "sample_30": [false, true, true, false, true], "sample_32": [false, false, true, false, true], "sample_33": [true, true, true, true, true], "sample_34": [false, false, false, false, false], "sample_35": [false, false, false, false, false], "sample_36": [true, true, true, true, true], "sample_37": [true, false, true, false, false], "sample_39": [false, false, false, false, false], "sample_38": [false, true, true, false, false], "sample_40": [true, true, true, false, true], "sample_42": [true, true, true, true, true], "sample_41": [true, true, true, true, true], "sample_43": [true, false, true, true, true], "sample_45": [false, false, false, false, false], "sample_44": [false, true, true, true, true], "sample_46": [false, false, false, false, false], "sample_48": [true, false, true, true, true], "sample_47": [true, true, true, true, true], "sample_50": [true, true, true, true, true], "sample_49": [true, true, true, true, true], "sample_51": [true, true, true, true, true], "sample_52": [true, true, true, true, true], "sample_53": [true, true, true, true, true], "sample_55": [true, true, true, true, true], "sample_54": [true, true, true, true, true], "sample_56": [false, false, false, true, false], "sample_58": [true, true, true, true, true], "sample_57": [true, true, true, true, true], "sample_60": [true, true, true, true, true], "sample_59": [true, true, true, true, false], "sample_61": [true, true, true, true, true], "sample_63": [false, false, false, false, false], "sample_62": [false, false, false, false, false], "sample_64": [true, true, true, true, true], "sample_66": [true, true, true, true, true], "sample_65": [false, false, false, false, false], "sample_68": [true, true, true, true, true], "sample_67": [true, true, true, true, true], "sample_69": [false, false, true, false, false], "sample_70": [false, false, false, true, false], "sample_71": [true, false, true, true, true], "sample_72": [false, true, false, true, true], "sample_73": [false, false, true, false, false], "sample_74": [false, true, false, false, false], "sample_75": [false, false, true, false, false], "sample_76": [false, false, false, false, false], "sample_77": [false, false, true, true, false], "sample_78": [false, false, false, false, false], "sample_79": [false, false, false, false, false], "sample_80": [false, false, false, false, false], "sample_82": [false, false, false, false, true], "sample_81": [false, false, false, false, false], "sample_83": [false, false, false, false, false], "sample_84": [false, false, false, false, false], "sample_85": [false, false, false, false, false], "sample_86": [false, false, false, true, false], "sample_87": [false, false, false, false, true], "sample_88": [false, false, false, false, false], "sample_89": [false, false, false, false, false], "sample_90": [false, false, false, false, false], "sample_91": [false, false, false, false, false], "sample_92": [false, false, false, false, false], "sample_93": [false, false, false, false, false], "sample_95": [false, false, false, false, false], "sample_94": [false, false, false, false, false], "sample_96": [false, true, false, false, true], "sample_97": [true, false, true, false, true], "sample_98": [false, false, true, false, false], "sample_99": [false, true, false, false, false], "sample_101": [false, false, true, false, false], "sample_100": [false, false, true, true, true], "sample_102": [false, true, true, false, true], "sample_103": [false, false, false, true, false], "sample_104": [false, false, false, false, false], "sample_105": [true, true, true, true, true], "sample_107": [false, true, false, false, true], "sample_106": [true, true, false, false, true], "sample_108": [false, true, true, false, false], "sample_109": [false, false, false, true, false], "sample_110": [true, true, true, false, true], "sample_111": [false, false, false, false, false], "sample_112": [true, false, true, true, false], "sample_113": [true, false, false, false, true], "sample_114": [false, true, false, false, false], "sample_115": [false, false, true, true, true], "sample_116": [false, false, false, false, false], "sample_117": [true, true, true, true, false], "sample_118": [true, true, false, false, true], "sample_119": [true, true, true, false, false], "sample_120": [true, false, false, true, true], "sample_121": [false, false, false, true, false], "sample_122": [true, true, true, true, false], "sample_123": [false, false, false, false, false], "sample_124": [false, false, true, false, false], "sample_125": [false, false, false, false, false], "sample_126": [false, false, true, false, false], "sample_127": [true, true, true, false, true], "sample_128": [false, false, false, false, false], "sample_129": [false, true, false, true, false], "sample_130": [true, true, false, true, false], "sample_131": [true, true, true, false, true], "sample_132": [false, false, false, false, false], "sample_133": [false, false, false, false, false], "sample_134": [true, true, true, true, true], "sample_135": [true, false, false, true, false], "sample_136": [false, false, false, true, true], "sample_137": [false, false, false, false, false], "sample_138": [false, false, true, false, false], "sample_139": [true, true, false, false, true], "sample_140": [false, false, false, true, true], "sample_142": [false, false, false, false, false], "sample_141": [true, false, false, false, false], "sample_143": [false, false, false, false, false], "sample_144": [false, true, true, false, true], "sample_145": [true, false, false, false, true], "sample_146": [true, false, true, false, false], "sample_147": [false, false, true, false, false], "sample_148": [true, false, false, false, false], "sample_149": [false, false, false, false, false], "sample_150": [false, false, false, false, false], "sample_151": [false, false, false, false, false], "sample_152": [false, true, false, true, true], "sample_153": [false, false, true, false, true], "sample_154": [true, false, true, true, false], "sample_155": [false, false, false, false, false], "sample_156": [false, false, false, false, true], "sample_157": [true, true, true, false, false], "sample_158": [false, false, false, false, false], "sample_159": [false, false, false, false, false]}}