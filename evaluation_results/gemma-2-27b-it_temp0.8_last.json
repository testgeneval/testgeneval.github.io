{"raw_generations": {"sample_792": ["    def test_decision_function_shape(self):\n        # Check that the shape of the decision_function is correct\n        X = np.array([[1, 0, 0], [1, 1, 0], [0, 1, 0], [1, 0, 1]])\n        y = np.array([0, 0, 1, 1])\n        clf = GaussianNB()\n        clf.fit(X, y)\n\n        # decision_function should return a 1D array with shape (n_samples,)\n        decision_values = clf.decision_function(X)\n        assert decision_values.shape == (4,)\n", "compilation error", "def test_sample_weight_normalization():\n    # Test that sample weights are normalized correctly.\n    X = np.array([[0, 1], [1, 0], [0, 1]])\n    y = np.array([0, 1, 0])\n    sample_weight = np.array([0.5, 1, 1.5])\n\n    clf = MultinomialNB()\n    clf.fit(X, y, sample_weight=sample_weight)\n\n    # Verify that the class counts are normalized by the sum of weights\n    assert_array_almost_equal(clf.class_count_,\n                              np.array([1.5, 1]))\n\n    # Verify the feature counts are also weighted correctly\n    expected_feature_count = np.array([[0.5, 1],\n                                      [1, 1.5]])\n    assert_array_almost_equal(clf.feature_count_, expected_feature_count)\n", "compilation error", "    def test_bnb_empty_class(self):\n        # Test BernoulliNB when a class has no examples\n        X = np.array([[1, 0], [1, 1], [0, 1], [0, 0]])\n        y = np.array([0, 0, 1, 1])\n        clf = BernoulliNB()\n        clf.fit(X, y)\n        # Predict probabilities should not raise an error\n        clf.predict_proba(X)\n\n"], "sample_275": ["    def test_ticket_19102_prefetch_related(self):\n        with self.assertNumQueries(1):\n            Login.objects.filter(\n                pk=self.l1.pk\n            ).filter(\n                orgunit__name__isnull=False\n            ).order_by(\n                'description'\n            ).prefetch_related('orgunit').delete()\n        self.assertFalse(Login.objects.filter(pk=self.l1.pk).exists())\n        self.assertTrue(Login.objects.filter(pk=self.l2.pk).exists())\n\n", "    def test_large_deletes_with_proxy_models(self):\n        \"\"\"\n        Regression for #13309 -- if the number of objects > chunk size,\n        deletion with proxy model references also occurs\n        \"\"\"\n        for x in range(300):\n            FooFile.objects.create(\n                my_file=File.objects.create(\n                    path=f\"file_{x}\",\n                )\n            )\n        # attach a signal to make sure we will not fast-delete\n\n            pass\n        models.signals.post_delete.connect(noop, sender=FooFile)\n        FooFile.objects.all().delete()\n        models.signals.post_delete.disconnect(noop, sender=FooFile)\n        self.assertEqual(FooFile.objects.count(), 0)\n        self.assertEqual(File.objects.count(), 0)\n\n", "    def test_delete_with_related_m2m_through_cascade(self):\n        \"\"\"\n        Test cascading deletion when deleting an object with a ForeignKey to\n        a through model that has a ManyToMany relationship, and the ON_DELETE\n        setting for the m2m relation is CASCADE.\n\n        Refs #24862\n        \"\"\"\n        toy = Toy.objects.create(name='Blocks')\n        child = Child.objects.create(name='Alice')\n        child.toys.add(toy)\n        played_with = PlayedWith.objects.create(child=child, toy=toy)\n        PlayedWithNote.objects.create(played=played_with, note='fun with blocks')\n\n        child.delete()\n\n        self.assertEqual(Toy.objects.count(), 0)\n        self.assertEqual(PlayedWith.objects.count(), 0)\n        self.assertEqual(PlayedWithNote.objects.count(), 0)\n\n", "    def test_delete_cascade_manytomany_through(self):\n        book1 = Book.objects.create(pagecount=100)\n        book2 = Book.objects.create(pagecount=200)\n        toy = Toy.objects.create(name='Teddy')\n        PlayedWith.objects.create(child=juan, toy=toy, book=book1)\n        PlayedWith.objects.create(child=juan, toy=toy, book=book2)\n        book1.delete()\n        self.assertEqual(PlayedWith.objects.filter(book=book1).count(), 0)\n        self.assertEqual(PlayedWith.objects.filter(book=book2).count(), 1)\n", "compilation error"], "sample_408": ["    def test_no_operations_with_dependencies(self):\n        class Migration(migrations.Migration):\n            dependencies = [\n                (\"another_app\", \"0001_initial\"),\n            ]\n            operations = []\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertIs(migration.suggest_name().startswith(\"auto_\"), True)\n\n", "    def test_operation_with_custom_name(self):\n        class CustomOperation(migrations.Operation):\n                super().__init__(*args, **kwargs)\n                self.name = kwargs.get(\"name\", \"custom_name\")\n\n                return self.__class__.__name__, [], [], {\"name\": self.name}\n\n        class Migration(migrations.Migration):\n            operations = [CustomOperation(name=\"my_custom_name\")]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"my_custom_name\")\n", "    def test_operation_with_custom_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    \"Person\",\n                    fields=[],\n                    name=\"custom_person_name\",\n                ),\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"custom_person_name\")\n", "    def test_multiple_delete_models(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.DeleteModel(\"Person1\"),\n                migrations.DeleteModel(\"Person2\"),\n                migrations.DeleteModel(\"Person3\"),\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(\n            migration.suggest_name(), \"delete_person1_person2_and_person3\"\n        )\n", "    def test_operation_with_custom_name(self):\n        class CustomOperation(migrations.Operation):\n                super().__init__(*args, **kwargs)\n\n                return \"Custom Operation\"\n\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\"Person\", fields=[]),\n                CustomOperation(),\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"person_custom_operation\")\n"], "sample_1130": ["compilation error", "def test_point_a2pt_theorys_with_frame_reorientation():\n    q, q2 = dynamicsymbols('q q2')\n    qd, q2d = dynamicsymbols('q q2', 1)\n    qdd, q2dd = dynamicsymbols('q q2', 2)\n    N = ReferenceFrame('N')\n\n    A = ReferenceFrame('A')\n    A.set_ang_vel(N, qd * A.z)\n\n    O = Point('O')\n    P = O.locatenew('P', q * A.x)\n    O.set_vel(N, 0)\n    assert P.a2pt_theory(O, N, A) == (\n        -qd**2 * q * A.x + qdd * A.y\n    )\n", "compilation error", "compilation error", "compilation error"], "sample_766": ["    def test_dict_learning_online_partial_fit_n_iter(self):\n        n_components = 5\n        rng = np.random.RandomState(0)\n        V = rng.randn(n_components, n_features)\n        dico1 = MiniBatchDictionaryLearning(n_components, n_iter=10 * len(X)//2,\n                                           batch_size=len(X)//2,\n                                           alpha=1, shuffle=False,\n                                           dict_init=V, random_state=0).fit(X)\n        dico2 = MiniBatchDictionaryLearning(n_components, alpha=1,\n                                           n_iter=1, dict_init=V,\n                                           random_state=0)\n        for i in range(10):\n            dico2.partial_fit(X)\n\n        assert_array_almost_equal(dico1.components_, dico2.components_,\n                                  decimal=2)\n\n\n", "def test_dict_learning_online_max_iter():\n    n_components = 5\n    X = rng.randn(10, 8)\n    dico = dict_learning_online(X, n_components=n_components,\n                                alpha=1, n_iter=1, random_state=rng)\n    assert dico[1].shape == (n_components, 8)\n    with pytest.raises(ConvergenceWarning):\n        dico = dict_learning_online(X, n_components=n_components,\n                                    alpha=1, n_iter=1,\n                                    method='cd', random_state=rng)\n", "    def test_dict_learning_online_empty_input(self):\n        X = np.array([], dtype=np.float64).reshape(0, 8)\n        with pytest.raises(ValueError):\n            dict_learning_online(X, n_components=5, random_state=0)\n", "    def test_dict_learning_online_partial_fit_n_iter(self):\n        n_components = 12\n        rng = np.random.RandomState(0)\n        V = rng.randn(n_components, n_features)  # random init\n        V /= np.sum(V ** 2, axis=1)[:, np.newaxis]\n        dico1 = MiniBatchDictionaryLearning(n_components, n_iter=10 * len(X),\n                                           batch_size=1,\n                                           alpha=1, shuffle=False,\n                                           dict_init=V, random_state=0).fit(X)\n        dico2 = MiniBatchDictionaryLearning(n_components, alpha=1,\n                                           dict_init=V,\n                                           random_state=0).fit(X)\n\n        assert_array_almost_equal(dico1.components_, dico2.components_,\n                                  decimal=2)\n", "    def test_dict_learning_online_partial_fit_batch_size():\n        n_components = 12\n        rng = np.random.RandomState(0)\n        V = rng.randn(n_components, n_features)  # random init\n        V /= np.sum(V ** 2, axis=1)[:, np.newaxis]\n        dict1 = MiniBatchDictionaryLearning(n_components, n_iter=10 * len(X),\n                                            batch_size=len(X),\n                                            alpha=1, shuffle=False, dict_init=V,\n                                            random_state=0).fit(X)\n        dict2 = MiniBatchDictionaryLearning(n_components, alpha=1,\n                                            n_iter=1, dict_init=V,\n                                            random_state=0).partial_fit(X)\n        assert_array_almost_equal(dict1.components_, dict2.components_,\n                                  decimal=2)\n"], "sample_803": ["compilation error", "    def test_lrap_sample_weight_all_zero():\n        y_true = np.array([[0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]],\n                          dtype=np.bool)\n        y_score = np.array([[0.1, 0.4, 0.2, 0.3], [0.3, 0.1, 0.5, 0.1],\n                           [0.2, 0.3, 0.1, 0.4]])\n\n        sample_weight = np.array([0, 1, 0])\n        assert_almost_equal(\n            label_ranking_average_precision_score(y_true, y_score,\n                                                  sample_weight=sample_weight),\n            label_ranking_average_precision_score(y_true[1:2], y_score[1:2]))\n", "compilation error", "    def test_coverage_error_empty_labels():\n        # Test case with empty label set for some samples\n        y_true = np.array([[1, 0, 0], [0, 0, 0], [0, 1, 1]])\n        y_score = np.array([[0.8, 0.1, 0.1],\n                            [0.2, 0.3, 0.5],\n                            [0.6, 0.2, 0.2]])\n        assert_almost_equal(coverage_error(y_true, y_score), 3 / 3)\n", "def test_lrap_zero_labels():\n    y_true = np.array([[0, 0, 0], [0, 0, 1], [1, 0, 0]], dtype=np.bool)\n    y_score = np.array([[0.2, 0.5, 0.3], [0.1, 0.2, 0.7],\n                        [0.4, 0.3, 0.2]])\n    assert_almost_equal(label_ranking_average_precision_score(y_true, y_score),\n                        (1 + 1 / 2 + 1) / 3)\n"], "sample_257": ["    def test_key_transform_with_q_object(self):\n        qs = NullableJSONModel.objects.filter(\n            Q(value__baz__has_key=KeyTransform('a', 'value__baz')),\n        )\n        self.assertSequenceEqual(qs, [self.objs[7]])\n", "    def test_key_transform_subquery_filter(self):\n        subquery = (\n            NullableJSONModel.objects.filter(id=OuterRef('pk'))\n            .values('value__a')\n            .annotate(\n                key=KeyTransform('a', 'value'),\n            )\n        )\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                Q(key__isnull=False) & Q(key__iexact='B'),\n            ).annotate(\n                key=Subquery(subquery),\n            ).filter(key__isnull=False),\n            self.objs[3:4],\n        )\n", "    def test_nested_key_transform_on_fkey(self):\n        obj = NullableJSONModel.objects.create(value={'foo': {'bar': 1}})\n        RelatedModel.objects.create(json_data={'baz': 1}, nullablejsonmodel=obj)\n        self.assertSequenceEqual(\n            RelatedModel.objects.filter(\n                nullablejsonmodel__value__foo__bar__gt=F('json_data__baz'),\n            ),\n            [RelatedModel.objects.get(json_data__baz=1)],\n        )\n", "    def test_key_transform_with_lookup(self):\n        obj = NullableJSONModel.objects.create(value={'foo': {'bar': 'baz'}})\n        self.assertEqual(NullableJSONModel.objects.filter(\n            value__foo__bar__iexact='baz',\n        ).get(), obj)\n", "    def test_key_path_lookup(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__0__path='a'),\n            [self.objs[5]],\n        )\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__baz__path='c'),\n            [self.objs[7]],\n        )\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__d__1__path='f'),\n            [self.objs[4]],\n        )\n\n"], "sample_1206": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1161": ["def test_issue_21835():\n    n = Symbol('n')\n    assert str(Sum(n*x**(n-1), (n, 1, 3))) == 'Sum(n*x**(n - 1), (n, 1, 3))'\n", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1120": ["   def test_determinant_properties():\n        A = MatrixSymbol('A', n, n)\n        B = MatrixSymbol('B', n, n)\n        assert det(A*B).doit() == det(A)*det(B)\n        assert det(A.T) == det(A)\n        assert det(A.inv()) == 1/det(A)\n", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1153": ["compilation error", "def test_issue_14215():\n    from sympy import polar_lift, conjugate, unpolarify\n    z = polar_lift(1 + I)\n    assert conjugate(unpolarify(z)) == conjugate(1 + I)\n", "compilation error", "compilation error", "    def test_issue_15562():\n        A = Symbol('A', positive=True)\n        B = Symbol('B', positive=True)\n        assert Abs(A + B) == A + B\n"], "sample_610": ["def test_cftimeindex_shift_preserve_calendar():\n    dates = [\n        xr.cftime.DatetimeGregorian(2000, 1, 1),\n        xr.cftime.DatetimeGregorian(2000, 1, 2),\n        xr.cftime.DatetimeGregorian(2000, 1, 3),\n    ]\n    idx = xr.CFTimeIndex(dates, calendar=\"gregorian\")\n    shifted = idx.shift(1)\n    assert shifted.calendar == \"gregorian\"\n", "def test_empty_cftimeindex():\n    index = CFTimeIndex([])\n    assert index.calendar is None\n", "def test_cftimeindex_to_numpy_datetime64():\n    index = xr.cftime_range(\"2000-01-01\", periods=3)\n    result = index.to_numpy('datetime64[ns]')\n    expected = np.array(['2000-01-01T00:00:00.000000000', '2000-01-02T00:00:00.000000000', '2000-01-03T00:00:00.000000000'], dtype='datetime64[ns]')\n    np.testing.assert_array_equal(result, expected)\n", "compilation error", "def test_infer_freq_mixed_periods():\n    # Test infer_freq with mixed periods (e.g., daily and monthly)\n    dates = xr.cftime_range(\"2023-01-01\", periods=5, freq=\"D\")\n    dates = np.append(dates, xr.cftime_range(\"2023-02-01\", periods=2, freq=\"MS\"))\n    with pytest.raises(ValueError, match=\"Need a consistent frequency\"):\n        xr.infer_freq(dates)\n"], "sample_569": ["    def test_lmplot_robust_regression(self):\n\n        p = lm.lmplot(x=\"x\", y=\"y\", data=self.df, robust=True)\n        assert p.regplot_func == lm._RegressionPlotter._fit_robust\n", "    def test_lmplot_hue_order(self):\n\n        g = lm.lmplot(x=\"x\", y=\"y\", data=self.df, hue=\"h\", hue_order=[\"y\", \"x\"])\n        assert g.hue_names == [\"y\", \"x\"]\n", "    def test_regplot_palette(self):\n\n        g = lm.lmplot(x=\"x\", y=\"y\", hue=\"g\", data=self.df, palette=\"Set1\")\n        assert g.hue_kws == {\"color\": color_palette(\"Set1\", n_colors=3)}\n", "    def test_lmplot_palette(self):\n        palette = sns.color_palette(\"husl\", n_colors=2)\n        g = lm.lmplot(x=\"x\", y=\"y\", data=self.df, hue=\"h\", palette=palette)\n        assert g._hue_colors == palette\n", "    def test_lmplot_col_wrap(self):\n\n        g = lm.lmplot(x=\"x\", y=\"y\", data=self.df, col=\"u\", col_wrap=3)\n        assert g.axes.shape == (3, 2)\n"], "sample_1009": ["compilation error", "def test_vector_dot():\n    q1, q2, q3, q4 = dynamicsymbols('q1 q2 q3 q4')\n    N = ReferenceFrame('N')\n    A = N.orientnew('A', 'Axis', [q1, N.z])\n    B = A.orientnew('B', 'Axis', [q2, A.x])\n\n    v1 = q2*A.x + q3*N.y\n    v2 = q3*B.x + q1*N.z\n    assert v1.dot(v2) == q2*q3*A.x.dot(B.x) + q3*q1*N.y.dot(N.z)\n    assert v1 & v2 == q2*q3*cos(q1)*N.x.dot(B.x) + q3*q1*N.y.dot(N.z)\n    assert dot(v1, v2) == q2*q3*cos(q1)*N.x.dot(B.x) + q3*q1*N.y.dot(N.z)\n", "compilation error", "compilation error", "compilation error"], "sample_854": ["compilation error", "def test_svr_fit_intercept_zero():\n    # Test that SVR(kernel=\"linear\") works if fit_intercept is False\n    # and intercept_scaling is irrelevant\n\n    X = np.array([[1, 1], [2, 2], [3, 3]])\n    y = np.array([1, 2, 3])\n    svr = svm.SVR(kernel='linear', fit_intercept=False)\n    svr.fit(X, y)\n\n    # check that predict doesn't rely on intercept\n    assert_array_almost_equal(svr.predict(X), np.dot(X, svr.coef_.ravel()))\n\n    svr = svm.SVR(kernel='linear', fit_intercept=False,\n                  intercept_scaling=100)\n    svr.fit(X, y)\n    assert_array_almost_equal(svr.predict(X), np.dot(X, svr.coef_.ravel()))\n", "compilation error", "def test_gamma_auto_deprecated():\n    X, y = [[0.0, 1.2], [1.0, 1.3]], [0, 1]\n\n    with pytest.warns(FutureWarning, match=\"The behavior of gamma='auto' will change\"):\n        svm.SVC(kernel='rbf', gamma='auto').fit(X, y)\n", "compilation error"], "sample_178": ["    def test_all_valid_with_mixed_formsets(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = formset_factory(FavoriteDrinkForm)(data, auto_id=False, prefix='drinks')\n        self.assertIs(all_valid((formset1, formset2)), True)\n", "    def test_all_valid_with_formsets_having_different_prefixes(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='other_choices')\n        self.assertIs(all_valid((formset1, formset2)), True)\n", "    def test_empty_formset_non_form_errors(self):\n        class CustomFormset(BaseFormSet):\n                raise ValidationError(\"This is a non-form error\")\n\n        EmptyFormset = formset_factory(Choice, extra=0, formset=CustomFormset)\n        formset = EmptyFormset(\n            data={'form-INITIAL_FORMS': '0', 'form-TOTAL_FORMS': '0'},\n        )\n        self.assertEqual(list(formset.non_form_errors()), ['This is a non-form error'])\n", "    def test_all_valid_empty_formsets(self):\n        class EmptyForm(Form):\n            pass\n        EmptyFormSet = formset_factory(EmptyForm, extra=0)\n        formset1 = EmptyFormSet(data={}, prefix='empty1')\n        formset2 = EmptyFormSet(data={}, prefix='empty2')\n        self.assertIs(all_valid((formset1, formset2)), True)\n", "    def test_empty_formset_initial_data(self):\n        data = {}\n        formset = ArticleFormSet(data, initial=[{'title': 'Initial Title'}])\n        self.assertEqual(formset.initial_form_count(), 1)\n        self.assertEqual(formset.total_form_count(), 1)\n"], "sample_189": ["    def test_cache_middleware_with_empty_response(self):\n        request = self.factory.get('/view/')\n        view = cache_page(3)(lambda req: HttpResponse())\n        response = view(request)\n        self.assertIsNone(response.content)\n\n        # Second request should return the same empty response.\n        response = view(request)\n        self.assertIsNone(response.content)\n", "    def test_cache_key_with_csrf_token(self):\n        request = self.factory.get(self.path)\n        request.META['CSRF_COOKIE'] = 'test_cookie'\n        template = engines['django'].from_string(\"This is a test\")\n        response = TemplateResponse(request, template)\n        learn_cache_key(request, response)\n\n        self.assertEqual(\n            get_cache_key(request),\n            'views.decorators.cache.cache_page.settingsprefix.GET.'\n            '58a0a05c8a5620f813686ff969c26853.'\n            '8a5c18214381731a09d850d60b5f891d'\n        )\n", "    def test_missing_cache_alias(self):\n        with self.assertRaises(ImproperlyConfigured):\n            caches['nonexistent']\n", "    def test_invalid_cache_alias(self):\n        with self.assertRaises(ImproperlyConfigured):\n            caches['nonexistent']\n", "    def test_cache_with_invalid_timeout(self):\n        with self.assertRaises(ValueError):\n            cache_page(timeout='invalid')(hello_world_view)\n"], "sample_1172": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_564": ["def test_set_xlabel_vertical_axis(vertical_axis, expected_xaxis_label):\n    ax = plt.subplot(1, 1, 1, projection=\"3d\")\n    ax.view_init(vertical_axis=vertical_axis)\n    ax.set_xlabel(\"Test\")\n\n    assert ax.xaxis.get_label().get_text() == expected_xaxis_label\n", "    def test_mutating_input_arrays_x_y(fig_test, fig_ref):\n        ax1 = fig_test.add_subplot(111, projection='3d')\n        x = [0.0, 0.0, 0.0]\n        y = [1, 2, 3]\n        z = [0.0, 0.0, 0.0]\n        ax1.plot(x, y, z, 'o-')\n\n        # mutate x and y to get a nontrivial line\n        x[:] = [1, 2, 3]\n        y[:] = [1, 2, 3]\n\n        # draw the same plot without mutating x and y\n        ax2 = fig_ref.add_subplot(111, projection='3d')\n        x = [0.0, 0.0, 0.0]\n        y = [1, 2, 3]\n        z = [0.0, 0.0, 0.0]\n        ax2.plot(x, y, z, 'o-')\n", "compilation error", "compilation error", "def test_mutating_input_arrays_x_and_y(fig_test, fig_ref):\n    \"\"\"\n    Test to see if the `x` and `y` axes do not get mutated\n    after a call to `Axes3D.plot`\n\n    test cases came from GH#8990\n    \"\"\"\n    ax1 = fig_test.add_subplot(111, projection='3d')\n    x = [0.0, 0.0, 0.0]\n    y = [1, 2, 3]\n    z = [0.0, 0.0, 0.0]\n    ax1.plot(x, y, z, 'o-')\n\n    # mutate x and y to get a nontrivial line\n    x[:] = [1, 2, 3]\n    y[:] = [1, 2, 3]\n\n    # draw the same plot without mutating x and y\n    ax2 = fig_ref.add_subplot(111, projection='3d')\n    x = [0.0, 0.0, 0.0]\n    y = [1, 2, 3]\n    z = [0.0, 0.0, 0.0]\n    ax2.plot(x, y, z, 'o-')\n"], "sample_900": ["    def test_lbfgs_batch_size():\n        # Test that lbfgs works with batch size > 1\n\n        X = X_digits_binary[:50]\n        y = y_digits_binary[:50]\n\n        clf = MLPClassifier(solver='lbfgs', hidden_layer_sizes=5, batch_size=10)\n        with ignore_warnings(category=ConvergenceWarning):\n            clf.fit(X, y)\n", "    def test_warm_start_multilabel():\n        X, y = make_multilabel_classification(n_samples=50, random_state=0,\n                                             return_indicator=True)\n        clf = MLPClassifier(hidden_layer_sizes=10, solver='sgd',\n                            warm_start=True, random_state=0)\n        clf.fit(X, y)\n        clf.fit(X, y)\n        assert clf.n_iter_ > 1\n", "    def test_batch_size():\n        # Test batch_size.\n        X = [[3, 2], [1, 6], [5, 6], [-2, -4]]\n        y = [1, 1, 0, 0]\n        for batch_size in [1, 2, len(X)]:\n            mlp = MLPClassifier(solver='sgd', hidden_layer_sizes=2,\n                                batch_size=batch_size, max_iter=1000,\n                                random_state=1)\n            mlp.fit(X, y)\n            assert mlp.n_iter_ > 0\n", "compilation error", "compilation error"], "sample_73": ["    def test_manifest_strict_false_missing_file(self):\n        missing_file_name = 'cached/missing.css'\n        configured_storage = storage.staticfiles_storage\n        configured_storage.manifest_strict = False\n        self.assertNotIn(missing_file_name, configured_storage.hashed_files)\n\n        # File doesn't exist on disk\n        err_msg = \"The file '%s' could not be found with %r.\" % (missing_file_name, configured_storage._wrapped)\n        with self.assertRaisesMessage(ValueError, err_msg):\n            self.hashed_file_path(missing_file_name)\n\n", "    def test_manifest_file_order(self):\n        # Create two CSS files that reference each other.\n        file1_content = 'body { background: url(\"styles2.a1b2c3d4.css\"); }\\n'\n        file2_content = 'h1 { background: url(\"styles1.a1b2c3d4.css\"); }\\n'\n        with open(self._get_filename_path('styles1.css'), 'w') as f:\n            f.write(file1_content)\n        with open(self._get_filename_path('styles2.css'), 'w') as f:\n            f.write(file2_content)\n\n        # Add the files to STATICFILES_DIRS so they get collected\n        self.patched_settings = self.settings(\n            STATICFILES_DIRS=settings.STATICFILES_DIRS + [self._temp_dir],\n        )\n        self.patched_settings.enable()\n        self.addCleanup(self.patched_settings.disable)\n\n        # Collect static files\n        self.run_collectstatic()\n\n        # Assert that the files references have been updated correctly\n        with open(self.hashed_file_path('styles1.css'), 'r') as f:\n            content = f.read()\n            self.assertIn('url(\"styles2.a1b2c3d4.css\")', content)\n        with open(self.hashed_file_path('styles2.css'), 'r') as f:\n            content = f.read()\n            self.assertIn('url(\"styles1.a1b2c3d4.css\")', content)\n", "    def test_import_order_independence(self):\n        filename1 = 'test1.css'\n        filename2 = 'test2.css'\n        with open(self._get_filename_path(filename1), 'w') as f:\n            f.write('@import url(\"test2.css\");')\n        with open(self._get_filename_path(filename2), 'w') as f:\n            f.write('.test2 { color: blue; }')\n\n        self.run_collectstatic()\n\n        hashed_filename1 = self.hashed_file_path(filename1)\n        hashed_filename2 = self.hashed_file_path(filename2)\n\n        with open(os.path.join(settings.STATIC_ROOT, hashed_filename1), 'r') as f:\n            content1 = f.read()\n\n        with open(os.path.join(settings.STATIC_ROOT, hashed_filename2), 'r') as f:\n            content2 = f.read()\n\n        self.assertIn(hashed_filename2, content1)\n        self.assertNotIn(hashed_filename1, content2)\n", "    def test_manifest_update_on_file_change(self):\n        filename = 'test.css'\n        file_path = self._get_filename_path(filename)\n        with open(file_path, 'w') as f:\n            f.write('body {}')\n        self.run_collectstatic()\n        # Get original hash\n        original_hash = storage.staticfiles_storage.hashed_files.get(storage.staticfiles_storage.hash_key(filename))\n        self.assertIsNotNone(original_hash)\n\n        # Modify the file\n        with open(file_path, 'a') as f:\n            f.write('\\n h1 {}')\n        self.run_collectstatic()\n\n        # The hash should change\n        updated_hash = storage.staticfiles_storage.hashed_files.get(storage.staticfiles_storage.hash_key(filename))\n        self.assertIsNotNone(updated_hash)\n        self.assertNotEqual(original_hash, updated_hash)\n", "    def test_manifest_cache_invalidation(self):\n        \"\"\"\n        Changes to a file's content should correctly invalidate the cache entry.\n\n        This test verifies that the manifest and hashed_files are updated \n        when a static file is modified.\n        \"\"\"\n        # Create a temporary file to modify\n        temp_file = self._get_filename_path(\"temp.css\")\n\n        with open(temp_file, 'w') as f:\n            f.write('.class { color: red; }')\n\n        self.run_collectstatic()\n\n        original_hash = self.hashed_file_path(\"test/temp.css\")\n\n        # Modify the file content\n        with open(temp_file, 'w') as f:\n            f.write('.class { color: blue; }')\n\n        self.run_collectstatic()\n\n        new_hash = self.hashed_file_path(\"test/temp.css\")\n\n        self.assertNotEqual(original_hash, new_hash)\n\n"], "sample_1169": ["def test_substitute_dummies_noncommutative():\n    i, j, k = symbols('i j k', below_fermi=True)\n    a, b, c = symbols('a b c', above_fermi=True)\n    \n    expr = a * Commutator(B(i), Bd(j)) * c\n    expr_sub = expr.subs(i, j)\n\n    assert substitute_dummies(expr_sub) == substitute_dummies(expr.subs(i, j))\n", "compilation error", "compilation error", "compilation error", "def test_substitute_dummies_repeated_indices():\n    i, j, k = symbols('i j k', below_fermi=True, cls=Dummy)\n    a, b = symbols('a b', above_fermi=True, cls=Dummy)\n\n    expr = atv(i, i, a, b)*att(a, i, j, k)*att(b, k, i, j)\n    assert substitute_dummies(expr) == atv(i, i, a, b)*att(a, i, j, k)*att(b, k, i, j) \n"], "sample_304": ["    def test_file_extension_validator_empty_list(self):\n        with self.assertRaises(ValueError):\n            FileExtensionValidator([])\n", "    def test_decimal_validator_invalid_decimal_places(self):\n        with self.assertRaises(ValueError):\n            DecimalValidator(max_digits=2, decimal_places=-1)\n", "    def test_prohibit_null_characters_validator_message(self):\n        v = ProhibitNullCharactersValidator(message='Null characters are not allowed.', code='null_char')\n        with self.assertRaisesMessage(ValidationError, 'Null characters are not allowed.'):\n            v('\\x00something')\n", "    def test_validate_comma_separated_integer_list_edge_cases(self):\n        v = validate_comma_separated_integer_list\n        self.assertEqual(v(''), [])\n        self.assertEqual(v(','), [])\n        self.assertEqual(v(',,'), [])\n        self.assertEqual(v(' , , '), [])\n", "    def test_prohibit_null_characters_validator_message(self):\n        v = ProhibitNullCharactersValidator(message='Invalid input')\n        with self.assertRaisesMessage(ValidationError, 'Invalid input'):\n            v('\\x00something')\n"], "sample_423": ["    def test_migration_number_handling(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\"Person\", fields=[]),\n            ]\n\n        migration = Migration(\"0001_initial\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"person\")\n\n        migration = Migration(\"0002_person\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"person_2\")\n\n        migration = Migration(\"0003_delete_person\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"delete_person_2\")\n", "    def test_two_operations_delete_create(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.DeleteModel(\"Animal\"),\n                migrations.CreateModel(\"Person\", fields=[]),\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"delete_animal_person\")\n", "    def test_multiple_models_with_common_prefix(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\"PeopleManager\", fields=[]),\n                migrations.CreateModel(\"PeopleWorker\", fields=[]),\n            ]\n\n        migration = Migration(\"0001_initial\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"peoplemanager_peopleworker\")\n", "    def test_model_rename_with_index_together(self):\n        before = ModelState(\n            \"testapp\",\n            \"OldModelName\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"field1\", models.CharField(max_length=200)),\n                (\"field2\", models.CharField(max_length=200)),\n            ],\n            {\n                \"index_together\": {(\"field1\", \"field2\")}, \n            },\n        )\n        after = ModelState(\n            \"testapp\",\n            \"NewModelName\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"field1\", models.CharField(max_length=200)),\n                (\"field2\", models.CharField(max_length=200)),\n            ],\n            {\n                \"index_together\": {(\"field1\", \"field2\")},\n            },\n        )\n        changes = self.get_changes([before], [after])\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(\n            changes,\n            \"testapp\",\n            0,\n            [\"RenameModel\", \"AlterIndexTogether\"],\n        )\n        self.assertOperationAttributes(\n            changes,\n            \"testapp\",\n            0,\n            0,\n            old_name=\"OldModelName\",\n            new_name=\"NewModelName\",\n        )\n\n        self.assertOperationAttributes(\n            changes,\n            \"testapp\",\n            0,\n            1,\n            name=\"NewModelName\",\n            index_together={(\"field1\", \"field2\")},\n        )\n", "    def test_alter_index_together_model_creation(self):\n        initial_author = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n            ],\n        )\n        author_with_index_together = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n            ],\n            {\n                \"index_together\": {(\"name\",)},\n            },\n        )\n        changes = self.get_changes([initial_author], [author_with_index_together])\n\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(\n            changes,\n            \"testapp\",\n            0,\n            [\"AlterModelTable\", \"AlterIndexTogether\"],\n        )\n        self.assertOperationAttributes(\n            changes,\n            \"testapp\",\n            0,\n            1,\n            name=\"author\",\n            index_together={(\"name\",)},\n        )\n\n"], "sample_898": ["def test_multilabel_binarize_representation_invariance():\n    n_classes = 4\n    n_samples = 50\n    random_state = check_random_state(0)\n\n    _, y_true = make_multilabel_classification(n_features=1,\n                n_classes=n_classes, random_state=random_state,\n                n_samples=n_samples, allow_unlabeled=False)\n    _, y_pred = make_multilabel_classification(n_features=1,\n                n_classes=n_classes, random_state=random_state,\n                n_samples=n_samples, allow_unlabeled=False)\n\n\n\n    for name in MULTILABELS_METRICS:\n        metric = ALL_METRICS[name]\n\n        y_true_binarized = LabelBinarizer().fit_transform(y_true)\n        y_pred_binarized = LabelBinarizer().fit_transform(y_pred)\n        \n\n        score_one_hot = metric(y_true_binarized, y_pred_binarized)\n        score_indicator = metric(y_true, y_pred)\n\n        assert_almost_equal(score_one_hot, score_indicator,\n                           err_msg=\"{} failed representation invariance \"\n                           \"between one-hot and indicator formats\".format(name))\n", "def test_precision_recall_fscore_support_empty_labels():\n    y_true = np.array([0, 1, 2, 2, 2])\n    y_pred = np.array([1, 1, 2, 2, 0])\n\n    precision, recall, fscore, support = precision_recall_fscore_support(\n        y_true, y_pred, beta=1, average=None)\n\n    assert_array_equal(precision, [0, 0.5, 1])\n    assert_array_equal(recall, [0, 0.33333333, 1])\n    assert_array_equal(fscore, [0, 0.4, 1])\n    assert_array_equal(support, [1, 1, 3])\n", "def test_averaging_multilabel_all_ones():\n    y_true = np.ones((20, 3))\n    y_pred = np.ones((20, 3))\n    y_score = np.ones((20, 3))\n    y_true_binarize = y_true\n    y_pred_binarize = y_pred\n\n    for name in METRICS_WITH_AVERAGING:\n        yield (check_averaging, name, y_true, y_true_binarize,\n               y_pred, y_pred_binarize, y_score)\n", "compilation error", "compilation error"], "sample_996": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_681": ["def test_logging_no_crash_on_setup_teardown_failure(testdir: Testdir) -> None:\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n\n            logging.error(\"intentional error during setup\")\n            raise ValueError(\"setup error\")\n\n            logging.error(\"intentional error during teardown\")\n            raise ValueError(\"teardown error\")\n\n            assert False\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(failed=1)\n\n    # Check that pytest errors are the only ones visible\n    result.stdout.fnmatch_lines(\n        [\n            \"*setup_module*\",\n            \"*ValueError: setup error*\",\n            \"*test_something*\",\n            \"*FAILED*\",\n            \"*teardown_module*\",\n            \"*ValueError: teardown error*\",\n        ]\n    )\n", "def test_log_cli_level_after_ini(testdir):\n    testdir.makeini(\"\"\"\n        [pytest]\n        log_cli=true\n        log_cli_level=WARNING\n    \"\"\")\n    testdir.makepyfile(\"\"\"\n        import logging\n\n            logging.getLogger().info(\"This shouldn't be shown\")\n            logging.getLogger().warning(\"This should be shown\")\n    \"\"\")\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\"test_log_cli_level_after_ini.py*\", \"*WARNING*This should be shown*\", \"1 passed\"]\n    )\n", "def test_log_file_level_overrides_cli(testdir):\n    log_file = testdir.tmpdir.join(\"pytest.log\").strpath\n\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file={}\n        log_file_level = DEBUG\n        \"\"\".format(\n            log_file\n        )\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import pytest\n\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_file_handler.level == logging.DEBUG\n            logging.getLogger('catchlog').info(\"This log message will be shown\")\n            print('PASSED')\n    \"\"\"\n    )\n\n    result = testdir.runpytest(\"-s\", \"--log-level=WARNING\")\n\n    result.stdout.fnmatch_lines([\"test_log_file_level_overrides_cli.py PASSED\"])\n\n    assert result.ret == 0\n    assert os.path.isfile(log_file)\n    with open(log_file) as rfh:\n        contents = rfh.read()\n        assert \"This log message will be shown\" in contents\n\n\n", "    def test_log_file_level_overridden(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import logging\n\n                logging.getLogger().debug(\"This should not appear in the log file\")\n                logging.getLogger().info(\"This should appear in the log file\")\n                \n            \"\"\"\n        )\n        log_file = testdir.tmpdir.join(\"pytest.log\").strpath\n        testdir.makeini(\n            f\"\"\"\n            [pytest]\n            log_file={log_file}\n            log_file_level = INFO\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.no_fnmatch_line(\"*DEBUG*\")\n        assert result.ret == 0\n        assert os.path.isfile(log_file)\n        with open(log_file, encoding=\"utf-8\") as rfh:\n            contents = rfh.read()\n            assert \"This should appear in the log file\" in contents\n            assert \"This should not appear in the log file\" not in contents \n", "    def test_logging_emit_error_with_log_file(testdir: Testdir) -> None:\n        \"\"\"\n        An exception raised during emit() should fail the test,\n        even when logging to a file.\n        \"\"\"\n        testdir.makeini(\"[pytest]\\nlog_file = logfile.txt\")\n        testdir.makepyfile(\n            \"\"\"\n            import logging\n\n                logging.warning('oops', 'first', 2)\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.assert_outcomes(failed=1)\n        result.stdout.fnmatch_lines(\n            [\n                \"====* FAILURES *====\",\n                \"*not all arguments converted during string formatting*\",\n            ]\n        )\n\n"], "sample_20": ["    def test_write_empty_table(self, tmp_path):\n", "def test_fits_mixins_column_reordering(tmp_path):\n\n    filename = tmp_path / \"test_reordering.fits\"\n    # Define the original order\n    original_order = list(mixin_cols.keys())\n    # Reorder names for the table\n    rearranged_names = original_order[1:] + [original_order[0]]\n    t = QTable([mixin_cols[name] for name in rearranged_names], names=rearranged_names)\n\n    t.write(filename, format=\"fits\")\n    t2 = QTable.read(filename, format=\"fits\", astropy_native=True)\n\n    assert t.colnames == t2.colnames\n    assert t.colnames == rearranged_names\n", "    def test_write_and_read_fits_with_empty_table():\n        \"\"\"\n        Test writing an empty `Table` to a FITS file and reading it back.\n        \"\"\"\n        t = Table()\n        with pytest.raises(ValueError) as excinfo:\n            t.write(\"empty.fits\", format=\"fits\")\n        assert \"Table is empty\" in str(excinfo.value)\n", "    def test_fits_mixins_with_nonascii(self, tmp_path):\n        filename = tmp_path / \"test_simple.fits\"\n        t = Table([[1, 2], [\"a\", \"b\\xe9\"]], names=[\"a\", \"b\"])\n        t.write(filename, format=\"fits\")\n        t2 = Table.read(filename)\n        assert t2[\"b\"][1] == \"b\\xe9\"\n", "    def test_write_table_to_fits(tmp_path):\n        filename = tmp_path / \"table.fits\"\n        table_data = {\"col1\": [1, 2, 3], \"col2\": [4, 5, 6]}\n        table = Table(table_data)\n        table.write(filename, format=\"fits\", overwrite=True)\n        with fits.open(filename) as hdul:\n            assert hdul[1].data[\"col1\"].tolist() == [1, 2, 3]\n            assert hdul[1].data[\"col2\"].tolist() == [4, 5, 6]\n"], "sample_168": ["    def test_no_delete_when_objects_exist(self):\n        \"\"\"\n        Don't delete a stale content type if there are related objects.\n        \"\"\"\n        post = Post.objects.create(title='post', content_type=self.content_type)\n        with captured_stdout() as stdout:\n            call_command('remove_stale_contenttypes', interactive=False, verbosity=2)\n        self.assertEqual(Post.objects.count(), 1)\n        output = stdout.getvalue()\n        self.assertIn('- Content type for contenttypes_tests.Fake', output)\n        self.assertIn('- 1 contenttypes_tests.Post object(s)', output)\n        self.assertNotIn('Deleting stale content type', output)\n", "    def test_include_stale_apps_flag(self):\n        \"\"\"\n        The --include-stale-apps flag includes content types from removed apps.\n        \"\"\"\n        ContentType.objects.create(app_label='removed_app', model='Fake')\n        with mock.patch('builtins.input', return_value='yes'), captured_stdout() as stdout:\n            call_command('remove_stale_contenttypes', include_stale_apps=True, verbosity=2)\n        self.assertIn(\"Deleting stale content type 'removed_app | Fake'\", stdout.getvalue())\n        self.assertEqual(ContentType.objects.count(), self.before_count)\n\n", "    def test_contenttypes_removed_in_installed_apps_with_models(self):\n        ContentType.objects.create(app_label='empty_models', model='Fake 1')\n        ContentType.objects.create(app_label='no_models', model='Fake 2')\n        with mock.patch('builtins.input', return_value='yes'), captured_stdout() as stdout:\n            call_command('remove_stale_contenttypes', verbosity=2)\n        self.assertIn(\n            \"Deleting stale content type 'empty_models | Fake 1'\",\n            stdout.getvalue(),\n        )\n        self.assertIn(\n            \"Deleting stale content type 'no_models | Fake 2'\",\n            stdout.getvalue(),\n        )\n        self.assertEqual(ContentType.objects.count(), self.before_count)\n\n", "    def test_include_stale_apps_true(self):\n        \"\"\"\n        include_stale_apps flag deletes stale content types even if the app is not\n        in INSTALLED_APPS.\n        \"\"\"\n        ContentType.objects.create(app_label='no_models', model='Fake 2')\n        with mock.patch('builtins.input', return_value='yes'), captured_stdout() as stdout:\n            call_command('remove_stale_contenttypes', include_stale_apps=True, verbosity=2)\n        self.assertIn(\"Deleting stale content type 'no_models | Fake 2'\", stdout.getvalue())\n        self.assertEqual(ContentType.objects.count(), self.before_count)\n", "    def test_stale_content_types_from_removed_apps(self):\n        \"\"\"Stale content types from removed apps are deleted by default.\"\"\"\n        another_app_config = apps.get_app_config('another_test_app')\n        ContentType.objects.create(app_label='another_test_app', model='Fake')\n        self.assertEqual(ContentType.objects.filter(app_label='another_test_app').count(), 1)\n        with captured_stdout() as stdout:\n            call_command('remove_stale_contenttypes', verbosity=2)\n        self.assertIn(\"Deleting stale content type 'another_test_app | Fake'\", stdout.getvalue())\n        self.assertEqual(ContentType.objects.filter(app_label='another_test_app').count(), 0)\n"], "sample_401": ["    def test_all_valid_with_empty_formset(self):\n        data = {\n            \"choices-TOTAL_FORMS\": \"0\",\n            \"choices-INITIAL_FORMS\": \"0\",\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        self.assertIs(all_valid((formset1, formset2)), True)\n        expected_errors = [{}, {}]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n", "    def test_all_valid_with_non_form_errors(self):\n        data = {\n            \"choices-TOTAL_FORMS\": \"2\",  # the number of forms rendered\n            \"choices-INITIAL_FORMS\": \"0\",  # the number of forms with initial data\n            \"choices-MAX_NUM_FORMS\": \"2\",  # max number of forms - should be ignored\n            \"choices-0-choice\": \"Zero\",\n            \"choices-0-votes\": \"0\",\n            \"choices-1-choice\": \"One\",\n            \"choices-1-votes\": \"1\",\n        }\n        ChoiceFormSet = formset_factory(Choice, extra=1, max_num=1, validate_max=True)\n        formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        self.assertFalse(all_valid((formset,)))\n        self.assertEqual(formset.non_form_errors(), [\"Maximum number of forms exceeded\"])\n\n", "    def test_all_valid_with_empty_formsets(self):\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data={}, auto_id=False, prefix=\"choices\")\n        formset2 = ChoiceFormSet(data={}, auto_id=False, prefix=\"choices\")\n        self.assertIs(all_valid((formset1, formset2)), True)\n", "    def test_all_valid_empty_formsets(self):\n        data = {}\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        self.assertIs(all_valid((formset1, formset2)), True)\n        expected_errors = [{}, {}]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n", "    def test_empty_formset_with_initial_data(self):\n        data = {\n            \"form-TOTAL_FORMS\": \"0\",\n            \"form-INITIAL_FORMS\": \"0\",\n        }\n        formset = ArticleFormSet(data)\n        self.assertTrue(formset.is_valid())\n        self.assertEqual(formset.cleaned_data, [])\n"], "sample_1059": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_420": ["    def test_model_form_factory_formfield_callback_with_nested_models(self):\n        class InnerForm(forms.ModelForm):\n            class Meta:\n                model = Inner\n                fields = \"__all__\"\n\n            if db_field.model == Inner:\n                kwargs[\"form_class\"] = InnerForm\n            return db_field.formfield(**kwargs)\n\n        class OuterForm(forms.ModelForm):\n            class Meta:\n                model = Outer\n                formfield_callback = callback\n                fields = \"__all__\"\n\n        form = OuterForm()\n        self.assertIsInstance(form.fields[\"inner\"].form, InnerForm)\n\n", "    def test_strict_assignment_on_initial(self):\n        form_class = modelform_factory(model=StrictAssignmentFieldSpecific, fields=[\"title\"])\n        form = form_class(initial={\"title\": \"testing setattr\"})\n        form.instance._should_error = True\n        self.assertFalse(form.is_valid())\n        self.assertEqual(\n            form.errors,\n            {\"title\": [\"Cannot set attribute\", \"This field cannot be blank.\"]},\n        )\n\n", "    def test_modelform_factory_with_custom_formfield_callback(self):\n            if isinstance(db_field, models.CharField):\n                return forms.CharField(widget=forms.Textarea)\n            return db_field.formfield(**kwargs)\n\n        NewForm = modelform_factory(Person, formfield_callback=callback)\n\n        form = NewForm()\n        self.assertEqual(type(form.fields[\"name\"].widget), forms.Textarea)\n", "    def test_strict_assignment_all_fields(self):\n        \"\"\"\n        A model ValidationError not using the dict form should put the error\n        message into __all__ (i.e. non-field errors) on the form, for all fields.\n        \"\"\"\n        form_class = modelform_factory(model=StrictAssignmentAll, fields=\"__all__\")\n        form = form_class(data={\"title\": \"testing setattr\", \"text\": \"data\"}, files=None)\n        # This line turns on the ValidationError; it avoids the model erroring\n        # when its own __init__() is called when creating form.instance.\n        form.instance._should_error = True\n        self.assertFalse(form.is_valid())\n        self.assertEqual(\n            form.errors,\n            {\n                \"__all__\": [\"Cannot set attribute\"],\n                \"title\": [\"This field cannot be blank.\"],\n                \"text\": [\"This field cannot be blank.\"],\n            },\n        )\n", "    def test_model_form_with_custom_error_messages_in_model_clean(self):\n        data = {\"name1\": \"FORBIDDEN_VALUE\", \"name2\": \"ABC\"}\n        form = CustomErrorMessageForm(data)\n        self.assertFalse(form.is_valid())\n        self.assertHTMLEqual(\n            str(form.errors[\"name1\"]),\n            '<ul class=\"errorlist\"><li>Model.clean() error messages.</li></ul>',\n        )\n"], "sample_10": ["    def test_masked_column_replace_inplace():\n        t = table.Table([[1, 2], [3, 4]], names=['a', 'b'], masked=True)\n        t['a'] = np.ma.array([5, 6], mask=[True, False])\n        assert np.all(t['a'].data == [5, 6])\n        assert np.all(t['a'].mask == [True, False])\n", "def test_table_attribute_with_units():\n    class MyTable(Table):\n        foo = TableAttribute(unit=u.m)\n\n    t = MyTable([[1, 2]])\n    assert t.foo.unit is u.m\n", "    def test_remove_columns_empty_list():\n        t = table.table_helpers.simple_table(1)\n        t.remove_columns([])\n        assert t.colnames == ['a', 'b', 'c']\n", "compilation error", "compilation error"], "sample_53": ["    def test_render_options_no_initial(self):\n        form = AlbumForm()\n        output = form.as_table()\n        self.assertIn(self.empty_option, output)\n", "    def test_render_options_with_placeholder(self):\n        rel = Album._meta.get_field('band').remote_field\n        w = AutocompleteSelect(rel, admin.site, attrs={'placeholder': 'Choose a band'})\n        output = str(w.render('band', None, attrs={}))\n        self.assertIn('placeholder=\"Choose a band\"', output)\n", "    def test_render_options_with_label(self):\n        beatles = Band.objects.create(name='The Beatles', style='rock')\n        form = AlbumForm(initial={'band': beatles.pk})\n        output = form['band'].as_widget()\n        self.assertIn('<label for=\"id_band\">', output)\n", "    def test_render_options_with_translation(self):\n        with translation.override('fr'):\n            beatles = Band.objects.create(name='The Beatles', style='rock')\n            form = AlbumForm(initial={'band': beatles.pk})\n            output = form.as_table()\n            selected_option = '<option value=\"%s\" selected>The Beatles</option>' % beatles.pk\n            self.assertIn(selected_option, output)\n", "    def test_render_options_language(self):\n        \"\"\"Empty option is present if the field isn't required.\"\"\"\n        with translation.override('de'):\n            form = NotRequiredBandForm()\n            output = form.as_table()\n        self.assertIn(self.empty_option, output)\n"], "sample_1110": ["compilation error", "compilation error", "compilation error", "def test_airy_prime():\n    from sympy import airyaiprime, airybiprime\n\n    expr1 = airyaiprime(x)\n    expr2 = airybiprime(x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr1) == 'scipy.special.airy(x)[1]'\n    assert prntr.doprint(expr2) == 'scipy.special.airy(x)[3]'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr1) == '  # Not supported in Python with NumPy:\\n  # airyaiprime\\naireyaiprime(x)'\n    assert prntr.doprint(expr2) == '  # Not supported in Python with NumPy:\\n  # airybiprime\\naireybiprime(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr1) == '  # Not supported in Python:\\n  # airyaiprime\\naireyaiprime(x)'\n    assert prntr.doprint(expr2) == '  # Not supported in Python:\\n  # airybiprime\\naireybiprime(x)'\n", "compilation error"], "sample_923": ["compilation error", "    def check_classes(role, tag, expected_classes):\n        classes_set = classes(role, tag)\n        assert classes_set == set(expected_classes), (\n            f\"Mismatch in classes for {role} with tag {tag}: \"\n            f\"expected {set(expected_classes)}, got {classes_set}\"\n        )\n", "compilation error", "def test_build_domain_cpp_warn_template_param_qualified_name_in_type(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"warn-template-param-qualified-name-in-type\")\n    assert len(ws) == 2\n    assert \"WARNING: cpp:type reference target not found: T::typeWarn\" in ws[0]\n    assert \"WARNING: cpp:type reference target not found: T::U::typeWarn\" in ws[1]\n", "compilation error"], "sample_1031": ["def test_get_units():\n    ms = UnitSystem((m, s), (c,))\n    assert sorted(ms.get_units()) == sorted((m, s, c))\n", "compilation error", "def test_get_base_units():\n    ms = UnitSystem((m, s), (c,))\n    assert ms.get_base_units() == (m, s)\n\n\n\n", "compilation error", "compilation error"], "sample_284": ["    def test_manifest_cache_busting(self):\n        \"\"\"\n        Ensure the cached version of the file is updated when the original file is modified.\n        \"\"\"\n        filename = 'test/styles.css'\n        filepath = self._get_filename_path(filename)\n\n        # Create an initial file\n        with open(filepath, 'w') as f:\n            f.write(\"body { color: blue; }\")\n\n        self.run_collectstatic()\n\n        # Get the hashed filename from the cached file\n        hashed_filename = self.hashed_file_path(filename)\n        hashed_filepath = self.storage.path(hashed_filename)\n\n        # Modify the original file\n        with open(filepath, 'a') as f:\n            f.write(\"\\nbody { font-size: 16px; }\")\n\n        self.run_collectstatic()\n\n        # Ensure the hashed filename has changed\n        new_hashed_filename = self.hashed_file_path(filename)\n        self.assertNotEqual(hashed_filename, new_hashed_filename)\n\n        # Ensure the new hashed file exists\n        new_hashed_filepath = self.storage.path(new_hashed_filename)\n        self.assertTrue(os.path.exists(new_hashed_filepath))\n\n        # Ensure the old hashed file is deleted\n        self.assertFalse(os.path.exists(hashed_filepath))\n", "    def test_hash_collision(self):\n        \"\"\"\n        Ensure that files with the same hash but different content are handled correctly.\n        \"\"\"\n        file1_content = b\"Content of file 1\"\n        file2_content = b\"Content of file 2\"\n\n        with open(self._get_filename_path('file1.css'), 'wb') as f:\n            f.write(file1_content)\n        with open(self._get_filename_path('file2.css'), 'wb') as f:\n            f.write(file2_content)\n\n        self.run_collectstatic()\n\n        file1_path = self.hashed_file_path(\"test/file1.css\")\n        file2_path = self.hashed_file_path(\"test/file2.css\")\n\n        self.assertNotEqual(file1_path, file2_path)\n\n        with storage.staticfiles_storage.open(file1_path) as f:\n            self.assertEqual(f.read(), file1_content)\n\n        with storage.staticfiles_storage.open(file2_path) as f:\n            self.assertEqual(f.read(), file2_content)\n", "    def test_cache_busting_with_relative_paths(self):\n        self.run_collectstatic()\n        relpath = self.hashed_file_path(\"cached/css/window.css\")\n        self.assertEqual(relpath, \"cached/css/window.5d5c10836967.css\")\n        with storage.staticfiles_storage.open(relpath) as relfile:\n            content = relfile.read()\n            self.assertIn(b'url(\"img/window.acae32e4532b.png\")', content)\n", "    def test_manifest_hash_collision(self):\n        # Ensure that two files with the same hash don't overwrite each other\n        with open(self._get_filename_path('file1.css'), 'w') as f:\n            f.write('/* file1 */')\n        with open(self._get_filename_path('file2.css'), 'w') as f:\n            f.write('/* file2 */')\n\n        self.run_collectstatic()\n\n        file1_path = self.hashed_file_path('test/file1.css')\n        file2_path = self.hashed_file_path('test/file2.css')\n        self.assertNotEqual(file1_path, file2_path)\n\n        with storage.staticfiles_storage.open(file1_path) as f1:\n            content1 = f1.read()\n        with storage.staticfiles_storage.open(file2_path) as f2:\n            content2 = f2.read()\n        self.assertIn(b'/* file1 */', content1)\n        self.assertIn(b'/* file2 */', content2)\n", "    def test_manifest_strict_missing_entry(self):\n        missing_file_name = 'cached/missing.css'\n        configured_storage = storage.staticfiles_storage\n        self.assertNotIn(missing_file_name, configured_storage.hashed_files)\n\n        with self.assertRaisesMessage(ValueError, \"Missing staticfiles manifest entry for '%s'\" % missing_file_name):\n            self.hashed_file_path(missing_file_name)\n"], "sample_426": ["    def test_negative_depth(self):\n        with self.assertRaises(ValueError):\n            timesince(self.t, depth=-1)\n", "    def test_timeuntil_with_date_object(self):\n        today = datetime.date.today()\n        self.assertEqual(timeuntil(today + self.oneday), \"1\\xa0day\")\n        self.assertEqual(timeuntil(today - self.oneday), \"0\\xa0minutes\")\n", "    def test_different_timezones_reversed(self):\n        \"\"\"When using two different timezones, reversed.\"\"\"\n        now = datetime.datetime.now()\n        now_tz = timezone.make_aware(now, timezone.get_default_timezone())\n        now_tz_i = timezone.localtime(now_tz, timezone.get_fixed_timezone(195))\n\n        self.assertEqual(timeuntil(now), \"0\\xa0minutes\")\n        self.assertEqual(timeuntil(now_tz), \"0\\xa0minutes\")\n        self.assertEqual(timeuntil(now_tz_i), \"0\\xa0minutes\")\n        self.assertEqual(timesince(now_tz, now_tz_i), \"0\\xa0minutes\")\n        self.assertEqual(timesince(now), \"0\\xa0minutes\")\n        self.assertEqual(timesince(now_tz), \"0\\xa0minutes\")\n        self.assertEqual(timesince(now_tz_i), \"0\\xa0minutes\")\n", "    def test_months_edge_leap_year(self):\n        t = datetime.datetime(2024, 1, 1)\n        tests = [\n            (datetime.datetime(2024, 1, 31), \"4\\xa0weeks, 2\\xa0days\"),\n            (datetime.datetime(2024, 2, 1), \"1\\xa0month\"),\n            (datetime.datetime(2024, 2, 29), \"1\\xa0month, 4\\xa0weeks\"),\n            (datetime.datetime(2024, 3, 1), \"2\\xa0months\"),\n            (datetime.datetime(2024, 3, 31), \"2\\xa0months, 4\\xa0weeks\"),\n            (datetime.datetime(2024, 4, 1), \"3\\xa0months\"),\n            (datetime.datetime(2024, 4, 30), \"3\\xa0months, 4\\xa0weeks\"),\n            (datetime.datetime(2024, 5, 1), \"4\\xa0months\"),\n            (datetime.datetime(2024, 5, 31), \"4\\xa0months, 4\\xa0weeks\"),\n            (datetime.datetime(2024, 6, 1), \"5\\xa0months\"),\n            (datetime.datetime(2024, 6, 30), \"5\\xa0months, 4\\xa0weeks\"),\n            (datetime.datetime(2024, 7, 1), \"6\\xa0months\"),\n            (datetime.datetime(2024, 7, 31), \"6\\xa0months, 4\\xa0weeks\"),\n            (datetime.datetime(2024, 8, 1), \"7\\xa0months\"),\n            (datetime.datetime(2024, 8, 31), \"7\\xa0months, 4\\xa0weeks\"),\n            (datetime.datetime(2024, 9, 1), \"8\\xa0months\"),\n            (datetime.datetime(", "    def test_months_leap_year(self):\n        t = datetime.datetime(2024, 1, 1)\n        tests = [\n            (datetime.datetime(2024, 1, 31), \"4\\xa0weeks, 2\\xa0days\"),\n            (datetime.datetime(2024, 2, 1), \"1\\xa0month\"),\n            (datetime.datetime(2024, 2, 29), \"1\\xa0month, 4\\xa0weeks\"),\n            (datetime.datetime(2024, 3, 1), \"2\\xa0months\"),\n            (datetime.datetime(2024, 3, 31), \"2\\xa0months, 4\\xa0weeks\"),\n            (datetime.datetime(2024, 4, 1), \"3\\xa0months\"),\n            (datetime.datetime(2024, 4, 30), \"3\\xa0months, 4\\xa0weeks\"),\n            (datetime.datetime(2024, 5, 1), \"4\\xa0months\"),\n            (datetime.datetime(2024, 5, 31), \"4\\xa0months, 4\\xa0weeks\"),\n            (datetime.datetime(2024, 6, 1), \"5\\xa0months\"),\n            (datetime.datetime(2024, 6, 30), \"5\\xa0months, 4\\xa0weeks\"),\n            (datetime.datetime(2024, 7, 1), \"6\\xa0months\"),\n            (datetime.datetime(2024, 7, 31), \"6\\xa0months, 4\\xa0weeks\"),\n            (datetime.datetime(2024, 8, 1), \"7\\xa0months\"),\n            (datetime.datetime(2024, 8, 31), \"7\\xa0months, 4\\xa0weeks\"),\n            (datetime.datetime(2024, 9, 1), \"8\\xa0months\"),\n            (datetime.datetime(20"], "sample_938": ["def test_show_urls(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'sphinxtests.1').read_text()\n    assert 'http://sphinx-doc.org/' in content\n", "def test_custom_man_pages(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'myproject.1').read_text()\n    assert 'My Project' in content\n\n", "def test_custom_man_pages(app, status, warning):\n    app.builder.build_all()\n    assert (app.outdir / 'mymanpage.8').exists()\n\n", "def test_empty_man_pages(app, status, warning):\n    app.build()\n    assert not (app.outdir / 'sphinxtests.1').exists()\n\n", "def test_multiple_man_pages(app, status, warning):\n    app.builder.build_all()\n    assert (app.outdir / 'section1.1').exists()\n    assert (app.outdir / 'section2.2').exists()\n    assert (app.outdir / 'section3.3').exists()\n\n"], "sample_639": ["def test_base_checker_comparison() -> None:\n    basic = OtherBasicChecker()\n    different = DifferentBasicChecker()\n    assert basic > different\n    assert different < basic\n    assert basic != different\n    assert basic == OtherBasicChecker()\n\n", "def test_base_checker_consistency() -> None:\n\n    with pytest.raises(InvalidMessageError) as excinfo:\n        class BrokenChecker(BaseChecker):\n            name = \"broken\"\n            msgs = {\n                \"W1234\": (\"msg1\", \"symbol1\", \"desc1\"),\n                \"W2345\": (\"msg2\", \"symbol2\", \"desc2\"),\n            }\n    assert \"Inconsistent checker part in message id\" in str(excinfo.value)\n", "def test_base_checker_comparison() -> None:\n    basic = OtherBasicChecker()\n    different = DifferentBasicChecker()\n    assert basic > different\n    assert basic != different\n\n", "def test_check_consistency_error() -> None:\n    class InconsistentChecker(BaseChecker):\n        name = 'inconsistent'\n        msgs = {\n            'W0001': ('First message', 'first-message', 'This is the first message'),\n            'W0002': ('Second message', 'second-message', 'This is the second message'),\n        }\n\n    checker = InconsistentChecker()\n    with pytest.raises(InvalidMessageError) as excinfo:\n        checker.check_consistency()\n    assert \"Inconsistent checker part in message id\" in str(excinfo.value)\n", "def test_base_checker_comparison() -> None:\n    basic = OtherBasicChecker()\n    different = DifferentBasicChecker()\n    assert basic > different\n    assert basic != different\n    assert hash(basic) != hash(different)\n"], "sample_1064": ["compilation error", "    def test_tensorflow_derivatives():\n        if not tf:\n            skip(\"TensorFlow not installed\")\n\n        x = Symbol('x')\n        expr = Derivative(sin(x), x)\n        assert tensorflow_code(expr) == \\\n                'tensorflow.gradients(tensorflow.math.sin(x), x)[0]'\n        _compare_tensorflow_scalar((x,), expr)\n", "compilation error", "compilation error", "compilation error"], "sample_316": ["    def test_imagefile_width_and_height(self):\n        img_path = os.path.join(os.path.dirname(__file__), \"test.png\")\n        with open(img_path, 'rb') as img:\n            image_file = images.ImageFile(img)\n            self.assertEqual(image_file.width, 540)\n            self.assertEqual(image_file.height, 405)\n", "    def test_imagefile_width_height_from_file_like(self):\n        img_path = os.path.join(os.path.dirname(__file__), \"test.png\")\n        with open(img_path, 'rb') as fh:\n            image = images.ImageFile(fh)\n            self.assertEqual(image.width, 540)\n            self.assertEqual(image.height, 405)\n", "    def test_image_file_width_and_height(self):\n        with open(os.path.join(os.path.dirname(__file__), \"test.png\"), 'rb') as fh:\n            image_file = images.ImageFile(fh)\n            self.assertEqual(image_file.width, 540)\n            self.assertEqual(image_file.height, 405)\n", "    def test_image_file_dimensions(self):\n        img_path = os.path.join(os.path.dirname(__file__), \"test.png\")\n        with open(img_path, 'rb') as fh:\n            image_file = images.ImageFile(fh)\n            self.assertEqual(image_file.width, 540)\n            self.assertEqual(image_file.height, 405)\n", "    def test_get_image_dimensions_with_context_manager(self):\n        \"\"\"\n        Test get_image_dimensions with a context manager.\n        \"\"\"\n        img_path = os.path.join(os.path.dirname(__file__), \"test.png\")\n        with open(img_path, 'rb') as fh:\n            with images.ImageFile(fh) as image:\n                size = images.get_image_dimensions(image)\n        self.assertEqual(size, (540, 405))\n"], "sample_404": ["    def test_invalid_block_tag_after_extend(self):\n        \"\"\"\n        #26598 -- Invalid block tag after {% extends %} should raise a\n        proper error.\n        \"\"\"\n        engine = self._engine()\n        t = engine.from_string(\n            \"{% extends 'base.html' %}{% invalid_block %}\"\n        )\n        with self.assertRaises(TemplateSyntaxError) as e:\n            t.render(Context())\n        if self.debug_engine:\n           self.assertEqual(e.exception.template_debug[\"during\"], \"{% invalid_block %}\") \n\n", "    def test_filter_expression_invalid_syntax(self):\n        engine = self._engine()\n        msg = \"Could not parse some characters: ' ' at ' ' \" \\\n              \"from 'value |invalid_syntax'\"\n\n        with self.assertRaisesMessage(TemplateSyntaxError, msg):\n            engine.from_string(\"{% if 1 %}{{ value |invalid_syntax }}{% endif %}\")\n\n", "    def test_variable_attribute_error(self):\n        \"\"\"\n        Ensure we're raising the right exception when accessing a non-existent\n        attribute on a variable.\n\n        Refs #28321\n        \"\"\"\n        engine = self._engine()\n        template = engine.from_string(\"{% if obj.does_not_exist %}{% endif %}\")\n        with self.assertRaises(AttributeError) as e:\n            template.render(Context({'obj': object()}))\n        self.assertEqual(str(e.exception), \"'object' object has no attribute 'does_not_exist'\")\n\n", "    def test_invalid_block_tag_with_extra_whitespace(self):\n        \"\"\"\n        Ensure that invalid block tags with extra whitespace are handled correctly.\n        \"\"\"\n        engine = self._engine()\n        msg = \"Invalid block tag on line 1: 'foobar  '. Did you forget to register or load this tag?\"\n        with self.assertRaisesMessage(TemplateSyntaxError, msg):\n            engine.from_string(\"lala{% foobar  %}\")\n", "    def test_variable_node_error(self):\n        \"Ensure variable nodes raise the correct exception when invalid\"\n        engine = self._engine()\n        with self.assertRaises(TemplateSyntaxError) as e:\n            engine.from_string(\"{{ invalid_variable }}\").render(Context())\n        if self.debug_engine:\n            self.assertEqual(e.exception.template_debug[\"during\"], \"{{ invalid_variable }}\")\n\n"], "sample_727": ["def test_imputation_dense_sparse_consistency():\n    # Test imputation on dense and sparse data with the same missing values\n    # Locations\n\n    rng = np.random.RandomState(0)\n\n    n_samples, n_features = 10, 5\n\n    X_dense = rng.rand(n_samples, n_features)\n    X_sparse = sparse.csr_matrix(X_dense)\n\n    missing_mask = rng.rand(n_samples, n_features) < 0.2\n    X_dense[missing_mask] = np.nan\n    X_sparse[missing_mask] = np.nan\n\n    strategies = ['mean', 'median', 'most_frequent']\n    for strategy in strategies:\n        imputer_dense = Imputer(strategy=strategy, missing_values=np.nan)\n        imputer_sparse = Imputer(strategy=strategy, missing_values=np.nan)\n\n        X_dense_imputed = imputer_dense.fit_transform(X_dense)\n        X_sparse_imputed = imputer_sparse.fit_transform(X_sparse)\n\n        assert_array_almost_equal(X_dense_imputed, X_sparse_imputed.toarray())\n", "def test_imputation_empty_columns():\n    # Test imputation when a column is entirely missing.\n    X = np.array([\n        [np.nan, np.nan, np.nan],\n        [np.nan, np.nan, np.nan],\n        [np.nan, np.nan, np.nan],\n    ])\n\n    for strategy in ['mean', 'median', 'most_frequent']:\n        imputer = Imputer(strategy=strategy)\n\n        with warnings.catch_warnings(record=True) as w:\n            imputer.fit_transform(X)\n            assert_warns_message(w, \"Deleting features without \"\n                                        \"observed values\")\n", "def test_imputation_empty_array():\n    # Test imputation for empty arrays.\n    X = np.array([])\n    X_empty = np.array([])\n    for strategy in [\"mean\", \"median\", \"most_frequent\"]:\n        imputer = Imputer(strategy=strategy)\n        assert_warns(UserWarning, imputer.fit_transform, X)\n        assert_array_equal(imputer.transform(X), X_empty.reshape(0, 0))\n    \n    X = np.array([], dtype=np.float64).reshape(0, 5)\n    X_empty = np.array([], dtype=np.float64).reshape(0, 5)\n    for strategy in [\"mean\", \"median\", \"most_frequent\"]:\n        imputer = Imputer(strategy=strategy)\n        assert_warns(UserWarning, imputer.fit_transform, X)\n        assert_array_equal(imputer.transform(X), X_empty)\n\n\n", "def test_imputation_error_sparse_axis():\n    # Test that ValueError is raised when using axis=1 with a sparse matrix\n    # and a strategy other than 'mean'.\n    X = sparse.csc_matrix([[1, 2], [3, np.nan], [4, 5]])\n\n    imputer = Imputer(strategy='median', axis=1)\n    assert_raises(ValueError, imputer.fit, X)\n\n    imputer = Imputer(strategy='most_frequent', axis=1)\n    assert_raises(ValueError, imputer.fit, X)\n", "def test_imputation_most_frequent_sparse():\n    # Test imputation using the most-frequent strategy on sparse matrices.\n    X = sparse.csr_matrix([\n        [-1, -1, 0, 5],\n        [-1, 2, -1, 3],\n        [-1, 1, 3, -1],\n        [-1, 2, 3, 7],\n    ])\n\n    X_true = sparse.csr_matrix([\n        [2, 0, 5],\n        [2, 3, 3],\n        [1, 3, 3],\n        [2, 3, 7],\n    ])\n\n    _check_statistics(X, X_true, \"most_frequent\", [np.nan, 2, 3, 3], -1)\n"], "sample_1167": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_209": ["    def test_model_with_evaluate_method_evaluate(self):\n        \"\"\"\n        You can filter by objects that have an 'evaluate' attr\n        and evaluate it\n\n        This is a continuation of test_model_with_evaluate_method\n        \"\"\"\n        dept = Department.objects.create(pk=1, name='abc')\n        dept.evaluate = 'abc'\n        Worker.objects.filter(department=dept, department__evaluate='abc')\n", "    def test_related_objects_with_evaluate_method(self):\n        dept = Department.objects.create(pk=1, name='abc')\n        dept.evaluate = 'abc'\n        Worker.objects.create(department=dept, name='worker1')\n        self.assertEqual(dept.worker_set.count(), 1)\n", "    def test_foreign_key_ordering(self):\n        dept1 = Department.objects.create(name=\"Dept 1\")\n        dept2 = Department.objects.create(name=\"Dept 2\")\n\n        worker1 = Worker.objects.create(department=dept1, name=\"Worker 1\")\n        worker2 = Worker.objects.create(department=dept2, name=\"Worker 2\")\n        worker3 = Worker.objects.create(department=dept1, name=\"Worker 3\")\n\n        # Make sure ordering works both ascending and descending\n        self.assertSequenceEqual(\n            Worker.objects.filter(department=dept1).order_by('name'),\n            [worker1, worker3],\n        )\n        self.assertSequenceEqual(\n            Worker.objects.filter(department=dept1).order_by('-name'),\n            [worker3, worker1],\n        )\n", "    def test_empty_related_field(self):\n        worker = Worker.objects.create(name=\"Empty\")\n        self.assertEqual(worker.department, None)\n", "    def test_default_ordering_without_unique_constraint(self):\n        # Regression test for #29595: default ordering doesn't work without\n        # unique constraint.\n        Model1.objects.create(pkey=1, name='a')\n        Model1.objects.create(pkey=2, name='b')\n        self.assertQuerysetEqual(\n            Model1.objects.all(), [\n                \"a\",\n                \"b\",\n            ],\n            attrgetter(\"name\")  \n        )\n\n"], "sample_236": ["    def test_fast_delete_through_m2m_related_model(self):\n        t = M2MTo.objects.create()\n        f1 = M2MFrom.objects.create()\n        f2 = M2MFrom.objects.create()\n        f1.m2m.add(t)\n        f2.m2m.add(t)\n        # 1 to delete f1, 1 to delete f2, 1 to fast-delete m2m for f1 and f2\n        self.assertNumQueries(3, f1.delete)\n        self.assertFalse(M2MFrom.objects.exists())\n        self.assertFalse(M2MTo.objects.exists())\n", "    def test_fast_delete_m2m_through(self):\n        p = M.objects.create()\n        m = M.objects.create()\n        r = R.objects.create()\n        MR.objects.create(m=m, r=r)\n        p.m2m.add(r)\n        p.save()\n        # 1 query for deleting p, 1 for deleting MR\n        # through the m2m field.\n        self.assertNumQueries(2, p.delete)\n\n", "    def test_fast_delete_cascade_through_proxy(self):\n        \"\"\"\n        #29473 - Avoid cascading through proxy models during fast delete.\n        \"\"\"\n        origin = ConcreteOrigin.objects.create()\n        proxy_ref = ProxyReferrer.objects.create(origin=origin)\n        with self.assertNumQueries(2):\n            origin.delete()\n\n", "    def test_fast_delete_multiple_m2m(self):\n        t1 = M2MTo.objects.create()\n        t2 = M2MTo.objects.create()\n        f = M2MFrom.objects.create()\n        f.m2m.add(t1, t2)\n        # 1 to delete f, 1 to fast-delete m2m for f\n        self.assertNumQueries(2, f.delete)\n", "    def test_fast_delete_proxy_model_inheritance(self):\n        parent = ParentProxy.objects.create()\n        child = ChildProxy.objects.create(parent=parent)\n\n        self.assertNumQueries(2, parent.delete)\n        self.assertFalse(ParentProxy.objects.exists())\n        self.assertFalse(ChildProxy.objects.exists())\n"], "sample_260": ["    def test_create_model_remove_field_with_fk_dependency(self):\n        \"\"\"\n        RemoveField should optimize into CreateModel even if it's a FK referenced model.\n        \"\"\"\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[\n                        (\"name\", models.CharField(max_length=255)),\n                        (\"age\", models.IntegerField()),\n                    ],\n                    options={'verbose_name': 'Foo'},\n                    bases=(UnicodeModel,),\n                ),\n                migrations.CreateModel(\n                    name=\"Bar\",\n                    fields=[('foo', models.ForeignKey('migrations.Foo', models.CASCADE))],\n                ),\n                migrations.RemoveField(\"Foo\", \"age\"),\n            ],\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[\n                        (\"name\", models.CharField(max_length=255)),\n                    ],\n                    options={'verbose_name': 'Foo'},\n                    bases=(UnicodeModel,),\n                ),\n                migrations.CreateModel(\n                    name=\"Bar\",\n                    fields=[('foo', models.ForeignKey('migrations.Foo', models.CASCADE))],\n                ),\n            ],\n        )\n", "    def test_optimize_through_fields_with_fk(self):\n        \"\"\"\n        field-level through checking is working with ForeignKeys.\n        \"\"\"\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\"Foo\", [(\"name\", models.CharField(max_length=255))]),\n                migrations.CreateModel(\"Bar\", [(\"size\", models.IntegerField())]),\n                migrations.AddField(\"Foo\", \"bar\", models.ForeignKey('migrations.Bar', models.CASCADE)),\n                migrations.AddField(\"Foo\", \"age\", models.IntegerField()),\n                migrations.AddField(\"Bar\", \"width\", models.IntegerField()),\n                migrations.AlterField(\"Foo\", \"age\", models.IntegerField()),\n                migrations.RenameField(\"Bar\", \"size\", \"dimensions\"),\n                migrations.RemoveField(\"Foo\", \"age\"),\n                migrations.RenameModel(\"Foo\", \"Phou\"),\n                migrations.RemoveField(\"Bar\", \"dimensions\"),\n                migrations.RenameModel(\"Phou\", \"Fou\"),\n                migrations.DeleteModel(\"Fou\"),\n            ],\n            [\n                migrations.CreateModel(\"Bar\", [(\"width\", models.IntegerField())]),\n            ],\n        )\n", "    def test_create_model_add_field_with_default(self):\n        \"\"\"\n        AddField with a default should optimize into CreateModel.\n        \"\"\"\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\"Foo\", [(\"name\", models.CharField(max_length=255))]),\n                migrations.AddField(\"Foo\", \"age\", models.IntegerField(default=42)),\n            ],\n            [\n                migrations.CreateModel(\"Foo\", [\n                    (\"name\", models.CharField(max_length=255)),\n                    (\"age\", models.IntegerField(default=42)),\n                ]),\n            ],\n        )\n", "    def test_create_alter_unique_field_no_optimize_across_app_label(self):\n        self._test_create_alter_foo_field(migrations.AlterUniqueTogether(\"Foo\", [[\"a\", \"b\"]]), app_label='another_app')\n", "    def test_create_alter_unique_together_field_ordering(self):\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\"Foo\", [(\"name\", models.CharField(max_length=255)), (\"b\", models.IntegerField()), (\"a\", models.IntegerField())]),\n                migrations.AlterUniqueTogether(\"Foo\", [[\"a\", \"b\"]]),\n                migrations.RenameField(\"Foo\", \"a\", \"x\"),\n                migrations.RenameField(\"Foo\", \"x\", \"a\"),\n            ],\n            [\n                migrations.CreateModel(\"Foo\", [(\"name\", models.CharField(max_length=255)), (\"a\", models.IntegerField()), (\"b\", models.IntegerField())],\n                options={'unique_together': {('a', 'b')}},\n            ],\n        )\n\n"], "sample_867": ["    def test_grid_search_with_refit_error():\n        # Test GridSearchCV when refit raises an error\n        X, y = make_classification(n_samples=20, n_features=10, random_state=0)\n\n        class FailingClassifier(BaseEstimator):\n                raise ValueError(\"Failing classifier failed as required\")\n\n                return np.zeros(X.shape[0])\n\n        clf = FailingClassifier()\n        gs = GridSearchCV(clf, {'param': [1]}, cv=2, refit=True,\n                          error_score='raise')\n        with pytest.raises(ValueError):\n            gs.fit(X, y)\n", "def test_grid_search_cv_results_empty():\n    \"\"\"\n    Test that GridSearchCV returns empty cv results when no candidates \n    are provided.\n    \"\"\"\n    X, y = make_classification(n_samples=10, random_state=0)\n\n    clf = LinearSVC(random_state=0)\n    gscv = GridSearchCV(clf, param_grid={}, cv=3)\n    gscv.fit(X, y)\n\n    assert 'mean_test_score' not in gscv.cv_results_\n\n", "    def test_gridsearchcv_refit_error_when_scoring_is_none():\n        X, y = make_classification(n_samples=100, random_state=0)\n        clf = LinearSVC(random_state=0)\n        # scoring=None should raise an error if refit is True\n        grid_search = GridSearchCV(clf, {'C': [0.1, 1]}, scoring=None,\n                                   return_train_score=True, refit=True)\n        with pytest.raises(ValueError, match='scoring must be provided'):\n            grid_search.fit(X, y)\n", "def test_grid_search_return_train_score_false():\n    # Test GridSearchCV when return_train_score is False\n    X, y = make_classification(n_samples=20, n_features=10, random_state=0)\n\n    clf = MockClassifier()\n\n    gs = GridSearchCV(clf, {'parameter': [0, 1, 2]}, scoring='accuracy',\n                      refit=False, return_train_score=False)\n    gs.fit(X, y)\n\n    assert 'mean_train_score' not in gs.cv_results_\n    assert 'std_train_score' not in gs.cv_results_\n", "    def test_search_cv_refit_error_handling():\n        class FailingEstimator(BaseEstimator):\n                raise ValueError(\"Intentional error during fit\")\n\n        X, y = make_classification(n_samples=20, n_features=10,\n                                    random_state=0)\n        grid = GridSearchCV(FailingEstimator(), {'param': [1, 2]},\n                            scoring='accuracy', refit=True)\n        assert_raises(ValueError, grid.fit, X, y)\n"], "sample_698": ["def test_coloredlogformatter_with_custom_level_name() -> None:\n    logfmt = \"%(filename)-25s %(lineno)4d %(levelname)-8s %(message)s\"\n    levelname = \"MYLEVEL\"\n    record = logging.LogRecord(\n        name=\"dummy\",\n        level=logging.INFO,\n        pathname=\"dummypath\",\n        lineno=10,\n        msg=\"Test Message\",\n        args=(),\n        exc_info=None,\n        levelname=levelname,\n    )\n\n    tw = TerminalWriter()\n    tw.hasmarkup = True\n    formatter = ColoredLevelFormatter(tw, logfmt)\n    output = formatter.format(record)\n    assert output == (\n        \"dummypath                   10 \\x1b[32mMYLEVEL  \\x1b[0m Test Message\"\n    )\n\n    tw.hasmarkup = False\n    formatter = ColoredLevelFormatter(tw, logfmt)\n    output = formatter.format(record)\n    assert output == (\"dummypath                   10 MYLEVEL   Test Message\")\n", "    def test_logging_plugin_with_log_cli_level(\n        self,\n        pytestconfig,\n        caplog,", "def test_coloredlogformatter_with_empty_message() -> None:\n    logfmt = \"%(filename)-25s %(lineno)4d %(levelname)-8s %(message)s\"\n\n    record = logging.LogRecord(\n        name=\"dummy\",\n        level=logging.INFO,\n        pathname=\"dummypath\",\n        lineno=10,\n        msg=\"\",\n        args=(),\n        exc_info=None,\n    )\n\n    tw = TerminalWriter()\n    tw.hasmarkup = True\n    formatter = ColoredLevelFormatter(tw, logfmt)\n    output = formatter.format(record)\n    assert output == \"dummypath                   10 \\x1b[32mINFO    \\x1b[0m \"\n\n    tw.hasmarkup = False\n    formatter = ColoredLevelFormatter(tw, logfmt)\n    output = formatter.format(record)\n    assert output == (\"dummypath                   10 INFO     \")\n", "def test_coloredlogformatter_error() -> None:\n    logfmt = \"%(filename)-25s %(lineno)4d %(levelname)-8s %(message)s\"\n\n    record = logging.LogRecord(\n        name=\"dummy\",\n        level=logging.ERROR,\n        pathname=\"dummypath\",\n        lineno=10,\n        msg=\"Test Message\",\n        args=(),\n        exc_info=None,\n    )\n\n    tw = TerminalWriter()\n    tw.hasmarkup = True\n    formatter = ColoredLevelFormatter(tw, logfmt)\n    output = formatter.format(record)\n    assert output == (\n        \"dummypath                   10 \\x1b[31mERROR   \\x1b[0m Test Message\"\n    )\n\n    tw.hasmarkup = False\n    formatter = ColoredLevelFormatter(tw, logfmt)\n    output = formatter.format(record)\n    assert output == (\"dummypath                   10 ERROR     Test Message\")\n\n", "def test_coloredlogformatter_with_empty_message() -> None:\n    logfmt = \"%(filename)-25s %(lineno)4d %(levelname)-8s %(message)s\"\n\n    record = logging.LogRecord(\n        name=\"dummy\",\n        level=logging.INFO,\n        pathname=\"dummypath\",\n        lineno=10,\n        msg=\"\",\n        args=(),\n        exc_info=None,\n    )\n\n    tw = TerminalWriter()\n    tw.hasmarkup = True\n    formatter = ColoredLevelFormatter(tw, logfmt)\n    output = formatter.format(record)\n    assert output == (\n        \"dummypath                   10 \\x1b[32mINFO    \\x1b[0m \"\n    )\n\n    tw.hasmarkup = False\n    formatter = ColoredLevelFormatter(tw, logfmt)\n    output = formatter.format(record)\n    assert output == (\"dummypath                   10 INFO     \")\n"], "sample_1021": ["compilation error", "def test_quaternion_mul_associativity():\n    q1 = Quaternion(1, 2, 3, 4)\n    q2 = Quaternion(5, 6, 7, 8)\n    q3 = Quaternion(9, 10, 11, 12)\n\n    assert (q1 * q2) * q3 == q1 * (q2 * q3)\n", "compilation error", "compilation error", "compilation error"], "sample_1180": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_504": ["compilation error", "def test_colorbar_label_location():\n    fig, ax = plt.subplots()\n    im = ax.imshow([[1, 2], [3, 4]])\n    cb = fig.colorbar(im, label='cbar')\n    cb.set_label_position('left')\n    assert cb.ax.get_ylabel() == 'cbar'\n    cb.set_label_position('right')\n    assert cb.ax.get_ylabel() == 'cbar'\n    cb.set_label_position('top')\n    assert cb.ax.get_ylabel() == 'cbar'\n", "compilation error", "compilation error", "compilation error"], "sample_219": ["    def test_expressionwrapper_repr(self):\n        expr = ExpressionWrapper(F('cost') * 2, output_field=IntegerField())\n        self.assertEqual(repr(expr), \"ExpressionWrapper(F(cost) * Value(2))\")\n", "    def test_output_field(self):\n        expr = ExpressionWrapper(F('field'), output_field=IntegerField())\n        self.assertEqual(expr.output_field, IntegerField())\n", "    def test_deconstruct(self):\n        value = ExpressionWrapper(Value('name'), output_field=CharField())\n        path, args, kwargs = value.deconstruct()\n        self.assertEqual(path, 'django.db.models.expressions.ExpressionWrapper')\n        self.assertEqual(args, (Value('name'), CharField()))\n        self.assertEqual(kwargs, {})\n", "    def test_as_sql_with_different_output_fields(self):\n        expr = ExpressionWrapper(\n            Value(3),\n            output_field=CharField(),\n        )\n        compiler = Time.objects.all().query.get_compiler(connection=connection)\n        sql, params = expr.as_sql(compiler, connection)\n        self.assertEqual(sql, '%s')\n        self.assertEqual(params, [3])\n", "    def test_expressionwrapper_output_field_None(self):\n        expr = ExpressionWrapper(F('cost') + F('tax'))\n        self.assertIsNone(expr.output_field)\n"], "sample_297": ["    def test_ticket_23605_subquery_count(self):\n        # Test that subqueries are correctly counted in the WHERE clause when using a complex Q object.\n        \n        a1 = Ticket23605A.objects.create()\n        a2 = Ticket23605A.objects.create()\n        c1 = Ticket23605C.objects.create(field_c0=10000.0)\n        Ticket23605B.objects.create(\n            field_b0=10000.0, field_b1=True,\n            modelc_fk=c1, modela_fk=a1)\n        complex_q = Q(pk__in=Ticket23605A.objects.filter(\n            Q(ticket23605b__field_b0__gte=1000000 / F(\"ticket23605b__modelc_fk__field_c0\")) &\n            Q(ticket23605b__field_b1=True) & ~Q(ticket23605b__pk__in=Ticket23605B.objects.filter(\n                ~(Q(field_b1=True) & Q(field_b0__gte=1000000 / F(\"modelc_fk__field_c0\"))\n                )\n            )).filter(ticket23605b__field_b1=True))\n        qs = Ticket23605A.objects.filter(complex_q)\n        self.assertEqual(qs.count(), 1)\n", "    def test_ticket_24605_exclude(self):\n        \"\"\"\n        Test exclude subquery with join and filter.\n        \"\"\"\n        i1 = Individual.objects.create(alive=True)\n        RelatedIndividual.objects.create(related=i1)\n        i2 = Individual.objects.create(alive=False)\n        RelatedIndividual.objects.create(related=i2)\n        i3 = Individual.objects.create(alive=True)\n        i4 = Individual.objects.create(alive=False)\n\n        self.assertSequenceEqual(\n            Individual.objects.exclude(Q(alive=False)).exclude(Q(related_individual__isnull=True)).order_by('pk'),\n            [i1, i3]\n        )   \n", "    def test_ticket_24605_no_join(self):\n        \"\"\"\n        Ensure no join is created when the relation field is excluded.\n        \"\"\"\n        i1 = Individual.objects.create(alive=True)\n        RelatedIndividual.objects.create(related=i1)\n        qs = Individual.objects.filter(alive=True).exclude(related_individual__isnull=False)\n        self.assertEqual(str(qs.query).count('JOIN'), 0)\n        self.assertSequenceEqual(qs, [i1])\n", "    def test_ticket_24851(self):\n        \"\"\"\n        Test that annotate() doesn't cause JOINs to be promoted incorrectly.\n        \"\"\"\n        p1 = Profile.objects.create(user=User.objects.create(username='p1'))\n        p2 = Profile.objects.create(user=User.objects.create(username='p2'))\n        p3 = Profile.objects.create(user=User.objects.create(username='p3'))\n\n        # Create posts for each profile\n        Post.objects.create(profile=p1)\n        Post.objects.create(profile=p2)\n        Post.objects.create(profile=p3)\n        Post.objects.create(profile=p3)\n\n        profiles = Profile.objects.annotate(post_count=Count('post')).filter(\n            post_count__gt=1\n        )\n        self.assertEqual(profiles.count(), 1)\n        self.assertIn(p3, profiles)\n", "    def test_ticket_24985(self):\n        \"\"\"\n        Test that joining through an intermediary model with a ManyToManyField\n        doesn't result in incorrect results when using select_related.\n\n        See: https://github.com/django/django/issues/24985\n        \"\"\"\n        t1 = Tag.objects.create(name='t1')\n        t2 = Tag.objects.create(name='t2')\n        a1 = Annotation.objects.create(tag=t1, name='a1')\n        a2 = Annotation.objects.create(tag=t2, name='a2')\n        n1 = Note.objects.create(note='foo', misc='bar')\n        n1.annotations.add(a1, a2)\n        n2 = Note.objects.create(note='baz', misc='qux')\n        n2.annotations.add(a1)\n\n        # The original issue was that select_related was incorrectly joining on\n        # Annotation.tag_id, leading to duplicate results for n1.\n        queryset = Note.objects.select_related('annotations__tag').filter(annotations__name='a1')\n        self.assertSequenceEqual(queryset, [n1, n2])\n"], "sample_347": ["    def test_make_aware_zoneinfo_invalid(self):\n        with self.assertRaises(TypeError):\n            timezone.make_aware(datetime.datetime(2011, 9, 1, 12, 20, 30), 'invalid_timezone')\n", "    def test_make_naive_zoneinfo_fold(self):\n        ambiguous = datetime.datetime(2015, 10, 25, 2, 30)\n        std = timezone.make_aware(ambiguous.replace(fold=1), timezone=PARIS_ZI)\n        dst = timezone.make_aware(ambiguous, timezone=PARIS_ZI)\n        self.assertEqual(std.fold, True)\n        self.assertEqual(dst.fold, False)\n        naive = timezone.make_naive(std, timezone=PARIS_ZI)\n        self.assertEqual(naive.fold, std.fold)\n", "    def test_make_naive_zoneinfo_dst_transition(self):\n        # Test converting naive datetime at the DST transition\n\n        # During the transition, time like 2:00 occurs twice due to DST\n        transition = datetime.datetime(2023, 3, 12, 2, 0)\n\n        paris_dt_std = timezone.make_naive(transition.replace(fold=1), PARIS_ZI)\n        paris_dt_dst = timezone.make_naive(transition, PARIS_ZI)\n\n        self.assertEqual(paris_dt_std, datetime.datetime(2023, 3, 12, 2, 0))\n        self.assertEqual(paris_dt_dst, datetime.datetime(2023, 3, 12, 3, 0))\n\n", "    def test_localtime_zoneinfo_ambiguous(self):\n        dt = datetime.datetime(2015, 10, 25, 2, 30)\n        with self.assertRaises(pytz.AmbiguousTimeError):\n            timezone.localtime(dt, timezone=timezone.get_fixed_timezone(2))\n        with self.assertRaises(pytz.AmbiguousTimeError):\n            timezone.localtime(dt, timezone=PARIS_ZI)\n", "    def test_localtime(self):\n        naive = datetime.datetime(2015, 1, 1, 0, 0, 1)\n        with self.assertRaisesMessage(ValueError, 'localtime() cannot be applied to a naive datetime'):\n            timezone.localtime(naive)\n        aware = datetime.datetime(2015, 1, 1, 0, 0, 1, tzinfo=ICT)\n        self.assertEqual(timezone.localtime(aware, timezone=EAT), aware.astimezone(EAT))\n        with timezone.override(EAT):\n            self.assertEqual(timezone.localtime(aware), aware.astimezone(EAT))\n\n        with mock.patch('django.utils.timezone.now', return_value=aware):\n            self.assertEqual(timezone.localtime(timezone=EAT), aware.astimezone(EAT))\n            with timezone.override(EAT):\n                self.assertEqual(timezone.localtime(), aware.astimezone(EAT))\n"], "sample_817": ["compilation error", "def test_sparse_variance_threshold():\n    # Test VarianceThreshold with sparse matrix and variance threshold.\n    X = csr_matrix([[1, 0, 1], [0, 0, 0], [1, 0, 1]])\n    selector = VarianceThreshold(threshold=0.1)\n    X_trans = selector.fit_transform(X)\n    assert_array_equal([0, 2], selector.get_support(indices=True))\n    assert_equal((3, 2), X_trans.shape)\n    assert_equal(csr_matrix([[1, 1], [0, 0], [1, 1]]), X_trans)\n\n", "def test_sparse_zero_threshold():\n    # Test VarianceThreshold with zero threshold and sparse input.\n\n    X = csr_matrix([[0, 1, 0],\n                   [0, 0, 1],\n                   [1, 1, 0]])\n\n    selector = VarianceThreshold(threshold=0)\n    X_transformed = selector.fit_transform(X)\n    assert_array_equal(X.indices, X_transformed.indices)\n    assert_array_equal(X.indptr, X_transformed.indptr)\n    assert_array_equal(X.data, X_transformed.data)\n\n", "    def test_sparse_variance_threshold():\n        # Test VarianceThreshold with sparse matrices and custom variance\n        X = csr_matrix(data)\n        selector = VarianceThreshold(threshold=0.5)\n        Xt = selector.fit_transform(X)\n        assert_equal(Xt.shape[1], 2)\n        assert_array_equal(selector.variances_,\n                           X.power(2).sum(axis=0).A1 / X.shape[0])\n", "compilation error"], "sample_501": ["compilation error", "    def test_legend_handler_map():\n        fig, ax = plt.subplots()\n        ax.plot([0, 1], [0, 1], label='line')\n        ax.scatter([0, 1], [0, 1], label='scatter')\n\n        handler_map = {\n            'line': mlegend.HandlerLine2D()\n        }\n\n        leg = ax.legend(handler_map=handler_map)\n        assert isinstance(leg.legendHandles[0], mlines.Line2D)\n\n        with pytest.raises(KeyError):\n            handler_map = {\n                'wrong_label': mlegend.HandlerLine2D()\n            }\n            leg = ax.legend(handler_map=handler_map)\n\n", "    def test_legend_handler_map():\n        fig, ax = plt.subplots()\n        ax.plot([0, 1], [0, 1], label='line')\n        ax.scatter([0, 1], [0, 1], label='scatter')\n        \n        handler_map = {'scatter': HandlerScatter(), 'line': HandlerLine2D()}\n        leg = ax.legend(handler_map=handler_map)\n        assert isinstance(leg.legendHandles[0], matplotlib.collections.PathCollection)\n        assert isinstance(leg.legendHandles[1], matplotlib.lines.Line2D)\n", "def test_legend_handler_error():\n    fig, ax = plt.subplots()\n    class BadHandler(mlegend.HandlerBase):\n                           xdescent, ydescent, width, height, fontsize,\n                           trans):\n            raise ValueError(\"Something bad happened!\")\n\n    with pytest.raises(ValueError, match=\"Something bad happened!\"):\n        ax.plot([1, 2], [3, 4], label='line')\n        ax.legend(handler_map={type(ax.lines[0]): BadHandler()})\n\n", "    def test_legend_handler_artist_properties(self):\n        fig, ax = plt.subplots()\n        ax.plot([0, 1], [0, 1], label='line', linewidth=5, color='red')\n        leg = ax.legend()\n        assert leg.legendHandles[0].get_linewidth() == 5\n        assert leg.legendHandles[0].get_color() == 'red'\n"], "sample_657": ["    def test_parametrize_markers_with_empty_string_id(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.parametrize('foo', ['a', 'b', 'c'], ids=['one', '', 'three'])\n                assert foo\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines(['*test_something[one]*', '*test_something[three]*'])\n", "    def test_pytest_param_id_allows_empty_string(self):\n        assert pytest.param(id=\"\")\n", "    def test_marker_id_for_parametrize(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.parametrize(\"input,expected\", [\n                pytest.param(1, 2, id=\"test1\"),\n                pytest.param(2, 4, id=\"test2\"),\n            ])\n                assert input * 2 == expected\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.assert_outcomes(passed=2)\n        all_ids = [x for x in result.stdout.str().splitlines() if x.startswith(\"test_something \")]\n        assert all_ids == [\"test_something test1\", \"test_something test2\"]\n\n", "    def test_pytest_param_id_with_unicode(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n                assert request.param.id == '\u4f60\u597d'\n            \n            @pytest.mark.parametrize('x', ['hello', '\u4f60\u597d'], ids=['hello', '\u4f60\u597d'])\n                pass\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        result.assert_outcomes(passed=2)\n", "    def test_pytest_param_id_doesnt_crash_if_no_id(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.parametrize(\"i\", range(2), ids=[\"a\", None])\n                pass\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        assert result.ret == 0\n"], "sample_748": ["    def test_grid_search_warn_if_n_jobs_is_not_int():\n        # Verify warning is raised when n_jobs is not an integer.\n\n        X, y = make_classification(n_samples=20, n_features=10, random_state=0)\n        clf = SVC(gamma='scale')\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(\"always\")\n            GridSearchCV(clf, {}, n_jobs='invalid', cv=2).fit(X, y)\n            assert len(w) == 1\n            assert \"n_jobs should be an integer\" in str(w[0].message)\n", "    def test_gridsearchcv_with_pipeline_and_validation_fraction(self):\n        # Test GridSearchCV with a Pipeline and validation_fraction\n        from sklearn.pipeline import Pipeline\n        from sklearn.preprocessing import StandardScaler\n\n        pipe = Pipeline([('scaler', StandardScaler()),\n                          ('svc', SVC())])\n        param_grid = {'svc__C': [0.1, 1.0, 10.0]}\n\n        grid_search = GridSearchCV(pipe, param_grid, cv=5,\n                                   validation_fraction=0.2,\n                                   n_jobs=1, verbose=1)\n        grid_search.fit(X, y)\n\n        assert hasattr(grid_search, 'best_estimator_')\n        assert hasattr(grid_search, 'best_params_')\n", "    def test_grid_search_with_sparse_data():\n        # Test grid search with sparse data input\n\n        X = sparse.csr_matrix(\n            [[1, 2, 0],\n             [0, 3, 4],\n             [5, 0, 6]]\n        )\n        y = [0, 1, 0]\n        est_parameters = {\"max_depth\": [1, 2, 3, 4]}\n\n        # Test with grid search cv\n        clf = DecisionTreeClassifier(random_state=0)\n        grid_search = GridSearchCV(clf, est_parameters, cv=KFold(random_state=0))\n        grid_search.fit(X, y)\n\n        # Test with randomized search cv\n        random_search = RandomizedSearchCV(clf, est_parameters, cv=KFold(\n            random_state=0), n_iter=3)\n        random_search.fit(X, y)\n", "compilation error", "def test_grid_search_njobs_in_parallel():\n    X, y = make_classification(random_state=0)\n    clf = SVC(gamma='scale')\n    grid_search = GridSearchCV(clf, {'C': [0.1, 1, 10]}, n_jobs=2)\n    grid_search.fit(X, y)\n    assert grid_search.n_jobs == 2\n\n\n"], "sample_473": ["    def test_get_script_name_empty(self):\n        script_name = get_script_name({\"SCRIPT_URL\": \"/\"})\n        self.assertEqual(script_name, \"/\")\n", "    def test_script_name_from_settings(self):\n        with override_settings(FORCE_SCRIPT_NAME=\"/myscriptname/\"):\n            self.assertEqual(\n                get_script_name({\"SCRIPT_URL\": \"/foobar/\"}), \"/myscriptname/\"\n            )\n", "    def test_script_name_with_query_string(self):\n        script_name = get_script_name({\"SCRIPT_URL\": \"/foobar/?query=string\"})\n        self.assertEqual(script_name, \"/foobar\")\n\n", "    def test_force_script_name(self):\n        response = self.client.get(\"/regular/\")\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.content, b\"regular content\")\n        self.assertEqual(response.context['request_path'], \"/base_path/regular/\")\n", "    def test_force_script_name(self):\n        response = self.client.get(\"/regular/\")\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.context[\"request_path\"], \"/prefix/regular/\")\n"], "sample_497": ["    def test_engformatter_rounding_cornercases(self):\n        formatter = mticker.EngFormatter()\n        # Test rounding behavior for values near EngFormatter thresholds.\n        assert formatter(999.9) == '1 k'\n        assert formatter(999.99) == '1 k'\n        assert formatter(999.999) == '1 k'\n        assert formatter(999.9999) == '1 k'\n", "    def test_EngFormatter_unicode_minus(self):\n        # Test unicode minus in EngFormatter\n        temp_form = mticker.EngFormatter()\n        temp_form.axis = FakeAxis()\n\n        plt.rcParams['axes.unicode_minus'] = True\n        assert temp_form(-1234.56789) == '\\N{MINUS SIGN}1.23457 k'\n", "    def test_engformatter_rounding_modes(self):\n        # Test rounding modes for EngFormatter\n        fmt = mticker.EngFormatter()\n        assert fmt(1.23456789e-6, rounding='round') == '1.235 \u00b5'\n        assert fmt(1.23456789e-6, rounding='floor') == '1.234 \u00b5'\n        assert fmt(1.23456789e-6, rounding='ceil') == '1.235 \u00b5'\n", "    def test_tick_formats(self):\n        fig, ax = plt.subplots(1, 1)\n        ax.plot([1e3, 1e6, 1e9])\n        ax.set_xscale('log')\n        formatter = mticker.LogFormatter(labelOnlyBase=False)\n        ax.xaxis.set_major_formatter(formatter)\n        ax.xaxis.set_tick_params(which='major', labelrotation=45)\n        fig.canvas.draw()\n        ticks = ax.xaxis.get_ticklabels()\n        # Ensure there's at least one label with a scientific notation\n        for tick in ticks:\n            if 'e' in tick.get_text():\n                return\n\n        assert False, \"No scientific notation found in x-axis tick labels\"\n\n", "    def test_formatter_unicode_minus(self):\n        fig, ax = plt.subplots()\n        ax.plot([1, 2, 3], [1, 2, 3])\n        plt.rcParams['axes.unicode_minus'] = True\n        fmt = mticker.FormatStrFormatter('%-5.1f')\n        ax.yaxis.set_major_formatter(fmt)\n        fig.canvas.draw()\n        labels = [label.get_text() for label in ax.get_yticklabels()]\n        assert labels == ['\u22121.0', '\u22120.0', '1.0', '2.0', '3.0']\n"], "sample_422": ["    def test_empty_queryset(self):\n        with self.assertNumQueries(1):\n            authors = Author.objects.prefetch_related(\n                Prefetch(\"books\", queryset=Book.objects.none())\n            ).all()\n\n        for author in authors:\n            with self.subTest(author=author):\n                self.assertEqual(list(author.books.all()), [])\n", "    def test_prefetch_related_ordering(self):\n        with self.assertNumQueries(2):\n            authors = list(\n                Author.objects.prefetch_related(\n                    Prefetch(\"books\", queryset=Book.objects.order_by(\"title\"),),\n                ).order_by(\"name\")\n            )\n        for author in authors:\n            self.assertEqual(list(author.books.all()), list(author.books.order_by(\"title\")))\n\n", "    def test_foreignkey_forward_limit(self):\n        books = Book.objects.all()\n        with self.assertNumQueries(3):\n            authors = list(\n                Author.objects.prefetch_related(\n                    Prefetch(\"first_book\", books),\n                    Prefetch(\"first_book\", books[1:2], to_attr=\"first_book_sliced\"),\n                )\n            )\n        for author in authors:\n            with self.subTest(author=author):\n                self.assertEqual(author.first_book_sliced, list(author.first_book.all())[1:2])\n\n\n", "    def test_prefetch_related_with_queryset_filtering(self):\n        books = Book.objects.filter(title__contains=\"Poems\")\n        with self.assertNumQueries(2):\n            authors = Author.objects.prefetch_related(\n                Prefetch(\"books\", queryset=books)\n            )\n\n        self.assertEqual(authors[0].books.all()[0].title, \"Poems\")\n\n", "    def test_prefetch_limit_nested(self):\n        authors = Author.objects.order_by(\"name\")[:2]\n        with self.assertNumQueries(3):\n            books = list(\n                Book.objects.prefetch_related(\n                    Prefetch(\n                        \"first_time_authors\",\n                        queryset=Author.objects.prefetch_related(\n                                    Prefetch(\"addresses\",  to_attr=\"sliced_addresses\")\n                                ),\n                        to_attr=\"sliced_authors\",\n                    )\n                )\n            )\n\n        for book in books:\n\n            with self.subTest(book=book):\n                self.assertEqual(book.sliced_authors.count(), 2)\n                for author in book.sliced_authors.all():\n                    self.assertEqual(author.sliced_addresses.count(), 0)\n"], "sample_753": ["    def test_logistic_regression_predict_proba_sparse_input():\n        # Test predict_proba with sparse input\n        X, y = make_classification(n_samples=50, n_features=20, random_state=0)\n        X_sparse = sparse.csr_matrix(X)\n\n        clf = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n        clf.fit(X, y)\n        proba_dense = clf.predict_proba(X)\n\n        clf.fit(X_sparse, y)\n        proba_sparse = clf.predict_proba(X_sparse)\n\n        assert_array_almost_equal(proba_dense, proba_sparse)\n", "    def test_logreg_lbfgs_warm_start_convergence():\n        # Check that warm start with L-BFGS solver correctly handles\n        # convergence when the solution hasn't changed.\n        X, y = make_classification(n_samples=50, n_features=20,\n                                   random_state=0)\n        lr = LogisticRegression(solver='lbfgs', warm_start=True,\n                                max_iter=1)\n        lr.fit(X, y)\n        initial_coef = lr.coef_.copy()\n        lr.fit(X, y)\n        assert_array_almost_equal(lr.coef_, initial_coef)\n", "    def test_decision_function_shape():\n        # Test that the shape of the decision function is correct for\n        # different input shapes and multi_class options.\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        y = np.array([0, 1, 0])\n\n        # Binary case\n        clf = LogisticRegression(multi_class='ovr', solver='lbfgs')\n        clf.fit(X, y)\n        decision_function = clf.decision_function(X)\n        assert_equal(decision_function.shape, (3,))\n\n        # Multinomial case\n        clf = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n        clf.fit(X, y)\n        decision_function = clf.decision_function(X)\n        assert_equal(decision_function.shape, (3, 2))\n\n", "    def test_logistic_regression_sparse_data_with_class_weight():\n        # Test that class weights are applied correctly to sparse data\n        X, y = make_classification(n_samples=50, n_features=20, random_state=0,\n                                   n_classes=3, n_informative=10)\n\n        # Convert to sparse matrix\n        X = sparse.csr_matrix(X)\n\n        # Create a class weight dictionary\n        class_weight = {0: 1, 1: 2, 2: 3}\n\n        # Fit logistic regression with class weights\n        clf = LogisticRegression(class_weight=class_weight, solver='lbfgs')\n        clf.fit(X, y)\n\n        # Check that the coefficients for each class are scaled appropriately\n        # according to the class weights.\n        coef = clf.coef_\n        assert_array_almost_equal(coef[0], coef[1] * 2)\n        assert_array_almost_equal(coef[0], coef[2] * 3)\n", "def test_penalty_l1_l2_consistency():\n    # Test that l1 and l2 penalties give consistent results when no\n    # regularization is applied\n    X, y = make_classification(n_samples=50, n_features=20, random_state=0)\n    lr_l1 = LogisticRegression(penalty=\"l1\", C=1e6, solver='saga',\n                               fit_intercept=False, max_iter=1000, tol=1e-10)\n    lr_l2 = LogisticRegression(penalty=\"l2\", C=1e6, solver='saga',\n                               fit_intercept=False, max_iter=1000, tol=1e-10)\n    lr_l1.fit(X, y)\n    lr_l2.fit(X, y)\n    assert_array_almost_equal(lr_l1.coef_, lr_l2.coef_)\n"], "sample_738": ["    def test_vectorizer_empty_vocabulary():\n        vect = CountVectorizer()\n        assert_raise_message(ValueError, \"Vocabulary cannot be empty\",\n                             vect.fit_transform, [])\n", "    def test_empty_vocabulary():\n        vect = CountVectorizer(vocabulary=[])\n        assert_raises(ValueError, vect.fit, ALL_FOOD_DOCS)\n", "def test_vectorizer_empty_vocabulary():\n    vect = CountVectorizer()\n    assert_raises(ValueError, vect.fit, [])\n    assert_raises(ValueError, vect.transform, [])\n", "    def test_vectorizer_empty_vocabulary():\n        # Test that CountVectorizer and TfidfVectorizer handle\n        # empty vocabularies gracefully\n        for Vectorizer in [CountVectorizer, TfidfVectorizer]:\n            vect = Vectorizer()\n            X = vect.fit_transform([])\n            assert_equal(X.shape, (0, 0))\n            assert_equal(vect.vocabulary_, {})\n", "    def test_countvectorizer_invalid_preprocessor_input():\n        message = \"The preprocessor should be a callable, got None instead.\"\n\n        vect = CountVectorizer(preprocessor=None)\n        assert_raise_message(TypeError, message, vect.fit, ALL_FOOD_DOCS)\n"], "sample_376": ["    def test_empty_message_storage(self):\n        storage = self.get_storage()\n        response = self.get_response()\n        self.assertEqual(self.stored_messages_count(storage, response), 0)\n        storage.update(response)\n        self.assertEqual(self.stored_messages_count(storage, response), 0)\n", "    def test_empty_cookie(self):\n        storage = self.storage_class(self.get_request())\n        response = self.get_response()\n        # No messages yet, no cookie should be set.\n        self.assertFalse(response.cookies.get(storage.cookie_name))\n\n        storage = self.storage_class(self.get_request())\n        response = self.get_response()\n        set_cookie_data(storage, [], encode_empty=True)\n\n        self.assertDictContainsSubset(\n            {'messages': {'domain': '.example.com',\n                          'httponly': True,\n                          'secure': True,\n                          'samesite': 'Strict',\n                          'max-age': None}},\n            response.cookies.as_dict()\n        )\n", "    def test_not_finished_sentinel(self):\n        storage = self.get_storage()\n        response = self.get_response()\n\n        # Create a message list that will fill the cookie.\n        msg_size = int((CookieStorage.max_cookie_size - 54) / 4.5 - 37)\n        for i in range(4):\n            storage.add(constants.INFO, get_random_string(msg_size))\n        storage.update(response)\n        # Assert that cookie contains messages and not_finished sentinel\n        self.assertEqual(self.stored_messages_count(storage, response), 4)\n\n        # Add one more message, exceeding the cookie size.\n        storage.add(constants.INFO, get_random_string(msg_size))\n        storage.update(response)\n        self.assertEqual(self.stored_messages_count(storage, response), 4)\n\n        # Retrieve and assert that messages are retrieved correctly.\n        retrieved_messages = list(storage)\n        self.assertEqual(len(retrieved_messages), 4)\n", "    def test_empty_cookie(self):\n        request = self.get_request()\n        storage = self.storage_class(request)\n        set_cookie_data(storage, [], encode_empty=True)\n        # The message actually contains what we expect.\n        self.assertEqual(list(storage), [])\n\n", "    def test_message_storage_empty(self):\n        \"\"\"\n        Test that messages are properly stored and retrieved from\n        an empty cookie.\n        \"\"\"\n        storage = self.get_storage()\n        response = self.get_response()\n        storage.add(constants.INFO, 'test')\n        storage.update(response)\n        self.assertEqual(storage._decode(response.cookies['messages'].value),\n                         [Message(constants.INFO, 'test')])\n"], "sample_795": ["    def test_check_class_weight_balanced_linear_classifier():\n        # Test that check_estimator works with balanced class weights\n        # for linear classifier.\n\n        check_class_weight_balanced_linear_classifier(\n            \"BadBalancedWeightsClassifier\"\n        )\n", "def test_check_class_weight_balanced_linear_classifier():\n    # check that check_class_weight_balanced_linear_classifier\n    # raises an error for estimators that don't support balanced weights\n    msg = (\"Estimator does not support 'balanced' class_weight\"\n           \" but pass check_class_weight_balanced_linear_classifier\")\n    assert_raises_regex(AssertionError, msg,\n                        check_class_weight_balanced_linear_classifier,\n                        BadBalancedWeightsClassifier)\n", "    def test_check_class_weight_balanced_linear_classifier():\n        # check balanced class weight usage for linear classifiers\n        msg = \"Estimator doesn't support balanced class weights\"\n        assert_raises_regex(AssertionError, msg,\n                            check_class_weight_balanced_linear_classifier,\n                            BadBalancedWeightsClassifier)\n", "compilation error", "    def test_check_class_weight_balanced_linear_classifier():\n        # Test if check_class_weight_balanced_linear_classifier correctly\n        # identifies estimators that do not support balanced class weights.\n        \n        # Create a dummy estimator that raises an error\n        # when 'class_weight' is set to 'balanced'.\n\n        class BrokenBalancedClassifier(BaseBadClassifier):\n                self.class_weight = 'balanced'\n                if self.class_weight == 'balanced':\n                    raise ValueError(\"Estimator does not support \"\n                                     \"'balanced' class weight\")\n                return self\n\n        # Assert that the check raises an error for the broken estimator.\n        assert_raises_regex(\n            AssertionError,\n            \"Estimator does not support 'balanced' class_weight\",\n            check_class_weight_balanced_linear_classifier,\n            BrokenBalancedClassifier())\n        \n        # Now check with a regular estimator\n\n\n        check_class_weight_balanced_linear_classifier(AdaBoostClassifier())\n\n"], "sample_28": ["    def test_header_to_string():\n        h = fits.Header()\n        h[\"HISTORY\"] = \"This is a test header\"\n        h[\"DATE\"] = \"2023-10-26\"\n        h.add_comment(\"A comment\")\n\n        s = h.tostring()\n        assert \"HISTORY\" in s\n        assert \"DATE\" in s\n\n        h2 = fits.Header.fromstring(s)\n\n        assert h2[\"HISTORY\"] == h[\"HISTORY\"]\n        assert h2[\"DATE\"] == h[\"DATE\"]\n\n        # Check that comments are preserved\n        assert h2.comments[\"HISTORY\"] == \"no comment\"\n        assert h2.comments[\"DATE\"] == \"no comment\"\n", "    def test_rvkc_after_append(self):\n        \"\"\"Ensure appending a card after an existing RVKC works correctly.\"\"\"\n        h = fits.Header()\n        h.append(('DP1', 'NAXIS: 2'))\n        h.append(('DP1', 'AXIS.1: 1'), after='DP1.NAXIS')\n        assert h['DP1.NAXIS'] == 2.0\n        assert h['DP1.AXIS.1'] == 1.0\n", "    def test_header_from_cards(self):\n        cards = [\n            fits.Card(\"SIMPLE\", True),\n            fits.Card(\"BITPIX\", 8),\n            fits.Card(\"NAXIS\", 2),\n            fits.Card(\"NAXIS1\", 100),\n            fits.Card(\"NAXIS2\", 100),\n        ]\n        h = fits.Header.from_cards(cards)\n        assert h[\"SIMPLE\"] == True\n        assert h[\"BITPIX\"] == 8\n        assert h[\"NAXIS\"] == 2\n        assert h[\"NAXIS1\"] == 100\n        assert h[\"NAXIS2\"] == 100\n", "    def test_set_and_get_rvkc_with_empty_string_value(self):\n        \"\"\"\n        Test setting and getting an RVKC with an empty string value.\n        \"\"\"\n\n        header = fits.Header()\n        header.set(\"FOO\", \"BAR: \", comment=\"A comment\")\n        assert header[\"FOO.BAR\"] == \"\"\n        assert header.cards[0].value == \"\"\n", "    def test_rvkc_update_after_delete(self):\n        \"\"\"\n        Test updating a RVKC after deleting another RVKC in the same\n        HDU.\n        \"\"\"\n        h = fits.Header()\n        h[\"DP1\"] = \"NAXIS: 2\"\n        h[\"DP1\"] = \"AXIS.1: 1\"\n        h[\"DP1\"] = \"AXIS.2: 2\"\n        del h[\"DP1.AXIS.1\"]\n        h[\"DP1.AXIS.2\"] = 3\n        assert h[\"DP1.AXIS.2\"] == 3\n\n"], "sample_342": ["    def test_filter_to_field_default_value(self):\n        class DefaultAuthorAdmin(AuthorAdmin):\n                return Author.objects.filter(id=1)\n        \n        with model_admin(Author, DefaultAuthorAdmin):\n            self.selenium.get(self.live_server_url + reverse('autocomplete_admin:admin_views_book_add'))\n            rows = self.selenium.find_elements_by_css_selector('.dynamic-authorship_set')\n            authors = rows[0].find_elements_by_css_selector('.select2-selection__rendered')\n            self.assertEqual(authors[0].text, 'Author object (1)')\n\n", "    def test_autocomplete_with_custom_to_field(\n        self", "    def test_autocomplete_field_with_custom_ordering(self):\n        class OrderedQuestionAdmin(QuestionAdmin):\n            ordering = ['-posted']\n        Question.objects.create(question='Question 1', posted=datetime.date(2021, 8, 7))\n        Question.objects.create(question='Question 2', posted=datetime.date(2021, 8, 9))\n        request = self.factory.get(self.url, {'term': 'question', **self.opts})\n        request.user = self.superuser\n        with model_admin(Question, OrderedQuestionAdmin):\n            response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n        self.assertEqual(response.status_code, 200)\n        data = json.loads(response.content.decode('utf-8'))\n        self.assertEqual(data, {\n            'results': [\n                {'id': str(q.pk), 'text': q.question}\n                for q in Question.objects.order_by('-posted')\n            ],\n            'pagination': {'more': False},\n        })\n\n", "    def test_field_not_allowed_permission_denied(self):\n        request = self.factory.get(self.url, {'term': 'is', **self.opts, 'field_name': 'question_not_allowed'})\n        request.user = self.superuser\n        with self.assertRaises(PermissionDenied):\n            AutocompleteJsonView.as_view(**self.as_view_args)(request)\n\n", "    def test_empty_search_field(self):\n        self.selenium.get(self.live_server_url + reverse('autocomplete_admin:admin_views_answer_add'))\n        elem = self.selenium.find_element_by_css_selector('.select2-selection')\n        elem.click()\n        results = self.selenium.find_element_by_css_selector('.select2-results')\n        self.assertTrue(results.is_displayed())\n        option = self.selenium.find_element_by_css_selector('.select2-results__option')\n        self.assertEqual(option.text, 'No results found')\n"], "sample_19": ["def test_wcs_copy():\n    w = wcs.WCS(naxis=2)\n    w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\"]\n    w.wcs.crval = [10, 20]\n    w_copy = w.copy()\n    assert w_copy.wcs.ctype == w.wcs.ctype\n    assert w_copy.wcs.crval == w.wcs.crval\n    w.wcs.crval = [30, 40]\n    assert w_copy.wcs.crval != w.wcs.crval\n\n", "def test_time_axis_with_sip():\n    \"\"\"Test that temporal WCS is correctly extracted when there is also SIP.\"\"\"\n    hdr_name = get_pkg_data_filename(\"data/sip.fits\")\n    with fits.open(hdr_name) as hdul:\n        w = wcs.WCS(hdul[0].header)\n    assert w.has_temporal\n    assert w.sub([wcs.WCSSUB_TIME]).is_temporal\n", "    def test_no_time_axis_selection():\n        w = wcs.WCS(naxis=2)\n        w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\"]\n        w.wcs.set()\n        assert not w.has_temporal\n        with pytest.raises(AttributeError):\n            w.sub([wcs.WCSSUB_TIME])\n        \n", "def test_time_axis_selection_with_sip():\n    w = wcs.WCS(naxis=3)\n    w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\", \"TIME\"]\n    w.wcs.set_sip()\n    assert list(w.sub([wcs.WCSSUB_TIME]).wcs.ctype) == [\"TIME\"]\n    assert (\n        w.wcs_pix2world([[1, 2, 3]], 0)[0, 2]\n        == w.sub([wcs.WCSSUB_TIME]).wcs_pix2world([[3]], 0)[0, 0]\n    )\n", "def test_wcslib_version():\n    assert _WCSLIB_VER is not None\n    assert isinstance(_WCSLIB_VER, Version)\n"], "sample_118": ["    def test_nested_outerref_rhs(self):\n        tag=Tag.objects.create(name='foo')\n        tag.articles.add(self.a1)\n        qs = Article.objects.annotate(\n            author_alias_exists=Exists(\n                Author.objects.filter(alias=OuterRef('headline'))\n            ),\n        )\n        self.assertEqual(\n            qs.filter(author_alias_exists=True).first(),\n            self.a1,\n        )\n", "    def test_nested_outerref_rhs(self):\n        tag = Tag.objects.create(name=self.au1.alias)\n        tag.articles.add(self.a1)\n        qs = Article.objects.annotate(\n            author_alias_match=Exists(\n                Author.objects.filter(\n                    alias=OuterRef('author__alias')\n                )\n            ),\n        )\n        self.assertEqual(qs.get(author_alias_match=True), self.a1)\n", "compilation error", "    def test_nested_outerref_rhs(self):\n        tag = Tag.objects.create(name=self.au1.alias)\n        tag.articles.add(self.a1)\n        qs = Article.objects.annotate(\n            author_alias_match=Exists(\n                Author.objects.filter(\n                    alias=OuterRef('author__alias')\n                )\n            ),\n        ).filter(author_alias_match=True)\n        self.assertEqual(qs.count(), 1)\n", "    def test_nested_outerref_rhs(self):\n        tag = Tag.objects.create(name=self.au1.alias)\n        tag.articles.add(self.a1)\n        qs = Article.objects.annotate(\n            author_alias_match=Exists(\n                Author.objects.filter(alias=OuterRef('tags__name'))\n            ),\n        )\n        self.assertTrue(qs.get(pk=self.a1.pk).author_alias_match)\n"], "sample_305": ["    def test_select_related_with_annotation(self):\n        qs = Charlie.objects.select_related('bravo').annotate(\n            Count('bravo__name')\n        ).filter(bravo__name='Bravo Name')\n        self.assertEqual(len(qs), 1)\n", "    def test_ticket_21150_related_name(self):\n        b = Bravo.objects.create()\n        c = Charlie.objects.create(bravo=b)\n        qs = Charlie.objects.select_related('alfa').annotate(Count('bravo__charlie'))\n        self.assertSequenceEqual(qs, [c])\n        self.assertIs(qs[0].alfa, None)\n        a = Alfa.objects.create()\n        c.alfa = a\n        c.save()\n        # Force re-evaluation\n        qs = qs.all()\n        self.assertSequenceEqual(qs, [c])\n        self.assertEqual(qs[0].alfa, a)\n", "    def test_select_related_after_annotate(self):\n        qs = Charlie.objects.annotate(Count('bravo')).select_related('alfa')\n        self.assertIn('LEFT OUTER JOIN ', str(qs.query))\n", "    def test_join_promotion_with_having(self):\n        qs = Book.objects.annotate(total_authors=Count('authors')).filter(total_authors__gt=1).having(total_authors__gt=1).order_by('name')\n        self.assertQuerysetEqual(\n            qs,\n            ['Artificial Intelligence: A Modern Approach', 'Python Web Development with Django',\n             'The Definitive Guide to Django: Web Development Done Right'],\n            lambda b: b.name)\n", "    def test_aggregate_with_union(self):\n        qs1 = Author.objects.filter(name__startswith='A').values('id')\n        qs2 = Author.objects.filter(name__startswith='P').values('id')\n        qs = qs1 | qs2\n        qs = qs.annotate(num_books=Subquery(Book.objects.filter(authors__in=OuterRef('id')).values('id').count()))\n        self.assertQuerysetEqual(\n            qs.order_by('id'),\n            [self.a1.pk, self.a2.pk, self.a3.pk] ,\n            lambda a: a\n        )\n\n"], "sample_1026": ["    def test_lambdify_Derivative():\n        # test derivatives\n        x = sympy.Symbol('x')\n        f = sympy.Function('f')(x)\n        df = f.diff()\n        f_expr = lambdify(x, f)\n        df_expr = lambdify(x, df)\n        assert f_expr(1) == f.subs(x, 1)\n        assert df_expr(1) == df.subs(x, 1)\n", "def test_lambdify_MatrixSymbol_issue_16630():\n    A = MatrixSymbol('A', 2, 2)\n    f = lambdify(A, A*A)\n    mat = numpy.array([[1, 2], [3, 4]])\n    assert (f(mat) == mat @ mat).all()\n", "compilation error", "compilation error", "compilation error"], "sample_925": ["    def test_mock_module_attributes():\n        modname = 'sphinx.unknown'\n        with mock([modname]):\n            imported_module = import_module(modname)\n            assert hasattr(imported_module, '__file__')\n            assert hasattr(imported_module, '__path__')\n            assert hasattr(imported_module, '__all__')\n", "    def test_mock_decorator_attribute():\n        mock = _MockObject()\n        \n        class MyClass:\n            @mock.attr_deco\n                \"\"\"docstring\"\"\"\n                pass\n        \n        obj = MyClass()\n        assert isinstance(obj.func, _MockObject) \n        assert obj.func.__doc__ == 'docstring'\n", "    def decorated():\n        pass\n", "    def test_mock_decorator_call():\n        mock = _MockObject()\n\n        @mock.function_deco\n            return \"something\"\n\n        assert func() == \"something\"\n", "    def test_mock_decorator_passthrough():\n        mock = _MockObject()\n\n        @mock.function_deco\n            return arg * 2\n\n        assert func(5) == 10\n"], "sample_480": ["    def test_key_transform_raw_sql(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.annotate(\n                expr=RawSQL(\n                    self.raw_sql,\n                    ['{\"x\": {\"a\": \"b\", \"c\": 1, \"d\": \"e\"}}'],\n                ),\n                key=KeyTransform(\"x\", \"expr\"),\n            ).filter(key__a=\"b\"),\n            [self.objs[7]],\n        )\n", "    def test_key_transform_with_subquery(self):\n        obj = NullableJSONModel.objects.create(\n            value={\"k\": {\"l\": \"m\", \"nested\": {\"a\": \"b\"}}}\n        )\n        subquery = (\n            NullableJSONModel.objects.filter(pk=OuterRef(\"pk\"))\n            .values(\"value__k__l\")\n            .annotate(nested_a=KeyTransform(\"a\", \"value__k__nested\"))\n        )\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.annotate(subquery=subquery).filter(\n                subquery__nested_a=\"b\"\n            ),\n            [obj],\n        )\n", "    def test_key_transform_with_complex_expression(self):\n        obj = NullableJSONModel.objects.create(\n            value={\n                \"a\": {\"b\": [{\"c\": 1}, {\"c\": 2}]},\n                \"d\": 1,\n            }\n        )\n        self.assertEqual(\n            NullableJSONModel.objects.filter(\n                value__a__b__1__c__gt=F(\"value__d\")\n            ).get(),\n            obj,\n        )\n\n", "    def test_key_transform_on_empty_json(self):\n        obj = NullableJSONModel.objects.create(value={})\n        self.assertIsNone(\n            NullableJSONModel.objects.filter(\n                value__foo=KeyTransform(\"foo\", \"value\")\n            ).get()\n        )\n", "    def test_key_transform_on_subquery(self):\n        subquery = (\n            NullableJSONModel.objects.filter(value__d__1__isnull=False)\n            .values(\"pk\", \"value__d__1__f\")\n        )\n        qs = NullableJSONModel.objects.annotate(\n            subquery_value=Subquery(subquery),\n            key=KT(\"subquery_value__f\"),\n        ).filter(key=\"g\")\n        self.assertSequenceEqual(qs, [self.objs[4]])\n"], "sample_372": ["    def test_include_with_dynamic_namespace(self):\n        class DynamicNamespaceModule:\n                self.namespace = 'dynamic_namespace'\n\n            urlpatterns = [\n                path('dynamic/', views.empty_view, name='dynamic-view'),\n            ]\n        \n        dynamic_module = DynamicNamespaceModule()\n        self.assertEqual(\n            include(dynamic_module.urlpatterns, namespace=dynamic_module.namespace),\n            (dynamic_module.urlpatterns, None, 'dynamic_namespace')\n        )\n", "    def test_duplicate_namespaced_url_patterns(self):\n        with self.assertRaises(ImproperlyConfigured) as e:\n            reverse('duplicate-name-1')\n        self.assertIn(\"The URL pattern 'duplicate-name-1' in namespace 'duplicate_namespace' is duplicate.\", str(e.exception))\n", "    def test_include_without_app_name(self):\n        msg = (\n            'Specifying a namespace without providing an '\n            'app_name is not supported.'\n        )\n        with self.assertRaisesMessage(ImproperlyConfigured, msg):\n            include(self.url_patterns, namespace='namespace')\n", "    def test_invalid_reverse(self):\n        test_urls = [\n            ('lookahead-positive', {'city': 'not-a-city'}),\n            ('lookahead-negative', {'city': 'not-a-city'}),\n            ('lookbehind-positive', {'city': 'other-city'}),\n            ('lookbehind-negative', {'city': 'other-city'}),\n        ]\n        for name, kwargs in test_urls:\n            with self.subTest(name=name, kwargs=kwargs):\n                with self.assertRaises(NoReverseMatch):\n                    reverse(name, kwargs=kwargs)\n\n", "    def test_reverse_with_non_string_kwargs_raises_TypeError(self):\n        msg = \"Keyword arguments must be strings. Got: 1\"\n        with self.assertRaisesMessage(TypeError, msg):\n            reverse('lookahead-positive', kwargs={'city': 1})\n"], "sample_915": ["    def test_getdoc_inherited_decorated_method_with_doc_in_parent(app):\n        from target.methods import Inherited\n\n        assert inspect.getdoc(Inherited.meth) == \"docstring from parent\" \n\n", "    def test_getdoc_inherited_decorated_method_with_docstring_override(app):\n        from target.methods import Subclass\n        assert inspect.getdoc(Subclass.meth) == 'overridden docstring'\n\n", "    def wrapped_meth(self):\n        pass\n", "    def test_getdoc_inherited_decorated_method_descriptor(app):\n        from target.methods import SubFoo\n\n        doc = inspect.getdoc(SubFoo.meth)\n        assert doc == \"docstring.\"\n", "    def test_getdoc_inherited_decorated_method_with_docstring(app):\n        from target.methods import Foo, SubFoo\n\n        assert inspect.getdoc(Foo.meth) == \"docstring.\"\n        assert inspect.getdoc(SubFoo.meth) == \"docstring.\"\n"], "sample_496": ["    def test_custom_settings_file(self):\n        args = ['runserver', '--settings=alternate_settings']\n        out, err = self.run_django_admin(args)\n        self.assertNoOutput(err)\n        self.assertOutput(out, 'Performing system checks...')\n", "    def test_invalid_output_format(self):\n        args = ['dumpdata', '--format=invalid_format']\n        out, err = self.run_manage(args)\n        self.assertNoOutput(out)\n        self.assertOutput(err, \"Error: Invalid output format 'invalid_format'.\")\n", "    def test_custom_settings(self):\n        self.write_settings('custom_settings.py', sdict={'SECRET_KEY': '\"test_key\"'})\n        self.addCleanup(self.remove_settings, 'custom_settings.py')\n        args = ['diffsettings', '--settings=custom_settings']\n        out, err = self.run_manage(args)\n        self.assertNoOutput(err)\n        self.assertOutput(out, \"SECRET_KEY = 'test_key'\")\n\n", "    def test_help_specific_command(self):\n        out, err = self.run_django_admin(['help', 'startapp'])\n        self.assertNoOutput(err)\n        self.assertOutput(out, 'Usage: django-admin startapp [OPTIONS] app_name')\n", "    def test_custom_project_template_without_project_name(self):\n        \"Make sure the startproject management command requires a project name when using a custom template\"\n        template_path = os.path.join(custom_templates_dir, 'project_template')\n        args = ['startproject', '--template', template_path]\n        testproject_dir = os.path.join(self.test_dir, 'customtestproject')\n        self.addCleanup(shutil.rmtree, testproject_dir, True)\n\n        out, err = self.run_django_admin(args)\n        self.assertOutput(err, \"Error: You must provide a project name.\")\n        self.assertFalse(os.path.exists(testproject_dir))\n"], "sample_45": ["        def test_time_kind(kind):\n            self.assertQuerysetEqual(\n                DTModel.objects.annotate(\n                    truncated=Trunc('start_time', kind, output_field=TimeField(), tzinfo=melb)\n                ).order_by('start_datetime'),\n                [\n                    (start_datetime, truncate_to(start_datetime.time(), kind)),\n                    (end_datetime, truncate_to(end_datetime.time(), kind))\n                ],\n                lambda m: (m.start_datetime, m.truncated)\n            )\n", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_663": ["def test_collect_sub_with_symlinks_and_double_symlinks(use_pkg, testdir):\n    sub = testdir.mkdir(\"sub\")\n    if use_pkg:\n        sub.ensure(\"__init__.py\")\n    sub.ensure(\"test_file.py\").write(\"def test_file(): pass\")\n\n    # Create a symlink to a symlink.\n    double_symlink = sub.join(\"test_double_symlink.py\")\n    double_symlink.mksymlinkto(sub.join(\"test_file.py\"))\n\n    result = testdir.runpytest(\"-v\", str(sub))\n    result.stdout.fnmatch_lines(\n        [\n            \"sub/test_file.py::test_file PASSED*\",\n            \"sub/test_double_symlink.py::test_file PASSED*\",\n            \"*2 passed in*\",\n        ]\n    )\n", "def test_collect_sub_with_symlinks_ignoring_broken(testdir):\n    sub = testdir.mkdir(\"sub\")\n    sub.ensure(\"__init__.py\")\n    sub.ensure(\"test_file.py\").write(\"def test_file(): pass\")\n\n    # Create a broken symlink.\n    sub.join(\"test_broken.py\").mksymlinkto(\"test_doesnotexist.py\")\n\n    # Symlink that gets collected.\n    sub.join(\"test_symlink.py\").mksymlinkto(\"test_file.py\")\n\n    result = testdir.runpytest(\"-v\", str(sub), \"--collect-only\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*collected 2 items*\",\n            \"sub/test_file.py\",\n            \"sub/test_symlink.py\",\n        ]\n    )\n    result.stdout.no_fnmatch_line(\"*test_broken*\")\n", "def test_collect_symlinks_recursive(testdir):\n    sub1 = testdir.mkdir(\"sub1\")\n    sub1.ensure(\"test_file.py\").write(\"def test_file(): pass\")\n    sub2 = testdir.mkdir(\"sub2\")\n    sub2.mksymlinkto(sub1)\n\n    result = testdir.runpytest(\"-v\", str(sub2))\n    result.stdout.fnmatch_lines(\n        [\n            \"sub1/test_file.py::test_file PASSED*\",\n            \"*1 passed in*\",\n        ]\n    )\n", "def test_collect_symlink_with_relative_path(testdir):\n    \"\"\"Test collecting a symlink pointing to a file within the same directory.\n    (issue #4325).\n    \"\"\"\n    real = testdir.makepyfile(\n        real=\"\"\"\n            assert request.node.nodeid == \"real.py::test_nodeid\"\n        \"\"\"\n    )\n    symlink = testdir.tmpdir.join(\"symlink.py\")\n    symlink.mksymlinkto(real.basename)\n    result = testdir.runpytest(\"-v\", symlink)\n    result.stdout.fnmatch_lines([\"real.py::test_nodeid PASSED*\", \"*1 passed in*\"])\n    assert result.ret == 0\n", "def test_collect_no_tests_in_subpackage(testdir):\n    \"\"\"Verify pytest exits with a specific code when no tests are found in a subpackage.\"\"\"\n    subdir = testdir.mkdir(\"sub\")\n    subdir.ensure(\"__init__.py\")\n\n    result = testdir.runpytest(\"sub\")\n    assert result.ret == ExitCode.NO_TESTS_COLLECTED\n"], "sample_387": ["    def test_empty_many_to_many(self):\n        from selenium.webdriver.common.by import By\n\n        self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_widgets_event_add\")\n        )\n\n        # Initially, the 'ManytoManyRawIdWidget' should be empty\n        self.assertEqual(\n            self.selenium.find_element(By.ID, \"id_supporting_bands\").get_attribute(\n                \"value\"\n            ),\n            \"\",\n        )\n\n        # Click the add button to open the popup window\n        self.selenium.find_element(By.ID, \"lookup_id_supporting_bands\").click()\n        self.wait_for_and_switch_to_popup()\n\n        # Click on the 'Cancel' button to return to the main window\n        self.selenium.find_element(By.CSS_SELECTOR, \".modal-footer button:last-of-type\").click()\n        self.selenium.switch_to.default_content()\n \n        # The field should still be empty\n        self.assertEqual(\n            self.selenium.find_element(By.ID, \"id_supporting_bands\").get_attribute(\n                \"value\"\n            ),\n            \"\",\n        )\n", "    def test_rawid_validation(self):\n        from selenium.webdriver.common.by import By\n        \n        self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_widgets_event_add\")\n        )\n\n        # Input a non-existent ID ---------------------------------------------\n        event_form = self.selenium.find_element(By.ID, \"event_form\")\n        self.selenium.find_element(By.ID, \"id_main_band\").send_keys(\"99999\")\n        event_form.submit()\n\n        self.assertEqual(\n            self.selenium.find_element(By.CSS_SELECTOR, \".errorlist li\").text,\n            \"Select a valid choice. That choice is not one of the available options.\",\n        )\n\n        # Input an empty ID ---------------------------------------------------\n        self.selenium.find_element(By.ID, \"id_main_band\").clear()\n        event_form.submit()\n        self.assertEqual(\n            self.selenium.find_element(By.CSS_SELECTOR, \".errorlist li\").text,\n            \"This field is required.\",\n        )\n\n", "    def test_rawid_search(self):\n        from selenium.webdriver.common.by import By\n        from selenium.webdriver.common.keys import Keys\n\n        self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_widgets_event_add\")\n        )\n\n        self.selenium.find_element(By.ID, \"lookup_id_main_band\").click()\n        self.wait_for_and_switch_to_popup()\n\n        search_box = self.selenium.find_element(By.ID, \"searchbar-textfield\")\n        search_box.clear()\n        search_box.send_keys(\"Bogey\")\n        search_box.send_keys(Keys.RETURN)\n\n        self.wait_for_text_in_element(\n            By.XPATH, '//a[contains(text(), \"Bogey Blues\")]', \"Bogey Blues\"\n        )\n\n        link = self.selenium.find_element(By.LINK_TEXT, \"Bogey Blues\")\n        self.assertIn(\"/band/42/\", link.get_attribute(\"href\"))\n        link.click()\n\n        self.selenium.switch_to.window(self.selenium.window_handles[0])\n        self.wait_for_value(\"#id_main_band\", \"42\")\n\n", "    def test_empty_input(self):\n        from selenium.webdriver.common.by import By\n\n        self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_widgets_event_add\")\n        )\n\n        # Clear the input field for the ForeignKey\n        self.selenium.find_element(By.ID, \"id_main_band\").clear()\n\n        # Save and check the database\n        with self.wait_page_loaded():\n            self.selenium.find_element(By.XPATH, '//input[@value=\"Save\"]').click()\n\n        event = Event.objects.get()\n        self.assertIsNone(event.main_band)\n\n        # Clear the input field for the ManyToManyField\n        self.selenium.find_element(By.ID, \"id_supporting_bands\").clear()\n\n        # Save and check the database\n        with self.wait_page_loaded():\n            self.selenium.find_element(By.XPATH, '//input[@value=\"Save\"]').click()\n\n        event = Event.objects.get()\n        self.assertEqual(event.supporting_bands.count(), 0)\n", "    def test_many_to_many_filtered(self):\n        from selenium.webdriver.common.by import By\n        from selenium.webdriver.common.keys import Keys\n\n        self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n        Band.objects.create(id=101, name=\"The Fuzzy Pickles\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_widgets_event_add\")\n        )\n        main_window = self.selenium.current_window_handle\n\n        # Open the popup window and filter the search\n\n        self.selenium.find_element(By.ID, \"lookup_id_supporting_bands\").click()\n        self.wait_for_and_switch_to_popup()\n\n        search_box = self.selenium.find_element(By.ID, \"searchbox\")\n        search_box.send_keys(\"Fuzzy\")\n        search_box.send_keys(Keys.ENTER)\n\n        self.wait_for_value(\"#searchfield\", \"Fuzzy\")\n\n        # Verify that only the expected band is displayed\n        self.assertEqual(\n            self.selenium.find_elements(By.CSS_SELECTOR, \"ul.filtered li\"), 1\n        )\n\n        link = self.selenium.find_element(By.LINK_TEXT, \"The Fuzzy Pickles\")\n\n        self.assertIn(\"/band/101/\", link.get_attribute(\"href\"))\n        link.click()\n\n        # The field now contains the selected band's id\n        self.selenium.switch_to.window(main_window)\n        self.wait_for_value(\"#id_supporting_bands\", \"101\")\n\n"], "sample_752": ["def test_iforest_set_params():\n    \"\"\"Check if setting parameters after instantiation works correctly.\"\"\"\n    X_train = iris.data[:50]\n\n    clf = IsolationForest()\n    clf.set_params(n_estimators=100, max_samples=0.5, contamination=0.2)\n    clf.fit(X_train)\n    assert_equal(clf.n_estimators, 100)\n    assert_equal(clf.max_samples, 0.5 * X_train.shape[0])\n    assert_equal(clf.contamination, 0.2)\n", "    def test_iforest_contamination_error():\n        \"\"\"Test that it gives proper exception on invalid contamination input.\"\"\"\n        X = iris.data\n        assert_raises(ValueError, IsolationForest, contamination=-0.1)\n        assert_raises(ValueError, IsolationForest, contamination=1.1)\n", "def test_iforest_contamination_parameter():\n    # Test that IsolationForest raises an error when contamination is not in [0, 0.5]\n    X = iris.data\n\n    # Test negative contamination value\n    assert_raises(ValueError, IsolationForest(contamination=-0.1).fit, X)\n    # Test contamination value greater than 0.5\n    assert_raises(ValueError, IsolationForest(contamination=0.6).fit, X)\n\n\n", "def test_iforest_contamination_warning():\n    X = iris.data\n    clf = IsolationForest(contamination=0.5).fit(X)\n    with pytest.warns(FutureWarning):\n        clf.predict(X) \n\n", "def test_iforest_n_estimators():\n    # test n_estimators\n    rng = check_random_state(0)\n    X = iris.data\n    for n_estimators in [1, 5, 10]:\n        clf = IsolationForest(n_estimators=n_estimators, random_state=rng).fit(X)\n        assert_equal(len(clf.estimators_), n_estimators)\n\n"], "sample_761": ["    def test_add_indicator_impute_strategy(self, strategy):\n        X = np.array([\n            [np.nan, 1,      5,       np.nan, 1],\n            [2,      np.nan, 1,       np.nan, 2],\n            [6,      3,      np.nan,  np.nan, 3],\n            [1,      2,      9,       np.nan, 4]\n        ])\n\n        imputer = SimpleImputer(missing_values=np.nan, add_indicator=True, strategy=strategy)\n        X_trans = imputer.fit_transform(X)\n\n        \n        assert X_trans.shape[1] == X.shape[1] * 2\n", "    def test_missing_indicator_feature_names(self, features):\n        X = np.array([['a', 1, 'b'],\n                      [1, 'b', 2],\n                      ['a', 2, 3]])\n\n        feature_names = ['f0', 'f1', 'f2']\n        mi = MissingIndicator(features=features, missing_values='a')\n        mi.fit(X, feature_names=feature_names)\n        assert mi.get_feature_names_out() == np.array([f'{f}_missing' for f in feature_names])[\n            mi.features_\n        ].tolist()\n\n", "    def test_simple_imputer_with_indicator(self, missing_values, strategy,\n                                          expected_value):\n        X = np.array([[1, missing_values],\n                      [2, 3],\n                      [missing_values, 4]])\n        imputer = SimpleImputer(missing_values=missing_values,\n                                strategy=strategy,\n                                add_indicator=True)\n        X_imputed = imputer.fit_transform(X)\n        # Check if the imputed value is correct\n        assert X_imputed[0, 1] == expected_value\n\n        if strategy == 'most_frequent':\n            assert X_imputed[2, 0] == 1.0\n        else:\n            assert X_imputed[2, 0] == expected_value\n", "    def test_missing_indicator_error(self, X_fit, X_trans, params, msg_err):\n        indicator = MissingIndicator(missing_values=-1)\n        indicator.set_params(**params)\n        with pytest.raises(ValueError, match=msg_err):\n            indicator.fit(X_fit).transform(X_trans)\n", "    def test_simple_imputer_add_indicator(self, strategy):\n        X = np.array([\n            [np.nan, 1,      5,       np.nan, 1],\n            [2,      np.nan, 1,       np.nan, 2],\n            [6,      3,      np.nan,  np.nan, 3],\n            [1,      2,      9,       np.nan, 4]\n\n        ])\n        imputer = SimpleImputer(missing_values=np.nan, strategy=strategy, add_indicator=True)\n        X_trans = imputer.fit_transform(X)\n        assert X_trans.shape[1] == X.shape[1] * 2 \n"], "sample_804": ["compilation error", "compilation error", "compilation error", "def test_ordinal_encoder_drop_first():\n    X = [['a', 1, 'x'],\n         ['b', 2, 'y'],\n         ['c', 3, 'z'],\n         ['a', 1, 'x']]\n    enc = OrdinalEncoder(drop='first')\n    exp = np.array([[0, 1],\n                    [1, 2],\n                    [2, 3],\n                    [0, 1]])\n    assert_array_equal(enc.fit_transform(X), exp)\n", "def test_one_hot_encoder_invalid_drop_type():\n    enc = OneHotEncoder(drop=1)\n    assert_raises_regex(\n        TypeError,\n        \"`drop` should be a list-like object\",\n        enc.fit, [['abc', 2, 55], ['def', 1, 55], ['def', 3, 59]])\n"], "sample_630": ["def test_infer_node_2(mock_infer, mock_get_annotation):\n    \"\"\"Return set() when astroid.InferenceError is raised and an annotation has\n    not been returned\n    \"\"\"\n    mock_get_annotation.return_value = None\n    node = astroid.extract_node(\"a: str = 'mystr'\")\n    assert infer_node(node) == {astroid.objects.IntType()}\n    assert mock_infer.called\n", "def test_infer_node_2(mock_infer, mock_get_annotation):\n    \"\"\"Return set() when astroid.InferenceError is raised and an annotation has\n    been returned\n    \"\"\"\n    mock_get_annotation.return_value = \"str\"\n    node = astroid.extract_node(\"a: str = 'mystr'\")\n    mock_infer.return_value = \"x\"\n    assert infer_node(node) == set()\n    assert mock_infer.called \n", "def test_infer_node_2(mock_infer, mock_get_annotation):\n    \"\"\"Return set() when astroid.InferenceError is raised and\n    no annotation is found\n    \"\"\"\n    mock_get_annotation.return_value = None\n    node = astroid.extract_node(\"a = 'mystr'\")\n    mock_infer.return_value = \"x\"\n    assert infer_node(node) == set()\n    assert mock_infer.called\n", "def test_infer_node_2(mock_infer, mock_get_annotation):\n    \"\"\"Return the annotation when astroid.InferenceError is raised and an annotation has\n    been returned\n    \"\"\"\n    mock_get_annotation.return_value = \"str\"\n    node = astroid.extract_node(\"a: str = 'mystr'\")\n    mock_infer.return_value = \"x\"\n    assert infer_node(node) == {'str'}\n    assert mock_infer.called\n", "def test_infer_node_2(mock_infer, mock_get_annotation):\n    \"\"\"Return the annotation if astroid.InferenceError is raised.\"\"\"\n    mock_get_annotation.return_value = \"str\"\n    node = astroid.extract_node(\"a: str = 'mystr'\")\n    mock_infer.side_effect = astroid.InferenceError\n    assert infer_node(node) == {\"str\"}\n    assert mock_infer.called\n\n"], "sample_198": ["    def test_expression_wrapper_output_field_none(self):\n        expr = ExpressionWrapper(F('cost'), output_field=None)\n        self.assertIsNone(expr.output_field)\n", "    def test_empty_having(self):\n        expr = ExpressionWrapper(Value(3), output_field=IntegerField())\n        self.assertEqual(expr.get_having_conditions(), [])\n", "    def test_output_field_attribute(self):\n        expr = ExpressionWrapper(Value(3), output_field=IntegerField())\n        self.assertEqual(expr.output_field, IntegerField())\n", "    def test_output_field_expression_wrapper(self):\n        ew = ExpressionWrapper(F('field'), output_field=CharField())\n        self.assertEqual(ew.output_field.get_internal_type(), 'CharField')\n", "    def test_expression_wrapper_with_alias(self):\n        expr = ExpressionWrapper(F('cost') + F('tax'), output_field=IntegerField(), alias='total_cost')\n        self.assertEqual(str(expr), '(\"cost\" + \"tax\") AS total_cost')\n"], "sample_165": ["    def test_modelmultiplechoicefield_empty_label(self):\n        e = {\n            'required': 'REQUIRED',\n            'invalid_choice': '%(value)s IS INVALID CHOICE',\n        }\n        f = ModelMultipleChoiceField(\n            queryset=ChoiceModel.objects.all(), error_messages=e,\n            empty_label='None'\n        )\n        self.assertFormErrors(['REQUIRED'], f.clean, '')\n        self.assertFormErrors(['None IS INVALID CHOICE'], f.clean, ['None'])\n", "    def test_to_python_modelmultiplechoicefield(self):\n        f = ModelMultipleChoiceField(queryset=ChoiceModel.objects.all())\n        self.assertEqual(\n            f.to_python(('1',)),\n            [ChoiceModel.objects.get(pk=1)]\n        )\n\n        self.assertEqual(\n            f.to_python(['1', '2']),\n\n            [ChoiceModel.objects.get(pk=1), ChoiceModel.objects.get(pk=2)]\n        )\n", "    def test_modelmultiplechoicefield_empty_label(self):\n        e = {\n            'required': 'REQUIRED',\n            'invalid_choice': '%(value)s IS INVALID CHOICE',\n            'invalid_list': 'NOT A LIST OF VALUES',\n        }\n        f = ModelMultipleChoiceField(queryset=ChoiceModel.objects.all(), empty_label='--', error_messages=e)\n        self.assertFormErrors(['REQUIRED'], f.clean, '')\n        self.assertFormErrors(['NOT A LIST OF VALUES'], f.clean, '3')\n        self.assertFormErrors(['4 IS INVALID CHOICE'], f.clean, ['4'])\n        self.assertEqual(f.choices[0][1], '--')\n\n\n\n", "    def test_modelchoicefield_to_field_name(self):\n        class MyModel(models.Model):\n            name = models.CharField(max_length=100)\n            code = models.CharField(max_length=100)\n\n        f = ModelChoiceField(queryset=MyModel.objects.all(), to_field_name='code')\n        self.assertFormErrors(['REQUIRED'], f.clean, '')\n        self.assertEqual(f.clean('abc'), MyModel.objects.get(code='abc'))\n", "    def test_modelchoicefield_empty_label(self):\n        # Create choices for the model choice field tests below.\n        ChoiceModel.objects.create(pk=1, name='a')\n        ChoiceModel.objects.create(pk=2, name='b')\n        ChoiceModel.objects.create(pk=3, name='c')\n\n        f = ModelChoiceField(queryset=ChoiceModel.objects.all(), empty_label='---------')\n        self.assertEqual(f.choices[0][1], '---------')\n"], "sample_291": ["    def test_context_object_name_override(self):\n        test_view = views.CustomMultipleObjectMixinView()\n        test_view.context_object_name = 'songs'\n        context = test_view.get_context_data()\n        self.assertEqual(context['songs'], test_view.queryset)\n", "    def test_get_context_data_with_empty_queryset(self):\n        test_view = views.CustomMultipleObjectMixinView()\n        test_view.queryset = []\n        context = test_view.get_context_data()\n        self.assertEqual(context['object_list'], [])\n\n", "    def test_get_context_data_without_object_list(self):\n        test_view = views.CustomMultipleObjectMixinView()\n        test_view.get(self.rf.get('/'))\n        context = test_view.get_context_data()\n        self.assertIn('object_list', context)\n", "    def test_empty_queryset(self):\n        test_view = views.CustomMultipleObjectMixinView()\n        test_view.queryset = []\n        test_view.get(self.rf.get('/'))\n        context = test_view.get_context_data()\n        self.assertEqual(context['object_list'], [])\n", "    def test_empty_queryset(self):\n        test_view = views.CustomMultipleObjectMixinView()\n        test_view.queryset = []\n        test_view.get(self.rf.get('/'))\n        context = test_view.get_context_data()\n        self.assertEqual(context['object_list'], [])\n"], "sample_972": ["def test_stringify_type_forwardref():\n    from typing import ForwardRef  # type: ignore\n    assert stringify(ForwardRef(\"myint\")) == \"myint\"\n    assert stringify(ForwardRef(\"myint\"), \"smart\") == \"myint\"\n\n", "    def test_stringify_type_hints_GenericMeta():\n        class MyClass(Generic[T]):\n            pass\n        assert stringify(MyClass[int]) == \"MyClass[int]\"\n        assert stringify(MyClass[int], \"smart\") == \"~tests.test_util_typing.MyClass[int]\"\n", "    def test_stringify_PEP_560():\n        from typing import _GenericAlias\n        assert stringify(_GenericAlias(List, (int,))) == \"List[int]\"\n        assert stringify(_GenericAlias(List, (int,)), \"smart\") == \"~typing.List[int]\"\n", "    def test_stringify_type_hints_param_spec():\n        from typing import ParamSpec  # type: ignore\n\n        P = ParamSpec(\"P\")\n        assert stringify(Callable[P, int]) == \"Callable[P, int]\"\n        assert stringify(Callable[P, int], \"fully-qualified\") == \"typing.Callable[P, int]\"\n        assert stringify(Callable[P, int], \"smart\") == \"~typing.Callable[P, int]\"\n", "    def test_stringify_type_hints_GenericAlias():\n        from typing import Generic, TypeVar  # type: ignore\n\n        T = TypeVar('T')\n        class MyGeneric(Generic[T]):\n            pass\n\n        assert stringify(MyGeneric[int]) == \"tests.test_util_typing.MyGeneric[int]\"\n        assert stringify(MyGeneric[int], \"smart\") == \"~tests.test_util_typing.MyGeneric[int]\"\n"], "sample_1117": ["def test_block_matrix():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = BlockMatrix([[A, B]])\n    assert ask(Q.diagonal(C))\n    assert ask(Q.symmetric(C))\n\n", "compilation error", "compilation error", "def test_matrix_element_sets_diagonal():\n    X = DiagMatrix(MatrixSymbol('x', 3, 1))\n    assert ask(Q.real(X[1, 1]), Q.real_elements(X))\n    assert ask(Q.integer(X[2, 2]), Q.integer_elements(X))\n    assert ask(Q.complex(X[0, 0]), Q.complex_elements(X))\n", "compilation error"], "sample_233": ["    def test_legacy_token_compatibility(self):\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        # Create a token using the legacy algorithm.\n        with self.settings(DEFAULT_HASHING_ALGORITHM='sha1'):\n            legacy_generator = PasswordResetTokenGenerator()\n            legacy_token = legacy_generator.make_token(user)\n\n        # Verify the legacy token can be checked with the modern generator.\n        modern_generator = PasswordResetTokenGenerator()\n        self.assertIs(modern_generator.check_token(user, legacy_token), True)\n", "    def test_token_legacy_format(self):\n        \"\"\"\n        The token can be checked with the legacy format even after deprecation.\n        \"\"\"\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n\n        # Check with the new format.\n        self.assertIs(p0.check_token(user, tk1), True)\n\n        # Mock the _now function to simulate a timestamp in the past.\n        past_now = datetime.now() - timedelta(days=1)\n        p1 = MockedPasswordResetTokenGenerator(past_now)\n\n        # Check with the legacy format.\n\n        self.assertIs(p1.check_token(user, tk1), True)\n\n\n\n", "    def test_token_with_password_change(self):\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        user.set_password('newtestpw')\n        user.save()\n        self.assertIs(p0.check_token(user, tk1), False)\n\n", "    def test_token_with_changed_password(self):\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        user.set_password('newpassword')\n        user.save()\n        self.assertIs(p0.check_token(user, tk1), False)\n", "    def test_password_change_invalidates_token(self):\n        \"\"\"Changing the user's password invalidates the token.\"\"\"\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        self.assertTrue(p0.check_token(user, tk1))\n        user.set_password('newpassword')\n        user.save()\n        self.assertFalse(p0.check_token(user, tk1)) \n"], "sample_1121": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_710": ["    def test_empty_teardown_runs(pytester: Pytester) -> None:\n        pytester.makepyfile(\n            test_foo=\"\"\"\n        import unittest\n        class MyTestCase(unittest.TestCase):\n                pass\n                assert True", "    def test_do_class_cleanups_on_success_with_nested_class(pytester: PyTester) -> None:\n        testpath = pytester.makepyfile(\n            \"\"\"\n            import unittest\n            class MyTestParent(unittest.TestCase):\n                values = []\n                @classmethod\n                        cls.values.append('parent')\n                    cls.addClassCleanup(cleanup)\n                \n                class MyTestChild(unittest.TestCase):\n                    @classmethod\n                            cls.values.append('child')\n                        cls.addClassCleanup(cleanup)\n                        pass\n                assert ['parent', 'child'] == MyTestParent.MyTestChild.values\n        \"\"\"\n        )\n        reprec = pytester.inline_run(testpath)\n        passed, skipped, failed = reprec.countoutcomes()\n        assert failed == 0\n        assert passed == 2\n", "    def test_do_class_cleanups_on_test_failure(pytester: Pytester) -> None:\n        testpath = pytester.makepyfile(\n            \"\"\"\n            import unittest\n            class MyTestCase(unittest.TestCase):\n                values = []\n                @classmethod\n                        cls.values.append(1)\n                    cls.addClassCleanup(cleanup)\n                    assert False\n                    pass\n                assert MyTestCase.values == [1]\n        \"\"\"\n        )\n        reprec = pytester.inline_run(testpath)\n        passed, skipped, failed = reprec.countoutcomes()\n        assert failed == 1\n        assert passed == 2\n\n\n\n", "            def test_with_setup_teardown(self):\n                self.addCleanup(lambda: self.assertEqual(1, 1))\n                self.assertEqual(1, 1)\n", "    def test_do_cleanups_on_class_setup_failure(pytester: PyTester) -> None:\n        testpath = pytester.makepyfile(\n            \"\"\"\n            import unittest\n            class MyTestCase(unittest.TestCase):\n                values = []\n                @classmethod\n                        cls.values.append(1)\n                    cls.addCleanup(cleanup)\n                    assert False\n                    pass\n                assert MyTestCase.values == [1] \n        \"\"\"\n        )\n        reprec = pytester.inline_run(testpath)\n        passed, skipped, failed = reprec.countoutcomes()\n        assert failed == 1\n        assert passed == 1 \n"], "sample_808": ["    def test_iforest_chunks_works2():\n        X = np.array([[0, 1], [1, 2], [2, 1],[3, 4]])\n        clf = IsolationForest(contamination=0.25).fit(X)\n        # The chunk size should be determined based on the working_memory\n        # budget.\n        # We'd like to test the following scenarios:\n        #\n        # 1. Number of samples is less than or equal to chunk size\n        # 2. Number of samples is greater than chunk size\n        # 3. Number of samples is much greater than chunk size\n        #\n        # For now, this test only asserts that the predict function \n        # does not raise any error.\n\n        clf.predict(X)\n", "def test_iforest_n_jobs():\n    rng = check_random_state(42)\n\n    # Generate some data\n    X = rng.randn(1000, 10)\n\n    # Fit IsolationForest with n_jobs=1\n    clf1 = IsolationForest(n_jobs=1, random_state=rng).fit(X)\n\n    # Fit IsolationForest with n_jobs=-1 (using all available cores)\n    clf2 = IsolationForest(n_jobs=-1, random_state=rng).fit(X)\n\n    # Assert that both models have the same predictions\n    assert_array_equal(\n        clf1.predict(X[:100]), clf2.predict(X[:100])\n    )\n", "def test_iforest_predict_proba():\n    \"\"\"\n    Test predict_proba for all contamination settings:\n\n    - 'auto':\n\n    - float in range (0.0, 0.5)\n    \"\"\"\n    rng = check_random_state(0)\n    X_train = rng.randn(100, 2)\n    X_test = rng.randn(10, 2)\n\n    # Test contamination='auto' \n\n    clf = IsolationForest(contamination=\"auto\", random_state=rng).fit(X_train)\n    proba = clf.predict_proba(X_test)\n    assert proba.shape == (10, 2)\n    assert np.all(proba >= 0) and np.all(proba <= 1)\n    assert np.allclose(proba[:, 0] + proba[:, 1], 1.0)\n\n    # Test contamination=float\n\n    contamination = 0.1\n\n    clf = IsolationForest(contamination=contamination, random_state=rng).fit(X_train)\n    proba = clf.predict_proba(X_test)\n    assert proba.shape == (10, 2)\n    assert np.all(proba >= 0) and np.all(proba <= 1)\n    assert np.allclose(proba[:, 0] + proba[:, 1], 1.0)\n", "    def test_iforest_chunks_works2(\n        mocked_get_chunk, contamination, n_predict_calls", "def test_iforest_predict_proba():\n    \"\"\"Check that predict_proba outputs probabilities.\"\"\"\n    X_train = [[1, 1], [1, 2], [2, 1], [3, 3], [3, 4]]\n    clf = IsolationForest().fit(X_train)\n    proba = clf.predict_proba(X_train)\n    assert proba.shape == (len(X_train), 2)\n    assert np.all(proba >= 0)\n    assert np.all(proba <= 1)\n\n"], "sample_386": ["    def test_safe_string_addition_with_non_string(self):\n        s = mark_safe(\"a\")\n        with self.assertRaises(TypeError):\n            s + 1\n", "    def test_safe_string_concatenation_with_different_types(self):\n        safe_str = SafeString(\"safe\")\n        regular_str = \"regular\"\n        result = safe_str + regular_str\n        self.assertIsInstance(result, str)\n        self.assertEqual(result, \"saferregular\")\n        self.assertFalse(isinstance(result, SafeData))\n", "    def test_safe_string_addition_with_non_string(self):\n        s = mark_safe(\"a\")\n        with self.assertRaises(TypeError):\n            s + 123\n", "    def test_safe_string_concatenation_with_non_safe_data(self):\n        s = SafeString(\"a\") + \"b\" + mark_safe(\"c\")\n        self.assertEqual(s, \"abc\")\n        self.assertNotIsInstance(s, SafeData)\n", "    def test_safe_string_addition_with_non_safe_string(self):\n        \"\"\"\n        Adding a SafeString to a non-SafeString should return a regular\n        string, not a SafeString.\n        \"\"\"\n        safe_str = SafeString(\"a&b\")\n        regular_str = \"c<d\"\n\n        result = safe_str + regular_str\n        self.assertRenderEqual(\"{{ result }}\", \"a&bc<d\", result=result)\n        self.assertFalse(isinstance(result, SafeData))\n"], "sample_82": ["compilation error", "    def test_input_formats(self):\n        widget = self.widget\n        self.assertEqual(widget.format_value('2000-12-1'), {'year': 2000, 'month': 12, 'day': 1})\n        self.assertEqual(widget.format_value('2000/12/1'), {'year': 2000, 'month': 12, 'day': 1})\n        self.assertEqual(widget.format_value('2000.12.1'), {'year': 2000, 'month': 12, 'day': 1})\n        self.assertEqual(widget.format_value('12/2000/1'), {'year': 2000, 'month': 12, 'day': 1})\n        self.assertEqual(widget.format_value('12.2000.1'), {'year': 2000, 'month': 12, 'day': 1})\n        self.assertEqual(widget.format_value('12-2000-1'), {'year': 2000, 'month': 12, 'day': 1})\n        self.assertIsNone(widget.format_value('2000-13-1'))\n        self.assertIsNone(widget.format_value('2000-12-32'))\n        self.assertIsNone(widget.format_value('invalid'))\n\n", "    def test_empty_year_month_day(self):\n        self.assertEqual(\n            self.widget.format_value(''),\n            {'day': None, 'month': None, 'year': None}\n        )\n\n", "    def test_widget_attrs(self):\n        widget = SelectDateWidget(attrs={'class': 'my-class'})\n        self.assertEqual(\n            widget.render('mydate', '2010-04-15', attrs={'placeholder': 'Enter your date'}),\n            f\"\"\"\n            <select name=\"mydate_month\" id=\"id_mydate_month\" class=\"my-class\">\n                <option value=\"\">---</option>\n                <option value=\"1\">January</option>\n                <option value=\"2\">February</option>\n                <option value=\"3\">March</option>\n                <option value=\"4\" selected>April</option>\n                <option value=\"5\">May</option>\n                <option value=\"6\">June</option>\n                <option value=\"7\">July</option>\n                <option value=\"8\">August</option>\n                <option value=\"9\">September</option>\n                <option value=\"10\">October</option>\n                <option value=\"11\">November</option>\n                <option value=\"12\">December</option>\n            </select>\n\n            <select name=\"mydate_day\" id=\"id_mydate_day\" class=\"my-class\">\n                <option value=\"\">---</option>\n                <option value=\"1\">1</option>\n                <option value=\"2\">2</option>\n                <option value=\"3\">3</option>\n                <option value=\"4\">4</option>\n                <option value=\"5\">5</option>\n                <option value=\"6\">6</option>\n                <option value=\"7\">7</option>\n                <option value=\"8\">8</option>\n                <option value=\"9\">9</option>\n                <option value=\"10\">10</option>\n                <option value=\"11\">11</option>\n                <option value=\"12\">12</option>\n                <option value=\"13\">13</option>\n                <option value=\"14\">14</option>\n                <option value=\"15\" selected>15</option>\n                <option value=\"16\">16</option>\n                <option value=\"17", "    def test_selectdate_disabled(self):\n        widget = SelectDateWidget(years=('2018', '2019'), attrs={'disabled': True})\n        self.check_html(widget, 'my_date', '', html=(\n            \"\"\"\n            <select name=\"my_date_month\" id=\"id_my_date_month\" disabled>\n                <option selected value=\"\">---</option>\n                <option value=\"1\">January</option>\n                <option value=\"2\">February</option>\n                <option value=\"3\">March</option>\n                <option value=\"4\">April</option>\n                <option value=\"5\">May</option>\n                <option value=\"6\">June</option>\n                <option value=\"7\">July</option>\n                <option value=\"8\">August</option>\n                <option value=\"9\">September</option>\n                <option value=\"10\">October</option>\n                <option value=\"11\">November</option>\n                <option value=\"12\">December</option>\n            </select>\n            <select name=\"my_date_day\" id=\"id_my_date_day\" disabled>\n                <option selected value=\"\">---</option>\n                <option value=\"1\">1</option>\n                <option value=\"2\">2</option>\n                <option value=\"3\">3</option>\n                <option value=\"4\">4</option>\n                <option value=\"5\">5</option>\n                <option value=\"6\">6</option>\n                <option value=\"7\">7</option>\n                <option value=\"8\">8</option>\n                <option value=\"9\">9</option>\n                <option value=\"10\">10</option>\n                <option value=\"11\">11</option>\n                <option value=\"12\">12</option>\n                <option value=\"13\">13</option>\n                <option value=\"14\">14</option>\n                <option value=\"15\">15</option>\n                <option value=\"16\">16</option>\n                <option value=\"17\">17</option>\n                <option value=\"18"], "sample_375": ["    def test_proxy_fk(self):\n        A = self.create_model(\"A\", foreign_keys=[models.ForeignKey('B', models.CASCADE)])\n        B = self.create_model(\"B\", proxy=True)\n        self.assertRelated(A, [B])\n        self.assertRelated(B, [A])\n", "    def test_abstract_base_with_fk(self):\n        A = self.create_model(\"A\", abstract=True, foreign_keys=[models.ForeignKey('B', models.CASCADE)])\n        B = self.create_model(\"B\")\n        C = self.create_model(\"C\", bases=(A,))\n        self.assertRelated(A, [B, C])\n        self.assertRelated(B, [A, C])\n        self.assertRelated(C, [B])\n\n", "    def test_proxy_with_related(self):\n        A = self.create_model(\"A\", foreign_keys=[models.ForeignKey('B', models.CASCADE)])\n        B = self.create_model(\"B\")\n        C = self.create_model(\"C\", bases=(B,), proxy=True)\n        self.assertRelated(A, [B, C])\n        self.assertRelated(B, [A, C])\n        self.assertRelated(C, [A])\n", "    def test_abstract_base_with_fk(self):\n        A = self.create_model(\"A\", abstract=True, foreign_keys=[models.ForeignKey('B', models.CASCADE)])\n        B = self.create_model(\"B\")\n        C = self.create_model(\"C\", bases=(A,))\n        self.assertRelated(A, [B, C])\n        self.assertRelated(B, [A, C])\n        self.assertRelated(C, [B])\n", "    def test_proxy_model_related(self):\n        A = self.create_model(\"A\", foreign_keys=[models.ForeignKey('B', models.CASCADE)])\n        B = self.create_model(\"B\", proxy=True)\n        self.assertRelated(A, [B])\n        self.assertRelated(B, [A])\n"], "sample_100": ["    def test_reloader_should_stop_when_no_watched_files(self):\n        self.reloader.stop()\n        self.assertFalse(self.reloader.is_running())\n\n", "    def test_watched_roots_contains_parent_directories(self):\n        self.reloader.watch_dir(self.tempdir / 'subdirectory', '*.py')\n        paths = self.reloader.watched_roots([])\n        self.assertIn(self.tempdir / 'subdirectory', paths)\n        self.assertIn(self.tempdir, paths)\n", "    def test_should_stop(self):\n        self.reloader.should_stop = True\n        self.assertTrue(self.reloader.should_stop())\n", "    def test_should_stop_returns_false_when_app_ready_event_not_set(self):\n        app_reg = Apps()\n        self.assertFalse(self.reloader.should_stop(app_reg))\n", "    def test_notify_file_changed_only_once(self):\n        with mock.patch('django.utils.autoreload.BaseReloader.should_stop') as mocked_should_stop:\n            mocked_should_stop.return_value = False\n            self.reloader.watch_dir(self.tempdir, '*.py')\n            self.increment_mtime(self.existing_file)\n            with self.tick_twice():\n                self.reloader.notify_file_changed(self.existing_file)\n                self.reloader.notify_file_changed(self.existing_file)\n            self.assertEqual(self.reloader._changed_files, {self.existing_file: 2})\n"], "sample_1150": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_202": ["    def test_empty_cookie_data(self):\n        request = self.get_request()\n        storage = self.storage_class(request)\n        set_cookie_data(storage, [], encode_empty=True)\n        # The message actually contains what we expect.\n        self.assertEqual(list(storage), [])\n", "    def test_empty_cookie(self):\n        storage = self.storage_class(self.get_request())\n        set_cookie_data(storage, [], encode_empty=True)\n        self.assertEqual(list(storage), [])\n", "    def test_empty_cookie_encoding(self):\n        storage = self.storage_class(self.get_request())\n        response = self.get_response()\n        set_cookie_data(storage, [], encode_empty=True)\n\n        self.assertEqual(self.stored_messages_count(storage, response), 0)\n", "    def test_empty_cookie(self):\n        storage = self.get_storage()\n        response = self.get_response()\n        # set cookie to empty string\n        set_cookie_data(storage, [], encode_empty=True)\n\n        self.assertEqual(storage._decode(response.cookies[CookieStorage.cookie_name].value), [])\n", "    def test_empty_cookie_storage(self):\n        \"\"\"\n        Test loading and retrieving the messages from an empty cookie.\n\n        \"\"\"\n        storage = self.get_storage()\n        response = self.get_response()\n        \n        self.assertEqual(list(storage), [])\n        self.assertEqual(self.stored_messages_count(storage, response), 0)\n\n        storage.update(response)\n        self.assertEqual(self.stored_messages_count(storage, response), 0)\n\n"], "sample_220": ["    def test_delete_cookie_with_path(self):\n        response = HttpResponse()\n        response.set_cookie('c', path='/mypath')\n        response.delete_cookie('c', path='/mypath')\n        self.assertEqual(response.cookies['c']['expires'], 'Thu, 01 Jan 1970 00:00:00 GMT')\n        self.assertEqual(response.cookies['c']['max-age'], 0)\n        self.assertEqual(response.cookies['c']['path'], '/mypath')\n\n", "    def test_delete_cookie_with_path(self):\n        \"\"\"\n        delete_cookie() respects the 'path' argument.\n        \"\"\"\n        response = HttpResponse()\n        response.set_cookie('c', path='/path/')\n        response.delete_cookie('c', path='/path/')\n        self.assertIn('c', response.cookies)\n        response.delete_cookie('c', path='/')\n        self.assertNotIn('c', response.cookies)\n", "    def test_delete_cookie_path(self):\n        response = HttpResponse()\n        response.set_cookie('c', path='/path')\n        response.delete_cookie('c', path='/path')\n        self.assertFalse(response.cookies.get('c'))\n", "    def test_delete_cookie_specified_path_domain(self):\n        response = HttpResponse()\n        response.set_cookie('c', path='/foo', domain='example.com')\n        response.delete_cookie('c', path='/foo', domain='example.com')\n        deleted_cookie = response.cookies['c']\n        self.assertEqual(deleted_cookie['expires'], 'Thu, 01 Jan 1970 00:00:00 GMT')\n        self.assertEqual(deleted_cookie['max-age'], 0)\n        self.assertEqual(deleted_cookie['path'], '/foo')\n        self.assertEqual(deleted_cookie['domain'], 'example.com')\n", "    def test_delete_cookie_path(self):\n        response = HttpResponse()\n        response.set_cookie('c', path='/path/')\n        response.delete_cookie('c', path='/path/')\n        self.assertEqual(response.cookies['c']['path'], '/path/')\n"], "sample_981": ["def test_from_cycle_form():\n    assert Permutation.from_cycle_form([(1, 2)]) == Permutation([0, 2, 1])\n    assert Permutation.from_cycle_form([(1, 2), (3, 4)]) == Permutation([0, 2, 1, 4, 3])\n    assert Permutation.from_cycle_form([(1, 2, 3)]) == Permutation([0, 2, 3, 1])\n    assert Permutation.from_cycle_form([]) == Permutation(0)\n", "def test_Cycle_len():\n    assert len(Cycle()) == 0\n    assert len(Cycle(1, 2)) == 2\n    assert len(Cycle(1, 2, 3)) == 3\n", "def test_cycle_signature():\n    p = Permutation([0, 3, 2, 1])\n    assert p.cyclic_form == [[0, 3, 2, 1]]\n    assert p.signature() == -1\n\n", "    def test_permutation_add():\n        p = Permutation([1, 0, 2, 3])\n        q = Permutation([3, 1, 0, 2])\n        assert p + q == Permutation([3, 1, 2, 0])\n        assert p + q == Permutation([3, 1, 2, 0])\n        assert p + Permutation() == p\n        assert Permutation() + q == q\n        raises(TypeError, lambda: p + [1, 2])\n        raises(TypeError, lambda: p + 3)\n", "def test_parity_and_sign():\n    assert Permutation([0, 1, 2, 3]).parity() == 0\n    assert Permutation([1, 0, 2, 3]).parity() == 1\n    assert Permutation([0, 2, 1, 3]).parity() == 1\n    assert Permutation([3, 2, 1, 0]).parity() == 0\n    assert Permutation([1, 3, 0, 2]).parity() == 0\n    assert Permutation([2, 0, 1, 3]).parity() == 1\n\n    assert Permutation([]).sign() == 1\n    assert Permutation([0, 1, 2, 3]).sign() == 1\n    assert Permutation([1, 0, 2, 3]).sign() == -1\n    assert Permutation([0, 2, 1, 3]).sign() == -1\n    assert Permutation([3, 2, 1, 0]).sign() == 1\n    assert Permutation([1, 3, 0, 2]).sign() == 1\n    assert Permutation([2, 0, 1, 3]).sign() == -1\n\n"], "sample_240": ["    def test_token_with_updated_password(self):\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        user.set_password('newtestpw')\n        user.save()\n        self.assertIs(p0.check_token(user, tk1), False)\n", "    def test_password_reset_token_with_modified_user_data(self):\n        user = User.objects.create_user(\n            'tokentestuser', 'test2@example.com', 'testpw'\n        )\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        user.password = 'newpassword'  # Change password\n        user.save()\n        self.assertIs(p0.check_token(user, tk1), False)\n", "    def test_token_with_different_password(self):\n        \"\"\"Updating the user password invalidates the token.\"\"\"\n        user = User.objects.create_user('changepwuser', 'test2@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk1), True)\n        user.set_password('newtestpw')\n        user.save()\n        self.assertIs(p0.check_token(user, tk1), False)\n", "    def test_token_with_empty_email(self):\n        \"\"\"\n        Test that token generation and validation works correctly when a user\n        has an empty email address.\n        \"\"\"\n        user = User.objects.create_user('emptyemailuser', '', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        self.assertIs(p0.check_token(user, tk1), True)\n", "    def test_password_reset_token_with_timestamp_handling(self):\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        now = datetime.now()\n        p0 = MockedPasswordResetTokenGenerator(now)\n        tk1 = p0.make_token(user)\n        # Modify the timestamp part of the token manually.\n        modified_token = tk1.split('-')\n        modified_token[0] = str(int(modified_token[0]) + 1000)  # Increase timestamp\n        modified_token = '-'.join(modified_token)\n        self.assertIs(p0.check_token(user, modified_token), False)\n"], "sample_1185": ["def test_compogen():\n    assert compogen([sin(x), cos(x)], x) == sin(cos(x))\n    assert compogen([x**2 + x + 1, sin(x)], x) == sin(x)**2 + sin(x) + 1\n    assert compogen([sqrt(x), 6*x**2 - 5], x) == sqrt(6*x**2 - 5)\n    assert compogen([sin(x), sqrt(x), cos(x), x**2 + 1], x) == sin(sqrt(cos(x**2 + 1)))\n    assert compogen([x**2 - x - 1, x**2 + x], x) == -x**2 - x + (x**2 + x)**2 - 1\n\n", "compilation error", "def test_compogen():\n    assert compogen([sin(x), cos(x)], x) == sin(cos(x))\n    assert compogen([x**2 + x + 1, sin(x)], x) == sin(x)**2 + sin(x) + 1\n    assert compogen([sqrt(x), 6*x**2 - 5], x) == sqrt(6*x**2 - 5)\n    assert compogen([sin(x), sqrt(x), cos(x), x**2 + 1], x) == sin(sqrt(cos(x**2 + 1)))\n    assert compogen([x**2 - x - 1, x**2 + x], x) == -x**2 - x + (x**2 + x)**2 - 1\n", "compilation error", "def test_compogen():\n    assert compogen([sin(x), cos(x)], x) == sin(cos(x))\n    assert compogen([x**2 + x + 1, sin(x)], x) == sin(x)**2 + sin(x) + 1\n    assert compogen([sqrt(x), 6*x**2 - 5], x) == sqrt(6*x**2 - 5)\n    assert compogen([sin(x), sqrt(x), cos(x), x**2 + 1], x) == sin(sqrt(cos(x**2 + 1)))\n    assert compogen([x**2 - x - 1, x**2 + x], x) == -x**2 - x + (x**2 + x)**2 - 1\n"], "sample_567": ["compilation error", "def test_text_pdf_fonttype(fonttype):\n    mpl.rcParams['pdf.fonttype'] = fonttype\n    fig, ax = plt.subplots()\n    ax.text(0.5, 0.5, 'Some Text')\n    fig.savefig(io.BytesIO(), format='pdf')\n", "compilation error", "    def test_tex_math_antialiased_off_default_vs_manual(fig_test, fig_ref,\n                                                    mpl_figure):\n        mpl.rcParams['text.usetex'] = True\n        fig_test.text(0.5, 0.5, r\"OutsideMath $\\sqrt{2}$\",\n                             antialiased=False)\n\n        mpl.rcParams['text.antialiased'] = False\n        fig_ref.text(0.5, 0.5, r\"OutsideMath $\\sqrt{2}$\")\n\n", "def test_text_antialiased_mixed_default_vs_manual():\n    fig_test = plt.figure()\n    fig_test.text(0.25, 0.5, 'Antialiased text', antialiased=True)\n    fig_test.text(0.75, 0.5, 'Non-Antialiased text', antialiased=False)\n\n    fig_ref = plt.figure()\n    mpl.rcParams['text.antialiased'] = True\n    fig_ref.text(0.25, 0.5, 'Antialiased text')\n    fig_ref.text(0.75, 0.5, 'Non-Antialiased text', antialiased=False)\n\n    return fig_test, fig_ref\n\n"], "sample_360": ["    def test_cache_middleware_custom_cache_alias(self):\n        middleware = CacheMiddleware(hello_world_view, cache_alias='other')\n        request = self.factory.get('/view/')\n        response = middleware.process_request(request)\n        self.assertIsNone(response)\n        response = hello_world_view(request, '1')\n        response = middleware.process_response(request, response)\n        self.assertEqual(response.content, b'Hello World 1')\n        result = middleware.process_request(request)\n        self.assertIsNotNone(result)\n        self.assertEqual(result.content, b'Hello World 1')\n\n", "    def test_cache_middleware_with_custom_cache_backend(self):\n        class CustomCacheBackend:\n                self.prefix = prefix\n                self.timeout = timeout\n                self.cache = cache\n\n                return self.cache.get(self.prefix + key)\n\n                self.cache.set(self.prefix + key, value, timeout or self.timeout)\n\n        cache_data = {}\n        custom_cache = CustomCacheBackend('custom:', 60, cache_data)\n        cache.clear()\n        middleware = CacheMiddleware(hello_world_view, cache=custom_cache)\n        request = self.factory.get('/view/')\n        response = middleware.process_response(request, hello_world_view(request, '1'))\n\n        self.assertEqual(response.content, b'Hello World 1')\n        self.assertIn('custom:middlewareprefix.GET.views.test_cache_middleware_with_custom_cache_backend',\n                       cache_data)\n", "    def test_cache_middleware_with_vary_headers(self):\n        view = cache_page(1)(hello_world_view)\n        request = self.factory.get('/view/')\n        request.META['HTTP_ACCEPT_ENCODING'] = 'gzip'\n        request.META['HTTP_ACCEPT'] = 'text/html'\n        response = view(request, 'test')\n        self.assertEqual(response.content, b'Hello World test')\n        response = view(request, 'test')\n        self.assertEqual(response.content, b'Hello World test')\n", "    def test_cache_key_with_language(self):\n        with override_settings(USE_I18N=True):\n            request = self.factory.get(self.path)\n            request.META['HTTP_ACCEPT_LANGUAGE'] = 'en-us'\n            template = engines['django'].from_string(\"This is a test\")\n            response = TemplateResponse(HttpRequest(), template)\n            key = learn_cache_key(request, response)\n            self.assertIn('en-us', key)\n", "    def test_cache_page_with_timeout_and_vary_on(self):\n        view = cache_page(timeout=10, vary_on=['lang'])(hello_world_view)\n        request = self.factory.get('/view/')\n        request.META['HTTP_ACCEPT_LANGUAGE'] = 'en-us'\n        response1 = view(request, '1')\n        request.META['HTTP_ACCEPT_LANGUAGE'] = 'es-MX'\n        response2 = view(request, '2')\n        self.assertEqual(response1.content, b'Hello World 1')\n        self.assertEqual(response2.content, b'Hello World 2')\n"], "sample_1011": ["compilation error", "compilation error", "def test_issue_12224():\n    x = Symbol('x')\n    f = Function('f')\n    expr = f(x).diff(x)\n    assert mcode(expr) == 'diff(f(x), x)'\n", "compilation error", "def test_MatrixElement_printing_indexed():\n    # test cases for issue #11821\n    A = MatrixSymbol(\"A\", 1, 3)\n    i = Symbol('i')\n    assert mcode(A[0, i]) == \"A(1, i + 1)\"\n    assert mcode(3 * A[0, i]) == \"3*A(1, i + 1)\"\n"], "sample_619": ["    def test_decode_cf_datetime_int64_with_cftime_overflow_error():\n        units = \"microseconds since 1700-01-01\"\n        calendar = \"360_day\"\n        num_dates = np.int64(1_000_000 * 86_400 * 360 * 500_000)\n        with pytest.raises(OverflowError):\n            decode_cf_datetime(num_dates, units, calendar)\n", "    def test_encode_cf_datetime_array(self, calendar):\n        date_range = pd.date_range(\"2000-01-01\", periods=3, freq=\"D\")\n        expected_num = np.arange(3)\n        expected_units = \"days since 2000-01-01\"\n        num, units, calendar = encode_cf_datetime(date_range, units=expected_units, \n                                                  calendar=calendar)\n        np.testing.assert_array_equal(num, expected_num)\n        assert units == expected_units\n        assert calendar == calendar\n", "def test_encode_cf_datetime_overflow_uint64():\n    dates = pd.date_range(\"2200\", periods=24, freq=\"D\")\n    units = \"days since 1700-01-01\"\n    encoded, _, _ = encode_cf_datetime(dates.to_numpy(), units)\n    assert encoded.dtype == np.uint64\n", "compilation error", "    def test_decode_cf_datetime_with_calendar_as_string(calendar):\n        # Regression test for GH 2649\n        units = \"days since 2000-01-01\"\n        num_dates = np.array([1, 2, 3])\n        dates = decode_cf_datetime(num_dates, units, calendar=str(calendar))\n        expected = pd.date_range(\"2000-01-02\", periods=3)\n        assert_array_equal(dates, expected)\n"], "sample_458": ["    def test_decimal_context(self):\n        with localcontext() as ctx:\n            ctx.prec = 4\n            self.assertEqual(floatformat(Decimal(\"1.23456789\"), 2), \"1.23\")\n            self.assertEqual(floatformat(Decimal(\"1.23456789\"), 4), \"1.2346\")\n", "    def test_decimal_context(self):\n        with localcontext() as ctx:\n            ctx.prec = 4\n            self.assertEqual(floatformat(Decimal(\"123.4567\"), 2), \"123.46\")\n", "    def test_floatformat03(self):\n        output = self.engine.render_to_string(\n            \"floatformat03\", {\"a\": mark_safe(\"1.42\")}\n        )\n        self.assertEqual(output, \"1.420\")\n\n", "    def test_floatformat_with_comma(self):\n        with localcontext() as ctx:\n            ctx.prec = 28\n            self.assertEqual(floatformat(Decimal(\"1234567890123456789012345678.9\"), 2),\n                             \"1234567890123456789012345678.90\")\n", "    def test_exponent_notation(self):\n        self.assertEqual(floatformat(1e10, 2), \"10,000,000,000.00\")\n        self.assertEqual(floatformat(1e-5, 3), \"0.000\")\n        self.assertEqual(floatformat(1e-10, 3), \"0.000\")\n"], "sample_632": ["def test_similarity_with_empty_lines():\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([SIMILAR1, SIMILAR2])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == (\n            \"\"\"", "def test_empty_files():\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([\n            str(INPUT / \"empty_file\"),\n            str(INPUT / \"empty_file2\"),\n        ])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == \"\"\"", "def test_similar_checker_with_pylint():\n    linter = PyLinter(reporter=Reporter())\n    checker = similar.SimilarChecker(linter)\n\n    linter.check([SIMILAR1, SIMILAR2])\n    stats = linter._stats\n    assert stats[\"nb_duplicated_lines\"] == 5\n    assert stats[\"percent_duplicated_lines\"] == pytest.approx(8.33)\n\n\n", "def test_similar_with_different_spacing():\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([SIMILAR1, SIMILAR2])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == \"\"\"", "def test_duplicates_min_lines():\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([\"-d\", \"5\", SIMILAR1, SIMILAR2])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == \"\"\""], "sample_99": ["    def test_trunc_func_with_timezone_and_naive_datetime(self):\n        start_datetime = datetime(2015, 6, 15, 14, 30, 50, 321)\n        self.create_model(start_datetime, start_datetime)\n\n        melb = pytz.timezone('Australia/Melbourne')\n        with self.assertRaisesMessage(ValueError, 'Cannot truncate naive datetime object'):\n            DTModel.objects.annotate(truncated=Trunc('start_datetime', 'day', tzinfo=melb)).first()\n", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1072": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1128": ["    def test_point_vel_multiple_paths():\n        t = dynamicsymbols._t\n        q1, q2 = dynamicsymbols('q1 q2')\n        N = ReferenceFrame('N')\n        B = N.orientnew('B', 'Axis', [q1, N.z])\n        O = Point('O')\n        P = O.locatenew('P', q2 * B.x)\n        Q = O.locatenew('Q', q2 * N.y)\n        P.set_vel(B, q1 * B.y)\n        Q.set_vel(N, q1 * N.x)\n        R = P.locatenew('R', 10 * N.x)\n        assert R.vel(N) == 10 * N.x + q1 * B.y\n", "    def test_point_acc():\n        q1, q2 = dynamicsymbols('q1 q2')\n        N = ReferenceFrame('N')\n        B = ReferenceFrame('B')\n        Q = Point('Q')\n        O = Point('O')\n        Q.set_pos(O, q1 * N.x)\n        O.set_vel(N, q2 * N.y)\n        O.set_acc(N, q2 * q2 * N.y)\n        assert O.acc(N) == q2 * q2 * N.y\n        raises(ValueError, lambda : Q.acc(N)) # Acceleration of Q is not defined\n\n", "    def test_point_locatenew():\n        q1, q2 = dynamicsymbols('q1 q2')\n\n        N = ReferenceFrame('N')\n        A = ReferenceFrame('A')\n\n        p = Point('p')\n        p1 = p.locatenew('p1', 10*N.x)\n\n        assert p1.pos_from(p) == 10*N.x\n\n        p2 = p1.locatenew('p2', 5*A.y)\n\n        assert p2.pos_from(p) == 10*N.x + 5*A.y\n\n        assert p2.pos_from(p1) == 5*A.y\n\n", "    def test_point_vel_with_non_inertial_frame():\n        q1, q2 = dynamicsymbols('q1 q2')\n        N = ReferenceFrame('N')\n        A = ReferenceFrame('A')\n        A.set_ang_vel(N, q1 * A.z)\n        P = Point('P')\n        P.set_pos(N.origin, q2 * N.x)\n        raises(ValueError, lambda: P.vel(A))\n", "    def test_point_vel_after_pos_update():\n        t = dynamicsymbols._t\n        q1, q2 = dynamicsymbols('q1 q2')\n        N = ReferenceFrame('N')\n        B = ReferenceFrame('B')\n        O = Point('O')\n        P = Point('P')\n        P.set_pos(O, q1 * N.x)\n        O.set_vel(N, q2 * N.y)\n        assert P.vel(N) == q1.diff(t) * N.x + q2 * N.y\n        P.set_pos(O, q1 * N.y)\n        assert P.vel(N) == q1.diff(t) * N.y + q2 * N.y\n"], "sample_1138": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1140": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_611": ["compilation error", "    def test_date_range_leap_day(self, start, freq, cal, use_cftime):\n        result = date_range(start, periods=2, freq=freq, calendar=cal, use_cftime=use_cftime)\n        if cal == \"all_leap\":\n            assert (result[0].year, result[0].month, result[0].day) == (2000, 2, 29)\n        else:\n            assert (result[0].year, result[0].month, result[0].day) == (2000, 3, 1)\n\n", "compilation error", "    def test_date_range_like_calendar_conversion(\n        start, freq, cal_src, cal_tgt, use_cftime, exp0, exp_pd", "compilation error"], "sample_135": ["    def test_format_with_naive_datetime_and_timezone_specifier(self):\n        dt = datetime(2009, 5, 16, 5, 30, 30)\n        tz = get_fixed_timezone(-210)\n\n        # This should use the default timezone if the datetime object is naive\n        self.assertEqual(dateformat.format(dt, 'e', tz), get_default_timezone().tzname(dt))\n\n", "    def test_ambiguous_datetime_with_explicit_timezone(self):\n        # dt is ambiguous in Europe/Copenhagen. pytz raises an exception for\n        # the ambiguity, but we can resolve it by providing an explicit timezone.\n        dt = datetime(2015, 10, 25, 2, 30, 0)\n        aware_dt = make_aware(dt, get_fixed_timezone(60))  \n\n        self.assertEqual(format(aware_dt, 'I'), '0')\n        self.assertEqual(format(aware_dt, 'O'), '+0100')\n        self.assertEqual(format(aware_dt, 'T'), '+0100')\n        self.assertEqual(format(aware_dt, 'Z'), '3600')\n", "    def test_date_with_tzinfo(self):\n        tz = get_fixed_timezone(180)\n        dt = datetime(2009, 5, 16, 5, 30, 30, tzinfo=tz)\n\n        self.assertEqual(dateformat.format(dt, 'jS'), '136th')\n", "    def test_format_with_non_naive_datetime_without_timezone(self):\n        dt = datetime(2009, 5, 16, 5, 30, 30)\n        dt_with_tz = dt.replace(tzinfo=get_default_timezone())\n        self.assertEqual(format(dt_with_tz, 'Z'), '+0200')\n", "    def test_format_with_invalid_date(self):\n        with self.assertRaises(TypeError):\n            dateformat.format('', 'Y-m-d')\n\n"], "sample_44": ["    def test_transpose(self):\n        assert np.all(self.mJy.transpose().value == self.mJy.view(u.Quantity).transpose().value)\n        assert self.mJy.transpose().unit == self.mJy.unit\n        with pytest.raises(AttributeError):\n            self.mJy.T\n", "    def test_transpose(self):\n        for mag in self.mags:\n            assert np.all(mag.T.value == mag.value.T)\n            assert mag.T.unit == mag.unit\n", "    def test_ravel_flatten(self, method):\n        for mag in self.mags:\n            res = getattr(mag, method)()\n            assert np.all(res.value ==\n                          getattr(mag._function_view, method)().value)\n            assert res.unit == mag.unit\n", "    def test_only_ok_if_dimensionless(self, method):\n        res = getattr(self.m1, method)()\n        assert np.all(res.value ==\n                      getattr(self.m1._function_view, method)().value)\n        assert res.unit == self.m1.unit\n        with pytest.raises(TypeError):\n            getattr(self.mJy, method)()\n", "    def test_numpy_ufunc_equivalency(self):\n        with u.set_enabled_equivalencies(u.logarithmic()):\n            result = np.log10(self.mJy)\n        assert result.unit == u.dimensionless_unscaled\n"], "sample_249": ["    def test_set_as_test_mirror(self):\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        primary_settings_dict = copy.deepcopy(test_connection.settings_dict)\n        creation.set_as_test_mirror(primary_settings_dict)\n\n        self.assertEqual(creation.connection.settings_dict['NAME'], primary_settings_dict['NAME'])\n", "    def test_datetime_field_serialization(self):\n        \"\"\"\n        Test that datetime fields are serialized and deserialized correctly.\n        \"\"\"\n        now = datetime.datetime.now()\n        obj = SchoolClass.objects.create(name='Test Class', established=now)\n        # Serialize objects.\n        with mock.patch('django.db.migrations.loader.MigrationLoader') as loader:\n            # serialize_db_to_string() serializes only migrated apps, so mark\n            # the backends app as migrated.\n            loader_instance = loader.return_value\n            loader_instance.migrated_apps = {'backends'}\n            data = connection.creation.serialize_db_to_string()\n        SchoolClass.objects.all().delete()\n        # Deserialize objects\n        connection.creation.deserialize_db_from_string(data)\n        deserialized_obj = SchoolClass.objects.get()\n        self.assertEqual(deserialized_obj.established, now)\n", "    def test_serialize_deserialize_field_types(self):\n        # Test serialization and deserialization of various field types.\n        school_class = SchoolClass.objects.create(name='Class A',\n                                               founded=datetime.date(2023, 10, 27))\n        # Serialize objects.\n        with mock.patch('django.db.migrations.loader.MigrationLoader') as loader:\n            # serialize_db_to_string() serializes only migrated apps, so mark\n            # the backends app as migrated.\n            loader_instance = loader.return_value\n            loader_instance.migrated_apps = {'backends'}\n            data = connection.creation.serialize_db_to_string()\n        SchoolClass.objects.all().delete()\n        # Deserialize objects.\n        connection.creation.deserialize_db_from_string(data)\n        deserialized_school_class = SchoolClass.objects.get()\n        self.assertEqual(deserialized_school_class.name, 'Class A')\n        self.assertEqual(deserialized_school_class.founded, datetime.date(2023, 10, 27))\n", "    def test_serialize_deserialize_datetime(self):\n        data = \"\"\"\n        [\n            {\n                \"model\": \"backends.object\",\n                \"pk\": 1,\n                \"fields\": {\"created_at\": \"2023-10-27T10:00:00\"}\n            }\n        ]\n        \"\"\"\n        connection.creation.deserialize_db_from_string(data)\n        obj = Object.objects.get()\n        self.assertEqual(obj.created_at, datetime.datetime(2023, 10, 27, 10, 0))\n\n", "    def test_clone_test_db(self):\n        # Test cloning a database, including objects with foreign keys.\n        original_connection = get_connection_copy()\n        original_connection.settings_dict['NAME'] = 'original_db'\n        creation = original_connection.creation_class(original_connection)\n        creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n\n        # Create initial objects in the original database.\n        school_class = SchoolClass.objects.create(name=\"Original Class\")\n        student1 = Object.objects.create(name=\"Student 1\", school_class=school_class)\n        student2 = Object.objects.create(name=\"Student 2\", school_class=school_class)\n\n        # Clone the database.\n        cloned_connection = get_connection_copy()\n        cloned_connection.settings_dict['NAME'] = 'cloned_db'\n        creation = cloned_connection.creation_class(cloned_connection)\n        creation.clone_test_db(suffix='_clone', verbosity=0, autoclobber=True)\n\n        # Check if objects exist in the cloned database.\n        with connection.cursor() as cursor:\n            cursor.execute(\"SELECT COUNT(*) FROM backends_schoolclass\")\n            count = cursor.fetchone()[0]\n            self.assertEqual(count, 1)\n\n        cloned_school_class = SchoolClass.objects.get(name=\"Original Class\")\n        self.assertEqual(cloned_school_class.objects.count(), 2)\n\n\n\n"], "sample_852": ["    def test_make_biclusters_noise():\n        X, _, _ = make_biclusters(shape=(100, 100), n_clusters=4,\n                                 noise=10, random_state=0)\n        assert X.shape == (100, 100), \"X shape mismatch\"\n\n", "def test_make_classification_multi_class():\n    \"\"\"Test multi-class classification generation.\"\"\"\n    X, y = make_classification(n_samples=100, n_features=20, n_informative=10,\n                               n_classes=5, random_state=0)\n    assert X.shape == (100, 20), \"X shape mismatch\"\n    assert y.shape == (100,), \"y shape mismatch\"\n    assert np.unique(y).shape == (5,), \"Unexpected number of classes\"\n", "    def test_make_classification_n_informative(self):\n        X, y = make_classification(n_samples=100, n_features=20,\n                                   n_informative=5, n_redundant=0,\n                                   n_repeated=0, random_state=0)\n        assert X.shape == (100, 20), \"X shape mismatch\"\n        assert y.shape == (100,), \"y shape mismatch\"\n        # Check if only the first 5 features have a strong correlation\n        # with the target variable\n        correlations = np.abs(np.corrcoef(X.T, y)[0, 1:])\n        assert np.all(correlations[:5] > 0.5)\n        assert np.all(correlations[5:] < 0.2)\n", "def test_make_classification_hypercube_informative_features():\n    # Test that informative features are placed on hypercube vertices.\n    n_samples = 100\n    n_features = 5\n    n_classes = 2\n    n_informative = 3\n    hypercube = True\n    flip_y = 0.0\n    random_state = 0\n\n    for n_clusters_per_class in range(1, 4):\n        X, y = make_classification(\n            n_samples=n_samples, n_features=n_features, n_classes=n_classes,\n            n_informative=n_informative, n_clusters_per_class=n_clusters_per_class,\n            hypercube=hypercube, flip_y=flip_y, random_state=random_state\n        )\n\n        assert X.shape == (n_samples, n_features)\n        assert y.shape == (n_samples,)\n\n        cluster_labels = np.unique(y)\n        for cluster_label in cluster_labels:\n           cluster_indices = np.where(y == cluster_label)[0]\n           cluster_data = X[cluster_indices]\n           for feature_idx in range(n_informative):\n               assert np.all(np.isclose(cluster_data[:, feature_idx],\n                                        np.array([-1, 1]))) or \\\n                      np.all(np.isclose(cluster_data[:, feature_idx],\n                                        np.array([1, -1])))\n", "def test_make_classification_balanced():\n    for weights in [None, [0.25, 0.25, 0.5]]:\n        X, y = make_classification(n_samples=100, n_features=20,\n                                   n_informative=5, n_classes=3,\n                                   weights=weights, random_state=0,\n                                   flip_y=0)\n\n        # Check that classes are roughly balanced\n        assert  np.allclose(np.bincount(y) / len(y),\n                            [1/3] * 3,\n                            rtol=0.1), \"Class balance is off\"\n"], "sample_1083": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_244": ["    def test_formset_with_initial_data_and_extra(self):\n        data = {\n            'choices-TOTAL_FORMS': '3',\n            'choices-INITIAL_FORMS': '2',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        initial_data = [\n            {'choice': 'Initial Zero', 'votes': 0},\n            {'choice': 'Initial One', 'votes': 1},\n        ]\n        formset = ChoiceFormSet(data, initial=initial_data, auto_id=False, prefix='choices')\n        self.assertTrue(formset.is_valid())\n        self.assertEqual(formset.cleaned_data, [\n            {'choice': 'Zero', 'votes': 0},\n            {'choice': 'One', 'votes': 1},\n            {'choice': '', 'votes': ''},\n        ])\n", "    def test_invalid_and_valid(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': '',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [{}, {'choice': ['This field is required.']}]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n", "    def test_all_valid_with_errors(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': '',\n            'choices-0-votes': '1',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [{'choice': ['This field is required.']}, {'votes': ['This field is required.']}]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n", "    def test_all_valid_with_error_in_first_formset(self):\n        data = {\n            'form-TOTAL_FORMS': '2',\n            'form-INITIAL_FORMS': '0',\n            'form-0-title': 'Test',\n            'form-0-pub_date': '1904-06-16',\n            'form-1-title': 'Test',\n            'form-1-pub_date': '',  # <-- this date is missing but required\n        }\n        formset1 = ArticleFormSet(data)\n        formset2 = ArticleFormSet(data)\n        self.assertFalse(all_valid((formset1, formset2)))\n        self.assertEqual([{}, {'pub_date': ['This field is required.']}], formset1.errors)\n        self.assertEqual(formset2.errors, [{}])\n\n", "    def test_all_valid_with_errors(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [{'votes': ['This field is required.']}, {}]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n"], "sample_184": ["    def test_unique_constraint_include_pointing_to_fk(self):\n        class Target(models.Model):\n            pass\n\n        class Model(models.Model):\n            fk = models.ForeignKey(Target, models.CASCADE)\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['id'],\n                        include=['fk'],\n                        name='name',\n                    ),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [])\n", "    def test_unique_constraint_include_pointing_to_fk(self):\n        class Target(models.Model):\n            pass\n\n        class Model(models.Model):\n            fk_1 = models.ForeignKey(Target, models.CASCADE, related_name='target_1')\n            fk_2 = models.ForeignKey(Target, models.CASCADE, related_name='target_2')\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['fk_1_id'],\n                        include=['fk_2'],\n                        name='name',\n                    ),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [])\n\n\n", "    def test_unique_constraint_include_pointing_to_fk(self):\n        class Target(models.Model):\n            pass\n\n        class Model(models.Model):\n            fk_1 = models.ForeignKey(Target, models.CASCADE, related_name='target_1')\n            fk_2 = models.ForeignKey(Target, models.CASCADE, related_name='target_2')\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['fk_1_id'],\n                        include=['fk_2'],\n                        name='name',\n                    ),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [])\n", "    def test_unique_constraint_include_pointing_to_fk(self):\n        class Target(models.Model):\n            pass\n\n        class Model(models.Model):\n            fk = models.ForeignKey(Target, models.CASCADE)\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['id'],\n                        include=['fk'],\n                        name='name',\n                    ),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [])\n", "    def test_unique_constraint_include_pointing_to_fk(self):\n        class Target(models.Model):\n            pass\n\n        class Model(models.Model):\n            fk_1 = models.ForeignKey(Target, models.CASCADE, related_name='target_1')\n            fk_2 = models.ForeignKey(Target, models.CASCADE, related_name='target_2')\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['fk_1_id'],\n                        include=['fk_2'],\n                        name='name',\n                    ),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [])\n"], "sample_1000": ["compilation error", "    def test_octave_inline_assign():\n        a = Symbol('a')\n        expr = a**2\n        assert mcode(expr, assign_to='a', inline=True) == \"a = a.^2;\"\n\n", "def test_octave_function_def():\n    code = \"def myfunc(x):\\n return x**2\" \n    assert mcode(sympify(code), assign_to='result') == (\n        \"function result = myfunc(x)\\n\"\n        \"  result = x.^2;\\n\"\n        \"end\"\n    )\n", "    def test_indexed():\n        from sympy import Eq, IndexedBase, Idx, ccode\n        len_y = 5\n        y = IndexedBase('y', shape=(len_y,))\n        t = IndexedBase('t', shape=(len_y,))\n        Dy = IndexedBase('Dy', shape=(len_y-1,))\n        i = Idx('i', len_y-1)\n        e = Eq(Dy[i], (y[i+1]-y[i])/(t[i+1]-t[i]))\n        assert mcode(e.rhs, assign_to=e.lhs, contract=False) == \\\n            'Dy(i) = (y(i + 1) - y(i))./(t(i + 1) - t(i));'\n", "    def test_indexed_assign():\n        i = Idx('i', 5)\n        A = IndexedBase('A', shape=(5,))\n        e = Eq(A[i], i**2)\n        assert mcode(e.rhs, assign_to=e.lhs, contract=False) == 'A(i) = i.^2'\n        assert mcode(e.rhs, assign_to=e.lhs, contract=True) == 'for i = 1:5\\n    A(i) = i.^2;\\n end'\n"], "sample_884": ["compilation error", "compilation error", "def test_deprecated_property():\n    with pytest.warns(FutureWarning, match=\"n_features_ is deprecated\"):\n        val = MockClass2().n_features_\n    assert val == 10\n", "    def test_deprecated_property(self, attr_name):\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"always\")\n            # Check if deprecation warning is raised when accessing the property\n            with pytest.warns(FutureWarning, match=\"n_features_ is deprecated\"):\n                getattr(MockClass2(), attr_name)\n", "def test_deprecated_property():\n    with pytest.warns(FutureWarning, match=\"n_features_ is deprecated\"):\n        _ = MockClass2().n_features_\n"], "sample_327": ["    def test_invalid_input_type(self):\n        field = JSONField()\n        with self.assertRaisesMessage(ValidationError, 'Enter a valid JSON.'):\n            field.clean(123)\n", "    def test_invalid_jsonstring(self):\n        field = JSONField()\n        with self.assertRaisesMessage(ValidationError, 'Enter a valid JSON.'):\n            field.clean(JSONString('{\"a: \"b\"}'))\n\n", "    def test_jsonstring_type(self):\n        field = JSONField()\n        value = field.clean('{\"a\": \"b\"}')\n        self.assertEqual(type(value), dict)\n        self.assertEqual(value, {'a': 'b'})\n        self.assertIsInstance(value, dict)\n\n", "    def test_jsonstring_representation(self):\n        field = JSONField()\n        self.assertEqual(str(field.clean('{\"a\": \"b\"}')), '{\"a\": \"b\"}')\n        self.assertEqual(\n            str(field.clean('[\"a\", \"b\", \"c\"]')),\n            '[\"a\", \"b\", \"c\"]',\n        )\n", "    def test_invalid_unicode(self):\n        field = JSONField()\n        with self.assertRaisesMessage(ValidationError, 'Enter a valid JSON.'):\n            field.clean('{\"a\": \"\\ud83d\\ude0a\"}')\n"], "sample_1032": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_907": ["    def check(target):\n        class Config:\n            cpp_id_attributes = [\"id_attr\"]\n            cpp_paren_attributes = [\"paren_attr\"]\n        parser = DefinitionParser(target, location=None,\n                                  config=Config())\n        ast, isShorthand = parser.parse_xref_object()\n        parser.assert_end()", "def test_domain_cpp_build_with_add_function_parentheses_is_True_multi(app, status, warning):\n    app.builder.build_all()\n\n        pattern = '<li><p>%s<a .*?><code .*?><span .*?>%s</span></code></a></p></li>' % spec\n        res = re.search(pattern, text)\n        if not res:\n            print(\"Pattern\\n\\t%s\\nnot found in %s\" % (pattern, file))\n            assert False\n    rolePatterns = [\n        ('', 'Sphinx'),\n        ('', 'Sphinx::version'),\n        ('', 'version'),\n        ('', 'List'),\n        ('', 'MyEnum')\n    ]\n    parenPatterns = [\n        ('ref function without parens ', r'paren_1\\(\\)'),\n        ('ref function with parens ', r'paren_2\\(\\)'),\n        ('ref function without parens, explicit title ', 'paren_3_title'),\n        ('ref function with parens, explicit title ', 'paren_4_title'),\n        ('ref op call without parens ', r'paren_5::operator\\(\\)\\(\\)'),\n        ('ref op call with parens ', r'paren_6::operator\\(\\)\\(\\)'),\n        ('ref op call without parens, explicit title ', 'paren_7_title'),\n        ('ref op call with parens, explicit title ', 'paren_8_title')\n    ]\n\n    f = 'multi.html'\n    t = (app.outdir / f).read_text()\n    for s in rolePatterns:\n        check(s, t, f)\n    for s in parenPatterns:\n        check(s, t, f)\n\n", "    def check(target):\n        class Config:\n            cpp_id_attributes = [\"id_attr\"]\n            cpp_paren_attributes = [\"paren_attr\"]\n        parser = DefinitionParser(target, location=None,\n                                  config=Config())\n        ast, isShorthand = parser.parse_xref_object()\n        parser.assert_end()", "def test_domain_cpp_build_no_default_value(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"no-default-value\")\n    assert len(ws) == 0\n", "def test_domain_cpp_build_xref_function_with_template_parameters(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"template-function-xref\")\n    assert len(ws) == 0\n\n\n"], "sample_1034": ["def test_apply_grover():\n    numqubits = 2\n    f = lambda qubits: qubits == IntQubit(2, nqubits=qubits.nqubits)\n    expected = IntQubit(2, nqubits=numqubits)\n    assert qapply(apply_grover(f, numqubits)) == expected\n", "def test_apply_grover():\n    numqubits = 2\n    f = lambda qubits: qubits == IntQubit(2, nqubits=numqubits)\n    expected = IntQubit(2, nqubits=numqubits)\n    assert qapply(apply_grover(f, numqubits)) == expected\n", "def test_apply_grover():\n    f = lambda qubits: qubits == IntQubit(2)\n    result = apply_grover(f, 2, iterations=3)\n    assert qapply(result) == IntQubit(2)\n", "def test_apply_grover():\n    numqubits = 2\n    f = lambda qubits: qubits == IntQubit(2)\n    result = apply_grover(f, numqubits)\n    assert qapply(result) == IntQubit(2, numqubits=numqubits)\n", "def test_apply_grover():\n    nqubits = 2\n    f = lambda qubits: qubits == IntQubit(2, nqubits=qubits.nqubits)\n    result = apply_grover(f, nqubits)\n    assert qapply(result) == IntQubit(2, nqubits=nqubits)\n\n"], "sample_631": ["    def test_unused_wildcard_import_in_try_except(self):\n        \"\"\"\n        Make sure wildcard imports aren't flagged as unused even\n        if they're in a try-except block.\n        \"\"\"\n        node = astroid.parse(\n            '''\n        try:\n            from package import *\n        except ImportError:\n            pass\n        '''\n        )\n        with self.assertNoMessages():\n            self.walk(node)\n", "    def test_cell_var_from_loop(self):\n        \"\"\"Test that cell variable from loop is detected.\"\"\"\n        node = astroid.parse(\n            \"\"\"\n            for i in range(10):\n                x = i\n            return x\n        \"\"\"\n        )\n        with self.assertAddsMessages(\n            Message(\"cell-var-from-loop\", node=node[\"return\"], args=\"x\")\n        ):\n            self.walk(node)\n\n", "    def test_unused_variable_after_nested_function(self):\n        node = astroid.parse(\n            \"\"\"\n            x = 10 \n                nonlocal x\n                x += 1\n        outer()\n        print(x)\n        \"\"\"\n        )\n        with self.assertNoMessages():\n            self.walk(node)\n", "    def test_cell_var_from_loop_nested(self):\n        code = \"\"\"\n            l = []\n            for i in range(10):\n                    nonlocal i\n                    l.append(i)\n                g()\n            return l\n\n        f()\n        \"\"\"\n        node = astroid.parse(code)\n        with self.assertAddsMessages(\n            Message(\"cell-var-from-loop\", node=node['f'][\"body\"][1][\"body\"][1][\"body\"][0]['name'], args='i')\n        ):\n            self.walk(node)\n", "    def test_cell_var_from_loop(self):\n        \"\"\"\n        https://github.com/PyCQA/pylint/issues/3471\n        Make sure cell variables are not flagged\n        as potentially used in loops\n        \"\"\"\n\n        node = astroid.parse(\n            \"\"\"\n            for i in range(10):\n                    print(i)\n        \"\"\"\n        )\n        with self.assertNoMessages():\n            self.walk(node)\n\n"], "sample_295": ["    def test_as_sql_with_complex_expression(self):\n        expr = ExpressionWrapper(\n            F('field1') + F('field2') * 2,\n            output_field=IntegerField(),\n        )\n        compiler = mock.Mock(spec=SQLCompiler)\n        compiler.quote_name.side_effect = lambda x: x\n        self.assertEqual(expr.as_sql(compiler, connection), ('(field1 + (field2 * 2))', []))\n", "    def test_expression_wrapper_deconstruct(self):\n        expr = ExpressionWrapper(F('cost') + F('tax'), IntegerField())\n        path, args, kwargs = expr.deconstruct()\n        self.assertEqual(path, 'django.db.models.expressions.ExpressionWrapper')\n        self.assertEqual(args, (F('cost') + F('tax'),))\n        self.assertEqual(kwargs, {'output_field': IntegerField()})\n", "    def test_resolve_output_field_default(self):\n        expr = ExpressionWrapper(Value(3))\n        self.assertIsInstance(expr.output_field, Value.DEFAULT_OUTPUT_FIELD) \n", "    def test_expression_wrapper_as_sql(self):\n        expr = ExpressionWrapper(Value('foo'), output_field=CharField())\n        compiler = Time.objects.all().query.get_compiler(connection=connection)\n        sql, params = expr.as_sql(compiler, connection)\n        self.assertEqual(sql, '%s')\n        self.assertEqual(params, ['foo'])\n", "    def test_output_field_default(self):\n        expr = ExpressionWrapper(Value(3)) \n        self.assertIsInstance(expr.output_field, IntegerField)\n"], "sample_661": ["def test_escaped_skipreason_unicode_issue3533(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.skip(reason='\u00fc <> 2')\n            pass\n    \"\"\"\n    )\n    _, dom = runandparse(testdir)\n    node = dom.find_first_by_tag(\"testcase\")\n    snode = node.find_first_by_tag(\"skipped\")\n    assert \"\u00fc <> 2\" in snode.text\n    snode.assert_attr(message=\"\u00fc <> 2\")\n", "def test_skip_reason_escaping_issue3533_edge_cases(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.skip(reason='This string contains a \\'&\\'')\n            pass\n\n        @pytest.mark.skip(reason='This string contains a  \"<\"')\n            pass\n    \"\"\"\n    )\n    _, dom = runandparse(testdir)\n    node1 = dom.find_by_tag(\"testcase\")[0]\n    snode1 = node1.find_first_by_tag(\"skipped\")\n    assert \"This string contains a '&'\" in snode1.text\n    snode1.assert_attr(message=\"This string contains a '&'\")\n\n    node2 = dom.find_by_tag(\"testcase\")[1]\n    snode2 = node2.find_first_by_tag(\"skipped\")\n    assert \"This string contains a  \\\"<\\\" \" in snode2.text\n    snode2.assert_attr(message=\"This string contains a  \\\"<\\\" \")\n", "    def test_escaped_skipreason_issue3533_unicode(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n            @pytest.mark.skip(reason='1 <> 2\\u20ac')\n                pass\n        \"\"\"\n        )\n        _, dom = runandparse(testdir)\n        node = dom.find_first_by_tag(\"testcase\")\n        snode = node.find_first_by_tag(\"skipped\")\n        assert \"1 <> 2\\u20ac\" in snode.text\n        snode.assert_attr(message=\"1 <> 2\\u20ac\")\n", "def test_escaped_skipreason_with_quotes(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        @pytest.mark.skip(reason='\"1 <> 2\"')\n            pass\n    \"\"\"\n    )\n    _, dom = runandparse(testdir)\n    node = dom.find_first_by_tag(\"testcase\")\n    snode = node.find_first_by_tag(\"skipped\")\n    assert '\"1 <> 2\"' in snode.text\n    snode.assert_attr(message='\"1 <> 2\"')\n", "    def test_record_testsuite_property_with_setup_teardown(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n                pytest.record_testsuite_property(\"stats\", \"setup\")\n\n                pytest.record_testsuite_property(\"stats\", \"teardown\")\n\n                pass"], "sample_830": ["def test_show_versions():\n    show_versions()\n", "def test_show_versions():\n    # Capture stdout output\n    import io\n    from contextlib import redirect_stdout\n\n    # Redirect stdout to capture output\n    with redirect_stdout(io.StringIO()) as f:\n        show_versions()\n        output = f.getvalue()\n\n    # Assert that expected information is present\n    assert 'System:' in output\n    assert 'Python deps:' in output\n    assert 'BLAS:' in output\n", "def test_show_versions():\n    show_versions()\n", "def test_show_versions():\n    show_versions()\n", "def test_show_versions():\n    show_versions()\n"], "sample_218": ["compilation error", "compilation error", "    def test_trunc_func_with_explicit_timezone_and_isdst(self):\n        start_datetime = datetime(2016, 10, 30, 1, 30, 50, 321)\n        end_datetime = datetime(2017, 3, 12, 3, 10, 50, 123)\n        start_datetime = timezone.make_aware(start_datetime, is_dst=False)\n        end_datetime = timezone.make_aware(end_datetime, is_dst=True)\n        self.create_model(start_datetime, end_datetime)\n\n        melb = pytz.timezone('Australia/Melbourne')\n        \n        self.assertQuerysetEqual(\n            DTModel.objects.annotate(\n                truncated_start=Trunc('start_datetime', 'day', tzinfo=melb, output_field=DateTimeField()),\n                truncated_end=Trunc('end_datetime', 'week', tzinfo=melb, output_field=DateTimeField()),\n            ).order_by('start_datetime'),\n            [\n                (start_datetime, \n                 truncate_to(start_datetime.astimezone(melb), 'day', melb),\n                 truncate_to(end_datetime.astimezone(melb), 'week', melb)),\n                (end_datetime,\n                 truncate_to(start_datetime.astimezone(melb), 'day', melb),\n                 truncate_to(end_datetime.astimezone(melb), 'week', melb)),\n            ],\n            lambda m: (m.start_datetime, m.truncated_start, m.truncated_end)\n        )\n\n", "compilation error", "compilation error"], "sample_1157": ["    def test_issue_11475():\n        x = Symbol('x')\n        assert parse_expr('x^-1', evaluate=False) == Pow(x, -1, evaluate=False)\n\n", "    def test_issue_13969():\n        x = Symbol('x')\n        y = Symbol('y')\n        assert parse_expr('2 x') == parse_expr('2*x') == 2*x\n        assert parse_expr('x 2') == parse_expr('x*2') == x*2\n        assert parse_expr('x y') == parse_expr('x*y') == x*y\n", "def test_convert_xor_issue_12800():\n    x = Symbol('x')\n    y = Symbol('y')\n    a = Symbol('a')\n    xfcn = Function('x')\n    yfcn = Function('y')\n    assert parse_expr(\"x ^ y\", transformations=standard_transformations + (convert_xor,)) == x**y\n    assert parse_expr(\"x^a\", transformations=standard_transformations + (convert_xor,)) == x**a\n    assert parse_expr(\"x^yfcn(x)\", transformations=standard_transformations + (convert_xor,)) == x**(yfcn(x))\n\n", "    def test_implicit_multiplication_application():\n        x = Symbol('x')\n        y = Symbol('y')\n        a = Symbol('a')\n        f = Function('f')\n        assert parse_expr(\"2x\", transformations=standard_transformations + (implicit_multiplication_application,)) == 2*x\n        assert parse_expr(\"2y(x+1)\", transformations=standard_transformations + (implicit_multiplication_application,)) == 2*y*(x+1)\n        assert parse_expr(\"a(x+1)\", transformations=standard_transformations + (implicit_multiplication_application,)) == a*(x+1)\n        assert parse_expr(\"f(x+1) y\", transformations=standard_transformations + (implicit_multiplication_application,)) == f(x+1)*y \n", "    def test_issue_13198():\n        assert parse_expr('1/2(x+1)') == (1/2)*(x+1)\n"], "sample_1197": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_946": ["compilation error", "def test_pyobject_type_signature(app):\n    text = \".. py:object:: MyClass\\n\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, addnodes.index)\n", "def test_python_use_qualified_type_names(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"n\"><a class=\"reference internal\" href=\"#foo.Name\" title=\"foo.Name\">'\n            '<span class=\"pre\">foo.Name</span></a></span>' in content)\n    assert '<span class=\"n\"><span class=\"pre\">foo.Age</span></span>' in content\n", "def test_pyclass_init(app):\n    text = (\".. py:class:: MyClass\\n\"\n            \"\\n\"\n            \"   def __init__(self, arg1, arg2):\\n\"\n            \"       self.arg1 = arg1\\n\"\n            \"       self.arg2 = arg2\\n\"\n            )\n    doctree = restructuredtext.parse(app, text)\n\n    # Verify the __init__ method is parsed correctly\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                [desc_name, \"MyClass\"])],\n                [desc_content,\n                 [nodes.paragraph, nodes.literal_block, nodes.line, nodes.literal_block, nodes.line]])]))\n\n", "def test_python_nosignatures(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert '<dt id=\"module-foo\"><code class=\"descname\">foo</code><span class=\"sig-paren\">(</span><span class=\"sig-paren\">)</span></dt>' not in content\n\n"], "sample_74": ["    def test_empty_database(self):\n        self.assertEqual(\n            self._run_it({\n                'user': 'someuser',\n                'password': 'somepassword',\n                'host': 'somehost',\n                'port': '444',\n            }), (\n                ['psql', '-U', 'someuser', '-h', 'somehost', '-p', '444'],\n                {'PGPASSWORD': 'somepassword'},\n            )\n        )\n\n", "    def test_empty_database(self):\n        self.assertEqual(\n            self._run_it({\n                'user': 'someuser',\n                'password': 'somepassword',\n                'host': 'somehost',\n                'port': '444',\n            }), (\n                ['psql', '-U', 'someuser', '-h', 'somehost', '-p', '444'],\n                {'PGPASSWORD': 'somepassword'},\n            )\n        )\n", "    def test_empty_args(self):\n        self.assertEqual(\n            self._run_it({\n                'database': 'dbname',\n            }), (\n                ['psql', 'dbname'],\n                {},\n            )\n        )\n", "    def test_empty_params(self):\n        self.assertEqual(\n            self._run_it({}), (\n                ['psql'],\n                {},\n            )\n        )\n", "    def test_empty_password(self):\n        self.assertEqual(\n            self._run_it({\n                'database': 'dbname',\n                'user': 'someuser',\n                'host': 'somehost',\n                'port': '444',\n                'password': '',\n            }), (\n                ['psql', '-U', 'someuser', '-h', 'somehost', '-p', '444', 'dbname'],\n                {},\n            )\n        )\n"], "sample_839": ["    def test_stop_words_custom_preprocessor_no_tokens():\n        # Regression test: ensure that the stop_words check passes when the\n        # preprocessor returns an empty list of tokens.\n\n        data = ['some text']\n        custom_preprocessor = lambda x: []\n        vec = CountVectorizer(preprocessor=custom_preprocessor,\n                              stop_words=['some'])\n\n        # Should not raise a UserWarning\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"error\")\n            vec.fit_transform(data)\n\n\n", "compilation error", "    def analyzer(doc):\n        raise ValueError(\"testing\")\n", "def test_callable_analyzer_file_not_found(tmpdir, Estimator):\n        return open(doc, 'r').read()\n\n    if issubclass(Estimator, HashingVectorizer):\n        pytest.xfail('HashingVectorizer is not supported on PyPy')\n    f = str(tmpdir.join(\"file.txt\"))\n    os.remove(f)\n    with pytest.raises(FileNotFoundError):\n        Estimator(analyzer=analyzer, input='filename').fit_transform([f])\n", "    def test_vectorizer_fit_transform_empty_input(Estimator):\n        # Test that fit_transform handles empty input gracefully\n        if issubclass(Estimator, HashingVectorizer):\n            pytest.xfail('HashingVectorizer is not supported on PyPy')\n        X = Estimator().fit_transform([])\n        assert X.shape[0] == 0\n        assert X.shape[1] == 0\n"], "sample_293": ["    def test_invalid_reverse(self):\n        msg = 'No reverse match found for \\'lookbehind-positive\\' with kwargs=\\'{\\'city\\': \\'not-a-city\\'}\\'.'\n        with self.assertRaisesMessage(NoReverseMatch, msg):\n            reverse('lookbehind-positive', kwargs={'city': 'not-a-city'})\n", "    def test_invalid_reverse_no_match(self):\n        with self.assertRaises(NoReverseMatch):\n            reverse('lookahead-positive', kwargs={'city': 'other-city'})\n", "    def test_reverse_missing_argument(self):\n        with self.assertRaises(ValueError):\n            reverse('lookahead-positive')\n", "    def test_lookahead_with_optional_group(self):\n        self.assertEqual(resolve('/lookahead-optional/city/').kwargs, {'city': 'city'})\n        self.assertEqual(resolve('/lookahead-optional/').kwargs, {})\n", "    def test_include_no_namespace_provided(self):\n        msg = 'You must provide app_name when using include() with a namespace.'\n        with self.assertRaisesMessage(ImproperlyConfigured, msg):\n            include(self.url_patterns, namespace='namespace')\n"], "sample_1134": ["compilation error", "    def test_issue_17453():\n        from sympy.printing.latex import latex\n        from sympy.functions.elementary.exponential import exp\n        from sympy import Symbol\n\n        x = Symbol('x')\n        assert latex(exp(2*x)*exp(-x)) == r'\\exp{\\left(x \\right)}'\n", "compilation error", "compilation error", "compilation error"], "sample_691": ["def test_get_timeout_config_value(pytester: Pytester) -> None:\n    \"\"\"Test that the timeout configuration value is correctly read.\"\"\"\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        faulthandler_timeout = 5.0\n        \"\"\"\n    )\n    result = pytester.runpytest_subprocess()\n    assert result.ret == 0\n\n    from _pytest.faulthandler import FaultHandlerHooks\n    config = pytester.parseconfig()\n    assert FaultHandlerHooks.get_timeout_config_value(config) == 5.0\n", "def test_plugin_registration(pytester: Pytester) -> None:\n    \"\"\"Test that the plugin is registered correctly.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n\n    result = pytester.runpytest_subprocess()\n    result.stdout.fnmatch_lines(\n        [\"*faulthandler-hooks*\", \"*1 passed*\", \"*1 passed in*\", \"*100%\"]\n    )\n    assert \"faulthandler-hooks\" in [p.name for p in result.config.pluginmanager.plugins]\n    assert result.ret == 0\n\n", "def test_faulthandler_cleanup(faulthandler_timeout: int, pytester: Pytester, tmpdir) -> None:\n    \"\"\"Test that faulthandler is properly cleaned up after pytest finishes.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n            import faulthandler\n            assert faulthandler.is_enabled()\n    \"\"\"\n    )\n    result = pytester.run(\n        sys.executable,\n        \"-X\",\n        \"faulthandler\",\n        \"-mpytest\",\n        pytester.path,\n        \"-o\",\n        f\"faulthandler_timeout={faulthandler_timeout}\",\n    )\n    result.stdout.fnmatch_lines(\"*1 passed*\")\n    assert result.ret == 0\n\n    # After pytest finishes, faulthandler should be enabled again and attached to sys.stderr\n    import faulthandler\n\n    assert faulthandler.is_enabled()\n    assert faulthandler.get_file() == sys.__stderr__\n\n", "def test_faulthandler_timeout_disabled(faulthandler_timeout: int, pytester: Pytester) -> None:\n    \"\"\"Test that setting faulthandler_timeout to 0 disables the timeout feature.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import time\n            time.sleep(1)\n    \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        faulthandler_timeout = {faulthandler_timeout}\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    assert result.ret == 0\n\n", "    def test_get_timeout_config_value(pytester: Pytester) -> None:\n        \"\"\"Test that get_timeout_config_value correctly retrieves the timeout value.\"\"\"\n        pytester.makeini(\n            \"\"\"\n            [pytest]\n            faulthandler_timeout = 10.5\n            \"\"\"\n        )\n        from _pytest.faulthandler import FaultHandlerHooks\n        assert FaultHandlerHooks.get_timeout_config_value(pytester.config) == 10.5\n"], "sample_80": ["    def test_related_isnull(self):\n        query = Query(ObjectC)\n        where = query.build_where(Q(objectb__isnull=True))\n        lookup = where.children[0]\n        self.assertIsInstance(lookup, RelatedIsNull)\n        self.assertEqual(lookup.lhs.target, ObjectC._meta.get_field('objectb'))\n", "    def test_related_isnull(self):\n        query = Query(Author)\n        where = query.build_where(Q(item__isnull=True))\n        lookup = where.children[0]\n        self.assertIsInstance(lookup, RelatedIsNull)\n        self.assertEqual(lookup.lhs.target, Author._meta.get_field('item'))\n\n", "    def test_related_isnull(self):\n        query = Query(Author)\n        where = query.build_where(Q(item__isnull=True))\n        lookup = where.children[0]\n        self.assertIsInstance(lookup, RelatedIsNull)\n        self.assertEqual(lookup.lhs.target, Author._meta.get_field('item').field)\n", "    def test_related_isnull(self):\n        query = Query(ObjectC)\n        where = query.build_where(Q(objectb__isnull=True))\n        lookup = where.children[0]\n        self.assertIsInstance(lookup, RelatedIsNull)\n        self.assertEqual(lookup.lhs.target, ObjectC._meta.get_field('objectb'))\n", "    def test_related_isnull(self):\n        query = Query(ObjectC)\n        where = query.build_where(Q(objectb__isnull=True))\n        lookup = where.children[0]\n        self.assertIsInstance(lookup, IsNull)\n        self.assertIsInstance(lookup.lhs, RelatedIsNull)\n        self.assertEqual(lookup.lhs.field, ObjectC._meta.get_field('objectb'))\n"], "sample_715": ["    def test_cross_val_predict_pandas_single_column():\n        # check cross_val_predict works with pandas dataframe with single column\n        data = {'col1': [1, 2, 3, 4, 5]}\n        df = pd.DataFrame(data)\n        y = np.array([0, 1, 0, 1, 0])\n        clf = LogisticRegression()\n        predictions = cross_val_predict(clf, df, y, cv=5)\n        assert_array_equal(len(predictions), len(y))\n", "    def test_cross_val_predict_pandas_index():\n        # check cross_val_predict preserves pandas index\n        iris = load_iris()\n        X, y = iris.data, iris.target\n        X_df = MockDataFrame(X, index=list(range(len(X))))\n        clf = LogisticRegression()\n        predictions = cross_val_predict(clf, X_df, y)\n        assert_array_equal(X_df.index, predictions.index)\n", "    def test_cross_val_predict_pandas_multioutput():\n        # check cross_val_predict doesn't destroy pandas dataframe for\n        # multioutput\n        types = [(MockDataFrame, MockDataFrame)]\n        try:\n            from pandas import Series, DataFrame\n            types.append((Series, DataFrame))\n        except ImportError:\n            pass\n        for TargetType, InputFeatureType in types:\n            # X dataframe, y series\n            iris = load_iris()\n            X, y = iris.data, iris.target\n            y_multi = np.vstack((y, y)).T\n            X_df, y_ser = InputFeatureType(X), TargetType(y_multi)\n            check_df = lambda x: isinstance(x, InputFeatureType)\n            check_series = lambda x: isinstance(x, TargetType)\n            clf = CheckingClassifier(check_X=check_df, check_y=check_series)\n            cross_val_predict(clf, X_df, y_ser)\n", "def test_cross_val_predict_with_groups():\n    X, y = make_classification(n_samples=30, n_features=10, n_classes=2,\n                               n_informative=5, random_state=0)\n    groups = np.repeat([0, 1], 15)\n    est = LogisticRegression()\n    cv = GroupKFold(n_splits=5)\n    predictions = cross_val_predict(est, X, y, groups=groups, cv=cv)\n    assert_equal(len(predictions), len(y))\n", "def test_cross_val_predict_pandas_empty():\n    # check cross_val_predict works with empty DataFrame\n\n    df = pd.DataFrame()\n    y = pd.Series([])\n\n    clf = LogisticRegression()\n    with warnings.catch_warnings(record=True) as w:\n        predictions = cross_val_predict(clf, df, y, cv=KFold(n_splits=2))\n    if len(w) > 0:\n        raise RuntimeError(\"Unexpected warning: %r\" % w[0].message)\n    assert_array_equal(predictions, [])\n\n"], "sample_1158": ["compilation error", "compilation error", "compilation error", "compilation error", "def test_issue_18493():\n    x = Symbol('x')\n    assert sympify('x**(-1)') == 1/x\n"], "sample_1067": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_31": ["    def test_to_latex_units(self, write, tmp_path, format):\n        fp = tmp_path / \"test_to_latex_units.tex\"\n        write(fp, format=format)\n        tbl = QTable.read(fp)\n        for col in tbl.columns:\n            assert tbl[col].unit in (None, u.one) or isinstance(tbl[col].unit, u.Unit)\n", "    def test_latex_unit(self, write, cosmo, tmp_path, format):\n        fp = tmp_path / \"test_latex_unit.tex\"\n        write(cosmo, fp, format=format)\n        tbl = QTable.read(fp)\n        for name in cosmo.__parameters__:\n            assert name in tbl.colnames\n\n", "    def test_write_latex_unit(self, write, tmp_path, format):\n        \"\"\"\n        Test writing a LaTeX file with units, ensuring units are \n        correctly handled in the output.\n        \"\"\"\n        fp = tmp_path / f\"test_write_latex_unit.{format.split('.')[0]}\"\n        write(fp, format=format)\n        tbl = QTable.read(fp)\n        for col_name in tbl.colnames[2:]:  # Skip cosmology class & name\n            param = getattr(self.cosmo, col_name, None)\n            if isinstance(param, Parameter) and param.unit is not None:\n                assert tbl[col_name].unit == param.unit\n", "    def test_write_latex_units(self, write, cosmo, tmp_path, format):\n        \"\"\"Test that units are correctly handled in the LaTeX output.\"\"\"\n        fp = tmp_path / \"test_write_latex_units.tex\"\n        write(fp, format=format)\n        tbl = QTable.read(fp)\n        for name in cosmo.__parameters__:\n            param = getattr(type(cosmo), name)\n            if isinstance(param, Parameter) and param.unit is not None:\n                assert tbl[name].unit == param.unit\n", "    def test_write_latex_unit_preservation(self, write, cosmo, tmp_path, format):\n        \"\"\"Test that units are preserved when writing to latex.\"\"\"\n        fp = tmp_path / \"test_write_latex_unit_preservation.tex\"\n        write(fp, format=format)\n\n        tbl = QTable.read(fp, format=format)\n        for name in cosmo.__parameters__:\n            param = getattr(type(cosmo), name)\n            if isinstance(param, Parameter) and param.unit is not None:\n                assert tbl[name].unit == param.unit\n"], "sample_666": ["def test_capture_with_logging_and_exception(testdir, capsys):\n    # Issue 3819\n    # capture should work with live cli logging even when exception occurs\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import sys\n\n        logger = logging.getLogger(__name__)\n\n            logging.info(\"something\")\n            try:\n                raise ValueError(\"Intentional Error\")\n            except Exception as e:\n                print(f\"Caught exception: {e}\")\n                sys.stdout.flush()\n                sys.stderr.flush()\n                logging.info(\"something else\")\n\n\n\n        \n        \"\"\"\n    )\n\n    result = testdir.runpytest()\n    assert result.ret == 1\n    out, err = capsys.readouterr()\n    assert \"something\" in err\n    assert \"something else\" in err\n    assert \"Caught exception: ValueError('Intentional Error')\" in out\n", "def test_capture_with_no_output():\n    with capsys.disabled():\n        import logging\n\n        logging.basicConfig(level=logging.INFO)\n        logging.info(\"this should not be captured\")\n\n    assert capsys.readouterr().out == \"\"\n", "def test_StdCaptureFD_inherit_fd(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import os\n        import sys\n\n            fd = os.dup(1)  \n            os.write(fd, b\"hello\\\\n\")\n            assert 0\n\n            with open(\"out.txt\", \"w\") as f:\n              os.dup2(f.fileno(), 1)\n\n            with capsys.capture(out=True, err=False, in_=False) as cap:\n                cap.start_capturing()\n                try:\n                    import pytest\n                    print(\"hello\")\n                    assert 0\n                finally:\n                    cap.stop_capturing()\n            out = cap.readouterr()[0]\n            assert \"hello\" in out\n\n            with capsys.capture(out=True, err=False, in_=False) as cap:\n                cap.start_capturing()\n                try:\n                    print(\"hello\")\n                    assert 0\n                finally:\n                    cap.stop_capturing()\n            out = cap.readouterr()[0]\n            assert \"hello\" in out\n\n    \"\"\"\n    )\n    result = testdir.runpytest_subprocess(p)\n\n    assert result.ret == 0\n", "def test_EncodedFile_write_errors():\n    ef = capture.EncodedFile(io.BytesIO(), encoding=\"utf-8\")\n    with pytest.raises(UnicodeEncodeError):\n        ef.write(b\"\\xff\" )\n", "def test_dontreadfrominput_get_out_err(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import sys\n            out, err = sys.stdout, sys.stderr\n            assert out.errors == \"replace\"\n            assert err.errors == \"replace\"\n    \"\"\"\n    )\n    reprec = testdir.inline_run()\n    reprec.assertoutcome(passed=1)\n"], "sample_673": ["    def test_doctest_report_invalid_format_with_incorrect_case(self, testdir):\n        result = self._run_doctest_report(testdir, \"UdIfF\")\n        result.stderr.fnmatch_lines(\n            [\n                \"*error: argument --doctest-report: invalid choice: 'UdIfF' (choose from*\"\n            ]\n        )\n", "    def test_doctest_report_default_minimal(self, testdir):\n        result = self._run_doctest_report(testdir, \"default\")\n        result.stdout.fnmatch_lines(\n            [\n                \"Expected:\",\n                \"       a  b\",\n                \"    0  1  4\",\n                \"    1  2  4\",\n                \"    2  3  6\",\n                \"Got:\",\n                \"       a  b\",\n                \"    0  1  4\",\n                \"    1  2  5\",\n                \"    2  3  6\",\n            ]\n        )\n", "    def test_doctest_report_none(self, testdir):\n        result = self._run_doctest_report(testdir, \"none\")\n        result.stdout.no_fnmatch_line(\"*Expected:*\")\n        result.stdout.no_fnmatch_line(\"*Got:*\")\n", "    def test_doctest_report_default(self, testdir):\n        result = self._run_doctest_report(testdir, \"default\")\n        result.stdout.fnmatch_lines(\n            [\n                \"*Expected:*\",\n                \"       a  b\",\n                \"    0  1  4\",\n                \"    1  2  4\",\n                \"    2  3  6\",\n                \"*Got:*\",\n                \"       a  b\",\n                \"    0  1  4\",\n                \"    1  2  5\",\n                \"    2  3  6\",\n            ]\n        )\n", "    def test_doctest_report_with_no_output(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n                '''\n                >>> foo()\n                '''\n                pass\n            \"\"\"\n        )\n        result = self._run_doctest_report(testdir, \"udiff\")\n        result.stdout.fnmatch_lines([\"--- a/test_doctest.py\"])\n"], "sample_789": ["compilation error", "    def test_sample_weight_shape():\n        # Test that sample_weight is correctly reshaped if it's a 1D array.\n\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        y = np.array([1, 2, 3])\n        sample_weight = np.array([0.1, 0.2, 0.3])\n\n        clf = AdaBoostClassifier(n_estimators=1)\n        clf.fit(X, y, sample_weight=sample_weight)\n        # Check that the shape of the sample_weight array used in the\n        # _boost method is correct.\n        assert clf.estimator_weights_.shape == (1,)\n        assert clf._n_classes == 3\n", "compilation error", "compilation error", "    def test_sample_weight_shape_mismatch():\n        # Test that it raises a ValueError if sample_weight shape doesn't match X\n        clf = AdaBoostClassifier()\n\n        # X with 5 samples, y with 4 samples\n        X = np.random.rand(5, 3)\n        y = np.random.randint(0, 2, size=(4,))\n        sample_weight = np.random.rand(4,)\n\n        assert_raises(ValueError, clf.fit, X, y, sample_weight=sample_weight)\n\n"], "sample_314": ["    def test_html_autocomplete_attributes(self):\n        form = AdminPasswordChangeForm(self.u1, {})\n        self.assertEqual(form.fields['password1'].widget.attrs['autocomplete'], 'new-password')\n        self.assertEqual(form.fields['password2'].widget.attrs['autocomplete'], 'new-password') \n", "    def test_html_autocomplete_attributes(self):\n        form = AdminPasswordChangeForm(self.u1, {})\n        self.assertEqual(form.fields['password1'].widget.attrs['autocomplete'], 'new-password')\n        self.assertEqual(form.fields['password2'].widget.attrs['autocomplete'], 'new-password')\n", "    def test_custom_email_field_password_reset(self):\n        email = 'test@mail.com'\n        CustomEmailField.objects.create_user('test name', 'test password', email)\n        data = {'email': email}\n        form = PasswordResetForm(data)\n        self.assertTrue(form.is_valid())\n        form.save()\n        self.assertEqual(form.cleaned_data['email'], email)\n        self.assertEqual(len(mail.outbox), 1)\n        self.assertEqual(mail.outbox[0].to, [email])\n\n", "    def test_html_autocomplete_attributes(self):\n        form = AdminPasswordChangeForm(User(), {'password1': 'test', 'password2': 'test'})\n        self.assertEqual(form.fields['password1'].widget.attrs['autocomplete'], 'new-password')\n        self.assertEqual(form.fields['password2'].widget.attrs['autocomplete'], 'new-password')\n", "    def test_html_autocomplete_attributes(self):\n        form = AdminPasswordChangeForm(self.u1)\n        self.assertEqual(form.fields['password1'].widget.attrs['autocomplete'], 'new-password')\n        self.assertEqual(form.fields['password2'].widget.attrs['autocomplete'], 'new-password')\n"], "sample_891": ["    def test_label_ranking_average_precision_score_error():\n        y_true = np.array([[1, 0, 0], [0, 0, 1]])\n        y_score = np.array([[0.5, 0.9, 0.6], [0, 0, 1]])\n        with pytest.raises(ValueError, match=\"labels\"):\n            label_ranking_average_precision_score(y_true, y_score, labels=[0, 1, 2])\n", "compilation error", "    def test_top_k_accuracy_score_empty_y_score():\n        # Check that the function handles empty y_score correctly\n        y_true = np.array([0, 1, 2, 3])\n        y_score = np.array([])\n        with pytest.raises(ValueError, match=\"y_score cannot be empty\"):\n            top_k_accuracy_score(y_true, y_score, k=2)\n", "    def test_label_ranking_average_precision_score_empty():", "compilation error"], "sample_485": ["    def test_urlize_nofollow(self):\n        tests = (\n            (\n                \"Search for google.com/?q=! and see.\",\n                'Search for <a href=\"http://google.com/?q=\" rel=\"nofollow\">google.com/?q=</a>! and see.',\n            ),\n        )\n        for value, output in tests:\n            with self.subTest(value=value):\n                self.assertEqual(urlize(value, nofollow=True), output)\n", "    def test_urlize_nofollow(self):\n        self.assertEqual(\n            urlize(\"https://www.django-project.org/\", nofollow=True),\n            '<a href=\"https://www.django-project.org/\" rel=\"nofollow\">https://www.django-project.org/</a>',\n        )\n        self.assertEqual(\n            urlize(\"http://example.com\", nofollow=True, trim_url_limit=10),\n            '<a href=\"http://example.com\" rel=\"nofollow\">http://example.com</a>',\n        )\n", "    def test_urlize_nofollow(self):\n        self.assertEqual(\n            urlize(\"https://example.com\", nofollow=True),\n            '<a href=\"https://example.com\" rel=\"nofollow\">https://example.com</a>',\n        )\n", "    def test_urlize_trim_url(self):\n        tests = (\n            (\"This is a very very very long url https://example.com/super/duper/long/path \"\n             \"and more text here\",\n             \"This is a very very very long url <a href=\\\"https://example.com/super/duper/long/path\\\">https://example.com/super/duper/long/p...</a> and more text here\"),\n            (\"https://example.com/super/duper/long/path\",\n             \"<a href=\\\"https://example.com/super/duper/long/path\\\">https://example.com/super/duper/long/path</a>\"),\n            (\"This is a very very very long url http://example.com/super/duper/long/path and more text here\",\n             \"This is a very very very long url <a href=\\\"http://example.com/super/duper/long/path\\\">http://example.com/super/duper/long/p...</a> and more text here\"),\n        )\n        for value, output in tests:\n            with self.subTest(value=value):\n                self.assertEqual(urlize(value, trim_url_limit=25), output)\n", "    def test_urlize_trim_url_limit(self):\n        self.assertEqual(\n            urlize(\"Here is a really really really long URL http://www.example.com/very/long/path/to/a/resource\", trim_url_limit=20),\n            'Here is a really really really long URL <a href=\"http://www.example.com/very/long/path/to/a/resource\">http://www.example.com/very/long/path...</a>',\n        )\n        self.assertEqual(\n            urlize(\"http://www.example.com\", trim_url_limit=20),\n            '<a href=\"http://www.example.com\">http://www.example.com</a>',\n        )\n"], "sample_418": ["    def test_none(self):\n        self.assertIs(length_is(None, 0), False)\n        self.assertIs(length_is(None, 1), False)\n", "    def test_length_is12(self):\n        output = self.engine.render_to_string(\"length_is12\", {\"var\": \"\"})\n        self.assertEqual(output, \"\")\n", "    def test_length_is12(self):\n        output = self.engine.render_to_string(\"length_is12\", {\"my_dict\": {\"a\": 1, \"b\": 2}})\n        self.assertEqual(output, \"True\")\n", "    def test_length_is12(self):\n        output = self.engine.render_to_string(\"length_is12\", {})\n        self.assertEqual(output, \"\")\n", "    def test_length_is12(self):\n        output = self.engine.render_to_string(\n            \"length_is12\", {\"my_list\": [\"one\", \"two\"]}\n        )\n        self.assertEqual(output, \"Two items\")\n"], "sample_1119": ["compilation error", "def test_inverse_properties():\n    assert (C.I*C).simplify() == Identity(n)\n    assert (C*C.I).simplify() == Identity(n)\n    assert (C.I*ZeroMatrix(n, n)).simplify() == ZeroMatrix(n, n)\n    assert (ZeroMatrix(m, n)*C.I).simplify() == ZeroMatrix(m, n)\n", "def test_MatPow():\n    assert MatPow(C, 2).shape == (n, n)\n    assert MatPow(C, -1).args == (C, -1)\n\n    assert MatPow(C, 0) == Identity(C.rows)\n    assert MatPow(C, 1) == C\n\n    assert MatPow(C, 2)*MatPow(C, -2) == Identity(C.rows)\n\n    assert (C**2).args == (C, 2)\n", "compilation error", "def test_power():\n    assert MatPow(C, 2).args == (C, 2)\n    assert MatPow(C, 2).shape == (n, n)\n\n    assert (C*C).refine(Q.invertible(C)) == MatPow(C, 2).refine(\n        Q.invertible(C))\n\n    assert MatPow(C, -1) == C.I\n\n    raises(ValueError, lambda: MatPow(C, -2))\n    raises(ValueError, lambda: MatPow(C, S.Half))\n\n\n"], "sample_622": ["    def test_encode_cf_variable_with_fill_value_and_dtype(self) -> None:\n        original = Variable([\"x\"], [1, 2, np.nan], {\"dtype\": \"int32\", \"_FillValue\": -999})\n        expected = Variable([\"x\"], [1, 2, -999], {\"dtype\": \"int32\"})\n        actual = conventions.encode_cf_variable(original)\n        assert_identical(expected, actual) \n", "    def test_decode_cf_variable_with_missing_value_and_fill_value(self) -> None:\n        v = Variable(\n            [\"t\"],\n            np.array([np.nan, 1, 2], dtype=float),\n            {\"missing_value\": 0, \"_FillValue\": -999},\n        )\n        decoded = conventions.decode_cf_variable(\"t\", v)\n        expected = Variable(\n            [\"t\"], [np.nan, 1, 2], {\"missing_value\": 0, \"_FillValue\": -999}\n        )\n        assert_identical(decoded, expected)\n", "    def test_encode_cf_variable_with_fill_value(self) -> None:\n        v = Variable(\n            [\"t\"],\n            [0, 1, np.nan],\n            {\"_FillValue\": -999, \"units\": \"meters\"},\n        )\n        expected = Variable(\n            [\"t\"],\n            [0, 1, np.nan],\n            {\"_FillValue\": -999, \"units\": \"meters\"},\n        )\n        actual = conventions.encode_cf_variable(\"t\", v)\n        assert_identical(actual, expected)\n", "    def test_decode_cf_with_invalid_units(self) -> None:\n        original = Variable([\"t\"], [0, 1, 2], {\"units\": \"invalid_units\"})\n        with pytest.warns(UserWarning, match=\"unable to decode\"):\n            actual = conventions.decode_cf_variable(\"t\", original)\n        assert_identical(actual, original)\n", "    def test_decode_cf_variable_with_scalar_fill_value(self) -> None:\n        v = Variable([\"t\"], [np.nan, np.nan, 2], {\"_FillValue\": np.nan})\n        v_decoded = conventions.decode_cf_variable(\"test2\", v)\n        assert_identical(v, v_decoded)\n"], "sample_962": ["    def test_restify_type_hints_NewType():\n        assert restify(MyInt) == \":py:class:`tests.test_util_typing.MyInt`\"\n\n\n", "def test_restify_type_hints_Type():\n    from types import TypeType  # type: ignore\n    assert restify(TypeType) == \":py:obj:`types.Type`\"\n    assert restify(Type[int]) == \":py:class:`type`\\\\ [:py:class:`int`]\"\n", "    def test_restify_type_hints_newtype():\n        MyNewType = NewType('MyNewType', int)\n        assert restify(MyNewType) == ':py:class:`tests.test_util_typing.MyNewType`'\n", "def test_restify_type_hints_param_spec():\n    from typing import ParamSpec\n    P = ParamSpec('P')\n    assert restify(Callable[P, int]) == ':py:class:`~typing.Callable`\\\\ [:py:obj:`P`], :py:class:`int`]'\n\n", "    def test_restify_type_hints_NewType():\n        MyInt = NewType('MyInt', int)\n        assert restify(MyInt) == ':py:class:`tests.test_util_typing.MyInt`'\n"], "sample_409": ["    def test_blocktranslate_with_empty_string(self):\n        \"\"\"\n        Test blocktranslate with an empty string as the translatable content.\n        \"\"\"\n        t = self.get_template(\n            \"{% load i18n %}{% blocktranslate %}{% endblocktranslate %}\"\n        )\n        with translation.override(\"de\"):\n            self.assertEqual(t.render(Context({})), \"\")\n", "    def test_blocktranslate_empty_string(self):\n        t = self.get_template(\n            \"{% load i18n %}{% blocktranslate %}{% endblocktranslate %}\"\n        )\n        with translation.override(\"de\"):\n            self.assertEqual(t.render(Context({})), \"\")\n", "    def test_blocktranslate_with_variable_as_context(self):\n        t = self.get_template(\n            \"{% load i18n %}{% blocktranslate context context_variable %}\"\n            \"Hello world!{% endblocktranslate %}\"\n        )\n        with translation.override(\"de\"):\n            self.assertEqual(\n                t.render(Context({\"context_variable\": \"greeting\"}),), \"Hallo Welt!\"\n            )\n", "    def test_blocktranslate_empty_string(self):\n        t = self.get_template(\"{% load i18n %}{% blocktranslate %}{% endblocktranslate %}\")\n        with translation.override(\"de\"):\n            self.assertEqual(t.render(Context({})), \"\")\n", "    def test_blocktranslate_tag_with_safe_string(self):\n        with translation.override(\"de\"):\n            t = self.get_template(\n                \"{% load i18n %}{% blocktranslate asvar title %}\"\n                \"{{title}}other text\"\n                \"{% endblocktranslate %}\"\n            )\n\n            output = t.render(Context({\"title\": \"<Main Title> \"}))\n            self.assertEqual(output, \"&lt;Main Title&gt;other text\")\n"], "sample_537": ["    def test_psd_detrend_linear_onesided():\n        u = np.array([0, 1, 2, 3, 1, 2, 1])\n        dt = 1.0\n        Su = np.abs(np.fft.fft(u) * dt)**2 / (dt * u.size)\n        P, f = mlab.psd(u, NFFT=u.size, Fs=1/dt, window=mlab.window_none,\n                    detrend=mlab.detrend_linear, noverlap=0, pad_to=None,\n                    scale_by_freq=None,\n                    sides='onesided')\n        Su_1side_detrend = np.append([Su[0]], Su[1:4] + Su[4:][::-1])\n        assert_allclose(P, Su_1side_detrend, atol=1e-06)\n", "    def test_psd_windowhanning_detrend_mean(self):\n        if self.NFFT_density is None:\n            return\n        ydata = np.arange(self.NFFT_density)\n        ycontrol = np.zeros(self.NFFT_density)\n        ydata1 = ydata+5\n        ydata2 = ydata+3.3\n        \n        windowVals = mlab.window_hanning(np.ones_like(ycontrol))\n        ycontrol1 = ycontrol\n        ycontrol2 = ycontrol\n        ydata = np.vstack([ydata1, ydata2])\n        ycontrol = np.vstack([ycontrol1, ycontrol2])\n        ydata = np.tile(ydata, (20, 1))\n        ycontrol = np.tile(ycontrol, (20, 1))\n        ydatab = ydata.T.flatten()\n        ydataf = ydata.flatten()\n        ycontrol = ycontrol.flatten()\n        spec_g, fsp_g = mlab.psd(x=ydataf,\n                                 NFFT=self.NFFT_density,\n                                 Fs=self.Fs,\n                                 noverlap=0,\n                                 sides=self.sides,\n                                 detrend=np.mean,\n                                 window=mlab.window_hanning)\n        spec_b, fsp_b = mlab.psd(x=ydatab,\n                                 NFFT=self.NFFT_density,\n                                 Fs=self.Fs,\n                                 noverlap=0,\n                                 sides=self.sides,\n                                 detrend=np.mean,\n                                 window=mlab.window_hanning)\n        spec_c, fsp_c = mlab.psd(x=ycontrol,\n                                 NFFT=self.NFFT_density,\n                                 Fs=self.Fs,\n                                 noverlap=0,\n                                 sides=self.sides,\n                                 window=mlab.window_none)\n        spec_c *= len(ycontrol1)/(windowVals**2).sum()\n        assert_array_equal(fsp_g, fsp_c)\n", "    def test_psd_complex(self):\n        x = np.random.randn(1024) + 1j * np.random.randn(1024)\n        Pxx, f = mlab.psd(x, NFFT=1024, Fs=1)\n        assert_allclose(Pxx, np.abs(np.fft.fft(x))**2 / 1024, atol=1e-06)\n", "    def test_psd_window_hanning_onesided_norm():\n        u = np.array([0, 1, 2, 3, 1, 2, 1])\n        dt = 1.0\n        Su = np.abs(np.fft.fft(u) * dt)**2 / (dt * u.size)\n        P, f = mlab.psd(u, NFFT=u.size, Fs=1/dt, window=mlab.window_hanning,\n                        detrend=mlab.detrend_none, noverlap=0, pad_to=None,\n                        scale_by_freq=None,\n                        sides='onesided')\n        Su_1side = np.append([Su[0]], Su[1:4] + Su[4:][::-1])\n        assert_allclose(P, Su_1side, atol=1e-06)\n", "    def test_specgram_window_input(self):\n        spec, freqs, t = mlab.specgram(x=self.y,\n                NFFT=self.NFFT_specgram,\n                Fs=self.Fs,\n                noverlap=self.nover_specgram,\n                pad_to=self.pad_to_specgram,\n                sides=self.sides,\n                window=np.ones(self.NFFT_specgram))\n        assert spec.shape[0] == freqs.shape[0]\n        assert spec.shape[1] == self.t_specgram.shape[0]\n"], "sample_660": ["def test_set_suite_name_with_empty_string(testdir):\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        junit_suite_name=\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n            pass\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    assert result.ret == 0\n    node = dom.find_first_by_tag(\"testsuite\")\n    node.assert_attr(name=\"pytest\")\n\n", "    def test_escaped_skipreason_issue3533_backslash(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n            @pytest.mark.skip(reason='1 \\\\<> 2')\n                pass\n        \"\"\"\n        )\n        _, dom = runandparse(testdir)\n        node = dom.find_first_by_tag(\"testcase\")\n        snode = node.find_first_by_tag(\"skipped\")\n        assert \"1 \\\\<> 2\" in snode.text\n        snode.assert_attr(message=\"1 \\\\<> 2\")\n", "    def test_escaped_skipreason_issue3533_with_newlines(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n            @pytest.mark.skip(reason='1 \\\\n<>\\\\n 2')\n                pass\n        \"\"\"\n        )\n        _, dom = runandparse(testdir)\n        node = dom.find_first_by_tag(\"testcase\")\n        snode = node.find_first_by_tag(\"skipped\")\n        assert \"1 \\n<>\\n 2\" in snode.text\n        snode.assert_attr(message=\"1 \\\\n<>\\\\n 2\")\n", "    def test_escaped_skipreason_issue3533_with_special_chars(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n            @pytest.mark.skip(reason='1 <> 2 & \\\" \\' < >')\n                pass\n        \"\"\"\n        )\n        _, dom = runandparse(testdir)\n        node = dom.find_first_by_tag(\"testcase\")\n        snode = node.find_first_by_tag(\"skipped\")\n        assert \"1 <> 2 & \\\" \\' < >\" in snode.text\n        snode.assert_attr(message=\"1 <> 2 & \\\" \\' < >\")\n", "    def test_escaped_skipreason_issue3533_double_quotes(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n            @pytest.mark.skip(reason='\"1 <> 2\"')\n                pass\n        \"\"\"\n        )\n        _, dom = runandparse(testdir)\n        node = dom.find_first_by_tag(\"testcase\")\n        snode = node.find_first_by_tag(\"skipped\")\n        assert '\"1 <> 2\"' in snode.text\n        snode.assert_attr(message='\"1 <> 2\"')\n"], "sample_1132": ["compilation error", "    def test_multiset_combinations_with_replacement():\n        ans = ['iii', 'iim', 'iip', 'iis', 'imp', 'ims', 'ipp', 'ips',\n               'iss', 'mpp', 'mps', 'mss', 'pps', 'pss', 'sss']\n        assert [''.join(i) for i in\n                list(multiset_combinations('mississippi', 3, with_replacement=True))] == ans\n        M = multiset('mississippi')\n        assert [''.join(i) for i in\n                list(multiset_combinations(M, 3, with_replacement=True))] == ans\n        assert list(multiset_combinations([0, 0, 1], 2, with_replacement=True)) == \\\n            [[0, 0], [0, 1], [0, 1], [1, 1]]\n\n", "compilation error", "    def test_necklaces_with_length():\n        assert list(necklaces_with_length(4, 2, 2)) == [\n            '0011', '0101', '0110', '1001', '1010', '1100'\n        ]\n", "compilation error"], "sample_24": ["    def test_array_equal_with_masked(self):\n        ma = Masked(np.array([1, 2, MaskedValue.MASKED]), mask=[True, False, False])\n        mb = Masked(np.array([MaskedValue.MASKED, 2, 3]), mask=[True, False, False])\n        assert not np.array_equal(ma, mb)\n", "    def test_nan_to_num(self):\n        self.check(np.nan_to_num)\n", "    def test_isfinite(self):\n        self.check(np.isfinite)\n", "    def test_nan_to_num(self):\n        self.check(np.nan_to_num, masked_result=False)\n", "    def test_nan_to_num(self):\n        self.check(np.nan_to_num, exact_fill_value=0)\n"], "sample_271": ["    def test_watch_dir_with_duplicate_files(self):\n        self.ensure_file(self.tempdir / 'file.py')\n        self.ensure_file(self.tempdir / 'dir' / 'file.py')\n        self.reloader.watch_dir(self.tempdir, '*.py')\n        watched_files = set(self.reloader.watched_files())\n        self.assertEqual(len(watched_files), 2)\n        self.assertIn(self.tempdir / 'file.py', watched_files)\n        self.assertIn(self.tempdir / 'dir' / 'file.py', watched_files)\n", "    def test_update_watches_no_changes(self):\n        with mock.patch.object(self.reloader, '_update_watches') as mocked_watches:\n            with mock.patch.object(self.reloader, 'check_server_status') as mocked_server_status:\n                mocked_server_status.return_value = True\n                self.reloader.update_watches()\n                self.assertFalse(mocked_watches.called)\n", "    def test_watch_dir_with_multiple_globs(self):\n        self.reloader.watch_dir(self.tempdir, ['*.py', '*.txt'])\n        watched_files = list(self.reloader.watched_files())\n        self.assertIn(self.existing_file, watched_files)\n", "    def test_should_stop(self, mock_sleep):\n        self.reloader.stop()\n        self.assertFalse(self.reloader.should_stop())\n        mock_sleep.side_effect = KeyboardInterrupt\n        self.assertTrue(self.reloader.should_stop())\n", "    def test_should_stop_returns_false_if_files_changed(self):\n        with mock.patch.object(self.reloader, 'snapshot_files') as mocked_snapshot:\n            mocked_snapshot.return_value = {self.existing_file: 'old'}\n            self.increment_mtime(self.existing_file)\n            self.assertFalse(self.reloader.should_stop())\n"], "sample_910": ["def test_prefixed_warnings(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    with prefixed_warnings(\"prefix:\"):\n        logger.warning('message')\n    assert 'prefix: WARNING: message' in warning.getvalue()\n\n", "    def test_prefixed_warnings(app, status, warning):\n        logging.setup(app, status, warning)\n        logger = logging.getLogger(__name__)\n\n        with logging.prefixed_warnings(\"prefix:\"):\n            logger.warning('message1')\n            logger.warning('message2')\n\n        assert 'prefix: WARNING: message1' in warning.getvalue()\n        assert 'prefix: WARNING: message2' in warning.getvalue()\n", "    def test_prefixed_warnings(app, status, warning):\n        logging.setup(app, status, warning)\n        logger = logging.getLogger(__name__)\n\n        with logging.prefixed_warnings(\"prefix:\"):\n            logger.warning('message1')\n\n        assert 'prefix: WARNING: message1' in warning.getvalue()\n", "    def test_prefixed_warnings(app, status, warning):\n        logging.setup(app, status, warning)\n        logger = logging.getLogger(__name__)\n\n        logger.warning('message1')\n        with logging.prefixed_warnings(\"prefix:\"):\n            logger.warning('message2')\n        logger.warning('message3')\n\n        assert 'WARNING: message1' in warning.getvalue()\n        assert 'WARNING: prefix: message2' in warning.getvalue()\n        assert 'WARNING: message3' in warning.getvalue()\n", "    def test_prefixed_warnings(app, status, warning):\n        logging.setup(app, status, warning)\n        logger = logging.getLogger(__name__)\n\n        logger.warning('message1')\n        with prefixed_warnings(\"prefix:\"):\n            logger.warning('message2')\n        logger.warning('message3')\n\n        assert 'WARNING: message1' in warning.getvalue()\n        assert 'WARNING: prefix: message2' in warning.getvalue()\n        assert 'WARNING: message3' in warning.getvalue()\n"], "sample_809": ["def test_mutual_info_with_sparse_data():\n    rng = check_random_state(0)\n    n_samples = 100\n    X = csr_matrix(rng.rand(n_samples, 5) > 0.5)\n    y = rng.randint(0, 2, size=n_samples)\n    mi = mutual_info_classif(X, y, discrete_features='auto')\n    assert_equal(mi.shape[0], 5)\n\n", "def test_mutual_info_sparse():\n    # Test mutual information calculation with sparse input data.\n    rng = check_random_state(0)\n\n    n_samples = 100\n    n_features = 5\n\n    # Create a sparse matrix with mostly zeros.\n    X = csr_matrix(rng.rand(n_samples, n_features) > 0.8)\n\n    # Generate a random target vector.\n    y = rng.randint(0, 2, size=n_samples)\n\n    mi_sparse = mutual_info_classif(X, y, discrete_features=True)\n\n    # Convert the sparse matrix to a dense matrix.\n    X_dense = X.toarray()\n\n    # Compute mutual information with dense input data.\n    mi_dense = mutual_info_classif(X_dense, y, discrete_features=True)\n\n    # The results should be approximately equal.\n    assert_array_equal(mi_sparse, mi_dense)\n", "def test_mutual_info_sparse():\n    # Test mutual information estimation with sparse matrix input.\n\n    rng = check_random_state(0)\n    n_samples = 1000\n    X = csr_matrix(rng.rand(n_samples, 3) > 0.5)\n    y = rng.rand(n_samples) > 0.5\n\n    mi_dense = mutual_info_classif(X.toarray(), y, random_state=0)\n    mi_sparse = mutual_info_classif(X, y, random_state=0)\n\n    assert_array_equal(mi_dense, mi_sparse)\n", "def test_mutual_info_empty_features():\n    # Test mutual_info_classif and mutual_info_regression handle empty feature array.\n    X_empty = np.empty((10, 0))\n    y = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])\n    assert_raises(ValueError, mutual_info_classif, X_empty, y)\n    assert_raises(ValueError, mutual_info_regression, X_empty, y)\n\n", "compilation error"], "sample_1015": ["def test_ccode_Assignment_with_op():\n    x, y, z = symbols('x y z')\n    assert ccode(Assignment(x, y + z)) == 'x = y + z;'\n    assert ccode(aug_assign(x, '+', y + z)) == 'x += y + z;'\n    assert ccode(aug_assign(x, '-', y + z)) == 'x -= y + z;'\n    assert ccode(aug_assign(x, '*', y + z)) == 'x *= y + z;'\n    assert ccode(aug_assign(x, '/', y + z)) == 'x /= y + z;'\n    assert ccode(aug_assign(x, '%', y + z)) == 'x %= y + z;'\n    assert ccode(aug_assign(x, '**', y + z)) == 'x = pow(x, y + z);'\n\n", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_175": ["    def test_fast_delete_related_object(self):\n        user = User.objects.create()\n        avatar = Avatar.objects.create(user=user)\n        self.assertNumQueries(2, avatar.delete)\n        self.assertFalse(Avatar.objects.exists())\n        self.assertFalse(User.objects.exists())\n\n", "    def test_fast_delete_m2m_through_table(self):\n        m = M.objects.create()\n        r = R.objects.create()\n        MR.objects.create(m=m, r=r)\n        self.assertNumQueries(2, m.delete)\n\n\n\n", "    def test_fast_delete_with_custom_primary_key(self):\n        obj = CustomPK.objects.create(my_id=1)\n        self.assertNumQueries(1, obj.delete)\n        self.assertFalse(CustomPK.objects.exists())\n", "    def test_fast_delete_multiple_inheritance(self):\n        mp = MultiParent.objects.create()\n        c = ChildMultiParent.objects.create(multiparent=mp)\n        # 1 for self, 1 for multiparent, 1 for fast delete of parent\n        self.assertNumQueries(3, c.delete)\n        self.assertFalse(ChildMultiParent.objects.exists())\n        self.assertFalse(MultiParent.objects.exists())\n", "    def test_fast_delete_with_post_delete_signal(self):\n        class MyModel(models.Model):\n            pass\n        called = False\n\n            nonlocal called\n            called = True\n\n        models.signals.post_delete.connect(\n            post_delete_handler, sender=MyModel, weak=False\n        )\n\n        obj = MyModel.objects.create()\n        obj.delete()\n        self.assertTrue(called)\n"], "sample_402": ["    def test_dont_compress_if_content_encoding_already_set(self):\n        \"\"\"\n        If Content-Encoding is already set, don't compress.\n        \"\"\"\n        self.resp[\"Content-Encoding\"] = \"identity\"\n        r = GZipMiddleware(self.get_response)(self.req)\n        self.assertEqual(r.content, self.compressible_string)\n        self.assertEqual(r.get(\"Content-Encoding\"), \"identity\")\n", "    def test_etag_none(self):\n        \"\"\"\n        GZipMiddleware handles ETag None correctly.\n        \"\"\"\n\n            response = HttpResponse(self.compressible_string)\n            response.headers[\"ETag\"] = None\n            return response\n\n        request = self.rf.get(\"/\", HTTP_ACCEPT_ENCODING=\"gzip, deflate\")\n        gzip_response = GZipMiddleware(get_response)(request)\n        self.assertIsNone(gzip_response.headers.get(\"ETag\"))\n", "    def test_no_compression_on_conditional_request(self):\n        \"\"\"\n        Compression is not applied if the request is conditional and the\n        response is not modified.\n        \"\"\"\n\n            response = HttpResponse(self.compressible_string)\n            response.headers[\"ETag\"] = '\"eggs\"'\n            return response\n\n        request = self.rf.get(\n            \"/\", HTTP_ACCEPT_ENCODING=\"gzip, deflate\", HTTP_IF_NONE_MATCH='\"eggs\"'\n        )\n        gzip_response = GZipMiddleware(get_response)(request)\n        self.assertEqual(gzip_response.status_code, 304)\n        self.assertIsNone(gzip_response.get(\"Content-Encoding\"))\n\n", "    def test_no_compress_for_staticfiles(self):\n        \"\"\"\n        Compression is not performed on responses served by the StaticFilesMiddleware\n        \"\"\"\n        with mock.patch('django.contrib.staticfiles.views.serve') as serve_mock:\n            serve_mock.return_value.content = self.compressible_string\n            serve_mock.return_value.status_code = 200\n            serve_mock.return_value[\"Content-Type\"] = \"text/css\"\n\n            r = GZipMiddleware(\n                lambda req: StaticFilesMiddleware(lambda req: HttpResponse(\n                    self.compressible_string, content_type=\"text/css\"))(req)\n            )(self.req)\n\n            self.assertEqual(r.content, self.compressible_string)\n            self.assertIsNone(r.get(\"Content-Encoding\"))\n", "    def test_no_etag_no_gzip(self):\n            response = HttpResponse(self.compressible_string)\n            return response\n\n        request = self.rf.get(\"/\", HTTP_ACCEPT_ENCODING=\"gzip, deflate\")\n        gzip_response = GZipMiddleware(get_response)(request)\n        self.assertNotIn(\"ETag\", gzip_response)\n"], "sample_108": ["    def test_reverse_value_error_fails(self):\n        @DynamicConverter.register_to_url\n            raise ValueError()\n\n        with self.assertRaises(ValueError):\n            reverse('dynamic', kwargs={'value': 'abc'})\n", "    def test_dynamic_converter_reverse_error_not_caught(self):\n        @DynamicConverter.register_to_url\n            raise TypeError('This type error propagates.')\n        with self.assertRaises(TypeError):\n            reverse('dynamic', kwargs={'value': 'abc'})\n", "    def test_reverse_value_error_propagates(self):\n        @DynamicConverter.register_to_url\n            raise ValueError()\n\n        with self.assertRaises(ValueError):\n            reverse('dynamic', kwargs={'value': 'abc'})\n", "    def test_reverse_type_error_propagates(self):\n        @DynamicConverter.register_to_url\n            raise TypeError('This type error propagates.')\n        with self.assertRaisesMessage(TypeError, 'This type error propagates.'):\n            reverse('dynamic', kwargs={'value': 1})\n", "    def test_reverse_value_error_means_no_reverse(self):\n        @DynamicConverter.register_to_url\n            raise ValueError()\n        with self.assertRaises(NoReverseMatch):\n            reverse('dynamic', kwargs={'value': 'abc'})\n"], "sample_1135": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1186": ["def test_array_slicing():\n    for ArrayType in array_types:\n        a = ArrayType([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n        assert a[0, :] == Array([1, 2, 3])\n        assert a[1, :] == Array([4, 5, 6])\n        assert a[:, 0] == Array([1, 4, 7])\n        assert a[:, 1] == Array([2, 5, 8])\n        assert a[:, 2] == Array([3, 6, 9])\n        assert a[0:2, :] == Array([[1, 2, 3], [4, 5, 6]])\n        assert a[:, 0:2] == Array([[1, 2], [4, 5], [7, 8]])\n        assert a[0:2, 0:2] == Array([[1, 2], [4, 5]])\n", "    def test_array_scalar_creation():\n        for array_type in array_types:\n            A = array_type(5)\n            assert isinstance(A, array_type)\n            assert A.shape == ()\n            assert A[()] == 5\n\n", "compilation error", "def test_array_eq():\n    a = ImmutableDenseNDimArray([1, 2, 3, 4], (2, 2))\n    b = ImmutableDenseNDimArray([1, 2, 3, 4], (2, 2))\n    assert (a == b) is True\n    c = ImmutableDenseNDimArray([1, 2, 3, 5], (2, 2))\n    assert (a == c) is False\n    d = ImmutableDenseNDimArray([1, 2], (2,))\n    assert (a == d) is False\n\n\n\n", "def test_array_shape():\n    for ArrayType in array_types:\n        assert ArrayType([]).shape == (0,)\n        assert ArrayType([1]).shape == (1,)\n        assert ArrayType([1, 2]).shape == (2,)\n        assert ArrayType([[1, 2], [3, 4]]).shape == (2, 2)\n        assert ArrayType([[1, 2, 3], [4, 5, 6], [7, 8, 9]]).shape == (3, 3)\n        assert ArrayType([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]).shape == (2, 2, 2)\n"], "sample_1189": ["    def test_lambdify_cse_issue_24428():\n        if not numpy:\n            skip(\"numpy not installed\")\n        x, y, z = symbols('x y z')\n        exprs = [x*y+z, x*y, z**2]\n        case = Case(args=(x, y, z), exprs=exprs, num_args=(1, 2, 3), requires_numpy=True)\n        # Check that cse optimization does not change results\n\n        f = case.lambdify(cse=minmem)\n        result = f(1, 2, 3)\n        case.assertAllClose(result)\n", "    def test_lambdify_cse_with_numpy():\n        case = Case(\n            args=(x, y),\n            exprs=[x*y + x, y*x + y],\n            num_args=(1.0, 2.0),\n            requires_numpy=True,\n        )\n        f_no_cse = case.lambdify(cse=dummy_cse)\n        f_cse = case.lambdify(cse=minmem)\n        assert case.assertAllClose(f_no_cse(\n            *case.num_args))\n        assert case.assertAllClose(f_cse(\n            *case.num_args))\n", "def test_lambdify_MatrixSymbol_multi():\n    if not numpy:\n        skip(\"numpy not installed\")\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    f = lambdify((A, B), A*B)\n    A_val = numpy.array([[1, 2], [3, 4]])\n    B_val = numpy.array([[5, 6], [7, 8]])\n    assert numpy.allclose(f(A_val, B_val), numpy.dot(A_val, B_val))\n", "    def test_lambdify_cse_complex_args():\n        cse = Case(\n            args=(x, y),\n            exprs=[sin(x*y), cos(x*y)],\n            num_args=(1.2 + 0.3j, 2.1 - 0.5j),\n            requires_numpy=True\n        )\n        f = cse.lambdify(cse=minmem)\n        result = f(*cse.num_args)\n        cse.assertAllClose(result)\n", "compilation error"], "sample_309": ["    def test_parse_http_date_invalid_date(self):\n        with self.assertRaises(ValueError):\n            parse_http_date('InvalidDate')\n", "    def test_parse_http_date_invalid(self):\n        with self.assertRaises(ValueError):\n            parse_http_date('Invalid date string')\n", "    def test_parsing_invalid_date(self):\n        with self.assertRaises(ValueError):\n            parse_http_date('invalid date string')\n", "    def test_escape_leading_slashes(self):\n        self.assertEqual(escape_leading_slashes('/path'), '/%2Fpath')\n        self.assertEqual(escape_leading_slashes('//example.com/path'), '/%2Fexample.com/path')\n        self.assertEqual(escape_leading_slashes('http://example.com/path'), 'http://example.com/path')\n", "    def test_parsing_invalid_date(self):\n        with self.assertRaises(ValueError):\n            parse_http_date('Invalid date string')\n"], "sample_1045": ["compilation error", "compilation error", "def test_Float_is_rational():\n    assert Float('1.0').is_rational is True\n    assert Float('1.5').is_rational is True\n    assert Float('1.0000000000000001').is_rational is False\n    assert Float('1.0000000000000002').is_rational is False\n    assert Float('-1.5').is_rational is True\n    assert Float('3.14').is_rational is False\n    assert Float('-3.14').is_rational is False\n\n", "compilation error", "compilation error"], "sample_629": ["def test_expand_modules_basic():\n    files_or_modules = [\"test_module.py\", \"package\"]\n    ignore_list = []\n    ignore_list_re = []\n    ignore_list_paths_re = []\n    results, errors = expand_modules(\n        files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re\n    )\n    assert len(results) == 2\n    assert len(errors) == 0\n", "def test_expand_modules_basic():\n    files = [\"test_module.py\", \"another_module.py\"]\n    ignore_list = []\n    ignore_list_re = []\n    ignore_list_paths_re = []\n    result, _ = expand_modules(files, ignore_list, ignore_list_re, ignore_list_paths_re)\n    assert len(result) == 2\n    assert result[0][\"name\"] == \"test_module\"\n    assert result[1][\"name\"] == \"another_module\"\n", "    def test_expand_modules_basic(self) -> None:\n        result, errors = expand_modules(\n            files_or_modules=[\"foo\"],\n            ignore_list=[],\n            ignore_list_re=[re.compile(\".*tests.*\")],\n            ignore_list_paths_re=[re.compile(\".*tests/.*\")],\n        )\n        assert result == [\n            {\"path\": \"foo\", \"name\": \"foo\", \"isarg\": True, \"basepath\": \"foo\", \"basename\": \"foo\"}\n        ]\n        assert not errors\n", "def test_expand_modules_basic():\n    files_or_modules = [\"my_module.py\", \"another_module\"]\n    ignore_list = []\n    ignore_list_re = []\n    ignore_list_paths_re = []\n    expected = [\n        {\n            \"path\": \"my_module.py\",\n            \"name\": \"my_module\",\n            \"isarg\": True,\n            \"basepath\": \"my_module.py\",\n            \"basename\": \"my_module\",\n        },\n        {\n            \"path\": \"another_module.py\",\n            \"name\": \"another_module\",\n            \"isarg\": True,\n            \"basepath\": \"another_module.py\",\n            \"basename\": \"another_module\",\n        },\n    ]\n    result, errors = expand_modules(\n        files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re\n    )\n    assert result == expected\n    assert not errors\n", "def test_expand_modules_basic():\n    files_or_modules = [\"test.py\"]\n    ignore_list = []\n    ignore_list_re = []\n    ignore_list_paths_re = []\n    result, errors = expand_modules(\n        files_or_modules,\n        ignore_list,\n        ignore_list_re,\n        ignore_list_paths_re,\n    )\n    assert len(result) == 1\n\n"], "sample_677": ["def test_ident_spacing(expr: str, expected: bool) -> None:\n    assert evaluate(expr, {\"ident\": True}.__getitem__)\n", "def test_dollar_prefix(expr: str, matcher: dict[str, bool], expected: bool) -> None:\n    assert evaluate(expr, lambda ident: matcher[ident]) is expected\n", "def test_ident_used_as_identifier(expr: str, ident: str) -> None:\n    assert evaluate(expr, {ident: True}.__getitem__)\n", "def test_valid_python_identifiers(expr: str) -> None:\n    assert evaluate(expr, {expr: True}.__getitem__) \n", "def test_unbalanced_parentheses(expr: str, message: str) -> None:\n    with pytest.raises(ParseError) as excinfo:\n        evaluate(expr, lambda ident: True)\n    assert excinfo.value.message == message\n"], "sample_323": ["    def test_minimize_rollbacks_complex_dependencies(self):\n        r\"\"\"\n        Minimize rollbacks when there are cross-app dependencies.\n\n        a: 1 <---- 3 <--\\\n              \\ \\- 2 <--- 4\n               \\       \\\n        b:      \\- 1 <--- 2\n        c:        \\\n                 \\- 1 <-- 2 <-- a3\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        a3_impl = FakeMigration('a3')\n        a3 = ('a', '3')\n        a4_impl = FakeMigration('a4')\n        a4 = ('a', '4')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        b2_impl = FakeMigration('b2')\n        b2 = ('b', '2')\n        c1_impl = FakeMigration('c1')\n        c1 = ('c', '1')\n        c2_impl = FakeMigration('c2')\n        c2 = ('c', '2')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(a3, a3_impl)\n        graph.add_node(a4, a4_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_node(b2, b2_impl)\n        graph.add_node(c1, c1_impl)\n        graph.add_node(c2, c2_impl)\n        graph.add_dependency(None, a2, a1)\n        graph.add_dependency(None, a3, a1)\n        graph.add_dependency(None, a4, a2)\n        graph.add_dependency(None, a4, a3)\n        graph.add_dependency(None, b2, b1)\n        graph.add_dependency(None,", "    def test_minimize_rollbacks_circular(self):\n        \"\"\"\n        Minimize rollbacks when there are circular dependencies.\n\n        a: 1 <--> b: 1\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_dependency(None, b1, a1)\n        graph.add_dependency(None, a1, b1)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            b1: b1_impl,\n        })\n\n        plan = executor.migration_plan({a1})\n\n        self.assertEqual(plan, [])\n", "    def test_minimize_rollbacks_complex(self):\n        r\"\"\"\n        Minimize rollbacks with branches and complex dependencies.\n\n        a: 1 <--- 3 <--\\\n              \\ \\- 2 <--- 4\n               \\       \\\n        b:      \\- 1 <--- 2\n        c: 1 <--- 2 <--- a-4\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        a3_impl = FakeMigration('a3')\n        a3 = ('a', '3')\n        a4_impl = FakeMigration('a4')\n        a4 = ('a', '4')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        b2_impl = FakeMigration('b2')\n        b2 = ('b', '2')\n        c1_impl = FakeMigration('c1')\n        c1 = ('c', '1')\n        c2_impl = FakeMigration('c2')\n        c2 = ('c', '2')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(a3, a3_impl)\n        graph.add_node(a4, a4_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_node(b2, b2_impl)\n        graph.add_node(c1, c1_impl)\n        graph.add_node(c2, c2_impl)\n        graph.add_dependency(None, a2, a1)\n        graph.add_dependency(None, a3, a1)\n        graph.add_dependency(None, a4, a2)\n        graph.add_dependency(None, a4, a3)\n        graph.add_dependency(None, b2, b1)\n        graph.add_dependency(None, b1, a1)", "    def test_unapplied_migrations_after_rollback(self):\n        \"\"\"\n        Test that unapplied migrations are correctly handled after a rollback.\n\n        Scenario:\n\n        1. Migrate appA to 0002.\n        2. Migrate appB to 0001 (which depends on appA 0001).\n        3. Rollback appB to 0000.\n        4. Expected: appA should remain at 0002, and appB should be at 0000\n\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_dependency(None, b1, a1)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {})\n\n        # Apply appA to 0002\n        plan = executor.migration_plan({a2})\n        self.assertEqual(plan, [(a2_impl, False)])\n        \n\n        plan = executor.migration_plan({b1})\n        self.assertEqual(plan, [(b1_impl, False)])\n\n        # Rollback appB to 0000\n        plan = executor.migration_plan({b1}, backwards=True)\n        self.assertEqual(plan, [(b1_impl, True)])\n        \n        \n        # Ensure appA remains at 0002\n        plan = executor.migration_plan({a2})\n        self.assertEqual(plan, [])\n\n\n\n", "    def test_minimize_rollbacks_complex_dependencies(self):\n        \"\"\"\n        Minimize rollbacks in complex dependency chains.\n\n        a: 1 <-- 3 <-- 5\n             \\ - 2\n              \\- 4\n        b: 1 <-- 2\n        c: 1 <-- 2 <-- 3 <-- 4\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        a3_impl = FakeMigration('a3')\n        a3 = ('a', '3')\n        a4_impl = FakeMigration('a4')\n        a4 = ('a', '4')\n        a5_impl = FakeMigration('a5')\n        a5 = ('a', '5')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        b2_impl = FakeMigration('b2')\n        b2 = ('b', '2')\n        c1_impl = FakeMigration('c1')\n        c1 = ('c', '1')\n        c2_impl = FakeMigration('c2')\n        c2 = ('c', '2')\n        c3_impl = FakeMigration('c3')\n        c3 = ('c', '3')\n        c4_impl = FakeMigration('c4')\n        c4 = ('c', '4')\n\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(a3, a3_impl)\n        graph.add_node(a4, a4_impl)\n        graph.add_node(a5, a5_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_node(b2, b2_impl)\n        graph.add_node(c1, c1_impl)\n        graph.add_node(c2, c2_impl)\n        graph.add_node(c3, c3_impl)"], "sample_597": ["    def test_merge_update_method(self):\n        data = create_test_data()\n\n        actual = data.copy()\n        actual.update(data[[\"var2\"]])\n        expected = data[[\"var1\",\"var2\"]]\n        assert_identical(actual, expected)\n\n        actual = data.copy()\n        actual.update(data[[\"var2\"]], overwrite_vars=\"var1\")\n        expected = data.copy()\n        expected[\"var1\"] = data[\"var2\"]\n        assert_identical(actual, expected)\n\n        actual = data.copy()\n        actual.update({\"var4\": ([\"y\"], [1, 2, 3]), \"x\": 0})\n        expected = data.copy()\n        expected[\"var4\"] = ([\"y\"], [1, 2, 3])\n        expected[\"x\"] = 0\n        assert_identical(actual, expected)\n\n", "    def test_merge_overwrite_vars(self):\n        ds1 = xr.Dataset({\"a\": [1, 2], \"b\": [3, 4]})\n        ds2 = xr.Dataset({\"a\": [5, 6], \"c\": [7, 8]})\n\n        expected = xr.Dataset({\"a\": [5, 6], \"b\": [3, 4], \"c\": [7, 8]})\n        actual = ds1.merge(ds2, overwrite_vars=\"a\")\n        assert expected.identical(actual)\n\n        expected = xr.Dataset({\"a\": [5, 6], \"b\": [3, 4], \"c\": [7, 8]})\n        actual = ds1.merge(ds2, overwrite_vars=[\"a\"])\n        assert expected.identical(actual)\n\n", "    def test_merge_overwrite_vars(self):\n        ds1 = xr.Dataset({\"a\": [1, 2], \"b\": [3, 4]})\n        ds2 = xr.Dataset({\"a\": [5, 6], \"c\": [7, 8]})\n\n        expected = xr.Dataset({\"a\": [5, 6], \"b\": [3, 4], \"c\": [7, 8]})\n        actual = ds1.merge(ds2, overwrite_vars=[\"a\"])\n        assert expected.identical(actual)\n\n        expected = xr.Dataset({\"a\": [1, 2], \"b\": [3, 4], \"c\": [7, 8]})\n        actual = ds1.merge(ds2, overwrite_vars=[\"c\"])\n        assert expected.identical(actual)\n\n", "    def test_merge_overwrite_vars(self):\n        data = create_test_data()\n        ds1 = data.copy()\n        ds2 = data.copy()\n        ds2[\"var1\"] = ds2[\"var1\"] + 1\n        ds2[\"var4\"] = ((\"x\", \"y\"), [[1, 2], [3, 4]])\n\n        actual = ds1.merge(ds2, overwrite_vars=[\"var1\"])\n        expected = data.copy()\n        expected[\"var1\"] = ds2[\"var1\"]\n        assert_identical(expected, actual)\n\n        actual = ds1.merge(ds2, overwrite_vars=[\"var1\", \"var4\"])\n        expected[\"var4\"] = ds2[\"var4\"]\n        assert_identical(expected, actual)\n", "    def test_merge_overwrite_vars(self):\n        ds1 = xr.Dataset({\"a\": 0, \"b\": 1})\n        ds2 = xr.Dataset({\"a\": 2, \"c\": 3})\n        actual = ds1.merge(ds2, overwrite_vars=[\"a\"])\n        expected = xr.Dataset({\"a\": 2, \"b\": 1, \"c\": 3})\n        assert_identical(actual, expected)\n\n"], "sample_553": ["def test_animation_blit(capsys, anim):\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], [4, 5, 6])\n    anim = animation.FuncAnimation(fig, lambda i: None, blit=True, frames=1)\n    with capsys.disabled():\n        anim._draw_frame(None)\n\n    captured = capsys.readouterr()\n    assert 'draw' in captured.out\n", "def test_init_func_raises():\n        raise ValueError(\"Init function error!\")\n\n    with pytest.raises(RuntimeError):\n        animation.FuncAnimation(\n            Fig, lambda i: [], init_func=init_func\n        )\n", "def test_animation_save_no_frames():\n    fig, ax = plt.subplots()\n    line, = ax.plot([])\n\n        return []\n\n    anim = animation.FuncAnimation(fig, animate, frames=5)\n    with pytest.warns(UserWarning, match=\"exhausted\"):\n        anim.save(\"test.gif\", writer='pillow')\n", "def test_animation_repeat_delay(anim):\n    anim = animation.FuncAnimation(**anim)\n\n    # Check if repeat_delay is set correctly\n    assert anim.repeat_delay == 0\n\n    # Set a new repeat delay\n    new_delay = 100\n    anim.repeat_delay = new_delay\n\n    assert anim.repeat_delay == new_delay\n", "def test_blitting_with_artists_removed(tmpdir):\n    fig, ax = plt.subplots()\n    line, = ax.plot([0, 1], [0, 1])\n    text = ax.text(0.5, 0.5, 'Text')\n    \n        return line, text\n\n        if i == 2:\n            text.remove()\n        return line,\n\n    anim = animation.FuncAnimation(fig, animate, init_func=init,\n                                  frames=5, blit=True)\n\n    with tmpdir.as_cwd():\n        anim.save(\"test.gif\", writer='pillow')  \n"], "sample_415": ["    def test_expressions_with_include(self):\n        msg = (\n            \"UniqueConstraint.include cannot be used with expressions. Use \"\n            \"django.contrib.postgres.indexes.Include() instead.\"\n        )\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                Lower(\"field\"),\n                name=\"test_func_include\",\n                include=[\"other_field\"],\n            )\n\n", "    def test_unique_constraint_on_expression_with_condition(self):\n        constraint = models.UniqueConstraint(Lower(\"name\"), condition=models.Q(color__isnull=True), name=\"name_lower_without_color_uniq\")\n        UniqueConstraintProduct.objects.create(name=\"p1\", color=None)\n        with self.assertRaises(IntegrityError):\n            UniqueConstraintProduct.objects.create(name=\"P1\", color=None)\n", "    def test_invalid_include_type(self):\n        msg = \"UniqueConstraint.include must contain only strings.\"\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                name=\"uniq_include\",\n                fields=[\"field\"],\n                include=[1],\n            )\n", "    def test_unique_constraint_with_empty_string(self):\n        UniqueConstraintProduct.objects.create(name=\"p1\", color=\"red\")\n        with self.assertRaises(IntegrityError):\n            UniqueConstraintProduct.objects.create(name=\"\", color=\"red\")\n", "    def test_validate_condition_with_exclude(self):\n        p1 = UniqueConstraintConditionProduct.objects.create(name=\"p1\")\n        constraint = UniqueConstraintConditionProduct._meta.constraints[0]\n        msg = \"Constraint \u201cname_without_color_uniq\u201d is violated.\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            constraint.validate(\n                UniqueConstraintConditionProduct,\n                UniqueConstraintConditionProduct(name=p1.name, color=None),\n                exclude={\"name\"},\n            )\n"], "sample_160": ["    def test_grouping_with_non_uniform_grouping(self):\n        self.assertEqual(nformat(1234567890, '.', grouping=(3, 2, 0), thousand_sep=' '), '123 45 67890')\n        self.assertEqual(nformat(1234567890, '.', grouping=(2, 2, 3), thousand_sep='.'), '12.34.567.890')\n        self.assertEqual(nformat(-1234567890, '.', grouping=(3, 2, 0), thousand_sep=' '), '-123 45 67890')\n\n", "    def test_zero_decimal_pos(self):\n        self.assertEqual(nformat(1234.5678, '.', decimal_pos=0), '1235')\n        self.assertEqual(nformat(Decimal('1234.5678'), '.', decimal_pos=0), '1235')\n        self.assertEqual(nformat(-1234.5678, '.', decimal_pos=0), '-1235')\n        self.assertEqual(nformat(Decimal('-1234.5678'), '.', decimal_pos=0), '-1235')\n", "    def test_format_zero(self):\n        self.assertEqual(nformat(0, '.'), '0')\n        self.assertEqual(nformat(0, '.', decimal_pos=2), '0.00')\n        self.assertEqual(nformat(0.0, '.'), '0.0')\n        self.assertEqual(nformat(0.0, '.', decimal_pos=2), '0.00')\n        self.assertEqual(nformat(Decimal('0'), '.'), '0')\n        self.assertEqual(nformat(Decimal('0'), '.', decimal_pos=2), '0.00')\n        self.assertEqual(nformat(Decimal('0.0'), '.'), '0.0')\n        self.assertEqual(nformat(Decimal('0.0'), '.', decimal_pos=2), '0.00')\n", "    def test_non_uniform_grouping(self):\n        self.assertEqual(nformat(1234567890, '.', grouping=[3, 2, 0]), '1,234,567,890')\n        self.assertEqual(nformat(1234567890.123, '.', decimal_pos=3, grouping=[3, 2, 0]), '1,234,567,890.123')\n        self.assertEqual(nformat(12345678901234567890, '.', grouping=[3, 2, 1, 0]), '123,456,789,012,345,678,90')\n        self.assertEqual(\n            nformat(Decimal('12345678901234567890'), '.', grouping=[3, 2, 1, 0]),\n            '123,456,789,012,345,678,90'\n        )\n", "    def test_grouping_non_uniform(self):\n        self.assertEqual(nformat(123456789.123, '.', grouping=[3, 2, 0], thousand_sep=','), '123,456,789.123')\n        self.assertEqual(nformat(123456789.123, '.', grouping=[3, 2, 0], thousand_sep=',', force_grouping=True), '123,456,789.123')\n        self.assertEqual(nformat(-123456789.123, '.', grouping=[3, 2, 0], thousand_sep=','), '-123,456,789.123')\n"], "sample_55": ["    def test_add_view_hidden_inline(self):\n        response = self.client.get(reverse('admin:admin_views_addhiddeninline_add'))\n        self.assertContains(response, '<div class=\"form-row hidden field-nested_inline')\n", "    def test_inline_formsets_in_change_view(self):\n        post_data = {\n            'name': '3',\n            'dependentchild_set-TOTAL_FORMS': '1',\n            'dependentchild_set-INITIAL_FORMS': '0',\n            'dependentchild_set-MAX_NUM_FORMS': '1',\n            'dependentchild_set-0-id': '',\n            'dependentchild_set-0-parent': '',\n            'dependentchild_set-0-family_name': 'Test1',\n        }\n        response = self.client.post(reverse('admin:admin_views_parentwithdependentchildren_add'), post_data)\n        self.assertEqual(response.status_code, 302)\n\n        pwdc = ParentWithDependentChildren.objects.get(name='3')\n\n        post_data = {\n            'name': '3',\n            'dependentchild_set-TOTAL_FORMS': '1',\n            'dependentchild_set-INITIAL_FORMS': '0',\n            'dependentchild_set-MAX_NUM_FORMS': '1',\n            'dependentchild_set-0-id': '',\n            'dependentchild_set-0-parent': str(pwdc.id),\n            'dependentchild_set-0-family_name': 'Test2',\n        }\n\n        response = self.client.post(\n            reverse('admin:admin_views_parentwithdependentchildren_change', args=(pwdc.id,)), post_data\n        )\n        self.assertEqual(response.status_code, 302)\n\n", "    def test_formsets_with_inlines_add_view_no_instance(self):\n        response = self.client.get(reverse('admin:admin_views_parentwithdependentchildren_add'))\n        self.assertContains(response, 'Add ParentWithDependentChildren')\n", "def test_add_view_with_inlines_validation(self):\n        \"\"\"\n        Issue #20522\n        Verifying that the formsets run validations even if the parent form fails\n        validation.\n\n        \"\"\"\n        post_data = {\n            'name': 'Test Restaurant',\n            'city': '1',\n            'worker_set-TOTAL_FORMS': '1',\n            'worker_set-INITIAL_FORMS': '0',\n            'worker_set-MAX_NUM_FORMS': '1',\n            'worker_set-0-id': '',\n            'worker_set-0-work_at': '1',\n            'worker_set-0-name': 'John',\n            'worker_set-0-surname': 'Doe',\n        }\n        response = self.client.post(reverse('admin:admin_views_restaurant_add'), post_data)\n        self.assertContains(response, 'This field is required.')\n        self.assertFormsetError(\n            response, 'inline_admin_formset', 0, None,\n            ['Workers must have a unique name within their restaurant.']\n        )\n", "    def test_add_view_with_inline(self):\n        post_data = {'name': 'New Restaurant', 'dependentrestaurant_set-TOTAL_FORMS': '1',\n                     'dependentrestaurant_set-INITIAL_FORMS': '0', 'dependentrestaurant_set-MAX_NUM_FORMS': '1',\n                     'dependentrestaurant_set-0-id': '', 'dependentrestaurant_set-0-restaurant': '',\n                     'dependentrestaurant_set-0-name': 'Dependent Restaurant'}\n        response = self.client.post(reverse('admin:admin_views_restaurantwithdependentrestaurant_add'), post_data)\n        self.assertEqual(response.status_code, 302)\n"], "sample_230": ["    def test_empty_string_as_input(self):\n        field = JSONField(required=False)\n        self.assertEqual(field.clean(''), None)\n\n", "    def test_formfield_disabled_initial(self):\n        class JSONForm(Form):\n            json_field = JSONField(disabled=True)\n\n        form = JSONForm(initial={'json_field': '[\"bar\"]'})\n        self.assertIn('[&quot;bar&quot;]</textarea>', form.as_p())\n", "    def test_empty_string(self):\n        field = JSONField()\n        self.assertEqual(field.clean(''), '')\n\n", "    def test_empty_string(self):\n        field = JSONField(required=False)\n        self.assertEqual(field.clean(''), '')\n        self.assertEqual(field.to_python(''), '')\n", "    def test_empty_string(self):\n        field = JSONField(required=False)\n        self.assertIsNone(field.clean(''))\n"], "sample_539": ["def test_polygon_selector_limits(draw_bounding_box):\n    fig, ax = plt.subplots()\n    ax.set_xlim(0, 10)\n    ax.set_ylim(0, 10)\n\n    verts = [(1, 1), (9, 1), (9, 9), (1, 9)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(15, 15),\n        *polygon_place_vertex(*verts[3]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    tool = widgets.PolygonSelector(ax, onselect=noop,\n                                     draw_bounding_box=draw_bounding_box)\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n    assert tool.verts == verts\n", "def test_polygon_selector_cancel(draw_bounding_box):\n    verts = [(50, 50), (150, 50), (50, 150)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        ('on_key_press', dict(key='escape')),\n        ('on_key_release', dict(key='escape')),\n    ]\n    check_polygon_selector(event_sequence, [], 0, draw_bounding_box=draw_bounding_box)\n", "    def test_polygon_selector_box_invisible(ax):", "def test_polygon_selector_update_props(draw_bounding_box):\n    verts = [(10, 10), (20, 30), (30, 10)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    ax = get_ax()\n    tool = widgets.PolygonSelector(ax, onselect=noop,\n                draw_bounding_box=draw_bounding_box,\n                props={'color': 'r', 'linewidth': 2},\n                handle_props={'marker': 'o', 'markerfacecolor': 'b'})\n\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    tool.set_props(color='g', linewidth=4)\n    tool.set_handle_props(marker='s', markerfacecolor='y')\n\n    assert tool.artist.get_color() == 'g'\n\n    assert tool.artist.get_linewidth() == 4\n    for artist in tool._handles_artists:\n        assert artist.get_marker() == 's'\n\n\n", "def test_polygon_selector_set_verts_empty(draw_bounding_box):\n    ax = get_ax()\n    tool = widgets.PolygonSelector(ax, onselect=noop, draw_bounding_box=draw_bounding_box)\n\n    tool.verts = []\n    assert tool.verts == []\n"], "sample_394": ["    def test_final_catch_all_view_disabled_nonexistent_url_staff(self):\n        superuser = User.objects.create_superuser(\n            username=\"super\",\n            password=\"secret\",\n            email=\"super@example.com\",\n        )\n        self.client.force_login(superuser)\n        url = '/test_admin/admin10/nonexistent_url/'\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 404)\n", "    def test_missing_slash_append_slash_true_force_script_name_no_catch_all(self):\n        superuser = User.objects.create_user(\n            username=\"staff\",\n            password=\"secret\",\n            email=\"staff@example.com\",\n            is_staff=True,\n        )\n        self.client.force_login(superuser)\n        known_url = reverse(\"admin10:admin_views_article_changelist\")\n        response = self.client.get(known_url[:-1])\n        self.assertRedirects(\n            response,\n            \"/prefix\" + known_url,\n            status_code=301,\n            fetch_redirect_response=False,\n        )\n", "    def test_authenticated_staff_user_no_append_slash_no_final_catch_all_view(\n        self,", "    def test_missing_slash_append_slash_true_with_login_required_without_final_catch_all_view(self):\n        superuser = User.objects.create_user(\n            username=\"staff\",\n            password=\"secret\",\n            email=\"staff@example.com\",\n            is_staff=True,\n        )\n        self.client.force_login(superuser)\n        known_url = reverse(\"admin10:admin_views_article_changelist\")\n        response = self.client.get(known_url[:-1], HTTP_REFERER=\"/test_admin/admin10/\")\n        self.assertRedirects(\n            response, known_url, status_code=301, target_status_code=403\n        )\n\n", "    def test_missing_slash_append_slash_false_without_final_catch_all_view_non_staff(self):\n        user = User.objects.create_user(\n            username=\"user\",\n            password=\"secret\",\n            email=\"user@example.com\",\n            is_staff=False,\n        )\n        self.client.force_login(user)\n        known_url = reverse(\"admin10:admin_views_article_changelist\")\n        response = self.client.get(known_url[:-1])\n        self.assertRedirects(\n            response,\n            \"/test_admin/admin10/login/?next=/test_admin/admin10/admin_views/article\",\n        )\n"], "sample_1094": ["def test_sort_key():\n    from sympy.core.symbol import Symbol\n    x = Symbol('x')\n    assert x.sort_key() == (1, (str(x),)) \n    y = Symbol('y')\n    assert (x + y).sort_key() == (2, (0, 1), (str(x), str(y)))\n\n", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_287": ["    def test_nested_fieldsets_with_duplicates(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            fieldsets = [\n                ('Main', {'fields': ['title', ('title', 'album')]}),\n            ]\n\n        errors = MyModelAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"There are duplicate field(s) in 'fieldsets[0][1]'.\",\n                obj=MyModelAdmin,\n                id='admin.E012'\n            )\n        ]\n        self.assertEqual(errors, expected)\n\n", "    def test_duplicate_fieldsets(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            fieldsets = [\n                (None, {'fields': ['title', 'album']}),\n                (None, {'fields': ['title', 'album']}),\n            ]\n\n        errors = MyModelAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'fieldsets' contains duplicate fieldset(s).\",\n                obj=MyModelAdmin,\n                id='admin.E011'\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_check_empty_fieldsets(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            fieldsets = []\n\n        errors = MyModelAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'fieldsets' cannot be an empty list.\",\n                obj=MyModelAdmin,\n                id='admin.E007'\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_valid_fieldsets_with_duplicate_non_model_fields(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            fields = ['title', 'state']\n            fieldsets = [\n                (None, {\n                    'fields': ['title', 'state', ['title', 'state']]\n                }),\n            ]\n\n        errors = MyModelAdmin(Song, AdminSite()).check()\n        self.assertEqual(errors, [])\n\n", "    def test_duplicate_fieldsets(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            fieldsets = [\n                (None, {'fields': ['title', 'album']}),\n                (None, {'fields': ['title', 'album']}),\n            ]\n\n        errors = MyModelAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"There are duplicate fieldsets.\",\n                obj=MyModelAdmin,\n                id='admin.E016'\n            )\n        ]\n        self.assertEqual(errors, expected)\n"], "sample_970": ["def test_getdoc_wrapped_method():\n    class Foo:\n            \"\"\"\n            docstring\n                indented text\n            \"\"\"\n\n    class Bar(Foo):\n            # wrapped method\n            super().meth()\n            pass\n\n    assert inspect.getdoc(Bar.meth, getattr, False, Bar, \"meth\") is None\n    assert inspect.getdoc(Bar.meth, getattr, True, Bar, \"meth\") == Foo.meth.__doc__\n\n", "        def __init__(self):\n            self.x = 10\n", "        def my_classmethod(cls):\n            pass\n", "        def my_method(self, arg: str, kwarg: int = 1) -> None:\n            \"\"\"This method does something.\"\"\"\n            pass\n", "    def test_signature_from_str_complex_annotations_2():\n        sig = inspect.signature_from_str('() -> Dict[str, Tuple[int, ...]]')\n        assert sig.return_annotation == 'Dict[str, Tuple[int, ...]]'\n"], "sample_543": ["def test_polygon_selector_set_props_after_draw(fig_test, fig_ref, draw_bounding_box):\n    verts = [(0.1, 0.4), (0.5, 0.9), (0.3, 0.2)]\n    ax_test = fig_test.add_subplot()\n\n    tool_test = widgets.PolygonSelector(\n        ax_test, onselect=noop, draw_bounding_box=draw_bounding_box)\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n    for (etype, event_args) in event_sequence:\n        do_event(tool_test, etype, **event_args)\n    \n    tool_test.set_props(facecolor='r', alpha=0.2)\n    \n    ax_ref = fig_ref.add_subplot()\n    tool_ref = widgets.PolygonSelector(\n        ax_ref, onselect=noop, facecolor='r', alpha=0.2, draw_bounding_box=draw_bounding_box)\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n    for (etype, event_args) in event_sequence:\n        do_event(tool_ref, etype, **event_args)\n\n", "def test_polygon_selector_box_resize(ax, draw_bounding_box):\n    verts = [(50, 50), (150, 50), (50, 150)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    tool = widgets.PolygonSelector(ax, onselect=noop, draw_bounding_box=draw_bounding_box)\n\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    # Resize the polygon by dragging a corner handle\n    t = ax.transData\n    canvas = ax.figure.canvas\n\n    MouseEvent(\n        \"button_press_event\", canvas, *t.transform((150, 50)), 1)._process()\n    MouseEvent(\"motion_notify_event\", canvas, *t.transform((175, 50)))._process()\n    MouseEvent(\n        \"button_release_event\", canvas, *t.transform((175, 50)), 1)._process()\n\n    np.testing.assert_allclose(tool.verts, [(50, 50), (175, 50), (50, 150)])\n", "    def test_polygon_selector_box_resize(self, ax, draw_bounding_box):\n        # Create a square shape\n        verts = [(10, 10), (30, 10), (30, 30), (10, 30)]\n        event_sequence = [\n            *polygon_place_vertex(*verts[0]),\n            *polygon_place_vertex(*verts[1]),\n            *polygon_place_vertex(*verts[2]),\n            *polygon_place_vertex(*verts[3]),\n            *polygon_place_vertex(*verts[0]),\n        ]\n\n        # Create selector\n        tool = widgets.PolygonSelector(ax, onselect=noop, draw_bounding_box=draw_bounding_box)\n        for (etype, event_args) in event_sequence:\n            do_event(tool, etype, **event_args)\n\n        # Resize the box from the bottom right corner\n        t = ax.transData\n        canvas = ax.figure.canvas\n\n        # Press and drag the bottom right corner\n        MouseEvent(\n            \"button_press_event\", canvas, *t.transform((30, 30)), 1\n        )._process()\n        MouseEvent(\n            \"motion_notify_event\", canvas, *t.transform((40, 40))\n        )._process()\n        MouseEvent(\n            \"button_release_event\", canvas, *t.transform((40, 40)), 1\n        )._process()\n\n        # Assert the polygon vertices have been updated\n        np.testing.assert_allclose(\n            tool.verts, [(10, 10), (30, 10), (40, 40), (10, 40)]\n        )\n\n        # Assert the bounding box extents have been updated\n        np.testing.assert_allclose(tool._box.extents, (10.0, 40.0, 10.0, 40.0))\n", "def test_polygon_selector_set_props_after_draw(fig_test, fig_ref, draw_bounding_box):\n    verts = [(0.2, 0.8), (0.3, 0.7), (0.8, 0.2)]\n    ax_test = fig_test.add_subplot()\n    tool_test = widgets.PolygonSelector(\n        ax_test, onselect=noop, draw_bounding_box=draw_bounding_box)\n\n    ax_ref = fig_ref.add_subplot()\n\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n    for (etype, event_args) in event_sequence:\n        do_event(tool_test, etype, **event_args)\n        if etype == 'on_select':\n            do_event(tool_test, etype, **event_args)\n\n    tool_test.set_props(facecolor='r', alpha=0.2, edgecolor='b')\n    \n    tool_ref = widgets.PolygonSelector(\n        ax_ref, onselect=noop, draw_bounding_box=draw_bounding_box)\n    for (etype, event_args) in event_sequence:\n        do_event(tool_ref, etype, **event_args)\n    tool_ref.get_artist().set_facecolor('r')\n    tool_ref.get_artist().set_alpha(0.2)\n\n    tool_ref.get_artist().set_edgecolor('b')\n", "def test_polygon_selector_draw_bounding_box_with_artist(fig_test, fig_ref):\n    verts = [(0.1, 0.4), (0.5, 0.9), (0.3, 0.2)]\n\n    ax_test = fig_test.add_subplot()\n    tool_test = widgets.PolygonSelector(\n        ax_test, onselect=noop, draw_bounding_box=True,\n        props={'facecolor': 'None', 'edgecolor':'r', 'linewidth': 2})\n    tool_test.verts = verts\n\n    ax_ref = fig_ref.add_subplot()\n    tool_ref = widgets.PolygonSelector(\n        ax_ref, onselect=noop, draw_bounding_box=True,\n        props={'facecolor': 'None', 'edgecolor':'r', 'linewidth': 2})\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n    for (etype, event_args) in event_sequence:\n        do_event(tool_ref, etype, **event_args)\n"], "sample_161": ["    def test_foreign_object_relationship_validation(self):\n        class Group(models.Model):\n            id = models.AutoField(primary_key=True)\n            name = models.CharField(max_length=100)\n\n        class User(models.Model):\n            id = models.AutoField(primary_key=True)\n            group_id = models.PositiveIntegerField()\n            name = models.CharField(max_length=100)\n\n            class Meta:\n                unique_together = ('group_id', 'name')\n\n        class Post(models.Model):\n            user = models.ForeignObject(\n                User,\n                from_fields=('group_id', 'name'),\n                to_fields=('id', 'name'),\n            )\n\n        field = Post._meta.get_field('user')\n        self.assertEqual(field.check(from_model=Post), [\n            Error(\n                \"ForeignObject cannot be used with a related model that has a composite primary key.\",\n                hint=(\n                    \"ForeignObject can only be used with models that have a single \"\n                    \"primary key field. \"\n                    \"Consider using a ManyToManyField or a regular ForeignKey instead.\"\n                ),\n                obj=field,\n                id='fields.E340',\n            ),\n        ])\n", "    def test_foreign_object_unique_together_multi_field(self):\n        class Parent(models.Model):\n            a = models.PositiveIntegerField()\n            b = models.PositiveIntegerField()\n            c = models.PositiveIntegerField()\n\n            class Meta:\n                unique_together = (('a', 'b'), ('b', 'c'))\n\n        class Child(models.Model):\n            a = models.PositiveIntegerField()\n            b = models.PositiveIntegerField()\n            value = models.CharField(max_length=255)\n            parent = models.ForeignObject(\n                Parent,\n                on_delete=models.SET_NULL,\n                from_fields=('a', 'b'),\n                to_fields=('a', 'b'),\n                related_name='children',\n            )\n\n        field = Child._meta.get_field('parent')\n        self.assertEqual(field.check(from_model=Child), [])\n\n", "    def test_hidden_through_fields(self):\n        class Fan(models.Model):\n            pass\n\n        class Event(models.Model):\n            invitees = models.ManyToManyField(Fan, through='Invitation', through_fields=('invitee', '+'))\n\n        class Invitation(models.Model):\n            event = models.ForeignKey(Event, models.CASCADE)\n            invitee = models.ForeignKey(Fan, models.CASCADE)\n\n        field = Event._meta.get_field('invitees')\n        self.assertEqual(field.check(from_model=Event), [\n            Error(\n                \"'Invitation.invitee' is not a valid through_field.\",\n                hint=\"Did you mean to use a related name '+invitee'?\",\n                obj=field,\n                id='fields.E336',\n            ),\n        ])\n", "    def test_duplicate_through_fields(self):\n        class Fan(models.Model):\n            pass\n\n        class Event(models.Model):\n            invitees = models.ManyToManyField(Fan, through='Invitation', through_fields=('invitee', 'invitee'))\n\n        class Invitation(models.Model):\n            event = models.ForeignKey(Event, models.CASCADE)\n            invitee = models.ForeignKey(Fan, models.CASCADE)\n\n        field = Event._meta.get_field('invitees')\n        self.assertEqual(field.check(from_model=Event), [\n            Error(\n                \"The 'through_fields' argument contains duplicate field names: ('invitee', 'invitee').\",\n                hint=\"Please ensure that 'through_fields' specifies two unique field names.\",\n                obj=field,\n                id='fields.E340',\n            ),\n        ])\n", "    def test_through_fields_not_unique_error(self):\n        class Fan(models.Model):\n            name = models.CharField(max_length=100)\n\n        class Event(models.Model):\n            name = models.CharField(max_length=100)\n\n        class Invitation(models.Model):\n            event = models.ForeignKey(Event, models.CASCADE)\n            invitee = models.ForeignKey(Fan, models.CASCADE)\n            inviter = models.ForeignKey(Fan, models.CASCADE, related_name='+')\n\n        class EventFollower(models.Model):\n            event = models.ForeignKey(Event, models.CASCADE)\n            fan = models.ForeignKey(Fan, models.CASCADE)\n\n        class EventFollowing(models.Model):\n            invitation = models.ForeignKey(Invitation, models.CASCADE)\n            follower = models.OneToOneField(EventFollower, models.CASCADE, related_name='+')\n\n        Event.invitees = models.ManyToManyField(\n            Fan,\n            through='EventFollowing',\n            through_fields=('invitation__invitee', 'follower__fan'),\n        )\n\n        self.assertEqual(Event.invitees.check(from_model=Event), [\n            Error(\n                \"Fields 'invitation__invitee' and 'follower__fan' in `through_fields` do not form a unique constraint on model 'EventFollowing'.\",\n                hint=\n                \"Make sure they together uniquely identify records in the intermediary table, \"\n                \"either by adding a unique constraint or \"\n                \"modifying the fields specified in `through_fields`.\",\n                obj=Event._meta.get_field('invitees'),\n                id='fields.E340',\n            ),\n        ])\n\n"], "sample_183": ["    def test_complex_when_expression(self):\n        self.assertQuerysetEqual(\n            Client.objects.annotate(\n                discount=Case(\n                    When(\n                        Q(account_type=Client.GOLD) & Q(registered_on__lt=date.today() - timedelta(days=30)),\n                        then=Value('5%'),\n                    ),\n                    When(\n                        Q(account_type=Client.PLATINUM) | Q(registered_on__lt=date.today() - timedelta(days=365)),\n                        then=Value('10%'),\n                    ),\n                    default=Value('0%'),\n                    output_field=CharField(),\n                ),\n            ).order_by('pk'),\n            [('Jane Doe', '0%'), ('James Smith', '5%'), ('Jack Black', '10%')],\n            transform=attrgetter('name', 'discount')\n        )\n", "    def test_case_with_null_value(self):\n        self.assertQuerysetEqual(\n            CaseTestModel.objects.annotate(\n                test=Case(\n                    When(integer=1, then=Value(None)),\n                    default=Value('default'),\n                    output_field=CharField(),\n                ),\n            ).order_by('pk'),\n            [\n                (1, 'None'), (2, 'default'), (3, 'default'), (2, 'default'), (3, 'default'),\n                (3, 'default'), (4, 'default')\n            ],\n            transform=attrgetter('integer', 'test')\n        )\n", "    def test_when_with_null_condition(self):\n        msg = 'When() condition cannot be None.'\n        with self.assertRaisesMessage(ValueError, msg):\n            When(condition=None)\n", "    def test_when_with_f_expression(self):\n        class MyModel(models.Model):\n            integer = models.IntegerField()\n            string = models.CharField(max_length=10)\n\n        MyModel.objects.create(integer=1, string='a')\n        MyModel.objects.create(integer=2, string='b')\n\n        qs = MyModel.objects.annotate(\n            test=Case(\n                When(F('integer') == 1, then=F('string')),\n                default=Value('default'),\n                output_field=CharField(),\n            ),\n        )\n        self.assertQuerysetEqual(\n            qs,\n            [('a', 'a'), ('b', 'default')],\n            lambda x: (x.string, x.test)\n        )\n\n\n\n", "    def test_empty_case(self):\n        with self.assertRaises(ValueError):\n            Case()\n"], "sample_560": ["def test_legend_bbox_to_anchor():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='line')\n    leg = ax.legend(bbox_to_anchor=(0.5, 1.0))\n    assert leg.get_bbox_to_anchor() == (0.5, 1.0)\n", "def test_legend_invalid_label(invalid_label):\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label=invalid_label)\n    with pytest.raises(TypeError):\n        ax.legend()\n", "def test_legend_handle_length_mismatch():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='line1')\n    ax.plot([2, 3], [4, 5], label='line2')\n    ax.legend(handles=[mlines.Line2D([], [])])\n    with pytest.warns(UserWarning) as records:\n        ax.legend()\n    assert len(records) == 1\n    assert str(records[0].message) == (\n        \"Number of handles and labels must be the same.\")\n\n", "def test_loc_string(loc):\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='line')\n    ax.legend(loc=loc)\n", "    def test_loc_str(self, loc):\n        fig, ax = plt.subplots()\n        ax.plot([1, 2], [1, 2], label='line')\n        ax.legend(loc=loc)\n\n"], "sample_173": ["    def test_sql_flush_with_tables(self):\n        with transaction.atomic():\n            Author.objects.create(name='Test Author')\n            Book.objects.create(title='Test Book', author=Author.objects.get(name='Test Author'))\n\n        sql_list = connection.ops.sql_flush(no_style(), ['backends', 'auth'])\n        self.assertGreater(len(sql_list), 0)\n", "    def test_sql_flush_with_tables(self):\n        with transaction.atomic():\n            Author.objects.create(name='Author 1')\n            Book.objects.create(title='Book 1', author=Author.objects.get(name='Author 1'))\n        sql_list = connection.ops.sql_flush(no_style(), [Author._meta.db_table, Book._meta.db_table])\n        self.assertGreater(len(sql_list), 0)\n", "    def test_sql_flush_with_tables(self):\n        with transaction.atomic():\n            Author.objects.create(name='Author 1', age=30)\n            Book.objects.create(title='Book 1', author=Author.objects.get(name='Author 1'))\n        sql_list = connection.ops.sql_flush(no_style(), connection.introspection.get_table_list(connection.cursor()))\n        self.assertGreater(len(sql_list), 0)\n", "    def test_sql_flush_with_reset_sequences(self):\n        with transaction.atomic():\n            author = Author.objects.create(name='Test Author')\n            Book.objects.create(author=author, title='Test Book')\n        self.assertEqual(Author.objects.count(), 1)\n        self.assertEqual(Book.objects.count(), 1)\n        self.assertEqual(connection.ops.sql_flush(\n            no_style(),\n            reset_sequences=True,\n        ), [\n            'DELETE FROM \"backends_book\";',\n            'DELETE FROM \"backends_author\";',\n            'ALTER SEQUENCE \"backends_author_id_seq\" RESTART WITH 1;',\n            'ALTER SEQUENCE \"backends_book_id_seq\" RESTART WITH 1;',\n        ])\n        self.assertEqual(Author.objects.count(), 0)\n        self.assertEqual(Book.objects.count(), 0)\n\n", "    def test_sql_flush_with_transactions(self):\n        with transaction.atomic():\n            Author.objects.create(name='Test Author')\n            Book.objects.create(title='Test Book', author=Author.objects.get(name='Test Author'))\n        \n        sql = connection.ops.sql_flush(no_style(), models=[Author, Book], allow_cascade=True)\n        self.assertTrue(len(sql) > 0)\n"], "sample_993": ["compilation error", "compilation error", "    def test_FreeGroupElm_cyclic_conjugates():\n        w1 = x*y*x*y*x\n        assert len(w1.cyclic_conjugates()) == 5\n        assert w1 in w1.cyclic_conjugates()\n        w2 = x*y**5*x\n        assert len(w2.cyclic_conjugates()) == 6\n        w3 = x**-1*y**5*x**-1\n        assert w3 not in w2.cyclic_conjugates()\n", "    def test_FreeGroupElm_word_reduction():\n        w1 = x*y*x**-1*y**-1\n        assert w1.is_reduced() == True\n        w2 = x*y*x*y**-1\n        assert w2.is_reduced() == False\n        w2 = w2.reduce()\n        assert w2 == x*y\n        w3 = x**2*y**3*x**-1*y\n        assert w3.reduce() == x*y**3\n\n", "compilation error"], "sample_814": ["def test_gradient_boosting_warm_start_partial_fit():\n    # Test that warm_start works correctly with partial_fit\n    X, y = make_classification(n_samples=100, random_state=0)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\n    gbc = GradientBoostingClassifier(n_estimators=5,\n                                    learning_rate=0.1,\n                                    random_state=42, warm_start=True)\n\n    gbc.partial_fit(X_train[:50], y_train[:50])\n    gbc.partial_fit(X_train[50:], y_train[50:])\n\n    score_after_partial_fit = gbc.score(X_test, y_test)\n\n    gbc = GradientBoostingClassifier(n_estimators=10,\n                                    learning_rate=0.1,\n                                    random_state=42)\n    gbc.fit(X_train, y_train)\n\n    score_full_fit = gbc.score(X_test, y_test)\n\n    assert_almost_equal(score_after_partial_fit, score_full_fit, decimal=2)\n", "def test_gradient_boosting_n_estimators():\n    X, y = datasets.make_classification(n_samples=100, random_state=0)\n\n    # Test if n_estimators is set correctly\n    gb = GradientBoostingClassifier(n_estimators=10)\n    gb.fit(X, y)\n    assert gb.n_estimators_ == 10\n\n    gb = GradientBoostingRegressor(n_estimators=20)\n    gb.fit(X, y)\n    assert gb.n_estimators_ == 20\n", "compilation error", "    def test_gradient_boosting_loss_with_weight_validation(self):\n        # ensure weight validation happens correctly when loss is specific\n        X = [[1]] * 10\n        y = [0, 0] + [1] * 8\n\n        # this loss is not defined for weights other than uniform\n        gb = GradientBoostingClassifier(n_iter_no_change=5, random_state=0,\n                                        loss='deviance',\n                                        validation_fraction=0.2)\n        with pytest.raises(ValueError,\n                           match=\"The 'deviance' loss does not support sample \"\n                                 \"weights\"):\n            gb.fit(X, y, sample_weight=np.ones(10) * 2)\n", "    def test_gradient_boosting_n_estimators_warning():\n\n        X = [[1, 2], [2, 3], [3, 4], [4, 5]]\n        y = [0, 0, 0, 1]\n\n        gbc = GradientBoostingClassifier(n_estimators=100,\n                                        n_iter_no_change=5)\n\n        with pytest.warns(UserWarning,\n                         match='The value of n_estimators is larger than '\n                               'the possible number of estimators for early '\n                               'stopping'):\n            gbc.fit(X, y)\n"], "sample_72": ["    def test_serialize_custom_manager(self):\n        class CustomManager(models.Manager):\n            pass\n        class MyModel(models.Model):\n            object = models.CharField(max_length=255)\n            my_manager = CustomManager()\n\n        self.assertSerializedEqual(MyModel.my_manager)\n", "    def test_serialize_custom_manager(self):\n        class CustomManager(models.Manager):\n            pass\n        \n        class TestModel(models.Model):\n            objects = CustomManager()\n\n        string, imports = MigrationWriter.serialize(TestModel._meta.object_manager)\n\n        self.assertEqual(string, 'migrations.test_writer.CustomManager()')\n        self.assertEqual(imports, {'import migrations.test_writer'})\n", "    def test_serialize_complex_types(self):\n        class ComplexType:\n                self.a = a\n                self.b = b\n        \n        complex_instance = ComplexType(1, 'hello')\n        \n        with self.assertRaises(ValueError):\n            self.serialize_round_trip(complex_instance)\n\n        MigrationWriter.register_serializer(ComplexType, ComplexSerializer)\n        serialized = self.serialize_round_trip(complex_instance)\n        self.assertEqual(serialized, 'complex((1, \\'hello\\'))') \n", "    def test_serialize_lazy_objects_with_attributes(self):\n        class LazyObject(SimpleLazyObject):\n                self.value = value\n                self._wrapped = self.value()\n        \n        lazy_obj = LazyObject(lambda: {'key': 'value'})\n        self.assertEqual(self.serialize_round_trip(lazy_obj.value), {'key': 'value'})\n", "    def test_serialize_complex_serializer(self):\n        class MyComplexType:\n            pass\n\n        complex_serializer = ComplexSerializer(MyComplexType())\n        string = MigrationWriter.serialize(complex_serializer)[0]\n        self.assertEqual(string, 'complex(<migrations.test_writer.ComplexSerializer object at 0x...>)')\n"], "sample_1133": ["compilation error", "    def test_refraction_angle_medium():\n        m1 = Medium('m1', n=1.5)\n        m2 = Medium('m2', n=1.33)\n        r1 = Ray3D(Point3D(-1, -1, 1), Point3D(0, 0, 0))\n        i = Matrix([1, 1, 1])\n        assert refraction_angle(r1, m1, m2, [0, 0, 1]) == refraction_angle(r1, 1.5, 1.33, [0, 0, 1])\n\n", "    def test_fresnel_coefficients_s_p():\n        assert all(ae(i, j, 5) for i, j in zip(\n            fresnel_coefficients(0.5, 1, 1.33, polarization='s'),\n            [0.11163, -0.17138]))\n        assert all(ae(i, j, 5) for i, j in zip(\n            fresnel_coefficients(0.5, 1.33, 1, polarization='p'),\n            [-0.07726, 0.20482]))\n        m1 = Medium('m1')\n        m2 = Medium('m2', n=2)\n        assert all(ae(i, j, 5) for i, j in zip(\n            fresnel_coefficients(0.3, m1, m2, polarization='s'),\n            [0.31784, -0.34865]))\n        ans = [[-0.23563, -0.97184], [0.81648, -0.57738]]\n        got = fresnel_coefficients(0.6, m2, m1, polarization='p')\n        for i, j in zip(got, ans):\n            for a, b in zip(i.as_real_imag(), j):\n                assert ae(a, b, 5)\n\n", "    def test_refraction_angle_invalid():\n        r1 = Ray3D(Point3D(-1, -1, 1), Point3D(0, 0, 0))\n        raises(TypeError, lambda: refraction_angle(r1, 1, 'a'))  \n        raises(TypeError, lambda: refraction_angle(r1, 'a', 1))\n        raises(TypeError, lambda: refraction_angle(r1, 1, 1, [1, 2, 3, 4]))  \n        raises(TypeError, lambda: refraction_angle(1, 1, 1, normal_ray, 1))\n", "    def test_fresnel_coefficients_complex():\n        assert all(ae(i, j, 5) for i, j in zip(\n            fresnel_coefficients(0.5 + 0.2j, 1, 1.33),\n            [0.11163 + 0.12345j, -0.17138 + 0.23456j, 0.83581 + 0.34567j,\n             0.82862 - 0.45678j]))\n\n"], "sample_1063": ["compilation error", "compilation error", "def test_issue_19402():\n    if not numpy:\n        skip(\"numpy not installed\")\n    f = lambdify(x, asin(sin(x)))\n    assert numpy.allclose(f(numpy.array([0, pi/2, pi])),\n                          numpy.array([0, pi/2, pi]))\n", "compilation error", "def test_gamma_scipy():\n    if not scipy:\n        skip(\"scipy not installed\")\n\n    f = gamma(x)\n    F = lambdify(x, f, modules='scipy')\n\n    assert abs(gamma(1.3) - F(1.3)) <= 1e-10\n"], "sample_266": ["    def test_loading_squashed_erroneous_dependent(self):\n        \"\"\"Tests loading a set of squashed migrations with an erroneous dependency.\"\"\"\n        loader = MigrationLoader(connection)\n        recorder = MigrationRecorder(connection)\n\n        # Empty database - should be able to load\n        loader.build_graph()\n\n        # Apply app1's migrations\n        recorder.record_applied(\"app1\", \"1_auto\")\n        recorder.record_applied(\"app1\", \"2_auto\")\n\n        # Should fail with an error as app2 depends on a replaced migration from app1\n        with self.assertRaises(NodeNotFoundError):\n            loader.build_graph()\n", "    def test_missing_migration_file(self):\n        \"\"\"\n        Tests that when a migration file is missing, the loader still loads other migrations.\n        \"\"\"\n        migration_dir = os.path.join(\n            os.path.dirname(__file__),\n            'migrations',\n            'test_migrations',\n        )\n\n        os.remove(os.path.join(migration_dir, '0002_second.py'))\n        loader = MigrationLoader(connection)\n        self.assertIn(('migrations', '0001_initial'), loader.disk_migrations)\n        self.assertNotIn(('migrations', '0002_second'), loader.disk_migrations)\n", "    def test_circular_dependencies(self):\n        \"\"\"\n        Tests that circular dependencies between migrations raise an exception.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        with self.assertRaises(ValueError):\n            loader.graph.build()\n", "    def test_loading_with_empty_migration_dir(self):\n        \"\"\"\n        MigrationLoader should not raise an error when\n        the migration directory is empty.\n        \"\"\"\n        migration_dir = os.path.join(\n            os.path.dirname(__file__), 'migrations', 'test_migrations'\n        )\n        os.rmdir(migration_dir)\n        os.makedirs(migration_dir)\n        loader = MigrationLoader(connection)\n        self.assertEqual(set(), set(loader.disk_migrations))\n", "    def test_loading_squashed_partial_applied(self):\n        \"Tests loading a squashed migration with some dependencies already applied\"\n        loader = MigrationLoader(connection)\n        recorder = MigrationRecorder(connection)\n        self.addCleanup(recorder.flush)\n\n        # Apply the first migration of the squashed set.\n        recorder.record_applied('migrations', '0001_initial')\n        loader.build_graph()\n\n        # The plan should now consist of the remaining squashed migrations and\n        # any subsequent migrations.\n        plan = set(loader.graph.forwards_plan(('migrations', '0003_third')))\n        expected_plan = {\n            ('migrations', '2_squashed_3'),\n            ('migrations', '3_auto'),\n        }\n        self.assertEqual(plan, expected_plan)\n"], "sample_1003": ["    def test_Gens_postprocess_with_domain(self):\n        opt = {'gens': (x, y), 'domain': ZZ}\n        Gens.postprocess(opt)\n\n        assert opt == {'gens': (x, y), 'domain': ZZ}\n", "    def test_Options_clone_deep():\n        opt = Options((x, y, z), {'domain': 'ZZ' })\n        opt['domain'] = ZZ\n        new_opt = opt.clone({'gens': (x, y), 'order': 'lex'})\n\n        assert opt.gens == (x, y, z)\n        assert opt.domain == ZZ\n        assert ('order' in opt) is False\n\n        assert new_opt.gens == (x, y)\n        assert new_opt.domain == ZZ\n        assert ('order' in new_opt) is True\n", "compilation error", "def test_build_options():\n    opt = build_options((x, y, z), {'domain': ZZ})\n\n    assert opt.gens == (x, y, z)\n    assert opt.domain == ZZ\n\n    opt = build_options((x, y, z), {'domain': ZZ, 'opt': {'domain': ZZ, 'sort': 'lex'}})\n\n    assert opt.gens == (x, y, z)\n    assert opt.domain == ZZ\n    assert opt.sort == 'lex'\n", "    def test_Options_defaults(self):\n        opt = Options((x, y, z), {'domain': 'ZZ'})\n\n        assert opt.defaults == {}\n\n        opt = Options((x, y, z), {'domain': 'ZZ'}, defaults={'order': 'lex'})\n\n        assert opt.defaults == {'order': 'lex'}\n"], "sample_180": ["    def test_check_constraints_on_proxy_model(self):\n        class Base(models.Model):\n            field = models.IntegerField()\n\n        class Proxy(Base):\n            class Meta:\n                proxy = True\n                constraints = [models.UniqueConstraint(fields=['field'], name='unique_field')]\n\n        errors = Proxy.check(databases=self.databases)\n        self.assertEqual(errors, [\n            Warning(\n                \"Proxy model 'Proxy' defines constraints, but constraints are \"\n                \"not supported on proxy models.\",\n                obj=Proxy,\n                id='models.W028',\n            ),\n        ])\n", "    def test_check_constraints_invalid_condition(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['age'],\n                        name='unique_age',\n                        condition='age > 18',\n                    ),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [\n            Error(\n                \"'constraints' contains an invalid 'condition' value: 'age > 18'. \"\n                \"Only Q objects are allowed.\",\n                obj=Model,\n                id='models.E028',\n            ),\n        ])\n", "    def test_check_invalid_constraint_type(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                constraints = ['InvalidConstraintType']\n\n        self.assertEqual(Model.check(databases=self.databases), [\n            Error(\n                \"Invalid constraint type 'InvalidConstraintType'.\",\n                obj=Model,\n                id='models.E017',\n            ),\n        ])\n", "    def test_check_index(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                indexes = [models.Index(fields=['age'], name='my_index')]\n\n        errors = Model.check(databases=self.databases)\n        warn = Warning(\n            '%s does not support indexes.' % connection.display_name,\n            hint=(\n                \"An index won't be created. Silence this warning if you \"\n                \"don't care about it.\"\n            ),\n            obj=Model,\n            id='models.W025',\n        )\n        expected = [] if connection.features.supports_indexes else [warn]\n        self.assertCountEqual(errors, expected)\n", "    def test_check_exclude_constraints(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n            name = models.CharField(max_length=100)\n\n            class Meta:\n                constraints = [\n                    models.ExcludeConstraint(fields=['age', 'name'], name='exclude_age_name'),\n                ]\n\n        errors = Model.check(databases=self.databases)\n        warn = Warning(\n            '%s does not support exclude constraints.' % connection.display_name,\n            hint=(\n                \"A constraint won't be created. Silence this warning if you don't care about it.\"\n            ),\n            obj=Model,\n            id='models.W039',\n        )\n        expected = [] if connection.features.supports_exclude_constraints else [warn]\n        self.assertEqual(errors, expected)\n"], "sample_999": ["compilation error", "compilation error", "def test_issue_14144():\n    from sympy import latex\n    from sympy.physics.quantum import Dagger\n    a = symbols('a')\n    assert latex(Dagger(a)) == r'\\dagger a'\n", "def test_issue_14323():\n    from sympy.physics.units import meter, second\n    assert latex(meter/second) == r\"\\frac{\\text{m}}{\\text{s}}\"\n", "compilation error"], "sample_1002": ["compilation error", "def test_Float_from_int():\n    assert Float(1) == Float('1.0')\n    assert Float(10) == Float('10.0')\n    assert Float(-1) == Float('-1.0')\n", "compilation error", "compilation error", "compilation error"], "sample_655": ["    def test_capture_with_logging_and_teardown(testdir, capsys):\n        testdir.makepyfile(\n            \"\"\"\n            import logging\n            import sys\n\n            logger = logging.getLogger(__name__)\n\n                print(\"hello\")\n                sys.stderr.write(\"world\\\\n\")\n                captured = capsys.readouterr()\n                assert captured.out == \"hello\\\\n\"\n                assert captured.err == \"world\\\\n\"\n\n                logger.info(\"something\")\n                print(\"next\")\n                \n\n            \"\"\"\n        )\n\n        result = testdir.runpytest()\n        assert result.ret == 0\n", "def test_capture_with_live_logging_using_capfd(testdir):\n    # Issue 3819\n    # capture should work with live cli logging\n\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            print(\"hello\")\n            sys.stderr.write(\"world\\\\n\")\n            out, err = capfd.readouterr()\n            assert out == \"hello\\\\n\"\n            assert err == \"world\\\\n\"\n\n            logging.info(\"something\")\n            print(\"next\")\n            logging.info(\"something\")\n\n            out, err = capfd.readouterr()\n            assert out == \"next\\\\n\"\n        \"\"\"\n    )\n\n    result = testdir.runpytest_subprocess(\"--log-cli-level=INFO\")\n    assert result.ret == 0\n", "    def test_capture_with_live_logging_and_error(testdir, capsys):\n        testdir.makepyfile(\n            \"\"\"\n            import logging\n            import sys\n\n            logger = logging.getLogger(__name__)\n\n                print(\"hello\")\n                sys.stderr.write(\"world\\\\n\")\n                captured = capsys.readouterr()\n                assert captured.out == \"hello\\\\n\"\n                assert captured.err == \"world\\\\n\"\n\n                try:\n                    raise ValueError\n                except ValueError:\n                    logging.exception(\"something\")\n                    print(\"next\")\n\n                captured = capsys.readouterr()\n                assert \"ValueError\" in captured.err\n                assert captured.out == \"next\\\\n\"\n        \"\"\"\n        )\n\n        result = testdir.runpytest_subprocess(\"--log-cli-level=ERROR\")\n        assert result.ret == 0\n", "def test_capture_with_live_logging_and_capture_in_setup_teardown(testdir, capsys):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import sys\n\n        logger = logging.getLogger(__name__)\n\n            print(\"setup\")\n            logging.info(\"setup\")\n\n            print(\"teardown\")\n            logging.info(\"teardown\")\n\n            print(\"hello\")\n            sys.stderr.write(\"world\\\\n\")\n            captured = capsys.readouterr()\n            assert captured.out == \"hello\\\\n\"\n            assert captured.err == \"world\\\\n\"\n            print(\"next\")\n        \"\"\"\n    )\n\n    result = testdir.runpytest_subprocess(\"--log-cli-level=INFO\")\n    assert result.ret == 0\n    out = result.stdout.lines\n    err = result.stderr.lines\n    assert \"setup\" in out\n    assert \"teardown\" in out\n", "    def test_capture_with_nested_function(testdir, capsys):\n        testdir.makepyfile(\n            \"\"\"\n            import sys\n\n                    print(\"hello from inner\")\n\n                inner()\n                print(\"hello from test\")\n                sys.stderr.write(\"error from test\\\\n\")\n\n                with capsys.disabled():\n                    test_nested()\n\n                captured = capsys.readouterr()\n                assert captured.out == \"hello from test\\\\n\"\n                assert captured.err == \"error from test\\\\n\"\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        assert result.ret == 0\n"], "sample_615": ["compilation error", "    def test_polyval_scalar_coeffs():\n        x = np.arange(10)\n        da = xr.DataArray(\n            np.stack((1.0 + x + 2.0 * x**2, 1.0 + 2.0 * x + 3.0 * x**2)),\n            dims=(\"d\", \"x\"),\n            coords={\"x\": x, \"d\": [0, 1]},\n        )\n        coeffs = xr.DataArray([2, 1, 1], dims=(\"degree\",), name=\"coeffs\")\n        da_pv = xr.polyval(x, coeffs)\n\n        expected = xr.DataArray(\n            np.stack((2 * x**2 + x + 1, 3 * x**2 + 2 * x + 1)),\n            dims=(\"d\", \"x\"),\n            coords={\"x\": x, \"d\": [0, 1]},\n        )\n        xr.testing.assert_allclose(expected, da_pv.T)\n\n", "compilation error", "compilation error", "def test_polyval_errors(use_dask) -> None:\n    if use_dask and not has_dask:\n        pytest.skip(\"requires dask\")\n\n    x = np.arange(10)\n    da = xr.DataArray(\n        np.stack((1.0 + x + 2.0 * x**2, 1.0 + 2.0 * x + 3.0 * x**2)),\n        dims=(\"d\", \"x\"),\n        coords={\"x\": x, \"d\": [0, 1]},\n    )\n\n    coeffs = xr.DataArray([[2, 1], [3, 2]], dims=(\"d\", \"degree\"), coords={\"d\": [0, 1], \"degree\": [1, 0]})\n\n    if use_dask:\n        coeffs = coeffs.chunk({\"d\": 2})\n    \n    with pytest.raises(ValueError, match = \"Dimension 'degree' must be present in both\"):\n        xr.polyval(da.x, coeffs.rename({\"degree\": \"deg\"}))\n  \n\n"], "sample_362": ["    def test_mti_inheritance_add_and_remove_fields(self):\n        Animal = ModelState('app', 'Animal', [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n        ])\n        Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n        DogModified = ModelState('app', 'Dog', [\n            (\"breed\", models.CharField(max_length=200)),\n        ], bases=('app.Animal',))\n        changes = self.get_changes([Animal, Dog], [Animal, DogModified])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['AddField'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='breed', model_name='dog')\n\n", "    def test_alter_field_to_fk_dependency_same_app(self):\n        changes = self.get_changes(\n            [self.author_empty, self.book_with_no_author_fk],\n            [self.author_empty, self.book_with_author_fk],\n        )\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['AlterField'])\n        self.assertMigrationDependencies(changes, 'testapp', 0, [])\n", "    def test_operation_with_custom_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Person', fields=[]),\n                migrations.AlterModelOptions('Person', name='CustomPerson'),\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'person_custom_person')\n\n", "    def test_operation_with_custom_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    'Person',\n                    fields=[],\n                    name='CustomPerson',\n                ),\n            ]\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'customperson')\n", "    def test_rename_model(self):\n        before = [\n            ModelState('testapp', 'OldName', [\n                ('id', models.AutoField(primary_key=True)),\n            ]),\n        ]\n        after = [\n            ModelState('testapp', 'NewName', [\n                ('id', models.AutoField(primary_key=True)),\n            ]),\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['RenameModel'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, old_name='OldName', new_name='NewName')\n"], "sample_114": ["    def test_delete_field_with_default(self):\n        \"\"\"\n        #23417 - Deleting a field with a default should\n        remove the default from the migration.\n        \"\"\"\n        changes = self.get_changes([self.author_with_age], [self.author_empty])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"RemoveField\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='age')\n", "    def test_alter_unique_together_with_field_addition(self):\n        \"\"\"\n        #24458 - Adding a unique_together constraint after adding a new field\n        should be handled correctly.\n\n        \"\"\"\n        model_state_before = ModelState('testapp', 'Book', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=100)),\n        ])\n        model_state_after = ModelState('testapp', 'Book', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=100)),\n            ('author', models.CharField(max_length=100)),\n        ],\n            options={'unique_together': [('title', 'author')]}\n        )\n        changes = self.get_changes([model_state_before], [model_state_after])\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\", \"AlterUniqueTogether\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='author')\n        self.assertOperationAttributes(changes,\n                                        'testapp',\n                                        0,\n                                        1,\n                                        name='Book',\n                                        unique_together=[('title', 'author')])\n", "    def test_alter_unique_together(self):\n        \"\"\"\n        Adding/removing unique_together should be a separate operation\n        \"\"\"\n        before = ModelState('testapp', 'Book', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=100)),\n            ('author', models.ForeignKey('testapp.Author', models.CASCADE)),\n        ])\n        after = ModelState('testapp', 'Book', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=100)),\n            ('author', models.ForeignKey('testapp.Author', models.CASCADE)),\n        ], unique_together=[('title', 'author')])\n        changes = self.get_changes([before], [after])\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['AlterUniqueTogether'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='Book', unique_together=[('title', 'author')])\n", "    def test_delete_field_with_default(self):\n        \"\"\"\n        #23514 - Deleting a field with a default value should\n        correctly handle the existing data and remove the default.\n        \"\"\"\n        changes = self.get_changes(\n            [self.author_with_default_age],\n            [self.author_name],\n        )\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"RemoveField\"])\n", "    def test_remove_field_with_default_value(self):\n        # Test removing a field that has a default value.\n\n        changes = self.get_changes([self.author_with_biography_default], [self.author_empty])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"RemoveField\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='biography')\n"], "sample_268": ["    def test_notify_file_changed_calls_should_stop_when_disabled(self):\n        self.reloader.should_stop = True\n        self.reloader.notify_file_changed(self.existing_file)\n        self.assertFalse(self.reloader.should_stop)\n", "    def test_watch_dir_with_multiple_files(self, mock_notify_file_changed):\n        self.reloader.watch_dir(self.tempdir, '*.py')\n        file1 = self.ensure_file(self.tempdir / 'file1.py')\n        file2 = self.ensure_file(self.tempdir / 'file2.py')\n        with self.tick_twice():\n            self.increment_mtime(file1)\n            self.increment_mtime(file2)\n        self.assertEqual(mock_notify_file_changed.call_count, 2)\n        self.assertCountEqual(mock_notify_file_changed.call_args_list, [mock.call(file1), mock.call(file2)])\n\n", "    def test_should_stop_notifies_file_changed(self, mocked_modules, notify_mock):\n        self.reloader.should_stop = True\n        self.reloader.watch_dir(self.tempdir, '*.py')\n        with self.tick_twice():\n            self.increment_mtime(self.existing_file)\n        self.assertEqual(notify_mock.call_count, 0)\n", "    def test_glob_with_symlink(self, mocked_modules, notify_mock):\n        symlink_target = self.ensure_file(self.tempdir / 'target.py')\n        os.symlink(str(symlink_target), str(self.tempdir / 'link.py'))\n        self.reloader.watch_dir(self.tempdir, '*.py')\n        with self.tick_twice():\n            self.increment_mtime(symlink_target)\n        self.assertEqual(notify_mock.call_count, 1)\n        self.assertCountEqual(notify_mock.call_args[0], [symlink_target])\n", "    def test_should_stop(self):\n        self.assertFalse(self.reloader.should_stop())\n        self.reloader._error_files = [self.existing_file]\n        self.assertTrue(self.reloader.should_stop())\n"], "sample_330": ["    def test_can_delete_with_fk_constraint(self):\n        obj = Object.objects.create()\n        ObjectReference.objects.create(obj=obj)\n        obj.delete()\n        self.assertEqual(ObjectReference.objects.count(), 0)\n", "    def test_cascade_delete(self):\n        obj = Object.objects.create()\n        obj_ref = ObjectReference.objects.create(obj=obj)\n        obj.delete()\n        with self.assertRaises(ObjectReference.DoesNotExist):\n            ObjectReference.objects.get(pk=obj_ref.pk)\n", "    def test_can_reference_non_existent_with_null(self):\n        ref = ObjectReference.objects.create(obj=None)\n        self.assertIsNone(ref.obj)\n", "    def test_can_reference_deleted(self):\n        obj = Object.objects.create()\n        ref = ObjectReference.objects.create(obj=obj)\n        obj.delete()\n        self.assertFalse(Object.objects.filter(id=obj.id).exists())\n        with self.assertRaises(Object.DoesNotExist):\n            ref.obj\n\n", "    def test_cascade_delete_related_objects(self):\n        obj = Object.objects.create()\n        ref1 = ObjectReference.objects.create(obj=obj)\n        ref2 = ObjectReference.objects.create(obj=obj)\n        obj.delete()\n        self.assertFalse(ObjectReference.objects.filter(obj=obj).exists())\n"], "sample_791": ["compilation error", "compilation error", "    def test_one_hot_encoder_drop_first_sparse(self):\n        X = [['abc', 2, 55], ['def', 1, 55], ['abc', 3, 55]]\n        enc = OneHotEncoder(sparse=True, drop='first')\n        X_tr = enc.fit_transform(X)\n        assert X_tr.shape == (3, 4)\n        assert isinstance(X_tr, sparse.csr_matrix)\n        exp = np.array([[0, 1, 1], [1, 0, 1], [0, 1, 0]])\n        assert_array_equal(X_tr.toarray(), exp) \n", "compilation error", "compilation error"], "sample_506": ["def test_spine_circular(fig_test, fig_ref):\n    fig_test.set_facecolor('lightgrey')\n    fig_ref.set_facecolor('lightgrey')\n\n    ax = fig_test.add_subplot(projection='polar')\n    ax.set_theta_zero_location(\"N\")\n    ax.set_theta_direction(-1)\n    ax.spines[\"polar\"].set_visible(False)\n\n    ax.spines.left.set_position(('data', 1))\n    ax.spines.left.set_smart_bounds(True)\n    ax.spines.left.set_patch_arc((0, 0), 1, 0, 90)\n    ax.spines.left.set_color('blue')\n\n    ax = fig_ref.add_subplot(projection='polar')\n    ax.set_theta_zero_location(\"N\")\n    ax.set_theta_direction(-1)\n    ax.spines[\"polar\"].set_visible(False)\n\n    ax.spines.left.set_position(('data', 1))\n    ax.spines.left.set_smart_bounds(True)\n    ax.spines.left.set_patch_arc((0, 0), 1, 0, 90)\n    ax.spines.left.set_color('blue')\n", "def test_spine_color():\n    fig, ax = plt.subplots()\n    ax.spines['top'].set_color('red')\n    ax.spines['right'].set_color('green')\n    ax.spines['bottom'].set_color('blue')\n    ax.spines['left'].set_color('yellow')\n", "    def test_spine_circular(fig_test, fig_ref):\n        ax = fig_test.add_subplot(1, 1, 1, aspect='equal')\n        ax.set_xlim(0, 1)\n        ax.set_ylim(0, 1)\n        ax.spines.left.set_visible(False)\n        ax.spines.right.set_visible(False)\n        ax.spines.top.set_visible(False)\n        ax.spines.bottom.set_visible(False)\n        spine = ax.spines.circular(0.5, 0.5, 0.4)\n        spine.set_color('red')\n        spine.set_linestyle('--')\n\n        ax = fig_ref.add_subplot(1, 1, 1, aspect='equal')\n        ax.set_xlim(0, 1)\n        ax.set_ylim(0, 1)\n        ax.spines.left.set_visible(False)\n        ax.spines.right.set_visible(False)\n        ax.spines.top.set_visible(False)\n        ax.spines.bottom.set_visible(False)\n        spine = ax.spines.circular(0.5, 0.5, 0.4)\n        spine.set_color('red')\n        spine.set_linestyle('--')\n", "def test_spines_patch_types():\n    fig, ax = plt.subplots()\n    ax.spines['top'].set_patch_arc(center=(0.5, 0.5), radius=0.5, theta1=0, theta2=180)\n    ax.spines['right'].set_patch_circle(center=(0.5, 0.5), radius=0.5)\n    ax.spines['bottom'].set_patch_line()\n    ax.set_xlim([0, 1])\n    ax.set_ylim([0, 1])\n    ax.set_aspect('equal')\n", "def test_spine_visibility():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1])\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n"], "sample_477": ["    def test_random03(self):\n        output = self.engine.render_to_string(\"random03\", {\"a\": [1, 2, 3]})\n        self.assertIn(output, [\"1\", \"2\", \"3\"])\n", "    def test_random03(self):\n        output = self.engine.render_to_string(\n            \"random03\", {\"a\": [\"a&b\", \"a&b\"], \"b\": [mark_safe(\"a&b\"), mark_safe(\"a&b\")]}\n        )\n        self.assertEqual(output, \"a&amp;b a&amp;b\")\n", "    def test_dictsort01(self):\n        output = self.engine.render_to_string(\n            \"dictsort01\",\n            {\n                \"a\": [\n                    {\"name\": \"Bob\", \"age\": 30},\n                    {\"name\": \"Alice\", \"age\": 25},\n                ]\n            },\n        )\n        self.assertEqual(\n            output,\n            '[{\"name\": \"Alice\", \"age\": 25}, {\"name\": \"Bob\", \"age\": 30}]',\n        )\n", "    def test_random03(self):\n        output = self.engine.render_to_string(\n            \"random03\", {\"a\": [\"a&b\", \"a&b\"], \"b\": [mark_safe(\"a&b\"), mark_safe(\"a&b\")]}\n        )\n        self.assertEqual(output, \"a&amp;b a&amp;b\")\n\n", "    def test_random03(self):\n        output = self.engine.render_to_string(\n            \"random03\", {\"a\": [\"a&b\", \"a&b\"], \"b\": [\"a&b\", \"a&b\"]}\n        )\n        self.assertEqual(output, \"a&amp;b a&amp;b\")\n"], "sample_450": ["    def test_get_admin_log_template_tag(self):\n        \"\"\"Test the get_admin_log template tag.\"\"\"\n        LogEntry.objects.create(\n            user=self.user,\n            content_type=ContentType.objects.get_for_model(Article),\n            object_id=self.a1.pk,\n            object_repr=repr(self.a1),\n            action_time=datetime.now(),\n            action_flag=CHANGE,\n            change_message=\"Changed something\",\n        )\n\n        template = \"\"\"\n        {% get_admin_log 5 as admin_log %}\n        {% for log in admin_log %}\n        {{ log.object_repr }}\n        {% endfor %}\n        \"\"\"\n        rendered = self.render_template(template)\n\n        self.assertIn(repr(self.a1), rendered)\n", "    def test_get_admin_log_templatetag(self):\n        \"\"\"\n        Test the get_admin_log template tag.\n        \"\"\"\n        with self.client.login(username=\"super\", password=\"secret\"):\n            response = self.client.get(reverse(\"admin:index\"))\n            self.assertEqual(response.status_code, 200)\n\n            # Test without any arguments\n            context = self.client.get_context_data()\n            self.assertNotIn('log_entries', context)\n\n            # Test with limit and user\n            template_code = '{% load admin_utils %}{% get_admin_log 5 as recent_actions for_user user %}'\n            result = render_to_string(template_code, context={'user': self.user})\n            self.assertIn('recent_actions', context)\n            recent_actions = context[\"recent_actions\"]\n            self.assertEqual(len(recent_actions), 1)\n\n            # Test with limit without user\n            template_code = '{% load admin_utils %}{% get_admin_log 1 as recent_actions %}'\n            result = render_to_string(template_code, context)\n            self.assertIn('recent_actions', context)\n            recent_actions = context[\"recent_actions\"]\n            self.assertEqual(len(recent_actions), 1)\n", "    def test_get_admin_log_template_tag(self):\n        template = \"{% get_admin_log 3 as recent_actions %}\"\n        rendered = template.render(\n            context={\"log_entries\": LogEntry.objects.all()}\n        )\n        self.assertEqual(rendered, \"\")\n        context = {\"log_entries\": LogEntry.objects.all()}\n        template = \"{% get_admin_log 3 as recent_actions for_user 23 %}\"\n        rendered = template.render(context)\n        self.assertEqual(rendered, \"\")\n        context = {\"user\": self.user, \"log_entries\": LogEntry.objects.all()}\n        template = \"\"\n        rendered = template.render(context)\n", "    def test_get_admin_log_template_tag(self):\n        \"\"\"\n        Tests the get_admin_log template tag.\n        \"\"\"\n        LogEntry.objects.create(\n            user=self.user,\n            action_time=datetime.now(),\n            content_type=ContentType.objects.get_for_model(Article),\n            object_id=self.a1.pk,\n            object_repr=repr(self.a1),\n            action_flag=CHANGE,\n        )\n        template = \"\"\"\n        {% load admin_utils_tags %}\n        {% get_admin_log 5 as admin_log %}\n        {% get_admin_log 10 as admin_log_user for_user user %}\n        <ul>\n        {% for log in admin_log %}\n            <li>{{ log.object_repr }}</li>\n        {% endfor %}\n        </ul>\n        <ul>\n        {% for log in admin_log_user %}\n            <li>{{ log.object_repr }}</li>\n        {% endfor %}\n        </ul>\n        \"\"\"\n        response = self.client.get(reverse(\"admin:index\"))\n        response.content = response.content.decode()\n        response.template = template\n\n        self.assertContains(response, \"<ul><li>\" + repr(self.a1) + \"</li></ul>\")\n        self.assertContains(response, \"<ul><li>\" + repr(self.a1) + \"</li></ul>\")\n", "    def test_get_admin_log_template_tag(self):\n        LogEntry.objects.create(\n            user=self.user,\n            content_type=ContentType.objects.get_for_model(Article),\n            object_id=self.a1.pk,\n            object_repr=repr(self.a1),\n            action_flag=CHANGE,\n            change_message=\"Changed something else\",\n        )\n        LogEntry.objects.create(\n            user=self.user,\n            content_type=ContentType.objects.get_for_model(Article),\n            object_id=self.a1.pk,\n            object_repr=repr(self.a1),\n            action_flag=CHANGE,\n            change_message=\"Another change\",\n        )\n\n        template = \"\"\""], "sample_162": ["    def test_project_locale_paths_with_app_locale_priority(self):\n        with override_settings(LOCALE_PATHS=[\n            os.path.join(self.test_dir, 'project_locale'),\n            os.path.join(self.test_dir, 'app_with_locale', 'locale')]):\n            management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n            project_de_locale = os.path.join(\n                self.test_dir, 'project_locale', 'de', 'LC_MESSAGES', 'django.po')\n            app_de_locale = os.path.join(\n                self.test_dir, 'app_with_locale', 'locale', 'de', 'LC_MESSAGES', 'django.po')\n            self.assertTrue(os.path.exists(project_de_locale))\n            self.assertTrue(os.path.exists(app_de_locale))\n\n\n", "    def test_project_locale_paths_with_non_standard_domain(self):\n        with override_settings(LOCALE_PATHS=[self.test_dir], DEFAULT_DOMAIN='my_domain'):\n            management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n            project_de_locale = os.path.join(\n                self.test_dir, 'locale', 'de', 'LC_MESSAGES', 'my_domain.po')\n            self.assertTrue(os.path.exists(project_de_locale))\n", "    def test_custom_locale_paths(self):\n        management.call_command('makemessages',\n                                locale=[LOCALE],\n                                verbosity=0,\n                                default_locale_path=os.path.join(self.test_dir, 'custom_locale'))\n        custom_locale = os.path.join(self.test_dir, 'custom_locale', LOCALE, 'LC_MESSAGES', 'django.po')\n        self.assertTrue(os.path.exists(custom_locale))\n", "    def test_custom_locale_path(self):\n        with override_settings(LOCALE_PATHS=[os.path.join(self.test_dir, 'custom_locale')]):\n            management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n            custom_locale_file = os.path.join(\n                self.test_dir, 'custom_locale', LOCALE, 'LC_MESSAGES', 'django.po')\n            self.assertTrue(os.path.exists(custom_locale_file))\n", "    def test_nested_project_locale_paths(self):\n        with override_settings(LOCALE_PATHS=[\n            os.path.join(self.test_dir, 'project_locale'),\n            os.path.join(self.test_dir, 'project_locale', 'nested_locale'),\n        ]):\n            management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n            project_de_locale = os.path.join(\n                self.test_dir, 'project_locale', 'de', 'LC_MESSAGES', 'django.po')\n            nested_de_locale = os.path.join(\n                self.test_dir, 'project_locale', 'nested_locale', 'de', 'LC_MESSAGES', 'django.po')\n            self.assertTrue(os.path.exists(project_de_locale))\n            self.assertTrue(os.path.exists(nested_de_locale))\n"], "sample_709": ["compilation error", "compilation error", "def test_pytester_mkdir_with_parent_dirs(pytester: Pytester) -> None:\n    \"\"\"Ensure that mkdir creates parent directories if they don't exist.\"\"\"\n    parent_dir = pytester.path / \"parent_dir\"\n    file_path = parent_dir / \"file.txt\"\n    pytester.mkdir(file_path)\n    assert parent_dir.exists()\n    assert file_path.exists()\n", "        def test_with_warning():\n            warnings.warn(\"some custom warning\")", "compilation error"], "sample_1024": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_794": ["    def test_ridgecv_warm_start():\n        rng = np.random.RandomState(0)\n        X = rng.randn(10, 5)\n        y = rng.randn(10)\n        alphas = [0.1, 1.0, 10.0]\n\n        ridgecv = RidgeCV(alphas=alphas, cv=5)\n        ridgecv.fit(X, y)\n\n        # Test warm start with same alphas\n        ridgecv.fit(X, y)\n        assert ridgecv.coef_.shape == (5,)\n        assert ridgecv.intercept_.shape == ()\n\n        # Test warm start with different alphas\n        ridgecv = RidgeCV(alphas=[0.5, 2.0, 5.0], cv=5, warm_start=True)\n        ridgecv.fit(X, y)\n        assert ridgecv.coef_.shape == (5,)\n        assert ridgecv.intercept_.shape == ()\n\n", "    def test_ridge_cv_empty_input():\n        # Test RidgeCV with empty input data\n\n        ridgecv_empty = RidgeCV()\n        X_empty = np.array([])\n        y_empty = np.array([])\n\n        assert_raises(ValueError, ridgecv_empty.fit, X_empty, y_empty)\n", "def test_ridge_classifier_with_precomputed_kernel():\n    # Test RidgeClassifier with precomputed kernel\n\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([0, 1, 0])\n\n    kernel = np.dot(X, X.T)\n    clf = RidgeClassifier(alpha=1.0, solver='svd')\n    clf.fit(kernel, y)\n    predictions = clf.predict(kernel)\n    assert predictions.shape == (3,)\n", "def test_ridge_warm_start():\n    rng = np.random.RandomState(42)\n    n_samples, n_features = 10, 5\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples)\n    alpha = 1.0\n    ridge = Ridge(alpha=alpha, solver='sag', max_iter=2, warm_start=True)\n\n    # First fit\n    ridge.fit(X, y)\n    coef_1 = ridge.coef_.copy()\n\n    # Second fit, warm start should reuse previous solution\n    ridge.fit(X, y)\n    coef_2 = ridge.coef_.copy()\n\n    # Assert coefficients are close, indicating warm start worked\n    assert np.allclose(coef_1, coef_2)\n", "def test_ridge_multi_output_intercept(fit_intercept):\n    rng = np.random.RandomState(42)\n    n_samples, n_features, n_targets = 50, 10, 3\n\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples, n_targets)\n    if fit_intercept:\n        true_intercept = rng.randn(n_targets)\n        y += true_intercept\n    ridge = Ridge(fit_intercept=fit_intercept)\n    ridge.fit(X, y)\n\n    if fit_intercept:\n        assert_allclose(ridge.intercept_, true_intercept, atol=1e-5)\n\n    predicted = ridge.predict(X)\n    assert_allclose(predicted, y, atol=1e-5)\n"], "sample_1027": ["compilation error", "compilation error", "def test_Poly_empty():\n    assert Poly() == Poly(0)\n", "compilation error", "compilation error"], "sample_519": ["    def test_gridspec_col_spans():\n        fig = plt.figure()\n        gs = fig.add_gridspec(2, 3)\n\n        ax1 = fig.add_subplot(gs[0, :])\n        ax2 = fig.add_subplot(gs[1, :2])\n\n        assert ax1.get_subplotspec().colspan.start == 0\n        assert ax1.get_subplotspec().colspan.stop == 3\n        assert ax2.get_subplotspec().colspan.start == 0\n        assert ax2.get_subplotspec().colspan.stop == 2\n\n\n", "    def plot_with_empty_array(self):\n        fig, ax = plt.subplots()\n        ax.plot([], [])\n        assert len(ax.lines) == 1\n", "def test_subfigure_gridspec():\n    fig = plt.figure()\n    gs = fig.add_gridspec(2, 2)\n    fig.add_subfigure(gs[:, 0]).subplots()\n    fig.add_subfigure(gs[0, 1]).subplots()\n    fig.add_subfigure(gs[1, 1]).subplots()\n", "def test_subfigure_gridspec_spacing():\n    fig = plt.figure(figsize=(10, 10), layout='constrained')\n    gs = fig.add_gridspec(2, 2, hspace=0.5, wspace=0.2)\n    subfigs = [fig.add_subfigure(gs[i, j]) for i in range(2) for j in range(2)]\n    for subfig in subfigs:\n        subfig.add_subplot()\n    fig.draw_without_rendering()\n    # Check that the subfigures have the expected spacing\n    expected_hspace = 0.5 * fig.get_figheight()\n    expected_wspace = 0.2 * fig.get_figwidth()\n    \n    np.testing.assert_allclose(subfigs[1].bbox.min[1] -\n                               subfigs[0].bbox.max[1],\n                               expected_hspace)\n    np.testing.assert_allclose(subfigs[2].bbox.min[0] -\n                               subfigs[0].bbox.max[0],\n                               expected_wspace)\n\n", "def test_figure_clear():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [2, 3])\n\n    fig.clear()\n    assert len(fig.axes) == 0\n    assert fig.get_label() == \"\"\n"], "sample_1089": ["def test_issue_8601():\n    a = Symbol('a', real=True)\n    b = Symbol('b', real=True)\n    c = Symbol('c', real=True)\n    assert _monotonic_sign(b*c - a*c) == 0\n    assert _monotonic_sign((b + c)**2 - a*c) == 2\n", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_725": ["    def test_check_array_dtype_object_to_numeric():\n        X = np.array([['1', '2'], ['3', '4']], dtype=object)\n        # Check that dtype is correctly converted to numeric\n        result = check_array(X, dtype=np.float64)\n        assert_equal(result.dtype, np.float64)\n        assert_allclose(result, [[1, 2], [3, 4]])\n\n        # Check that warning is raised when warn_on_dtype is True\n        with assert_warns(DataConversionWarning):\n            result = check_array(X, dtype=np.float64,\n                                     warn_on_dtype=True)\n        assert_equal(result.dtype, np.float64)\n", "    def test_check_array_dtype_object_with_allow_nd(self):\n        X = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]], dtype=object)\n        # check that object dtype array is converted to float64 by default\n        X_checked = check_array(X, allow_nd=True)\n        assert_equal(X_checked.dtype, np.float64)\n        assert_array_equal(X_checked, np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]))\n", "    def test_check_consistent_length_empty_lists(self):\n        check_consistent_length([], [])\n        assert_raises(ValueError, check_consistent_length, [])\n", "compilation error", "    def test_check_X_y_dtype_object(self):\n        # test that X_y with dtype object get converted correctly\n        X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=object)\n        y = np.array([0, 1, 2], dtype=object)\n\n        X_checked, y_checked = check_X_y(X, y)\n        assert_equal(X_checked.dtype, np.int64)\n        assert_equal(y_checked.dtype, np.int64)\n"], "sample_542": ["compilation error", "def test_text_units():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], [4, 5, 6])\n    text = ax.text(1, 5, \"Point 1\")\n    assert text.get_unit() == 'data'\n    text.set_unit('points')\n    assert text.get_unit() == 'points'\n", "    def test_text_with_unicode_escape():\n        fig, ax = plt.subplots()\n        ax.text(0.5, 0.5, r\"$\\unicode{x00A9}$\", fontsize=12)\n        fig.canvas.draw()\n", "    def test_text_with_utf8():\n        fig, ax = plt.subplots()\n        ax.text(0.5, 0.5, '\u20ac', fontsize=12)\n        fig.canvas.draw()\n        bbox = ax.texts[0].get_window_extent(fig.canvas.get_renderer())\n        assert bbox.width > 0\n", "    def test_fontfamily_missing(self):\n        with pytest.raises(ValueError):\n            plt.text(0.5, 0.5, \"foo\", fontfamily=\"MissingFont\")\n"], "sample_48": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_9": ["def test_write_table_html_row_formats():\n    \"\"\"\n    Test that HTML writer can write rows with custom column formats.\n    \"\"\"\n    t = Table([[1, 2], [3, 4]], names=('a', 'b'))\n    row_formats = {'a': '02d', 'b': '.2f'}\n    expected = \"\"\"\\", "def test_write_table_html_with_units():\n    \"\"\"\n    Test to make sure that the HTML writer writes tables with units correctly.\n    \"\"\"\n    table = Table([[1, 2],\n                   [3, 4]], names=('A', 'B'),\n                  units=('m', 'kg'))\n\n    expected = \"\"\"\\", "def test_write_table_html_missing_data():\n    \"\"\"\n    Test that missing data are correctly handled when writing HTML tables.\n    \"\"\"\n    buffer_output = StringIO()\n    t = Table([[1, 2], [None, 4]], names=('a', 'b'))\n    ascii.write(t, buffer_output, format='html',\n                htmldict={'missing_data': 'N/A'})\n\n    t_expected = Table([[1, 2], ['N/A', 4]], names=('a', 'b'))\n    buffer_expected = StringIO()\n    ascii.write(t_expected, buffer_expected, format='html')\n\n    assert buffer_output.getvalue() == buffer_expected.getvalue()\n", "    def test_write_table_html_fill_values_formatted_columns():\n        \"\"\"\n        Test to make sure that the HTML writer writes out using the\n        supplied formatting and correctly replaces fill values.\n        \"\"\"\n\n        col1 = [1, 2]\n        col2 = [1.234567e-11, -9.876543e11]\n        formats = {\"C1\": \"04d\", \"C2\": \".2e\"}\n        table = Table([col1, col2], names=formats.keys())\n\n        expected = \"\"\"\\", "def test_write_table_html_format_escaped():\n    \"\"\"\n    Test to make sure that the HTML writer writes out correctly\n    escaped characters when using format strings \n    \"\"\"\n\n    col1 = ['<html>', '&']\n    table = Table([col1], names=['C1'])\n\n    expected = \"\"\"\\"], "sample_734": ["    def test_fowlkes_mallows_score_sparse():\n        labels_a = np.array([0, 0, 0, 1, 1, 1])\n        labels_b = np.array([0, 0, 1, 1, 2, 2])\n\n        score_dense = fowlkes_mallows_score(labels_a, labels_b)\n        score_sparse = fowlkes_mallows_score(labels_a, labels_b, sparse=True)\n\n        assert_almost_equal(score_dense, score_sparse)\n", "def test_normalized_mutual_info_score():\n    # Compute the Normalized Mutual Information and test against known\n    # values\n    labels_a = np.array([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3])\n    labels_b = np.array([1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 3, 1, 3, 3, 3, 2, 2])\n    # Mutual information\n    mi = mutual_info_score(labels_a, labels_b)\n    # Normalized Mutual Information\n    nmi = normalized_mutual_info_score(labels_a, labels_b)\n    assert_almost_equal(nmi, 0.45413, 5)\n    # with provided sparse contingency\n    C = contingency_matrix(labels_a, labels_b, sparse=True)\n    nmi = normalized_mutual_info_score(labels_a, labels_b,\n                                       contingency=C)\n    assert_almost_equal(nmi, 0.45413, 5)\n    # with provided dense contingency\n    C = contingency_matrix(labels_a, labels_b)\n    nmi = normalized_mutual_info_score(labels_a, labels_b,\n                                       contingency=C)\n    assert_almost_equal(nmi, 0.45413, 5)\n\n", "compilation error", "    def test_fowlkes_mallows_score_sparse():\n        labels_a = np.array([1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3])\n        labels_b = np.array([1, 1, 1, 1, 2, 1, 2, 2, 2, 2, 3, 1, 3, 3, 3, 2, 2])\n        score = fowlkes_mallows_score(labels_a, labels_b, sparse=True)\n        score_dense = fowlkes_mallows_score(labels_a, labels_b)\n        assert_almost_equal(score, score_dense)\n", "    def test_fowlkes_mallows_score_empty():\n        # Test fowlkes_mallows_score with empty arrays\n        score = fowlkes_mallows_score([], [])\n        assert_equal(score, 1.)\n"], "sample_489": ["    def test_update_conflicts_multiple_unique_fields(self):\n        self._test_update_conflicts(unique_fields=[\"number\", \"rank\"])\n", "    def test_update_conflicts_multiple_unique_fields(self):\n        self._test_update_conflicts(unique_fields=[\"number\", \"rank\"])\n", "    def test_update_conflicts_multiple_unique_fields(self):\n        self._test_update_conflicts(unique_fields=[\"number\", \"rank\"])\n", "    def test_update_conflicts_unique_fields_multiple(self):\n        self._test_update_conflicts(unique_fields=[\"number\", \"rank\"])\n", "    def test_update_conflicts_multiple_unique_fields(self):\n        self._test_update_conflicts(unique_fields=[\"number\", \"rank\"])\n"], "sample_771": ["compilation error", "    def test_power_transformer_sparse_matrix(self):\n        # Test PowerTransformer on sparse matrices\n        X = sparse.csr_matrix(X_2d)\n\n        for standardize in [True, False]:\n            pt = PowerTransformer(method='box-cox', standardize=standardize)\n            X_trans = pt.fit_transform(X)\n            assert sparse.isspmatrix(X_trans), \"Returned data should be sparse\"\n            X_trans_dense = X_trans.toarray()\n\n            X_expected = pt.fit_transform(X.toarray())\n            assert_almost_equal(X_expected, X_trans_dense)\n            assert_almost_equal(pt.inverse_transform(X_trans).toarray(),\n                               X.toarray())\n", "compilation error", "def test_power_transformer_sparse(method):\n    # test PowerTransformer with sparse matrix input\n    rng = np.random.RandomState(0)\n    X = sparse.csr_matrix(rng.randn(10, 5))\n\n    pt = PowerTransformer(method=method)\n    X_trans = pt.fit_transform(X)\n    assert isinstance(X_trans, sparse.csr_matrix)\n", "    def test_power_transformer_sparse_input(self):\n        # Check that PowerTransformer works with sparse input\n        X = sparse.csr_matrix(X_2d)\n        pt = PowerTransformer()\n        X_trans = pt.fit_transform(X)\n        assert sparse.isspmatrix_csr(X_trans)\n\n"], "sample_982": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_68": ["    def test_cleanse_setting_nested_dict(self):\n        data = {'a': 'test', 'b': {'c': 'password', 'd': 'test'}}\n        cleaned_data = cleanse_setting('b', data)\n        self.assertEqual(cleaned_data, {'a': 'test', 'b': {'c': CLEANSED_SUBSTITUTE, 'd': 'test'}})\n", "    def test_cleanse_setting_nested(self):\n        settings = {\n            'DATABASE': {\n                'PASSWORD': 'secret',\n            },\n            'API_KEY': 'private_key',\n        }\n        cleansed_settings = cleanse_settings(settings)\n        self.assertEqual(cleansed_settings['DATABASE']['PASSWORD'], CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleansed_settings['API_KEY'], CLEANSED_SUBSTITUTE)\n", "    def test_cleanse_setting_list_values(self):\n        self.assertEqual(cleanse_setting('SECRET_KEYS', ['key1', 'key2', 'super_secret']),\n                         ['key1', 'key2', CLEANSED_SUBSTITUTE])\n", "    def test_cleanse_setting_nested_dict(self):\n        data = {'DATABASE_SETTINGS': {'PASSWORD': 'super_secret', 'OTHER': 'value'}, 'OTHER': 'value'}\n        expected = {'DATABASE_SETTINGS': {'PASSWORD': CLEANSED_SUBSTITUTE, 'OTHER': 'value'}, 'OTHER': 'value'}\n        self.assertEqual(cleanse_setting_dict(data), expected)\n", "    def test_cleanse_setting_nested(self):\n        settings = {'API_KEYS': {'key1': 'secret1', 'key2': 'secret2'}}\n        cleansed_settings = cleanse_settings(settings)\n        self.assertEqual(cleansed_settings['API_KEYS']['key1'], CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleansed_settings['API_KEYS']['key2'], CLEANSED_SUBSTITUTE)\n"], "sample_357": ["    def test_adding_field_to_existing_model(self):\n        before = [\n            ModelState('app', 'book', [\n                ('id', models.AutoField(primary_key=True)),\n            ]),\n        ]\n        after = [\n            ModelState('app', 'book', [\n                ('id', models.AutoField(primary_key=True)),\n                ('title', models.CharField(max_length=200)),\n            ]),\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['AddField'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='title', model_name='book')\n", "    def test_two_operations_same_model(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.AddField('Person', 'age', models.IntegerField()),\n                migrations.AlterField('Person', 'name', models.CharField(max_length=100)),\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'add_age_to_person')\n", "    def test_operation_with_custom_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Person', fields=[], name='custom_person_name'),\n            ]\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'custom_person_name')\n", "    def test_operation_add_field_with_default(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.AddField(\n                    model_name='person',\n                    name='age',\n                    field=models.IntegerField(default=0),\n                ),\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'add_age_to_person')\n\n", "    def test_squashed_migration_suggestions(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Person', fields=[]),\n                migrations.CreateModel('Animal', fields=[]),\n            ]\n        migration = Migration('0003_squashed_0001_initial_0002_animal', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'person_animal')\n\n"], "sample_95": ["    def test_vary_on_headers(self):\n        @vary_on_headers('Accept-Language', 'User-Agent')\n            return HttpResponse()\n        r = a_view(HttpRequest())\n        self.assertEqual(r['Vary'], 'Accept-Language, User-Agent')\n", "    def test_cache_control_decorator_arguments(self):\n        @cache_control(private=True, max_age=60)\n            return HttpResponse()\n        r = a_view(HttpRequest())\n        self.assertEqual(r['Cache-Control'], 'private, max-age=60')\n", "    def test_method_decorator_with_kwargs_in_decorator(self):\n                    return func(*args, **kwargs)\n                wrapper.arg1 = arg1\n                wrapper.arg2 = arg2\n                return wrapper\n            return decorator\n\n        class Test:\n            @method_decorator(my_decorator('value1', arg2='value2'), name='method')\n                return True\n\n        test = Test()\n        self.assertEqual(test.method.arg1, 'value1')\n        self.assertEqual(test.method.arg2, 'value2')\n\n\n\n", "    def test_cache_control_inheritance(self):\n        @cache_control(no_cache=True)\n            return HttpResponse()\n        r = a_view(HttpRequest())\n        self.assertEqual(\n            set(r['Cache-Control'].split(', ')),\n            {'no-cache'},\n        )\n\n", "    def test_vary_on_headers(self):\n            return HttpResponse(\"OK\")\n        my_view_varied = vary_on_headers('Accept-Language')(my_view)\n        request = HttpRequest()\n        request.META['HTTP_ACCEPT_LANGUAGE'] = 'en-US'\n        response = my_view_varied(request)\n        self.assertIn('Vary', response['Vary'])\n        self.assertEqual(response['Vary'], 'Accept-Language')\n"], "sample_687": ["def test_caplog_captures_from_setup_and_teardown(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logging.getLogger(__name__).info(\"setup_module\")\n\n            logging.getLogger(__name__).info(\"teardown_module\")\n\n            logging.getLogger(__name__).info(\"test_something\")\n            assert 'setup_module' in caplog.text\n            assert 'teardown_module' not in caplog.text\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\"*setup_module*\", \"*test_something*\", \"*teardown_module*\"]\n    )\n    assert result.ret == 0\n\n", "def test_log_level_in_class(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n\n        class TestClass:\n                logging.getLogger(__name__).setLevel(logging.DEBUG)\n                logging.debug(\"This should be logged\")\n                assert \"This should be logged\" in caplog.text\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    assert result.ret == 0\n", "def test_log_capture_handler_level_is_restored(caplog, testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import pytest\n\n            caplog.set_level(logging.INFO)\n            logger = logging.getLogger(__name__)\n            logger.info(\"Initial log message\")\n            assert caplog.records[0].levelname == \"INFO\"\n\n            with caplog.at_level(logging.DEBUG):\n                logger.debug(\"Log message at DEBUG level\")\n                assert caplog.records[1].levelname == \"DEBUG\"\n\n            logger.info(\"Log message after DEBUG context\")\n            assert caplog.records[2].levelname == \"INFO\"\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=1)\n\n\n\n", "def test_log_capture_fixture_with_different_logger_names(testdir, caplog):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n\n            logger1 = logging.getLogger('logger1')\n            logger2 = logging.getLogger('logger2')\n\n            logger1.info(\"This log comes from logger1\")\n            logger2.info(\"This log comes from logger2\")\n\n            assert 'logger1' in caplog.text\n            assert 'logger2' in caplog.text\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    assert result.ret == 0\n", "def test_caplog_with_different_log_levels(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n\n            logger1 = logging.getLogger('logger1')\n            logger2 = logging.getLogger('logger2')\n\n            caplog.set_level(logging.INFO, logger=logger1.name)\n            caplog.set_level(logging.DEBUG, logger=logger2.name)\n\n            logger1.warning(\"logger1 warning\")\n            logger1.info(\"logger1 info\")\n            logger2.debug(\"logger2 debug\")\n            logger2.info(\"logger2 info\")\n\n            assert \"logger1 warning\" in caplog.text\n            assert \"logger1 info\" in caplog.text\n            assert \"logger2 debug\" in caplog.text\n            assert \"logger2 info\" in caplog.text\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    assert result.ret == 0\n"], "sample_850": ["compilation error", "    def test_nystroem_sparse_input():\n        rnd = np.random.RandomState(0)\n        X = rnd.uniform(size=(10, 4))\n        X_sparse = csr_matrix(X)\n        for kern in kernel_metrics():\n            if kern == 'precomputed':\n                continue\n            nystroem = Nystroem(kernel=kern, n_components=5, random_state=rnd)\n            X_transformed_dense = nystroem.fit_transform(X)\n            X_transformed_sparse = nystroem.fit_transform(X_sparse)\n            assert_array_almost_equal(X_transformed_dense, X_transformed_sparse)\n", "    def test_nystroem_precomputed_kernel():\n        rnd = np.random.RandomState(42)\n        X = rnd.uniform(size=(10, 4))\n\n        # Precomputed kernel\n        K = rbf_kernel(X)\n        nystroem = Nystroem(kernel=\"precomputed\", n_components=5)\n\n        # Test if fitting with precomputed kernel works\n        nystroem.fit(K)\n        X_transformed = nystroem.transform(K)\n        assert X_transformed.shape == (K.shape[0], 5)\n", "compilation error", "    def test_nystroem_incorrect_kernel():\n        X = np.array([[1, 2], [3, 4]])\n        nystroem = Nystroem(n_components=2, kernel='not_a_kernel')\n\n        with pytest.raises(ValueError):\n             nystroem.fit(X)\n"], "sample_301": ["    def test_notify_file_changed_handles_missing_files(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.nonexistent_file]):\n            self.reloader.notify_file_changed(self.nonexistent_file)\n", "    def test_watches_across_directories(self, mocked_modules, notify_mock):\n        inner_dir = self.tempdir / 'inner'\n        inner_dir.mkdir()\n        inner_file = self.ensure_file(inner_dir / 'test.py')\n        self.reloader.watch_dir(self.tempdir, '**/*.py')\n        with self.tick_twice():\n            self.increment_mtime(inner_file)\n        self.assertEqual(notify_mock.call_count, 1)\n        self.assertCountEqual(notify_mock.call_args[0], [inner_file])\n", "    def test_should_stop_returns_true_when_stop_flag_is_set(self):\n        self.reloader.stop()\n        self.assertTrue(self.reloader.should_stop())\n\n", "    def test_should_stop_returns_true_when_interrupted(self):\n        with mock.patch('threading.Event') as mocked_event:\n            mocked_event.is_set.return_value = True\n            self.assertTrue(self.reloader.should_stop())\n", "    def test_notify_file_changed_calls_ensure_echo_on(self):\n        with mock.patch('django.utils.autoreload.ensure_echo_on') as mocked_echo:\n            self.reloader.notify_file_changed(self.existing_file)\n            mocked_echo.assert_called_once_with()\n"], "sample_1020": ["    def test_Sum():\n        assert mcode(Sum(x**2, (x, 1, 5))) == \"Hold[Sum[x^2, {x, 1, 5}]]\"\n", "def test_Sum():\n    assert mcode(Sum(x*y, (x, 1, 10), (y, 2, 5))) == \\\n        \"Hold[Sum[x*y, {x, 1, 10}, {y, 2, 5}]]\"\n", "def test_Sum():\n    assert mcode(Sum(x**3, (x, 1, 10))) == \"Hold[Sum[x^3, {x, 1, 10}]]\"\n    assert mcode(Sum(x**2*y, (x, 0, 10), (y, -5, 5))) == \\\n        \"Hold[Sum[x^2*y, {x, 0, 10}, {y, -5, 5}]]\"\n", "def test_Sum():\n    assert mcode(Sum(x*y, (x, 1, 10))) == \"Hold[Sum[x*y, {x, 1, 10}]]\"\n    assert mcode(Sum(x**2, (x, 1, n))) == \"Hold[Sum[x^2, {x, 1, n}]]\"\n\n", "def test_Sum():\n    assert mcode(Sum(x**2, (x, 1, 10))) == \"Hold[Sum[x^2, {x, 1, 10}]]\" \n\n"], "sample_356": ["    def test_autodetector_dependency_order(self):\n        # Test that the Autodetector orders dependencies correctly based on the \n        # dependency graph.\n\n        with patch('django.db.migrations.questioner.MigrationQuestioner.ask_not_null_addition',\n                side_effect=AssertionError(\"Should not have prompted for not null addition\")):\n            app_label = 'testapp'\n\n            # Create some initial models\n            state_before = [\n                ModelState(app_label, 'Author', [\n                    ('id', models.AutoField(primary_key=True)),\n                ]),\n                ModelState(app_label, 'Book', [\n                    ('id', models.AutoField(primary_key=True)),\n                    ('author', models.ForeignKey('testapp.Author', models.CASCADE)),\n                ]),\n            ]\n\n            # Create a model with a ForeignKey to Book\n            state_after = [\n                ModelState(app_label, 'Review', [\n                    ('id', models.AutoField(primary_key=True)),\n                    ('book', models.ForeignKey('testapp.Book', models.CASCADE)),\n                ]),\n            ]\n\n            changes = self.get_changes(state_before, state_after)\n\n            # Assert dependencies are ordered correctly\n            self.assertNumberMigrations(changes, app_label, 1)\n            self.assertMigrationDependencies(changes, app_label, 0, [('testapp', 'auto_1')])\n", "    def test_operation_with_custom_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Person', fields=[]),\n                migrations.AlterModelOptions(\n                    name='Person', options={'custom_name': 'SuperPerson'}\n                ),\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'person_supersuperperson')\n", "    def test_remove_model_with_mti(self):\n        class Parent(models.Model):\n            name = models.CharField(max_length=200)\n\n        class Child(Parent):\n            extra_field = models.CharField(max_length=200)\n\n        before = [\n            ModelState('app', 'Parent', [\n                ('name', models.CharField(max_length=200)),\n            ]),\n            ModelState('app', 'Child', [\n                ('extra_field', models.CharField(max_length=200)),\n            ], bases=('app.Parent',)),\n        ]\n        after = [\n            ModelState('app', 'Parent', [\n                ('name', models.CharField(max_length=200)),\n            ]),\n        ]\n\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['DeleteModel'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='Child')\n", "    def test_operation_with_no_field_name(self):\n        class Migration(migrations.Migration):\n            operations = [migrations.AddField('Person', 'name', models.CharField(max_length=200))]\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'add_field_to_person')\n         \n", "    def test_operation_with_custom_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    'Person',\n                    fields=[],\n                    options={'name': 'custom_person'},\n                ),\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'custom_person')\n"], "sample_1182": ["compilation error", "compilation error", "compilation error", "compilation error", "def test_issue_21940():\n    from sympy.functions.elementary.trigonometric import cos\n    from sympy import Function, I\n\n    f = Function('f')\n    expr = cos(x + I*f(x))\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.cos(x + 1j*f(x))'\n"], "sample_258": ["    def test_receiver_dispatch_uid(self):\n            return val\n\n            return val\n\n        a_signal.connect(receiver_1, dispatch_uid=\"uid\")\n        a_signal.connect(receiver_2, dispatch_uid=\"uid2\")\n        result = a_signal.send(sender=self, val=\"test\")\n        self.assertEqual(len(result), 2)\n        a_signal.disconnect(dispatch_uid=\"uid\")\n        result = a_signal.send(sender=self, val=\"test\")\n        self.assertEqual(len(result), 1)\n", "    def test_receiver_with_dispatch_uid(self):\n        self.state = False\n\n        @receiver(a_signal, dispatch_uid=\"test_uid\")\n            self.state = val\n\n        a_signal.send(sender=self, val=True)\n        self.assertFalse(self.state)\n\n        a_signal.send(sender=self, val=True, dispatch_uid=\"test_uid\")\n        self.assertTrue(self.state)\n", "    def test_receiver_disconnect(self):\n        @receiver(a_signal)\n            self.state = val\n        self.state = False\n        a_signal.connect(f)\n        a_signal.send(sender=self, val=True)\n        self.assertTrue(self.state)\n        a_signal.disconnect(f)\n        a_signal.send(sender=self, val=True)\n        self.assertTrue(self.state)\n", "    def test_receiver_disconnect(self):\n        @receiver(a_signal)\n            pass\n        self.assertTrue(a_signal.has_listeners())\n        del f\n        garbage_collect()\n        self.assertFalse(a_signal.has_listeners())\n", "    def test_receiver_disconnect(self):\n        self.state = False\n        @receiver(a_signal)\n            self.state = True\n        with self.assertLogs('django.dispatch', 'DEBUG') as cm:\n            a_signal.disconnect(f)\n        self.assertFalse(self.state)\n"], "sample_197": ["    def test_depth_zero(self):\n        with self.assertRaises(ValueError):\n            timesince(self.t, depth=0)\n\n", "    def test_zero_depth(self):\n        with self.assertRaises(ValueError):\n            timesince(self.t, depth=0)\n        with self.assertRaises(ValueError):\n            timeuntil(self.t, depth=0)\n\n", "    def test_reversed(self):\n        self.assertEqual(timesince(self.t, self.t + self.oneday, reversed=True), '1\\xa0day')\n        self.assertEqual(timeuntil(self.t, self.t + self.oneday), '1\\xa0day')\n", "    def test_timesince_depth_zero(self):\n        with self.assertRaises(ValueError):\n            timesince(self.t, depth=0)\n        with self.assertRaises(ValueError):\n            timeuntil(self.t, depth=0)\n", "    def test_time_strings_customization(self):\n        time_strings = {\n            'year': ngettext_lazy('%d year ago', '%d years ago'),\n            'month': ngettext_lazy('%d month ago', '%d months ago'),\n            'week': ngettext_lazy('%d week ago', '%d weeks ago'),\n            'day': ngettext_lazy('%d day ago', '%d days ago'),\n            'hour': ngettext_lazy('%d hour ago', '%d hours ago'),\n            'minute': ngettext_lazy('%d minute ago', '%d minutes ago'),\n        }\n        t = self.t + self.oneyear + self.onemonth + self.oneweek + self.oneday + self.onehour\n        self.assertEqual(timesince(self.t, t, time_strings=time_strings), '1\\xa0year ago, 1\\xa0month ago, 1\\xa0week ago, 1\\xa0day ago, 1\\xa0hour ago')\n        self.assertEqual(timeuntil(t, self.t, time_strings=time_strings), '1\\xa0year ago, 1\\xa0month ago, 1\\xa0week ago, 1\\xa0day ago, 1\\xa0hour ago')\n\n"], "sample_364": ["    def test_resolve_empty_value_means_no_match(self):\n        @DynamicConverter.register_to_python\n            if not value:\n                raise ValueError()\n            return value\n        with self.assertRaises(Resolver404):\n            resolve('/dynamic/') \n", "    def test_include_with_app_name(self):\n        match = resolve('/namespaced/base64/aGVsbG8=/subpatterns/d29ybGQ=/')\n        self.assertEqual(match.url_name, 'subpattern-base64')\n        self.assertEqual(match.app_name, 'namespaced-base64')\n        self.assertEqual(match.kwargs, included_kwargs)\n", "    def test_reverse_type_error_propagates(self):\n        @DynamicConverter.register_to_url\n            raise TypeError('This type error propagates.')\n        with self.assertRaisesMessage(TypeError, 'This type error propagates.'):\n            reverse('dynamic', kwargs={'value': object()}) \n", "    def test_reverse_type_error_propagates(self):\n        @DynamicConverter.register_to_url\n            raise TypeError('This type error propagates.')\n        with self.assertRaisesMessage(TypeError, 'This type error propagates.'):\n            reverse('dynamic', kwargs={'value': object()})\n", "    def test_path_converters_are_called_once(self):\n        calls = {'int': 0, 'str': 0, 'path': 0, 'slug': 0, 'uuid': 0}\n\n        class CountingConverter:\n                self.converter_name = converter_name\n\n                calls[self.converter_name] += 1\n                return value\n\n                calls[self.converter_name] += 1\n                return value\n\n        DynamicConverter.register_to_python(CountingConverter('int'))\n        DynamicConverter.register_to_python(CountingConverter('str'))\n        DynamicConverter.register_to_python(CountingConverter('path'))\n        DynamicConverter.register_to_python(CountingConverter('slug'))\n        DynamicConverter.register_to_python(CountingConverter('uuid'))\n\n        DynamicConverter.register_to_url(CountingConverter('int'))\n        DynamicConverter.register_to_url(CountingConverter('str'))\n        DynamicConverter.register_to_url(CountingConverter('path'))\n        DynamicConverter.register_to_url(CountingConverter('slug'))\n        DynamicConverter.register_to_url(CountingConverter('uuid'))\n\n        resolve('/int/123/str/abc/path/with%20spaces/slug/this-is-a-slug/uuid/39da9369-838e-4750-91a5-f7805cd82839/')\n\n        for converter_name, count in calls.items():\n            self.assertEqual(count, 2, f'Converter {converter_name} was called {count} times.')\n"], "sample_366": ["    def test_iso8601_duration(self):\n        test_values = (\n            ('P1DT15H30M', timedelta(days=1, hours=15, minutes=30)),\n            ('P1DT15H30M30S', timedelta(days=1, hours=15, minutes=30, seconds=30)),\n            ('P1DT15H30M30.123S', timedelta(days=1, hours=15, minutes=30, seconds=30, milliseconds=123)),\n            (\n                'P1DT15H30M30.123456S',\n                timedelta(days=1, hours=15, minutes=30, seconds=30, microseconds=123456),\n            ),\n            ('PT15H30M', timedelta(hours=15, minutes=30)),\n            ('PT30.123S', timedelta(seconds=30.123)),\n            ('-P1D', timedelta(days=-1)),\n            ('-P1DT15H30M30S', timedelta(days=-1, hours=-15, minutes=-30, seconds=-30)),\n            ('P-1D', timedelta(days=-1)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n", "    def test_iso_8601_duration(self):\n        test_values = (\n            ('P4DT12H30M', timedelta(days=4, hours=12, minutes=30)),\n            ('PT12H30M', timedelta(hours=12, minutes=30)),\n            ('PT30M', timedelta(minutes=30)),\n            ('P1D', timedelta(days=1)),\n            ('P-1D', timedelta(days=-1)),\n            ('P1DT12H30M15S', timedelta(days=1, hours=12, minutes=30, seconds=15)),\n            ('P1DT12H30M15.5S', timedelta(days=1, hours=12, minutes=30, seconds=15, microseconds=500000)),\n            ('P0.5D', timedelta(hours=12)),\n            ('PT0.5H', timedelta(minutes=30)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n", "    def test_invalid_iso8601(self):\n        invalid_inputs = (\n            'P1D2H',\n            'PT1H2M',\n            'PT1H2M30S.123456789',\n            'P1Y2M3DT4H5M6S',\n            'P1D2H3M',\n            'PT1H2M30S.1234567890',\n        )\n        for value in invalid_inputs:\n            with self.subTest(value=value):\n                self.assertIsNone(parse_duration(value))\n", "    def test_timezone_offset(self):\n        self.assertEqual(parse_datetime('2012-04-23T10:20:30+02:30'), \n                        datetime(2012, 4, 23, 10, 20, 30, tzinfo=get_fixed_timezone(150)))\n        self.assertEqual(parse_datetime('2012-04-23T10:20:30-02:30'), \n                        datetime(2012, 4, 23, 10, 20, 30, tzinfo=get_fixed_timezone(-150)))\n        self.assertEqual(parse_datetime('2012-04-23T10:20:30Z'), \n                        datetime(2012, 4, 23, 10, 20, 30, tzinfo=get_fixed_timezone(0)))\n", "    def test_iso8601_duration(self):\n        test_values = (\n            ('P1DT2H30M45S', timedelta(days=1, hours=2, minutes=30, seconds=45)),\n            ('PT2H30M45S', timedelta(hours=2, minutes=30, seconds=45)),\n            ('PT23H59M59S', timedelta(hours=23, minutes=59, seconds=59)),\n            ('P2DT', timedelta(days=2)),\n            ('P1D', timedelta(days=1)),\n            ('-P1DT2H30M45S', timedelta(days=-1, hours=-2, minutes=-30, seconds=-45)),\n            ('-P2DT', timedelta(days=-2)),\n            ('P1DT2H30M45,123S', timedelta(days=1, hours=2, minutes=30, seconds=45, microseconds=123000)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n\n"], "sample_1035": ["compilation error", "compilation error", "compilation error", "compilation error", "def test_apply_grover():\n    nqubits = 2\n    v = OracleGate(nqubits, return_one_on_one)\n    assert apply_grover(nqubits, v) == IntQubit(1, nqubits=nqubits)\n"], "sample_671": ["    def test_importorskip_with_reason(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            pytest.importorskip(\"nonexistent_module\", reason=\"need this for something\")\n\n                pass\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines(\n            [\n                \"SKIPPED [nonexistent_module] test_importorskip_with_reason.py:4: need this for something\",\n            ]\n        )\n", "    def test_importorskip_skip_message(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n                pytest.importorskip(\"doesnotexist\", reason=\"Testing skip reason\")\n                assert False\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines(\n            [\n                \"*SKIP*\",\n                \"*Testing skip reason*\",\n            ]\n        )\n\n", "    def test_importorskip_existing(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n            pytest.importorskip(\"os\")\n\n                assert True\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        assert result.ret == 0\n        result.stdout.no_fnmatch_line(\"*SKIP*\")\n", "    def test_importorskip_module_found(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n            import os\n\n            pytest.importorskip(\"os\")\n\n                assert os.path.exists('/')\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines([\"*1 passed*\"])\n", "    def test_importorskip_with_optional_reason(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n                pytest.importorskip('optional_module', reason='This module is optional')\n                # rest of the test code\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines(\n            [\n                \"*SKIPPED*test_importorskip*\",\n                \"*reason: This module is optional*\",\n            ]\n        )\n"], "sample_952": ["    def test_getdoc_override_method():\n        class Foo:\n                \"\"\"Foo docstring\"\"\"\n                pass\n\n        class Bar(Foo):\n                \"\"\"Bar docstring\"\"\"\n                pass\n\n        assert inspect.getdoc(Bar.meth, getattr, False, Bar, \"meth\") == Bar.meth.__doc__\n", "        def meth(cls):\n            pass\n", "    def test_is_signature_from_str_valid():\n", "compilation error", "def test_is_builtin_class_method_inherited():\n    class MyInt(int):\n            pass\n\n    assert inspect.ismethod(MyInt.my_method) is True\n    assert inspect.isfunction(MyInt.my_method) is True\n    assert inspect.isbuiltin(MyInt.my_method) is False\n"], "sample_953": ["def test_templatedir(tempdir):\n    sourcedir = tempdir / 'mytemplates'\n    sourcedir.mkdir()\n    \n    answers = {\n        'Root path': tempdir,\n        'Project name': 'Sphinx Test',\n        'Author name': 'Georg Brandl',\n        'Project version': '0.1',\n    }\n    qs.term_input = mock_input(answers)\n    d = {}\n    qs.ask_user(d)\n    qs.generate(d, templatedir=sourcedir)\n\n    conffile = tempdir / 'conf.py'\n    assert conffile.isfile()\n    ns = {}\n    exec(conffile.read_text(), ns)\n    assert ns['project'] == 'Sphinx Test'\n", "def test_templatedir_argument(tempdir):\n    answers = {\n        'Root path': tempdir,\n        'Project name': 'Test Project',\n        'Author name': 'John Doe',\n        'Project version': '1.0',\n    }\n    template_dir = path.join(tempdir, 'templates')\n    os.makedirs(template_dir)\n    qs.term_input = mock_input(answers)\n    d = {}\n    qs.ask_user(d)\n    qs.generate(d, templatedir=template_dir)\n\n    conffile = tempdir / 'conf.py'\n    assert conffile.isfile()\n    ns = {}\n    exec(conffile.read_text(), ns)\n    assert ns['project'] == 'Test Project'\n", "def test_quickstart_dot_in_project_name(tempdir):\n    answers = {\n        'Root path': tempdir,\n        'Project name': 'My.Project',\n        'Author name': 'Georg Brandl',\n        'Project version': '0.1',\n    }\n    qs.term_input = mock_input(answers)\n    d = {}\n    qs.ask_user(d)\n    qs.generate(d)\n\n    assert (tempdir / 'source' / 'conf.py').isfile()\n    assert (tempdir / 'source' / 'My.Project').isfile()\n", "def test_quickstart_sep(tempdir):\n    answers = {\n        'Root path': tempdir,\n        'Separate source and build': 'y',\n        'Project name': 'Sphinx Test',\n        'Author name': 'Georg Brandl',\n        'Project version': '0.1',\n    }\n    qs.term_input = mock_input(answers)\n    d = {}\n    qs.ask_user(d)\n    qs.generate(d)\n\n    assert (tempdir / 'source' / 'conf.py').isfile()\n    assert (tempdir / 'build').isdir()\n\n", "def test_exits_when_existing_confpy_and_no_overwrite(monkeypatch, tmpdir):\n    # The code detects existing conf.py with path.isfile() \n    # so we mock it as True with pytest's monkeypatch\n        return True\n    monkeypatch.setattr(path, 'isfile', mock_isfile)\n\n    qs.term_input = mock_input({\n        'Please enter a new root path (or just Enter to exit)': ''\n    })\n    d = {}\n    with pytest.raises(SystemExit):\n        qs.ask_user(d, overwrite=False)\n"], "sample_1077": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_216": ["    def test_unique_together_changes(self):\n        \"\"\"\n        Check unique_together changes are correctly detected.\n        \"\"\"\n        before = ModelState('app', 'MyModel', [\n            ('id', models.AutoField(primary_key=True)),\n            ('name', models.CharField(max_length=100)),\n            ('other_field', models.CharField(max_length=100)),\n        ])\n        after = ModelState('app', 'MyModel', [\n            ('id', models.AutoField(primary_key=True)),\n            ('name', models.CharField(max_length=100)),\n            ('other_field', models.CharField(max_length=100)),\n        ],\n            options={'unique_together': [('name', 'other_field')]}\n        )\n        changes = self.get_changes([before], [after])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['AlterModelOptions'])\n", "    def test_create_model_with_unique_together(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    'Person',\n                    fields=[\n                        ('id', models.AutoField(primary_key=True)),\n                        ('first_name', models.CharField(max_length=200)),\n                        ('last_name', models.CharField(max_length=200)),\n                    ],\n                    options={\n                        'unique_together': {('first_name', 'last_name')},\n                    },\n                ),\n            ]\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'person')\n\n", "    def test_delete_model_and_add_field(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.DeleteModel('Person'),\n                migrations.AddField('Animal', name='name', field=models.CharField(max_length=100)),\n            ]\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'delete_person_add_name_to_animal')\n", "    def test_add_unique_together_constraint(self):\n        \"\"\"\n        #24292 - Adding a unique_together constraint should create a migration\n        operation.\n        \"\"\"\n        before = [\n            ModelState('app', 'Product', [\n                ('id', models.AutoField(primary_key=True)),\n                ('name', models.CharField(max_length=200)),\n                ('description', models.TextField()),\n            ]),\n        ]\n        after = [\n            ModelState('app', 'Product', [\n                ('id', models.AutoField(primary_key=True)),\n                ('name', models.CharField(max_length=200)),\n                ('description', models.TextField()),\n            ],\n                options={\n                    'unique_together': [\n                        ('name', 'description'),\n                    ],\n                }),\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['AlterModelOptions'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='Product', options={'unique_together': [('name', 'description')]})\n\n", "    def test_delete_model_with_fk_dependent_on_it(self):\n        class Migration(migrations.Migration):\n            initial = True\n            dependencies = [\n                ('otherapp', '0001_initial'),\n            ]\n            operations = [\n                migrations.CreateModel(\n                    name='PetOwner',\n                    fields=[\n                        ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                        ('name', models.CharField(max_length=200)),\n                    ],\n                ),\n                migrations.CreateModel(\n                    name='Pet',\n                    fields=[\n                        ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                        ('name', models.CharField(max_length=200)),\n                        ('owner', models.ForeignKey(on_delete=models.CASCADE, to='testapp.PetOwner')),\n                    ],\n                ),\n            ]\n\n        migration = Migration('0001_initial', 'testapp')\n        self.assertEqual(migration.suggest_name(), 'pet_petowner')\n"], "sample_120": ["    def test_serialize_lambda_function_error(self):\n        with self.assertRaisesMessage(ValueError, 'Cannot serialize lambda function'):\n            self.serialize_round_trip(lambda x: x + 1)\n", "    def test_serialize_lazy_proxy_object(self):\n        lazy_proxy = models.LazyProxy(models.CharField)\n        self.assertSerializedEqual(lazy_proxy)\n", "    def test_serialize_custom_model_class(self):\n        class CustomModel(models.Model):\n            pass\n\n        string, imports = MigrationWriter.serialize(CustomModel)\n        self.assertEqual(string, 'migrations.test_writer.CustomModel')\n        self.assertEqual(imports, {'from migrations.test_writer import CustomModel'})\n", "    def test_serialize_function_with_closure(self):\n            x = 5\n\n                return x\n            return inner_function\n\n        func = outer_function()\n        result = self.serialize_round_trip(func())\n        self.assertEqual(result, 5)\n", "    def test_serialize_complex_serializer(self):\n        class MyClass:\n                self.value = value\n\n        class MyClassSerializer(BaseSerializer):\n                return 'MyClass(%r)' % self.value.value, {}\n\n        my_instance = MyClass('hello world')\n        serializer = MyClassSerializer(my_instance)\n        self.assertEqual(self.serialize_round_trip(serializer), ('MyClass(\\'hello world\\')', {}))\n"], "sample_869": ["def test_balanced_accuracy_score_weights():\n    y_true = [0, 1, 0, 1, 1]\n    y_pred = [0, 0, 1, 1, 1]\n    weights = [0.25, 0.75, 0.25, 0.75, 1]\n    balanced_score = balanced_accuracy_score(y_true, y_pred, sample_weight=weights)\n    assert balanced_score == pytest.approx(0.625) \n", "compilation error", "compilation error", "compilation error", "def test_balanced_accuracy_score_multiclass():\n    true_labels = [0, 1, 2, 0, 1, 2, 0, 1, 2]\n    pred_labels = [0, 1, 1, 0, 2, 2, 0, 0, 1]\n    assert balanced_accuracy_score(true_labels, pred_labels) == pytest.approx(0.3333)\n"], "sample_647": ["def test_unformatted_warning(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            pytest.warn(pytest.UnformattedWarning[pytest.PytestWarning](\n                pytest.PytestWarning,\n                \"Something bad happened in {module} at {lineno}\"\n            ))\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"E       pytest.PytestWarning: Something bad happened in <module> at 3\",\n        ]\n    )\n\n", "    def test_warn_explicit_for(pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n        import pytest\n            pass\n        \"\"\"\n        )\n        pytester.runpytest()\n        warnings_raised = pytester.captured_warnings()\n        assert len(warnings_raised) == 0\n\n        pytester.makepyfile(\n            \"\"\"\n        import pytest\n            pass\n        pytest.warn_explicit_for(test_function, pytest.PytestWarning(\"some warning\"))\n        \"\"\"\n        )\n        result = pytester.runpytest()\n        result.stdout.fnmatch_lines([\"W       pytest.PytestWarning: some warning\"])\n        warnings_raised = pytester.captured_warnings()\n        assert len(warnings_raised) == 1\n        assert warnings_raised[0].category == pytest.PytestWarning\n\n", "    def test_unformatted_warning(pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n\n                pytest.UnformattedWarning(\n                    pytest.PytestWarning,\n                    \"some message {placeholder}\"\n                ).format(placeholder=\"value\")\n        \"\"\"\n        )\n        result = pytester.runpytest()\n        result.stdout.fnmatch_lines(\n            [\"E       pytest.PytestWarning: some message value\"]\n        )\n", "    def test_unformatted_warning(pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n\n                pytest.warn(pytest.UnformattedWarning(\n                    pytest.PytestWarning,\n                    \"some warning with {param}\"\n                ), param=\"value\")\n        \"\"\"\n        )\n        result = pytester.runpytest()\n        result.stdout.fnmatch_lines([\"E       pytest.PytestWarning: some warning with value\"])\n\n", "def test_warning_types_messages(warning_class, warning_message):\n    assert warning_class.__doc__ == warning_message\n"], "sample_1126": ["def test_Dagger_with_IdentityOperator():\n    A = Operator('A')\n    assert Dagger(A * IdentityOperator()) == Dagger(A)\n", "def test_dagger_of_dagger():\n    O = Operator('O')\n    assert Dagger(Dagger(O)) == O\n", "    def test_Dagger_of_Dagger():\n        O = Operator('O')\n        assert Dagger(Dagger(O)) == O\n", "def test_Dagger_of_Dagger():\n    A = Operator('A')\n    assert Dagger(Dagger(A)) == A\n", "    def test_Dagger_of_Dagger():\n        O = Operator('O')\n        assert Dagger(Dagger(O)) == O\n"], "sample_242": ["    def test_year_lookup_bounds(self):\n        lookup = YearExact(Value(DateTimeField()), Value(2023))\n        with mock.patch('django.db.backends.utils.DatabaseOperations.year_lookup_bounds_for_datetime_field') as mock_bounds:\n            mock_bounds.return_value = (datetime(2023, 1, 1, 0, 0, 0), datetime(2024, 1, 1, 0, 0, 0))\n            lookup.year_lookup_bounds(mock.MagicMock(), 2023)\n            mock_bounds.assert_called_once_with(2023)\n\n", "    def test_year_lookups(self):\n        lookup = YearLookup(Value(DateTimeField()), Value(2023))\n        with mock.patch(\"django.db.models.lookups.YearLookup.year_lookup_bounds\") as mock_bounds:\n            mock_bounds.return_value = (datetime(2023, 1, 1), datetime(2024, 1, 1))\n            lookup.as_sql(None, None)\n            mock_bounds.assert_called_once_with(None, 2023)\n", "    def test_year_lookup_bounds(self):\n        class MockConnection:\n                return (f\"start_{year}\", f\"finish_{year}\")\n\n                return (f\"start_{year}_datetime\", f\"finish_{year}_datetime\")\n\n        year_lookup = YearLookup(Value(DateTimeField()), Value(2023))\n        start, finish = year_lookup.year_lookup_bounds(MockConnection(), 2023)\n        self.assertEqual(start, \"start_2023_datetime\")\n        self.assertEqual(finish, \"finish_2023_datetime\")\n\n        year_lookup = YearLookup(Value(DateTimeField()), Value(2024))\n        start, finish = year_lookup.year_lookup_bounds(MockConnection(), 2024)\n        self.assertEqual(start, \"start_2024_datetime\")\n        self.assertEqual(finish, \"finish_2024_datetime\")\n", "    def test_year_lookup_bounds(self):\n        lookup = YearExact(Value(DateTimeField()), Value(2023))\n        with mock.patch('django.db.backends.utils.DatabaseOperations.year_lookup_bounds_for_datetime_field') as mock_bounds:\n            start, finish = lookup.year_lookup_bounds(mock.MagicMock(), 2023)\n            mock_bounds.assert_called_once_with(2023)\n            self.assertEqual(start, \n                     datetime(2023, 1, 1, 0, 0, 0))\n            self.assertEqual(finish,\n                     datetime(2024, 1, 1, 0, 0, 0))\n\n", "    def test_year_lookup_bounds(self):\n        lookup = YearLookup(Value(DateTimeField()), Value(2023))\n        with mock.patch('django.db.models.lookups.connection') as mock_connection:\n            mock_connection.ops.year_lookup_bounds_for_datetime_field.return_value = (\n                'start',\n                'finish',\n            )\n            bounds = lookup.year_lookup_bounds(mock_connection, 2023)\n            self.assertEqual(bounds, ('start', 'finish'))\n"], "sample_106": ["    def test_cache_middleware_delete(self):\n        request = self.factory.get('/view/')\n\n        # Put the request through the request middleware\n        response = hello_world_view(request, '1')\n        response = self.middleware.process_response(request, response)\n        \n        # Delete the cached response\n        self.middleware.delete(request)\n\n        # Repeating the request should result in a cache miss\n        result = self.middleware.process_request(request)\n        self.assertIsNone(result)\n", "    def test_cache_control_no_cache_not_cached(self):\n        view_with_no_cache = cache_page(3)(cache_control(no_cache=True)(hello_world_view))\n        request = self.factory.get('/view/')\n        response = view_with_no_cache(request, '1')\n        self.assertEqual(response.content, b'Hello World 1')\n        response = view_with_no_cache(request, '2')\n        self.assertEqual(response.content, b'Hello World 2') \n", "    def test_cache_disabled(self):\n        with override_settings(USE_CACHE=False):\n            self.assertIsNone(caches.get('default'))\n", "    def test_cache_middleware_not_cached_when_status_code_is_404(self):\n        view = mock.Mock(return_value=HttpResponseNotFound('My 404'))\n        view = cache_page(2)(view)\n        request = self.factory.get('/view/')\n        response = view(request)\n\n        self.assertEqual(view.call_count, 1)\n        \n\n\n\n", "    def test_cache_control_private_explicitly_set(self):\n        \"\"\"Responses with 'Cache-Control: private' explicitly set are not cached.\"\"\"\n        view_with_private_cache = cache_page(3)(\n            cache_control(private=True)(hello_world_view)\n        )\n        request = self.factory.get('/view/')\n        response = view_with_private_cache(request, '1')\n        self.assertEqual(response.content, b'Hello World 1')\n        response = view_with_private_cache(request, '2')\n        self.assertEqual(response.content, b'Hello World 2')\n"], "sample_61": ["    def test_ascii_validator(self):\n        valid_usernames = ['joe', 'Rene', 'BIGBIRD']\n        invalid_usernames = [\n            \"o'connell\", \"\u0623\u062d\u0645\u062f\",\n            \"zerowidth\\u200Bspace\", \"nonbreaking\\u00A0space\",\n            \"en\\u2013dash\", 'trailingnewline\\u000A',\n        ]\n        v = validators.ASCIIUsernameValidator()\n        for valid in valid_usernames:\n            with self.subTest(valid=valid):\n                v(valid)\n        for invalid in invalid_usernames:\n            with self.subTest(invalid=invalid):\n                with self.assertRaises(ValidationError):\n                    v(invalid)\n\n", "    def test_ascii_validator(self):\n        valid_usernames = ['joe', 'Rene', 'BIGBIRD', 'Ahmed']\n        invalid_usernames = [\n            \"o'connell\", \"\u0639\u0628\u062f \u0627\u0644\",\n            \"zerowidth\\u200Bspace\", \"nonbreaking\\u00A0space\",\n            \"en\\u2013dash\", 'trailingnewline\\u000A',\n        ]\n        v = validators.ASCIIUsernameValidator()\n        for valid in valid_usernames:\n            with self.subTest(valid=valid):\n                v(valid)\n        for invalid in invalid_usernames:\n            with self.subTest(invalid=invalid):\n                with self.assertRaises(ValidationError):\n                    v(invalid)\n\n", "    def test_ascii_validator(self):\n        valid_usernames = ['joe', 'Rene', 'BIGBIRD']\n        invalid_usernames = [\n            \"o'connell\", '\u0623\u062d\u0645\u062f', \"zerowidth\\u200Bspace\", \"nonbreaking\\u00A0space\",\n            \"en\\u2013dash\", 'trailingnewline\\u000A',\n        ]\n        v = validators.ASCIIUsernameValidator()\n        for valid in valid_usernames:\n            with self.subTest(valid=valid):\n                v(valid)\n        for invalid in invalid_usernames:\n            with self.subTest(invalid=invalid):\n                with self.assertRaises(ValidationError):\n                    v(invalid)\n\n", "    def test_ascii_validator(self):\n        valid_usernames = ['joe', 'Rene', 'BIGBIRD']\n        invalid_usernames = [\n            \"o'connell\", \"\u0623\u062d\u0645\u062f\",\n            \"zerowidth\\u200Bspace\", \"nonbreaking\\u00A0space\",\n            \"en\\u2013dash\", 'trailingnewline\\u000A',\n        ]\n        v = validators.ASCIIUsernameValidator()\n        for valid in valid_usernames:\n            with self.subTest(valid=valid):\n                v(valid)\n        for invalid in invalid_usernames:\n            with self.subTest(invalid=invalid):\n                with self.assertRaises(ValidationError):\n                    v(invalid)\n", "    def test_ascii_validator(self):\n        valid_usernames = ['joe', 'john.doe', 'john_doe', 'john+doe']\n        invalid_usernames = [\n            'Ren\u00e9', '\u1d2e\u1d35\u1d33\u1d2e\u1d35\u1d3f\u1d30', '\u0623\u062d\u0645\u062f', \"o'connell\",\n            \"zerowidth\\u200Bspace\", \"nonbreaking\\u00A0space\",\n            \"en\\u2013dash\", 'trailingnewline\\u000A',\n        ]\n        v = validators.ASCIIUsernameValidator()\n        for valid in valid_usernames:\n            with self.subTest(valid=valid):\n                v(valid)\n        for invalid in invalid_usernames:\n            with self.subTest(invalid=invalid):\n                with self.assertRaises(ValidationError):\n                    v(invalid)\n"], "sample_508": ["    def test_artist_is_transformed():\n        fig, ax = plt.subplots()\n        patch = mpatches.Circle((0, 0), radius=1)\n        ax.add_patch(patch)\n        assert patch.is_transformed() is False\n        patch.draw(renderer=ax.figure.canvas.get_renderer())\n        assert patch.is_transformed() is True\n", "def test_text_alignment():\n    fig, ax = plt.subplots()\n    ax.text(0.5, 0.5, \"centered\", ha=\"center\", va=\"center\", fontsize=16)\n    ax.text(0.2, 0.2, \"left-top\", ha=\"left\", va=\"top\", fontsize=16)\n    ax.text(0.8, 0.8, \"right-bottom\", ha=\"right\", va=\"bottom\", fontsize=16)\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n", "def test_patch_transform():\n    fig, ax = plt.subplots()\n    ax.set_xlim(0, 10)\n    ax.set_ylim(0, 10)\n    rect = mpatches.Rectangle((1, 1), 3, 2, transform=ax.transData)\n    rect2 = mpatches.Rectangle((2, 2), 3, 2,  transform=mtransforms.Affine2D().rotate_deg(45))\n    ax.add_patch(rect)\n    ax.add_patch(rect2)\n\n", "    def test_remove_callbacks():\n        class MyArtist(martist.Artist):\n                self.callbacks = []\n                super().__init__()\n\n                self.callbacks.append(func)\n\n                self.callbacks.remove(oid)\n\n        art = MyArtist()\n        func1 = lambda x: None\n        func2 = lambda x: None\n\n        oid1 = art.add_callback(func1)\n        oid2 = art.add_callback(func2)\n        assert len(art.callbacks) == 2\n\n        art.remove_callback(oid1)\n        assert len(art.callbacks) == 1\n        assert func2 in art.callbacks\n", "def test_artist_inspector_get_valid_values_empty():\n    class TestArtist(martist.Artist):\n        pass\n\n    valid_values = martist.ArtistInspector(TestArtist).get_valid_values('f')\n    assert valid_values == 'unknown'\n"], "sample_690": ["    def test_importorskip_skipif(pytester):\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n            pytest.skip(\"reason\")\n            importorskip(\"doesnotexist\")\n\n                pass\n        \"\"\"\n        )\n        result = pytester.runpytest()\n        result.stdout.fnmatch_lines([\"*SKIP*reason*\"])\n", "    def test_importorskip_nested(pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n\n                pytest.importorskip(\"os\")\n                assert os.path.exists(\".\")\n        \"\"\"\n        )\n        result = pytester.runpytest()\n        result.assert_outcomes(passed=1)\n", "    def test_importorskip_reason(pytester: PyTester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n\n                pytest.importorskip('doesnotexist', reason='need it for testing')\n                assert False\n        \"\"\"\n        )\n        result = pytester.runpytest(\"-rs\")\n        result.stdout.fnmatch_lines(\n            [\"*SKIP*need it for testing*\", \"*1 skipped*\"]\n        )\n", "    def test_xfail_without_reason_and_runtime_error(pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.xfail\n                raise ValueError(\"boom\")\n        \"\"\"\n        )\n        result = pytester.runpytest(\"-rsxX\")\n        result.stdout.fnmatch_lines([\"*XFAIL*test_that_raises*\", \"*ValueError: boom*\"])\n", "    def test_importorskip_conditional(pytester):\n        pytester.makepyfile(\n            \"\"\"\n            import sys\n            import pytest\n\n            if sys.platform == 'win32':\n                pytest.skip(\"skipping on windows\")\n\n                pass\n        \"\"\"\n        )\n        result = pytester.runpytest()\n        assert result.ret == 0\n        result.stdout.fnmatch_lines([\"*SKIP*skipping on windows*\"])\n"], "sample_341": ["    def test_all_valid_with_empty_formsets(self):\n        data = {\n            'choices-TOTAL_FORMS': '0',\n            'choices-INITIAL_FORMS': '0',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset1, formset2)), True)\n", "    def test_all_valid_with_formsets_that_have_errors(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [{'votes': ['This field is required.']}, {}]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n", "    def test_empty_formset_is_valid(self):\n        data = {'form-TOTAL_FORMS': '0', 'form-INITIAL_FORMS': '0'}\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertTrue(formset.is_valid())\n", "    def test_empty_formset_errors(self):\n        formset = formset_factory(Choice, extra=0)()\n        self.assertEqual(formset.errors, [])\n", "    def test_all_valid_with_empty_formsets(self):\n        data = {}\n        ChoiceFormSet = formset_factory(Choice, extra=0)\n        formset1 = ChoiceFormSet(data)\n\n        self.assertIs(all_valid((formset1,)), True)\n"], "sample_932": ["compilation error", "        def __str__(self):\n            return f\"{self.name}:\\n\" \\\n                   f\"  Root classes: {self.classes}\\n\" \\\n                   f\"  Content classes: {self.content_classes}\"\n", "    def assert_consistent_classes(self, other_role):\n        \"\"\"Assert that the classes for this role and another role are the same.\"\"\"\n        assert self.classes == other_role.classes, (f'Classes for {self.name} and {other_role.name} differ: {self.classes} vs {other_role.classes}')\n        for tag in self.content_classes:\n            assert tag in other_role.content_classes, f'Tag {tag} not found in {other_role.name}'\n            assert self.content_classes[tag] == other_role.content_classes[tag], (f'Classes for {self.name} and {other_role.name} differ for tag {tag}: {self.content_classes[tag]} vs {other_role.content_classes[tag]}')\n", "    def test_role_classes():\n        roles = {\n            'class': [\"xref\", \"cpp-class\", \"class\"],\n            'func': [\"xref\", \"cpp-func\", \"function\"],\n            'var': [\"xref\", \"cpp-var\", \"variable\"],\n            'member': [\"xref\", \"cpp-member\", \"member\"],\n            'type': [\"xref\", \"cpp-type\", \"type\"],\n        }\n\n        for role, expected_classes in roles.items():\n            role_classes = RoleClasses(role, 'span', ['code', 'a'])\n\n            assert role_classes.classes == expected_classes\n\n            for tag, content_classes in role_classes.content_classes.items():\n                assert content_classes == expected_classes.union({'cpp-' + tag})\n", "    def test_xref_consistency(self):\n        role = 'func'\n        check = [\n            ('span', {'xref-func', 'ref'}),\n            ('span', {'xref-func', 'ref'}),\n            ('a', {'reference', 'xref-func', 'ref'}),\n            ('a', {'reference', 'xref-func', 'ref'}),\n        ]\n        for tag, expected_classes in check:\n            for i, actual_classes in enumerate(self.content_classes[tag]):\n                if actual_classes not in expected_classes:\n                    raise AssertionError(\n                        f'Unexpected class \"{actual_classes}\" in role \"{self.name}\" tag \"{tag}\", '\n                        f'expected \"{expected_classes}\" in {i}-th element '\n                    )\n"], "sample_12": ["    def test_angle_from_string(angle_str, expected):\n        assert Angle(angle_str) == expected\n", "    def test_angle_infinity(cls, input, expected_value):\n        q = cls(input)\n        assert np.all(q.value == expected_value)\n\n", "    def test_angle_to_string_precision(cls):\n        a = cls(1.23456789, u.deg)\n        assert a.to_string(precision=2) == '1d14m00s'\n        assert a.to_string(precision=5) == '1d14m00.000s'\n        assert a.to_string(precision=8) == '1d14m00.00000000s'\n", "    def test_angle_unit_conversion(angle_value):\n        angle = Angle(angle_value)\n        for unit in [u.deg, u.hourangle, u.rad]:\n            to_angle = angle.to(unit)\n            assert to_angle.unit == unit\n", "def test_latitude_limits_errors(value):\n      with pytest.raises(ValueError, match=\"Latitude must be within the range [-90, 90] degrees\"):\n          Latitude(value)\n"], "sample_1044": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_845": ["    def test_empty_vocabulary():\n        vect = CountVectorizer()\n        with pytest.raises(ValueError):\n            vect.fit([])\n", "compilation error", "    def test_vectorizer_empty_vocabulary(self):\n        # Test for empty vocabulary handling in CountVectorizer and TfidfVectorizer\n        vect = CountVectorizer()\n        with pytest.raises(ValueError):\n            vect.fit([])\n\n        vect = TfidfVectorizer()\n        with pytest.raises(ValueError):\n            vect.fit([])\n", "def test_empty_vocabulary(Estimator):\n    # test if vectorizer handles empty vocabulary gracefully\n    vec = Estimator()\n    with pytest.raises(ValueError):\n        vec.fit([])\n\n    # Check if fit_transform doesn't crash with empty input\n    X = vec.fit_transform([])\n    assert X.shape == (0, 0)\n", "    def test_vectorizer_empty_input(self):\n        # Test empty input list\n        vect = CountVectorizer()\n        with pytest.raises(ValueError, match='input cannot be empty'):\n            vect.fit_transform([])\n"], "sample_540": ["def test_save_count_no_length(anim):\n    frames = lambda: iter([])\n    with pytest.warns(UserWarning, match=\n                      r\"Can not start iterating the frames for the initial draw\"):\n\n        anim = animation.FuncAnimation(\n            **{**anim, 'frames': frames, 'save_count': 5}\n        )\n        anim._init_draw()\n\n", "def test_save_count_override_warnings_no_length(anim):\n    save_count = 5\n\n    with pytest.warns(UserWarning, match=\n                      f\"You passed in an explicit {save_count=} which is \"\n                      f\"being ignored in favor of {anim._save_count=}\"):\n        anim = animation.FuncAnimation(\n            **{**anim, 'save_count': save_count}\n        )\n    assert anim._save_count == anim._save_count\n\n\n", "def test_save_count_warning_no_length(anim):\n    save_count = 5\n    frames = iter(range(5))\n    match_target = (\n        f\"{frames=!r} which we can infer the length of, \"\n        \"did not pass an explicit *save_count* \"\n        f\"and passed {save_count=}.\"\n    )\n\n    with pytest.warns(UserWarning, match=re.escape(match_target)):\n        anim = animation.FuncAnimation(\n            **{**anim, 'frames': frames, 'save_count': save_count}\n        )\n\n    assert anim._save_count == save_count\n    anim._init_draw()\n", "def test_save_count_override_no_warnings(anim):\n    save_count = 5\n    frames = list(range(5))\n    anim = animation.FuncAnimation(\n        **{**anim, 'frames': frames, 'save_count': save_count}\n    )\n    assert anim._save_count == save_count\n    anim._init_draw()\n", "def test_init_func_bad_return(anim):\n        return 'string'\n\n    with pytest.raises(RuntimeError,\n                       match='The init_func must return a sequence of Artist objects.'):\n        anim = animation.FuncAnimation(**anim, init_func=bad_init)\n"], "sample_977": ["def test_Sum():\n    assert mcode(Sum(x**2, (x, 1, 10))) == \"Hold[Sum[x^2, {x, 1, 10}]]\"\n    assert mcode(Sum(x**n, (n, 0, oo))) == \"Hold[Sum[x^n, {n, 0, Infinity}]]\"\n", "    def test_Sum():\n        assert mcode(Sum(sin(x), (x, 0, 10))) == \"Hold[Sum[Sin[x], {x, 0, 10}]]\"\n        assert mcode(Sum(x**2, (x, 1, n))) == \"Hold[Sum[x^2, {x, 1, n}]]\"\n", "    def test_Sum():\n        assert mcode(Sum(x**n, (n, 1, 10))) == \"Hold[Sum[x^n, {n, 1, 10}]]\"\n", "def test_Sum():\n    assert mcode(Sum(x**n, (n, 1, 10))) == \"Hold[Sum[x^n, {n, 1, 10}]]\"\n    assert mcode(Sum(x*y**n/(n + 1), (n, 0, oo))) == \\\n        \"Hold[Sum[x*y^n/(n + 1), {n, 0, Infinity}]]\"\n", "def test_Sum():\n    assert mcode(Sum(x*y, (x, 1, 5), (y, 2, 10))) == \\\n        \"Hold[Sum[x*y, {x, 1, 5}, {y, 2, 10}]]\"\n"], "sample_712": ["compilation error", "compilation error", "def test_one_hot_encoder_handle_unknown_sparse():\n    X = np.array([[0, 2, 1], [1, 0, 3], [1, 0, 2]])\n    X2 = np.array([[4, 1, 1]])\n\n    # Test that one hot encoder raises error for unknown features\n    # present during transform.\n    oh = OneHotEncoder(handle_unknown='error', sparse=True)\n    assert_warns(FutureWarning, oh.fit, X)\n    assert_raises(ValueError, oh.transform, X2)\n\n    # Test the ignore option, ignores unknown features (giving all 0's)\n    oh = OneHotEncoder(handle_unknown='ignore', sparse=True)\n    oh.fit(X)\n    X2_passed = X2.copy()\n    assert_array_equal(\n        oh.transform(X2_passed).toarray(),\n        np.array([[0.,  0.,  0.,  0.,  1.,  0.,  0.]]))\n    # ensure transformed data was not modified in place\n    assert_allclose(X2, X2_passed)\n\n", "    def test_one_hot_encoder_categories_preserve_order(self, X, expected_categories):\n        enc = OneHotEncoder(categories='auto')\n        enc.fit(X)\n        for i, categories in enumerate(enc.categories_):\n            assert list(categories) == expected_categories[i]\n\n", "compilation error"], "sample_950": ["def test_py_exception(app):\n    text = \".. py:exception:: MyError\\n\"\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"exception \"],\n                [desc_name, \"MyError\"])],\n                [desc_content, ])]))\n    assert 'MyError' in domain.objects\n    assert domain.objects['MyError'] == ('index', 'MyError', 'exception', False)\n", "def test_py_param_type_escaped(app):\n    text = (\".. py:function:: example\\n\"\n            \"   :param str name: blah blah\\n\"\n            \"   :param int 'id': blah blah\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc, addnodes.index, desc))\n    assert_node(doctree[1][1][0][0], [nodes.paragraph, ([addnodes.literal_strong, \"name\"],\n                 \" (\",\n                 [pending_xref, addnodes.literal_emphasis, \"str\"],\n                 \")\",\n                 \" -- \",\n                 \"blah blah\")])\n    assert_node(doctree[1][1][0][1], [nodes.paragraph, ([addnodes.literal_strong, \"'id'\"],\n                 \" (\",\n                 [pending_xref, addnodes.literal_emphasis, \"int\"],\n                 \")\",\n                 \" -- \",\n                 \"blah blah\")])\n", "compilation error", "def test_pyattribute_alias(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"      :type: str\\n\"\n            \"      :alias: name\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'attr (Class attribute)', 'Class.attr', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_name, \"attr\"],\n                                                     [desc_annotation, \": str\"]),\n                                                     \n                                                     [desc_annotation, \" -- alias: \"],\n                                                     [desc_literal, \"name\"])],\n                                   [desc_content, ()]))\n    assert 'Class.attr' in domain.objects\n    assert domain.objects['Class.attr'] == ('index', 'Class.attr', 'attribute', False)\n", "def test_python_use_qualified_type_names(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"n\"><a class=\"reference internal\" href=\"#foo.Name\" title=\"foo.Name\">'\n            '<span class=\"pre\">foo.Name</span></a></span>' in content)\n    assert ('<span class=\"n\"><a class=\"reference internal\" href=\"#foo.Age\" title=\"foo.Age\">'\n            '<span class=\"pre\">foo.Age</span></a></span>' in content)\n"], "sample_445": ["    def test_timeuntil_future_date(self):\n        t = datetime.datetime(2024, 1, 1, 0, 0, 0)\n        now = datetime.datetime.now()\n        self.assertEqual(timeuntil(t, now), \"0\\xa0minutes\")\n\n", "    def test_timesince_with_tzinfo(self):\n        \"\"\"timesince should work with datetime objects with tzinfo.\"\"\"\n        now_tz = timezone.make_aware(self.t, timezone.get_default_timezone())\n        past_tz = timezone.make_aware(self.t - self.oneday, timezone.get_default_timezone())\n        self.assertEqual(timesince(past_tz, now_tz), \"1\\xa0day\")\n        self.assertEqual(timesince(now_tz, past_tz), \"0\\xa0minutes\")\n", "    def test_timeuntil_with_future_date(self):\n        \"\"\"\n        Test that timeuntil correctly handles future dates.\n        \"\"\"\n        future_date = datetime.date.today() + datetime.timedelta(days=7)\n        self.assertEqual(timeuntil(future_date), \"1\\xa0week\")\n\n", "    def test_timeuntil_with_tzinfo(self):\n        with override_settings(TIME_ZONE=\"America/Los_Angeles\"):\n            pacific_now = timezone.now()\n            utc_now = timezone.make_aware(pacific_now, timezone.get_default_timezone())\n            self.assertEqual(\n                timeuntil(utc_now, pacific_now), \"0\\xa0minutes\"\n            )\n", "    def test_zero_days(self):\n        self.assertEqual(timesince(self.t, self.t + self.oneday), \"1\\xa0day\")\n        self.assertEqual(timesince(self.t, self.t + self.oneday - datetime.timedelta(seconds=1)), \"1\\xa0day\")\n"], "sample_340": ["    def test_circular_dependencies(self):\n        loader = MigrationLoader(connection)\n            return loader.applied_migrations\n\n        with self.assertRaises(InconsistentMigrationHistory):\n            with patch.object(MigrationRecorder, 'applied_migrations',\n                            new=applied_migrations):\n                loader.build_graph()\n", "    def test_empty_migration_file(self):\n        \"\"\"MigrationLoader should handle empty migration files gracefully.\"\"\"\n        loader = MigrationLoader(connection)\n        self.assertEqual(len(loader.disk_migrations), 0)\n        \n", "    def test_loading_empty_file(self):\n        \"\"\"MigrationLoader should handle empty  migration files.\"\"\"\n        loader = MigrationLoader(connection)\n        self.assertIn(('migrations', '0001_initial'), loader.disk_migrations)\n", "    def test_missing_migration_file(self):\n        \"\"\"\n        Tests the behavior when a migration file is missing.\n        \"\"\"\n        test_migration_dir = os.path.join(\n            os.path.dirname(__file__), 'migrations', 'test_migrations'\n        )\n        missing_migration_file = os.path.join(test_migration_dir, '0003_missing.py')\n        os.remove(missing_migration_file)\n        loader = MigrationLoader(connection)\n        with self.assertRaises(IOError) as exc:\n            loader.load_disk()\n        self.assertEqual(str(exc.exception), f\"Migrations directory '{test_migration_dir}' is missing file '0003_missing.py'.\")\n", "    def test_loading_with_byte_compiled_files(self):\n        \"\"\"\n        MigrationLoader should handle migrations compiled to bytecode (.pyc) files.\n        \"\"\"\n        with self.temporary_migration_module(\n                module='migrations.test_migrations') as migration_dir:\n            # Compile .py files to .pyc files and delete .py files.\n\n            compileall.compile_dir(migration_dir, force=True, quiet=1, legacy=True)\n            for name in os.listdir(migration_dir):\n                if name.endswith('.py'):\n                    os.remove(os.path.join(migration_dir, name))\n            loader = MigrationLoader(connection)\n            self.assertIn(('migrations', '0001_initial'), loader.disk_migrations)\n\n            # Modify a .pyc file to simulate a corrupted file.\n            pyc_file = os.path.join(migration_dir, '0001_initial.pyc')\n            with open(pyc_file, 'wb') as f:\n                f.write(b'\\x00' * 100)\n\n            # Ensure that the corrupted file is skipped without raising an error.\n            loader = MigrationLoader(connection)\n            self.assertEqual(len(loader.disk_migrations), 1)\n"], "sample_1069": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_412": ["    def test_urlize_nofollow(self):\n        tests = (\n            (\n                \"Search for google.com/?q=!\",\n                'Search for <a href=\"http://google.com/?q=\" rel=\"nofollow\">google.com/?q=</a>!',\n            ),\n            (\n                lazystr(\"Search for google.com/?q=!\"),\n                'Search for <a href=\"http://google.com/?q=\" rel=\"nofollow\">google.com/?q=</a>!',\n            ),\n        )\n        for value, output in tests:\n            with self.subTest(value=value):\n                self.assertEqual(urlize(value, nofollow=True), output)\n\n", "    def test_urlize_nofollow(self):\n        tests = (\n            (\n                \"Search for google.com/?q=!\",\n                'Search for <a href=\"http://google.com/?q=\" rel=\"nofollow\">google.com/?q=</a>!',\n            ),\n            (\n                lazystr(\"Search for google.com/?q=!\"),\n                'Search for <a href=\"http://google.com/?q=\" rel=\"nofollow\">google.com/?q=</a>!',\n            ),\n        )\n        for value, output in tests:\n            with self.subTest(value=value):\n                self.assertEqual(urlize(value, nofollow=True), output)\n\n", "    def test_urlize_nofollow(self):\n        tests = (\n            (\n                \"Search for google.com/?q=!\",\n                'Search for <a href=\"http://google.com/?q=\" rel=\"nofollow\">google.com/?q=</a>!',\n            ),\n            (\n                \"Search for google.com/ and see.\",\n                'Search for <a href=\"http://google.com/\" rel=\"nofollow\">google.com/</a> and see.',\n            ),\n            (\n                \"Search for www.google.com and see.\",\n                'Search for <a href=\"http://www.google.com\" rel=\"nofollow\">www.google.com</a> and see.',\n            ),\n            (\n                lazystr(\"Search for google.com/?q=!\"),\n                'Search for <a href=\"http://google.com/?q=\" rel=\"nofollow\">google.com/?q=</a>!',\n            ),\n        )\n        for value, output in tests:\n            with self.subTest(value=value):\n                self.assertEqual(urlize(value, nofollow=True), output)\n", "    def test_urlize_nofollow(self):\n        self.assertEqual(\n            urlize(\"http://example.com/nofollow\", nofollow=True),\n            '<a href=\"http://example.com/nofollow\" rel=\"nofollow\">example.com/nofollow</a>',\n        )\n", "    def test_urlize_nofollow(self):\n        self.assertEqual(\n            urlize(\"http://example.com\", nofollow=True),\n            '<a href=\"http://example.com\" rel=\"nofollow\">http://example.com</a>',\n        )\n"], "sample_324": ["    def test_csrf_cookie_http_only_false(self):\n        req = self._get_GET_no_csrf_cookie_request()\n        mw = CsrfViewMiddleware(token_view)\n        mw.process_view(req, token_view, (), {})\n        resp = mw(req)\n        self.assertFalse(resp.cookies.get('csrfcookie').get('HttpOnly'))\n", "    def test_bare_secret_accepted_and_replaced_with_session(self):\n        \"\"\"\n        The csrf token is reset from a bare secret when using sessions.\n        \"\"\"\n        req = self._get_POST_bare_secret_csrf_cookie_request()\n        mw = CsrfViewMiddleware(token_view)\n        mw.process_request(req)\n        resp = mw.process_view(req, token_view, (), {})\n        self.assertIsNone(resp)\n        resp = mw(req)\n        self.assertIn(settings.CSRF_COOKIE_NAME, resp.cookies)\n        csrf_cookie = resp.cookies[settings.CSRF_COOKIE_NAME]\n        self.assertEqual(len(csrf_cookie.value), CSRF_TOKEN_LENGTH)\n        self._check_token_present(resp, csrf_id=csrf_cookie.value)\n", "    def test_bare_secret_accepted_and_replaced_in_session(self):\n        \"\"\"\n        The csrf token is reset from a bare secret in the session.\n        \"\"\"\n        req = self._get_POST_bare_secret_csrf_cookie_request()\n        mw = CsrfViewMiddleware(token_view)\n        mw.process_request(req)\n        resp = mw.process_view(req, token_view, (), {})\n        self.assertIsNone(resp)\n        resp = mw(req)\n        self.assertIn(settings.CSRF_COOKIE_NAME, resp.cookies, \"Cookie was not reset from bare secret\")\n        csrf_cookie = resp.cookies[settings.CSRF_COOKIE_NAME]\n        self.assertEqual(len(csrf_cookie.value), CSRF_TOKEN_LENGTH)\n        self._check_token_present(resp, csrf_id=csrf_cookie.value)\n\n", "    def test_csrf_cookie_age_zero(self):\n        \"\"\"\n        CSRF cookie age 0 results in a session-based cookie.\n        \"\"\"\n        req = self._get_GET_no_csrf_cookie_request()\n\n        MAX_AGE = 0\n        with self.settings(CSRF_COOKIE_NAME='csrfcookie',\n                           CSRF_COOKIE_DOMAIN='.example.com',\n                           CSRF_COOKIE_AGE=MAX_AGE,\n                           CSRF_COOKIE_PATH='/test/',\n                           CSRF_COOKIE_SECURE=True,\n                           CSRF_COOKIE_HTTPONLY=True):\n            # token_view calls get_token() indirectly\n            mw = CsrfViewMiddleware(token_view)\n            mw.process_view(req, token_view, (), {})\n            resp = mw(req)\n            max_age = resp.cookies.get('csrfcookie').get('max-age')\n            self.assertEqual(max_age, '')\n", "    def test_bare_secret_accepted_and_replaced_session(self):\n        \"\"\"\n        The csrf token is reset from a bare secret when CSRF_USE_SESSIONS is True.\n        \"\"\"\n        req = self._get_POST_bare_secret_csrf_cookie_request()\n        mw = CsrfViewMiddleware(token_view)\n        mw.process_request(req)\n        resp = mw.process_view(req, token_view, (), {})\n        self.assertIsNone(resp)\n        resp = mw(req)\n        self.assertIn(settings.CSRF_COOKIE_NAME, resp.cookies, \"Cookie was not reset from bare secret\")\n        csrf_cookie = resp.cookies[settings.CSRF_COOKIE_NAME]\n        self.assertEqual(len(csrf_cookie.value), CSRF_TOKEN_LENGTH)\n        self._check_token_present(resp, csrf_id=csrf_cookie.value)\n"], "sample_71": ["def test_grouping_with_non_uniform_size(self):\n    self.assertEqual(nformat(123456789, '.', grouping=[3, 2, 0],\n                            thousand_sep=','), '12,34,56789')\n", "    def test_grouping_sequence(self):\n        self.assertEqual(nformat(1234567890, '.', grouping=(3, 2, 0)), '1,234,567,890')\n", "    def test_grouping_with_sequence(self):\n        self.assertEqual(nformat(1234567890, '.', grouping=[3, 2, 0]), '1,234,567,890')\n        self.assertEqual(nformat(123456789012, '.', grouping=[2, 3, 4, 0]), '12,34,567,89012')\n        self.assertEqual(nformat(123456789012345, '.', grouping=[1, 1, 2, 3, 0]), '1,2,34,56,789,012,345')\n", "    def test_zero_decimal(self):\n        self.assertEqual(nformat(0, '.', decimal_pos=2), '0.00')\n        self.assertEqual(nformat(Decimal('0'), '.', decimal_pos=2), '0.00')\n", "    def test_format_non_numeric_input(self):\n        with self.assertRaises(TypeError):\n            nformat('abc', '.')\n"], "sample_281": ["    def test_autocomplete_filtering(self):\n        q1 = Question.objects.create(question='Where are we going?')\n        q2 = Question.objects.create(question='Where do we go from here?')\n        q3 = Question.objects.create(question='What is happening?')\n\n        self.selenium.get(self.live_server_url + reverse('autocomplete_admin:admin_views_answer_add'))\n        elem = self.selenium.find_element_by_css_selector('.select2-selection')\n        elem.click()\n        search = self.selenium.find_element_by_css_selector('.select2-search__field')\n        search.send_keys('Where')\n        with self.select2_ajax_wait():\n            results = self.selenium.find_elements_by_css_selector('.select2-results__option')\n            self.assertEqual(len(results), 2)\n\n            \n", "    def test_custom_filtering_in_autocomplete_view(self):\n        book = Book.objects.create(title='Test Book')\n        author = Author.objects.create(name='Test Author')\n        authorship = Authorship.objects.create(book=book, author=author)\n        self.selenium.get(self.live_server_url + reverse('autocomplete_admin:admin_views_authorship_add'))\n        search = self.selenium.find_element_by_css_selector('.select2-search__field')\n        search.send_keys('Test')\n        search.send_keys(Keys.RETURN)\n        select = Select(self.selenium.find_element_by_id('id_author'))\n        self.assertEqual(select.first_selected_option.get_attribute('value'), str(author.pk))\n\n", "    def test_select_with_filter(self):\n        from selenium.webdriver.common.keys import Keys\n        from selenium.webdriver.support.ui import Select\n        self.selenium.get(self.live_server_url + reverse('autocomplete_admin:admin_views_answer_add'))\n        elem = self.selenium.find_element_by_css_selector('.select2-selection')\n        elem.click()  # Open the autocomplete dropdown.\n        results = self.selenium.find_element_by_css_selector('.select2-results')\n        self.assertTrue(results.is_displayed())\n        option = self.selenium.find_element_by_css_selector('.select2-results__option')\n        self.assertEqual(option.text, 'No results found')\n        elem.click()  # Close the autocomplete dropdown.\n        Question.objects.create(question='Who am I?', is_active=True)\n        Question.objects.create(question='What is the meaning of life?', is_active=False)\n        Question.objects.bulk_create(Question(question=str(i)) for i in range(PAGINATOR_SIZE + 10))\n        elem.click()  # Reopen the dropdown now that some objects exist.\n        result_container = self.selenium.find_element_by_css_selector('.select2-results')\n        self.assertTrue(result_container.is_displayed())\n        results = result_container.find_elements_by_css_selector('.select2-results__option')\n        self.assertEqual(len(results), PAGINATOR_SIZE + 2)\n        search = self.selenium.find_element_by_css_selector('.select2-search__field')\n        search.send_keys('Who')\n        search.send_keys(Keys.ARROW_DOWN)\n        search.send_keys(Keys.ENTER)\n        select = Select(self.selenium.find_element_by_id('id_question'))\n        self.assertEqual(select.first_selected_option.get_attribute('value'), '1')\n\n", "    def test_search_use_distinct_with_empty_results(self):\n        q1 = Question.objects.create(question='question 1')\n        q2 = Question.objects.create(question='question 2')\n        q2.related_questions.add(q1)\n        q3 = Question.objects.create(question='question 3')\n        q3.related_questions.add(q1)\n        request = self.factory.get(self.url, {'term': 'nonexistent', **self.opts})\n        request.user = self.superuser\n\n        class DistinctQuestionAdmin(QuestionAdmin):\n            search_fields = ['related_questions__question', 'question']\n\n        with model_admin(Question, DistinctQuestionAdmin):\n            response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n        self.assertEqual(response.status_code, 200)\n        data = json.loads(response.content.decode('utf-8'))\n        self.assertEqual(data, {\n            'results': [],\n            'pagination': {'more': False},\n        })\n", "    def test_autocomplete_limit_choices_to_custom_to_field_with_filter(self):\n        class QuestionAdminWithFilter(QuestionAdmin):\n            search_fields = ['question']\n                return qset.filter(author__name='Frida Kahlo')\n        \n        Author.objects.create(name='Frida Kahlo', gender=2, code=\"painter\", alive=False)\n        Author.objects.create(name='Another Author', gender=1, code=\"writer\", alive=True)\n        q1 = Question.objects.create(question='Is this a question?', author=Author.objects.get(name='Frida Kahlo'))\n        q2 = Question.objects.create(question='Another question', author=Author.objects.get(name='Another Author'))\n\n        with model_admin(Question, QuestionAdminWithFilter):\n            request = self.factory.get(self.url, {'term': '', **self.opts, 'field_name': 'question_with_to_field'})\n            request.user = self.superuser\n            response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n\n        data = json.loads(response.content.decode('utf-8'))\n        self.assertEqual(data['results'], [{'id': str(q1.uuid), 'text': q1.question}])\n"], "sample_603": ["def test_summarize_coord_multiindex_with_missing_level_name(multiindex):\n    multiindex = xr.Dataset({'x': (('x',), [1, 2])}, coords={'x': multiindex.x})\n    idx = multiindex.x.variable.to_index_variable()\n    formatted = fh._summarize_coord_multiindex(\"foo\", idx)\n    assert \"level_1\" not in formatted  \n    assert \"level_2\" not in formatted\n    assert \"MultiIndex\" in formatted\n    assert \"<span class='xr-has-index'>foo</span>\" in formatted\n", "def test_collapsible_section_with_n_items_zero(dataset):\n    formatted = fh.collapsible_section(\"Coordinates\", n_items=0)\n\n    assert \"title='Expand/collapse section'\" not in formatted\n    assert \"disabled\" in formatted\n    assert \"checked\" in formatted\n    \n", "def test_summarize_coord_multiindex_with_empty_levels(multiindex):\n    idx = multiindex.x.variable.to_index_variable()\n    new_coords = idx.copy()\n    new_coords[\"level_1\"] = new_coords[\"level_1\"].to_numpy(dtype=\"object\")\n    new_coords[\"level_1\"][:] = None\n    formatted = fh._summarize_coord_multiindex(\"foo\", new_coords)\n    assert \"(level_1, level_2)\" in formatted\n    assert \"MultiIndex\" in formatted\n    assert \"<span class='xr-has-index'>foo</span>\" in formatted\n", "def test_summarize_coord_with_dask_array(dask_dataarray):\n    formatted = fh.summarize_coord(\"x\", dask_dataarray)\n    assert \"dask.array\" in formatted\n", "    def test_array_repr_empty_array(self):\n        empty_array = xr.DataArray(np.array([]), dims=(\"time\",))\n        formatted = fh.array_repr(empty_array)\n        assert \"Dimensions: (time: 0)\" in formatted\n        assert \"Data:\" in formatted\n\n"], "sample_166": ["    def test_get_random_string_length(self):\n        self.assertEqual(len(get_random_string(length=32)), 32)\n", "    def test_get_random_string_length(self):\n        self.assertEqual(len(get_random_string(32)), 32)\n", "    def test_get_random_string_length(self):\n        self.assertEqual(len(get_random_string(length=32)), 32)\n        self.assertEqual(len(get_random_string(length=0)), 0)\n", "    def test_get_random_string_with_length(self):\n        self.assertEqual(len(get_random_string(20)), 20)\n", "    def test_get_random_string_with_length(self):\n        self.assertEqual(len(get_random_string(32)), 32)\n"], "sample_1004": ["compilation error", "compilation error", "def test_ConditionSet_contains():\n    C = ConditionSet\n    assert 1 in C(x, x < 2, S.Reals)\n    assert 3 in C(x, x < 4, {1, 2, 3})\n    assert 4 not in C(x, x < 4, {1, 2, 3})\n    assert pi in C(x, x < 2*pi, Interval(0, 2*pi))\n    assert 2*pi not in C(x, x < 2*pi, Interval(0, 2*pi))\n", "def test_ConditionSet_empty():\n    assert ConditionSet(x, Eq(x, 0), EmptySet()) == EmptySet()\n    assert ConditionSet(x, x > 0, EmptySet()) == EmptySet()\n    assert ConditionSet(x, x < 0, EmptySet()) == EmptySet()\n", "def test_ConditionSet_empty_base():\n\n    assert ConditionSet(x, x < 1, EmptySet()) == EmptySet()\n    assert ConditionSet(x, x > 1, EmptySet()) == EmptySet()\n    assert ConditionSet(x, x == 1, EmptySet()) == EmptySet()\n\n"], "sample_643": ["def test_colorized_reporter_ansi_codes(capsys):\n    output = StringIO()\n    linter = PyLinter()\n    linter.reporter = ColorizedTextReporter(output)\n    linter.set_option('colorized_output', True)\n    linter.open()\n    linter.set_current_module(\"test_module\")\n    linter.add_message(\"C0301\", line=1, args=(1, 2))\n    linter.add_message(\"W0101\", line=2, args=(3, 4))\n    linter.generate_reports()\n    captured = capsys.readouterr()\n    assert ANSI_RESET in captured.out\n\n", "def test_colorized_text_reporter_color_mapping(linter: PyLinter, capfd) -> None:\n    \"\"\"Test the color mapping in ColorizedTextReporter.\"\"\"\n\n    class CustomColorizedReporter(ColorizedTextReporter):\n            return MessageStyle(\"yellow\", (\"bold\",))\n\n    linter.reporter.set_output(sys.stdout)\n    linter.set_option(\"color\", True)\n    linter.reporter = CustomColorizedReporter(linter)\n\n    linter.add_message(\"C0301\", line=1, args=(1, 2))\n    output, _ = capfd.readouterr()\n    assert \"\\033[1;33m\" in output and \"\\033[0m\" in output  # Check for boldness and reset\n\n", "def test_colorized_output(linter: PyLinter, capsys):\n    output = StringIO()\n    linter.reporter.set_output(output)\n    linter.set_option(\"colorize-output\", True)\n    linter.set_option(\"output-format\", \"colorized\")\n    linter.open()\n    linter.set_current_module(\"0123\")\n    linter.add_message(\"C0301\", line=1, args=(1, 2))\n    linter.add_message(\"line-too-long\", line=2, args=(3, 4))\n    linter.generate_reports()\n    captured = capsys.readouterr()\n    assert captured.out.startswith(\"\\033\")\n    assert captured.out.endswith(\"\\033[0m\\n\")\n", "def test_multi_reporter_empty_output(tmp_path):\n    \"\"\"Test that MultiReporter handles empty reporters gracefully.\"\"\"\n    output_file = tmp_path / \"output.txt\"\n\n    class EmptyReporter(BaseReporter):\n            super().__init__(output)\n\n            pass\n\n            pass\n\n    multi_reporter = MultiReporter([EmptyReporter(output_file)], lambda: None)\n    linter = PyLinter()\n    linter.load_default_plugins()\n    linter.set_option(\"persistent\", False)\n    linter.reporter = multi_reporter\n    linter.open()\n    linter.check_single_file_item(FileItem(\"somemodule\", tmp_path / \"somemodule.py\", \"somemodule\"))\n    linter.generate_reports()\n    assert not output_file.exists()\n\n", "def test_multi_reporter_different_output_formats():\n    output_text = StringIO()\n    output_json = StringIO()\n    with redirect_stdout(output_text):\n        linter = PyLinter()\n        linter.load_default_plugins()\n        linter.set_option(\"persistent\", False)\n        linter.set_option(\"reports\", True)\n        linter.set_option(\"score\", True)\n        linter.set_option(\"output-format\", \"text,json:\" + output_json.name)\n        linter.open()\n        linter.check_single_file_item(FileItem(\"somemodule\", \"somemodule.py\", \"somemodule\"))\n        linter.add_message(\"line-too-long\", line=1, args=(1, 2))\n        linter.generate_reports()\n\n\n\n"], "sample_758": ["    def fit(self, X, y):\n            pass\n", "compilation error", "compilation error", "    def fit(self, X):\n        pass\n", "    def __len__(self):\n            return 10\n"], "sample_1098": ["compilation error", "    def test_appellf1_special_cases():\n        a, b1, b2, c, x, y = symbols('a b1 b2 c x y')\n        assert appellf1(1, 1, 1, 2, x, y) == (1 - x)*(1 - y)/(1 - x - y)\n        assert appellf1(1, 0, 0, 1, x, y) == 1/(1 - x - y)\n        assert appellf1(0, b1, b2, c, x, y) == 1\n", "compilation error", "compilation error", "compilation error"], "sample_746": ["compilation error", "compilation error", "compilation error", "def test_zero_division_in_log_loss():\n    # Test that log_loss handles cases where predicted probabilities are zero,\n    # preventing ZeroDivisionError.\n    y_true = [0, 1, 2]\n    y_pred = [[1, 0, 0], [0, 1, 0], [0, 0, 0]]\n    loss = log_loss(y_true, y_pred, eps=1e-15)\n    assert_almost_equal(loss, 0.0)\n", "compilation error"], "sample_1146": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_460": ["    def test_unknown_url_redirects_login_if_not_authenticated_with_append_slash_false(self):\n        unknown_url = \"/test_admin/admin11/unknown/\"\n        response = self.client.get(unknown_url)\n        self.assertRedirects(\n            response, \"%s?next=%s\" % (reverse(\"admin11:login\"), unknown_url)\n        )\n", "    def test_missing_slash_append_slash_true_script_name_query_string_without_final_catch_all_view(\n        self,", "    def test_non_staff_user_missing_slash_append_slash_true_without_final_catch_all_view(\n        self", "    def test_missing_slash_append_slash_true_script_name_without_final_catch_all_view(\n        self,", "    def test_missing_slash_append_slash_true_non_staff_user_without_final_catch_all_view(self):\n        user = User.objects.create_user(\n            username=\"user\",\n            password=\"secret\",\n            email=\"user@example.com\",\n            is_staff=False,\n        )\n        self.client.force_login(user)\n        known_url = reverse(\"admin10:admin_views_article_changelist\")\n        response = self.client.get(known_url[:-1])\n        self.assertEqual(response.status_code, 404)\n"], "sample_21": ["    def test_read_write_with_err_specs(tmp_path):\n        test_file = tmp_path / \"test.qdp\"\n        t1 = Table()\n        t1.add_column(Column(name=\"a\", data=[1, 2, 3]))\n        t1.add_column(Column(name=\"b\", data=[1.1, 2.2, 3.3]))\n        t1.add_column(\n            MaskedColumn(data=[4.0, np.nan, 3.0], name=\"c\", mask=[False, False, False])\n        )\n        t1.write(\n            test_file, format=\"ascii.qdp\", err_specs={\"terr\": [1], \"serr\": [2]}\n        )\n        t2 = Table.read(test_file, table_id=0, format=\"ascii.qdp\", names=[\"a\", \"b\", \"c\"])\n\n        assert np.allclose(t2[\"a\"], t1[\"a\"])\n        assert np.allclose(t2[\"b\"], t1[\"b\"])\n        assert np.allclose(t2[\"c\"], t1[\"c\"])\n        assert \"c_perr\" in t2.columns\n", "compilation error", "    def test_read_write_with_err_spec(tmp_path):\n        test_file = tmp_path / \"test.qdp\"\n        t1 = Table()\n        t1[\"a\"] = [1, 2, 3]\n        t1[\"b_err\"] = [0.1, 0.2, 0.3]\n        t1[\"b\"] = [4, 5, 6]\n        t1[\"c_perr\"] = [0.5, 0.6, 0.7]\n        t1[\"c_nerr\"] = [-0.5, -0.6, -0.7]\n        t1.write(test_file, format=\"ascii.qdp\", err_specs={\"terr\": [1], \"serr\": [3]})\n        t2 = Table.read(test_file, format=\"ascii.qdp\", table_id=0)\n        assert np.allclose(t1[\"a\"], t2[\"a\"])\n        assert np.allclose(t1[\"b\"], t2[\"b\"])\n        assert np.allclose(t1[\"b_err\"], t2[\"b_err\"])\n        assert np.allclose(t1[\"c_perr\"], t2[\"c_perr\"])\n        assert np.allclose(t1[\"c_nerr\"], t2[\"c_nerr\"])\n\n\n\n", "def test_read_write_err_col_names(tmp_path):\n    example_qdp = \"\"\"\n        ! Initial comment line 1\n        READ SERR 1 2\n        !a a(pos) a(neg) b c\n        53000.5   0.25  -0.5   1  1.5\n        54000.5   1.25  -1.5   2  2.5\n        NO NO NO NO NO\n        \"\"\"\n    test_file = tmp_path / \"test.qdp\"\n\n    with open(test_file, \"w\") as fp:\n        print(example_qdp, file=fp)\n\n    t = Table.read(test_file, format=\"ascii.qdp\", table_id=0, names=['a', 'b', 'c'])\n    assert t.colnames == ['a', 'a_perr', 'a_nerr', 'b', 'c']\n    assert t['a_perr'][0] == 0.25\n    assert t['a_nerr'][0] == -0.5\n\n\n\n", "compilation error"], "sample_548": ["compilation error", "    def test_colorbar_labelpad(self):\n        fig, ax = plt.subplots()\n        im = ax.imshow([[0, 1], [2, 3]])\n        cb = fig.colorbar(im, label='Colorbar Label')\n        cb.ax.set_ylabel('Y Label')\n        assert cb.ax.get_ylabel() == 'Y Label'\n        cb.set_label('New Colorbar Label', labelpad=20)\n        assert cb.ax.get_ylabel() == 'New Colorbar Label'\n        assert cb.ax.yaxis.labelpad == 20\n", "    def test_colorbar_extendfrac_vertical(self):\n        fig, ax = plt.subplots()\n        im = ax.imshow([[0, 1], [2, 3]])\n        cb = fig.colorbar(im, extend='both', extendfrac=(0.1, 0.2),\n                          orientation='vertical')\n        fig.canvas.draw()\n        # Check that the extend triangles are the correct size\n        assert cb.ax.patches[0].get_width() / cb.ax.get_xlim()[1] == 0.1\n        assert cb.ax.patches[1].get_width() / cb.ax.get_xlim()[1] == 0.2\n", "    def test_colorbar_labelpad(self):\n        fig, ax = plt.subplots()\n        im = ax.imshow([[0, 1], [2, 3]])\n        cb = fig.colorbar(im, label='My Label')\n        cb.ax.set_labelpad(20)\n        assert cb.ax.get_ylabel().get_window_extent(\n            fig.canvas.get_renderer()).x0 > cb.ax.get_position().x0 + 20\n", "def test_colorbar_ticks_format():\n    fig, ax = plt.subplots()\n    data = np.arange(0, 100).reshape(10, 10)\n    im = ax.imshow(data)\n    fig.colorbar(im, format='%.2e')\n"], "sample_577": ["    def test_legend_title(self, xy):\n        s = pd.Series([\"a\", \"b\", \"a\", \"c\"], name=\"s\")\n        p = Plot(**xy, color=s).add(MockMark(), title=\"My Legend\").plot()\n        legend, = p._figure.legends\n        assert legend.get_title().get_text() == \"My Legend\"\n", "    def test_legend_title(self, xy):\n\n        color = pd.Series([\"a\", \"b\", \"a\", \"c\"], name=\"color\")\n        p = Plot(**xy, color=color).add(MockMark(), legend=\"My Legend\").plot()\n        legend, = p._figure.legends\n        assert legend.get_title().get_text() == \"My Legend\" \n", "    def test_legend_hidden_when_legend_false(self, xy):\n\n        s = pd.Series([\"a\", \"b\", \"a\", \"c\"], name=\"s\")\n        p = Plot(**xy).add(MockMark(), color=s, legend=False).plot()\n        assert not p._legend_contents\n\n", "    def test_legend_title(self, xy):\n\n        s = pd.Series([\"a\", \"b\", \"a\", \"c\"], name=\"s\")\n        p = Plot(**xy, color=s).add(MockMark(), legend_title=\"My Title\").plot()\n        legend, = p._figure.legends\n        assert legend.get_title().get_text() == \"My Title\"\n", "    def test_legend_title_from_variable(self, xy):\n\n        s = pd.Series([\"a\", \"b\", \"a\", \"c\"], name=\"my_var\")\n        p = Plot(**xy, color=s).add(MockMark()).plot()\n        legend, = p._figure.legends\n\n        assert legend.get_title().get_text() == \"my_var\"\n"], "sample_195": ["    def test_raw_sql(self):\n        msg = 'This backend does not support raw SQL execution.'\n        with self.assertRaisesMessage(NotSupportedError, msg):\n            connection.ops.raw_sql(\"SELECT 1\")\n\n", "    def test_sql_flush_allow_cascade(self):\n        with transaction.atomic():\n            Author.objects.create(name='Author 1')\n            Book.objects.create(author=Author.objects.get(name='Author 1'), title='Book 1')\n\n        sql = connection.ops.sql_flush(no_style(), ['backends_author'], allow_cascade=True)\n        self.assertEqual(len(sql), 2)\n        self.assertIn(\"DELETE FROM backends_book;\", sql)\n        self.assertIn(\"DELETE FROM backends_author;\", sql)\n", "    def test_sql_flush_with_references(self):\n        with transaction.atomic():\n            Book.objects.create(title=\"Book1\", author=Author.objects.create(name=\"Author1\"))\n        references = connection.ops._references_graph('backends_book')\n        sql = connection.ops.sql_flush(no_style(), references)\n        self.assertEqual(len(sql), 2)\n        self.assertIn('DELETE FROM backends_author', sql[0])\n        self.assertIn('DELETE FROM backends_book', sql[1])\n", "    def test_sql_flush_with_foreign_key_references(self):\n        with transaction.atomic():\n            Author.objects.create(name='Author 1')\n            Book.objects.create(title='Book 1', author=Author.objects.get(name='Author 1'))\n        \n        # Simulate cascade delete\n        sql = connection.ops.sql_flush(no_style(), ['backends_author'], allow_cascade=True)\n        self.assertIn('DELETE FROM backends_book;', sql)\n\n", "    def test_adapted_jsonb_value(self):\n        value = {'key': 'value'}\n        with self.assertRaisesMessage(NotImplementedError, 'JSONField is not supported on this database backend.'):\n            self.ops.adapt_field_value(value, None)\n"], "sample_659": ["    def test_raises_chained_exception(self):\n        class InnerException(Exception):\n            pass\n\n        class OuterException(Exception):\n                super().__init__(*args)\n                self.inner = inner\n\n        with pytest.raises(OuterException) as excinfo:\n\n            raise OuterException(\n                \"Outer Exception\", inner=InnerException(\"Inner Exception\")\n            )\n\n        assert isinstance(excinfo.value, OuterException)\n\n        with pytest.raises(InnerException):\n            raise excinfo.value.inner\n\n", "    def test_raises_with_custom_exception_bases(self):\n        class MyError(Exception):\n            pass\n\n        class MyOtherError(Exception):\n            pass\n\n        class MyCustomError(MyError, MyOtherError):\n            pass\n\n        with pytest.raises(MyCustomError):\n            raise MyCustomError()\n\n        with pytest.raises((MyError, MyOtherError)):\n            raise MyCustomError()\n", "    def test_raises_with_raising_dunder_class_no_match(self):\n        \"\"\"Test current behavior with regard to exceptions via __class__ (#4284).\n\n        Ensure the match is ignored if an exception of the wrong type is raised\n        \"\"\"\n\n        class CrappyClass(Exception):\n            @property\n                assert False, \"via __class__\"\n\n        with pytest.raises(ValueError, match=\"some_pattern\"):\n            raise CrappyClass()\n", "    def test_raises_with_raises_after_fail(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n                assert False\n                with pytest.raises(ValueError):\n                    raise ValueError(\"This should not be skipped\")\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines([\"*1 failed*\"])\n", "    def test_raises_with_raising_exception_with_custom_name(self):\n        class MyException(Exception):\n                super().__init__(message)\n                self.custom_name = custom_name\n\n            raise MyException(\"Something went wrong\", \"CustomName\")\n\n        with pytest.raises(MyException, match=\"Something went wrong\"):\n            raising_function()\n\n"], "sample_945": ["def test_python_python_use_unqualified_type_names_no_module(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"n\"><a class=\"reference internal\" href=\"#Name\" title=\"Name\">'\n            '<span class=\"pre\">Name</span></a></span>' in content)\n    assert '<span class=\"n\"><span class=\"pre\">Age</span></span>' in content \n", "def test_typehints(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert '<span class=\"n\"><span class=\"pre\">int</span></span>' in content\n    assert '<span class=\"n\"><span class=\"pre\">str</span></span>' in content\n    assert '<span class=\"n\"><span class=\"pre\">List</span></span>[<span class=\"n\"><span class=\"pre\">str</span></span>]' in content\n", "compilation error", "def test_pyattribute_noindexentry(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"      :noindexentry:\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, [desc, (desc_signature, [desc_content, addnodes.index, desc])]))\n    assert_node(doctree[1][1][1], addnodes.index, entries=[])\n", "def test_py_exception(app):\n    text = \".. py:exception:: MyException\\n\"\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"exception \"],\n                                                     [desc_name, \"MyException\"])],\n                                  [desc_content, ()])]))\n    assert 'MyException' in domain.objects\n    assert domain.objects['MyException'] == ('index', 'MyException', 'exception', False)\n"], "sample_163": ["    def test_user_delete(self):\n        u = User.objects.get(username=\"testclient\")\n        original_count = User.objects.count()\n        response = self.client.post(\n            reverse(\"auth_test_admin:auth_user_delete\", args=(u.pk,))\n        )\n        self.assertRedirects(response, reverse(\"auth_test_admin:auth_user_changelist\"))\n        self.assertEqual(User.objects.count(), original_count - 1)\n", "    def test_user_changelist_add_user_permission(self):\n        staff_user = User.objects.create_user(\n            username=\"staffuser\", password=\"password\", is_staff=True\n        )\n        # Get the latest last_login value.\n        self.logout()\n        self.login(username=\"staffuser\", password=\"password\")\n        # Check if the add user button is present when a staff user\n        response = self.client.get(reverse(\"auth_test_admin:auth_user_changelist\"))\n\n        self.assertContains(response, '<a href=\".+add_user/\">', count=1)\n\n", "    def test_staff_user_change_password(self):\n        u = User.objects.get(email=\"staffmember@example.com\")\n        original_password = u.password\n        u.is_superuser = False\n        u.save()\n        u.user_permissions.add(get_perm(User, \"change_user\"))\n        password_change_url = reverse(\n            \"auth_test_admin:auth_user_password_change\", args=(u.pk,)\n        )\n        response = self.client.post(\n            password_change_url,\n            {\n                \"password1\": \"password1\",\n                \"password2\": \"password1\",\n            },\n        )\n        self.assertRedirects(response, reverse(\"auth_test_admin:auth_user_change\", args=(u.pk,)))\n        row = LogEntry.objects.latest(\"id\")\n        self.assertEqual(row.get_change_message(), \"Changed password.\")\n        u.refresh_from_db()\n        self.assertNotEqual(u.password, original_password)\n\n", "    def test_user_change_permissions_for_staff_user(self):\n        u = User.objects.get(email=\"staffmember@example.com\")\n        perm = get_perm(User, \"add_user\")\n        self.assertFalse(u.has_perm(perm))\n        data = self.get_user_data(u)\n        data[\"permissions\"] = [perm.pk]\n\n        response = self.client.post(\n            reverse(\"auth_test_admin:auth_user_change\", args=(u.pk,)), data\n        )\n        self.assertRedirects(response, reverse(\"auth_test_admin:auth_user_changelist\"))\n        row = LogEntry.objects.latest(\"id\")\n        self.assertEqual(row.get_change_message(), \"Changed permissions.\")\n        u.refresh_from_db()\n        self.assertTrue(u.has_perm(perm))\n", "    def test_user_change_password_no_permissions(self):\n        u = User.objects.get(username=\"testclient\")\n        u.is_superuser = False\n        u.save()\n        u.user_permissions.clear()\n        response = self.client.get(\n            reverse(\"auth_test_admin:auth_user_password_change\", args=(u.pk,))\n        )\n        self.assertEqual(response.status_code, 403)\n"], "sample_456": ["    def test_all_valid_with_errors(self):\n        data = {\n            \"choices-TOTAL_FORMS\": \"2\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"Zero\",\n            \"choices-0-votes\": \"\",  # <-- this field is required\n            \"choices-1-choice\": \"One\",\n            \"choices-1-votes\": \"1\",\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [{'votes': ['This field is required.']}, {}]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n", "    def test_all_valid_with_invalid_formset(self):\n        data = {\n            \"choices-TOTAL_FORMS\": \"2\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"Zero\",\n            \"choices-0-votes\": \"\",\n            \"choices-1-choice\": \"One\",\n            \"choices-1-votes\": \"1\",\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [{'votes': ['This field is required.']}, {}]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n", "    def test_all_valid_with_invalid_formset(self):\n        data = {\n            \"choices-TOTAL_FORMS\": \"2\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"Zero\",\n            \"choices-0-votes\": \"\",\n            \"choices-1-choice\": \"One\",\n            \"choices-1-votes\": \"1\",\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        self.assertIs(all_valid((formset1, formset2)), False)\n", "    def test_empty_formset_with_initial_data(self):\n        data = {}\n        formset = ChoiceFormSet(data, initial=[{'choice': 'test', 'votes': 1}])\n        self.assertEqual(formset.cleaned_data, [{'choice': 'test', 'votes': 1}])\n        self.assertTrue(formset.is_valid())\n", "    def test_all_valid_empty_formset(self):\n        formset = formset_factory(Choice, extra=0)()\n        self.assertTrue(all_valid((formset,)))\n"], "sample_109": ["    def test_render_options_language_translation(self):\n        \"\"\"Test that option values are correctly translated.\"\"\"\n        with translation.override('fr'):\n            beatles = Band.objects.create(name='The Beatles', style='rock')\n            form = AlbumForm(initial={'band': beatles.pk})\n            output = form['band'].field.widget.render('band', beatles.pk, {})\n            self.assertIn('The Beatles', output)\n", "    def test_render_options_with_language(self):\n        from django.utils.translation import activate\n\n        # Set the language to a code supported by select2 translations.\n        activate('es')\n\n        beatles = Band.objects.create(name='The Beatles', style='rock')\n        form = AlbumForm(initial={'band': beatles.pk})\n        output = form.as_table()\n        selected_option = '<option value=\"%s\" selected>The Beatles</option>' % beatles.pk\n        self.assertIn(selected_option, output)\n", "    def test_language_code(self):\n        with translation.override('es'):\n            form = AlbumForm()\n            output = form.as_table()\n            self.assertIn('select2/i18n/es.js', output)\n", "    def test_render_options_zh_hans(self):\n        beatles = Band.objects.create(name='The Beatles', style='rock')\n        who = Band.objects.create(name='The Who', style='rock')\n\n        form = AlbumForm(initial={'band': beatles.pk})\n        output = form.as_table()\n        selected_option = '<option value=\"%s\" selected>The Beatles</option>' % beatles.pk\n        option = '<option value=\"%s\">The Who</option>' % who.pk\n        self.assertIn(selected_option, output)\n        self.assertNotIn(option, output)\n", "    def test_language_code(self):\n        with translation.override('zh-hans'):\n            form = AlbumForm()\n            attrs = form['band'].field.widget.get_context(name='my_field', value=None, attrs={})['widget']['attrs']\n            self.assertEqual(attrs['data-ajax--url'], '/admin_widgets/band/autocomplete/')\n"], "sample_800": ["    def test_check_estimator_class_weight_balanced_linear_classifier():\n        # Check that check_class_weight_balanced_linear_classifier\n        # raises ValueError if estimator doesn't handle 'balanced' class_weight\n        print('Test check_estimator_class_weight_balanced_linear_classifier')\n\n        msg = (\n            \"Estimator BadBalancedWeightsClassifier does not support \"\n            r\"balanced class weights 'balanced'\"\n        )\n        assert_raises_regex(\n            ValueError,\n            msg,\n            check_class_weight_balanced_linear_classifier,\n            BadBalancedWeightsClassifier\n        )\n", "    def test_check_estimator_with_deprecated_parameter():\n        class EstimatorWithDeprecatedParameter(BaseEstimator):\n                super().__init__()\n                # simulate a deprecated parameter\n                self.deprecated_param = deprecated_param\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            check_estimator(EstimatorWithDeprecatedParameter)  \n", "    def test_check_estimator_balanced_classifier():\n        msg = (\"Estimator doesn't seem to correctly handle balanced \"\n               \"class weights\")\n        assert_raises_regex(AssertionError, msg, check_estimator,\n                            BadBalancedWeightsClassifier)\n", "def test_check_estimator_sample_weight_pandas_series():\n    # check that estimators accept pandas.Series as sample weight\n    try:\n        from pandas import Series  # noqa\n        check_estimator(NoSampleWeightPandasSeriesType())\n    except ImportError:\n        pass\n", "def test_check_estimator_class_weights_balanced():\n    # Tests that check_estimator correctly handles estimators that\n    # support balanced class weights\n    msg = \"Estimator does not handle 'balanced' class weight correctly\"\n    assert_raises_regex(AssertionError, msg, check_estimator,\n                        BadBalancedWeightsClassifier)\n"], "sample_637": ["    def test_regex_match(self) -> None:\n        code = \"\"\"a = 1\n                # TODO1\n                \"\"\"\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"fixme\", line=2, args=\"TODO1\", col_offset=17)\n        ):\n            self.checker.process_tokens(_tokenize_str(code))\n", "    def test_regex_codetag(self) -> None:\n        code = \"\"\"a = 1\n                # TODO\n                # FIXME\n                # BUG\n                \"\"\"\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"fixme\", line=2, args=\"TODO\", col_offset=17),\n            MessageTest(msg_id=\"fixme\", line=3, args=\"FIXME\", col_offset=17),\n        ):\n            self.checker.process_tokens(_tokenize_str(code))\n\n", "    def test_regex_note(self) -> None:\n        code = \"\"\"a = 1\n                # TODO_ME\n                \"\"\"\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"fixme\", line=2, args=\"TODO_ME\", col_offset=17)\n        ):\n            self.checker.process_tokens(_tokenize_str(code))\n\n", "    def test_regex_codetag(self) -> None:\n        code = \"\"\"a = 1\n                # BUG \n                \"\"\"\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"fixme\", line=2, args=\"BUG\", col_offset=17)\n        ):\n            self.checker.process_tokens(_tokenize_str(code))\n\n", "    def test_notes_rgx(self) -> None:\n        code = \"\"\"a = 1\n                # TODO: this is a test\n                \"\"\"\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"fixme\", line=2, args=\"TODO: this is a test\", col_offset=17)\n        ):\n            self.checker.process_tokens(_tokenize_str(code))\n"], "sample_193": ["    def test_related_indexes(self):\n        A = self.create_model(\"A\", foreign_keys=[models.ForeignKey('B', models.CASCADE)])\n        B = self.create_model(\"B\", indexes=[models.Index(fields=['id'])])\n        self.assertRelated(A, [B])\n        self.assertRelated(B, [A])\n", "    def test_proxy_fk(self):\n        A = self.create_model(\"A\")\n        B = self.create_model(\"B\", bases=(A,), proxy=True)\n        C = self.create_model(\"C\", foreign_keys=[models.ForeignKey('B', models.CASCADE)])\n        self.assertRelated(A, [B, C])\n        self.assertRelated(B, [A, C])\n        self.assertRelated(C, [A, B])\n", "    def test_proxy_m2m(self):\n        A = self.create_model(\"A\", foreign_keys=[models.ManyToManyField('B')])\n        B = self.create_model(\"B\")\n        PA = self.create_model(\"PA\", bases=(A,), proxy=True)\n        self.assertRelated(A, [A.a_1.rel.through, B])\n        self.assertRelated(B, [A, A.a_1.rel.through])\n        self.assertRelated(PA, [A.a_1.rel.through, B]) \n", "    def test_proxy_fk(self):\n        A = self.create_model(\"A\")\n        B = self.create_model(\"B\", bases=(A,), proxy=True)\n        C = self.create_model(\"C\", foreign_keys=[models.ForeignKey('B', models.CASCADE)])\n        self.assertRelated(A, [B, C])\n        self.assertRelated(B, [A, C])\n        self.assertRelated(C, [A, B])\n", "    def test_proxy_fk(self):\n        A = self.create_model(\"A\")\n        B = self.create_model(\"B\", bases=(A,), proxy=True)\n        C = self.create_model(\"C\", foreign_keys=[models.ForeignKey('B', models.CASCADE)])\n        self.assertRelated(A, [B, C])\n        self.assertRelated(B, [A, C])\n        self.assertRelated(C, [A, B])\n"], "sample_1124": ["    def test_FracElement_compose():\n        F, x, y, z = field(\"x,y,z\", ZZ)\n        f = (x**2 + 3*y)/z\n\n        assert f.compose(x, x) == (x**4 + 6*x**2*y + 9*y**2)/z\n        raises(ZeroDivisionError, lambda: f.compose(x, z))\n", "    def test_FracElement_compose():\n        F, x,y,z = field(\"x,y,z\", ZZ)\n        f = (x**2 + 3*y)/z\n\n        assert f.compose(x, x + y) == ((x + y)**2 + 3*y)/z\n        raises(ZeroDivisionError, lambda: f.compose(z, 0))\n", "compilation error", "def test_FracElement_compose():\n    F, x,y,z = field(\"x,y,z\", ZZ)\n    f = (x**2 + 3*y)/z\n\n    assert f.compose(x, [ [x, y], [y, z] ]) == (x**2*y + 3*y**2)/z\n", "compilation error"], "sample_878": ["    def test_transformer_with_pandas_out_remainder(self, remainder, expected_names):\n        pd = pytest.importorskip(\"pandas\")\n        X_df = pd.DataFrame({\"feat0\": [1.0, 2.0, 3.0], \"feat1\": [2.0, 3.0, 4.0]})\n        ct = ColumnTransformer(\n            [\n                (\"trans_0\", PandasOutTransformer(offset=3.0), [\"feat1\"]),\n            ],\n            remainder=remainder,\n        )\n        ct.set_output(transform=\"pandas\")\n        X_trans_df = ct.fit_transform(X_df)\n        assert isinstance(X_trans_df, pd.DataFrame)\n        assert_array_equal(X_trans_df.columns, expected_names)\n", "    def test_get_feature_names_out_with_transformers_custom_output_names(\n        self, transformers, remainder, expected_output_names", "def test_column_transformer_set_output_with_sparse_output(remainder):\n    pd = pytest.importorskip(\"pandas\")\n    df = pd.DataFrame(\n        {\n            \"pet\": pd.Series([\"dog\", \"cat\", \"snake\"], dtype=\"category\"),\n            \"color\": pd.Series([\"green\", \"blue\", \"red\"], dtype=\"object\"),\n            \"age\": [1.4, 2.1, 4.4],\n            \"height\": [20, 40, 10],\n            \"distance\": pd.Series([20, pd.NA, 100], dtype=\"Int32\"),\n        }\n    )\n    ct = ColumnTransformer(\n        [\n            (\n                \"color_encode\",\n                OneHotEncoder(sparse_output=True, dtype=\"int8\"),\n                [\"color\"],\n            ),\n            (\"age\", StandardScaler(), [\"age\"]),\n        ],\n        remainder=remainder,\n        verbose_feature_names_out=False,\n    ).set_output(transform=\"pandas\")\n    X_trans = ct.fit_transform(df)\n\n    assert isinstance(X_trans, pd.DataFrame)\n    assert_array_equal(ct.get_feature_names_out(), X_trans.columns)\n", "    def test_column_transformer_set_output_mixed_dtypes(remainder):\n        pd = pytest.importorskip(\"pandas\")\n        df = pd.DataFrame(\n            {\n                \"pet\": pd.Series([\"dog\", \"cat\", \"snake\"], dtype=\"category\"),\n                \"color\": pd.Series([\"green\", \"blue\", \"red\"], dtype=\"object\"),\n                \"age\": [1.4, 2.1, 4.4],\n                \"height\": [20, 40, 10],\n                \"distance\": pd.Series([20, pd.NA, 100], dtype=\"Int32\"),\n            }\n        )\n\n        ct = ColumnTransformer(\n            [\n                (\n                    \"color_encode\",\n                    OneHotEncoder(sparse_output=False, dtype=\"int8\"),\n                    [\"color\"],\n                ),\n                (\"age\", StandardScaler(), [\"age\"]),\n            ],\n            remainder=remainder,\n            verbose_feature_names_out=False,\n        ).set_output(transform=\"pandas\")\n\n        X_trans = ct.fit_transform(df)\n        assert isinstance(X_trans, pd.DataFrame)\n\n", "def test_column_transformer_set_output_with_custom_transformer_that_returns_numpy_but_does_not_define_get_feature_names_out(\n    remainder"], "sample_531": ["compilation error", "def test_subfigure_gridspec_sharing():\n    fig = plt.figure()\n    gs = fig.add_gridspec(2, 2)\n\n    subfig_tl = fig.add_subfigure(gs[0, 0])\n    subfig_br = fig.add_subfigure(gs[1, 1])\n\n    ax1 = subfig_tl.add_subplot(1, 1, 1)\n    ax2 = subfig_br.add_subplot(1, 1, 1, sharex=ax1, sharey=ax1)\n    ax1.plot([0, 1], [0, 1])\n\n    assert ax1.sharedx_axes == ax2\n    assert ax1.sharey_axes == ax2\n", "    def test_add_subplot_existing_gridspec(self):\n        fig, ax = plt.subplots()\n        gs = ax.get_gridspec()\n        ax2 = fig.add_subplot(gs[0, 1])\n        assert ax2.get_gridspec() is gs\n        assert len(fig.axes) == 2\n", "def test_subplotspec_projection(projection):\n    fig = plt.figure()\n    gs = fig.add_gridspec(1, 1)\n    ax = fig.add_subplot(gs[0, 0], projection=projection)\n    assert ax.name == projection\n", "compilation error"], "sample_383": ["    def test_ticket_24605_with_q(self):\n        i1 = Individual.objects.create(alive=True)\n        RelatedIndividual.objects.create(related=i1)\n        i2 = Individual.objects.create(alive=False)\n        RelatedIndividual.objects.create(related=i2)\n        i3 = Individual.objects.create(alive=True)\n        i4 = Individual.objects.create(alive=False)\n\n        qs = Individual.objects.filter(\n            Q(alive=False) & Q(related_individual__isnull=True)\n        )\n        self.assertSequenceEqual(qs, [i4])\n", "    def test_ticket_24605_related(self):\n        i1 = Individual.objects.create(alive=True)\n        RelatedIndividual.objects.create(related=i1)\n        i2 = Individual.objects.create(alive=False)\n        RelatedIndividual.objects.create(related=i2)\n        i3 = Individual.objects.create(alive=True)\n        i4 = Individual.objects.create(alive=False)\n\n        self.assertSequenceEqual(\n            Individual.objects.filter(\n                related_individual__isnull=True\n            ).exclude(alive=False),\n            [i3],\n        )\n\n", "    def test_ticket_24605_values(self):\n        \"\"\"\n        Subquery table names should be quoted, even when using values().\n        \"\"\"\n        i1 = Individual.objects.create(alive=True)\n        RelatedIndividual.objects.create(related=i1)\n        i2 = Individual.objects.create(alive=False)\n        RelatedIndividual.objects.create(related=i2)\n        i3 = Individual.objects.create(alive=True)\n        i4 = Individual.objects.create(alive=False)\n\n        self.assertSequenceEqual(\n            Individual.objects.values(\"pk\").filter(\n                Q(alive=False), Q(related_individual__isnull=True)\n            ),\n            [{\"pk\": i4.pk}],\n        )\n", "    def test_ticket_24605_related_count(self):\n        \"\"\"\n        Test COUNT(*) subquery with join promotion.\n\n        Ticket 24605 reported a problem with subqueries that used COUNT(*)\n        when join promotion was enabled.\n        \"\"\"\n        i1 = Individual.objects.create(alive=True)\n        RelatedIndividual.objects.create(related=i1)\n        i2 = Individual.objects.create(alive=False)\n        RelatedIndividual.objects.create(related=i2)\n        i3 = Individual.objects.create(alive=True)\n        i4 = Individual.objects.create(alive=False)\n\n        self.assertSequenceEqual(\n            Individual.objects.annotate(\n                related_count=Count(\"related_individual\")\n            ).filter(related_count=1, alive=True),\n            [i1],\n        )\n", "    def test_ticket_24605_values(self):\n        i1 = Individual.objects.create(alive=True)\n        RelatedIndividual.objects.create(related=i1)\n        i2 = Individual.objects.create(alive=False)\n        RelatedIndividual.objects.create(related=i2)\n        i3 = Individual.objects.create(alive=True)\n        i4 = Individual.objects.create(alive=False)\n\n        self.assertSequenceEqual(\n            Individual.objects.values(\"id\")\n            .filter(Q(alive=False), Q(related_individual__isnull=True)),\n            [{\"id\": i4.pk}],\n        )\n        self.assertSequenceEqual(\n            Individual.objects.exclude(\n                Q(alive=False), Q(related_individual__isnull=True)\n            ).values(\"id\").distinct().order_by(\"id\"),\n            [{\"id\": i1.pk}, {\"id\": i2.pk}, {\"id\": i3.pk}],\n        )\n"], "sample_164": ["    def test_server_formatter_uses_default_log_time(self):\n        self.client.get('/innocent/')\n        self.assertEqual(self.logger_output.getvalue(), '')\n", "    def test_exclude_internal_ips_from_email(self):\n        self.client.get('/internal_server_error/')\n        self.assertEqual(len(mail.outbox), 0)\n", "    def test_logging_for_unhandled_exception(self):\n        self.assertLogsRequest(\n            url='/unhandled_exception_logging/',\n            level='ERROR',\n            msg='Unhandled exception',\n            status_code=500,\n        )\n", "    def test_logging_for_uncaught_exception_with_custom_handler(self):\n        class CustomHandler(logging.Handler):\n                self.called = True\n                self.record = record\n        handler = CustomHandler()\n        self.logger.addHandler(handler)\n        self.client.get('/uncaught_exception/')\n        self.assertTrue(handler.called)\n        self.assertEqual(handler.record.exc_info[0], views.UncaughtException)\n", "    def test_server_formatter_uses_server_time(self):\n        formatter = ServerFormatter()\n        record = logging.makeLogRecord({'msg': 'log message', 'server_time': '2016-09-25 10:20:30'})\n        self.assertEqual(formatter.format(record).split('[')[1].split(']')[0], '2016-09-25 10:20:30')\n"], "sample_310": ["    def test_model_detail_related_object_permissions(self):\n        # User has permission to view Person, but not Company\n        self.client.force_login(self.staff_user)\n\n        response = self.client.get(reverse('django-admindocs-models-detail', args=['admin_docs', 'Person']))\n        self.assertEqual(response.status_code, 200)\n\n        response = self.client.get(reverse('django-admindocs-models-detail', args=['admin_docs', 'Company']))\n        self.assertEqual(response.status_code, 403)\n", "    def test_model_with_abstract_base_class(self):\n        response = self.client.get(reverse('django-admindocs-models-detail', args=['admin_docs', 'BasePerson']))\n        self.assertEqual(response.status_code, 404)\n", "    def test_model_detail_with_custom_fields(self):\n        class CustomModel(models.Model):\n            custom_field = CustomField()\n\n        response = self.client.get(reverse('django-admindocs-models-detail', args=['myapp', 'CustomModel']))\n        self.assertContains(response, '<td>A custom field type</td>')\n", "    def test_simplify_regex(self):\n        self.assertEqual(simplify_regex('^(?P<sport_slug>\\w+)/athletes/(?P<athlete_slug>\\w+)/$'), '/<sport_slug>/athletes/<athlete_slug>/')\n        self.assertEqual(simplify_regex('^articles/(?P<year>\\d{4})/(?P<month>\\d{2})/(?P<day>\\d{2})/(?P<slug>[-_\\w]+)/$'), '/articles/<year>/<month>/<day>/<slug>/')\n        self.assertEqual(simplify_regex('^users/(?P<user_id>\\d+)/$'), '/users/<user_id>/')\n        self.assertEqual(simplify_regex('^$'), '/')\n", "    def test_model_detail_with_custom_field(self):\n        class CustomModel(models.Model):\n            custom_field = CustomField()\n\n        response = self.client.get(reverse('django-admindocs-models-detail', args=['admin_docs', 'CustomModel']))\n        self.assertContains(response, '<td>custom_field</td>')\n        self.assertContains(response, 'A custom field type')\n"], "sample_91": ["    def test_permission_denied(self):\n        \"The permission_denied view raises a 403 status\"\n        response = self.client.get('/permission_denied/', HTTP_X_CSRFTOKEN='invalid') \n        self.assertEqual(response.status_code, 403)\n        self.assertIn(b'<h1>403 Forbidden</h1>', response.content)\n", "    def test_permission_denied(self):\n        \"The permission_denied view raises a 403 status and renders the correct template\"\n        response = self.client.get('/permission_denied/')\n        self.assertContains(response, b'This is a test template for a 403 error.', status_code=403)\n\n", "    def test_permission_denied(self):\n        \"403 Forbidden status returned by permission_denied view\"\n        view = lambda request: forbidden('You shall not pass!')\n        response = self.client.get('/permission_denied/', view=view)\n        self.assertEqual(response.status_code, 403)\n        self.assertIn(b'<h1>403 Forbidden</h1>', response.content)\n", "    def test_permission_denied(self):\n        \"The permission_denied view returns a 403 status\"\n        response = self.client.get('/permission_denied/', HTTP_AUTHORIZATION='invalid')\n        self.assertEqual(response.status_code, 403)\n        self.assertIn(b'<h1>403 Forbidden</h1>', response.content)\n", "    def test_permission_denied(self):\n        \"A 403 status is returned by the permission_denied view\"\n        response = permission_denied(self.request_factory.get('/'), Exception())\n        self.assertEqual(response.status_code, 403)\n        self.assertIn(b'<h1>403 Forbidden</h1>', response.content)\n\n"], "sample_228": ["    def test_all_valid_with_invalid_formset(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': '',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset1, formset2)), False)\n", "    def test_all_valid_with_formsets_having_different_prefixes(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='otherchoices')\n        self.assertIs(all_valid((formset1, formset2)), True)\n", "    def test_partially_valid(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '',  \n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [{}, {'votes': ['This field is required.']}]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n", "    def test_formset_with_initial_data_and_errors(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '1',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset = ChoiceFormSet(data, initial=[{'choice': 'Zero', 'votes': 10}], auto_id=False, prefix='choices')\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(formset.errors, [{}, {'votes': ['This field is required.']}])\n        self.assertEqual(\n            formset.cleaned_data,\n            [{'choice': 'Zero', 'votes': 10}, {}]\n        )\n", "    def test_all_valid_with_non_form_errors(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset1.non_form_errors().append(ValidationError('Something went wrong'))\n        self.assertIs(all_valid((formset1, formset2)), False)\n        self.assertEqual(formset1.non_form_errors(), ['Something went wrong'])\n"], "sample_285": ["    def test_get_finder_returns_instance(self):\n        finder_class = get_finder(\"django.contrib.staticfiles.finders.FileSystemFinder\")\n        self.assertIsInstance(finder_class, FileSystemFinder)\n", "    def test_dirs_warning_if_not_exist(self):\n        self.assertEqual(check_finders(None), [\n            Warning(\n                f\"The directory '{Path(TEST_ROOT) / 'project' / 'documents'}\"\n                f\"' in the STATICFILES_DIRS setting does not exist.\",\n                id='staticfiles.W004',\n            )\n        ])\n\n", "    def test_dirs_missing(self):\n        with self.settings(STATICFILES_DIRS=[os.path.join(TEST_ROOT, 'nonexistent')]):\n            self.assertEqual(check_finders(None), [\n                Warning(\n                    f\"The directory '{os.path.join(TEST_ROOT, 'nonexistent')}' in the STATICFILES_DIRS setting does not exist.\",\n                    id='staticfiles.W004',\n                )\n            ])\n", "    def test_dirs_not_exist(self):\n        with self.settings(STATICFILES_DIRS=[str(Path(TEST_ROOT) / 'no_such_directory')]):\n            self.assertEqual(check_finders(None), [\n                Warning(\n                    f\"The directory '{Path(TEST_ROOT) / 'no_such_directory'}\"\n                    f\"' in the STATICFILES_DIRS setting does not exist.\",\n                    id='staticfiles.W004',\n                )\n            ])\n", "    def test_dirs_not_exist(self):\n        with mock.patch('os.path.isdir') as mock_isdir:\n            mock_isdir.return_value = False\n            with self.settings(STATICFILES_DIRS=[str(Path(TEST_ROOT) / 'nonexistent_dir')]):\n                errors = check_finders(None)\n                self.assertEqual(len(errors), 1)\n                self.assertIsInstance(errors[0], Warning)\n                self.assertIn(\n                    \"The directory 'nonexistent_dir' in the STATICFILES_DIRS setting does not exist.\",\n                    errors[0].msg,\n                )\n"], "sample_517": ["compilation error", "compilation error", "def test_text_bbox_transform():\n    fig, ax = plt.subplots()\n    ax.set_xlim([-1, 1])\n    ax.set_ylim([-1, 1])\n\n    text = ax.text(0.5, 0.5, 'text', transform=ax.transAxes)\n    bbox = text.get_bbox_transform()\n    assert isinstance(bbox, mtransforms.Affine2D)\n    assert bbox.transform(np.array([0, 0]))[0] == 0\n\n", "compilation error", "    def test_text_bbox_tight(self):\n        fig, ax = plt.subplots()\n        ax.text(0.5, 0.5, 'foo')\n        bbox = ax.get_tightbbox(fig.canvas.get_renderer())\n        assert bbox.width > 0 and bbox.height > 0\n"], "sample_1070": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_329": ["    def test_serialize_model_with_custom_field(self):\n        class CustomField(models.Field):\n                self.custom_arg = kwargs.pop('custom_arg', 'default')\n                super().__init__(*args, **kwargs)\n\n                name, path, args, kwargs = super().deconstruct()\n                kwargs['custom_arg'] = self.custom_arg\n                return name, path, args, kwargs\n\n        class MyModel(models.Model):\n            custom_field = CustomField(custom_arg='special_value')\n\n        migration = type(\"Migration\", (migrations.Migration,), {\n            \"operations\": [\n                migrations.CreateModel(\n                    \"MyModel\",\n                    [('custom_field', CustomField(custom_arg='special_value'))],\n                    {},\n                    (models.Model,)\n                ),\n            ],\n            \"dependencies\": [],\n        })\n        writer = MigrationWriter(migration)\n        output = writer.as_string()\n        result = self.safe_exec(output)\n        self.assertIn(\"'custom_arg': 'special_value'\", result)\n", "    def test_serialize_proxy_model(self):\n        class ProxyModel(models.Model):\n            class Meta:\n                proxy = True\n        \n        field = models.ForeignKey(ProxyModel, on_delete=models.CASCADE)\n        string = MigrationWriter.serialize(field)[0]\n        self.assertIn(\"models.ForeignKey\", string)\n", "    def test_serialize_decimal_field(self):\n        field = models.DecimalField(max_digits=10, decimal_places=2)\n        string, imports = MigrationWriter.serialize(field)\n        self.assertEqual(string, \"models.DecimalField(max_digits=10, decimal_places=2)\")\n        self.assertEqual(imports, {'from django.db import models'})\n", "    def test_serialize_namedtuples(self):\n        Point = collections.namedtuple('Point', ['x', 'y'])\n        point = Point(1, 2)\n        serialized = self.serialize_round_trip(point)\n        self.assertEqual(serialized, point)\n", "    def test_serialize_model_with_custom_manager(self):\n        class MyManager(models.Manager):\n                return super().get_queryset().filter(name__startswith='A')\n\n        class MyModel(models.Model):\n            name = models.CharField(max_length=255)\n            objects = MyManager()\n\n        migration = type(\"Migration\", (migrations.Migration,), {\n            \"operations\": [\n                migrations.CreateModel(\n                    'MyModel',\n                    fields=[\n                        ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                        ('name', models.CharField(max_length=255)),\n                    ],\n                    options={\n                        'abstract': False,\n                    },\n                    managers=[\n                        ('objects', MyManager()),\n                    ],\n                ),\n            ],\n            \"dependencies\": []\n        })\n        writer = MigrationWriter(migration)\n        output = writer.as_string()\n        self.assertIn('MyManager', output)\n        self.assertIn(\"super().get_queryset().filter(name__startswith='A')\", output)\n\n"], "sample_430": ["    def test_rename_model_with_related_fields(self):\n        before = [\n            ModelState(\n                \"testapp\",\n                \"Author\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"name\", models.CharField(max_length=200)),\n                    (\"book\", models.ForeignKey(\"otherapp.Book\", models.CASCADE)),\n                ],\n            ),\n            ModelState(\n                \"otherapp\",\n                \"Book\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"title\", models.CharField(max_length=200)),\n                ],\n            ),\n        ]\n        after = [\n            ModelState(\n                \"testapp\",\n                \"Writer\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"name\", models.CharField(max_length=200)),\n                    (\"book\", models.ForeignKey(\"otherapp.Book\", models.CASCADE)),\n                ],\n            ),\n            ModelState(\n                \"otherapp\",\n                \"Book\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"title\", models.CharField(max_length=200)),\n                ],\n            ),\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(\n            changes,\n            \"testapp\",\n            0,\n            [\"RenameModel\", ],\n        )\n        self.assertOperationAttributes(\n            changes, \"testapp\", 0, 0, old_name=\"Author\", new_name=\"Writer\"\n        )\n", "def test_alter_field_with_choices(self):\n    initial_author = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n            (\"status\", models.CharField(max_length=20, choices=[('draft', 'Draft'), ('published', 'Published')])),\n        ],\n    )\n    author_updated_choices = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n            (\"status\", models.CharField(max_length=20, choices=[('draft', 'Draft'), ('published', 'Published'), ('archived', 'Archived')])),\n        ],\n    )\n    changes = self.get_changes([initial_author], [author_updated_choices])\n\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(\n        changes,\n        \"testapp\",\n        0,\n        [\"AlterField\"],\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        model_name=\"author\",\n        name=\"status\",\n        choices=[('draft', 'Draft'), ('published', 'Published'), ('archived', 'Archived')],\n    )\n", "    def test_multiple_operations_same_model(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.AddField(\n                    \"Person\",\n                    \"age\",\n                    models.IntegerField(),\n                    preserve_default=False,\n                ),\n                migrations.AlterField(\n                    \"Person\",\n                    \"name\",\n                    models.CharField(max_length=100),\n                ),\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"person_alter\")\n", "    def test_operation_with_invalid_chars_in_suggested_name_2(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.AlterField(\n                    \"Person\", \"name\", field=models.CharField(max_length=100, unique=True)\n                )\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"person_name\")\n", "    def test_create_and_delete_same_model(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\"Person\", fields=[]),\n                migrations.DeleteModel(\"Person\"),\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"person\")\n"], "sample_406": ["    def test_refresh_fk_does_not_load_related(self):\n        s1 = SelfRef.objects.create()\n        s2 = SelfRef(selfref=s1)\n        s2.save()\n        s2.refresh_from_db(fields=[\"pk\"])\n        self.assertEqual(s2.selfref, s1)\n", "    def test_refresh_clears_m2m_field(self):\n        article = Article.objects.create(\n            headline=\"Parrot programs in Python\",\n            pub_date=datetime(2005, 7, 28),\n        )\n        tag1 = Tag.objects.create(name=\"python\")\n        tag2 = Tag.objects.create(name=\"programming\")\n        article.tags.add(tag1, tag2)\n        article.refresh_from_db()\n        self.assertCountEqual(article.tags.all(), [tag1, tag2])\n\n", "    def test_refresh_clears_m2m_fields(self):\n        article = Article.objects.create(\n            headline=\"Parrot programs in Python\",\n            pub_date=datetime(2005, 7, 28),\n        )\n        tag1 = Tag.objects.create(name=\"python\")\n        tag2 = Tag.objects.create(name=\"programming\")\n        article.tags.add(tag1, tag2)\n        self.assertEqual(list(article.tags.all()), [tag1, tag2])\n        tag1.name = \"Python\"\n        tag1.save()\n        article.refresh_from_db()\n        self.assertEqual(list(article.tags.all()), [tag1, tag2])\n", "    def test_refresh_m2m_fields(self):\n        a = Article.objects.create(pub_date=datetime.now())\n        t1 = Tag.objects.create(name=\"tag1\")\n        t2 = Tag.objects.create(name=\"tag2\")\n        a.tags.add(t1, t2)\n        a.refresh_from_db()\n        self.assertEqual(len(a.tags.all()), 2)\n", "    def test_refresh_no_db_field(self):\n        s = SelfRef.objects.create()\n        s.refresh_from_db(fields=[\"nonexistent_field\"])\n        with self.assertNumQueries(0):\n            s.nonexistent_field\n\n"], "sample_40": ["def test_equivalent_units_in_registry():\n    registry = u.get_current_unit_registry()\n    for unit in registry.all_units:\n        equiv = unit.find_equivalent_units()\n        for eq in equiv:\n            assert eq in registry.all_units\n\n", "def test_equivalent_units_with_scaling():\n    with u.add_enabled_units(imperial):\n        with pytest.warns(u.UnitsWarning):\n            assert u.m.to(imperial.ft, equivalencies=[(u.m, 3.28084 *\n                                                     u.ft)]) == 1.0\n", "def test_redshift_velocity():\n    z = 0.5\n\n    c = constants.c\n\n    v_c = (c * z) / (1 + z)\n\n    assert_quantity_allclose(z.to(u.km/u.s, equivalencies=u.redshift_to_velocity()), v_c)\n", "    def test_angle_equivalencies():\n        # Test that angle equivalencies work for both scalar and array inputs\n        angle = 45 * u.degree\n        angle_array = np.array([45, 90, 135]) * u.degree\n\n        angle_rad = angle.to(u.radian, equivalencies=u.dimensionless_angles())\n        angle_array_rad = angle_array.to(u.radian, equivalencies=u.dimensionless_angles())\n\n        assert_allclose(angle_rad.value, np.deg2rad(45))\n        assert_allclose(angle_array_rad.value, np.deg2rad(angle_array.value))\n\n        # Test with dimensionless units\n        angle_dimensionless = angle.to(u.dimensionless_unscaled, equivalencies=u.dimensionless_angles())\n        angle_array_dimensionless = angle_array.to(u.dimensionless_unscaled, equivalencies=u.dimensionless_angles())\n\n        assert_allclose(angle_dimensionless.value, angle.to(u.radian).value)\n        assert_allclose(angle_array_dimensionless.value, angle_array.to(u.radian).value)\n", "def test_equivalent_units_with_exponent():\n    # Check that equivalent units are correctly handled when an exponent is\n    # applied to a Quantity\n    flux = 1 * u.Jy\n    \n    with pytest.raises(u.UnitsError):\n        flux**2 / (u.sr ** 2)\n\n    equiv_units = flux**2 / (u.sr ** 2).find_equivalent_units()\n\n    assert (u.MJy/u.sr**2) in equiv_units\n"], "sample_644": ["    def test_import_outside_toplevel(self) -> None:\n        module = astroid.MANAGER.ast_from_module_name(\"import_outside_toplevel\", REGR_DATA)\n        import_from = module.body[0]\n        msg = MessageTest(\n            msg_id=\"import-outside-toplevel\",\n            node=import_from,\n            args=\"my_module\",\n            confidence=UNDEFINED,\n            line=2,\n            col_offset=4,\n            end_line=2,\n            end_col_offset=19,\n        )\n        with self.assertAddsMessages(msg):\n            self.checker.visit_importfrom(import_from)\n", "    def test_wrong_import_position(self) -> None:\n        module = astroid.MANAGER.ast_from_module_name(\"wrong_import_position\", REGR_DATA)\n        import_from = module.body[1]\n\n        msg = MessageTest(\n            msg_id=\"wrong-import-position\",\n            node=import_from,\n            args='from wrong_package import something',\n            line=2,\n            col_offset=0,\n            end_line=2,\n            end_col_offset=37,\n        )\n        with self.assertAddsMessages(msg):\n            self.checker.visit_importfrom(import_from)\n", "    def test_import_after_non_import(self) -> None:\n        module = astroid.MANAGER.ast_from_module_name(\n            \"import_after_non_import\", REGR_DATA\n        )\n        import_nodes = [node for node in module.body if isinstance(node, astroid.ImportFrom)]\n\n        # The first import should trigger a wrong-import-position message\n        msg = MessageTest(\n            msg_id=\"wrong-import-position\",\n            node=import_nodes[0],\n            args=\"import_after_non_import\",\n            line=3,\n            col_offset=0,\n            end_line=3,\n            end_col_offset=22,\n        )\n        with self.assertAddsMessages(msg):\n            self.checker.visit_importfrom(import_nodes[0])\n\n        # Subsequent imports should not trigger a message\n        with self.assertNoMessages():\n            for import_node in import_nodes[1:]:\n                self.checker.visit_importfrom(import_node)\n", "    def test_nested_imports(self) -> None:\n        module = astroid.MANAGER.ast_from_module_name(\"nested_imports\", REGR_DATA)\n        import_from = module.body[1]\n\n        with self.assertNoMessages():\n            self.checker.visit_importfrom(import_from)\n", "    def test_import_after_function(self) -> None:\n        module = astroid.MANAGER.ast_from_module_name(\n            \"import_after_function\", REGR_DATA\n        )\n        first_import = module.body[1]\n        second_import = module.body[2]\n\n        with self.assertNoMessages():\n            self.checker.visit_import(first_import)\n        with self.assertAddsMessages(\n            MessageTest(\n                msg_id=\"wrong-import-position\",\n                node=second_import,\n                args=\"import pandas\",\n                line=4,\n                col_offset=0,\n                end_line=4,\n                end_col_offset=14,\n            )\n        ):\n            self.checker.visit_import(second_import)\n\n"], "sample_1036": ["compilation error", "def test_matmul_empty_list():\n    assert MatMul().doit() == 1\n", "compilation error", "def test_matmul_scalar_matmul():\n    # Issue 13424\n    A = MatrixSymbol('A', 2, 2)\n    assert MatMul(2*MatMul(A, A)).doit() == 2*A*A\n", "compilation error"], "sample_584": ["    def test_auto_combine_empty_datasets(self):\n        objs = [Dataset(), Dataset({'x': 0})]\n        actual = auto_combine(objs)\n        expected = Dataset({'x': 0})\n        assert_identical(expected, actual)\n", "    def test_auto_combine_data_array(self):\n        ds1 = Dataset({'a': DataArray([0], dims='x')}, coords={'x': [0]})\n        ds2 = Dataset({'a': DataArray([1], dims='x')}, coords={'x': [1]})\n        actual = auto_combine([ds1, ds2])\n        expected = Dataset({'a': DataArray([0, 1], dims='x')}, coords={'x': [0, 1]})\n        assert_identical(actual, expected)\n", "    def test_auto_combine_inconsistent_coords(self):\n        objs = [Dataset({'x': ('y', [0])}, coords={'y': ('y', [0])}),\n                Dataset({'x': ('y', [1])}, coords={'y': ('y', [1, 2])})]\n        with pytest.raises(ValueError):\n            auto_combine(objs)\n", "    def test_auto_combine_empty_list(self):\n        with pytest.raises(ValueError, match='Input must not be an empty list'):\n            auto_combine([])\n", "    def test_combine_nested_empty_datasets(self):\n        objs = [Dataset({}), Dataset({'x': [0]})]\n        actual = combine_nested(objs, concat_dim='x')\n        expected = Dataset({'x': [0]})\n        assert_identical(expected, actual)\n\n"], "sample_159": ["    def test_missing_username_field(self):\n        \"\"\"\n        Check the error raised if the USERNAME_FIELD is missing.\n        \"\"\"\n        class CustomUserNoUsernameField(AbstractBaseUser):\n            date_of_birth = models.DateField()\n\n            REQUIRED_FIELDS = ['date_of_birth']\n\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The field named as the 'USERNAME_FIELD' for a custom user model must not be included in 'REQUIRED_FIELDS'.\",\n                hint=(\n                    \"The 'USERNAME_FIELD' is currently set to '<undefined>', you \"\n                    \"should remove 'date_of_birth' from the 'REQUIRED_FIELDS'.\"\n                ),\n                obj=CustomUserNoUsernameField,\n                id='auth.E002',\n            ),\n        ])\n", "    def test_permission_names_length_with_long_model_name(self):\n        long_model_name = 'X' * 80\n        class LongModelName(models.Model):\n            class Meta:\n                verbose_name = 'Some Model'\n\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The verbose_name of model 'auth_tests.%s' must be at most 174 \"\n                \"characters for its builtin permission names to be at most 255 characters.\" % long_model_name,\n                obj=LongModelName,\n                id='auth.E007',\n            ),\n        ])\n", "    def test_required_fields_empty_tuple(self):\n        \"\"\"REQUIRED_FIELDS should not be an empty tuple.\"\"\"\n        class CustomUserWithWeirdRequiredFields(AbstractBaseUser):\n            username = models.CharField(max_length=30, unique=True)\n            date_of_birth = models.DateField()\n\n            USERNAME_FIELD = 'username'\n            REQUIRED_FIELDS = tuple()\n\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"'REQUIRED_FIELDS' must be a list or tuple.\",\n                obj=CustomUserWithWeirdRequiredFields,\n                id='auth.E001',\n            ),\n        ])\n\n", "    def test_builtin_permission_name_length(self):\n        class Checked(models.Model):\n            pass\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [])\n", "    def test_custom_user_model_is_anonymous_is_authenticated(self):\n        \"\"\"\n        Test that checks for is_anonymous and is_authenticated methods properly\n        handle custom User models.\n        \"\"\"\n        class CustomUser(AbstractBaseUser):\n            username = models.CharField(max_length=30, unique=True)\n            USERNAME_FIELD = 'username'\n\n        errors = checks.run_checks(app_configs=self.apps.get_app_configs())\n        self.assertEqual(errors, [])\n"], "sample_716": ["    def test_ridge_classifier_predict_proba(self):\n        # Test RidgeClassifier predict_proba\n        X, y = make_classification(n_samples=100, n_features=10, random_state=42)\n        clf = RidgeClassifier(random_state=0)\n        clf.fit(X, y)\n        proba = clf.predict_proba(X)\n        # Check if probabilities sum to 1 for each sample\n        assert_allclose(proba.sum(axis=1), np.ones(proba.shape[0]))\n", "    def test_ridge_classifier_decision_function(self):\n        # Test the decision_function method of RidgeClassifier\n        X = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0],\n                      [1.0, 1.0], [1.0, 0.0]])\n        y = [1, 1, 1, -1, -1]\n\n        reg = RidgeClassifier()\n        reg.fit(X, y)\n        decision_values = reg.decision_function(X)\n\n        # Check shapes\n        assert decision_values.shape == (len(X),)\n", "def test_ridge_classifier_decision_function():\n    # Test the decision_function of RidgeClassifier.\n\n    X = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0],\n                  [1.0, 1.0], [1.0, 0.0]])\n    y = [1, 1, 1, -1, -1]\n\n    reg = RidgeClassifier()\n    reg.fit(X, y)\n    decision_values = reg.decision_function(X)\n    assert decision_values.shape == (5,)\n\n    # Test for multi-class case\n    y_multi = [1, 1, 2, 2, 3]\n    reg_multi = RidgeClassifier()\n    reg_multi.fit(X, y_multi)\n    decision_values_multi = reg_multi.decision_function(X)\n    assert decision_values_multi.shape == (5, 3)\n", "    def test_ridge_classifiercv_multiclass(self):\n        # Test RidgeClassifierCV with multiclass classification\n        X, y = load_iris(return_X_y=True)\n        ridge = RidgeClassifierCV(cv=5)\n        ridge.fit(X, y)\n        y_pred = ridge.predict(X)\n        assert_equal(len(ridge.classes_), 3)\n", "    def test_ridge_classifiercv_sample_weight():\n        X = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0],\n                      [1.0, 1.0], [1.0, 0.0]])\n        y = [1, 1, 1, -1, -1]\n        sample_weight = np.array([1, 2, 1, 1, 1])\n\n        ridge = RidgeClassifierCV(alphas=[.01, .1, 1],\n                            cv=KFold(2, shuffle=True,\n                                     random_state=42))\n        ridge.fit(X, y, sample_weight=sample_weight)\n\n        # Check if the best alpha is consistent\n        ridge_no_weight = RidgeClassifierCV(alphas=[.01, .1, 1],\n                                         cv=KFold(2, shuffle=True,\n                                                  random_state=42))\n        ridge_no_weight.fit(X, y)\n        assert ridge.alpha_ != ridge_no_weight.alpha_\n"], "sample_1017": ["compilation error", "def test_issue_13901():\n    assert (x & True).simplify() == x\n    assert (x & False).simplify() == False\n    assert (True & x).simplify() == x\n    assert (False & x).simplify() == False\n", "def test_issue_13728():\n    assert And(x > 0).as_set() == Interval(0, oo)\n    assert And(x < 1, x > 0).as_set() == Interval(0, 1)\n", "compilation error", "compilation error"], "sample_971": ["def test_prefixed_warnings(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    with logging.prefixed_warnings(\"prefix:\"):\n        logger.warning('message1')\n        logger.warning('message2', location='index')\n\n    assert 'prefix: WARNING: message1' in warning.getvalue()\n    assert 'prefix: index.txt: WARNING: message2' in warning.getvalue()\n\n", "def test_prefixed_warnings(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    with logging.prefixed_warnings(\"prefix:\"):\n        logger.warning('message1')\n        logger.warning('message2')\n\n    assert 'prefix: WARNING: message1' in warning.getvalue()\n    assert 'prefix: WARNING: message2' in warning.getvalue()\n", "def test_prefixed_warnings(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    with logging.prefixed_warnings(\"prefix:\"):\n        logger.warning('message1')\n        logger.warning('message2', location='index')\n\n    assert 'prefix: WARNING: message1' in warning.getvalue()\n    assert 'prefix: WARNING: message1' in warning.getvalue()\n", "def test_prefixed_warnings(app, status, warning):\n    logging.setup(app, status, warning)\n    logger = logging.getLogger(__name__)\n\n    with logging.prefixed_warnings('prefix:'):\n        logger.warning('message1')\n        logger.warning('message2', location='index')\n\n    assert 'prefix: WARNING: message1' in warning.getvalue()\n    assert 'prefix: index.txt: WARNING: message2' in warning.getvalue()\n", "    def test_prefixed_warnings(app, status, warning):\n        logging.setup(app, status, warning)\n        logger = logging.getLogger(__name__)\n\n        with logging.prefixed_warnings(\"prefix:\"):\n            logger.warning('message1', location='index')\n            logger.warning('message2')\n\n        assert 'index.txt: prefix:WARNING: message1' in warning.getvalue()\n        assert 'prefix:WARNING: message2' in warning.getvalue()\n"], "sample_796": ["    def test_huber_intercept():\n        # Test that HuberRegressor correctly fits the intercept\n\n         rng = np.random.RandomState(0)\n         X = rng.randn(100, 2)\n         y = 3 + 2 * X[:, 0] - X[:, 1]\n\n        hubre = HuberRegressor(fit_intercept=True)\n        hubre.fit(X, y)\n        assert_almost_equal(hubre.intercept_, 3, 2)\n", "def test_huber_epsilons():\n    # Test that for different epsilon values, we get different number of outliers\"\"\"\n    X, y = make_regression_with_outliers(n_samples=100, n_features=2)\n    huber_small = HuberRegressor(fit_intercept=True, epsilon=0.5)\n    huber_small.fit(X, y)\n    n_outliers_small = np.sum(huber_small.outliers_)\n\n    huber_large = HuberRegressor(fit_intercept=True, epsilon=2.0)\n    huber_large.fit(X, y)\n    n_outliers_large = np.sum(huber_large.outliers_)\n\n    assert n_outliers_small > n_outliers_large\n", "def test_huber_intercept_scaling():\n    # Test that the intercept is scaled correctly when fit_intercept is True\n\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([10, 20, 30])\n    huber = HuberRegressor(fit_intercept=True, alpha=0)\n    huber.fit(X, y)\n    intercept = huber.intercept_\n\n    # Scale the data and refit\n    X_scaled = np.array([[2, 4], [6, 8], [10, 12]])\n    y_scaled = np.array([20, 40, 60])\n    huber_scaled = HuberRegressor(fit_intercept=True, alpha=0)\n    huber_scaled.fit(X_scaled, y_scaled)\n    intercept_scaled = huber_scaled.intercept_\n    assert_almost_equal(intercept_scaled, intercept * 2)\n", "def test_huber_intercept_constant():\n    # Test for constant feature\n    X = np.ones((10, 1))\n    y = np.arange(10)\n    huber = HuberRegressor(fit_intercept=True)\n    huber.fit(X, y)\n    assert huber.coef_[0] == 0\n    assert huber.intercept_ == 5 \n\n", "    def test_huber_epsilon_scaling():\n        # Test if epsilon scaling works correctly\n        X, y = make_regression_with_outliers()\n        huber = HuberRegressor(fit_intercept=True)\n        huber.fit(X, y)\n        coef_base = huber.coef_\n        scale_base = huber.scale_\n\n        # Double epsilon and check if the scale increases accordingly\n        huber = HuberRegressor(fit_intercept=True, epsilon=2 * huber.epsilon)\n        huber.fit(X, y)\n        assert huber.scale_ > scale_base\n\n        # Double epsilon again and check if the scale increases again\n        huber = HuberRegressor(fit_intercept=True, epsilon=4 * huber.epsilon)\n        huber.fit(X, y)\n        assert huber.scale_ > huber.scale_\n\n        # Check if the coefficients are proportionally smaller compared to base\n        assert np.all(np.abs(huber.coef_) < np.abs(coef_base))\n"], "sample_573": ["    def test_multi_grouper(self, df):\n\n        groupby = GroupBy([\"color\", \"group\"])\n        res = PolyFit(gridsize=50)(df, groupby, \"x\", {})\n\n        assert_array_equal(res.columns, [\"x\", \"y\", \"color\", \"group\"])\n\n        ngroups = df.groupby([\"color\", \"group\"]).ngroups\n        assert_array_equal(res.index, np.arange(ngroups * 50))\n", "    def test_small_sample(self, df):\n\n        df = df.iloc[: 3]\n        groupby = GroupBy([\"group\"])\n        res = PolyFit(order=1, gridsize=100)(df[[\"x\", \"y\"]], groupby, \"x\", {})\n\n        assert_array_equal(res.columns, [\"x\", \"y\"])\n\n        assert len(res) == 100\n", "    def test_duplicate_x(self, df):\n\n        df = pd.DataFrame(dict(\n            x=np.repeat([1, 2, 3], 3),\n            y=np.arange(9),\n        ))\n        groupby = GroupBy([\"group\"])\n\n        res = PolyFit(order=2, gridsize=100)(df, groupby, \"x\", {})\n\n        assert_frame_equal(res, pd.DataFrame(columns=['x', 'y'])) \n", "    def test_multiple_groupers(self, df):\n\n        groupby = GroupBy([\"group\", \"color\"])\n        res = PolyFit(gridsize=50)(df, groupby, \"x\", {})\n\n        assert res.columns.to_list() == [\"x\", \"y\", \"group\", \"color\"]\n\n        ngroups = df.groupby([\"group\", \"color\"]).size().sum()\n        assert_array_equal(res.index, np.arange(ngroups * 50))\n\n        for _, part in res.groupby([\"group\", \"color\"]):\n            grid = np.linspace(part[\"x\"].min(), part[\"x\"].max(), 50)\n            assert_array_equal(part[\"x\"], grid)\n            assert part[\"y\"].diff().diff().dropna().abs().gt(0).all()\n\n", "    def test_with_multiple_groupers(self, df):\n        groupby = GroupBy([\"color\", \"group\"])\n        gridsize = 50\n        res = PolyFit(gridsize=gridsize)(df, groupby, \"x\", {})\n\n        assert res.columns.to_list() == [\"x\", \"y\", \"color\", \"group\"]\n\n        ngroups = df.groupby([\"color\", \"group\"]).ngroups\n        assert_array_equal(res.index, np.arange(ngroups * gridsize))\n\n        for (color, group), part in res.groupby([\"color\", \"group\"]):\n            grid = np.linspace(part[\"x\"].min(), part[\"x\"].max(), gridsize)\n            assert_array_equal(part[\"x\"], grid)\n            assert part[\"y\"].diff().diff().dropna().abs().gt(0).all() \n"], "sample_741": ["    def test_grid_search_with_empty_grid():\n        # Test GridSearchCV with an empty param_grid\n        clf = LinearSVC(random_state=0)\n        grid_search = GridSearchCV(clf, {}, cv=2)\n        with pytest.raises(ValueError):\n            grid_search.fit(X, y)\n\n", "    def test_grid_search_refit_with_error_score():\n        # Test refit with error_score when best estimator raises a ValueError\n        # on predict\n\n        class FailingPredictor(BaseEstimator):\n                return self\n\n                raise ValueError(\"Failed to predict!\")\n\n        X = np.arange(20).reshape(5, -1)\n        y = [0, 0, 1, 1, 1]\n        clf = FailingPredictor()\n        gs = GridSearchCV(clf, {}, scoring='accuracy',\n                          error_score='raise', refit=True)\n        assert_raises(ValueError, gs.fit, X, y)\n\n        gs = GridSearchCV(clf, {}, scoring='accuracy',\n                          error_score=0.0, refit=True)\n        gs.fit(X, y)\n\n        # Check if refit worked even though best estimator failed\n        assert hasattr(gs, 'best_estimator_')\n\n", "    def array_scorer(estimator, X, y):\n        y_pred = estimator.predict(X)\n        return precision_recall_fscore_support(y, y_pred, average='macro')\n", "def test_grid_search_custom_scorer():\n    X, y = make_classification(n_samples=20, n_features=10, random_state=0)\n\n        y_pred = estimator.predict(X)\n        return f1_score(y_true, y_pred)\n\n    clf = LogisticRegression(random_state=0)\n    grid_search = GridSearchCV(clf, {'C': [0.1, 1]}, scoring=custom_scorer)\n    grid_search.fit(X, y)\n\n    # Check that the best estimator has the highest score according to the\n    # custom scorer\n    best_estimator = grid_search.best_estimator_\n    best_score = custom_scorer(best_estimator, X, y)\n    for candidate in grid_search.grid_scores_:\n        estimator = candidate[0]\n        score = candidate[1]\n        assert best_score >= score\n", "    def test_grid_search_with_pipeline(self):\n        # Test grid search with a pipeline\n\n        X, y = make_classification(n_samples=100, random_state=0)\n\n        estimators = [\n            ('scaler', StandardScaler()),\n            ('clf', LogisticRegression())\n        ]\n\n        pipe = Pipeline(estimators)\n        param_grid = {\n            'clf__C': [0.1, 1, 10],\n            'clf__penalty': ['l1', 'l2']\n        }\n        grid_search = GridSearchCV(pipe, param_grid=param_grid, cv=5)\n        grid_search.fit(X, y)\n\n        assert 'scaler' in grid_search.best_estimator_.named_steps\n        assert 'clf' in grid_search.best_estimator_.named_steps\n"], "sample_877": ["    def test_isotonic_regression_zero_variance():\n        \"\"\"Test IsotonicRegression with zero variance case.\"\"\"\n        X = np.full(10, 1.0)\n        y = np.arange(10)\n\n        ireg = IsotonicRegression().fit(X, y)\n        y_pred = ireg.predict(X)\n        assert_allclose(y_pred, y)\n", "    def test_isotonic_regression_empty_input():\n        \"\"\"Test IsotonicRegression with empty input arrays\"\"\"\n        X = np.array([])\n        y = np.array([])\n        with pytest.raises(ValueError, match=\"Input arrays should not be empty\"):\n            IsotonicRegression().fit(X, y)\n", "    def test_isotonic_regression_empty_input():", "    def test_isotonic_regression_sample_weight_empty():\n        X = np.array([1, 2, 3, 4, 5])\n        y = np.array([1, 2, 3, 4, 5])\n        sample_weight = np.array([])\n\n        with pytest.raises(ValueError, match=\"Sample weights array is empty\"):\n            isotonic_regression(y, sample_weight=sample_weight)\n\n        ireg = IsotonicRegression()\n        with pytest.raises(ValueError, match=\"Sample weights array is empty\"):\n            ireg.fit(X, y, sample_weight=sample_weight)\n", "compilation error"], "sample_5": ["compilation error", "compilation error", "compilation error", "compilation error", "def test_models_evaluate_magunits(model):\n    if not HAS_SCIPY and model['class'] in SCIPY_MODELS:\n        pytest.skip()\n\n    m = model['class'](**model['parameters'])\n    for args in model['evaluation']:\n        assert_quantity_allclose(m(*args[:-1]), args[-1])\n"], "sample_1188": ["def test_pretty_printing_dyadics():\n    x = symbols('x')\n    y = symbols('y')\n    z = symbols('z')\n    N = CoordSys3D('N')\n    C = N.orient_new_axis('C', x, N.k)\n    v = N.i\n    w = C.j  \n    dyad1 = v | w\n    dyad2 = (N.i + N.j + N.k) | (C.i + C.j)\n\n    assert upretty(dyad1) == \"(i_N|j_C)\"\n    assert upretty(dyad2) == \"(i_N + j_N + k_N|i_C + j_C)\"\n    assert pretty(dyad1) == \"(i_N|j_C)\"\n    assert pretty(dyad2) == \"(i_N + j_N + k_N|i_C + j_C)\"\n    ", "def test_issue_23058_expr_eval():\n    from sympy import symbols, sin, cos, pi, UnevaluatedExpr\n\n    delop = Del()\n    CC_   = CoordSys3D(\"C\")\n    y     = CC_.y\n    xhat  = CC_.i\n\n    t = symbols(\"t\")\n    ten = symbols(\"10\", positive=True)\n    eps, mu = 4*pi*ten**(-11), ten**(-5)\n    Bx = 2 * ten**(-4) * cos(ten**5 * t) * sin(ten**(-3) * y)\n    vecB = Bx * xhat\n    vecE = (1/eps) * Integral(delop.cross(vecB/mu).doit(), t)\n    vecE = vecE.doit()\n\n    assert vecE.evalf(subs={t: 0.5}) == vecE_evalf_05\n\n\n", "    def test_issue_23058_2():\n        from sympy import symbols, sin, cos, pi, UnevaluatedExpr\n\n        delop = Del()\n        CC_   = CoordSys3D(\"C\")\n        y     = CC_.y\n        xhat  = CC_.i\n\n        t = symbols(\"t\")\n        ten = symbols(\"10\", positive=True)\n        eps, mu = 4*pi*ten**(-11), ten**(-5)\n\n        Bx = 2 * ten**(-4) * cos(ten**5 * t) * sin(ten**(-3) * y)\n        vecB = Bx * xhat\n        vecE = (1/eps) * Integral(delop.cross(vecB/mu).doit(), t)\n        vecE = vecE.doit()\n\n        vecB_str = \"\"\"\\", "def test_del_printing():\n    assert pretty(Del()) == '\u2207'\n    assert upretty(Del()) == '\u2207'\n    assert pretty(Del(xyz=N)) == '\u2207_N'\n    assert upretty(Del(xyz=N)) == '\u2207_N'\n", "def test_issue_23573():\n    from sympy import symbols, sin, cos, pi, UnevaluatedExpr\n\n    delop = Del()\n    CC_   = CoordSys3D(\"C\")\n    y     = CC_.y\n    xhat  = CC_.i\n\n    t = symbols(\"t\")\n    ten = symbols(\"10\", positive=True)\n    eps, mu = 4*pi*ten**(-11), ten**(-5)\n\n    Bx = 2 * ten**(-4) * cos(ten**5 * t) * sin(ten**(-3) * y)\n    vecB = Bx * xhat\n    vecE = (1/eps) * Integral(delop.cross(vecB/mu).doit(), t)\n    vecE = vecE.doit()\n    vecE = vecE.rewrite(sin, cos)\n\n    vecE_str = \"\"\"\\"], "sample_813": ["def test_ard_coef_pruning():\n    # Test if ARDRegression prunes irrelevant features (coefficients)\n    n_samples = 100\n    n_features = 20\n    random_state = check_random_state(42)\n    X = random_state.randn(n_samples, n_features)\n\n    # Create a target that depends only on the first 5 features\n    w = np.zeros(n_features)\n    w[:5] = random_state.randn(5)\n    b = random_state.randn()\n    y = f(X) + 0.1 * random_state.randn(n_samples)\n\n    clf = ARDRegression()\n    clf.fit(X, y)\n\n    # Check if the coefficients corresponding to irrelevant features are close to zero\n    assert np.allclose(clf.coef_[5:], 0, atol=1e-3)\n", "def test_ard_coefficients():\n    # Test ARD Regression by checking if some coefficients are driven towards 0.\n    X = np.array([[1, 0, 1],\n                  [0, 1, 1],\n                  [1, 1, 0],\n                  [0, 1, 0]])\n    y = np.array([1, 2, 3, 4])\n    clf = ARDRegression(n_iter=100)\n    clf.fit(X, y)\n    # We expect some coefficients to be close to 0 after ARD regression\n    assert np.any(np.isclose(clf.coef_, 0, atol=1e-2))\n\n\n", "compilation error", "compilation error", "compilation error"], "sample_1106": ["def test_matmul_zero_matrix():\n    assert MatMul(ZeroMatrix(n, m), C) == ZeroMatrix(n, n)\n    assert MatMul(C, ZeroMatrix(m, k)) == ZeroMatrix(n, k)\n", "def test_matmul_zeromatrix():\n    assert MatMul(ZeroMatrix(2, 2), A) == ZeroMatrix(2, m)\n    assert MatMul(A, ZeroMatrix(m, 2)) == ZeroMatrix(n, 2)\n\n", "compilation error", "def test_matmul_empty():\n    assert MatMul().doit() == 1\n    assert MatMul(C).doit() == C\n", "def test_matmul_zero_matrix_multiplication():\n    Z = ZeroMatrix(n, m)\n    assert MatMul(C, Z).doit() == ZeroMatrix(n, m)\n    assert MatMul(Z, D).doit() == ZeroMatrix(n, n)\n"], "sample_1054": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_526": ["compilation error", "compilation error", "compilation error", "    def test_date2num_pandas_tzaware():\n        import pandas as pd\n        # Test for issue #3896.\n        # Ensure date2num handles pandas DatetimeIndex with timezone correctly\n        \n        dt_range = pd.date_range(start='2014-03-30', freq='33min',\n                                periods=8, tz='Europe/Brussels')\n        dt_utc = dt_range.tz_convert('UTC')\n        t0 = 735322.0 + mdates.date2num(np.datetime64('0000-12-31'))\n        expected_ordinalf = [t0 + (i * (33 / 1440)) for i in range(8)]\n        \n        actual_ordinalf = list(mdates.date2num(dt_utc))\n        assert actual_ordinalf == expected_ordinalf \n", "compilation error"], "sample_259": ["    def test_prefetch_deep_twice(self):\n        book1 = Book.objects.get(id=self.book1.id)\n        with self.assertNumQueries(1):\n            prefetch_related_objects([book1], Prefetch('authors__books'))\n        with self.assertNumQueries(1):\n            prefetch_related_objects([book1], Prefetch('authors__books'))\n        with self.assertNumQueries(0):\n            self.assertCountEqual(book1.authors.first().books.all(), [self.book1])\n\n", "    def test_prefetch_object_with_queryset(self):\n        book1 = Book.objects.get(id=self.book1.id)\n        with self.assertNumQueries(1):\n            prefetch_related_objects([book1], Prefetch('authors', queryset=Author.objects.filter(name__startswith='A')))\n\n        with self.assertNumQueries(0):\n            self.assertCountEqual(book1.authors.all(), [self.author1, self.author2])\n", "    def test_prefetch_related_objects_already_populated(self):\n        authors = list(Author.objects.all())\n        for author in authors:\n            author.books.all()  # Force population\n        with self.assertNumQueries(0):\n            prefetch_related_objects(authors, 'books')\n", "    def test_prefetch_object_twice_different_queryset(self):\n        book1 = Book.objects.get(id=self.book1.id)\n        book2 = Book.objects.get(id=self.book2.id)\n        with self.assertNumQueries(1):\n            prefetch_related_objects([book1], Prefetch('authors'))\n        with self.assertNumQueries(1):\n            prefetch_related_objects([book2], Prefetch('authors'))\n        with self.assertNumQueries(0):\n            self.assertCountEqual(book1.authors.all(), [self.author1, self.author2, self.author3])\n            self.assertCountEqual(book2.authors.all(), [self.author1])\n\n", "    def test_prefetch_object_nested_chained(self):\n        book1 = Book.objects.get(id=self.book1.id)\n        with self.assertNumQueries(1):\n            prefetch_related_objects([book1], 'authors__books')\n\n        with self.assertNumQueries(0):\n            self.assertCountEqual([b for author in book1.authors.all() for b in author.books.all()],\n                                 [self.book1, self.book2, self.book1, self.book1, self.book3])\n\n"], "sample_231": ["    def test_sensitive_args_function_caller(self):\n        \"\"\"\n        Sensitive variables don't leak in the sensitive_variables decorator's\n        frame, when those variables are passed as arguments to the decorated\n        function.\n        \"\"\"\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(sensitive_args_function_caller)\n            self.verify_unsafe_email(sensitive_args_function_caller)\n\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(sensitive_args_function_caller, check_for_POST_params=False)\n            self.verify_safe_email(sensitive_args_function_caller, check_for_POST_params=False)\n", "    def test_sensitive_variables_with_arguments(self):\n        message = 'sensitive_variables() decorator should only decorate functions.'\n        with self.assertRaisesMessage(TypeError, message):\n            @sensitive_variables('password')\n\n                pass\n\n", "    def test_sensitive_variables_decorator_with_arguments(self):\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(sensitive_variables_with_arguments_view, check_for_vars=False)\n\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(sensitive_variables_with_arguments_view, check_for_vars=False)\n", "    def test_sensitive_variables_with_default_arguments(self):\n        @sensitive_variables\n            pass\n        with self.settings(DEBUG=True):\n            response = self.client.get('/sensitive_variables_with_default_arguments/')\n            self.assertContains(response, 'password', status_code=500)\n", "    def test_sensitive_variables_with_starargs(self):\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(sensitive_variables_starargs_view)\n            self.verify_unsafe_email(sensitive_variables_starargs_view)\n\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(sensitive_variables_starargs_view)\n            self.verify_safe_email(sensitive_variables_starargs_view)\n"], "sample_571": ["    def test_lmplot_legend_title(self):\n        g = lm.lmplot(x=\"x\", y=\"y\", data=self.df, hue=\"h\", legend=\"brief\")\n        assert g._legend.get_title().get_text() == \"h\"\n\n        g = lm.lmplot(x=\"x\", y=\"y\", data=self.df, hue=\"h\", legend=\"full\")\n        assert g._legend.get_title().get_text() == \"h\"\n\n        g = lm.lmplot(x=\"x\", y=\"y\", data=self.df, hue=\"h\", legend=None)\n        assert g._legend is None\n", "    def test_regplot_x_partial_y_partial(self):\n        p = lm.regplot(x=\"x\", y=\"y\", data=self.df, x_partial=\"z\", y_partial=\"h\")\n        assert p.plot_kws['ci'] is None\n", "    def test_regplot_facetgrid(self):\n\n        g = sns.FacetGrid(self.df, col=\"g\", row=\"h\")\n        g.map(lm.regplot, \"x\", \"y\")\n\n\n", "    def test_lmplot_facet_kws_subplot_titles(self):\n\n        g = lm.lmplot(\n            data=self.df, x=\"x\", y=\"y\", col=\"h\",\n            facet_kws={\"title\":\"Column Title\"}\n        )\n\n        for ax in g.axes.flat:\n            assert ax.title.get_text() == \"Column Title\" \n\n", "    def test_lmplot_palette_and_hue_order(self):\n\n        g = lm.lmplot(x=\"x\", y=\"y\", data=self.df, hue=\"h\", palette=\"Set1\")\n\n        colors = g.hue_kws[\"color\"]\n        assert colors == color_palette(\"Set1\", n_colors=2)\n\n        self.df[\"new_h\"] = self.df[\"h\"][::-1]\n        g = lm.lmplot(x=\"x\", y=\"y\", data=self.df, hue=\"new_h\", palette=\"Set1\")\n\n        colors = g.hue_kws[\"color\"]\n        assert colors == color_palette(\"Set1\", n_colors=2)[::-1]\n"], "sample_1055": ["compilation error", "    def test_big_bg():\n        p, q = 1000000007, 1000000009\n        pri = bg_private_key(p, q)\n        pub = bg_public_key(p, q)\n        msg = 12345678901234567890\n        enc = encipher_bg(msg, pub)\n        dec = decipher_bg(enc, pri)\n        assert dec == msg\n", "compilation error", "    def test_decipher_bg():\n        ps = [67, 7, 71, 103, 11, 43, 107, 47,\n              79, 19, 83, 23, 59, 127, 31]\n        qs = qs = [7, 71, 103, 11, 43, 107, 47,\n                   79, 19, 83, 23, 59, 127, 31, 67]\n        messages = [\n            0, 328, 343, 148, 1280, 758, 383,\n            724, 603, 516, 766, 618, 186,\n        ]\n\n        for p, q in zip(ps, qs):\n          pri = bg_private_key(p, q)\n          for msg in messages:\n                pub = bg_public_key(p, q)\n                enc = encipher_bg(msg, pub)\n                dec = decipher_bg(enc, pri)\n\n                assert dec == msg\n", "compilation error"], "sample_557": ["def test_subfigure_scatter_size_tight():\n    fig = plt.figure(constrained_layout=True, figsize=(6, 4))\n    gs = fig.add_gridspec(1, 2)\n    ax0 = fig.add_subplot(gs[1])\n    ax0.scatter([1, 2, 3], [1, 2, 3], s=30, marker='s')\n    ax0.scatter([3, 4, 5], [1, 2, 3], s=[20, 30, 40], marker='s')\n\n    sfig = fig.add_subfigure(gs[0])\n    axs = sfig.subplots(1, 2)\n    for ax in [ax0, axs[0]]:\n        ax.scatter([1, 2, 3], [1, 2, 3], s=30, marker='s', color='r')\n        ax.scatter([3, 4, 5], [1, 2, 3], s=[20, 30, 40], marker='s', color='g')\n", "    def test_set_size_inches(self):\n        fig, ax = plt.subplots(figsize=(4, 5))\n        # Assert initial size\n        assert fig.get_size_inches() == (4, 5)\n        # Set new size\n        fig.set_size_inches((6, 7))\n        # Assert new size\n        assert fig.get_size_inches() == (6, 7)\n", "    def test_figure_get_children():\n        fig, ax = plt.subplots()\n        ax.plot([1, 2, 3], [4, 5, 6])\n        children = fig.get_children()\n        assert ax in children\n        assert isinstance(children[0], Axes)\n\n", "    def test_add_subplot_projection_after_gridspec():\n        fig = plt.figure()\n        gs = fig.add_gridspec(1, 1)\n        ax = fig.add_subplot(gs[0, 0], projection='polar')\n        assert isinstance(ax, PolarAxes)\n", "def test_add_axes_overlap():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1])\n    ax2 = fig.add_axes([0.5, 0.5, 0.5, 0.5])\n    ax2.plot([0, 1], [1, 0])\n"], "sample_980": ["def test_order_with_empty_cycle():\n    p = Permutation(Cycle(1, 2), Cycle())\n    assert p.order() == 2\n", "compilation error", "compilation error", "compilation error", "def test_cycle_form():\n    p = Permutation([2, 5, 1, 6, 3, 0, 4])\n    assert p.cyclic_form == [[0, 5, 1, 2], [3, 6, 4]]\n    assert p.full_cyclic_form == [[0, 5, 1, 2], [3, 6, 4]]\n    q = Permutation([0, 2, 1, 3, 4, 5, 6])\n    assert q.cyclic_form == [[0, 2, 1, 3], [4, 5, 6]]\n    q = Permutation([1, 0, 2, 3, 4, 5, 6])\n    assert q.cyclic_form == [[0, 1], [2, 3], [4, 5, 6]]\n     \n"], "sample_448": ["    def test_opclasses_database_constraint(self):\n        UniqueConstraintOpclasses.objects.create(name=\"p1\", color=\"red\")\n        with self.assertRaises(IntegrityError):\n            UniqueConstraintOpclasses.objects.create(name=\"p1\", color=\"blue\")\n", "    def test_opclasses_with_include(self):\n        msg = (\n            \"UniqueConstraint.opclasses cannot be used with include fields. Use \"\n            \"django.contrib.postgres.indexes.OpClass() instead.\"\n        )\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                fields=[\"name\"],\n                name=\"test_opclass_include\",\n                opclasses=[\"text_pattern_ops\"],\n                include=[\"color\"],\n            )\n", "    def test_partitioned_model(self):\n        part_model = models. partitioned_model(\n            PartitionConstraintProduct,\n             partition_by=[\"color\"],\n             name=\"partitioned_product\"\n        )\n        part_model.objects.create(name=\"p1\", color=\"red\")\n        with self.assertRaises(IntegrityError):\n            part_model.objects.create(name=\"p2\", color=\"red\")\n", "    def test_deferred_constraint_validation(self):\n        obj = UniqueConstraintDeferrable.objects.create(name=\"p1\", shelf=\"front\")\n        obj.name = \"p2\"\n        with self.assertNumQueries(0):\n            obj.save()\n        with self.assertRaises(IntegrityError):\n            with atomic():\n                obj.save()\n\n", "    def test_database_constraint_with_condition_and_include(self):\n        UniqueConstraintConditionIncludeProduct.objects.create(name=\"p1\")\n        UniqueConstraintConditionIncludeProduct.objects.create(name=\"p2\")\n        with self.assertRaises(IntegrityError):\n            UniqueConstraintConditionIncludeProduct.objects.create(\n                name=\"p1\", color=\"blue\"\n            )\n"], "sample_1025": ["compilation error", "compilation error", "compilation error", "def test_python_code_printer_complex():\n    p = PythonCodePrinter()\n    assert p.doprint(1 + 2j) == '1 + 2j'\n    assert p.doprint(2 - 3j) == '2 - 3j'\n", "compilation error"], "sample_87": ["    def test_should_stop_returns_false_with_no_errors(self):\n        with mock.patch.object(autoreload, '_error_files', []):\n            self.assertFalse(self.reloader.should_stop())\n", "    def test_should_stop(self, mocked_stat):\n        mocked_stat.side_effect = OSError\n        self.assertTrue(self.reloader.should_stop)\n", "    def test_notify_file_changed_calls_callback(self):\n        mock_callback = mock.MagicMock()\n        self.reloader.notify_file_changed = mock_callback\n        self.reloader.watch_file(self.existing_file)\n        self.increment_mtime(self.existing_file)\n        self.reloader.tick()\n        mock_callback.assert_called_once_with(self.existing_file)\n", "    def test_watch_file_on_nonexistent_path(self, mocked_os):\n        mocked_os.path.exists.return_value = False\n\n        with self.assertRaises(ValueError):\n            self.reloader.watch_file(self.nonexistent_file)\n", "    def test_notify_file_changed_not_watched(self):\n        self.reloader.notify_file_changed(self.nonexistent_file)\n        self.assertFalse(self.reloader.should_stop.is_set())\n"], "sample_777": ["def test_gradient_boosting_init_with_base_estimator():\n    # Check that GradientBoostingRegressor can be initialized with a base\n    # estimator and that the base estimator's parameters are passed correctly\n\n    class DummyEstimatorWithParams(BaseEstimator):\n            self.param1 = param1\n            self.param2 = param2\n\n            pass\n\n            pass\n\n    X, y = make_regression()\n    init_estimator = DummyEstimatorWithParams(param1=2, param2='foo')\n    gb = GradientBoostingRegressor(init=init_estimator)\n\n    gb.fit(X, y)\n    assert gb.estimator_.param1 == 2\n    assert gb.estimator_.param2 == 'foo'\n", "def test_gradient_boosting_empty_features():\n    \"\"\"Check that GradientBoostingClassifier and Regressor work with empty\n    feature datasets.\n\n    This test ensures that the code handles edge cases where the feature set is empty,\n    which can happen in certain data preprocessing scenarios.\n    \"\"\"\n    X_empty = np.empty((10, 0))\n    y = np.zeros(10)\n\n    for Estimator in [GradientBoostingClassifier, GradientBoostingRegressor]:\n        est = Estimator(random_state=0)\n        est.fit(X_empty, y)\n\n        # Check if it predicts correctly on an empty feature set\n        predictions = est.predict(X_empty)\n\n        # For regression, ensure predictions are consistent\n        if Estimator == GradientBoostingRegressor:\n            assert np.all(predictions == predictions[0])\n\n        # For classification, ensure no errors during prediction\n        else:\n            assert len(predictions) == 10\n", "    def test_gradient_boosting_with_categorical_features():\n        # Check that GradientBoostingRegressor works with categorical features\n        X = np.array([[1, 'a'], [2, 'b'], [3, 'a'], [4, 'b']])\n        y = np.array([1, 2, 3, 4])\n        enc = OneHotEncoder(handle_unknown='ignore')\n        X_enc = enc.fit_transform(X[:, 1].reshape(-1, 1)).toarray()\n        X = np.concatenate((X[:, :1], X_enc), axis=1)\n        gb = GradientBoostingRegressor(random_state=0).fit(X, y)\n        gb.predict(X)\n", "    def test_gradient_boosting_classifier_with_class_weight():\n        # Test that class_weight parameter works correctly for GradientBoostingClassifier\n        X, y = make_classification(n_samples=100, n_features=2,\n                                      n_informative=2, n_redundant=0,\n                                      n_classes=3, random_state=0,\n                                      weights=[0.1, 0.4, 0.5])\n\n        gbc = GradientBoostingClassifier(n_estimators=100,\n                                        random_state=42).fit(X, y)\n        # Check if predicting probabilities sums to 1 for each sample\n        probabilities = gbc.predict_proba(X)\n        assert_allclose(np.sum(probabilities, axis=1), np.ones(len(X)))\n        \n        # Test with class_weight parameter\n\n        gbc_weighted = GradientBoostingClassifier(n_estimators=100, \n                                                class_weight='balanced',\n                                                random_state=42).fit(X, y)\n        \n        # Check if predicting probabilities sums to 1 for each sample\n        probabilities_weighted = gbc_weighted.predict_proba(X)\n        assert_allclose(np.sum(probabilities_weighted, axis=1),\n                        np.ones(len(X))) \n\n", "compilation error"], "sample_102": ["    def test_difference_empty_result(self):\n        qs1 = Number.objects.filter(pk__in=[])\n        qs2 = Number.objects.all()\n        self.assertEqual(qs1.difference(qs2).count(), 0)\n", "def test_empty_result_union(self):\n        qs1 = Number.objects.filter(num=100)\n        qs2 = Number.objects.filter(num=200)\n        self.assertNumbersEqual(qs1.union(qs2), [])\n", "    def test_empty_union_with_distinct(self):\n        qs1 = Number.objects.filter(pk__in=[])\n        qs2 = Number.objects.filter(pk__in=[])\n        self.assertEqual(len(list(qs1.union(qs2, all=True))), 0)\n\n", "    def test_union_select_related(self):\n        ReservedName.objects.create(name='rn1', order=1)\n        qs1 = Number.objects.select_related('reserved_names').filter(num=1)\n        qs2 = Number.objects.select_related('reserved_names').filter(num=2)\n        self.assertEqual(qs1.union(qs2).count(), 2)\n", "    def test_empty_qs_difference(self):\n        qs1 = Number.objects.filter(pk__in=[])\n        qs2 = Number.objects.all()\n        self.assertEqual(qs1.difference(qs2).count(), 0)\n"], "sample_1087": ["    def test_fateman_poly_F_3():\n        f, g, h = fateman_poly_F_3(1)\n        F, G, H = dmp_fateman_poly_F_3(1, ZZ)\n\n        assert [ t.rep.rep for t in [f, g, h] ] == [F, G, H]\n\n        f, g, h = fateman_poly_F_3(3)\n        F, G, H = dmp_fateman_poly_F_3(3, ZZ)\n\n        assert [ t.rep.rep for t in [f, g, h] ] == [F, G, H]\n", "compilation error", "compilation error", "def test_fateman_poly_F_3():\n    f, g, h = fateman_poly_F_3(1)\n    F, G, H = dmp_fateman_poly_F_3(1, ZZ)\n\n    assert [ t.rep.rep for t in [f, g, h] ] == [F, G, H]\n\n    f, g, h = fateman_poly_F_3(3)\n    F, G, H = dmp_fateman_poly_F_3(3, ZZ)\n\n    assert [ t.rep.rep for t in [f, g, h] ] == [F, G, H]\n\n", "compilation error"], "sample_763": ["    def __len__(self):\n            return 1\n", "    def __len__(self):\n            return 1\n", "    def __len__(self):\n        return 1\n", "compilation error", "compilation error"], "sample_30": ["def test_empty_table():\n    votable = parse(get_pkg_data_filename(\"data/empty_table.xml\"))\n    table = votable.get_first_table()\n    assert table.array.size == 0\n", "def test_empty_table():\n    votable = parse(get_pkg_data_filename(\"data/empty_table.xml\"))\n    table = votable.get_first_table()\n    assert len(table.array) == 0\n", "def test_mask_array():\n    votable = parse(get_pkg_data_filename(\"data/regression.xml\"))\n    table = votable.get_first_table()\n\n    mask_array = table.array.mask\n\n    assert isinstance(mask_array, np.ndarray)\n    assert mask_array.dtype == np.dtype([('filename', '?'), ('int', '?'), ('float', '?'), ('doublearray', '?', (2,)), ('bit', '?'), ('bitarray', '?', (3,)), ('bitarray2', '?', (4,)), ('bool', '?'), ('boolArray', '?', (2,)), ('nulls', '?'), ('nulls_array', '?', (2, 2))])\n", "def test_empty_array():\n    votable = parse(get_pkg_data_filename(\"data/empty_array.xml\"))\n\n    table = votable.get_first_table()\n    assert len(table) == 0\n", "compilation error"], "sample_46": ["    def test_foreign_key_pragma(self):\n        # Tests that setting PRAGMA foreign_keys=OFF doesn't break UUID primary key lookups.\n        with connection.cursor() as cursor:\n            cursor.execute(\"PRAGMA foreign_keys=OFF\")\n        PrimaryKeyUUIDModel.objects.create()\n        loaded = PrimaryKeyUUIDModel.objects.get()\n        self.assertIsInstance(loaded.pk, uuid.UUID)\n\n", "    def test_delete_object_with_related_model_instances(self):\n        u1 = PrimaryKeyUUIDModel.objects.create()\n        RelatedToUUIDModel.objects.create(uuid_fk=u1)\n        u1.delete()\n        with self.assertRaises(RelatedObjectDoesNotExist):\n            RelatedToUUIDModel.objects.get()\n", "    def test_filter_with_uuid_field_lookup(self):\n        UUIDModel.objects.create(field=uuid.uuid4())\n        UUIDModel.objects.create(field=uuid.uuid4())\n        with transaction.atomic():\n            instance = UUIDModel.objects.create(field=uuid.uuid4())\n            UUIDModel.objects.filter(field=instance.field).update(field=uuid.uuid4())\n        loaded = UUIDModel.objects.get(field=instance.field)\n        self.assertIsNotNone(loaded)\n\n", "    def test_delete_with_related_model(self):\n        u1 = PrimaryKeyUUIDModel.objects.create()\n        r = RelatedToUUIDModel.objects.create(uuid_fk=u1)\n        u1.delete()\n        with self.assertRaises(RelatedObjectDoesNotExist):\n            r.refresh_from_db()\n", "    def test_ordering_by_uuid_field(self):\n        uuids = [\n            uuid.UUID('550e8400-e29b-41d4-a716-446655440000'),\n            uuid.UUID('00000000-0000-0000-0000-000000000000'),\n            uuid.UUID('ffffffff-ffff-ffff-ffff-ffffffffffff'),\n        ]\n        for uuid_val in uuids:\n            PrimaryKeyUUIDModel.objects.create(id=uuid_val)\n        ordered = PrimaryKeyUUIDModel.objects.order_by('id').values_list('id', flat=True)\n        self.assertEqual(list(ordered), uuids)\n"], "sample_196": ["    def test_sql_flush_tables(self):\n        with transaction.atomic():\n            Author.objects.create(name='Author 1')\n            Book.objects.create(author=Author.objects.get(name='Author 1'), title='Book 1')\n        \n        sql_list = connection.ops.sql_flush(no_style(), [Author._meta.db_table, Book._meta.db_table])\n\n        self.assertGreater(len(sql_list), 0)\n        self.assertIn('TRUNCATE TABLE \"backends_author\"', sql_list[0])\n        self.assertIn('TRUNCATE TABLE \"backends_book\"', sql_list[1])\n", "    def test_sql_flush_with_tables(self):\n        # Create a test table\n        with transaction.atomic():\n            Author.objects.create(name='Test Author', age=30)\n            Book.objects.create(title='Test Book', author=Author.objects.get(name='Test Author'))\n\n        sql_list = connection.ops.sql_flush(no_style(), tables=['backends_author', 'backends_book'])\n        self.assertEqual(len(sql_list), 2)\n        # Ensure DROP TABLE statements are generated for both tables\n        self.assertIn('DROP TABLE \"backends_author\"', sql_list[0])\n        self.assertIn('DROP TABLE \"backends_book\"', sql_list[1])\n\n", "    def test_sql_flush_with_tables(self):\n        with transaction.atomic():\n            Author.objects.create(name='Test Author')\n            Book.objects.create(title='Test Book', author=Author.objects.get(name='Test Author'))\n        sql_list = connection.ops.sql_flush(no_style(), ['backends'])\n        self.assertGreater(len(sql_list), 0)\n", "    def test_sql_flush_tables(self):\n        with transaction.atomic():\n            Author.objects.create(name='Test Author')\n            Book.objects.create(title='Test Book', author=Author.objects.get(name='Test Author'))\n            sql_list = connection.ops.sql_flush(no_style(), tables=['backends_author', 'backends_book'])\n            with self.assertRaises(DatabaseError):\n                self.execute_sql_flush(sql_list)\n", "    def test_prep_for_like_query(self):\n        self.assertEqual(self.ops.prep_for_like_query('%'), r'\\%')\n        self.assertEqual(self.ops.prep_for_like_query('_'), r'\\_')\n        self.assertEqual(self.ops.prep_for_like_query('abc%def'), r'abc\\%def')\n        self.assertEqual(self.ops.prep_for_like_query('abc_def'), r'abc\\_def')\n"], "sample_227": ["    def test_emptylistfieldfilter_related_field_with_default(self):\n        class BookAdminWithRelatedEmptyFieldFilter(ModelAdmin):\n            list_filter = (('author__email', EmptyFieldListFilter),)\n\n        modeladmin = BookAdminWithRelatedEmptyFieldFilter(Book, site)\n        request = self.request_factory.get('/')\n        request.user = self.alfred\n        changelist = modeladmin.get_changelist_instance(request)\n        filterspec = changelist.get_filters(request)[0][0]\n        self.assertEqual(filterspec.title, 'Verbose Author')\n        choices = list(filterspec.choices(changelist))\n        self.assertEqual(len(choices), 3)\n\n        self.assertEqual(choices[0]['display'], 'All')\n        self.assertIs(choices[0]['selected'], True)\n        self.assertEqual(choices[0]['query_string'], '?')\n\n        self.assertEqual(choices[1]['display'], 'Empty')\n        self.assertIs(choices[1]['selected'], False)\n        self.assertEqual(choices[1]['query_string'], '?author__email__isempty=1')\n\n        self.assertEqual(choices[2]['display'], 'Not empty')\n        self.assertIs(choices[2]['selected'], False)\n        self.assertEqual(choices[2]['query_string'], '?author__email__isempty=0')\n\n", "    def test_emptylistfieldfilter_related_field(self):\n        class AuthorAdminWithEmptyFieldListFilter(ModelAdmin):\n            list_filter = [('books', EmptyFieldListFilter)]\n\n        modeladmin = AuthorAdminWithEmptyFieldListFilter(Author, site)\n        request = self.request_factory.get('/')\n        request.user = self.alfred\n        msg = (\n            \"The list filter 'EmptyFieldListFilter' cannot be used with field \"\n            \"'books' which is a related manager and doesn't allow empty strings \"\n            \"and nulls.\"\n        )\n        with self.assertRaisesMessage(ImproperlyConfigured, msg):\n            modeladmin.get_changelist_instance(request)\n", "    def test_emptylistfieldfilter_invalid_field(self):\n        class BookAdminWithInvalidEmptyFieldListFilter(ModelAdmin):\n            list_filter = [('publication_date', EmptyFieldListFilter)]\n\n        modeladmin = BookAdminWithInvalidEmptyFieldListFilter(Book, site)\n        request = self.request_factory.get('/')\n        request.user = self.alfred\n        msg = (\n            \"The list filter 'EmptyFieldListFilter' only works on fields \"\n            \"with 'CharField', 'TextField', 'EmailField', 'SlugField', \"\n            \"'URLField', 'IPAddressField', 'GenericIPAddressField' or \"\n            \"one-to-many or many-to-many relations to these types.\"\n        )\n        with self.assertRaisesMessage(ImproperlyConfigured, msg):\n            modeladmin.get_changelist_instance(request)\n", "    def test_emptylistfieldfilter_with_default(self):\n        empty_description = Department.objects.create(code='EMPT', description='')\n        none_description = Department.objects.create(code='NONE', description=None)\n        department_admin = DepartmentAdminWithEmptyFieldListFilter(Department, site)\n\n        tests = [\n            # Testing default value when no query string is provided.\n            ({'description__isempty': None}, [empty_description, none_description, self.dev, self.design]),\n        ]\n        for query_string, expected_result in tests:\n            with self.subTest(query_string=query_string):\n                request = self.request_factory.get('/', query_string)\n                request.user = self.alfred\n                changelist = department_admin.get_changelist_instance(request)\n                queryset = changelist.get_queryset(request)\n                self.assertCountEqual(queryset, expected_result)\n", "    def test_listfilter_ordering_empty_list(self):\n        empty_description = Department.objects.create(code='EMPT', description='')\n        modeladmin = DepartmentAdminWithEmptyFieldListFilter(Department, site)\n        request = self.request_factory.get('/', {'description__isempty': '1'})\n        request.user = self.alfred\n        changelist = modeladmin.get_changelist_instance(request)\n        queryset = changelist.get_queryset(request)\n        self.assertEqual(\n            list(queryset.order_by('code')),\n            [empty_description, self.design]\n        )\n"], "sample_32": ["    def test_de_density_scale(self, cosmo, z):\n        \"\"\"Test :meth:`astropy.cosmology.w0wzCDM.de_density_scale`.\"\"\"\n        # TODO: these are just sanity checks because there isn't a great independent\n        #  test case... need to derive a more robust one\n        if z == 0:\n            assert cosmo.de_density_scale(z) == 1\n        else:\n            assert cosmo.de_density_scale(z) > 1\n", "    def test_de_density_scale(self, cosmo):\n        \"\"\"Test :meth:`astropy.cosmology.w0wzCDM.de_density_scale`.\"\"\"\n        z_array = np.linspace(0, 2, 5)\n\n        # Make sure the output has the right shape\n        result = cosmo.de_density_scale(z_array)\n        assert result.shape == z_array.shape\n\n        # Test some known values\n        assert u.allclose(cosmo.de_density_scale(0), 1)\n        assert u.allclose(cosmo.de_density_scale(1), (2 ** (3 * (1 + cosmo._w0 - cosmo._wz))) * np.exp(3 * cosmo._wz))\n", "    def test_de_density_scale(self, cosmo, z):\n        \"\"\"Test :meth:`astropy.cosmology.w0wzCDM.de_density_scale`.\"\"\"\n        expected = np.exp(3.0 * cosmo._wz * z) * (1 + z) ** (\n            3.0 * (1.0 + cosmo._w0 - cosmo._wz)\n        )\n\n        assert u.allclose(cosmo.de_density_scale(z), expected)\n", "    def test_de_density_scale(self, cosmo, z):\n        \"\"\"Test :meth:`astropy.cosmology.w0wzCDM.de_density_scale`.\"\"\"\n        expected = (1 + z) ** (3.0 * (1.0 + cosmo._w0 - cosmo._wz)) * np.exp(\n            3.0 * cosmo._wz * z\n        )\n        assert u.allclose(cosmo.de_density_scale(z), expected)\n", "    def test_de_density_scale(self, cosmo, z):\n        \"\"\"Test :meth:`astropy.cosmology.w0wzCDM.de_density_scale`.\"\"\"\n        expected = ([1.0, 1.1, 1.5625] *cosmo.Ode0).to(u.dimensionless_unscaled)\n        assert u.allclose(cosmo.de_density_scale(z), expected, rtol=1e-4)\n\n"], "sample_780": ["def test_lda_partial_fit_n_samples():\n    # Test LDA partial_fit with n_samples for total_samples\n    rng = np.random.RandomState(0)\n    n_components = rng.randint(3, 6)\n    X = np.random.randint(4, size=(20, 10))\n    lda = LatentDirichletAllocation(n_components=n_components,\n                                    total_samples=20, random_state=rng)\n    lda.partial_fit(X)\n    lda.partial_fit(X)\n    assert lda.n_iter_ == 2\n", "def test_lda_partial_fit_empty_doc():\n    \"\"\"Test LDA partial_fit on empty documents after initial fit.\"\"\"\n    lda = LatentDirichletAllocation(n_components=2, random_state=0)\n    X = np.array([[1, 0], [0, 1]])\n    lda.partial_fit(X)\n    \n    # Introduce an empty document\n    empty_doc = np.zeros((1, X.shape[1]))\n    lda.partial_fit(empty_doc)\n\n    assert lda.n_iter_ > 0\n    assert lda.components_.shape[0] == 2\n", "def test_lda_partial_fit_negative():\n    # test LDA partial_fit with negative input\n    rng = np.random.RandomState(0)\n    X = rng.randint(4, size=(20, 10))\n    lda = LatentDirichletAllocation(n_components=3, random_state=rng)\n    X_neg = np.array(X, dtype=np.float64)\n    X_neg[0, 0] = -1.0\n    assert_raises_regexp(ValueError, r\"^Negative values in data passed\",\n                         lda.partial_fit, X_neg)\n", "def test_lda_partial_fit_empty_doc():\n    \"\"\"Test LDA partial_fit with empty documents.\"\"\"\n    n_components = 3\n    lda = LatentDirichletAllocation(n_components=n_components,\n                                    learning_offset=10., total_samples=100,\n                                    random_state=0)\n\n    # First partial fit with some documents\n    X_1 = np.random.randint(4, size=(20, 10))\n    lda.partial_fit(X_1)\n\n    # Second partial fit with empty document\n    X_2 = np.zeros((5, 10))\n    lda.partial_fit(X_2)\n\n    # Check if the model still works as expected\n    X_3 = np.random.randint(4, size=(10, 10))\n    lda.partial_fit(X_3)\n    assert lda.components_.shape == (n_components, 10)\n", "def test_lda_init():\n    \"\"\"\n    Test LDA initialization with custom prior parameters and initializations.\n    \"\"\"\n    n_components = 5\n    n_features = 10\n    doc_topic_prior = np.full(n_components, 0.1)\n    topic_word_prior = np.full(n_features, 0.01)\n    initial_doc_topic = np.random.rand(10, n_components)\n    initial_topic_word = np.random.rand(n_components, n_features)\n    lda = LatentDirichletAllocation(\n        n_components=n_components,\n        doc_topic_prior=doc_topic_prior,\n        topic_word_prior=topic_word_prior,\n        random_state=0,\n        init='custom',\n        n_jobs=1,\n        evaluate_every=1,\n        max_iter=1,\n        learning_method='batch',\n        learning_offset=10.,\n        total_samples=100\n    )\n\n    lda.components_ = initial_topic_word\n    lda.fit(np.random.randint(4, size=(10, 10)))\n\n    assert_array_equal(lda.doc_topic_prior, doc_topic_prior)\n    assert_array_equal(lda.components[:], initial_topic_word)\n\n\n"], "sample_651": ["    def test_deprecated_call_re_emit(self, warning_type, mode) -> None:\n            warnings.warn(warning_type(\"hi\"))\n            return 10\n\n        if mode == \"call\":\n            with pytest.deprecated_call(f):\n                warnings.warn(warning_type(\"hi\"))\n        else:\n            with pytest.deprecated_call():\n                with pytest.deprecated_call():\n                    f()\n                    warnings.warn(warning_type(\"hi\"))\n", "    def test_warns_with_parametrize(self, pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n            import warnings\n\n            @pytest.mark.parametrize('warn_type', [UserWarning, RuntimeWarning])\n                with pytest.warns(warn_type):\n                    warnings.warn('My warning!', warn_type)\n        \"\"\"\n        )\n        result = pytester.runpytest()\n        result.assert_outcomes(passed=2)\n", "    def test_warns_with_filterwarnings(self, pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import warnings\n            import pytest\n\n            @pytest.mark.filterwarnings(\"ignore:some warning\")\n                warnings.warn(\"some warning\", UserWarning)\n                warnings.warn(\"other warning\", UserWarning)\n\n                with pytest.warns(UserWarning):\n                    warnings.warn(\"other warning\", UserWarning)\n\n        \"\"\"\n        )\n        result = pytester.runpytest()\n        result.stdout.fnmatch_lines([\"*2 passed in*\"])\n", "    def test_warns_context_manager_with_param_args(self) -> None:\n            warnings.warn(\"a warning\", warnclass)\n\n        with pytest.warns(UserWarning):\n            f(UserWarning)\n        with pytest.raises(pytest.fail.Exception):\n            with pytest.warns(RuntimeWarning):\n                f(UserWarning)\n\n", "    def test_warns_with_no_warning_types(self) -> None:\n        with pytest.raises(TypeError) as excinfo:\n            with pytest.warns():\n                pass\n        assert \"expected_warning\" in str(excinfo.value)\n"], "sample_594": ["    def test_summarize_attr_unicode(self):\n        attr_val = \"\u3053\u308c\u306f\u65e5\u672c\u8a9e\u306e\u6587\u5b57\u5217\u3067\u3059\"\n        result = formatting.summarize_attr(\"key\", attr_val)\n        assert result == f\"    key: \u3053\u308c\u306f\u65e5\u672c\u8a9e\u306e\u6587\u5b57\u5217\u3067\u3059\"\n", "    def test_array_repr_with_attrs():\n        data = xr.DataArray(\n            np.arange(12).reshape(3, 4),\n            dims=(\"x\", \"y\"),\n            attrs={\"long_attr\": \"This is a long attribute description\"},\n        )\n\n        expected_repr = dedent(\n            \"\"\"\\\n        <xarray.DataArray (x: 3, y: 4)>\n        array([[ 0,  1,  2,  3],\n               [ 4,  5,  6,  7],\n               [ 8,  9, 10, 11]])\n        Dimensions without coordinates: x, y\n        Attributes:\n            long_attr: This is a long attribute description\"\"\"\n        )\n\n        actual = repr(data)\n        assert actual == expected_repr\n\n", "    def test_short_numpy_repr_complex(self):\n        array = np.array([[1 + 2j, 3 + 4j], [5 + 6j, 7 + 8j]])\n        expected = (\n            \"array([[1.+2.j, 3.+4.j],\\n\"\n            \"       [5.+6.j, 7.+8.j]])\"\n        )\n        actual = formatting.short_numpy_repr(array)\n        assert actual == expected\n", "    def test_short_data_repr_dask_array(self):\n        import dask.array as da\n\n        da_array = da.random.random((500), chunks=(100,))\n\n        actual = formatting.short_data_repr(da_array)\n\n        assert \"dask.array\" in actual\n        assert len(actual) < 100\n", "    def test_format_timestamp_out_of_bounds(self):\n\n        with pytest.raises(ValueError):\n            formatting.format_timestamp(datetime(1000, 12, 1))\n\n        with pytest.raises(ValueError):\n            formatting.format_timestamp(datetime(2500, 12, 1))\n"], "sample_302": ["    def test_runshell(self):\n        # Mocking psql command execution\n        with mock.patch('django.db.backends.postgresql.client.subprocess.Popen') as mock_popen:\n            mock_popen.return_value.communicate.return_value = ('', '')\n            self.client.runshell(['some', 'parameters'])\n            # Ensure that runshell calls Popen with the expected arguments\n            mock_popen.assert_called_once()\n", "    def test_runshell(self):\n        with mock.patch('django.db.backends.postgresql.client.DatabaseClient.settings_to_cmd_args_env') as mock_settings_to_cmd_args_env:\n            mock_settings_to_cmd_args_env.return_value = (['psql'], None)\n            with mock.patch('subprocess.Popen') as mock_Popen:\n                self.client.runshell([])\n                mock_Popen.assert_called_once_with(['psql'], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, close_fds=True)\n", "    def test_runshell(self):\n        with mock.patch('django.db.backends.postgresql.client.DatabaseClient.settings_to_cmd_args_env',\n                       return_value=([], {})):\n            with mock.patch('subprocess.Popen') as mock_popen:\n                self.client.runshell('')\n                mock_popen.assert_called_once()\n", "    def test_runshell(self):\n        with mock.patch('django.db.backends.postgresql.base.DatabaseClient.settings_to_cmd_args_env') as mocked_settings_to_cmd_args_env, \\\n                mock.patch('django.db.backends.postgresql.base.DatabaseClient.shell_command') as mocked_shell_command:\n            mocked_settings_to_cmd_args_env.return_value = ('psql', None)\n            client = DatabaseClient(connection=connection)\n            client.runshell([])\n            mocked_settings_to_cmd_args_env.assert_called_once_with(connection.settings_dict, [])\n            mocked_shell_command.assert_called_once_with('psql') \n", "    def test_runshell(self):\n        with mock.patch('django.db.backends.postgresql.base.DatabaseClient.settings_to_cmd_args_env') as mock_settings_to_cmd_args_env:\n            mock_settings_to_cmd_args_env.return_value = (['psql'], {})\n            with mock.patch('subprocess.Popen') as mock_popen:\n                self.client.runshell(['arg1', 'arg2'])\n                mock_popen.assert_called_once_with(\n                    ['psql', 'arg1', 'arg2'],\n                    stdin=subprocess.PIPE,\n                    stdout=subprocess.PIPE,\n                    stderr=subprocess.PIPE,\n                    env={}\n                )\n"], "sample_831": ["def test_plot_tree_errors():\n    clf = DecisionTreeClassifier(max_depth=3, random_state=2)\n    \n    # Check not-fitted decision tree error\n    assert_raises(NotFittedError, plot_tree, clf)\n    \n    clf.fit(X, y)\n    \n    # Check error for invalid feature_names\n    assert_raises(IndexError, plot_tree, clf, feature_names=['a', 'b', 'c'])\n    \n    # Check if it raises ValueError for non-integer max_depth\n    assert_raises(TypeError, plot_tree, clf, max_depth=2.5)\n\n    # Check if it raises ValueError for negative max_depth\n    assert_raises(ValueError, plot_tree, clf, max_depth=-1)\n    \n    #Check if it raises ValueError for non-integer `filled`\n    assert_raises(TypeError, plot_tree, clf, filled=2.5)\n", "compilation error", "def test_plot_tree_unicode():\n    # Test plot_tree with unicode feature names\n    clf = DecisionTreeClassifier(max_depth=3, random_state=0)\n    clf.fit(X, y)\n    feature_names = ['\u4f60\u597d', '\u4e16\u754c']\n    nodes = plot_tree(clf, feature_names=feature_names)\n    assert len(nodes) == 3\n    assert nodes[0].get_text().startswith(\"\u4f60\u597d <= 0.0\")\n    assert nodes[1].get_text().startswith(\"samples = 3\")\n    assert nodes[2].get_text().startswith(\"samples = 3\")\n", "    def test_plot_tree_errors():\n        clf = DecisionTreeClassifier(max_depth=3, random_state=0)\n\n        # Check not-fitted decision tree error\n        assert_raises(NotFittedError, plot_tree, clf)\n\n        clf.fit(X, y)\n\n        # Check if it errors when length of feature_names\n        # mismatches with number of features\n        assert_raises(ValueError, plot_tree, clf, feature_names=[\"a\"])\n        assert_raises(ValueError, plot_tree, clf, feature_names=[\"a\", \"b\", \"c\"])\n\n        # Check error when argument is not an estimator\n        assert_raises(TypeError, plot_tree, clf.fit(X, y).tree_)\n\n\n\n", "    def test_plot_tree_errors():\n\n        # Check for error when estimator is not fitted\n        clf = DecisionTreeClassifier()\n        assert_raises(NotFittedError, plot_tree, clf)\n\n        # Check for errors if the estimator is not a tree\n        clf = LogisticRegression()\n        clf.fit(X, y)\n        assert_raises(TypeError, plot_tree, clf)\n\n        # Check error when feature names is not a list\n        clf.fit(X, y)\n        assert_raises(TypeError, plot_tree, clf, feature_names=None)\n        # Check error when feature names length does not match number of features\n        assert_raises(ValueError, plot_tree, clf, feature_names=[\"a\"])\n\n"], "sample_3": ["def test_empty_list_in_variable_length():\n    \"\"\"Test that an empty list in a variable length array is properly handled.\n    The ECSV standard defines an empty list as an empty string.\n\n    Note: This test case was added to address a bug where empty lists were\n        sometimes written as '[]' instead of an empty string.\n\n    \"\"\"\n    t = Table()\n    t['arr'] = [\n        [[1, 2], [3, 4]],\n        [],\n        [[5, 6]],\n    ]\n    out = StringIO()\n    t.write(out, format='ascii.ecsv')\n    t2 = Table.read(out.getvalue(), format='ascii.ecsv')\n    assert t2['arr'][1] == []\n", "compilation error", "def test_empty_string_column():\n    \"\"\"Test reading/writing a column with empty strings.\"\"\"\n    t = Table()\n    t['a'] = ['', 'hello', 'world', '']\n    out = StringIO()\n    t.write(out, format='ascii.ecsv')\n    t2 = Table.read(out.getvalue(), format='ascii.ecsv')\n    assert t2.colnames == t.colnames\n    assert np.all(t2['a'] == t['a'])\n", "def test_write_masked_array_with_dtype():\n    t = Table()\n    data = np.ma.array([1, 2, 3], mask=[False, True, False], dtype=np.int64)\n    t['a'] = data\n    out = StringIO()\n    t.write(out, format='ascii.ecsv', serialize_method='data_mask')\n\n    t2 = Table.read(out.getvalue())\n    assert np.all(t2['a'] == data)\n    assert np.all(t2['a'].mask == data.mask)\n", "def test_empty_ecsv_file():\n    \"\"\"Test reading an empty ECSV file.\"\"\"\n    filename = os.path.join(TEST_DIR, 'data', 'empty.ecsv')\n    t = Table.read(filename, format='ascii.ecsv')\n    assert t == Table()\n"], "sample_626": ["    def test_from_variables_with_coords(self) -> None:\n        data = np.array([1, 2, 3], dtype=np.float32)\n        var = xr.Variable(\"x\", data, coords={\"y\": [4, 5, 6]})\n        expected = PandasIndex(pd.Index(data, name=\"x\"), \"x\", coord_dtype=np.float32)\n        actual = PandasIndex.from_variables({\"x\": var}, {})\n        assert actual.equals(expected)\n", "    def test_safe_cast_to_index_errors(self):\n        with pytest.raises(TypeError):\n            safe_cast_to_index(object())\n        with pytest.raises(TypeError):\n            safe_cast_to_index({\"key\": \"value\"})\n\n", "def test_PandasMultiIndex_stack_empty_levels():\n    prod_vars = {\"x\": xr.Variable(\"x\", pd.Index([], dtype=\"int64\")),\n                 \"y\": xr.Variable(\"y\", pd.Index([1, 2, 3]))}\n    index = PandasMultiIndex.stack(prod_vars, \"z\")\n    assert index.dim == \"z\"\n    assert index.index.names == [\"x\", \"y\"]\n    np.testing.assert_array_equal(index.index.codes, [[], [0, 1, 2]])\n", "    def test_indexes_from_variables_error(self):\n        variables = {\n            \"x\": Variable(\"x\", [0, 1, 2]),\n            \"y\": Variable(\"y\", [3, 4, 5]),\n        }\n        with pytest.raises(ValueError, match=r\"must specify dims.*\"):\n            Indexes.from_variables(variables, dims=None)\n", "    def test_from_variables_duplicate_indexes(self):\n        pd_idx = pd.Index([1, 2, 3])\n        var1 = xr.Variable(\"x\", pd_idx)\n        var2 = xr.Variable(\"x\", pd_idx)\n        \n        with pytest.raises(ValueError, match=r\"duplicate index found.*\"):\n            Indexes.from_variables({\"x\": var1, \"x\": var2}, {})\n"], "sample_561": ["def test_marker_size(marker, s, expected_size):\n    assert marker.get_size(s) == expected_size\n", "def test_marker_scaled_xy(marker, sx, sy, expected):\n    new_marker = marker.scaled(sx, sy)\n    assert new_marker is not marker\n    assert new_marker.get_user_transform() == expected\n    assert marker._user_transform is not new_marker._user_transform\n", "    def test_marker_scaled_invalid():\n        marker = markers.MarkerStyle(\"o\")\n        with pytest.raises(ValueError):\n            new_marker = marker.scaled(0)\n        with pytest.raises(ValueError):\n            new_marker = marker.scaled(1, 0)\n\n", "    def test_marker_get_path(self, marker):\n        path = marker.get_path()\n        assert isinstance(path, Path)\n", "def test_marker_is_filled(marker, expected):\n    assert marker.is_filled() == expected\n"], "sample_348": ["    def test_actions_are_callable(self):\n        class BandAdmin(ModelAdmin):\n            actions = 'not_callable'\n\n        self.assertIsInvalid(\n            BandAdmin, Band,\n            \"The value of 'actions' must be a list or tuple of callables.\",\n            id='admin.E131',\n        )\n\n", "        def action2(modeladmin, request, queryset):\n            pass\n", "        def action2(modeladmin, request, queryset):\n            pass\n", "        def action2(modeladmin, request, queryset):\n            pass\n", "        def action2(modeladmin, request, queryset):\n            pass\n"], "sample_674": ["def test_node_repr_failure_tbstyle(testdir):\n    items = testdir.getfunction(\"def test_func(): raise ValueError('hello')\")\n    items[0].ihook.pytest_exception_caught = lambda excinfo: excinfo\n    excinfo = items[0].runtest()\n    assert excinfo.errisinstance(ValueError)\n\n    repr_failure = items[0].repr_failure(excinfo, style=\"short\")\n    assert \"ValueError: hello\" in repr_failure\n    repr_failure = items[0].repr_failure(excinfo, style=\"long\")\n    assert \"ValueError: hello\" in repr_failure\n", "def test_node_keywords(testdir):\n    items = testdir.getitems(\n        \"\"\"\n        import pytest\n        \n        @pytest.mark.foo\n            pass\n    \"\"\"\n    )\n    assert len(items[0].keywords) == 1\n    assert \"foo\" in items[0].keywords\n\n", "def test_nodeid_handles_absolute_paths():\n    node = nodes.Node(\n        name=\"test\",\n        parent=None,\n        config=None,\n        session=None,\n        fspath=py.path.local(\"/absolute/path/to/test.py\"),\n    )\n    assert node._nodeid == \"/absolute/path/to/test.py::test\"\n", "def test_repr_failure_short_style(testdir):\n    testdir.create_file(\"test_example.py\", \"\"\"\n            assert 0 == 1\n    \"\"\")\n    result = testdir.runpytest()\n    result.assert_outcomes(failed=1)\n    for rep in result.getreports(\"failed\"):\n        assert rep.shortrepr != \"\"\n        assert \"AssertionError\" in rep.shortrepr\n\n\n\n", "    def test_get_fslocation_from_item(testdir):\n        items = testdir.getitems(\"\"\"\n                pass\n        \"\"\")\n        item = items[0]\n        assert item.obj is not None\n        location = nodes.get_fslocation_from_item(item)\n        assert location == (testdir.getpath(\"test_\"), 2)\n"], "sample_902": ["compilation error", "    def test_pipeline_memory_with_duplicate_steps():\n        # Test that an error is raised when there are duplicate steps in the\n        # pipeline\n        iris = load_iris()\n        X = iris.data\n        y = iris.target\n        cachedir = mkdtemp()\n        try:\n            memory = Memory(cachedir=cachedir, verbose=10)\n            pipe = Pipeline([\n                ('transf', DummyTransf()),\n                ('transf', DummyTransf()),\n                ('svc', SVC())],\n                memory=memory)\n\n            # an error should be raised while fitting\n            assert_raises_regex(ValueError, \"Found 2 steps with name \"\n                                \"transf. Pipeline steps must have unique names.\",\n                                pipe.fit, X, y)\n        finally:\n            shutil.rmtree(cachedir)\n", "    def test_pipeline_memory_different_cachedir():\n        iris = load_iris()\n        X = iris.data\n        y = iris.target\n        cachedir1 = mkdtemp()\n        cachedir2 = mkdtemp()\n        try:\n            memory1 = Memory(cachedir=cachedir1, verbose=10)\n            memory2 = Memory(cachedir=cachedir2, verbose=10)\n            # Test with Transformer + SVC\n            clf = SVC(probability=True, random_state=0)\n            transf = DummyTransf()\n            pipe1 = Pipeline([('transf', transf), ('svc', clf)],\n                             memory=memory1)\n            pipe2 = Pipeline([('transf', transf), ('svc', clf)],\n                             memory=memory2)\n\n            # Memoize the transformer in pipe1\n            pipe1.fit(X, y)\n            # Check that pipe2 does not have the cached result\n            pipe2.fit(X, y)\n            assert_array_equal(pipe1.predict(X), pipe2.predict(X))\n            assert_false(np.array_equal(pipe1.named_steps['transf'].means_,\n                                       pipe2.named_steps['transf'].means_))\n        finally:\n            shutil.rmtree(cachedir1)\n            shutil.rmtree(cachedir2)\n", "compilation error", "    def test_pipeline_memory_empty_data():\n        # Test that pipeline with memory works even with empty data\n        iris = load_iris()\n        X = iris.data[:0]\n        y = iris.target[:0]\n        cachedir = mkdtemp()\n        try:\n            memory = Memory(cachedir=cachedir, verbose=10)\n            clf = SVC(probability=True, random_state=0)\n            transf = DummyTransf()\n            pipe = Pipeline([('transf', transf), ('svc', clf)],\n                            memory=memory)\n            pipe.fit(X, y)\n        finally:\n            shutil.rmtree(cachedir)\n"], "sample_826": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_827": ["compilation error", "compilation error", "compilation error", "def test_inplace_csr_row_normalize_l1():\n    X = np.array([[1., 2., 3.],\n                  [4., 5., 6.],\n                  [7., 8., 9.]], dtype=np.float64)\n    X_csr = sp.csr_matrix(X)\n\n    # Test l1 normalization\n    inplace_csr_row_normalize_l1(X_csr)\n    expected = np.array([[0.16666667, 0.33333333, 0.5],\n                         [0.26666667, 0.33333333, 0.4],\n                         [0.29166667, 0.29166667, 0.41666667]],  dtype=np.float64)\n\n    assert_allclose(X_csr.toarray(), expected)\n\n\n", "compilation error"], "sample_128": ["    def test_covering_index_with_descending_field(self):\n        index = Index(\n            name='covering_headline_idx',\n            fields=['-headline'],\n            include=['pub_date', 'published'],\n        )\n        with connection.schema_editor() as editor:\n            self.assertIn(\n                '(%s DESC) INCLUDE (%s, %s)' % (\n                    editor.quote_name('headline'),\n                    editor.quote_name('pub_date'),\n                    editor.quote_name('published'),\n                ),\n                str(index.create_sql(Article, editor)),\n            )\n            editor.add_index(Article, index)\n            with connection.cursor() as cursor:\n                constraints = connection.introspection.get_constraints(\n                    cursor=cursor, table_name=Article._meta.db_table,\n                )\n                self.assertIn(index.name, constraints)\n                self.assertEqual(\n                    constraints[index.name]['columns'],\n                    ['headline', 'pub_date', 'published'],\n                )\n            editor.remove_index(Article, index)\n            with connection.cursor() as cursor:\n                self.assertNotIn(index.name, connection.introspection.get_constraints(\n                    cursor=cursor, table_name=Article._meta.db_table,\n                ))\n", "    def test_index_name_with_long_table_name(self):\n        \"\"\"\n        Test index naming when table name exceeds the maximum allowed length.\n        \"\"\"\n        very_long_table_name = 'a' * 60\n        editor = connection.schema_editor()\n        with self.assertRaises(ValueError) as e:\n            editor._create_index_name(\n                table_name=very_long_table_name,\n                column_names=('c1',),\n                suffix='idx',\n            )\n        self.assertIn(\"Table name is too long\", str(e.exception))\n", "    def test_empty_include(self):\n        index = Index(\n            name='empty_include_idx',\n            fields=['headline'],\n            include=[],\n        )\n        with connection.schema_editor() as editor:\n            self.assertIn(\n                '(%s)' % editor.quote_name('headline'),\n                str(index.create_sql(Article, editor)),\n            )\n            editor.add_index(Article, index)\n            with connection.cursor() as cursor:\n                constraints = connection.introspection.get_constraints(\n                    cursor=cursor, table_name=Article._meta.db_table,\n                )\n                self.assertIn(index.name, constraints)\n                self.assertEqual(\n                    constraints[index.name]['columns'],\n                    ['headline'],\n                )\n            editor.remove_index(Article, index)\n            with connection.cursor() as cursor:\n                self.assertNotIn(index.name, connection.introspection.get_constraints(\n                    cursor=cursor, table_name=Article._meta.db_table,\n                ))\n", "    def test_include_invalid_field(self):\n        with self.assertRaises(ValueError):\n            Index(\n                name='invalid_include_idx',\n                fields=['headline'],\n                include=['nonexistent_field'],\n            )\n", "    def test_covering_index_with_opclass(self):\n        index = Index(\n            name='covering_headline_idx',\n            fields=['headline'],\n            opclasses=['varchar_pattern_ops'],\n            include=['pub_date', 'published'],\n        )\n        with connection.schema_editor() as editor:\n            self.assertIn(\n                '(%s varchar_pattern_ops) INCLUDE (%s, %s)' % (\n                    editor.quote_name('headline'),\n                    editor.quote_name('pub_date'),\n                    editor.quote_name('published'),\n                ),\n                str(index.create_sql(Article, editor)),\n            )\n            editor.add_index(Article, index)\n            with connection.cursor() as cursor:\n                constraints = connection.introspection.get_constraints(\n                    cursor=cursor, table_name=Article._meta.db_table,\n                )\n                self.assertIn(index.name, constraints)\n                self.assertEqual(\n                    constraints[index.name]['columns'],\n                    ['headline', 'pub_date', 'published'],\n                )\n            editor.remove_index(Article, index)\n            with connection.cursor() as cursor:\n                self.assertNotIn(index.name, connection.introspection.get_constraints(\n                    cursor=cursor, table_name=Article._meta.db_table,\n                ))\n"], "sample_1030": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_724": ["def test_imputation_empty_array():\n    X = np.array([])\n    X = X.reshape(0, 10)\n    imputer = Imputer(missing_values=0, strategy=\"mean\")\n    assert_raises(ValueError, imputer.fit, X)\n\n", "    def test_imputation_axis_and_sparse():\n        # Test the imputation with different axis and sparse matrices\n        X_dense = np.array([\n            [np.nan, 1, 2, np.nan],\n            [3, np.nan, 5, 6],\n            [7, 8, np.nan, 10],\n            [np.nan, 11, 12, 13],\n        ])\n\n        X_sparse = sparse.csc_matrix(X_dense)\n\n        for strategy in ['mean', 'median', 'most_frequent']:\n            for axis in [0, 1]:\n                imputer_dense = Imputer(strategy=strategy, axis=axis)\n                imputer_dense.fit(X_dense)\n                X_dense_imputed = imputer_dense.transform(X_dense)\n\n\n                imputer_sparse = Imputer(strategy=strategy, axis=axis)\n                imputer_sparse.fit(X_sparse)\n                X_sparse_imputed = imputer_sparse.transform(X_sparse)\n\n                assert_array_almost_equal(\n                    X_dense_imputed,\n                    X_sparse_imputed.toarray(),\n                    err_msg=\"Fail to impute data with axis = %s and\"\n                    \" strategy = %s\" % (axis, strategy)\n                )\n", "def test_imputation_sparse_zero_only():\n    # Test imputation on a sparse matrix with only zeros as missing values.\n    X = sparse.csr_matrix(((0, 0, 0, 0), (0, 0, 0, 0), (0, 0, 0, 0)),\n                         shape=(3, 4))\n\n    X_imputed_mean = sparse.csr_matrix(((0, 0, 0, 0), (0, 0, 0, 0),\n                                        (0, 0, 0, 0)), shape=(3, 4))\n    X_imputed_median = X_imputed_mean.copy()\n    X_imputed_most_frequent = X_imputed_mean.copy()\n\n    statistics_mean = [0, 0, 0, 0]\n    statistics_median = [0, 0, 0, 0]\n    statistics_most_frequent = [0, 0, 0, 0]\n\n    _check_statistics(X, X_imputed_mean, \"mean\", statistics_mean, 0)\n    _check_statistics(X, X_imputed_median, \"median\", statistics_median, 0)\n    _check_statistics(X, X_imputed_most_frequent, \"most_frequent\",\n                      statistics_most_frequent, 0)\n", "    def test_imputation_empty_data():\n        # Test imputation on empty data.\n        X_empty = np.array([])\n        X_empty = X_empty.reshape(0, 5)\n\n        imputer = Imputer(missing_values=\"NaN\", strategy=\"mean\")\n        assert_raises(ValueError, imputer.fit, X_empty)\n", "def test_imputation_axis():\n    # Test imputation with axis parameter.\n    X = np.array([\n        [np.nan, 1, 3],\n        [4, 5, np.nan],\n        [7, np.nan, 9],\n    ])\n\n    # Test axis=0 (column-wise imputation)\n    imputer = Imputer(missing_values='NaN', strategy='mean', axis=0)\n    X_imputed = imputer.fit_transform(X)\n    assert_array_almost_equal(X_imputed, np.array([\n        [4, 1, 3],\n        [4, 5, 6],\n        [7, 3, 9],\n    ]))\n\n    # Test axis=1 (row-wise imputation)\n    imputer = Imputer(missing_values='NaN', strategy='mean', axis=1)\n    X_imputed = imputer.fit_transform(X)\n    assert_array_almost_equal(X_imputed, np.array([\n        [np.nan, 1, 3],\n        [4, 5, np.nan],\n        [7, 6, 9],\n    ]))\n"], "sample_979": ["compilation error", "compilation error", "compilation error", "    def test_sparse_matrix():\n        M = SparseMatrix(2, 2, {(0, 1): 2, (1, 0): 3})\n        assert M[0, 1] == 2\n        assert M[1, 0] == 3\n", "compilation error"], "sample_541": ["def test_polygon_selector_clear(draw_bounding_box):\n    verts = [(50, 50), (150, 50), (50, 150)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    ax = get_ax()\n    tool = widgets.PolygonSelector(ax, onselect=noop,\n                                   draw_bounding_box=draw_bounding_box)\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n\n    assert len(tool.verts) == 3\n    assert tool._handles_artists\n    tool.clear()\n    assert len(tool.verts) == 0\n    assert not tool._handles_artists\n", "compilation error", "compilation error", "def test_polygon_selector_empty_polygon(draw_bounding_box):\n    verts = []\n    event_sequence = []\n    check_polygon_selector(event_sequence, verts, 0,\n                           draw_bounding_box=draw_bounding_box)\n", "def test_polygon_selector_box_delete_all(ax):\n    # Create a diamond shape\n    verts = [(20, 0), (0, 20), (20, 40), (40, 20)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[3]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n\n    # Create selector\n    tool = widgets.PolygonSelector(ax, onselect=noop, draw_bounding_box=True)\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n    # Delete all points\n    MouseEvent(\n        \"button_press_event\", ax.figure.canvas,\n        *ax.transData.transform((20, 20)), 3)._process()\n    MouseEvent(\n        \"button_release_event\", ax.figure.canvas,\n        *ax.transData.transform((20, 20)), 3)._process()\n\n    assert tool.verts == []\n"], "sample_786": ["    def test_sparse_matrix():\n        X = sp.csr_matrix(np.array([[0, 1, 2, 3], [4, 5, 6, 7]]))\n        est = KBinsDiscretizer(n_bins=3, encode='onehot-dense')\n\n        Xt = est.fit_transform(X)\n        assert_array_equal(OneHotEncoder(\n                           categories=[np.arange(i) for i in [3, 3, 3, 3]],\n                           sparse=False)\n                       .fit_transform(est._encoder.transform(sp.csr_matrix(X))), Xt)\n\n", "    def test_transform_all_samples_in_same_bin(self, encode):\n        X = np.full((10, 3), 2.5)\n        kbd = KBinsDiscretizer(n_bins=3, encode=encode)\n        Xt = kbd.fit_transform(X)\n        if 'onehot' in encode:\n            assert_array_equal(Xt.sum(axis=0), [10, 0, 0])\n        else:\n            assert_array_equal(Xt.flatten(), np.full(30, 1)) \n", "def test_transform_empty_input():\n    X = np.array([]).reshape(0, 4)\n    est = KBinsDiscretizer(n_bins=3).fit(X)\n    Xt = est.transform(X)\n    assert Xt.shape == (0, 4)\n", "    def test_inverse_transform_with_sparse_input():\n        X = np.array([[1, 2, 3],\n                      [4, 5, 6]])\n        est = KBinsDiscretizer(n_bins=3, encode='onehot-dense').fit(X)\n        Xt = est.transform(X)\n        assert sp.issparse(Xt)\n        Xinv = est.inverse_transform(Xt)\n        assert not np.any(np.isnan(Xinv))\n\n\n\n", "def test_fit_transform_with_all_same_values():\n    X = np.full((10, 3), 2.)\n    est = KBinsDiscretizer(n_bins=3)\n    Xt = est.fit_transform(X)\n    assert_array_equal(Xt, np.zeros((10, 3)))\n"], "sample_1183": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_563": ["def test_anchoredoffsetbox_set_properties(prop):\n    fig, ax = plt.subplots()\n    ab = AnchoredOffsetbox('upper left', child=DrawingArea(20, 20))\n    getattr(ab, f'set_{prop}')('test')\n    assert getattr(ab, prop) == 'test'\n\n", "    def test_paddedbox_draw_frame(self):\n        fig, ax = plt.subplots()\n        ta = TextArea(\"foo\")\n        pb = PaddedBox(ta, pad=5, draw_frame=False)\n        ab = AnchoredOffsetbox('upper left', child=pb)\n        ax.add_artist(ab)\n        fig.canvas.draw()\n        assert len(ax.patches) == 0\n\n        pb = PaddedBox(ta, pad=5, draw_frame=True)\n        ab = AnchoredOffsetbox('upper left', child=pb)\n        ax.add_artist(ab)\n        fig.canvas.draw()\n        assert len(ax.patches) == 1\n\n", "def test_anchoredoffsetbox_loc(location):\n    fig, ax = plt.subplots()\n    offsetbox = AnchoredOffsetbox(loc=location, child=DrawingArea(10, 10))\n    ax.add_artist(offsetbox)\n    fig.draw_without_rendering()\n", "def test_paddedbox_properties(pad, draw_frame):\n    ta = TextArea(\"foo\")\n    pb = PaddedBox(ta, pad=pad, draw_frame=draw_frame)\n    assert pb.pad == pad\n    assert pb.draw_frame == draw_frame\n", "def test_paddedbox_padding(pad, bbox_to_anchor):\n    fig, ax = plt.subplots()\n    ta = TextArea(\"foo\")\n    pb = PaddedBox(ta, pad=pad)\n    ab = AnchoredOffsetbox(bbox_to_anchor=bbox_to_anchor, child=pb)\n    ax.add_artist(ab)\n    fig.canvas.draw()\n"], "sample_1048": ["def test_parabola_intersection():\n    p1 = Point(0, 0)\n    p2 = Point(3, 7)\n    l1 = Line(Point(4, 0), Point(4, 9))\n    l2 = Line(Point(7, 6), Point(3, 6))\n    l3 = Line(Point(0, 0), Point(1, 1))\n    l4 = Segment2D(Point(1, 1), Point(5, 5))\n    r1 = Ray2D(Point(0, 0), Point(1, 1))\n\n    pa1 = Parabola(p1, l1)\n    pa2 = Parabola(p2, l2)\n\n    assert pa1.intersection(l1) == []\n    assert pa1.intersection(l3) == [Point(0, 0)]\n    assert pa1.intersection(l4) == []\n    assert pa1.intersection(r1) == [Point(0, 0)]\n\n\n\n", "compilation error", "compilation error", "compilation error", "def test_parabola_intersection():\n    x, y = symbols('x y')\n    p1 = Point(0, 0)\n    p2 = Point(3, 7)\n    d1 = Line(Point(4, 0), Point(4, 9))\n    d2 = Line(Point(7, 6), Point(3, 6))\n    pa1 = Parabola(p1, d1)\n    pa2 = Parabola(p2, d2)\n    l1 = Line(Point(1, -2), Point(-1,-2))\n    l2 = Line(Point(-7, 3), Point(12, 3))\n    s1 = Segment2D((-12, -65), (14, -68))\n    e1 = Ellipse(Point(0, 0), 2, 5)\n\n    assert pa1.intersection(l1) == [Point2D(-2, 0), Point2D(2, 0)]\n    assert pa1.intersection(l2) == [Point2D(-4, 3), Point2D(4, 3)]\n    assert pa1.intersection(s1) == []\n    assert pa1.intersection(e1) == [Point2D(-2, 0), Point2D(2, 0)]\n"], "sample_1202": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_111": ["    def test_search_with_pagination(self):\n        \"\"\"\n        Regression test for #17952: Ensure searching works correctly across \n        multiple pages of results.\n        \"\"\"\n        # Create 21 objects\n        for i in range(21):\n            Child.objects.create(name=f'child {i}', age=i)\n\n        user = self._create_superuser('superuser')\n        self.client.force_login(user)\n        child_admin = ChildAdmin(Child, custom_site)\n        # Set list_per_page to 10 to ensure pagination\n        child_admin.list_per_page = 10\n\n        # Search for a term that appears in multiple objects\n        response = self.client.post(\n            reverse('admin:admin_changelist_child_changelist'), {'name': 'child'}\n        )\n\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, 'child 0')\n        self.assertContains(response, 'child 10')\n        # Check if the search results are paginated\n        self.assertContains(response, '<a href=\"?page=2\">2</a>')\n\n        # Go to the second page of search results\n        response = self.client.get(reverse('admin:admin_changelist_child_changelist') + '?page=2')\n\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, 'child 11')\n        self.assertContains(response, 'child 20')\n\n", "    def test_changelist_view_search_filter_and_list_editable(self):\n        a = Swallow.objects.create(origin='Swallow A', load=4, speed=1)\n        Swallow.objects.create(origin='Swallow B', load=2, speed=2)\n        data = {\n            'form-TOTAL_FORMS': '2',\n            'form-INITIAL_FORMS': '2',\n            'form-MIN_NUM_FORMS': '0',\n            'form-MAX_NUM_FORMS': '1000',\n            'form-0-uuid': str(a.pk),\n            'form-0-load': '10',\n            'q': a.origin,\n            '_save': 'Save',\n        }\n        superuser = self._create_superuser('superuser')\n        self.client.force_login(superuser)\n        changelist_url = reverse('admin:admin_changelist_swallow_changelist')\n        response = self.client.post(changelist_url, data=data)\n        self.assertContains(response, a.origin)\n        self.assertContains(response, '1 swallow was changed successfully.')\n\n", "    def test_ordering_by_related_model_field(self):\n        parent1 = Parent.objects.create(name='parent 1')\n        parent2 = Parent.objects.create(name='parent 2')\n        Child.objects.create(name='child 1', parent=parent1, age=10)\n        Child.objects.create(name='child 2', parent=parent2, age=20)\n        Child.objects.create(name='child 3', parent=parent1, age=30)\n\n        class ChildAdmin(admin.ModelAdmin):\n            list_display = ('name', 'age', 'parent')\n            ordering = ('parent__name', 'age')\n\n        custom_site.register(Child, ChildAdmin)\n\n        m = ChildAdmin(Child, custom_site)\n        request = self._mocked_authenticated_request('/child/', self.superuser)\n        response = m.changelist_view(request)\n        self.assertEqual(list(response.context_data['cl'].result_list), [\n            Child.objects.get(name='child 1'),\n            Child.objects.get(name='child 3'),\n            Child.objects.get(name='child 2'),\n        ])\n        custom_site.unregister(Child)\n\n", "    def test_changelist_view_list_editable_changed_objects_uses_filter(self):\n        \"\"\"list_editable edits use a filtered queryset to limit memory usage.\"\"\"\n        a = Swallow.objects.create(origin='Swallow A', load=4, speed=1)\n        Swallow.objects.create(origin='Swallow B', load=2, speed=2)\n        data = {\n            'form-TOTAL_FORMS': '2',\n            'form-INITIAL_FORMS': '2',\n            'form-MIN_NUM_FORMS': '0',\n            'form-MAX_NUM_FORMS': '1000',\n            'form-0-uuid': str(a.pk),\n            'form-0-load': '10',\n            '_save': 'Save',\n        }\n        superuser = self._create_superuser('superuser')\n        self.client.force_login(superuser)\n        changelist_url = reverse('admin:admin_changelist_swallow_changelist')\n        with CaptureQueriesContext(connection) as context:\n            response = self.client.post(changelist_url, data=data)\n            self.assertEqual(response.status_code, 200)\n            self.assertIn('WHERE', context.captured_queries[4]['sql'])\n            self.assertIn('IN', context.captured_queries[4]['sql'])\n            # Check only the first few characters since the UUID may have dashes.\n            self.assertIn(str(a.pk)[:8], context.captured_queries[4]['sql'])\n\n", "    def test_changelist_view_filter_by_foreign_key(self):\n        \"\"\"\n        Tests filtering changelist by foreign key field.\n        \"\"\"\n        parent = Parent.objects.create(name='parent')\n        for i in range(10):\n            Child.objects.create(name='child %s' % i, parent=parent)  \n\n        driver = self.selenium\n        driver.get('%s/admin/child/changelist/' % self.live_server_url)\n        self.driver.login('super', 'secret')\n        self.assertEqual(self.driver.title, \"Child objects\")\n        # Apply a filter for Parent\n        driver.find_element_by_xpath('//select[@name=\"parent__exact\"]').click()\n        driver.find_element_by_xpath('//option[text()=\"parent\"]').click()\n        driver.find_element_by_xpath('//input[@value=\"Go\"]').click()\n\n        # Check if the filtered list only contains children associated with 'parent'\n        rows = driver.find_elements_by_css_selector('table#result_list tbody tr')\n        self.assertEqual(len(rows), 10)\n        for row in rows:\n            parent_name = row.find_elements_by_css_selector('td')[1].text\n            self.assertEqual(parent_name, 'parent')\n"], "sample_1041": ["def test_ZeroMatrix_transpose():\n    Z = ZeroMatrix(n, m)\n    assert Z.T == ZeroMatrix(m, n)\n", "def test_issue_10948():\n\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n    assert (A*B*C).transpose() == C.transpose()*B.transpose()*A.transpose()\n\n", "def test_GenericIdentityMatrix():\n    i = GenericIdentityMatrix()\n    A = MatrixSymbol(\"A\", n, n)\n\n    assert i == i\n    assert i != A\n    assert A != i\n\n    assert i.is_Identity\n\n    raises(TypeError, lambda: i.shape)\n    raises(TypeError, lambda: i.rows)\n    raises(TypeError, lambda: i.cols)\n\n    assert MatMul(i, A) == A\n    assert MatMul(A, i) == A\n\n    # Make sure it is hashable\n    hash(i) \n", "    def test_GenericIdentity():\n        i = GenericIdentity()\n        A = MatrixSymbol(\"A\", n, n)\n\n        assert i == i\n        assert i != A\n        assert A != i\n\n        assert i.is_Identity\n\n        raises(TypeError, lambda: i.shape)\n        raises(TypeError, lambda: i.rows)\n        raises(TypeError, lambda: i.cols)\n\n        assert MatMul() == i\n        assert MatMul(i, A) == MatMul(A)\n        assert MatMul(A, i) == MatMul(A)\n        # Make sure it is hashable\n        hash(i)\n", "compilation error"], "sample_77": ["    def test_urlize_trim_url_limit(self):\n        long_url = 'https://www.example.com/very/long/path/to/a/page/with/lots/of/segments'\n        output = (\n            '<a href=\"%s\">%s\u2026</a>' % (\n                long_url,\n                long_url[:30 - 3],\n            )\n        )\n        self.assertEqual(urlize(long_url, trim_url_limit=30), output)\n\n", "    def test_urlize_trim_url_limit(self):\n        self.assertEqual(\n            urlize('This is a very long URL: https://www.example.com/foo/bar/baz/?param1=value1&param2=value2',\n            trim_url_limit=20),\n            'This is a very long URL: <a href=\"https://www.example.com/foo/bar/baz/?param1=value1&#x26;param2=value2\">https://www.example.com/foo/bar/baz/\u2026</a>'\n        )\n", "    def test_urlize_trim_url_limit(self):\n        url = 'https://www.example.com/very/long/path/with/many/segments?query=string'\n        output = urlize('Visit {}.'.format(url), trim_url_limit=20)\n        self.assertEqual(output, 'Visit <a href=\"{}\">{}\u2026</a>.'.format(url, url.split('/')[2]))\n", "    def test_urlize_trim_url_limit(self):\n        text = 'This link should be truncated https://www.example.com/very/long/path/to/resource is too long.'\n        expected = 'This link should be truncated <a href=\"https://www.example.com/very/long/path/to/resource\">https://www.example.com/very/long/path/to/r\u2026</a> is too long.'\n        self.assertEqual(urlize(text, trim_url_limit=30), expected)\n", "    def test_urlize_trim_url_limit(self):\n        value = 'Visit this very very very very very long url: http://www.example.com/verylongpath/thispage.html'\n        output = 'Visit this very very very very very long url: <a href=\"http://www.example.com/verylongpath/thispage.html\">http://www.example.com/verylongpath/\u2026</a>'\n        self.assertEqual(urlize(value, trim_url_limit=40), output)\n"], "sample_507": ["    def test_plot_empty_data(self, plotter):\n        ax = plt.figure().subplots()\n        plotter(ax, [], [])\n        # Empty data should not raise exceptions and leave axis units unset\n        assert ax.xaxis.units is None\n        assert ax.yaxis.units is None\n", "def test_empty_plot(plotter):\n    ax = plt.figure().subplots()\n    plotter(ax, [], [])\n    assert ax.xaxis.get_majorticklocs() == []\n    assert ax.yaxis.get_majorticklocs() == []\n\n", "    def test_plot_empty_data(self, plotter):\n        ax = plt.figure().subplots()\n        plotter(ax, [], [])\n        assert ax.xaxis.units is None\n        assert ax.yaxis.units is None\n", "    def test_unit_data_update_with_existing_data(self):\n        data = ['a', 'd']\n        locs = [0, 1]\n\n        unit = cat.UnitData(data)\n        assert list(unit._mapping.keys()) == data\n        assert list(unit._mapping.values()) == locs\n\n        # Update with existing data, should not change mapping\n        unit.update(data)\n        assert list(unit._mapping.keys()) == data\n        assert list(unit._mapping.values()) == locs\n", "    def test_plot_empty_data(self, plotter):\n        ax = plt.figure().subplots()\n        plotter(ax, [], [])\n        assert ax.xaxis.get_majorticklocs() == []\n"], "sample_441": ["def test_empty_password_field(self):\n        user = User.objects.get(username=\"testclient\")\n        data = {\"password1\": \"\", \"password2\": \"\"}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors[\"password1\"], [\"\"])\n        self.assertEqual(form.errors[\"password2\"], [\"\"])\n", "    def test_field_order(self):\n        user = User.objects.get(username=\"testclient\")\n        self.assertEqual(\n            list(AdminPasswordChangeForm(user, {}).fields),\n            [\"password1\", \"password2\"],\n        )    \n", "    def test_empty_password(self):\n        user = User.objects.get(username=\"testclient\")\n        data = {\"password1\": \"\", \"password2\": \"\"}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors[\"password1\"], [Field.default_error_messages[\"required\"]])\n        self.assertEqual(form.errors[\"password2\"], [Field.default_error_messages[\"required\"]])\n", "    def test_empty_password(self):\n        user = User.objects.get(username=\"testclient\")\n        data = {\"password1\": \"\", \"password2\": \"\"}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors[\"password1\"], [form.error_messages[\"required\"]])\n        self.assertEqual(form.errors[\"password2\"], [form.error_messages[\"required\"]])\n        self.assertEqual(form.changed_data, [])\n", "    def test_super_long_password(self):\n        user = User.objects.get(username=\"testclient\")\n        data = {\"password1\": \"a\" * 128, \"password2\": \"a\" * 128}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(\n            form.errors[\"password1\"], [form.error_messages[\"password_too_long\"]]\n        )\n        self.assertEqual(\n            form.errors[\"password2\"], [form.error_messages[\"password_too_long\"]]\n        )\n        self.assertEqual(form.changed_data, [\"password\"])\n"], "sample_482": ["    def test_empty_input(self):\n        output = self.engine.render_to_string(\"escapeseq_empty_list\")\n        self.assertEqual(output, \"-- &lt;p&gt;\")\n", "    def test_empty_list(self):\n        output = self.engine.render_to_string(\"escapeseq_empty_list\")\n        self.assertEqual(output, \"\") \n", "    def test_empty_list(self):\n        output = self.engine.render_to_string(\"escapeseq_empty_list\")\n        self.assertEqual(output, \"\")\n", "    def test_empty_list(self):\n        output = self.engine.render_to_string(\"escapeseq_empty\", {\"a\": []})\n        self.assertEqual(output, \"\")\n", "    def test_empty_list(self):\n        output = self.engine.render_to_string(\"escapeseq_empty_list\", {\"a\": []})\n        self.assertEqual(output, \"\")\n"], "sample_90": ["    def test_model_form_factory_with_formfield_callback_and_localized_fields(self):\n            if isinstance(db_field, models.CharField):\n                return forms.CharField(widget=forms.Textarea, localize=True)\n            return db_field.formfield(**kwargs)\n\n        class LocalizedForm(forms.ModelForm):\n            class Meta:\n                model = Triple\n                localized_fields = '__all__'\n\n        NewForm = modelform_factory(Triple, form=LocalizedForm, formfield_callback=callback)\n        self.assertTrue(NewForm().fields['left'].localize)\n        self.assertTrue(NewForm().fields['middle'].localize)\n        self.assertTrue(NewForm().fields['right'].localize)\n", "    def test_strict_assignment_form_with_no_fields(self):\n        form_class = modelform_factory(model=StrictAssignmentFieldSpecific, fields=[])\n        form = form_class(data={}, files=None)\n        # This line turns on the ValidationError; it avoids the model erroring\n        # when its own __init__() is called when creating form.instance.\n        form.instance._should_error = True\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors, {'__all__': ['Cannot set attribute']}) \n", "    def test_modelform_with_custom_metaclass_with_modelformmetaclass(self):\n        class CustomMetaclassForm(forms.ModelForm, metaclass=CustomMetaclass):\n            class Meta:\n                model = StrictAssignmentFieldSpecific\n                fields = '__all__'\n\n        form = CustomMetaclassForm({'title': 'testing setattr'})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors, {\n            'title': ['Cannot set attribute', 'This field cannot be blank.']\n        })\n", "    def test_strict_assignment_field_not_in_form(self):\n        class MyForm(forms.ModelForm):\n            class Meta:\n                model = StrictAssignmentFieldSpecific\n                fields = ['foo']\n\n        form = MyForm({'foo': 'bar'})\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.instance.title, None)\n", "    def test_strict_assignment_in_form_init(self):\n        \"\"\"A ValidationError during form __init__ should be caught and handled.\"\"\"\n        form_class = modelform_factory(model=StrictAssignmentAll, fields=['title'])\n        with self.assertRaises(ValidationError) as context:\n            form_class(data={'title': 'testing setattr'}, files=None)\n\n        self.assertEqual(str(context.exception), 'Cannot set attribute')\n"], "sample_25": ["    def test_header_from_string_with_extra_spaces(self):\n        header_str = \"\"\"", "    def test_header_slice_assignment():\n        header = fits.Header()\n        header['A'] = 1\n\n        header[0:1] = fits.Header([('B', 2)])\n        assert header['B'] == 2\n        assert 'A' not in header\n", "    def test_append_card_with_empty_comment():\n        h = fits.Header()\n        h.append((\"COMMENT\", \"This is a comment\"))\n        h.append((\"HISTORY\", \"This is a history entry\"))\n        h.append((\"AUTHOR\", \"Test Author\"))\n        h.append((\"DATE\", \"2023-10-27\"))\n        \n        assert h.comments['COMMENT'] == \"This is a comment\"\n        assert h.comments['HISTORY'] == \"no comment\"\n        assert h.comments['AUTHOR'] == \"no comment\"\n        assert h.comments['DATE'] == \"no comment\"\n", "    def test_header_get_item_by_index():\n        \"\"\"Test accessing header items by index.\"\"\"\n        header = fits.Header()\n        header['NAXIS'] = 2\n        header['NAXIS1'] = 10\n        header['NAXIS2'] = 20\n        assert header[0] == 2\n        assert header[1] == 10\n        assert header[2] == 20\n        assert header[-1] == 20\n", "    def test_fitsheader_script_output(self, capsys):\n        from astropy.io.fits.scripts import fitsheader\n\n        with open(self.data(\"zerowidth.fits\")) as fitsobject:\n            fitsheader.main([\"zerowidth.fits\"])\n            out, err = capsys.readouterr()\n            assert \"EXTNAME = 'AIPS FQ\" in out\n            assert \"BITPIX\" in out\n            assert \"NAXIS1\" in out\n\n"], "sample_201": ["    def test_store_empty_message_list(self):\n        \"\"\"\n        Test that storing an empty message list correctly sets the cookie.\n        \"\"\"\n\n        storage = self.get_storage()\n        response = self.get_response()\n\n        storage._store([], response)\n        self.assertEqual(response.cookies['messages'].value, '')\n        self.assertEqual(response.cookies['messages']['domain'], '.example.com')\n        self.assertEqual(response.cookies['messages']['expires'], '')\n        self.assertIs(response.cookies['messages']['secure'], True)\n        self.assertIs(response.cookies['messages']['httponly'], True)\n        self.assertEqual(response.cookies['messages']['samesite'], 'Strict')\n\n", "    def test_remove_oldest_messages(self):\n        storage = self.get_storage()\n        response = self.get_response()\n\n        # Add messages exceeding the cookie size limit\n        for i in range(5):\n            message = str(i) * CookieStorage.max_cookie_size // 5\n            storage.add(constants.INFO, message)\n\n        # Check that the oldest messages are removed\n        unstored_messages = storage.update(response)\n        self.assertEqual(len(unstored_messages), 1)\n        self.assertEqual(unstored_messages[0].message, '0' * (CookieStorage.max_cookie_size // 5))\n        self.assertEqual(len(storage), 4)\n\n\n\n", "    def test_empty_cookie(self):\n        storage = self.get_storage()\n        response = self.get_response()\n        \n        self.assertEqual(self.stored_messages_count(storage, response), 0)\n\n        storage._store([], response, encode_empty=True)\n\n        self.assertEqual(self.stored_messages_count(storage, response), 0)\n", "    def test_empty_cookie(self):\n        storage = self.get_storage()\n        response = self.get_response()\n        self.assertEqual(self.stored_messages_count(storage, response), 0)\n        storage._store([], response)\n        self.assertEqual(self.stored_messages_count(storage, response), 0)\n        storage._store([], response, encode_empty=True)\n        self.assertEqual(self.stored_messages_count(storage, response), 0)", "    def test_empty_cookie(self):\n        storage = self.get_storage()\n        response = self.get_response()\n        \n        # Store nothing (empty cookie).\n        storage.update(response)\n\n        # Check that we don't have any messages stored.\n        self.assertEqual(self.stored_messages_count(storage, response), 0)\n\n"], "sample_119": ["    def test_related_field_lookup(self):\n        query = Query(Item)\n        where = query.build_where(Q(creator__author__name='John Doe'))\n        lookup = where.children[0]\n        self.assertIsInstance(lookup, Exact)\n        self.assertEqual(lookup.lhs.target.attname, 'creator__author__name')\n\n", "    def test_related_isnull_negated(self):\n        query = Query(ObjectC)\n        where = query.build_where(~Q(objecta=None))\n        lookup = where.children[0]\n        self.assertIsInstance(lookup, RelatedIsNull)\n        self.assertIsInstance(lookup.lhs, SimpleCol)\n        self.assertEqual(lookup.lhs.target, ObjectC._meta.get_field('objecta'))\n        self.assertFalse(lookup.negated)\n", "    def test_related_isnull_and(self):\n        query = Query(ObjectC)\n        where = query.build_where(Q(objecta__isnull=True) & Q(objectb__isnull=True))\n        a_isnull = where.children[0]\n        self.assertIsInstance(a_isnull, RelatedIsNull)\n        self.assertIsInstance(a_isnull.lhs, SimpleCol)\n        self.assertEqual(a_isnull.lhs.target, ObjectC._meta.get_field('objecta'))\n        b_isnull = where.children[1]\n        self.assertIsInstance(b_isnull, RelatedIsNull)\n        self.assertIsInstance(b_isnull.lhs, SimpleCol)\n        self.assertEqual(b_isnull.lhs.target, ObjectC._meta.get_field('objectb'))\n", "    def test_related_isnull_negated(self):\n        query = Query(ObjectC)\n        where = query.build_where(~Q(objecta=None))\n        negated_lookup = where.children[0]\n        self.assertTrue(negated_lookup.negated)\n        self.assertIsInstance(negated_lookup.lhs, RelatedIsNull)\n        self.assertIsInstance(negated_lookup.lhs.lhs, SimpleCol)\n        self.assertEqual(negated_lookup.lhs.lhs.target, ObjectC._meta.get_field('objecta'))\n", "    def test_related_isnull(self):\n        query = Query(Author)\n        where = query.build_where(Q(item__isnull=True))\n        lookup = where.children[0]\n        self.assertIsInstance(lookup, RelatedIsNull)\n        self.assertEqual(lookup.lhs.target, Author._meta.get_field('item'))\n"], "sample_624": ["compilation error", "def test_coords_repr_limited(display_max_rows, expected_length):\n    coords = {f\"coord_{i}\": np.arange(100) for i in range(100)}\n    ds = xr.Dataset(coords=coords)\n    with xr.set_options(display_max_rows=display_max_rows):\n        actual = formatting.coords_repr(ds.coords).split(\"\\n\")\n        assert len(actual) == expected_length \n", "compilation error", "compilation error", "compilation error"], "sample_70": ["    def test_fast_delete_with_related_objects(self):\n        r = R.objects.create()\n        s = S.objects.create(r=r)\n        t = T.objects.create(s=s)\n        # We can fast delete `r`, resulting in also deleting `s` and `t`.\n        self.assertNumQueries(2, r.delete)\n        self.assertFalse(R.objects.exists())\n        self.assertFalse(S.objects.exists())\n        self.assertFalse(T.objects.exists())\n\n", "    def test_fast_delete_on_delete_set_null(self):\n        a = create_a('setnull')\n        r = a.setnull\n\n        # 1 query to delete a\n        self.assertNumQueries(1, a.delete)\n        self.assertFalse(A.objects.exists())\n        self.assertIsNotNone(r.pk)\n        self.assertIsNone(a.setnull)\n", "    def test_fast_delete_m2m_related_objects(self):\n        t1 = M2MTo.objects.create()\n        t2 = M2MTo.objects.create()\n        f = M2MFrom.objects.create()\n        f.m2m.add(t1, t2)\n        # 1 query to delete f, 1 query to fast-delete m2m for f\n        self.assertNumQueries(2, f.delete)\n        self.assertFalse(M2MFrom.objects.exists())\n        self.assertTrue(M2MTo.objects.filter(pk=t1.pk).exists())\n        self.assertTrue(M2MTo.objects.filter(pk=t2.pk).exists())\n", "    def test_fast_delete_m2m_through(self):\n        m1 = M.objects.create()\n        m2 = M.objects.create()\n        r = R.objects.create()\n        MR.objects.create(m=m1, r=r)\n        MR.objects.create(m=m2, r=r)\n        self.assertEqual(MR.objects.count(), 2)\n        self.assertNumQueries(2, r.delete)\n        self.assertEqual(MR.objects.count(), 0)\n\n", "    def test_fast_delete_related_object_with_custom_pk(self):\n        class MyModel(models.Model):\n            custom_pk = models.CharField(max_length=20, primary_key=True)\n            name = models.CharField(max_length=20)\n\n        class RelatedModel(models.Model):\n            custom_pk = models.ForeignKey(\n                MyModel,\n                on_delete=models.CASCADE,\n                primary_key=True\n            )\n\n        m = MyModel.objects.create(custom_pk='pk1', name='model1')\n        r = RelatedModel.objects.create(custom_pk='pk2', custom_pk_id=m)\n\n        self.assertNumQueries(1, m.delete)\n        self.assertFalse(MyModel.objects.exists())\n        self.assertFalse(RelatedModel.objects.exists())\n"], "sample_940": ["    def test_is_builtin_class_method_with_inheritance():\n        class MyInt(int):\n                pass\n\n        assert inspect.is_builtin_class_method(MyInt.my_method) is False\n", "    def test_is_builtin_class_method_with_slots(app):\n        class MyInt(int):\n            __slots__ = ()\n                pass\n        assert inspect.is_builtin_class_method(MyInt.my_method) is False\n", "    def test_unpartial_with_keyword_arguments():\n            pass\n\n        func2 = functools.partial(func1, a=1, c=3)\n        func2.__doc__ = \"func2\"\n\n        assert inspect.unpartial(func2) is func1\n\n", "    def test_getdoc_with_docstring_in_inherited_class():\n        class Foo:\n            pass\n\n        class Bar(Foo):\n                \"\"\"docstring.\"\"\"\n\n        assert inspect.getdoc(Bar.meth, getattr, True, Bar, \"meth\") == \"docstring.\"\n\n\n\n", "def test_is_builtin_class_method(app):\n    from target.methods import Base\n\n    assert inspect.is_builtin_class_method(Base.meth) is False\n    assert inspect.is_builtin_class_method(Base.staticmeth) is False\n    assert inspect.is_builtin_class_method(Base.classmeth) is False\n    assert inspect.is_builtin_class_method(int.from_bytes) is True\n"], "sample_887": ["    def test_calibration_with_non_sample_aligned_fit_param_error(data):\n        \"\"\"Check that CalibratedClassifierCV raises an error if fit parameters\n\n        are not properly aligned with the input samples.\n        \"\"\"\n\n        class TestClassifier(LogisticRegression):\n                assert fit_param is not None\n                return super().fit(X, y, sample_weight=sample_weight)\n\n        with pytest.raises(ValueError, match=\"Length of fit_param\"):\n            CalibratedClassifierCV(estimator=TestClassifier()).fit(\n                *data, fit_param=np.ones(len(data[1]) - 1)  \n            ) \n", "def test_calibration_with_multioutput_estimator(data):\n    \"\"\"Check that CalibratedClassifierCV works with multi-output estimators.\"\"\"\n    X, y = data\n    X = np.hstack((X, np.zeros((X.shape[0], 1))))  # Add a dummy feature\n    y = np.column_stack((y, y))  # Create multi-output target\n    estimator = MultiOutputClassifier(LogisticRegression())\n    calibrated_classifier = CalibratedClassifierCV(estimator=estimator)\n    calibrated_classifier.fit(X, y)\n    calibrated_classifier.predict_proba(X)\n\n", "    def test_calibration_with_n_jobs(data):\n        X, y = data\n        clf = LogisticRegression(max_iter=1000)\n        for n_jobs in [1, -1]:\n            calibrated_clf = CalibratedClassifierCV(clf, n_jobs=n_jobs)\n            calibrated_clf.fit(X, y)\n", "    def test_calibration_n_jobs(data):\n        # Check that the n_jobs parameter is passed to the underlying base estimator\n\n        clf = CheckingClassifier(expected_n_jobs=True)\n        pc_clf = CalibratedClassifierCV(clf)\n\n        pc_clf.fit(*data, n_jobs=2)\n\n", "    def test_calibration_with_prefit_cv(data):\n        X, y = data\n        clf = LogisticRegression()\n        clf.fit(X, y)\n        calibrated_clf = CalibratedClassifierCV(\n            estimator=clf, cv=\"prefit\", method=\"sigmoid\"\n        )\n        calibrated_clf.fit(X, y)\n        assert calibrated_clf.calibrated_classifiers_[0].calibrated_classifier_.coef_ == clf.coef_\n"], "sample_335": ["    def test_decimalfield_null_input(self):\n        f = DecimalField(null=True)\n        self.assertIsNone(f.clean(''))\n        self.assertIsNone(f.clean(None))\n", "    def test_decimalfield_has_changed_localized(self):\n        f = DecimalField(max_digits=2, decimal_places=2, localize=True)\n        with translation.override('de'):\n            self.assertFalse(f.has_changed(decimal.Decimal(\"0.1\"), '0,1'))\n", "    def test_decimalfield_invalid_input(self):\n        f = DecimalField(max_digits=4, decimal_places=2)\n        with self.assertRaisesMessage(ValidationError, \"'Enter a number.'\"):\n            f.clean('abc')\n        with self.assertRaisesMessage(ValidationError, \"'Enter a number.'\"):\n            f.clean(None)\n", "    def test_decimalfield_support_decimal_separator_with_localization(self):\n        f = DecimalField(localize=False)\n        self.assertEqual(f.clean('1001.10'), decimal.Decimal(\"1001.10\"))\n        with translation.override('fr'):\n            self.assertEqual(f.clean('1001,10'), decimal.Decimal(\"1001.10\"))\n", "    def test_decimalfield_custom_format(self):\n        f = DecimalField(max_digits=6, decimal_places=2, localize=True)\n        with self.assertRaises(ValidationError):\n            f.clean('1,234,567.89')\n        with translation.override('fr'):\n            f = DecimalField(max_digits=6, decimal_places=2, localize=True)\n            self.assertEqual(f.clean('1 234 567,89'), decimal.Decimal(\"1234567.89\"))\n            self.assertEqual(f.clean('1.234.567,89'), decimal.Decimal(\"1234567.89\"))\n\n\n\n"], "sample_368": ["    def test_migrate_skips_schema_creation_if_record_table_fails(self):\n        \"\"\"\n        The django_migrations table is not created if it fails to be recorded.\n        \"\"\"\n        class MigrationRecorderWithFailure:\n                raise RuntimeError('MigrationRecorder initialization failed.')\n\n        with mock.patch('django.db.migrations.executor.MigrationRecorder', MigrationRecorderWithFailure):\n            executor = MigrationExecutor(connection)\n            with self.assertRaisesMessage(RuntimeError, 'MigrationRecorder initialization failed.'):\n                executor.migrate([], plan=[])\n\n", "    def test_minimize_rollbacks_complex(self):\n        r\"\"\"\n        Minimize rollbacks in a complex dependency graph.\n\n        a: 1 <-- 3\n              \\\n               2 <-- 4\n        b: 1\n        c: 1 <--- 2\n        d: 1\n        e: 1 <-- 2\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        a3_impl = FakeMigration('a3')\n        a3 = ('a', '3')\n        a4_impl = FakeMigration('a4')\n        a4 = ('a', '4')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        c1_impl = FakeMigration('c1')\n        c1 = ('c', '1')\n        c2_impl = FakeMigration('c2')\n        c2 = ('c', '2')\n        d1_impl = FakeMigration('d1')\n        d1 = ('d', '1')\n        e1_impl = FakeMigration('e1')\n        e1 = ('e', '1')\n        e2_impl = FakeMigration('e2')\n        e2 = ('e', '2')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(a3, a3_impl)\n        graph.add_node(a4, a4_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_node(c1, c1_impl)\n        graph.add_node(c2, c2_impl)\n        graph.add_node(d1, d1_impl)\n        graph.add_node(e1, e1_impl)\n        graph.add_node(e2, e2_impl)\n\n        graph.add_dependency(None, a2, a1)\n        graph.add_", "    def test_minimize_rollbacks_circular(self):\n        r\"\"\"\n        Minimize rollbacks when there's a circular dependency.\n        \"\"\"\n\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_dependency(None, a2, a1)\n        graph.add_dependency(None, b1, a2)\n        graph.add_dependency(None, a1, b1)\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            b1: b1_impl,\n            a2: a2_impl,\n        })\n\n        plan = executor.migration_plan({a1})\n\n        self.assertEqual(plan, [])\n\n", "    def test_minimize_rollbacks_multiple_apps(self):\n        r\"\"\"\n        Minimize rollbacks across multiple apps.\n\n        a: 1 <---- 3 <--\\\n              \\ \\- 2 <--- 4\n               \\       \\\n        b:      \\- 1 <--- 2\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        a3_impl = FakeMigration('a3')\n        a3 = ('a', '3')\n        a4_impl = FakeMigration('a4')\n        a4 = ('a', '4')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        b2_impl = FakeMigration('b2')\n        b2 = ('b', '2')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(a3, a3_impl)\n        graph.add_node(a4, a4_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_node(b2, b2_impl)\n        graph.add_dependency(None, a2, a1)\n        graph.add_dependency(None, a3, a1)\n        graph.add_dependency(None, a4, a2)\n        graph.add_dependency(None, a4, a3)\n        graph.add_dependency(None, b2, b1)\n        graph.add_dependency(None, b1, a1)\n        graph.add_dependency(None, b2, a2)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            b1: b1_impl,\n            a2: a2_impl,\n        })\n\n        plan = executor.migration_plan({a1, b1})\n\n        should_be_rolled_back", "    def test_multiple_replacements(self):\n        \"\"\"\n        Test handling of situations where multiple migrations are replaced by a\n        single migration.\n\n        This test verifies that when multiple migrations are replaced by a\n        single squashed migration, applying the squashed migration marks\n        all replaced migrations as applied.\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        a3_impl = FakeMigration('a3')\n        a3 = ('a', '3')\n        squashed_impl = FakeMigration('squashed')\n        squashed = ('a', 'squashed')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(a3, a3_impl)\n        graph.add_node(squashed, squashed_impl)\n        # a1 -> a2 -> a3 -> squashed\n        graph.add_dependency(None, a2, a1)\n        graph.add_dependency(None, a3, a2)\n        graph.add_dependency(None, squashed, a3)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            a2: a2_impl,\n            a3: a3_impl,\n            squashed: squashed_impl,\n        })\n        plan = executor.migration_plan({squashed})\n        self.assertEqual(plan, [(squashed_impl, True)])\n"], "sample_361": ["    def test_urlize_trim_url_limit(self):\n        text = 'This is a link to a very long website: https://example.website.thatisverylong.com/superlongpath?query=parameters'\n        expected = 'This is a link to a very long website: <a href=\"https://example.website.thatisverylong.com/superlongpath?query=parameters\">https://example.website.thatisverylong.com/superlongpath... </a>'\n        self.assertEqual(urlize(text, trim_url_limit=50), expected)\n", "    def test_urlize_nofollow(self):\n        self.assertEqual(\n            urlize('https://example.com', nofollow=True),\n            '<a href=\"https://example.com\" rel=\"nofollow\">https://example.com</a>'\n        )\n", "    def test_urlize_trim_url_limit(self):\n        long_url = 'https://www.reallylongwebsiteaddressthatneedstobetruncated.com/really/long/path/?q=something'\n        output = f'<a href=\"{long_url}\">{long_url[:20]}\u2026</a>'\n        self.assertEqual(urlize(long_url, trim_url_limit=20), output)\n\n", "    def test_urlize_nofollow(self):\n        tests = (\n            (\n                'Search for google.com/?q=! and see.',\n                'Search for <a href=\"http://google.com/?q=\" rel=\"nofollow\">google.com/?q=</a>! and see.'\n            ),\n\n        )\n        for value, output in tests:\n            with self.subTest(value=value):\n                self.assertEqual(urlize(value, nofollow=True), output)\n", "    def test_urlize_nofollow(self):\n        tests = (\n            ('Search for google.com/?q=! and see.',\n             'Search for <a href=\"http://google.com/?q=\" rel=\"nofollow\">google.com/?q=</a>! and see.'),\n        )\n        for value, output in tests:\n            with self.subTest(value=value):\n                self.assertEqual(urlize(value, nofollow=True), output)\n\n"], "sample_113": ["    def test_simplify_regex(self):\n        self.assertEqual(simplify_regex(r'^/admin/company/(?P<pk>\\d+)/$'), r'/admin/company/(\\d+)/')\n        self.assertEqual(simplify_regex(r'^/admin/company/(?P<company_id>\\d+)/edit/$'), r'/admin/company/(\\d+)/edit/')\n        self.assertEqual(simplify_regex(r'^/admin/company/(?P<pk>\\d+)/details/(?P<type>\\w+)/$'), r'/admin/company/(\\d+)/details/(?P<type>\\w+)/')\n        self.assertEqual(simplify_regex(r'^/admin/company/new/$'), r'/admin/company/new/')\n        self.assertEqual(simplify_regex(r'^/admin/company/(?P<pk>\\d+)/delete/$'), r'/admin/company/(\\d+)/delete/')\n\n", "    def test_simplify_regex(self):\n        self.assertEqual(simplify_regex(r'^foo/bar/(?P<baz>\\d+)/$'), r'^foo/bar/\\d+/')\n        self.assertEqual(simplify_regex(r'^foo/(?P<bar>.*)/baz/$'), r'^foo/.*?/baz/')\n        self.assertEqual(simplify_regex(r'^foo/(?P<bar>\\w+)/baz/(?P<qux>\\w+)$'), r'^foo/\\w+/baz/\\w+$')\n        self.assertEqual(simplify_regex(r'^foo/(?P<bar>\\w+)/(?P<qux>\\d+)$'), r'^foo/\\w+/\\d+$')\n        self.assertEqual(simplify_regex(r'^foo/(?P<bar>\\w+)/baz/(?P<qux>\\d+)/$'), r'^foo/\\w+/baz/\\d+/')\n        self.assertEqual(simplify_regex(r'^foo/bar/(\\w+)/$'), r'^foo/bar/\\w+/')\n        self.assertEqual(simplify_regex(r'^foo/bar/(.*)/$'), r'^foo/bar/.*/')\n        self.assertEqual(simplify_regex(r'^foo/(.*)/(baz)/$'), r'^foo/.*/(baz)/')\n\n", "    def test_simplify_regex(self):\n        self.assertEqual(simplify_regex(r'^/admin/company/(\\d+)/?$'), r'^/admin/company/\\d+/$')\n        self.assertEqual(simplify_regex(r'^/admin/company/(\\d+)/edit/?$'), r'^/admin/company/\\d+/edit/$')\n        self.assertEqual(simplify_regex(r'^/admin/company/(?P<company_id>\\d+)/?$'), r'^/admin/company/\\d+/$')\n        self.assertEqual(simplify_regex(r'^/admin/company/(?P<company_id>\\d+)/edit/$'), r'^/admin/company/\\d+/edit/$')\n        self.assertEqual(simplify_regex(r'^/admin/company/(?P<company_id>\\d+)/edit/?$'), r'^/admin/company/\\d+/edit/$')\n\n", "    def test_simplify_regex(self):\n        self.assertEqual(simplify_regex(r'\\A(?P<id>\\d{1,3})-(?P<name>[^/]+)\\Z'), '<var>-<var>')\n", "    def test_regex_simplification(self):\n        self.assertEqual(simplify_regex(r'^foo\\/bar\\/(\\d+)$'), r'^foo\\/bar\\/(\\d+)$')\n        self.assertEqual(simplify_regex(r'^foo\\/bar\\/(.*)$'), r'^foo\\/bar\\/.*$')\n        self.assertEqual(simplify_regex(r'^foo\\/bar\\/(?P<id>\\d+)$'), r'^foo\\/bar\\/(\\d+)$')\n        self.assertEqual(simplify_regex(r'^foo\\/bar\\/(?P<id>\\w+)$'), r'^foo\\/bar\\/\\w+$')\n        self.assertEqual(simplify_regex(r'^foo\\/bar\\/(\\d+)\\/baz\\/(\\w+)$'), r'^foo\\/bar\\/\\d+\\/baz\\/\\w+$')\n        self.assertEqual(simplify_regex(r'^foo\\/(?P<category>[^/]+)\\/(?P<id>\\d+)$'), r'^foo\\/[^/]+\\/\\d+$')\n\n"], "sample_1193": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_676": ["    def test_line_with_reprcrash_long_nodeid(monkeypatch):\n        import _pytest.terminal\n        from wcwidth import wcswidth\n\n        mocked_verbose_word = \"FAILED\"\n\n        mocked_pos = \"some::very::long::nodeid\"\n\n            return mocked_pos\n\n        monkeypatch.setattr(_pytest.terminal, \"_get_pos\", mock_get_pos)\n\n        class config(object):\n            pass\n\n        class rep(object):\n                return mocked_verbose_word\n", "    def test_line_with_reprcrash_short(monkeypatch):\n        import _pytest.terminal\n        from wcwidth import wcswidth\n\n        mocked_verbose_word = \"FAILED\"\n\n        mocked_pos = \"some::nodeid\"\n\n            return mocked_pos\n\n        monkeypatch.setattr(_pytest.terminal, \"_get_pos\", mock_get_pos)\n\n        class config(object):\n            pass\n\n        class rep(object):\n                return mocked_verbose_word\n\n                return \"this is a reprcrash\"\n", "    def test_line_with_reprcrash_long(monkeypatch):\n        import _pytest.terminal\n        from wcwidth import wcswidth\n\n        mocked_verbose_word = \"FAILED\"\n\n        mocked_pos = \"some::very::long::nodeid::to::test::length::and::wrapping\"\n\n            return mocked_pos\n\n        monkeypatch.setattr(_pytest.terminal, \"_get_pos\", mock_get_pos)\n\n        class config(object):\n            pass\n\n        class rep(object):\n                return mocked_verbose_word\n", "    def test_show_skips():\n        class X(object):\n            pass\n\n        ev1 = X()\n        ev1.when = \"execute\"\n        ev1.skipped = True\n        ev1.longrepr = (\"xyz\", 3, \"justso\")\n\n        ev2 = X()\n        ev2.when = \"collect\"\n        ev2.skipped = True\n        ev2.longrepr = (\"abc\", 5, \"because\")\n\n        values = _folded_skips([ev1, ev2])\n        assert len(values) == 2\n", "    def test_line_with_reprcrash(monkeypatch):\n        import _pytest.terminal\n\n            return 10\n\n        monkeypatch.setattr(_pytest.terminal, \"get_string_width\", mock_get_string_width)\n\n        config = object()\n        rep = object()\n        rep.longrepr = \"some\\nmultiline\\nerror\"\n\n        res = _pytest.terminal.line(\"some::nodeid\", rep, config)\n        assert res == (\n            \"some::nodeid \"\n            \"FAILED                                  \\n\"\n            \"some\\n\"\n            \"multiline\\n\"\n            \"error\"\n        )\n"], "sample_835": ["    def test_oob_score_adaboost():\n        # Check the out-of-bag score for AdaBoostClassifier\n        from sklearn.datasets import make_classification\n        from sklearn.ensemble import RandomForestClassifier\n        from sklearn.model_selection import cross_val_score\n\n        X, y = make_classification(n_samples=100, n_features=10, random_state=0)\n\n        # RandomForestClassifier is used as a base estimator since\n        # it supports oob_score\n        boost = AdaBoostClassifier(RandomForestClassifier(oob_score=True),\n                                  n_estimators=10, random_state=0)\n\n        scores = cross_val_score(boost, X, y, cv=5, scoring=\"accuracy\")\n        assert all(np.isfinite(s) for s in scores)\n", "    def test_n_jobs():\n        # Check parallel training and prediction.\n        # Classification\n        clf = AdaBoostClassifier(n_estimators=10, random_state=0)\n        clf.fit(iris.data, iris.target)\n        score = clf.score(iris.data, iris.target)\n        predictions = clf.predict(iris.data)\n        proba = clf.predict_proba(iris.data)\n\n        # Parallel training\n        clf_parallel = AdaBoostClassifier(n_estimators=10, n_jobs=2,\n                                         random_state=0)\n        clf_parallel.fit(iris.data, iris.target)\n        score_parallel = clf_parallel.score(iris.data, iris.target)\n        predictions_parallel = clf_parallel.predict(iris.data)\n        proba_parallel = clf_parallel.predict_proba(iris.data)\n\n        assert score == score_parallel\n        assert_array_equal(predictions, predictions_parallel)\n        assert_array_almost_equal(proba, proba_parallel)\n\n        # Regression\n        reg = AdaBoostRegressor(n_estimators=10, random_state=0)\n        reg.fit(boston.data, boston.target)\n        score = reg.score(boston.data, boston.target)\n        predictions = reg.predict(boston.data)\n\n        # Parallel training\n        reg_parallel = AdaBoostRegressor(n_estimators=10, n_jobs=2,\n                                        random_state=0)\n        reg_parallel.fit(boston.data, boston.target)\n        score_parallel = reg_parallel.score(boston.data, boston.target)\n        predictions_parallel = reg_parallel.predict(boston.data)\n\n        assert score == score_parallel\n        assert_array_almost_equal(predictions, predictions_parallel)\n", "    def test_estimator_errors(self):\n        # Test that estimator_errors_ is populated correctly.\n        # This test checks the logic inside the _boost method\n        # in _AdaBoostRegressor and AdaBoostClassifier.\n\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        y = np.array([1, -1, 1])\n        estimator = DecisionTreeClassifier()\n        boost = AdaBoostClassifier(estimator, n_estimators=2)\n        boost.fit(X, y)\n\n        assert len(boost.estimator_errors_) == 2\n\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        y = np.array([1, 2, 3])\n        estimator = DecisionTreeRegressor()\n        boost = AdaBoostRegressor(estimator, n_estimators=2)\n        boost.fit(X, y)\n\n        assert len(boost.estimator_errors_) == 2\n", "    def test_invalid_base_estimator():\n        \n        class DummyEstimator:\n                pass\n\n                return np.zeros(X.shape[0])\n\n        \n        assert_raises(TypeError, AdaBoostClassifier, base_estimator=DummyEstimator)\n        assert_raises(TypeError, AdaBoostRegressor, base_estimator=DummyEstimator) \n", "    def test_n_estimators_validation():\n        \"\"\"Test for validation of n_estimators parameter.\"\"\"\n        # Test that n_estimators is validated when initialized\n        assert_raises(ValueError, AdaBoostClassifier, n_estimators=0)\n        assert_raises(ValueError, AdaBoostRegressor, n_estimators=0)\n\n        # Test that n_estimators is validated in fit_predict\n        clf = AdaBoostClassifier(n_estimators=10)\n        assert_raises(ValueError, clf.fit, X, y_class, n_estimators=0)\n        reg = AdaBoostRegressor(n_estimators=10)\n        assert_raises(ValueError, reg.fit, X, y_regr, n_estimators=0)\n\n"], "sample_1010": ["compilation error", "def test_issue_15334():\n    from sympy.physics.mechanics import dynamicsymbols\n    x = dynamicsymbols('x')\n    dxdt = dynamicsymbols('x', 1)\n    assert latex(dxdt) == r\"\\dot{x}\"\n", "compilation error", "compilation error", "def test_latex_conjugate():\n    from sympy import conjugate\n    x = symbols('x')\n    assert latex(conjugate(x)) == r\"\\overline{x}\"\n    assert latex(conjugate(3*x + 2)) == r\"\\overline{3 x + 2}\"\n    assert latex(conjugate(x*y)) == r\"\\overline{x y}\"\n\n"], "sample_532": ["compilation error", "compilation error", "compilation error", "    def test_contour_empty_array():\n        fig, ax = plt.subplots()\n        data = np.array([])\n        with pytest.raises(ValueError,\n                           match=\"Input z must be a 2D array\"):\n            ax.contour(data)\n", "    def test_contourf_nan_in_data():\n        x = np.array([1, 2, 3, 4])\n        y = np.array([1, 2, 3, 4])\n        z = np.array([[1, 2, 3, np.nan],\n                      [2, 3, np.nan, 4],\n                      [3, np.nan, 5, 6],\n                      [np.nan, 4, 5, 6]])\n        fig, ax = plt.subplots()\n        with pytest.warns(UserWarning, match=\"NaN values were encountered\"):\n            cs = ax.contourf(x, y, z)\n"], "sample_855": ["    def test_dummy_multioutputregressor_constant_different_shapes():\n        X = [[0]] * 3\n        y = np.array([[1, 2], [3, 4], [5, 6]])\n        reg = DummyRegressor(strategy='constant', constant=np.array([1, 2]))\n        assert_raises(ValueError, reg.fit, X, y)\n\n", "def test_dummy_regressor_on_empty_array():\n    X = np.array([])\n    y = np.array([])\n    reg = DummyRegressor()\n    assert_raises(ValueError, reg.fit, X, y)\n\n", "    def test_dummy_regressor_sparse_input(self):\n        X = sp.csc_matrix([[0], [0], [1], [1]])\n        y = [2, 1, 2, 2]\n        reg = DummyRegressor(strategy=\"mean\")\n        reg.fit(X, y)\n        y_pred = reg.predict(X)\n        assert_array_equal(y_pred, [np.mean(y)] * len(y))\n", "    def test_dummy_regressor_empty_train():\n        est = DummyRegressor(strategy=\"mean\")\n        with pytest.raises(ValueError):\n            est.fit([], [])\n", "def test_dummy_regressor_constant_strategy_with_input_shape_mismatch():\n    X = [[0]] * 3\n    y = [1, 2, 3]\n    reg = DummyRegressor(strategy=\"constant\", constant=[1, 2, 3, 4])\n    # mismatch between n_outputs in y and constant\n\n    with pytest.raises(ValueError,\n                       match=r\"Shape of constant does not match\"):\n\n        reg.fit(X, y)\n"], "sample_1097": ["compilation error", "    def test_blockcut_identity():\n        n = 4\n        I = Identity(n)\n        B = blockcut(I, (n//2, n//2), (n//2, n//2))\n        assert B == BlockDiagMatrix(Identity(n//2), Identity(n//2))\n", "def test_blockinverse_1x1():\n    A = MatrixSymbol('A', n, n)\n    assert block_collapse(Inverse(BlockMatrix([[A]]))) == A.I\n\n", "compilation error", "def test_BlockMatrix_structurally_equal():\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, n)\n    C = MatrixSymbol('C', m, m)\n    X = BlockMatrix([[A, B], [C, A]])\n    Y = BlockMatrix([[A, B], [C, A]])\n    Z = BlockMatrix([[A, B], [C, C]])\n    assert X.structurally_equal(Y)\n    assert not X.structurally_equal(Z)\n"], "sample_470": ["    def test_keep_lazy_text_decorator_on_method(self):\n        class MyClass:\n            @keep_lazy_text\n                return str(arg)\n\n        test_obj = MyClass()\n        lazy_result = test_obj.my_method(lazy(lambda: 123, int))\n        self.assertEqual(lazy_result(), \"123\")\n", "    def test_keep_lazy(self):\n            return \"Lazy value\"\n        \n        @keep_lazy(str)\n            return arg1 + arg2\n\n        lazy_result = my_function(lazy_function(), \"!\")\n        self.assertEqual(lazy_result, \"Lazy value!\")\n        self.assertIsInstance(lazy_result, str)\n", "    def test_keep_lazy(self):\n            return a + b\n\n        lazy_add = keep_lazy(int)(add)\n        self.assertEqual(lazy_add(2, 3), 5)\n        self.assertEqual(lazy_add(lazy(lambda: 2, int), lazy(lambda: 3, int)), 5)\n\n", "    def test_keep_lazy_text(self):\n        @keep_lazy_text\n            return a + b\n        self.assertEqual(func(\"hello\", \"world\"), \"helloworld\")\n        self.assertEqual(func(\"test\"), \"testdefault\")\n", "    def test_lazy_proxy_pickle(self):\n        obj = lazy(lambda: {\"key\": \"value\"}, dict)\n        pickled = pickle.dumps(obj)\n        unpickled = pickle.loads(pickled)\n        self.assertEqual(unpickled(), {\"key\": \"value\"})\n"], "sample_41": ["    def test_dimensionless_unit_to_quantity():\n        assert (u.Unit('') * 10).is_equivalent(10.)\n", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_247": ["    def test_order_by_alias_with_reverse(self):\n        qs = Author.objects.alias(other_age=F('age')).order_by('-other_age')\n        self.assertIs(hasattr(qs.first(), 'other_age'), False)\n        self.assertQuerysetEqual(qs, [57, 46, 35, 34, 34], lambda a: a.age)\n", "    def test_annotate_alias_after_f_expression(self):\n        qs = Book.objects.annotate(\n            price_diff=F('price') - 10,\n        ).alias(\n            price_diff_alias=F('price_diff')\n        ).values('price_diff', 'price_diff_alias')\n        book = qs.first()\n        self.assertEqual(book['price_diff'], book['price_diff_alias'])\n", "    def test_values_with_alias(self):\n        qs = Book.objects.alias(rating_alias=F('rating') - 1).values('isbn', 'rating_alias')\n        self.assertCountEqual(qs, [\n            {'isbn': '159059725', 'rating_alias': 3.5},\n            {'isbn': '159059996', 'rating_alias': 3.0},\n            {'isbn': '013790395', 'rating_alias': 3.0},\n            {'isbn': '155860191', 'rating_alias': 4.0},\n        ])\n", "    def test_exclude_alias(self):\n        qs = Book.objects.alias(rating_alias=F('rating') - 1).exclude(rating_alias=3.5)\n        self.assertIs(hasattr(qs.first(), 'rating_alias'), False)\n        self.assertSequenceEqual(qs, [self.b2, self.b3, self.b4])\n\n", "    def test_select_related_alias(self):\n        qs = Book.objects.select_related('publisher').alias(\n            publisher_name=F('publisher__name'),\n        )\n        self.assertIs(hasattr(qs.first(), 'publisher_name'), False)\n"], "sample_1016": ["compilation error", "def test_octave_matrix_assign_to_complex():\n    A = Matrix([[x, 2, 3*I]])\n    B = MatrixSymbol('B', 1, 3)\n    assert mcode(A, assign_to=B) == \"B = [x 2 3i];\"\n", "compilation error", "def test_octave_matrix_element_substitution():\n    A = MatrixSymbol(\"A\", 3, 3)\n    B = MatrixSymbol(\"B\", 3, 3)\n    C = A[1, 1]\n\n    assert mcode(C.subs(A, B)) == \"B(2, 2)\"\n", "def test_MatrixElement_substitution():\n    A = MatrixSymbol(\"A\", 3, 3)\n    B = MatrixSymbol(\"B\", 3, 3)\n    C = MatrixSymbol(\"C\", 3, 3)\n\n    F = C[0, 0].subs(C, A*B)\n    assert mcode(F) == \"(A*B)(1, 1)\" \n\n"], "sample_555": ["def test_patch_with_data_coordinates():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1])\n    patch = Rectangle((0.5, 0.5), 0.1, 0.1, transform=ax.transData)\n    ax.add_patch(patch)\n", "compilation error", "    def test_empty_patch_collection(self):\n        fig, ax = plt.subplots()\n        col = mcollections.PatchCollection(\n            [], match_original=True\n        )\n        ax.add_collection(col)\n        ax.autoscale_view()\n", "def test_annulus_empty_radii():\n    with pytest.raises(ValueError, match=\"radii cannot be empty\"):\n        Annulus((0, 0), (), 0.1)\n", "compilation error"], "sample_1066": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_805": ["compilation error", "compilation error", "def test_r2_score_edge_cases():\n    # Test case for empty arrays\n    assert_raises(ValueError, r2_score, [], [])\n    \n    # Test case for arrays with only one unique value\n    y_true = [1, 1, 1]\n    y_pred = [2, 2, 2]\n    assert_allclose(r2_score(y_true, y_pred), -1.0)\n", "compilation error", "compilation error"], "sample_742": ["    def test_logistic_regression_decision_function_shape():\n        # Test that the decision_function output shape is correct for different\n        # input shapes and multi_class options\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        y = np.array([0, 1, 0])\n\n        for multi_class in ['ovr', 'multinomial']:\n            clf = LogisticRegression(multi_class=multi_class)\n            clf.fit(X, y)\n\n            # Test decision_function shape for single sample\n            single_sample = [[7, 8]]\n            decision_function_single = clf.decision_function(single_sample)\n            assert decision_function_single.shape == (1,\n                                                     clf.classes_.shape[0] if multi_class == 'multinomial' else 1)\n\n            # Test decision_function shape for multiple samples\n            decision_function = clf.decision_function(X)\n            assert decision_function.shape == (X.shape[0],\n                                              clf.classes_.shape[0] if multi_class == 'multinomial' else 1)\n\n", "    def test_warm_start_with_cv():\n        # Test warm start with cross validation\n        X, y = make_classification(n_samples=50, n_features=20,\n                                   random_state=0)\n        for solver in ['newton-cg', 'sag', 'saga', 'lbfgs']:\n            clf = LogisticRegressionCV(solver=solver, warm_start=True,\n                                       cv=3, random_state=42)\n            clf.fit(X, y)\n        # Assert that the fitted model has non-zero coefficients\n        assert np.any(clf.coef_)\n", "    def test_logistic_regression_sample_weights_ovr():\n        X, y = make_classification(n_samples=20, n_features=5, n_informative=3,\n                                   n_classes=3, random_state=0)\n        sample_weight = y + 1\n\n        for solver in ['lbfgs', 'newton-cg', 'sag', 'saga']:\n            clf_sw_none = LogisticRegression(solver=solver, multi_class='ovr',\n                                            random_state=42)\n            clf_sw_none.fit(X, y)\n            clf_sw_ones = LogisticRegression(solver=solver, multi_class='ovr',\n                                            random_state=42)\n            clf_sw_ones.fit(X, y, sample_weight=np.ones(y.shape[0]))\n            assert_array_almost_equal(\n                clf_sw_none.coef_, clf_sw_ones.coef_, decimal=4)\n\n            clf_sw = LogisticRegression(solver=solver, multi_class='ovr',\n                                        random_state=42)\n            clf_sw.fit(X, y, sample_weight=sample_weight)\n            assert_array_almost_equal(\n                clf_sw_none.coef_, clf_sw.coef_, decimal=4)\n", "def test_predict_proba_consistency():\n    # Test for consistency between predict_proba and predict\n    X, y = make_classification(n_samples=50, n_features=20, random_state=0)\n    clf = LogisticRegression(random_state=42)\n    clf.fit(X, y)\n    y_proba = clf.predict_proba(X)\n    y_pred = clf.predict(X)\n    assert_equal(np.argmax(y_proba, axis=1), y_pred)\n", "    def test_predict_proba_binary():\n        # Make sure binary predict_proba outputs probabilities\n        # for both classes with shape (n_samples, 2)\n        X, y = make_classification(n_samples=50, n_features=20, random_state=0,\n                                   n_classes=2)\n\n        for solver in ['newton-cg', 'liblinear', 'sag', 'saga', 'lbfgs']:\n            clf = LogisticRegression(solver=solver, multi_class='ovr')\n            clf.fit(X, y)\n            probabilities = clf.predict_proba(X)\n            assert_equal(probabilities.shape, (50, 2))\n            assert_almost_equal(probabilities[:, 0] + probabilities[:, 1],\n                                np.ones(50))\n"], "sample_50": ["    def test_no_user(self):\n        self.assertEqual(\n            self._run_it({\n                'database': 'dbname',\n                'host': 'somehost',\n                'port': '444',\n            }), (\n                ['psql', '-h', 'somehost', '-p', '444', 'dbname'],\n                None,\n            )\n        )\n", "    def test_defaults(self):\n        self.assertEqual(\n            self._run_it({\n                'database': 'dbname',\n            }), (\n                ['psql', 'dbname'],\n                None,\n            )\n        )\n", "    def test_defaults(self):\n        self.assertEqual(\n            self._run_it({\n                'database': 'dbname',\n            }), (\n                ['psql', 'dbname'],\n                None,\n            )\n        )\n", "    def test_no_user(self):\n        self.assertEqual(\n            self._run_it({\n                'database': 'dbname',\n                'host': 'somehost',\n                'port': '444',\n            }), (\n                ['psql', '-h', 'somehost', '-p', '444', 'dbname'],\n                None,\n            )\n        )\n\n", "    def test_no_user(self):\n        self.assertEqual(\n            self._run_it({\n                'database': 'dbname',\n                'host': 'somehost',\n                'port': '444',\n            }), (\n                ['psql', '-h', 'somehost', '-p', '444', 'dbname'],\n                None,\n            )\n        )\n"], "sample_130": ["    def test_related_isnull_filtering(self):\n        query = Query(Item)\n        # Test filtering items that have a related author or those that don't\n        with self.subTest(\"filter with author\"):\n            where = query.build_where(Q(creator__isnull=False))\n            lookup = where.children[0]\n            self.assertIsInstance(lookup, RelatedIsNull)\n            self.assertFalse(lookup.negated)\n\n        with self.subTest(\"filter without author\"):\n            where = query.build_where(Q(creator__isnull=True))\n            lookup = where.children[0]\n            self.assertIsInstance(lookup, RelatedIsNull)\n            self.assertTrue(lookup.negated)\n", "    def test_double_underscore_lookup(self):\n        query = Query(ObjectC)\n        where = query.build_where(Q(objecta__objectb__id=1))\n        lookup = where.children[0]\n        self.assertIsInstance(lookup, Exact)\n        self.assertEqual(lookup.lhs.target.name, 'objecta__objectb__id')\n", "    def test_related_field_isnull(self):\n        query = Query(Item)\n        where = query.build_where(Q(creator__isnull=True))\n        lookup = where.children[0]\n        self.assertIsInstance(lookup, IsNull)\n        self.assertIsInstance(lookup.lhs, SimpleCol)\n        self.assertEqual(lookup.lhs.target, Item._meta.get_field('creator'))\n", "    def test_related_field_isnull(self):\n        query = Query(Ranking)\n        where = query.build_where(Q(author__isnull=True))\n        lookup = where.children[0]\n        self.assertIsInstance(lookup, IsNull)\n        self.assertEqual(lookup.lhs.target, Ranking._meta.get_field('author'))\n", "    def test_multiple_select_related(self):\n        query = Query(Item)\n        query.add_select_related(['creator'])\n        query.add_select_related(['note'])\n        self.assertEqual(query.select_related, {'creator': {}, 'note': {}})\n"], "sample_1142": ["compilation error", "compilation error", "def test_ZeroMatrix_add():\n    z = ZeroMatrix(2, 2)\n    a = Matrix([[1, 2], [3, 4]])\n    assert z + a == a\n    assert a + z == a\n", "def test_MatrixSet_subs():\n    M = MatrixSet(2, 2, set=S.Reals)\n    A = MatrixSymbol('A', 2, 2)\n    assert A.subs(A, M.example()) in M\n", "    def test_ZeroMatrix_add():\n        z1 = ZeroMatrix(2, 2)\n        z2 = ZeroMatrix(2, 2)\n        assert z1 + z2 == z1\n        assert z1 + z2 == ZeroMatrix(2, 2)\n\n"], "sample_127": ["    def test_bulk_create_with_deferred_fields(self):\n        class MyDeferredModel(models.Model):\n            name = models.CharField(max_length=100)\n            description = models.TextField(blank=True)\n\n            class Meta:\n                deferred = True\n\n        MyDeferredModel.objects.bulk_create([\n            MyDeferredModel(name='Deferred Model 1'),\n            MyDeferredModel(name='Deferred Model 2', description='This is a description'),\n        ])\n        self.assertEqual(MyDeferredModel.objects.count(), 2)\n", "    def test_bulk_create_with_default_value(self):\n        class DefaultModel(models.Model):\n            name = models.CharField(max_length=100)\n            value = models.IntegerField(default=42)\n\n        DefaultModel.objects.bulk_create([\n            DefaultModel(name='one'),\n            DefaultModel(name='two'),\n        ])\n        objs = DefaultModel.objects.all()\n        self.assertEqual(objs[0].value, 42)\n        self.assertEqual(objs[1].value, 42)\n", "    def test_bulk_create_with_existing_pk(self):\n        country = Country.objects.create(name='United States of America', iso_two_letter='US')\n        existing_pk = country.pk\n        with self.assertRaises(IntegrityError):\n            Country.objects.bulk_create([Country(id=existing_pk, name='New Name', iso_two_letter='US')])\n", "    def test_ignore_conflicts(self):\n        TwoFields.objects.bulk_create([\n            TwoFields(f1=1, f2=2),\n            TwoFields(f1=2, f2=3),\n        ])\n        with self.assertNumQueries(1):\n            TwoFields.objects.bulk_create([\n                TwoFields(id=1, f1=1, f2=2),\n                TwoFields(f1=3, f2=4),\n            ], ignore_conflicts=True)\n        self.assertEqual(TwoFields.objects.count(), 3)\n", "    def test_bulk_create_with_null_boolean_field(self):\n        class TestModel(models.Model):\n            name = models.CharField(max_length=100)\n            is_active = models.BooleanField(null=True)\n\n        TestModel.objects.bulk_create([\n            TestModel(name='Alice', is_active=True),\n            TestModel(name='Bob', is_active=False),\n            TestModel(name='Charlie', is_active=None),\n        ])\n        self.assertEqual(TestModel.objects.count(), 3)\n        alice = TestModel.objects.get(name='Alice')\n        bob = TestModel.objects.get(name='Bob')\n        charlie = TestModel.objects.get(name='Charlie')\n        self.assertTrue(alice.is_active)\n        self.assertFalse(bob.is_active)\n        self.assertIsNone(charlie.is_active)\n\n"], "sample_1086": ["compilation error", "def test_issue_16703():\n    from sympy import MatrixSymbol, eye, zeros, ones\n    A = MatrixSymbol('A', 2, 2)\n    assert str(A.func(*A.args)) == str(A)\n    assert str(eye(2).func(*eye(2).args)) == str(eye(2))\n    assert str(zeros(2, 3).func(*zeros(2, 3).args)) == str(zeros(2, 3))\n    assert str(ones(3, 2).func(*ones(3, 2).args)) == str(ones(3, 2)) \n", "compilation error", "compilation error", "compilation error"], "sample_692": ["    def test_tmpdir_factory_with_given_basetemp(pytester: Pytester) -> None:\n        tmp_dir = pytester.mkdir(\"mytmp\")\n        pytester.makepyfile(\n            \"\"\"\n                assert str(tmpdir_factory.getbasetemp()) == '{}'\n        \"\"\".format(tmp_dir)\n        )\n        result = pytester.runpytest(\"--basetemp=\" + str(tmp_dir))\n        assert result.ret == 0\n\n", "    def test_tmp_path_factory_create_numbered_dir_with_cleanup(\n        self, tmp_path, tmp_path_factory: TempPathFactory", "    def test_tmpdir_cleanup_on_error(pytester: Pytester) -> None:\n        p = pytester.makepyfile(\n            \"\"\"\n            import os\n                f = tmpdir.join(\"file.txt\")\n                f.write(\"hello\")\n                os.chmod(str(f), 0o444)  # Make file read-only\n\n                # This will fail because the file is read-only \n                f.write(\"world\")\n        \"\"\"\n        )\n        result = pytester.runpytest(p)\n        assert result.ret == 1\n        assert result.stderr.fnmatch_lines([\"*IOError*permission*\"])\n        assert pytester.path.join(p.basename).exists()\n", "    def test_tmpdir_invalid_basetemp(pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import os\n                nonexistent_basetemp = tmpdir / 'nonexistent_basetemp'\n                os.mkdir(nonexistent_basetemp)\n                assert os.path.exists(nonexistent_basetemp)\n        \"\"\"\n        )\n        result = pytester.runpytest(\"--basetemp=nonexistent_basetemp\")\n        assert result.ret == 1\n        result.stdout.fnmatch_lines([\"*ValueError*\"])\n", "def test_tmpdir_with_unicode(pytester: Pytester) -> None:\n    p = pytester.makepyfile(\n        \"\"\"\n        import os\n            path = tmpdir.join('f\u00f6\u0151')\n            path.mkdir()\n            assert path.exists()\n    \"\"\"\n    )\n    result = pytester.runpytest(p)\n    assert result.ret == 0\n"], "sample_492": ["    def test_serialize_foreignkey_with_to_field(self):\n        class AnotherModel(models.Model):\n            name = models.CharField(max_length=20)\n\n        class MyModel(models.Model):\n            related_model = models.ForeignKey(AnotherModel, on_delete=models.CASCADE, to_field=\"name\")\n        field = MyModel._meta.get_field(\"related_model\")\n        string = MigrationWriter.serialize(field)[0]\n        self.assertEqual(\n            string, \"models.ForeignKey(AnotherModel, on_delete=models.CASCADE, to_field='name')\"\n        )\n        self.serialize_round_trip(field)\n", "    def test_custom_serializer_deconstruction(self):\n        class CustomModel(models.Model):\n            field = models.CharField(default=DeconstructibleInstances)\n\n        migration = type(\n            \"Migration\",\n            (migrations.Migration,),\n            {\n                \"operations\": [\n                    migrations.AddField(\n                        model_name=\"custommodel\",\n                        name=\"field\",\n                        field=models.CharField(default=DeconstructibleInstances),\n                    ),\n                ],\n            },\n        )\n        writer = MigrationWriter(migration)\n        output = writer.as_string()\n        self.assertIn(\n            \"models.CharField(default=migrations.test_writer.DeconstructibleInstances)\",\n            output,\n        )\n\n", "    def test_serialize_custom_field_instance(self):\n        class CustomField(models.Field):\n                super().__init__(*args, **kwargs)\n                self.instance = instance\n\n        field = CustomField(instance=\"some_instance\")\n        self.assertSerializedEqual(field)\n\n        # Test deconstructing a custom field instance\n        # with a non-default argument\n        field = CustomField(instance=\"some_instance\")\n        string, imports = MigrationWriter.serialize(field)\n        self.assertEqual(string, 'migrations.test_writer.CustomField(instance=\"some_instance\")')\n", "    def test_serialize_path_with_spaces(self):\n        path = pathlib.Path('/path with spaces/file.txt')\n        string, imports = MigrationWriter.serialize(path)\n        self.assertEqual(string, \"pathlib.Path('/path with spaces/file.txt')\")\n        self.assertIn(\"import pathlib\", imports)\n\n", "    def test_serialize_function_with_defaults(self):\n            return a + b\n\n        self.assertSerializedResultEqual(\n            example_func,\n            (\n                \"migrations.test_writer.example_func\",\n                {\"import migrations.test_writer\"},\n            ),\n        )\n\n"], "sample_1152": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_870": ["def test_kernel_diagonal(kernel):\n    \"\"\"\n    Test the kernel.diag() method and ensure the input array X is not mutated.\n    \"\"\"\n    rng = np.random.RandomState(0)\n    X = rng.rand(5, 3)\n    X_copy = X.copy()  # Make a copy to compare against\n\n    kernel.diag(X)\n\n    assert np.allclose(X, X_copy), \"The input matrix X should not be mutated by kernel.diag()\"\n", "def test_kernel_diag_not_modified(kernel):\n    rng = np.random.RandomState(0)\n    X = rng.rand(10, 5)\n    X_copy = X.copy()\n    gpr = GaussianProcessRegressor(kernel=kernel)\n    gpr.fit(X, rng.rand(10))\n    gpr.log_marginal_likelihood()\n    assert np.allclose(X, X_copy)\n\n", "    def test_kernel_diag_input_preservation():\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        kernel = CustomKernel(1.0)\n        diag = kernel.diag(X)\n        assert np.array_equal(diag, np.array([1, 3, 5]))\n        # Test that X hasn't been modified\n        assert np.array_equal(X, np.array([[1, 2], [3, 4], [5, 6]]))\n", "    def test_custom_kernel():\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        y = np.array([1, 2, 3])\n\n        kernel = CustomKernel(constant_value=1.0)\n        gpr = GaussianProcessRegressor(kernel=kernel, normalize_y=False)\n        gpr.fit(X, y)\n        assert_array_equal(gpr.kernel_.diag(X), X[:, 0])\n", "    def test_custom_kernel_diag(self):\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        kernel = CustomKernel(constant_value=1.0)\n        result = kernel.diag(X)\n        assert_array_equal(result, np.array([1, 3, 5]))\n"], "sample_486": ["    def test_inlineformset_factory_auto_pk_parent_uuid_child_with_unique_constraint(self):\n        \"\"\"\n        Test that inline formsets with a parent model using AutoField primary key \n        and a child model using UUID primary key work correctly with a unique \n        constraint on the child model.\n        \"\"\"\n        FormSet = inlineformset_factory(\n            AutoPKParent, UUIDPKChild, fields=\"__all__\", extra=1\n        )\n        formset = FormSet()\n        self.assertEqual(len(formset.forms), 2)\n        # Attempt to create two identical UUIDPKChild instances\n        formset.forms[0].cleaned_data = {\"name\": \"Foo\", \"parent\": formset.instance}\n        formset.forms[1].cleaned_data = {\"name\": \"Foo\", \"parent\": formset.instance}\n        self.assertFalse(formset.is_valid())\n", "    def test_inlineformset_factory_nulls_default_pks_child_with_fk_to_uuid_field(self):\n        \"\"\"\n        #24958 - Variant of test_inlineformset_factory_nulls_default_pks for\n        the case of a parent object with a UUID primary key and a child\n        object that refers to the parent via a foreign key to a UUID field.\n        \"\"\"\n        FormSet = inlineformset_factory(\n            UUIDPKParent, ChildRelatedViaAK, fields=\"__all__\"\n        )\n        formset = FormSet()\n        self.assertIsNone(formset.forms[0].fields[\"parent\"].initial)\n", "    def test_inlineformset_factory_ignores_default_pks_on_submit_with_editable_pk(self):\n        \"\"\"\n        #24958 - Ensure that when submitting a formset with a child having\n        an editable primary key (natural key), the default value is ignored\n        for the related parent.\n        \"\"\"\n        FormSet = inlineformset_factory(UUIDPKParent, ChildWithEditablePK, fields=\"__all__\")\n        formset = FormSet(\n            {\n                \"childwithediTablePK_set-TOTAL_FORMS\": 3,\n                \"childwithediTablePK_set-INITIAL_FORMS\": 0,\n                \"childwithediTablePK_set-MAX_NUM_FORMS\": \"\",\n                \"childwithediTablePK_set-0-name\": \"Foo\",\n                \"childwithediTablePK_set-1-name\": \"\",\n                \"childwithediTablePK_set-2-name\": \"\",\n                \"childwithediTablePK_set-0-pk\": \"foo-pk\",\n            }\n        )\n        self.assertTrue(formset.is_valid())\n        self.assertIsNone(formset.instance.uuid)\n        self.assertIsNone(formset.forms[0].instance.parent_id)\n", "    def test_inlineformset_factory_handles_null_parent_pk(self):\n        \"\"\"\n        #25068 - Ensure inlineformset_factory handles cases where the parent\n        object's primary key is None.\n        \"\"\"\n        FormSet = inlineformset_factory(UUIDPKParent, UUIDPKChild, fields=\"__all__\")\n\n        parent = UUIDPKParent.objects.create()\n        formset = FormSet(instance=parent)\n        self.assertIsNone(formset.forms[0].fields[\"parent\"].initial)\n\n", "    def test_inlineformset_factory_save_new_sets_parent_pk(self):\n        FormSet = inlineformset_factory(UUIDPKParent, UUIDPKChild, fields=\"__all__\")\n        parent = UUIDPKParent.objects.create(uuid=uuid.uuid4())\n        formset = FormSet(\n            {\n                \"uuidpkchild_set-TOTAL_FORMS\": 1,\n                \"uuidpkchild_set-INITIAL_FORMS\": 0,\n                \"uuidpkchild_set-MAX_NUM_FORMS\": \"\",\n                \"uuidpkchild_set-0-name\": \"Foo\",\n                \"uuidpkchild_set-0-parent\": parent.id,\n            }\n        )\n        self.assertTrue(formset.is_valid())\n        formset.save()\n        child = UUIDPKChild.objects.filter(parent=parent).first()\n        self.assertEqual(child.parent_id, parent.id) \n"], "sample_1163": ["compilation error", "compilation error", "def test_Abs_conjugate():\n    x = Symbol('x', real=True)\n    y = Symbol('y', imaginary=True)\n    assert Abs(conjugate(x)) == Abs(x)\n    assert Abs(conjugate(y)) == Abs(y)\n    assert Abs(conjugate(x + y)) == Abs(x + y)\n", "compilation error", "compilation error"], "sample_1177": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_141": ["    def test_datetime_serialization(self):\n        obj = Score.objects.create(score=5.0, pub_date=datetime.datetime(2023, 10, 26, 10, 0))\n        serialized = serializers.serialize('json', [obj])\n        deserialized_obj = list(serializers.deserialize('json', serialized))[0].object\n        self.assertEqual(deserialized_obj.pub_date, obj.pub_date)\n", "    def test_json_serializer_unicode(self):\n        \"\"\"\n        Test that the JSON serializer handles unicode characters correctly.\n        \"\"\"\n        s = serializers.json.Serializer()\n        article = Article(headline=\"\u00dcnicode test\", pub_date=datetime.datetime.now())\n        article.save()\n        data = s.serialize([article])\n        deserialized_data = json.loads(data)\n        self.assertEqual(deserialized_data[0]['fields']['headline'], \"\u00dcnicode test\")\n", "    def test_serialize_datetime_field_with_timezone(self):\n        with self.timezone('US/Eastern'):\n                now = datetime.datetime.now()\n                article = self.article_factory(headline=\"Article with timezone\", pub_date=now)\n        \n        serialized = serializers.json.Serializer().serialize([article])\n\n        data = json.loads(serialized)\n        self.assertEqual(data[0]['fields']['pub_date'], now.isoformat()[:-6] + now.strftime('%z')) \n", "    def test_json_deserializer_datetime(self):\n        test_string = \"\"\"\n        [\n            {\n                \"pk\": 1,\n                \"model\": \"serializers.article\",\n                \"fields\": {\n                    \"headline\": \"Poker has no place on ESPN\",\n                    \"pub_date\": \"2006-06-16T11:00:00Z\"\n                }\n            }\n        ]\n        \"\"\"\n        article = list(serializers.deserialize('json', test_string))[0].object\n        self.assertEqual(article.pub_date, datetime.datetime(2006, 6, 16, 11, 0, tzinfo=datetime.timezone.utc))\n", "    def test_serialize_datetime_aware(self):\n        with self.timezone.override(datetime.timezone.utc):\n            article = Article.objects.create(\n                headline=\"Time zone aware serialization\",\n                pub_date=datetime.datetime(2023, 10, 26, 12, 0, 0, tzinfo=datetime.timezone.utc),\n            )\n\n        data = serializers.json.Serializer().serialize(\n            [article], use_natural_foreign_keys=True\n        )\n        data = json.loads(data)\n        self.assertEqual(data[0]['fields']['pub_date'], '2023-10-26T12:00:00Z')\n"], "sample_4": ["    def test_readwrite_html_table_units(self, cosmo, read, write, tmp_path, add_cu):\n        \"\"\"Test cosmology with units to ascii.html and back.\"\"\"\n        fp = tmp_path / \"test_readwrite_html_table_units.html\"\n\n        write(fp, format=\"ascii.html\")\n\n        # some checks on the saved file\n        tbl = QTable.read(fp)\n        for col in tbl.colnames:\n            if col == 'name':\n                continue\n\n            try:\n                unit = getattr(type(cosmo), col).unit\n                assert tbl[col].unit == unit\n            except AttributeError:\n              # Not all parameters will have units\n              pass\n\n        # ------------\n        # From Table\n\n        got = read(fp, format=\"ascii.html\")\n        assert got == cosmo\n", "    def test_readwrite_html_nonascii_names(self, cosmo_cls, cosmo, read, write, tmp_path, add_cu):\n        \"\"\"Test cosmology with non-ascii characters in parameter names.\"\"\"\n        fp = tmp_path / \"test_readwrite_html_nonascii_names.html\"\n\n        # Create a copy of the cosmology with a non-ascii character in a parameter name\n        cosmo_nonascii = cosmo.clone(name=\"cosmo_nonascii\")\n        cosmo_nonascii.parameters[\"H0\"].name = \"H\u2080\"\n\n        write(fp, format=\"ascii.html\", latex_names=False)\n\n        # Read the cosmology back in\n        got = cosmo_cls.read(fp, format=\"ascii.html\")\n\n        # Assert that the cosmology was read in correctly, including the non-ascii character\n        assert got.H\u2080 == cosmo_nonascii.H\u2080\n", "    def test_readwrite_html_units(self, cosmo, read, write, tmp_path, add_cu):\n        \"\"\"Test writing with and without unit conversion.\"\"\"\n        fp = tmp_path / \"test_readwrite_html_units.html\"\n\n        # Write with unit conversion\n        write(fp, format=\"ascii.html\")\n        tbl = QTable.read(fp)\n        for n in tbl.colnames:\n            if n not in (\"name\", \"cosmology\"):\n                assert tbl[n].unit is not None\n        cosmo_from_table = read(fp, format=\"ascii.html\")\n        assert cosmo_from_table == cosmo\n\n        # Write without unit conversion\n        write(fp, format=\"ascii.html\", cosmology=cosmo)\n        tbl = QTable.read(fp)\n        for n in tbl.colnames:\n            if n not in (\"name\", \"cosmology\"):\n                assert tbl[n].unit is None\n        cosmo_from_table = read(fp, format=\"ascii.html\")\n        assert cosmo_from_table == cosmo\n\n", "    def test_readwrite_html_table_units(self, cosmo, read, write, tmp_path, add_cu):\n        \"\"\"Test cosmology -> ascii.html -> cosmology with units.\"\"\"\n        fp = tmp_path / \"test_readwrite_html_table_units.html\"\n\n        # ------------\n        # To Table\n\n        write(fp, format=\"ascii.html\")\n\n        # ------------\n        # From Table\n\n        got = read(fp, format=\"ascii.html\")\n        assert got == cosmo\n", "    def test_readwrite_html_table_units(self, cosmo, read, write, tmp_path, add_cu):\n        \"\"\"Test cosmology -> ascii.html -> cosmology, including units.\"\"\"\n        fp = tmp_path / \"test_readwrite_html_table_units.html\"\n\n        # ------------\n        # To Table\n\n        write(fp, format=\"ascii.html\")\n\n        # ------------\n        # From Table\n\n        tbl = QTable.read(fp)\n\n        for col in tbl.colnames:\n            if col not in  _FORMAT_TABLE.values(): \n                param = getattr(cosmo, col, None)\n                if param is not None and param.unit is not None:\n                    assert tbl[col].unit == param.unit\n        \n        got = cosmo.read(fp, format=\"ascii.html\")\n        assert got == cosmo\n"], "sample_277": ["    def test_combine_empty_negated_q(self):\n        q1 = ~Q()\n        q2 = Q(x=1)\n        self.assertEqual(q1 & q2, q2)\n        self.assertEqual(q2 & q1, q2)\n\n        q1 = ~Q()\n        q2 = Q(x__in={}.keys())\n        self.assertEqual(q1 & q2, q2)\n        self.assertEqual(q2 & q1, q2)\n\n", "    def test_in_empty_list(self):\n        q = Q(x__in=[])\n        self.assertEqual(str(q), 'WHERE NOT x IS NOT NULL')\n", "    def test_resolve_expression(self):\n        q = Q(product__name__contains='foo')\n        # ... code to test resolve_expression ...\n", "    def test_reconstruct_nested(self):\n        q = Q(Q(price__gt=F('discounted_price')))\n        path, args, kwargs = q.deconstruct()\n        self.assertEqual(Q(*args, **kwargs), q)\n", "    def test_combine_with_empty_q_objects(self):\n        q1 = Q(x=1)\n        q2 = Q()\n        self.assertEqual(q1 & q2, q2)\n        self.assertEqual(q1 | q2, q1)\n"], "sample_440": ["    def test_update_conflicts_unique_fields_both(self):\n        self._test_update_conflicts(unique_fields=[\"number\", \"rank\"])\n", "    def test_update_conflicts_no_unique_fields(self):\n        self._test_update_conflicts(unique_fields=[])\n", "    def test_update_conflicts_unique_fields_multiple(self):\n        self._test_update_conflicts(unique_fields=[\"number\", \"rank\"])\n", "    def test_update_conflicts_unique_fields_both(self):\n        self._test_update_conflicts(unique_fields=[\"number\", \"rank\"])\n", "    def test_update_conflicts_no_unique_fields(self):\n        self._test_update_conflicts(unique_fields=[])\n"], "sample_739": ["compilation error", "compilation error", "compilation error", "    def test_label_binarize_empty_array(self):\n        y = np.array([], dtype=int)\n        classes = [0, 1, 2]\n        pos_label = 2\n        neg_label = 0\n\n        expected = np.array([], dtype=int).reshape((0, 3))\n\n        for sparse_output in [True, False]:\n            binarized = label_binarize(y, classes, neg_label=neg_label,\n                                       pos_label=pos_label,\n                                       sparse_output=sparse_output)\n            assert_array_equal(toarray(binarized), expected)\n", "compilation error"], "sample_754": ["def test_sparse_pca_max_iter():\n    rng = np.random.RandomState(0)\n    Y, _, _ = generate_toy_data(3, 10, (8, 8), random_state=rng)\n    spca = SparsePCA(n_components=3, max_iter=5, random_state=rng,\n                     normalize_components=norm_comp)\n    spca.fit(Y)\n    assert_true(spca.n_iter_ <= 5)\n", "def test_sparse_pca_error():\n    rng = np.random.RandomState(0)\n    X_dense = rng.randn(10, 5)\n    X_sparse = sp.csr_matrix(X_dense)\n    spca = SparsePCA(n_components=3, random_state=rng,\n                     normalize_components=norm_comp)\n\n    spca.fit(X_dense)\n    error_dense = spca.error_\n    spca.fit(X_sparse)\n    error_sparse = spca.error_\n    assert_allclose(error_dense, error_sparse)\n", "    def test_sparsepca_warm_start():\n        rng = np.random.RandomState(0)\n        n_samples = 100\n        n_features = 20\n        n_components = 5\n        X = rng.randn(n_samples, n_features)\n\n        # Initial fit\n        spca = SparsePCA(n_components=n_components, random_state=rng)\n        spca.fit(X)\n\n        # Warm start with existing components and loadings\n        spca_warm = SparsePCA(n_components=n_components, random_state=rng,\n                             U_init=spca.components_.T, V_init=spca.components_)\n        spca_warm.fit(X)\n\n        # Assert that warm start leads to the same components\n        assert_allclose(spca.components_, spca_warm.components_)\n", "def test_sparse_pca_ridge_alpha():\n    rng = np.random.RandomState(0)\n    Y, _, _ = generate_toy_data(3, 10, (8, 8), random_state=rng)  # Wide array\n    # Test that ridge_alpha parameter works correctly\n    spca_lars = SparsePCA(n_components=3, method='lars', alpha=1,\n                          ridge_alpha=0.1, random_state=0,\n                          normalize_components=norm_comp)\n    spca_lars.fit(Y)\n    U1 = spca_lars.transform(Y)\n    spca_lars2 = SparsePCA(n_components=3, method='lars', alpha=1,\n                          ridge_alpha=0.5, random_state=0,\n                          normalize_components=norm_comp).fit(Y)\n    U2 = spca_lars2.transform(Y)\n    assert_false(np.allclose(U1, U2))\n", "    def test_sparse_pca_raises_error_on_empty_data(self):\n        with pytest.raises(ValueError, match=\"Input data should have at least one sample.\"):\n            SparsePCA(n_components=2).fit(np.array([]))\n"], "sample_768": ["    def test_time_series_split_errors():\n        X = np.arange(10).reshape(-1, 1)\n        y = np.ones(10)\n\n        # Test for invalid values of n_splits\n        assert_raises(ValueError, TimeSeriesSplit, n_splits=0)\n        assert_raises(ValueError, TimeSeriesSplit, n_splits=-1)\n        assert_raises(ValueError, TimeSeriesSplit, n_splits=1)\n        \n        # Test for invalid values of max_train_size\n        assert_raises(ValueError, TimeSeriesSplit, max_train_size=-1)\n        assert_raises(ValueError, TimeSeriesSplit, max_train_size=0)\n", "    def test_check_cv_default_warn_error():\n        # Test that warnings are raised when cv='warn' with\n        # parameter error\n        with pytest.warns(UserWarning, match='The provided value for cv='):\n            check_cv(cv='warn', y=None)\n", "    def test_repeated_kfold_determinstic_split_with_shuffle():\n\n        X = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13, 14]]\n        y = [1, 1, 1, 0, 0, 0, 0]\n        random_state = 1944695409\n        rskf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=random_state)\n\n        for _ in range(3):\n            splits = rskf.split(X, y)\n            train, test = next(splits)\n            assert_array_equal(train, [1, 4])\n            assert_array_equal(test, [0, 2, 3])\n\n            train, test = next(splits)\n            assert_array_equal(train, [0, 2, 3])\n            assert_array_equal(test, [1, 4])\n\n            train, test = next(splits)\n            assert_array_equal(train, [2, 3])\n            assert_array_equal(test, [0, 1, 4])\n\n            train, test = next(splits)\n            assert_array_equal(train, [0, 1, 4])\n            assert_array_equal(test, [2, 3])\n\n            assert_raises(StopIteration, next, splits)\n\n", "    def test_check_cv_error_message():\n        # Check that check_cv raises an informative error message\n        msg = \"The provided 'cv' is neither a string nor a cross-validator instance.\"\n        with pytest.raises(ValueError, match=msg):\n            check_cv(123)\n", "compilation error"], "sample_939": ["def test_unparse_function_definition(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0]) == expected\n", "def test_unparse_function(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0], source) == expected\n", "def test_unparse_function_signature(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0], source) == expected\n", "def test_parse_type_comments():\n    code = \"\"\"\n        return a, b\n    \"\"\"\n    module = ast.parse(code, mode='exec', type_comments=True)\n\n    func_def = module.body[0]\n    assert func_def.args.args[0].annotation\n    assert func_def.args.args[1].annotation\n    assert func_def.returns\n", "def test_unparse_function_def():\n    source = \"\"\""], "sample_145": ["    def test_actions_valid(self):\n            pass\n\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (action1, action2)\n\n        self.assertIsValid(BandAdmin, Band)\n", "        def action2(modeladmin, request, queryset):\n            pass\n", "    def test_actions_valid(self):\n            pass\n\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (action1, action2)\n        \n        self.assertIsValid(BandAdmin, Band) \n", "        def action2(modeladmin, request, queryset):\n            pass\n", "        def action2(modeladmin, request, queryset):\n            pass\n"], "sample_1198": ["    def test_parse_mathematica_functions_2():\n        assert parse_mathematica(\"Sin[x] Cos[y]\") == sin(x)*cos(y)\n        assert parse_mathematica(\"Sin[x]^2 Cos[y]^2\") == sin(x)**2*cos(y)**2\n        assert parse_mathematica(\"Cos[x]^2 (1 - Cos[y]^2)\") == cos(x)**2*(1-cos(y)**2)\n", "def test_mathematica_parse_errors():\n\n    parser = MathematicaParser()\n    raises(SyntaxError, lambda: parser.parse_mathematica(\"a +\"))\n    raises(SyntaxError, lambda: parser.parse_mathematica(\"a -\"))\n    raises(SyntaxError, lambda: parser.parse_mathematica(\"a *\"))\n    raises(SyntaxError, lambda: parser.parse_mathematica(\"a /\"))\n    raises(SyntaxError, lambda: parser.parse_mathematica(\"a ^\"))\n    raises(SyntaxError, lambda: parser.parse_mathematica(\"(a + b\"))\n    raises(SyntaxError, lambda: parser.parse_mathematica(\"(* a + b\"))\n    raises(SyntaxError, lambda: parser.parse_mathematica(\"a)\"))\n    raises(SyntaxError, lambda: parser.parse_mathematica(\"[a\"))\n    raises(SyntaxError, lambda: parser.parse_mathematica(\"{a\"))\n    raises(SyntaxError, lambda: parser.parse_mathematica(\"a[[b\"))\n    raises(SyntaxError, lambda: parser.parse_mathematica(\"a[[b,c,d\"))\n\n\n\n", "def test_mathematica_function_args():\n    parser = MathematicaParser()\n    assert parser.parse_mathematica(\"f[x, y, z]\") == Function(\"f\")(x, y, z)\n    assert parser.parse_mathematica(\"f[x, y, z, a, b]\") == Function(\"f\")(x, y, z, a, b)\n    assert parser.parse_mathematica(\"f[x + y, z^2]\") == Function(\"f\")(x + y, z**2)\n    assert parser.parse_mathematica(\"f[g[x], h[y]]\") == Function(\"f\")(Function(\"g\")(x), Function(\"h\")(y))\n", "    def test_mathematica_parser_escaped_characters():\n        parser = MathematicaParser()\n\n        assert parser._from_mathematica_to_tokens('a\\\\\"b') == ['a', '\"', 'b']\n        assert parser._from_mathematica_to_tokens('a\\\\\\\\b') == ['a', '\\\\', 'b']\n        assert parser._from_mathematica_to_tokens('\"a\\\\\\\\b\"') == ['\"a\\\\\\\\b\"']\n        assert parser._from_mathematica_to_tokens('\"a\\\\\"b\\\\\"c\"') == ['\"a\"b\"c\"']\n", "compilation error"], "sample_1043": ["def test_user_functions():\n    from sympy import symbols, Function\n    x, y = symbols(\"x y\")\n    f = Function(\"f\")\n\n    printer = MCodePrinter({\"user_functions\": {\"my_func\": [(lambda x: True, \"MyFunc\")]}})\n    assert printer.doprint(f(x)) == \"MyFunc[x]\"\n    assert printer.doprint(f(x, y)) == \"MyFunc[x, y]\"\n", "    def test_user_functions():\n        from sympy.printing.mathematica import mathematica_code as mcode\n        f = Function('f')\n        g = Function('g')\n        assert mcode(f(x) + g(x)) == \"f[x] + g[x]\"\n\n        userfuncs = {'f': [(lambda *x: True, 'myf')],\n                      'g': [(lambda *x: True, 'myg')]}\n        assert mcode(f(x) + g(x), user_functions=userfuncs) == \"myf[x] + myg[x]\"\n\n", "compilation error", "def test_user_functions():\n    f = Function('f')\n    g = Function('g')\n    f_code = mcode(f(x))\n    g_code = mcode(g(x))\n\n    assert f_code == \"f[x]\"\n    assert g_code == \"g[x]\"\n\n    mcode.known_functions['f'] = [(lambda x: True, 'my_f')]\n    assert mcode(f(x)) == \"my_f[x]\"\n\n    mcode.known_functions['g'] = [(lambda x: True, 'my_g')]\n    assert mcode(g(x)) == \"my_g[x]\"\n\n    mcode.known_functions['f'] = []\n    mcode.known_functions['g'] = []\n", "    def test_user_functions():\n        f = symbols('f')\n        g = symbols('g')\n        mcode.known_functions.update({'f': [(lambda x: True, 'MyF')]})\n        assert mcode(f(x,y)) == 'MyF[x, y]'\n        assert mcode(f(x)) == 'MyF[x]'\n        mcode.known_functions.update({'g': [(lambda *x: True, 'MyG')]})\n        assert mcode(g(x,y,z)) == 'MyG[x, y, z]'\n        assert mcode(g(x)) == 'MyG[x]'\n\n"], "sample_596": ["    def test_concat_mixed_dtypes():\n        da1 = DataArray([1, 2, 3], dims=\"x\", dtype=np.int64)\n        da2 = DataArray([4.0, 5.0, 6.0], dims=\"x\", dtype=np.float64)\n\n        expected = DataArray(np.array([1, 2, 3, 4, 5, 6]), dims=\"x\").astype(\"float64\")\n        actual = concat([da1, da2], dim=\"x\")\n        assert_identical(actual, expected)\n", "    def test_concat_merge_multiindex_level_coords():\n        # Test the case where datasets have multiindex level coordinates\n        # and we want to keep all levels\n        ds1 = Dataset(\n            {\"foo\": ((\"x\", \"y\"), [[1, 2, 3]])},\n            coords={\"x\": [1, 2, 3], \"y\": pd.MultiIndex.from_tuples([(1, \"a\"), (1, \"b\"), (2, \"a\")])},\n        )\n        ds2 = Dataset(\n            {\"foo\": ((\"x\", \"y\"), [[4, 5, 6]])},\n            coords={\n                \"x\": [4, 5, 6],\n                \"y\": pd.MultiIndex.from_tuples([(4, \"a\"), (4, \"b\"), (5, \"a\")]),\n            },\n        )\n\n        expected = Dataset(\n            {\"foo\": ((\"x\", \"y\"), [[1, 2, 3, 4, 5, 6]])},\n            coords={\n                \"x\": [1, 2, 3, 4, 5, 6],\n                \"y\": pd.MultiIndex.from_tuples(\n                    [(1, \"a\"), (1, \"b\"), (2, \"a\"), (4, \"a\"), (4, \"b\"), (5, \"a\")]\n                ),\n            },\n        )\n\n        actual = concat([ds1, ds2], \"x\", coords=\"all\", compat=\"override\")\n        assert_identical(expected, actual)\n", "    def test_concat_merge_multi_non_dim_coord():\n        da1 = DataArray([1, 2, 3], dims=\"x\", coords={\"x\": [1, 2, 3], \"y\": 1, \"z\": 10})\n        da2 = DataArray([4, 5, 6], dims=\"x\", coords={\"x\": [4, 5, 6], \"y\": 2})\n\n        expected = DataArray(range(1, 7), dims=\"x\", coords={\"x\": range(1, 7), \"y\": [1, 2]})\n\n        for coords in [\"different\", \"minimal\"]:\n            actual = concat([da1, da2], \"x\", coords=coords)\n            assert_identical(actual, expected)\n", "    def test_concat_merge_single_non_dim_coord_with_different_values():\n        da1 = DataArray([1, 2, 3], dims=\"x\", coords={\"x\": [1, 2, 3], \"y\": 1})\n        da2 = DataArray([4, 5, 6], dims=\"x\", coords={\"x\": [4, 5, 6], \"y\": 2})\n\n        with raises_regex(ValueError, \"'y' not present in all datasets\"):\n            concat([da1, da2], dim=\"x\")\n", "    def test_concat_multiindex_coords():\n        x1 = pd.MultiIndex.from_product([[1], ['a', 'b']])\n        x2 = pd.MultiIndex.from_product([[2], ['c', 'd']])\n        da1 = DataArray(np.arange(4).reshape(2, 2), dims=[\"x\", \"y\"], coords={\"x\": x1})\n        da2 = DataArray(np.arange(4).reshape(2, 2), dims=[\"x\", \"y\"], coords={\"x\": x2})\n\n        expected = DataArray(\n            np.arange(8).reshape(4, 2),\n            dims=[\"x\", \"y\"],\n            coords={\"x\": pd.MultiIndex.from_arrays([[1, 2], ['a', 'b', 'c', 'd']]),\n                    \"y\": [0, 1]},\n        )\n        actual = concat([da1, da2], \"x\")\n        assert_identical(actual, expected)\n"], "sample_494": ["    def test_serialize_complex_serializer(self):\n        class MyComplexType:\n            pass\n\n        serializer = ComplexSerializer(MyComplexType())\n        result = self.serialize_round_trip(serializer)\n        self.assertEqual(result.__class__, ComplexSerializer)\n        self.assertEqual(result.value, MyComplexType())\n", "    def test_serialize_nested_class_method_with_arguments(self):\n        self.assertSerializedResultEqual(\n            self.NestedChoices.method_with_arguments(1, 2),\n            (\n                \"migrations.test_writer.WriterTests.NestedChoices.method_with_arguments(1, 2)\",\n                {\"import migrations.test_writer\"},\n            ),\n        )\n\n", "    def test_deconstruct_nested_class(self):\n\n        class OuterClass:\n            class InnerClass:\n                pass\n\n        string, imports = MigrationWriter.serialize(OuterClass.InnerClass())\n        self.assertEqual(string, \"migrations.test_writer.OuterClass.InnerClass()\")\n        self.assertEqual(imports, {\"from migrations.test_writer import OuterClass\"})\n\n", "    def test_serialize_model_with_proxy_model(self):\n        class BaseProxy(models.Model):\n            name = models.CharField(max_length=100)\n\n        class ProxyModel(BaseProxy):\n            class Meta:\n                proxy = True\n\n        migration = type(\n            \"Migration\",\n            (migrations.Migration,),\n            {\n                \"operations\": [\n                    migrations.CreateModel(\n                        \"ProxyModel\",\n                        fields=[\n                            (\"id\", models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                            ('name', models.CharField(max_length=100)),\n                        ],\n                        options={\n                            \"proxy\": True,\n                        },\n                        bases=(BaseProxy,),\n                    ),\n                ],\n                \"dependencies\": [],\n            },\n        )\n\n        writer = MigrationWriter(migration)\n        output = writer.as_string()\n        self.assertIn(\"class Meta:\", output)\n        self.assertIn(\"proxy = True\", output)\n", "    def test_serialize_complex_queryset(self):\n        qs = TestModel1.objects.all()\n        result = MigrationWriter.serialize(qs)\n        self.assertEqual(result[0], \"TestModel1.objects.all()\")\n        self.assertEqual(result[1], {'from django.db import models'})\n"], "sample_1005": ["def test_latex_Piecewise():\n    from sympy.functions.piecewise import Piecewise\n    pw = Piecewise((x, x < 1), (x**2, x >= 1))\n    assert latex(pw) == r\"\\begin{cases} x & \\text{for}\\ x < 1 \\\\ x^{2} & \\text{for}\\ x \\ge 1 \\end{cases}\"\n    pw = Piecewise((x, Eq(x, 0)), (x**2, True))\n    assert latex(pw) == r\"\\begin{cases} x & \\text{for}\\ x = 0 \\\\ x^{2} & \\text{otherwise} \\end{cases}\"\n", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_278": ["    def test_expression_wrapper_output_field(self):\n        expr = ExpressionWrapper(F('cost') + F('tax'), output_field=DecimalField())\n        self.assertIsInstance(expr.output_field, DecimalField)\n", "    def test_expression_wrapper_output_field(self):\n\n        expr = ExpressionWrapper(\n            F('cost') + F('tax'),\n            output_field=DecimalField(),\n        )\n        self.assertIsInstance(expr.output_field, DecimalField)\n", "    def test_wrapped_expression_get_source_expressions(self):\n        expr = ExpressionWrapper(F('cost') + F('tax'), output_field=IntegerField())\n        self.assertEqual(expr.get_source_expressions(), [F('cost'), F('tax')])\n", "    def test_expression_wrapper_output_field(self):\n        expr = ExpressionWrapper(F('cost') + F('tax'), output_field=DecimalField())\n        self.assertIsInstance(expr.output_field, DecimalField)\n        self.assertEqual(expr.as_sql(connection), ('%s', ['cost' + 'tax']))\n", "    def test_resolve_output_field_from_rhs(self):\n        class CustomOutputField(CharField):\n            pass\n        lhs = Expression(IntegerField())\n        rhs = Expression(CustomOutputField())\n        expr = CombinedExpression(lhs, Combinable.ADD, rhs)\n        self.assertIsInstance(expr.output_field, CustomOutputField)\n"], "sample_269": ["    def test_javascript_gettext_with_context(self):\n        self.selenium.get(self.live_server_url + '/jsi18n_template/')\n\n        elem = self.selenium.find_element_by_id(\"pgettext_context\")\n        self.assertEqual(elem.text, \"Diese Nachricht wird \u00fcbersetzt\")\n", "    def test_i18n_no_translations(self):\n        \"\"\"\n        Check that JavaScriptCatalog returns an empty catalog when no\n        translations are found for the requested language and package.\n        \"\"\"\n        with self.settings(LANGUAGE_CODE='en-us'), override('fr'):\n            response = self.client.get('/jsi18n/app6/')\n            self.assertEqual(json.loads(response.content.decode())['catalog'], {})\n", "    def test_i18n_template_with_context_processor(self):\n        self.selenium.get(self.live_server_url + '/i18n_template/')\n        elem = self.selenium.find_element_by_id(\"context_processor_text\")\n        self.assertEqual(elem.text, \"This text is from a context processor.\")\n", "    def test_i18n_emoji_translation(self):\n        with self.settings(LANGUAGE_CODE='en-us'), override('fr'):\n            self.selenium.get(self.live_server_url + '/jsi18n_emoji_translation/')\n            elem = self.selenium.find_element_by_id(\"emoji\")\n            self.assertEqual(elem.text, 'emoji \ud83d\udc4d')\n", "    def test_jsi18n_with_custom_catalog(self):\n        with self.settings(LANGUAGE_CODE='fr'), override('fr'):\n            response = self.client.get('/jsi18n_custom_catalog/')\n            self.assertContains(response, 'Custom translation')\n"], "sample_229": ["    def test_union_with_different_field_names(self):\n        class AnotherNumber(models.Model):\n            num1 = models.IntegerField()\n\n            class Meta:\n                db_table = \"number\"\n\n        AnotherNumber.objects.bulk_create(AnotherNumber(num1=i) for i in range(10))\n        self.assertNumbersEqual(\n            Number.objects.filter(num__lte=1).union(AnotherNumber.objects.filter(num1__gte=8)),\n            [0, 1, 8, 9],\n            ordered=False,\n        )\n", "    def test_union_with_subquery(self):\n        qs1 = Number.objects.filter(num__gte=5).values('num')\n        qs2 = Number.objects.filter(num__lt=5).values('num')\n        qs3 = Number.objects.filter(pk__in=qs1.union(qs2)).values('num')\n        self.assertNumbersEqual(qs3, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], ordered=False)\n", "    def test_union_with_empty_resultset(self):\n        qs1 = Number.objects.filter(num=10)\n        qs2 = Number.objects.filter(num=11)\n        self.assertEqual(len(qs1.union(qs2)), 0)\n", "    def test_union_with_filtered_and_annotated_qs(self):\n        qs1 = Number.objects.filter(num__lt=5).annotate(is_even=F('num') % 2 == 0)\n        qs2 = Number.objects.filter(num__gt=5).annotate(is_even=F('num') % 2 == 0)\n        self.assertCountEqual(\n            qs1.union(qs2).values_list('num', 'is_even'),\n            [(0, True), (1, False), (2, True), (3, False), (4, True), (6, True), (7, False), (8, True), (9, False)],\n        )\n\n", "    def test_union_with_empty_resultset_and_ordering(self):\n        qs1 = Number.objects.filter(num=100)\n        qs2 = Number.objects.filter(num=200)\n        self.assertSequenceEqual(list(qs1.union(qs2).order_by('num')), []) \n"], "sample_788": ["def test_n_bins_validation_errors():\n    # Test invalid types for n_bins\n    est = KBinsDiscretizer(n_bins='invalid')\n    assert_raise_message(TypeError, 'n_bins should be an integer or '\n                         'array-like of integers, got invalid',\n                         est.fit, X)\n\n    # Test invalid n_bins value (negative)\n    est = KBinsDiscretizer(n_bins=-1)\n    assert_raise_message(ValueError, \n                         \"n_bins should be a positive integer, got -1.\",\n                         est.fit, X)\n\n", "    def test_transform_empty_input():\n        est = KBinsDiscretizer(n_bins=2)\n        # Test empty array\n        assert_array_equal(est.fit_transform(np.array([]).reshape(0, 4)),\n                            np.array([]).reshape(0, 4))\n        # Test empty array (sparse)\n        assert_array_equal(est.fit_transform(sp.csr_matrix((0, 4))),\n                            sp.csr_matrix((0, 4)))\n\n", "def test_feature_names_out(encode):\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    kbd = KBinsDiscretizer(n_bins=3, encode=encode)\n    kbd.fit(X)\n    Xt = kbd.transform(X)\n    feature_names = kbd.get_feature_names_out()\n    assert feature_names == ['feature_0_0', 'feature_0_1', 'feature_0_2',\n                           'feature_1_0', 'feature_1_1', 'feature_1_2']\n\n", "def test_invalid_n_bins_type():\n    est = KBinsDiscretizer(n_bins='invalid')\n    assert_raise_message(TypeError, \"n_bins must be an int or array-like. \"\n                         \"Received 'invalid' instead.\", est.fit, X) \n", "def test_feature_names_out():\n    X = np.array([[0, 1, 2],\n                  [3, 4, 5]])\n    est = KBinsDiscretizer(n_bins=2, encode='onehot-dense')\n    Xt = est.fit_transform(X)\n    assert_array_equal(est.get_feature_names_out(), ['feature_0_0',\n                                                    'feature_0_1',\n                                                    'feature_1_0',\n                                                    'feature_1_1',\n                                                    'feature_2_0',\n                                                    'feature_2_1'])\n\n"], "sample_476": ["    def test_delete_model_with_image(self):\n        p = self.PersonModel(name=\"Joe\", mugshot=self.file1)\n        p.save()\n        self.assertTrue(os.path.exists(p.mugshot.path))\n        p.delete()\n        self.assertFalse(os.path.exists(p.mugshot.path))\n", "    def test_dimensions_after_saving_again(self):\n        p = self.PersonModel(name=\"Joe\", mugshot=self.file1)\n        p.save()\n        self.check_dimensions(p, 4, 8, \"mugshot\")\n\n        # Updating the image field and saving again should not\n        # recalculate dimensions\n        p.mugshot.save(\"mug\", self.file1) \n        self.check_dimensions(p, 4, 8, \"mugshot\")\n", "    def test_imagefield_file_name(self):\n        # Test saving with a file name that is not unique\n        with self.assertRaises(IOError):\n            p = self.PersonModel(name=\"Joe\")\n            p.mugshot.save(\"mug\", self.file1)\n            p.save()\n            p2 = self.PersonModel(name=\"Jane\")\n            p2.mugshot.save(\"mug\", self.file1)\n            p2.save()\n", "    def test_resize_image_field(self):\n        \"\"\"\n        Test that resizing an image field works correctly and that dimensions are updated.\n        \"\"\"\n        p = self.PersonModel(name=\"Joe\", mugshot=self.file1)\n        self.check_dimensions(p, 4, 8)\n        # Resize the image. This will return a new Image object.\n        resized_image = p.mugshot.resize((100, 100))\n        # Save the resized image back to the field.\n        p.mugshot = resized_image\n        # The dimensions should be updated.\n        self.check_dimensions(p, 100, 100)\n\n", "    def test_save_after_load(self):\n        p = self.PersonModel.objects.create(name=\"Joe\", mugshot=self.file1)\n        self.check_dimensions(p, 4, 8)\n        p = self.PersonModel.objects.get(pk=p.pk)\n        self.check_dimensions(p, 4, 8)\n        p.mugshot = self.file2\n        p.save()\n        self.check_dimensions(p, 8, 4)\n\n"], "sample_79": ["    def test_pluralize_with_comma_invalid(self):\n        self.check_values(('invalid', 'invalid'))\n", "    def test_default_plural_word(self):\n        self.check_values(('0', 'people'), ('1', 'person'), ('2', 'people'))\n", "    def test_invalid_argument(self):\n        self.check_values(('a', 'vote'), ('True', 'vote'), ('None', 'vote'))\n", "    def test_pluralize_with_zero_value(self):\n        self.check_values(('0', 'candies'))\n", "    def test_invalid_input(self):\n        with self.assertRaises(ValueError):\n            pluralize('not a number')\n"], "sample_1200": ["    def test_issue_22861():\n        from sympy.physics.units import watt, volt, ampere\n        assert watt.convert_to(volt*ampere) == 1\n", "def test_issue_24312():\n    from sympy.physics.units import Quantity, meter, kilogram, second\n    q1 = Quantity(\"q1\")\n    SI.set_quantity_dimension(q1, meter/second)\n    q2 = Quantity(\"q2\")\n    SI.set_quantity_dimension(q2, kilogram*meter**2/second**2)\n    expr = q1*q2\n    assert SI.get_dimensional_expr(expr) == kilogram*meter**3/second**3\n", "compilation error", "compilation error", "compilation error"], "sample_150": ["    def test_appcommand_check_called(self, mocked_apps, mocked_check):\n        mocked_apps.get_app_config.return_value = mock.MagicMock()\n        class TestCommand(BaseCommand):\n                pass\n\n        command = TestCommand()\n        command.handle('some_app')\n\n        mocked_check.assert_called_once()\n", "    def test_command_check_calls_system_checks(self, mocked_check):\n        from django.core.management.base import Command\n        Command().check()\n        mocked_check.assert_called_once_with(\n            app_configs=None,\n            tags=None,\n            display_num_errors=False,\n            include_deployment_checks=False,\n            fail_level=checks.ERROR,\n            databases=None\n        )\n", "    def test_check_migrations_called(self):\n        from django.management.base import BaseCommand\n        \n        command = mock.MagicMock(spec=BaseCommand)\n        command.stdout = mock.MagicMock()\n\n        command.check_migrations()\n        command.stdout.write.assert_not_called()\n\n        with mock.patch('django.db.migrations.executor.MigrationExecutor.__init__',\n                       return_value=None):\n            command.check_migrations()\n            command.stdout.write.assert_called()\n", "    def test_check_migrations_called(self, mocked_check):\n        from django.core.management.base import BaseCommand\n        command = BaseCommand()\n        command.check_migrations()\n        self.assertTrue(mocked_check.called)\n\n", "    def test_database_checks_error(self, mocked_error):\n        with mock.patch('django.core.management.base.BaseCommand.check'):\n            mocked_error.return_value = 'test_error'\n            try:\n                check_database_backends(databases=self.databases)\n            except Exception as e:\n                self.assertEqual(str(e), 'test_error')\n\n\n\n"], "sample_1081": ["compilation error", "compilation error", "compilation error", "def test_is_amicable():\n    assert is_amicable(220) is True\n    assert is_amicable(284) is True\n    assert is_amicable(504) is True\n    assert is_amicable(1184) is True\n    assert is_amicable(2620) is True\n    assert is_amicable(10) is False\n    assert is_amicable(12) is False\n", "compilation error"], "sample_926": ["    def check_role_classes(role, root, contents):\n        \"\"\"Checks if the role classes are consistent.\"\"\"\n        a = RoleClasses(role, root, contents)\n        print(a.content_classes)\n        print('{} ({}): {}'.format(role, root, a.classes))\n        for tag, content_classes in a.content_classes.items():\n            content_classes = set(content_classes)\n            print('{} ({}): {}'.format(role, tag, content_classes))\n            assert a.classes.issuperset(content_classes), (\n                \"Role '{}' has inconsistent classes between '{}' and '{}' elements.\".format(\n                    role, root, tag))\n", "    def __str__(self):\n        return f'Role \"{self.name}\" with classes: {self.classes}'\n", "    def assert_equal_classes(self, other_role):\n        assert self.classes == other_role.classes, (\n            f\"Roles '{self.name}' and '{other_role.name}' disagree on common classes: {self.classes} vs {other_role.classes}\"\n        )\n", "compilation error", "    def assert_class_consistency(self, other):\n        \"\"\"Assert that the given RoleClasses object has the same classes.\"\"\"\n        assert self.classes == other.classes, (\n            f'Inconsistent classes for role \"{self.name}\":\\n'\n            f'Expected: {self.classes}\\nGot: {other.classes}'\n        )\n        for tag in self.content_classes:\n            assert tag in other.content_classes, (\n                f'Missing content tag \"{tag}\" for role \"{self.name}\"'\n            )\n            assert self.content_classes[tag] == other.content_classes[tag], (\n                f'Inconsistent classes for role \"{self.name}\" and tag \"{tag}\":\\n'\n                f'Expected: {self.content_classes[tag]}\\nGot: {other.content_classes[tag]}'\n            )\n"], "sample_578": ["    def test_baseline(self, x, y):\n\n        baseline = -1\n        p = Plot(x, y).add(Bars(baseline=baseline)).plot()\n        ax = p._figure.axes[0]\n        paths = ax.collections[0].get_paths()\n        for i, path in enumerate(paths):\n            verts = path.vertices\n            assert verts[0, 1] == pytest.approx(baseline)\n            assert verts[3, 1] == y[i] + baseline\n", "    def test_legend(self):\n\n        x = [\"a\", \"b\"]\n        y = [1, 2]\n        mark = Bar(color=\"C0\")\n\n        p = Plot(x, y).add(mark).plot()\n        ax = p._figure.axes[0]\n        legend = ax.legend()\n        assert len(legend.get_patches()) == 1\n        assert legend.get_patches()[0].get_facecolor() == to_rgba(\"C0\")\n\n", "    def test_baseline(self, x, y):\n\n        p = Plot(x, y).add(Bars(baseline=2)).plot()\n        ax = p._figure.axes[0]\n        paths = ax.collections[0].get_paths()\n        for i, path in enumerate(paths):\n            verts = path.vertices\n            assert verts[0, 1] == pytest.approx(2)\n\n", "    def test_baseline(self, x, y):\n\n        p = Plot(x, y).add(Bars(baseline=2)).plot()\n        ax = p._figure.axes[0]\n        paths = ax.collections[0].get_paths()\n        for i, path in enumerate(paths):\n            verts = path.vertices\n            assert verts[0, 1] == pytest.approx(2)\n            assert verts[3, 1] == pytest.approx(y[i] + 2)\n\n", "    def test_legend_artist(self):\n\n        mark = Bars()\n        artist = mark._legend_artist([\"x\"], 1, {})\n        assert isinstance(artist, mpl.patches.Patch)\n        assert artist.get_facecolor() == to_rgba(mark.color, mark.alpha)\n        assert artist.get_edgecolor() == to_rgba(mark.edgecolor, mark.edgealpha)\n        assert artist.get_linewidth() == mark.edgewidth\n        assert artist.get_linestyle() == mark.edgestyle\n"], "sample_182": ["    def test_empty_qs_with_subcompound_qs(self):\n        qs1 = Number.objects.filter(pk__in=[])\n        qs2 = Number.objects.intersection(Number.objects.filter(num__gt=1))\n        self.assertEqual(qs1.difference(qs2).count(), 0)\n", "    def test_union_with_distinct_on_different_fields(self):\n        qs1 = Number.objects.filter(num__lt=3).values('num')\n        qs2 = Number.objects.filter(num__gt=7).values('num').distinct()\n        self.assertEqual(\n            len(list(qs1.union(qs2, all=True))),\n            9,\n        )\n        self.assertEqual(\n            len(list(qs1.union(qs2))),\n            7,\n        )\n\n", "    def test_union_filter_afterwards(self):\n        qs1 = Number.objects.filter(num=1).values('num')\n        qs2 = Number.objects.filter(num=2).values('num')\n        union = qs1.union(qs2)\n        self.assertEqual(union.filter(num=1).count(), 1)\n", "    def test_order_by_different_models(self):\n        qs1 = Number.objects.values_list('num', flat=True)\n        ReservedName.objects.create(name='rn1', order=1)\n        qs2 = ReservedName.objects.values_list('order', flat=True)\n        self.assertEqual(list(qs1.union(qs2).order_by('num')), list(range(10)) + [1])\n\n", "    def test_intersection_with_complex_ordering(self):\n        qs1 = Number.objects.annotate(\n            half_num=F('num') / 2\n        ).order_by('half_num', '-num')\n        qs2 = Number.objects.annotate(\n            double_num=F('num') * 2\n        ).order_by('double_num', 'num')\n        self.assertQuerysetEqual(\n            qs1.intersection(qs2),\n            qs1.filter(num__in=[2, 4, 6, 8]),\n            lambda x, y: (x.num == y.num and x.half_num == y.half_num and x.double_num == y.double_num)\n        )\n"], "sample_848": ["def test_multi_output_regression_with_different_regressors():\n    X, y = datasets.make_regression(n_targets=3)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n                                                      random_state=42)\n\n    regressors = [LinearRegression(),\n                  Ridge(),\n                  Lasso()]\n\n    multi_target_regressors = []\n    for reg in regressors:\n        multi_target_regressor = MultiOutputRegressor(regressor=reg)\n        multi_target_regressor.fit(X_train, y_train)\n        multi_target_regressors.append(multi_target_regressor)\n\n    for regressor in multi_target_regressors:\n        y_pred = regressor.predict(X_test)\n        assert_almost_equal(y_pred.shape, y_test.shape)\n        assert isinstance(regressor.estimators_, list)\n        assert len(regressor.estimators_) == y.shape[1]\n\n", "def test_multi_output_classification_partial_fit_classes_mismatch():\n    sgd_linear_clf = SGDClassifier(loss='log', random_state=1, max_iter=5)\n    multi_target_linear = MultiOutputClassifier(sgd_linear_clf)\n\n    # train with 3 classes\n    half_index = X.shape[0] // 2\n\n    multi_target_linear.partial_fit(X[:half_index], y[:half_index],\n                                     classes=[np.unique(y[:, 0])])\n    \n    err_msg = \"The number of classes for each output should be the same\"\n    with pytest.raises(ValueError, match=err_msg):\n        multi_target_linear.partial_fit(X[half_index:], y[half_index:])\n", "    def test_multi_output_classification_partial_fit_with_label_indicator(self):\n        # Test partial_fit with label indicator\n\n        X = [[1, 2, 3], [4, 5, 6], [1.5, 2.5, 3.5]]\n        y = np.array([[0, 1], [1, 0], [0, 1]])\n\n        sgd_linear_clf = SGDClassifier(random_state=1, max_iter=20)\n\n        multi_target_linear = MultiOutputClassifier(sgd_linear_clf)\n\n        # train the multi_target_linear and also get the predictions.\n\n        multi_target_linear.partial_fit(X, y)\n\n        predictions = multi_target_linear.predict(X)\n        assert (len(X), 2) == predictions.shape\n\n        # check if the predictions are consistent with the input data\n\n        for i in range(len(X)):\n            assert (predictions[i] == y[i]).any()\n", "    def test_multi_output_regression_with_sparse_input(self):\n        # Test multi-output regression with sparse input data\n        X, y = datasets.make_regression(n_targets=3, random_state=0)\n        X_sparse = sp.csr_matrix(X)\n        rgr = MultiOutputRegressor(Ridge())\n        rgr.fit(X_sparse, y)\n        y_pred = rgr.predict(X_sparse)\n        assert_almost_equal(y, y_pred)\n", "def test_regressor_chain_partial_fit():\n    # Test partial fit for RegressorChain\n    X, y = datasets.make_regression(n_targets=3, random_state=42)\n    chain = RegressorChain(Ridge())\n    n_samples = X.shape[0]\n    half_index = n_samples // 2\n    X_train, y_train = X[:half_index], y[:half_index]\n    X_test, y_test = X[half_index:], y[half_index:]\n    chain.partial_fit(X_train, y_train)\n    y_pred = chain.predict(X_test)\n    assert y_pred.shape == y_test.shape\n    chain.partial_fit(X_test, y_test)\n    y_pred_2 = chain.predict(X_test)\n    assert y_pred_2.shape == y_test.shape\n    assert not np.allclose(y_pred, y_pred_2)\n\n"], "sample_1090": ["compilation error", "compilation error", "compilation error", "def test_evalf():\n    x = Symbol('x')\n\n    assert (S.Pi + 1).evalf(2) == 4.14\n    assert (S.Pi).evalf(10) == 3.1415926535\n\n    assert (x + 2).evalf(2, {'x': 3.1}) == 5.10\n    assert (x**2).evalf(2, {'x': 3.1}) == 9.61\n\n    assert (S.GoldenRatio).evalf(5) == 1.61803\n\n    with pytest.raises(TypeError):\n        (S.Pi + x).evalf(2)\n", "compilation error"], "sample_313": ["    def test_watch_for_template_changes_no_dirs(self, mock_get_template_directories):\n        mock_get_template_directories.return_value = set()\n        mock_reloader = mock.MagicMock()\n        autoreload.watch_for_template_changes(mock_reloader)\n        self.assertEqual(mock_reloader.watch_dir.call_count, 0)\n", "    def test_watch_for_template_changes_no_directories(self, mock_get_directories):\n        mock_get_directories.return_value = set()\n        mock_reloader = mock.MagicMock()\n        autoreload.watch_for_template_changes(mock_reloader)\n        self.assertEqual(mock_reloader.watch_dir.call_count, 0)\n", "    def test_reset_loaders_called_when_template_changed_in_django_path(self):\n        mock_reset = mock.MagicMock()\n\n        autoreload.reset_loaders = mock_reset\n        sender = mock.MagicMock()\n\n        template_path_in_django_path = Path(\n            '/path/to/django/project/templates/my_template.html')\n        autoreload.template_changed(sender, template_path_in_django_path)\n\n        mock_reset.assert_not_called()\n", "    def test_watch_for_template_changes_no_directories(self, mock_get_dirs):\n        mock_get_dirs.return_value = set()\n        mock_reloader = mock.MagicMock()\n        autoreload.watch_for_template_changes(mock_reloader)\n        self.assertEqual(mock_reloader.watch_dir.call_count, 0)\n", "    def test_no_template_dirs(self):\n        self.assertEqual(autoreload.get_template_directories(), set()) \n"], "sample_212": ["    def test_session_middleware_save_on_empty_session(self):\n        class MockRequest:\n            session = None\n            COOKIES = {'sessionid': 'fake-session-id'}\n            method = \"GET\"\n\n        class MockResponse:\n            status_code = 200\n            committed = False\n\n                pass\n\n                self.committed = True\n\n        middleware = SessionMiddleware(get_response=lambda r: MockResponse())\n        request = MockRequest()\n        response = middleware.process_request(request)\n        self.assertFalse(response.committed)\n        request.session = middleware.SessionStore()\n        response = middleware.process_response(request, response)\n        self.assertTrue(response.committed)\n", "    def test_middleware_session_is_empty(self):\n        request = HttpRequest()\n        request.COOKIES = {settings.SESSION_COOKIE_NAME: 'fake_session_key'}\n        request.session = self.session_store()\n        request.session.save()\n        response = HttpResponse()\n\n        middleware = SessionMiddleware(get_response=lambda r: response)\n        middleware.process_response(request, response)\n\n        self.assertEqual(response.cookies.get(settings.SESSION_COOKIE_NAME), 'fake_session_key')\n\n        request.session.clear()\n        middleware.process_response(request, response)\n\n        self.assertFalse(settings.SESSION_COOKIE_NAME in response.cookies)\n", "    def test_session_middleware_set_cookie(self):\n        class MockRequest:\n                self.COOKIES = {'sessionid': 'test_session_id'}\n                self.session = MockSession()\n\n        class MockSession:\n            modified = True\n            session_key = 'new_session_id'\n            get_expire_at_browser_close = lambda self: False\n            get_expiry_age = lambda self: 3600\n        \n            \n\n                pass\n\n        class MockResponse:\n                self.status_code = 200\n                self.cookies = {}\n\n                self.cookies[key] = (value, kwargs)\n\n                del self.cookies[key]\n\n        request = MockRequest()\n        response = MockResponse()\n\n        middleware = SessionMiddleware(get_response=lambda r: response)\n        middleware.process_response(request, response)\n\n        self.assertEqual(response.cookies['sessionid'][0], 'new_session_id')\n        self.assertEqual(response.cookies['sessionid'][1]['max_age'], 3600)\n", "    def test_session_save_behavior(self):\n        class MockRequest:\n                self.session = MockSession()\n\n        class MockSession:\n                self.accessed = True\n                self.modified = True\n                self._empty = False\n\n                return self._empty\n\n                pass\n\n                return 3600\n\n                return False\n\n                return 'session_key'\n\n        request = MockRequest()\n        response = HttpResponse()\n        middleware = SessionMiddleware(lambda r: response)\n\n        middleware.process_response(request, response)  \n\n        self.assertIn('Set-Cookie', response.headers)\n        self.assertIn('session_key', response['Set-Cookie'])\n\n        request.session._empty = True \n        middleware.process_response(request, response)\n        self.assertIn('Cookie', response.headers)\n        self.assertNotIn('Set-Cookie', response.headers)\n", "    def test_session_middleware_save_on_every_request(self):\n        class MockSessionStore:\n                self.session_key = session_key\n                self.accessed = False\n                self.modified = False\n                self.empty = False\n\n                self.modified = True\n\n        settings.SESSION_SAVE_EVERY_REQUEST = True\n        request = HttpRequest()\n        request.COOKIES = {settings.SESSION_COOKIE_NAME: 'mock_session_key'}\n        request.session = MockSessionStore(request.COOKIES.get(settings.SESSION_COOKIE_NAME))\n        response = HttpResponse()\n\n        middleware = SessionMiddleware(lambda r: response)\n        middleware.process_response(request, response)\n        self.assertTrue(request.session.modified)\n"], "sample_467": ["    def test_value_from_datadict_with_empty_values(self):\n        data = {\"field_year\": \"\", \"field_month\": \"12\", \"field_day\": \"1\"}\n        self.assertEqual(\n            self.widget.value_from_datadict(data, {}, \"field\"), \"0-12-1\"\n        )\n", "    def test_invalid_date_string(self):\n        self.assertEqual(\n            self.widget.value_from_datadict(\n                {\"date_year\": \"2024\", \"date_month\": \"13\", \"date_day\": \"32\"}, {}, \"date\"\n            ),\n            None,\n        )\n", "    def test_selectdate_empty_label_invalid_input(self):\n        w = SelectDateWidget(years=(\"2014\",), empty_label=(\"empty_year\", \"empty_month\", \"empty_day\"))\n\n        self.assertHTMLEqual(\n            w.render(\"mydate\", \"invalid-date\"),\n            \"\"\"\n            <select name=\"mydate_month\" id=\"id_mydate_month\">\n                <option value=\"\">empty_month</option>\n                <option value=\"1\">January</option>\n                <option value=\"2\">February</option>\n                <option value=\"3\">March</option>\n                <option value=\"4\">April</option>\n                <option value=\"5\">May</option>\n                <option value=\"6\">June</option>\n                <option value=\"7\">July</option>\n                <option value=\"8\">August</option>\n                <option value=\"9\">September</option>\n                <option value=\"10\">October</option>\n                <option value=\"11\">November</option>\n                <option value=\"12\">December</option>\n            </select>\n\n            <select name=\"mydate_day\" id=\"id_mydate_day\">\n                <option value=\"\">empty_day</option>\n                <option value=\"1\">1</option>\n                <option value=\"2\">2</option>\n                <option value=\"3\">3</option>\n                <option value=\"4\">4</option>\n                <option value=\"5\">5</option>\n                <option value=\"6\">6</option>\n                <option value=\"7\">7</option>\n                <option value=\"8\">8</option>\n                <option value=\"9\">9</option>\n                <option value=\"10\">10</option>\n                <option value=\"11\">11</option>\n                <option value=\"12\">12</option>\n                <option value=\"13\">13</option>\n                <option value=\"14\">14</option>\n                <option value=\"15\">15</option>\n                <option value=\"16\">16</option>\n                <option value=\"17\">1", "    def test_invalid_date_strings(self):\n        data = {\n            \"date_year\": \"2023\",\n            \"date_month\": \"13\",\n            \"date_day\": \"32\",\n        } \n        with self.assertRaises(ValidationError):\n            self.widget.value_from_datadict(data, {}, \"date\")\n", "    def test_empty_date_with_empty_label(self):\n        widget = SelectDateWidget(years=(\"2014\",), empty_label=(\"empty_year\", \"empty_month\", \"empty_day\"))\n        self.assertHTMLEqual(\n            widget.render(\"mydate\", \"\"),\n            \"\"\"\n            <select name=\"mydate_month\" id=\"id_mydate_month\">\n                <option selected value=\"\">empty_month</option>\n                <option value=\"1\">January</option>\n                <option value=\"2\">February</option>\n                <option value=\"3\">March</option>\n                <option value=\"4\">April</option>\n                <option value=\"5\">May</option>\n                <option value=\"6\">June</option>\n                <option value=\"7\">July</option>\n                <option value=\"8\">August</option>\n                <option value=\"9\">September</option>\n                <option value=\"10\">October</option>\n                <option value=\"11\">November</option>\n                <option value=\"12\">December</option>\n            </select>\n\n            <select name=\"mydate_day\" id=\"id_mydate_day\">\n                <option selected value=\"\">empty_day</option>\n                <option value=\"1\">1</option>\n                <option value=\"2\">2</option>\n                <option value=\"3\">3</option>\n                <option value=\"4\">4</option>\n                <option value=\"5\">5</option>\n                <option value=\"6\">6</option>\n                <option value=\"7\">7</option>\n                <option value=\"8\">8</option>\n                <option value=\"9\">9</option>\n                <option value=\"10\">10</option>\n                <option value=\"11\">11</option>\n                <option value=\"12\">12</option>\n                <option value=\"13\">13</option>\n                <option value=\"14\">14</option>\n                <option value=\"15\">15</option>\n                <option value=\"16\">16</option>\n                <option value=\"17\">17</option"], "sample_267": ["    def test_check_sqlite_version_warning(self):\n        msg = (\n            'SQLite 3.9.0 or later is required (found 3.8.11.1). '\n            'You are using an older version of SQLite, which may not be fully '\n            'supported by Django. Consider upgrading SQLite to a newer version.'\n        )\n        with mock.patch.object(dbapi2, 'sqlite_version_info', (3, 8, 11, 1)), \\\n                mock.patch.object(dbapi2, 'sqlite_version', '3.8.11.1'), \\\n                self.assertLogs('django.db.backends.sqlite3.base', level='WARNING'):\n            check_sqlite_version()\n", "    def test_sqlite_datetime_cast_time(self):\n        tests = (\n            ('2023-10-26 12:00:00', None, None, '12:00:00'),\n            (None, None, None, None),\n            ('2023-10-26 12:00:00.123456', 'UTC', None, '12:00:00'),\n            ('2023-10-26 12:00:00', None, 'America/Los_Angeles', '05:00:00'),\n\n        )\n        for dt, tzname, conn_tzname, expected in tests:\n            with self.subTest((dt, tzname, conn_tzname)):\n                with connection.cursor() as cursor:\n                    cursor.execute('SELECT CAST(%s AS TIME)', [dt])\n                    value = cursor.fetchone()[0]\n                self.assertEqual(str(value), expected)\n\n", "    def test_atomic_no_transaction(self):\n        with self.assertRaises(NotSupportedError) as context:\n            with connection.cursor() as cursor:\n                with transaction.atomic(using='default'):\n                    pass\n        self.assertIn(\"Atomic blocks cannot be nested inside another transaction.\",\n                      str(context.exception))\n", "    def test_sqlite3_datetime_cast_date(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\"SELECT DATE('now')\")\n            self.assertEqual(cursor.fetchone()[0].isoformat(), datetime.date.today().isoformat())\n\n\n", "    def test_savepoint_rollback(self):\n        with transaction.atomic():\n            with connection.cursor() as cursor:\n                cursor.execute(\"SAVEPOINT my_savepoint\")\n                Item.objects.create(name='test1')\n                cursor.execute(\"ROLLBACK TO my_savepoint\")\n            self.assertEqual(Item.objects.count(), 0)\n"], "sample_868": ["def test_empty_cluster_handling(metric_name):\n    X = np.random.randint(10, size=(10, 10))\n    # Empty cluster in the predictions\n    labels = [0, 0, 0, 1, 1, 1, 2, 2, -1, -1]\n    metric = UNSUPERVISED_METRICS[metric_name]\n    with pytest.warns(UserWarning, match=\"empty cluster\"):\n        metric(X, labels)\n", "def test_empty_cluster(metric_name, y1):\n    metric = SUPERVISED_METRICS[metric_name]\n    y2 = np.zeros_like(y1)\n    with pytest.raises(ValueError):\n        metric(y1, y2)\n", "compilation error", "def test_perfect_clustering(metric_name, expected_score):\n    metric = SUPERVISED_METRICS[metric_name]\n    y_true = np.array([0, 0, 1, 1])\n    y_pred = np.array([0, 0, 1, 1])\n    assert metric(y_true, y_pred) == pytest.approx(expected_score)\n", "    def test_empty_cluster(metric_name):\n        metric = SUPERVISED_METRICS[metric_name]\n        y_true = np.array([0, 0, 1, 1, 2])\n        y_pred = np.array([0, 0, 1, 1, -1])  # one point is in a cluster\n        # by itself\n        score = metric(y_true, y_pred)\n        assert not np.isnan(score)\n"], "sample_57": ["    def test_custom_password_validator(self):\n        class CustomPasswordValidator:\n                if 'bad' in password:\n                    raise ValidationError('Bad password: %s' % password)\n        \n        class CustomAdminPasswordChangeForm(AdminPasswordChangeForm):\n                super().__init__(*args, **kwargs)\n                self.cleaned_data['password1']\n                self.fields['password1'].validators.append(CustomPasswordValidator())\n\n        user = User.objects.get(username='testclient')\n        data = {'password1': 'passwordbad', 'password2': 'passwordbad'}\n        form = CustomAdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['password1'], ['Bad password: passwordbad'])\n", "    def test_password_validators(self):\n        user = User.objects.get(username='testclient')\n        data = {'password1': 'short', 'password2': 'short'}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertIn('password1', form.errors)\n        self.assertIn('password2', form.errors)\n\n", "    def test_password_validation(self):\n        user = User.objects.get(username='testclient')\n        data = {'password1': 'short', 'password2': 'short'}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertIn(\n            'Ensure this field has at least 8 characters.',\n            form.errors['password1'] \n        ) \n", "    def test_html_autocomplete_attributes(self):\n        user = User.objects.get(username='testclient')\n        form = AdminPasswordChangeForm(user)\n        self.assertEqual(form.fields['password1'].widget.attrs['autocomplete'], 'new-password')\n        self.assertEqual(form.fields['password2'].widget.attrs['autocomplete'], 'new-password')\n", "    def test_invalid_password_length(self):\n        user = User.objects.get(username='testclient')\n        data = {'password1': 'pass', 'password2': 'pass'}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertIn('password1', form.errors)\n        self.assertIn('password2', form.errors)\n"], "sample_429": ["    def test_url_validator_extended_schemes(self):\n        v = URLValidator(EXTENDED_SCHEMES)\n        self.assertIsNone(v(\"ftp://example.com\"))\n        self.assertIsNone(v(\"git+ssh://git@github.com/example/project.git\"))\n", "    def test_int_list_validator_equality(self):\n        self.assertEqual(int_list_validator(), int_list_validator())\n        self.assertEqual(int_list_validator(sep=\",\"), int_list_validator(sep=\",\"))\n        self.assertNotEqual(int_list_validator(), int_list_validator(sep=\" \"))\n", "    def test_range_validator(self):\n        validator = RangeValidator(min_value=1, max_value=10)\n        self.assertEqual(validator(5), 5)\n        with self.assertRaises(ValidationError):\n            validator(0)\n        with self.assertRaises(ValidationError):\n            validator(11)\n", "    def test_int_list_validator_equality(self):\n        self.assertEqual(\n            int_list_validator(sep=\".\"),\n            int_list_validator(sep=\".\"),\n        )\n        self.assertNotEqual(\n            int_list_validator(sep=\".\"),\n            int_list_validator(sep=\",\"),\n        )\n        self.assertEqual(\n            int_list_validator(allow_negative=True),\n            int_list_validator(allow_negative=True),\n        )\n        self.assertNotEqual(\n            int_list_validator(allow_negative=True),\n            int_list_validator(allow_negative=False),\n        )\n", "    def test_validate_comma_separated_integer_list_empty(self):\n        v = validate_comma_separated_integer_list\n        self.assertEqual(v(\"\"), [])\n"], "sample_336": ["    def test_include_in_nested_namespace(self):\n        url_patterns = [\n            path('inner/', include('urlpatterns_reverse.urls_include_nested', namespace='nested')),\n        ]\n\n        resolver = URLResolver(RegexPattern(r'^$'), url_patterns)\n        match = resolver.resolve('/inner/included/nested')\n        self.assertEqual(match.url_name, 'nested:included:included-view')\n", "    def test_include_with_multiple_namespace_levels(self):\n\n        urlpatterns = [\n            path('one/', views.empty_view, name='one'),\n            include([\n                path('two/', views.empty_view, name='two'),\n                include([\n                    path('three/', views.empty_view, name='three'),\n                ], namespace='nested'),\n            ], namespace='inner'),\n        ]\n        self.assertEqual(include(urlpatterns), (urlpatterns, 'inner', 'inner'))\n\n", "    def test_double_lookbehind_valid_resolve(self):\n        test_urls = [\n            '/lookbehind+/lookbehind+/a-city/',\n        ]\n        for test_url in test_urls:\n            with self.subTest(url=test_url):\n                self.assertEqual(resolve(test_url).kwargs, {'city': 'a-city'})\n", "    def test_include_empty_tuples():\n        msg = 'include() received an empty tuple as the first argument'\n        with self.assertRaisesMessage(ImproperlyConfigured, msg):\n            include(())\n", "    def test_reverse_with_trailing_slash(self):\n        self.assertEqual(reverse('trailing-slash-view'), '/trailing-slash/')\n        with self.assertRaises(NoReverseMatch):\n            reverse('trailing-slash-view') \n"], "sample_279": ["    def test_opclasses_database_constraint(self):\n        UniqueConstraintProduct.objects.create(name='p1', color='red')\n        with self.assertRaises(IntegrityError):\n            UniqueConstraintProduct.objects.create(name='p1', color='blue')\n", "    def test_database_constraint_with_opclasses(self):\n        UniqueConstraintProduct.objects.create(\n            name='p1', color='red', price=10, discounted_price=5\n        )\n        with self.assertRaises(IntegrityError):\n            UniqueConstraintProduct.objects.create(\n                name='p1',\n                color='blue',\n                price=10,\n                discounted_price=5,\n            )\n\n", "    def test_database_constraint_deferred_then_immediate(self):\n        obj_1 = UniqueConstraintDeferrable.objects.create(name='p1', shelf='front')\n        obj_2 = UniqueConstraintDeferrable.objects.create(name='p2', shelf='back')\n\n        # Initially deferred constraint allows duplicate values.\n        obj_1.shelf, obj_2.shelf = obj_2.shelf, obj_1.shelf\n        obj_1.save()\n        obj_2.save()\n\n        # Changing the constraint to IMMEDIATE should raise an error.\n        with connection.cursor() as cursor:\n            constraint_name = connection.ops.quote_name('sheld_init_immediate_uniq')\n            cursor.execute('SET CONSTRAINTS %s IMMEDIATE' % constraint_name)\n        with self.assertRaises(IntegrityError):\n            obj_1.save()\n", "    def test_opclasses_database_constraint(self):\n        UniqueConstraintProduct.objects.create(name='p1', price=10)\n        with self.assertRaises(IntegrityError):\n            UniqueConstraintProduct.objects.create(name='p1', price=10)\n\n", "    def test_database_constraint_deferrable_update(self):\n        obj_1 = UniqueConstraintDeferrable.objects.create(name='p1', shelf='front')\n        obj_2 = UniqueConstraintDeferrable.objects.create(name='p2', shelf='back')\n        obj_1.name = obj_2.name\n        obj_1.save()\n        with self.assertRaises(IntegrityError):\n            with atomic():\n                obj_2.save()\n"], "sample_382": ["    def test_get_template_directories_no_dirs(self):\n        self.assertEqual(autoreload.get_template_directories(), set())\n", "    def test_watch_for_template_changes_no_template_dirs(self, mock_get_template_directories):\n        mock_get_template_directories.return_value = set()\n        mock_reloader = mock.MagicMock()\n        autoreload.watch_for_template_changes(mock_reloader)\n        self.assertEqual(mock_reloader.watch_dir.call_count, 0)\n", "    def test_get_template_directories_no_dirs(self):\n        self.assertSetEqual(\n            autoreload.get_template_directories(),\n            set()\n        ) \n", "    def test_watch_for_template_changes_no_template_directories(self, mock_reset_loaders, mock_get_template_directories):\n        mock_get_template_directories.return_value = set()\n        mock_reloader = mock.MagicMock()\n        autoreload.watch_for_template_changes(mock_reloader)\n        mock_reset_loaders.assert_not_called()\n", "    def test_template_changed_with_absolute_path(self, mock_reset):\n        template_path = Path(str(ROOT / 'templates' / 'index.html'))\n        self.assertTrue(autoreload.template_changed(None, template_path))\n        mock_reset.assert_called_once()\n"], "sample_960": ["def test_py_exception(app):\n    text = \".. py:exception:: MyException\\n\"\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_addname, \"\"],\n                                                    [desc_name, \"MyException\"])],\n                                  desc_content)]))\n    assert 'MyException' in domain.objects\n    assert domain.objects['MyException'] == ('index', 'MyException', 'exception', False)\n\n", "def test_pyobject_with_alias(app):\n    text = (\".. py:object:: MyObject\\n\"\n            \"   :alias: TheObject\\n\"\n            \"\\n\"\n            \".. py:object::  TheObject\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"MyObject\"])],\n                                  [desc_content, (addnodes.index,\n                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'MyObject (built-in class)', 'MyObject', '', None)])\n    assert_node(doctree[1][1][1], desc,\n                desctype=\"class\", domain=\"py\", objtype=\"class\", noindex=False)\n\n    assert 'MyObject' in app.env.domaindata['py']['objects']\n\n\n\n", "def test_python_python_use_unqualified_type_names_add_module_names(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"n\"><a class=\"reference internal\" href=\"#foo.Name\" title=\"foo.Name\">'\n            '<span class=\"pre\">Name</span></a></span>' in content)\n    assert '<span class=\"n\"><span class=\"pre\">foo.Age</span></span>' in content\n    assert ('<p><strong>name</strong> (<a class=\"reference internal\" href=\"#foo.Name\" '\n            'title=\"foo.Name\"><em>Name</em></a>) \u2013 blah blah</p>' in content)\n    assert '<p><strong>age</strong> (<em>foo.Age</em>) \u2013 blah blah</p>' in content\n\n", "def test_python_python_use_unqualified_type_names_alias(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"n\"><a class=\"reference internal\" href=\"#foo.Name\" title=\"foo.Name\">'\n            '<span class=\"pre\">Name</span></a></span>' in content)\n    assert '<span class=\"n\"><span class=\"pre\">foo.Age</span></span>' in content\n    assert ('<p><strong>name</strong> (<a class=\"reference internal\" href=\"#foo.Name\" '\n            'title=\"foo.Name\"><em>Name</em></a>) \u2013 blah blah</p>' in content)\n    assert '<p><strong>age</strong> (<em>foo.Age</em>) \u2013 blah blah</p>' in content\n", "def test_pydecoratormethod_signature_with_return_type(app):\n    text = \".. py:decoratormethod:: deco\"\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_addname, \"@\"],\n                                                    [desc_name, \"deco\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"method\",\n                domain=\"py\", objtype=\"method\", noindex=False)\n\n    assert 'deco' in domain.objects\n    assert domain.objects['deco'] == ('index', 'deco', 'method', False)\n"], "sample_43": ["compilation error", "def test_empty_input():\n    with pytest.raises(ValueError):\n        bayesian_blocks([])\n", "def test_bad_input():\n\n    with pytest.raises(ValueError):\n        bayesian_blocks(t=None)\n", "    def test_empty_input():\n        with pytest.raises(ValueError):\n            bayesian_blocks([])\n\n        with pytest.raises(ValueError):\n            bayesian_blocks([], x = [])\n\n", "    def test_empty_input():\n        with pytest.raises(ValueError):\n            bayesian_blocks([])\n"], "sample_509": ["    def test_datestr2num_invalid():\n        with pytest.raises(ValueError, match=\"Invalid date string\"):\n            mdates.datestr2num(\"2022-01-32\")\n        with pytest.raises(ValueError, match=\"Invalid date string\"):\n            mdates.datestr2num(\"2022-13-01\")\n\n", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_237": ["    def test_model_unique_constraint_non_unique_username(self):\n        # Ensure that the check passes when using UNIQUE constraint on username\n        # even if AUTH_USER_MODEL is not unique by default.\n        class CustomUserNonUniqueUsername(AbstractBaseUser):\n            username = models.CharField(max_length=30, unique=False)\n            date_of_birth = models.DateField()\n\n            USERNAME_FIELD = 'username'\n            REQUIRED_FIELDS = ['date_of_birth']\n\n            class Meta:\n                constraints = [UniqueConstraint(fields=['username'], name='unique_username')]\n\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [])\n\n", "    def test_unique_username_with_partial_unique_constraint(self):\n        class CustomUserUniqueUsername(AbstractBaseUser):\n            username = models.CharField(max_length=30, unique=True)\n            USERNAME_FIELD = 'username'\n\n        errors = checks.run_checks(app_configs=self.apps.get_app_configs())\n        self.assertEqual(errors, [])\n\n\n\n", "    def test_is_anonymous_authenticated_attributes(self):\n        class BadUser(AbstractBaseUser):\n            username = models.CharField(max_length=30, unique=True)\n            USERNAME_FIELD = 'username'\n\n            is_anonymous = True\n            is_authenticated = True\n\n        errors = checks.run_checks(app_configs=self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Critical(\n                '%s.is_anonymous must be an attribute or property rather than '\n                'a method. Ignoring this is a security issue as anonymous '\n                'users will be treated as authenticated!' % BadUser,\n                obj=BadUser,\n                id='auth.C009',\n            ),\n            checks.Critical(\n                '%s.is_authenticated must be an attribute or property rather '\n                'than a method. Ignoring this is a security issue as anonymous '\n                'users will be treated as authenticated!' % BadUser,\n                obj=BadUser,\n                id='auth.C010',\n            ),\n        ])\n", "    def test_custom_permissions_with_unique_constraint(self):\n        class CustomUserUniqueConstraint(AbstractBaseUser):\n            username = models.CharField(max_length=30)\n            date_of_birth = models.DateField()\n            USERNAME_FIELD = 'username'\n\n            class Meta:\n                constraints = [\n                    UniqueConstraint(fields=['username'], name='username_unique'),\n                ]\n                permissions = [\n                    ('change_user', 'Can change user'),\n                ]\n\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [])\n", "    def test_permission_codename_over_100_characters(self):\n        class Checked(models.Model):\n            class Meta:\n                permissions = [\n                    ('change_checked_1234567890_1234567890_1234567890_1234567890_1234567890',\n                     'Custom permission'),\n                ]\n\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(\n            errors,\n            [\n                checks.Error(\n                    \"The permission codenamed 'change_checked_1234567890_1234567890_1234567890_1234567890_1234567890' of model 'auth_tests.Checked' is longer than 100 characters.\",\n                    obj=Checked,\n                    id='auth.E012',\n                ),\n            ],\n        )\n"], "sample_172": ["    def test_raw_id_required(self):\n        self.admin_login(username='super', password='secret', login_url='/')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_event_add'))\n\n        # Try submitting the form without selecting a band\n        self.selenium.find_element_by_xpath('//input[@value=\"Save\"]').click()\n        self.wait_for_error_message('This field is required.')\n", "    def test_ManyToMany_filtering(self):\n        self.admin_login(username='super', password='secret', login_url='/')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_event_add'))\n        main_window = self.selenium.current_window_handle\n\n        self.selenium.find_element_by_id('lookup_id_supporting_bands').click()\n        self.wait_for_and_switch_to_popup()\n\n        input_field = self.selenium.find_element_by_css_selector('input[name=\"q\"]')\n        input_field.send_keys('Bogey')\n        input_field.send_keys(Keys.RETURN)\n\n        self.wait_for_element_text('.result-list a', 'Bogey Blues')\n        self.assertEqual(\n            self.selenium.find_elements_by_css_selector('.result-list a').__len__(),\n            1,\n        )\n        self.selenium.switch_to.window(main_window)\n", "    def test_raw_id_field_with_add_another(self):\n        self.admin_login(username='super', password='secret', login_url='/')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_event_add'))\n        main_window = self.selenium.current_window_handle\n\n        # No value has been selected yet\n        self.assertEqual(self.selenium.find_element_by_id('id_venues').get_attribute('value'), '')\n        # Click on the \"Add Another\" button\n        add_another_button = self.selenium.find_element_by_id('add_id_venues')\n        add_another_button.click()\n\n        # Wait for new formset element to appear\n        self.wait_for_element_present('#id_venues-1')\n\n        # Select a band in the first fieldset\n        self.selenium.find_element_by_id('lookup_id_venues-0-band').click()\n        self.wait_for_and_switch_to_popup()\n        link = self.selenium.find_element_by_link_text('Bogey Blues')\n        self.assertIn('/band/42/', link.get_attribute('href'))\n        link.click()\n\n        # Switch back to the main window\n        self.selenium.switch_to.window(main_window)\n\n        self.wait_for_value('#id_venues-0-band', '42')\n\n        # Choose a different band for the second fieldset\n\n        self.selenium.find_element_by_id('lookup_id_venues-1-band').click()\n        self.wait_for_and_switch_to_popup()\n        link = self.selenium.find_element_by_link_text('Green Potatoes')\n        self.assertIn('/band/98/', link.get_attribute('href'))\n        link.click()\n\n        # Switch back to the main window\n        self.selenium.switch_to.window(main_window)\n\n        self.wait_for_value('#id_venues-1-band', '98')\n\n", "    def test_empty_value(self):\n        self.admin_login(username='super', password='secret', login_url='/')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_event_add'))\n\n        # Save the form with an empty value for the ForeignKey field.\n        self.selenium.find_element_by_xpath('//input[@value=\"Save\"]').click()\n\n        # Check if the error message is displayed.\n        error_message = self.selenium.find_element_by_css_selector('.errorlist li').text\n        self.assertEqual(error_message, 'This field is required.')\n", "    def test_help_text(self):\n        \"Test that the help text for the raw ID widget is displayed correctly\"\n        self.admin_login(username='super', password='secret', login_url='/')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_event_add'))\n        help_text = self.selenium.find_element_by_css_selector('.field-main_band div.help').text\n        self.assertEqual(\n            help_text,\n            'Enter the ID of the main band.'\n        )\n"], "sample_702": ["def test_popen_communicate_timeout(pytester: Pytester) -> None:\n    proc = pytester.popen(\n        [sys.executable, \"-c\", \"import time; time.sleep(1)\"],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        stdin=subprocess.PIPE,\n    )\n    with pytest.raises(subprocess.TimeoutExpired):\n        proc.communicate(timeout=0.1) \n", "def test_makefile_joins_absolute_path_with_extension(pytester: Pytester) -> None:\n    absfile = pytester.path / \"absfile.py\"\n    p1 = pytester.makepyfile(**{str(absfile): \"\"})\n    assert str(p1) == str(pytester.path / \"absfile.py\")\n", "def test_pytester_popen_input_no_close(pytester: Pytester) -> None:\n    proc = pytester.popen([sys.executable, \"-c\", \"import sys; print(sys.stdin.read())\"],\n                          stdout=subprocess.PIPE, stderr=subprocess.PIPE, stdin=subprocess.PIPE)\n    proc.stdin.write(b\"input\\n2ndline\")\n    proc.stdin.close()\n    stdout, stderr = proc.communicate()\n    assert stdout.decode(\"utf8\").splitlines() == [\"input\", \"2ndline\"]\n    assert stderr == b\"\"\n    assert proc.returncode == 0\n", "    def test_pytester_run_timeout_with_env(pytester: Pytester) -> None:\n        testfile = pytester.makepyfile(\n            \"\"\"\n            import time\n            import os\n\n                time.sleep(10)\n                print(os.environ.get('PYTEST_ADDOPTS'))\n        \"\"\"\n        )\n        with pytest.raises(pytester.TimeoutExpired):\n            pytester.runpytest_subprocess(testfile, timeout=1, env={\"PYTEST_ADDOPTS\": \"--verbose\"})\n", "def test_spawned_process_handles_non_zero_exit(pytester: Pytester) -> None:\n    p1 = pytester.makepyfile(\n        \"\"\"\n        import sys\n        sys.exit(42)\n    \"\"\"\n    )\n    child = pytester.spawn_pytest(str(p1))\n    child.wait()\n    assert child.ret == 42\n"], "sample_1208": ["compilation error", "def test_MatrixGammaDistribution():\n    M = MatrixGammaDistribution(1, 2, [[1, 0], [0, 1]])\n    assert M.n_rows == 2\n    assert M.n_cols == 2\n    assert M.scale_matrix == Matrix([[1, 0], [0, 1]])\n    assert M.alpha == 1\n    assert M.beta == 2\n    raises(ValueError, lambda: MatrixGammaDistribution(-1, 2, [[1, 0], [0, 1]]))\n    raises(ValueError, lambda: MatrixGammaDistribution(1, -2, [[1, 0], [0, 1]]))\n    raises(ValueError, lambda: MatrixGammaDistribution(1, 2, [[1, 0], [2, 1]]))\n\n", "compilation error", "compilation error", "compilation error"], "sample_54": ["    def test_file_response_with_filename(self):\n        response = FileResponse(io.BytesIO(b'binary content'), filename='myfile.txt')\n        self.assertEqual(response['Content-Disposition'], 'inline; filename=\"myfile.txt\"')\n        self.assertEqual(list(response), [b'binary content'])\n", "    def test_file_response_with_filename_argument(self):\n        response = FileResponse(open(__file__, 'rb'), filename='different_name.py')\n        self.assertEqual(response['Content-Length'], str(os.path.getsize(__file__)))\n        self.assertIn(response['Content-Type'], ['text/x-python', 'text/plain'])\n        self.assertEqual(response['Content-Disposition'], 'inline; filename=\"different_name.py\"')\n        response.close()\n", "    def test_file_response_with_filename_from_contentfile(self):\n        content_file = ContentFile(\"some content\")\n        response = FileResponse(content_file, filename='test.txt')\n        self.assertEqual(response['Content-Disposition'], 'attachment; filename=\"test.txt\"')\n        self.assertEqual(list(response), [b'some content'])\n", "    def test_file_response_with_filename(self):\n        response = FileResponse(io.BytesIO(b'binary content'), filename='download.txt')\n        self.assertEqual(response['Content-Length'], '14')\n        self.assertEqual(response['Content-Type'], 'application/octet-stream')\n        self.assertEqual(response['Content-Disposition'], 'inline; filename=\"download.txt\"')\n", "    def test_file_response_with_custom_filename(self):\n        response = FileResponse(ContentFile(b'binary content'), filename='custom_name.txt')\n        self.assertEqual(response['Content-Disposition'], 'attachment; filename=\"custom_name.txt\"')\n        self.assertEqual(list(response), [b'binary content'])\n"], "sample_116": ["    def test_cache_middleware_no_cache(self):\n            return HttpResponse('Hello World')\n\n        request = self.factory.get('/view/')\n        response = view(request)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.content, b'Hello World')\n\n        # No cache middleware should mean no cached response\n        response = CacheMiddleware().process_request(request)\n        self.assertIsNone(response)\n", "    def test_cache_control_no_cache(self):\n        with self.settings(CACHE_MIDDLEWARE_SECONDS=30):\n            view = cache_page(30)(cache_control(no_cache=True)(hello_world_view))\n\n            request = self.factory.get('/view/')\n            response = view(request, '1')\n\n            self.assertEqual(response['Cache-Control'], 'no-cache')\n            response = view(request, '2')\n            self.assertEqual(response.status_code, 200)\n            self.assertEqual(response.content, b'Hello World 2')\n\n", "    def test_cache_control_max_age(self):\n        cache_control = CacheControl(max_age=30)\n        view = cache_page(30)(cache_control(hello_world_view))\n        request = self.factory.get('/view/')\n        response = view(request, '1')\n        self.assertEqual(response.content, b'Hello World 1')\n\n        # Check for Cache-Control header\n        self.assertIn(\"Cache-Control\", response)\n        self.assertEqual(response['Cache-Control'], 'max-age=30')\n\n        # We expect the response to be cached.\n        response = view(request, '2')\n        self.assertEqual(response.content, b'Hello World 1')\n", "    def test_cache_middleware_error_handling(self):\n        middleware = CacheMiddleware()\n        request = self.factory.get('/view/')\n\n            raise Exception(\"Boom!\")\n\n        try:\n            response = middleware.process_view(request, view_raising_exception, (), {})\n            self.fail(\"Should have raised an exception\")\n        except Exception as e:\n            self.assertEqual(str(e), \"Boom!\")\n        \n        \n", "    def test_cache_key_etag_header(self):\n        request = self.factory.get(self.path)\n        template = engines['django'].from_string(\"This is a test\")\n        response = TemplateResponse(HttpRequest(), template)\n        response['ETag'] = 'some-etag'\n        learn_cache_key(request, response)\n        self.assertEqual(\n            get_cache_key(request),\n            'views.decorators.cache.cache_page.settingsprefix.GET.'\n            '58a0a05c8a5620f813686ff969c26853.some-etag'\n        )\n"], "sample_920": ["    def test_raises_types_with_quotes(self):\n        docstrings = [(\"\"\"", "    def test_numpydocstring_with_empty_sections(self):\n        docstring = \"\"\"", "    def test_google_style_section_header_underline_length(self):\n        docstrings = [(\"\"\"", "    def test_examples_with_different_indentation(self):\n        docstring = \"\"\"", "    def test_parameter_types_with_parentheses(self):\n        docstring = \"\"\""], "sample_527": ["compilation error", "    def test_toolmanager_is_active(self):\n        with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n            plt.rcParams['toolbar'] = 'toolmanager'\n        fig = plt.gcf()\n        fig.canvas.manager.toolmanager.trigger_tool('zoom')\n        assert fig.canvas.manager.toolmanager.is_active('zoom')\n        fig.canvas.manager.toolmanager.trigger_tool('pan')\n        assert not fig.canvas.manager.toolmanager.is_active('zoom')\n        assert fig.canvas.manager.toolmanager.is_active('pan')\n", "def test_toolmanager_toggle():\n    with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n        plt.rcParams['toolbar'] = 'toolmanager'\n    fig = plt.gcf()\n    toolmanager = fig.canvas.manager.toolmanager\n    assert 'zoom' in toolmanager.tools\n    assert not fig.canvas.manager.toolmanager.tool_active('zoom')\n\n    toolmanager.trigger_tool('zoom')\n    assert fig.canvas.manager.toolmanager.tool_active('zoom')\n\n    toolmanager.trigger_tool('zoom')\n    assert not fig.canvas.manager.toolmanager.tool_active('zoom') \n", "    def test_toolmanager_tool_class(self, monkeypatch):\n        with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n            plt.rcParams['toolbar'] = 'toolmanager'\n        fig = plt.gcf()\n        fig.canvas.manager.toolmanager\n\n        class CustomTool(NavigationToolbar2):\n            name = 'Custom'\n\n        monkeypatch.setattr(fig.canvas.manager.toolmanager, 'tools', {})\n        fig.canvas.manager.toolmanager.add_tool(CustomTool, 'Custom')\n        assert isinstance(fig.canvas.manager.toolmanager.get_tool('Custom'),\n                          CustomTool)\n", "    def test_toolbar_mode_toggle(self, key, expected_mode):\n        with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n            plt.rcParams['toolbar'] = 'toolmanager'\n        fig, ax = plt.subplots()\n        tb = NavigationToolbar2(fig.canvas)\n        tb.pan()\n        assert ax.get_navigate_mode() == \"PAN\"\n\n        if key:\n            tb.press_pan(MouseEvent(\"button_press_event\", fig.canvas,\n                                     *ax.transData.transform((0.5, 0.5)).astype(int),\n                                     button=MouseButton.LEFT, key=key))\n            tb.drag_pan(MouseEvent(\"motion_notify_event\", fig.canvas,\n                                    *ax.transData.transform((0.6, 0.6)).astype(int),\n                                    button=MouseButton.LEFT, key=key))\n        else:\n            tb.press_pan(MouseEvent(\"button_press_event\", fig.canvas,\n                                     *ax.transData.transform((0.5, 0.5)).astype(int),\n                                     button=MouseButton.LEFT))\n            tb.drag_pan(MouseEvent(\"motion_notify_event\", fig.canvas,\n                                    *ax.transData.transform((0.6, 0.6)).astype(int),\n                                    button=MouseButton.LEFT))\n        assert ax.get_navigate_mode() == expected_mode\n"], "sample_879": ["    def test_ordinal_encoder_handle_missing_and_unknown(X, expected_X_trans, X_test):\n        \"\"\"Test the interaction between missing values and handle_unknown\"\"\"\n\n        oe = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n\n        X_trans = oe.fit_transform(X)\n        assert_allclose(X_trans, expected_X_trans)\n\n        assert_allclose(oe.transform(X_test), [[-1.0]])\n", "    def test_ordinal_encoder_sparse_input_error_message(self):\n        \"\"\"Check error message for OrdinalEncoder when sparse input is passed.\"\"\"\n        X_sparse = sparse.csr_matrix([[1, 2], [3, 4]])\n        encoder = OrdinalEncoder()\n\n        err_msg = (\n            \"A sparse matrix was passed, but dense data is required. \"\n            \"Sparse input is not supported by OrdinalEncoder.\"\n        )\n        with pytest.raises(TypeError, match=err_msg):\n            encoder.fit(X_sparse)\n\n        with pytest.raises(TypeError, match=err_msg):\n            encoder.fit_transform(X_sparse)\n\n        with pytest.raises(TypeError, match=err_msg):\n            encoder.inverse_transform(X_sparse\n", "    def test_ordinal_encoder_missing_unknown_encoding_max_pandas():\n        \"\"\"Check missing value or unknown encoding can equal the cardinality\n\n        with pandas DataFrame.\n        \"\"\"\n        pd = pytest.importorskip(\"pandas\")\n        X = pd.DataFrame([[\"dog\"], [\"cat\"], [np.nan]], columns=[\"animal\"])\n        X_trans = OrdinalEncoder(encoded_missing_value=2).fit_transform(X)\n        assert_allclose(X_trans, [[1], [0], [2]])\n\n        enc = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=2).fit(X)\n        X_test = pd.DataFrame([[\"snake\"]], columns=[\"animal\"])\n        X_trans = enc.transform(X_test)\n        assert_allclose(X_trans, [[2]])\n", "def test_onehot_encoder_empty_categories():\n    \"\"\"Check OneHotEncoder handles empty categories gracefully.\"\"\"\n    ohe = OneHotEncoder(categories=[[], [\"a\", \"b\"]])\n    with pytest.raises(ValueError, match=\"Found empty categories in feature 0\"):\n        ohe.fit([[\"\"], [\"a\"]])\n", "def test_ordinal_encoder_unknown_integer_categoricals():\n    \"\"\"Check OrdinalEncoder with unknown categories in integer data.\n    \"\"\"\n    X = np.array([[1, 2], [3, 4], [5, 6]], dtype=int)\n    enc = OrdinalEncoder(handle_unknown='error').fit(X)\n    X_test = np.array([[1, 7]], dtype=int)\n    with pytest.raises(ValueError, match=\"Found unknown categories\"):\n        enc.transform(X_test)\n\n"], "sample_733": ["    def test_count_vectorizer_empty_corpus():\n        vect = CountVectorizer()\n        assert_raises(ValueError, vect.fit, [])\n        assert_raises(ValueError, vect.transform, [])\n", "    def test_vectorizer_empty_document_list(self):\n        vect = CountVectorizer()\n        assert_raises(ValueError, vect.fit, [])\n        assert_raises(ValueError, vect.transform, [])\n\n", "def test_countvectorizer_empty_input():\n    vect = CountVectorizer()\n    assert_raises(ValueError, vect.fit, [])\n    assert_raises(ValueError, vect.fit_transform, [])\n    assert_raises(ValueError, vect.transform, [])\n\n", "def test_tfidfvectorizer_max_features_with_tokenizer():\n    # test max_features with a custom tokenizer that produces more tokens than\n    # max_features.\n    tokenizer = lambda text: text.split()\n    vectorizer = TfidfVectorizer(tokenizer=tokenizer, max_features=2)\n    X = vectorizer.fit_transform([\"This is a test sentence.\",\n                                  \"This is another sentence.\"])\n    assert_equal(len(vectorizer.vocabulary_), 2)\n", "    def test_vectorizer_empty_document_list():\n        message = \"Document list is empty, cannot fit.\"\n        for vec in [CountVectorizer(), TfidfVectorizer(), HashingVectorizer()]:\n            assert_raise_message(ValueError, message, vec.fit, [])\n"], "sample_672": ["def test_saferepr_maxsize_of_huge_list():\n    l = list(range(10000))\n    result = saferepr(l, maxsize=100)\n    assert len(result) <= 100\n    assert result.startswith(\"[\") and result.endswith(\"]\")\n    assert \"...\" in result\n", "def test_saferepr_with_long_strings():\n    long_string = \"x\" * 1000\n    assert len(saferepr(long_string)) <= 240\n\n", "def test_saferepr_with_custom_maxsize():\n    class LargeObject:\n            return \"x\" * 1000\n\n    maxsize = 50\n    result = saferepr(LargeObject(), maxsize=maxsize)\n    assert len(result) == maxsize\n    assert result == repr(\"x\" * (maxsize // 2 - 2)) + \"...\" + repr(\"x\" * (maxsize // 2 - 2))\n", "def test_saferepr_instance_with_exception():\n    class BrokenInstance:\n            self.ex = Exception(\"Broken instance\")\n\n            raise self.ex\n\n    obj = BrokenInstance()\n    repr_str = saferepr(obj)\n    assert \"Exception\" in repr_str\n    assert \"Broken instance\" in repr_str\n    assert \"BrokenInstance\" in repr_str\n", "def test_saferepr_recursion():\n    class Recursive:\n            self.value = value\n\n            return repr(Recursive(self.value + 1))\n\n    r = Recursive(1)\n    s = saferepr(r, maxsize=100)\n    assert '<[RecursionError raised in repr()] Recursive object at 0x' in s\n"], "sample_824": ["compilation error", "    def test_check_preserve_type_sparse():\n        # Ensures that type float32 is preserved for sparse matrices.\n        XA = np.resize(np.arange(40), (5, 8)).astype(np.float32)\n        XB = np.resize(np.arange(40), (5, 8)).astype(np.float32)\n        XA_sparse = csr_matrix(XA)\n        XB_sparse = csr_matrix(XB)\n\n        XA_checked, XB_checked = check_pairwise_arrays(XA_sparse, None)\n        assert_equal(XA_checked.dtype, np.float32)\n\n        # both float32\n        XA_checked, XB_checked = check_pairwise_arrays(XA_sparse, XB_sparse)\n        assert_equal(XA_checked.dtype, np.float32)\n        assert_equal(XB_checked.dtype, np.float32)\n", "def test_pairwise_kernels_sparse_input_and_output():\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n    Y = rng.random_sample((3, 4))\n    Xcsr = csr_matrix(X)\n    Ycsr = csr_matrix(Y)\n\n    # Test that sparse input yields sparse output when dense_output=False\n    K1 = pairwise_kernels(Xcsr, Ycsr, metric=\"linear\", dense_output=False)\n    assert issparse(K1)\n\n    # Test that sparse input yields dense output when dense_output=True\n    K2 = pairwise_kernels(Xcsr, Ycsr, metric=\"linear\", dense_output=True)\n    assert not issparse(K2)\n\n    # Test that dense input yields dense output when dense_output=True\n    K3 = pairwise_kernels(X, Y, metric=\"linear\", dense_output=True)\n    assert not issparse(K3)\n\n    # Test equality of dense and sparse outputs\n    assert_array_almost_equal(K1.todense(), K2)\n    assert_array_almost_equal(K2, K3)\n", "def test_haversine_distances_single_point():\n    # Check haversine distance with single point\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((1, 2))\n    Y = rng.random_sample((10, 2))\n    D = haversine_distances(X, Y)\n    assert D.shape == (1, 10)\n", "            def test_check_preserves_sparse_format():\n                # Ensure the sparse format is not changed by check functions.\n                rng = np.random.RandomState(0)\n                XA = rng.random_sample((5, 4)).astype(np.float32)\n                XA_sparse = csr_matrix(XA)\n                XB = rng.random_sample((5, 4)).astype(np.float32)\n                XB_sparse = csr_matrix(XB)\n\n                XA_checked, XB_checked = check_pairwise_arrays(XA_sparse,\n                                                              XB_sparse)\n                assert isinstance(XA_checked, csr_matrix)\n                assert isinstance(XB_checked, csr_matrix)\n\n"], "sample_1091": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_755": ["def test_davies_bouldin_score():\n    assert_raises_on_only_one_label(davies_bouldin_score)\n\n    assert_raises_on_all_points_same_cluster(davies_bouldin_score)\n\n    # Assert the value is 0. when all samples are equals\n    assert 0. == davies_bouldin_score(np.ones((10, 2)),\n                                  [0] * 5 + [1] * 5)\n\n    # Assert the value is close to 0. when all the mean cluster are very close\n    assert pytest.approx(davies_bouldin_score([[-1, -1], [1, 1]] * 10,\n                                            [0] * 10 + [1] * 10), 0)\n\n    # General case (with non numpy arrays)\n    X = ([[0, 0], [1, 1]] * 5 + [[3, 3], [4, 4]] * 5 +\n         [[0, 4], [1, 3]] * 5 + [[3, 1], [4, 0]] * 5)\n    labels = [0] * 10 + [1] * 10 + [2] * 10 + [3] * 10\n    pytest.approx(davies_bouldin_score(X, labels),\n                  0.5*(1. / 2.5 + 1. / 2.)*(1+1+1+1))\n", "def test_davies_bouldin_score():\n    assert_raises_on_only_one_label(davies_bouldin_score)\n    assert_raises_on_all_points_same_cluster(davies_bouldin_score)\n\n    # Assert the value is 0. when all samples are equals\n    assert 0. == davies_bouldin_score(np.ones((10, 2)),\n                                    [0] * 5 + [1] * 5)\n\n    # Assert the value is 0. when all the clusters are identical\n    assert 0. == davies_bouldin_score([[-1, -1], [1, 1]] * 10,\n                                    [0] * 10 + [1] * 10)\n\n    # General case (with non numpy arrays)\n    X = ([[0, 0], [1, 1]] * 5 + [[3, 3], [4, 4]] * 5 +\n         [[0, 4], [1, 3]] * 5 + [[3, 1], [4, 0]] * 5)\n    labels = [0] * 10 + [1] * 10 + [2] * 10 + [3] * 10\n    pytest.approx(davies_bouldin_score(X, labels), 0.5)\n\n", "def test_davies_bouldin_score():\n    assert_raises_on_only_one_label(davies_bouldin_score)\n\n    assert_raises_on_all_points_same_cluster(davies_bouldin_score)\n\n    # Assert the value is 0. when all samples are equals\n    assert 0. == davies_bouldin_score(np.ones((10, 2)),\n                                    [0] * 5 + [1] * 5)\n\n    # Assert the value is high when all the mean cluster are close\n    X = [[0, 0], [0.1, 0.1], [1, 1], [1.1, 1.1]] * 5\n    labels = [0] * 5 + [1] * 5 + [2] * 5 + [3] * 5\n    assert davies_bouldin_score(X, labels) > 5\n\n", "    def test_davies_bouldin_score():\n        assert_raises_on_only_one_label(davies_bouldin_score)\n\n        assert_raises_on_all_points_same_cluster(davies_bouldin_score)\n\n        # Assert that davies_bouldin_score is not defined for less than 2 labels\n        X = np.array([[0, 0], [1, 1]])\n        labels = [0]\n\n        assert_raises_regexp(ValueError, \"Number of labels is 1. Valid values are 2 to n_samples - 1 \\(inclusive\\)\",\n                         davies_bouldin_score, X, labels)\n\n        # Assert the value is 0. when all samples are equals\n        assert 0. == davies_bouldin_score(np.ones((10, 2)),\n                                             [0]*5 + [1]*5)\n", "compilation error"], "sample_176": ["    def test_mti_inconsistent_inheritance_removal(self):\n        Animal = ModelState('app', 'Animal', [\n            (\"id\", models.AutoField(primary_key=True)),\n        ])\n        Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n        Cat = ModelState('app', 'Cat', [], bases=('app.Animal',))\n        changes = self.get_changes([Animal, Dog, Cat], [Animal, Dog])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['DeleteModel'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='Cat')\n", "    def test_mti_inheritance_field_removal(self):\n        Animal = ModelState('app', 'Animal', [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=100)),\n        ])\n        Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n        changes = self.get_changes([Animal, Dog], [Animal, Dog,])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['RemoveField'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='name', model_name='Animal')\n", "    def test_mti_removal_with_proxy(self):\n        Animal = ModelState('app', 'Animal', [\n            (\"id\", models.AutoField(primary_key=True)),\n        ])\n        Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n        DogProxy = ModelState('app', 'DogProxy', [], bases=('app.Dog',), abstract=True)\n        changes = self.get_changes([Animal, Dog, DogProxy], [Animal])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['DeleteModel', 'DeleteModel'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='Dog')\n        self.assertOperationAttributes(changes, 'app', 0, 1, name='DogProxy')\n", "    def test_remove_and_recreate_model(self):\n        Animal = ModelState('app', 'Animal', [\n            (\"id\", models.AutoField(primary_key=True)),\n        ])\n        Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n        changes = self.get_changes([Animal, Dog], [])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['DeleteModel', 'DeleteModel', 'CreateModel', 'CreateModel'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='Animal')\n        self.assertOperationAttributes(changes, 'app', 0, 1, name='Dog')\n", "    def test_mti_inheritance_field_removal(self):\n        Animal = ModelState('app', 'Animal', [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=100)),\n        ])\n        Dog = ModelState('app', 'Dog', [], bases=('app.Animal',))\n        changes = self.get_changes([Animal, Dog], [Animal, Dog])\n\n        Animal.fields.pop()\n        changes = self.get_changes([Animal, Dog], [Animal, Dog])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['RemoveField'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='name', model_name='Animal')\n"], "sample_634": ["    def test_expand_modules_with_ignore_paths(self, files_or_modules, ignore_paths, expected):\n        \"\"\"Test expand_modules with specified ignore-paths\"\"\"\n        ignore_list, ignore_list_re = [], []\n        set_config(self, ignore_paths=ignore_paths)\n        modules, errors = expand_modules(\n            files_or_modules,\n            ignore_list,\n            ignore_list_re,\n            get_global_option(self, \"ignore-paths\"),\n        )\n        modules.sort(key=lambda d: d[\"name\"])\n        assert modules == expected\n        assert not errors\n", "    def test_expand_modules_ignore(self, files_or_modules, ignore, expected):\n        \"\"\"Test expand_modules with ignore pattern matching.\"\"\"\n        ignore_list = ignore\n        ignore_list_re = [re.compile(p) for p in ignore]\n        modules, errors = expand_modules(\n            files_or_modules,\n            ignore_list,\n            ignore_list_re,\n            get_global_option(self, \"ignore-paths\"),\n        )\n        modules.sort(key=lambda d: d[\"name\"])\n        assert modules == expected\n        assert not errors\n\n", "    def test_expand_modules_ignore_paths(\n        self, files_or_modules, ignore_paths, expected", "    def test_expand_modules_ignore_paths(\n        self, files_or_modules, ignore_paths, expected", "    def test_expand_modules_package(self, files_or_modules, expected):\n        \"\"\"Test expand_modules with a package as input\"\"\"\n        ignore_list, ignore_list_re = [], []\n        modules, errors = expand_modules(\n            files_or_modules,\n            ignore_list,\n            ignore_list_re,\n            get_global_option(self, \"ignore-paths\"),\n        )\n        modules.sort(key=lambda d: d[\"name\"])\n        assert modules == expected\n        assert not errors\n\n"], "sample_668": ["def test_deprecated_fixturenames(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            pass\n\n            pass\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: The `funcargnames` attribute was an alias for `fixturenames`*\",\n            \"*since pytest 2.3 - use the newer attribute instead.*\",\n        ]\n    )\n\n", "def test_fixture_positional_arguments_warning(testdir, fixture_name, fixture_args):\n    testdir.makepyfile(\n        f\"\"\"\n        def {fixture_name}({', '.join(str(arg) for arg in fixture_args)}):\n            return {fixture_name}\n\n             pass\n\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    warning_msg = (\n        \"Passing arguments to pytest.fixture() as positional arguments is deprecated - pass them \"\n        \"as a keyword argument instead.\"\n    )\n    result.stdout.fnmatch_lines([warning_msg])\n", "def test_fixture_positional_arguments_warning(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            return arg1 + arg2\n\n            assert my_fixture == 3\n\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"Passing arguments to pytest.fixture() as positional arguments is deprecated - pass them \",\n            \"as a keyword argument instead.\"\n        ]\n    )\n\n", "def test_fixture_positional_arguments_warning(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            return arg1, arg2\n\n            a, b = my_fixture\n            assert a == 'value1'\n            assert b == 'default'\n\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*Passing arguments to pytest.fixture() as positional arguments is deprecated*\",\n            \"*pass them as a keyword argument instead*\",\n        ]\n    )\n", "def test_node_direct_ctor_warning_with_parent(testdir):\n    class MockConfig:\n        pass\n\n    ms = MockConfig()\n    with pytest.warns(\n        DeprecationWarning,\n        match=\"Direct construction of .* has been deprecated, please use .*.from_parent.*\",\n    ) as w:\n        nodes.Item(\"test\", parent=ms)\n    assert w[0].lineno == inspect.currentframe().f_lineno - 1\n    assert w[0].filename == __file__\n"], "sample_60": ["    def test_generic_inline_admin_with_fk(self):\n        category = Category.objects.create(name='male')\n        contact = Contact.objects.create(name='John Doe', category=category)\n\n        # Simulate a POST request with inline data\n        post_data = {\n            \"name\": \"Updated Name\",\n            \"category\": str(category.id),\n            'generic_inline_admin-phonenumber-content_type-object_id-TOTAL_FORMS': '2',\n            'generic_inline_admin-phonenumber-content_type-object_id- INITIAL_FORMS': '0',\n            'generic_inline_admin-phonenumber-content_type-object_id-MAX_NUM_FORMS': '',\n            'generic_inline_admin-phonenumber-content_type-object_id-0-id': '',\n            'generic_inline_admin-phonenumber-content_type-object_id-0-phone_number': '555-123-4567',\n            'generic_inline_admin-phonenumber-content_type-object_id-0-category': str(category.id),\n            'generic_inline_admin-phonenumber-content_type-object_id-1-id': '',\n            'generic_inline_admin-phonenumber-content_type-object_id-1-phone_number': '555-987-6543',\n            'generic_inline_admin-phonenumber-content_type-object_id-1-category': str(category.id),\n        }\n\n        response = self.client.post(reverse('admin:generic_inline_admin_contact_change', args=(contact.pk,)), post_data)\n        self.assertEqual(response.status_code, 302)\n        self.assertRedirects(response, reverse('admin:generic_inline_admin_contact_change', args=(contact.pk,)))\n        #assert that the contact has the updated phone numbers after redirect\n\n", "    def test_custom_form_meta_exclude_with_exclude(self):\n        class MediaForm(ModelForm):\n            class Meta:\n                model = Media\n                exclude = ['url']\n\n        class MediaInline(GenericTabularInline):\n            exclude = ['description']\n            form = MediaForm\n            model = Media\n\n        class EpisodeAdmin(admin.ModelAdmin):\n            exclude = ['keywords']\n            inlines = [\n                MediaInline\n            ]\n\n        ma = EpisodeAdmin(Episode, self.site)\n        self.assertEqual(\n            list(list(ma.get_formsets_with_inlines(request))[0][0]().forms[0].fields),\n            ['id', 'DELETE'])\n\n", "    def test_get_extra_from_request(self):\n        class ExtraInline(GenericTabularInline):\n            model = Media\n            extra = 0\n\n                return request.GET.get('extra', 0)\n\n        modeladmin = admin.ModelAdmin(Episode, admin_site)\n        modeladmin.inlines = [ExtraInline]\n\n        e = self._create_object(Episode)\n        request = self.factory.get(reverse('admin:generic_inline_admin_episode_change', args=(e.pk,)), {'extra': '5'})\n        request.user = User(username='super', is_superuser=True)\n        response = modeladmin.changeform_view(request, object_id=str(e.pk))\n        formset = response.context_data['inline_admin_formsets'][0].formset\n        self.assertEqual(formset.extra, 5)\n", "    def test_get_formset_kwargs_with_fk_name(self):\n        class MediaInline(GenericTabularInline):\n            model = Media\n            fk_name = 'content_object'\n\n        media_inline = MediaInline(Episode, AdminSite())\n\n        # Create a formset with default arguments\n        formset = media_inline.get_formset(request)\n        self.assertEqual(formset.fk_name, 'content_object')\n", "    def test_get_formset_with_custom_fk_name(self):\n        class CustomFKInline(GenericTabularInline):\n            model = Media\n            fk_name = 'content_object'\n\n        class EpisodeAdmin(admin.ModelAdmin):\n            inlines = [CustomFKInline]\n\n        ma = EpisodeAdmin(Episode, self.site)\n        formset = ma.get_formset(request)\n        self.assertEqual(formset.fk_name, 'content_object')\n"], "sample_774": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_823": ["def test_pairwise_distances_chunked_empty():\n    # Check that pairwise_distances_chunked works with empty input arrays.\n    X = np.array([], dtype=np.float64).reshape(0, 4)\n    # Check that no error is raised when working with empty arrays\n\n    chunks = list(pairwise_distances_chunked(X, working_memory=1))\n    assert len(chunks) == 0\n\n", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_213": ["    def test_file_object_read_after_save(self):\n        \"\"\"\n        Test that reading a file-like object after saving it still works.\n        \"\"\"\n        file_like = io.BytesIO(b'some content')\n        self.storage.save('test.txt', file_like)\n        file_like.seek(0)\n        content = file_like.read()\n        self.assertEqual(content, b'some content')\n", "    def test_file_like_object_from_stringio(self):\n        output = StringIO('content')\n        self.storage.save('tests/stringio.txt', output)\n        self.client.get('/tests/stringio.txt')\n        self.assertEqual(self.client.status_code, 200)\n        self.assertEqual(self.client.content, b'content')\n", "    def test_file_like_object(self):\n        file_like = StringIO('Content')\n        self.storage.save('file-like.txt', file_like)\n        path = os.path.join(self.storage.location, 'file-like.txt')\n        self.assertTrue(os.path.exists(path))\n        with open(path, 'r') as f:\n            self.assertEqual(f.read(), 'Content')\n\n", "    def test_file_object_with_seek(self):\n        # Test saving a file-like object that supports seek.\n        output = io.BytesIO()\n        output.write(b'content')\n        output.seek(0)  \n\n        self.storage.save('tests/file_object_seek', output)\n        self.assertTrue(self.storage.exists('tests/file_object_seek'))\n        with self.storage.open('tests/file_object_seek') as f:\n            self.assertEqual(f.read(), b'content')\n\n", "    def test_file_like_object_save(self):\n        # Test saving a file-like object to storage.\n        with open('test.txt', 'wb') as f:\n            f.write(b'some content')\n        with open('test.txt', 'rb') as f:\n            self.storage.save('file_like_object.txt', f)\n        self.assertTrue(os.path.exists(os.path.join(self.temp_dir, 'file_like_object.txt')))\n        with open(os.path.join(self.temp_dir, 'file_like_object.txt'), 'rb') as f:\n            self.assertEqual(f.read(), b'some content')\n"], "sample_154": ["    def test_database_checks_returns_issues(self, mocked_check):\n        mocked_check.side_effect = lambda **kwargs: ['issue1', 'issue2']\n        issues = check_database_backends(databases=self.databases)\n        self.assertEqual(issues, ['issue1', 'issue2'])\n", "    def test_database_checks_return_value(self, mocked_check):\n        mocked_check.return_value = ['issue1', 'issue2']\n        issues = check_database_backends(databases=self.databases)\n        self.assertEqual(issues, ['issue1', 'issue2']) \n", "    def test_database_checks_results(self, mocked_check):\n        mocked_check.return_value = ['issue 1', 'issue 2']\n        issues = check_database_backends(databases=self.databases)\n        self.assertEqual(issues, ['issue 1', 'issue 2'])\n", "    def test_database_checks_returns_issues(self, mocked_check):\n        mocked_check.return_value = ['issue 1', 'issue 2']\n        issues = check_database_backends(databases=self.databases)\n        self.assertEqual(issues, ['issue 1', 'issue 2'])\n", "    def test_database_checks_return_issues(self, mocked_check):\n        mocked_check.return_value = ['issue 1', 'issue 2']\n        issues = check_database_backends(databases=self.databases)\n        self.assertEqual(issues, ['issue 1', 'issue 2']) \n"], "sample_1037": ["compilation error", "compilation error", "compilation error", "def test_MatMul_scalar_expand():\n    A = MatrixSymbol('A', 2, 2)\n    x = symbols('x')\n    assert MatMul(x, A) == x*A\n    assert MatMul(A, x) == A*x\n    # Test cases where the scalar is not the first or last argument\n\n    assert MatMul(A, 2, A) == 2*MatMul(A, A)\n    assert MatMul(2, A, A) == 2*MatMul(A, A)\n\n\n\n", "    def test_transpose_identity():\n        A = MatrixSymbol('A', 2, 2)\n        assert (A.T).I == (A.I).T\n"], "sample_1046": ["def test_tensor_replacement_complex_numbers():\n    L = TensorIndexType(\"L\")\n    i, j, k = tensor_indices(\"i j k\", L)\n    A, B = tensorhead(\"A B\", [L], [[1]])\n    H = tensorhead(\"H\", [L, L], [[1], [1]])\n\n    expr = A(i)*B(j) + 1j*H(i, j)\n    repl = {A(i): [1, 2], B(j): [3, 4], H(i, j): [[5, 6], [7, 8]]}\n    assert expr.replace_with_arrays(repl) == Array([[1*3 + 1j*5, 1*4 + 1j*6],\n                                                  [2*3 + 1j*7, 2*4 + 1j*8]])\n", "compilation error", "def test_issue_11173_SimplifyTensMul():\n    L = TensorIndexType(\"Lorentz\", dummy_fmt='L', dim=4)\n    i0, i1, i2 = tensor_indices('i_0:3', L)\n    a, b, c, d = tensor_indices('a b c d', L)\n\n    A = tensorhead(\"A\", [L], [[1]])\n    B = tensorhead(\"B\", [L, L], [[1], [1]])\n    C = tensorhead(\"C\", [L, L, L], [[1], [1], [1]])\n\n    expr = A(i0)*B(i0, i1)\n    assert (expr.simplify()).equals(A(i0)*B(i0, i1))\n\n    expr = A(i0)*B(i0, -i1)*A(-i1)\n    assert (expr.simplify()).equals(A(i0)*B(i0, -i1)*A(-i1))\n\n    expr = A(i0)*B(i0, i1)*C(i1, i2, i0)\n    assert (expr.simplify()).equals(A(i0) * B(i0, i1) * C(i1, i2, i0))\n\n\n", "def test_tensor_index_contraction():\n    L = TensorIndexType(\"L\")\n\n    i, j, k = tensor_indices(\"i j k\", L)\n    A, B, C = tensorhead(\"A B C\", [L], [[1]])\n\n    expr = A(i)*B(i)\n    assert expr.contract() == A(i)*B(i)\n    assert expr.contract(i) == A(i)*B(i)\n    expr = A(i)*B(j)*C(j)\n    assert expr.contract(j) == A(i)*B(j)*C(j).contract(j)\n    assert expr.contract(j).contract(i) == A(i)*B(j)*C(j).contract(j,i)\n    assert expr.contract(i, j) == A(i)*B(j)*C(j).contract(i,j)\n    assert expr.contract(j, i) == A(i)*B(j)*C(j).contract(j,i)\n", "compilation error"], "sample_8": ["    def test_masked_array_from_numpy_ma(self):\n        np_ma = np.ma.masked_where(self.a > 5, self.a)\n        msa = Masked(np_ma)\n        assert type(msa) is MaskedNDArray\n        assert type(msa.data) is self._data_cls\n        assert type(msa.mask) is np.ndarray\n        assert_array_equal(msa.data, np_ma.data)\n        assert_array_equal(msa.mask, np_ma.mask)\n", "    def test_masked_array_to_masked(self):\n        ma = Masked(self.a, mask=self.mask_a)\n        np_ma = np.ma.masked_array(ma)\n        assert_array_equal(np_ma.data, ma.unmasked)\n        assert_array_equal(np_ma.mask, ma.mask)\n", "    def test_masked_array_to_masked(self):\n        ma = Masked(self.a, mask=self.mask_a)\n        assert type(ma) is Masked\n        assert type(ma.unmasked) is self._data_cls\n        assert type(ma.mask) is np.ndarray\n        assert_array_equal(ma.unmasked, self.a)\n        assert_array_equal(ma.mask, self.mask_a) \n", "    def test_masked_array_to_masked(self):\n        \"\"\"Check that we can convert a MaskedArray to our MaskedArray.\"\"\"\n        np_ma = np.ma.MaskedArray(self.a, mask=self.mask_a)\n        ma = Masked(np_ma)\n        assert type(ma) is Masked\n        assert type(ma.unmasked) is self._data_cls\n        assert type(ma.mask) is np.ndarray\n        assert_array_equal(ma.unmasked, self.a)\n        assert_array_equal(ma.mask, self.mask_a)\n", "    def test_masked_array_from_numpy_ma(self):\n        \"\"\"Check that we can initialize a MaskedArray from a numpy.ma.MaskedArray.\"\"\"\n        np_ma = np.ma.MaskedArray(self.a, mask=self.mask_a)\n        ma = Masked(np_ma)\n        assert type(ma) is Masked\n        assert type(ma.unmasked) is self._data_cls\n        assert type(ma.mask) is np.ndarray\n        assert_array_equal(ma.unmasked, self.a)\n        assert_array_equal(ma.mask, self.mask_a) \n"], "sample_1143": ["   def test_issue_9558():\n       assert (1/pi).evalf(5) == Float('0.31831', 5)\n       assert (1/pi.n(5)).evalf() == Float('0.31831', 5)\n", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_29": ["    def test_latex_units(self, write, cosmo, tmp_path):\n        \"\"\"Test that units are correctly serialized.\"\"\"\n        fp = tmp_path / \"test_latex_units.tex\"\n        write(fp, format=\"latex\")\n        tbl = QTable.read(fp)\n        for name, unit in cosmo.parameters.items():\n            if unit is not None and not unit.is_unity():\n                assert tbl[name].unit == unit\n\n", "    def test_latex_units(self, write, tmp_path):\n        fp = tmp_path / \"test_latex_units.tex\"\n        write(fp, format=\"latex\")\n        tbl = QTable.read(fp)\n        for col in tbl.colnames[2:]:\n            assert tbl[col].unit == self.cosmo.get_param(col).unit\n\n", "    def test_latex_unit_conversion(self, cosmo, write, tmp_path):\n        \"\"\"Test that units are properly converted to the parameter units.\"\"\"\n        fp = tmp_path / \"test_latex_unit_conversion.tex\"\n        write(fp, format=\"latex\")\n        tbl = QTable.read(fp)\n        for name in cosmo.__parameters__:\n            param = getattr(type(cosmo), name)\n            if isinstance(param, Parameter) and param.unit is not None:\n                assert tbl[name].unit == param.unit\n", "    def test_write_latex_units(self, cosmo, tmp_path):\n        \"\"\"Test that units are correctly applied in the LaTeX table.\"\"\"\n        fp = tmp_path / \"test_write_latex_units.tex\"\n        cosmo.write(fp, format=\"latex\", latex_names=True)\n        tbl = QTable.read(fp)\n        for name in cosmo.__parameters__:\n            param = getattr(type(cosmo), name)\n            if isinstance(param, Parameter) and param.unit is not None:\n                assert tbl[name].unit == param.unit\n", "def test_latex_parameter_units(self, write, tmp_path):\n    \"\"\"Test parameter units are correctly included in LaTeX table\"\"\"\n    fp = tmp_path / \"test_latex_parameter_units.tex\"\n    write(fp, format=\"latex\")\n    tbl = QTable.read(fp)\n    for col_name in tbl.colnames[2:]:\n        if col_name in _FORMAT_TABLE.values():\n            param = getattr(self.cosmo, col_name.replace(_FORMAT_TABLE.get(col_name, col_name), \"\"), None)\n            if param is not None:\n                assert tbl[col_name].unit == param.unit\n\n"], "sample_614": ["    def test_diff_attrs_repr_with_string(self) -> None:\n        attrs_a = {\"attr\": \"hello\"}\n\n        attrs_b = {\"attr\": \"world\"}\n        expected = dedent(\n            \"\"\"\\\n            Differing attributes:\n            L   attr: hello\n            R   attr: world\n            \"\"\"\n        ).strip()\n        actual = formatting.diff_attrs_repr(attrs_a, attrs_b, \"equals\")\n        assert expected == actual\n", "def test_short_numpy_repr_empty_array() -> None:\n    actual = formatting.short_numpy_repr(np.array([]))\n    expected = \"[]\"\n    assert actual == expected\n", "    def test_diff_dataset_repr_identical_with_different_dim_order(self) -> None:\n        ds1 = xr.Dataset(\n            data_vars={\"a\": ((\"x\", \"y\"), np.arange(6).reshape(2, 3))},\n            coords={\"x\": [0, 1], \"y\": [0, 1, 2]},\n        )\n        ds2 = xr.Dataset(\n            data_vars={\"a\": ((\"y\", \"x\"), np.arange(6).reshape(3, 2))},\n            coords={\"y\": [0, 1, 2], \"x\": [0, 1]},\n        )\n        expected = dedent(\n            \"\"\"\\\n        Left and right Dataset objects are identical\n        \"\"\"\n        )\n        actual = formatting.diff_dataset_repr(ds1, ds2, \"identical\")\n        assert actual == expected\n", "def test_format_timestamp_out_of_bounds_with_timezone() -> None:\n    from datetime import datetime, timezone\n\n    date = datetime(1300, 12, 1, tzinfo=timezone.utc)\n    expected = \"1300-12-01 00:00:00+00:00\"\n    result = formatting.format_timestamp(date)\n    assert result == expected\n\n    date = datetime(2300, 12, 1, tzinfo=timezone.utc)\n    expected = \"2300-12-01 00:00:00+00:00\"\n    result = formatting.format_timestamp(date)\n    assert result == expected\n", "    def test_diff_data_vars_repr_with_different_coords(self) -> None:\n        ds_a = xr.Dataset(\n            data_vars={\"var1\": (\"x\", np.arange(3))},\n            coords={\"x\": np.array([\"a\", \"b\", \"c\"])},\n        )\n\n        ds_b = xr.Dataset(\n            data_vars={\"var1\": (\"x\", np.arange(3))},\n            coords={\"x\": np.array([\"a\", \"b\", \"d\"])},\n        )\n        expected = dedent(\n            \"\"\"\\\n        Left and right Dataset objects are not identical\n        Differing coordinates:\n        L   x        (x) <U1 'a' 'b' 'c'\n        R   x        (x) <U1 'a' 'b' 'd'\n        Data variables only on the left and right object:\n            var1     (x) int64 0 1 2\n        \"\"\"\n        )\n        actual = formatting.diff_dataset_repr(ds_a, ds_b, \"identical\")\n        assert actual == expected\n\n"], "sample_558": ["compilation error", "compilation error", "    def test_anchored_sizebar_units(self):\n        fig, ax = plt.subplots()\n        ax.imshow(np.random.rand(10, 10))\n        asb = AnchoredSizeBar(ax.transData, 0.2, r\"0.2 units\", loc='lower right',\n                              pad=0.3, borderpad=0.4, sep=4, fill_bar=True,\n                              frameon=False, label_top=True,\n                              prop={'size': 20}, size_vertical=0.05,\n                              units=\"inches\")\n        ax.add_artist(asb)\n        fig.canvas.draw()\n        assert asb.size_inches == 0.2\n", "compilation error", "compilation error"], "sample_892": ["    def test_adaboost_warm_start_with_estimator_weights(self):\n        # Check that warm_start works correctly when estimator_weights is given.\n\n        iris = datasets.load_iris()\n        X, y = iris.data, iris.target\n        rng = np.random.RandomState(42)\n\n        # First, train a model with warm_start=False\n        clf1 = AdaBoostClassifier(n_estimators=5, random_state=rng)\n        clf1.fit(X, y)\n\n        # Store the estimator weights for later comparison\n        initial_estimator_weights = clf1.estimator_weights_.copy()\n\n        # Now train a model with warm_start=True and the same estimator_weights\n        clf2 = AdaBoostClassifier(\n            n_estimators=10, estimator_weights=initial_estimator_weights, warm_start=True, random_state=rng\n        )\n        clf2.fit(X, y)\n\n        # Assert that the estimator weights match for the initial estimators\n        assert_array_equal(clf1.estimator_weights_, clf2.estimator_weights_[:5])\n\n        # Assert that the new estimators have been added\n        assert len(clf2.estimator_weights_) == 10\n\n", "    def test_estimator_params_deprecation(self):\n        model = AdaBoostClassifier()\n        with pytest.warns(FutureWarning, match=\"`estimator_params` was deprecated\"):\n            model._get_params()\n", "    def test_estimator_validation():\n        # Test that estimator validation works\n        with pytest.raises(TypeError, match=\"`estimator` must be a classifier\"):\n            AdaBoostClassifier(estimator=LinearRegression())\n        with pytest.raises(TypeError, match=\"`estimator` must be a regressor\"):\n            AdaBoostRegressor(estimator=DecisionTreeClassifier())\n", "def test_adaboost_with_multiclass_dummy_estimator():\n    # Check that AdaBoostClassifier works with a multi-class DummyEstimator\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([0, 1, 2])\n    clf = AdaBoostClassifier(estimator=DummyClassifier(strategy=\"uniform\", random_state=0))\n    clf.fit(X, y)\n    assert clf.predict(X).shape == (3,)\n\n\n", "    def test_adaboost_estimator_init_deprecation(self):\n        estimator = DecisionTreeClassifier()\n        warn_msg = (\n            \"The `estimator` argument was changed to a required parameter \"\n            \"in version 1.2. It will always be needed in future versions.\"\n        )\n        with pytest.warns(FutureWarning, match=warn_msg):\n            AdaBoostClassifier(estimator=estimator)\n"], "sample_787": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_378": ["    def test_empty_field_updates(self):\n        for i in range(10):\n            Note.objects.create(note='test-%s' % i, misc='test')\n        notes = list(Note.objects.all())\n        for note in notes:\n            note.misc = ''\n        rows_updated = Note.objects.bulk_update(notes, ['misc'])\n        self.assertEqual(rows_updated, 10)\n        self.assertCountEqual(Note.objects.filter(misc=''), notes)\n", "    def test_related_object_creation(self):\n        related_objects = [RelatedObject(single=SingleObject()) for _ in range(5)]\n        RelatedObject.objects.bulk_create(related_objects)\n        self.assertEqual(\n            RelatedObject.objects.filter(single__isnull=False).count(), 5\n        )\n\n", "    def test_related_objects_with_null_fk(self):\n        parent = RelatedObject.objects.create(single_id=None)\n        RelatedObject.objects.bulk_update([parent], fields=['single_id'])\n        parent.refresh_from_db()\n        self.assertIsNone(parent.single_id)\n", "    def test_non_existent_related_object(self):\n        parent = RelatedObject.objects.create()\n        non_existent_pk = 9999\n        msg = (\n            f\"bulk_update() prohibited to prevent data loss due to \"\n            f\"non-existent related object with primary key '{non_existent_pk}' \"\n            \"in field 'single'.\"\n        )\n        with self.assertRaisesMessage(ValueError, msg):\n            parent.single_id = non_existent_pk\n            RelatedObject.objects.bulk_update([parent], fields=['single_id'])\n", "    def test_empty_batch(self):\n        notes = []\n        rows_updated = Note.objects.bulk_update(notes, ['note'])\n        self.assertEqual(rows_updated, 0)\n"], "sample_834": ["    def test_transform(self):\n        \"\"\"Test that transform raises ValueError for incompatible input shape.\"\"\"\n        X = iris_data\n        nca = NeighborhoodComponentsAnalysis(random_state=0).fit(X, iris_target)\n        X_transformed = nca.transform(X)\n\n        # Test with incompatible input shape\n        with pytest.raises(ValueError, match=\"Shape of input is incompatible with training data\"):\n            nca.transform(np.ones((X.shape[0] + 1, X.shape[1])))\n", "    def test_n_components_greater_than_features(self):\n        X = np.arange(12).reshape(4, 3)\n        y = [1, 1, 2, 2]\n        n_components = 4\n\n        nca = NeighborhoodComponentsAnalysis(n_components=n_components)\n        assert_raises(ValueError, nca.fit, X, y)\n", "def test_transform_shape(n_samples, n_features, n_components):\n    rng = np.random.RandomState(42)\n    X = rng.randn(n_samples, n_features)\n    y = rng.randint(0, 3, size=n_samples)\n\n    nca = NeighborhoodComponentsAnalysis(n_components=n_components)\n    nca.fit(X, y)\n\n    X_transformed = nca.transform(X)\n    assert X_transformed.shape == (n_samples, n_components)\n", "    def test_n_components_equal_n_features(self):\n        X = iris_data\n        y = iris_target\n        n_components = X.shape[1]\n        nca = NeighborhoodComponentsAnalysis(n_components=n_components)\n        nca.fit(X, y)\n        assert nca.components_.shape == (X.shape[1], X.shape[1])\n", "    def test_n_samples_less_than_n_features(self):\n        n_samples = 4\n        n_features = 5\n        X = np.random.randn(n_samples, n_features)\n        y = np.random.randint(0, 2, size=n_samples)\n        \n        nca = NeighborhoodComponentsAnalysis(n_components=2)\n        with pytest.raises(ValueError) as excinfo:\n            nca.fit(X, y)\n        assert \"Number of samples\" in str(excinfo.value)\n\n"], "sample_866": ["def test_affinity_propagation_sparse_input():\n    # Test AffinityPropagation with sparse input\n    X_sparse = csr_matrix(X)\n\n    af = AffinityPropagation(affinity=\"precomputed\").fit(X_sparse)\n    labels_sparse = af.labels_\n\n    af = AffinityPropagation(affinity=\"euclidean\").fit(X_sparse)\n    labels = af.labels_\n    assert_array_equal(labels, labels_sparse)\n", "    def test_affinity_propagation_sparse_input():\n        # Test affinity propagation with sparse input.\n        X_sparse = csr_matrix(X)\n        af = AffinityPropagation(affinity='euclidean')\n        labels_sparse = af.fit_predict(X_sparse)\n        labels_dense = af.fit_predict(X)\n        assert_array_equal(labels_sparse, labels_dense)\n", "    def test_affinity_propagation_precomputed_sparse():\n        # Test Affinity Propagation with sparse precomputed affinity matrix.\n        S = csr_matrix(np.dot(X, X.T))\n        af = AffinityPropagation(affinity=\"precomputed\").fit(S)\n        labels = af.labels_\n        # check if labels are valid\n        assert len(np.unique(labels)) > 0\n\n", "def test_affinity_propagation_sparse_input():\n    # Test AffinityPropagation with sparse input\n    from scipy.sparse import csr_matrix\n    X_sparse = csr_matrix(X)\n\n    af = AffinityPropagation(affinity=\"euclidean\")\n    labels_sparse = af.fit_predict(X_sparse)\n    labels_dense = af.fit_predict(X)\n    assert_array_equal(labels_sparse, labels_dense)\n\n\n", "def test_affinity_propagation_precomputed_affinity_matrix():\n    # Test when affinity matrix is precomputed.\n    S = -euclidean_distances(X, squared=True)\n    af = AffinityPropagation(affinity='precomputed').fit(S)\n    assert_array_equal(af.affinity_matrix_, S)\n"], "sample_223": ["    def test_ticket_24605_complex(self):\n        \"\"\"\n        More complex subqueries with joins should also quote table names.\n        \"\"\"\n        i1 = Individual.objects.create(alive=True)\n        RelatedIndividual.objects.create(related=i1)\n        i2 = Individual.objects.create(alive=False)\n        RelatedIndividual.objects.create(related=i2)\n        i3 = Individual.objects.create(alive=True)\n        i4 = Individual.objects.create(alive=False)\n\n        qs = Individual.objects.filter(\n            Q(alive=True) & Q(related_individual__related__alive=False))\n        self.assertSequenceEqual(qs, [i1])\n", "    def test_ticket_24605_exclude_inner(self):\n        \"\"\"\n        Test exclusion with inner subquery and related fields.\n        \"\"\"\n        i1 = Individual.objects.create(alive=True)\n        RelatedIndividual.objects.create(related=i1)\n        i2 = Individual.objects.create(alive=False)\n        RelatedIndividual.objects.create(related=i2)\n        i3 = Individual.objects.create(alive=True)\n        i4 = Individual.objects.create(alive=False)\n\n        self.assertSequenceEqual(\n            Individual.objects.exclude(\n                Q(alive=False) & Q(related_individual__isnull=True)\n            ).order_by('pk'),\n            [i1, i2, i3, i4],\n        )\n", "    def test_ticket_24605_multiple(self):\n        i1 = Individual.objects.create(alive=True)\n        RelatedIndividual.objects.create(related=i1)\n        i2 = Individual.objects.create(alive=False)\n        RelatedIndividual.objects.create(related=i2)\n        i3 = Individual.objects.create(alive=True)\n        i4 = Individual.objects.create(alive=False)\n\n        self.assertQuerysetEqual(\n            Individual.objects.filter(\n                Q(alive=True) | Q(related_individual__isnull=True)\n            ).order_by('pk'),\n            [i1, i3, i4],\n        ) \n\n", "    def test_ticket_22332(self):\n        \"\"\"\n        Test that WHERE and JOIN clauses are combined correctly when applying\n        a reverse foreign key lookup using '__in'.\n        \"\"\"\n        s1 = Sensor.objects.create()\n        s2 = Sensor.objects.create()\n        d1 = DataPoint.objects.create(sensor=s1)\n        DataPoint.objects.create(sensor=s1)\n        DataPoint.objects.create(sensor=s2)\n        qs = Sensor.objects.filter(datapoint__id__in=[d1.pk])\n        self.assertSequenceEqual(qs, [s1])\n", "    def test_ticket_24605_reverse(self):\n        \"\"\"\n        Subquery table names should be quoted, even in reverse relations.\n        \"\"\"\n        i1 = Individual.objects.create(alive=True)\n        i2 = Individual.objects.create(alive=False)\n        RelatedIndividual.objects.create(related=i1)\n        RelatedIndividual.objects.create(related=i2)\n        i3 = Individual.objects.create(alive=True)\n        i4 = Individual.objects.create(alive=False)\n\n        self.assertSequenceEqual(RelatedIndividual.objects.filter(related__alive=False, related__related_individual__isnull=True), [])\n        self.assertSequenceEqual(\n            RelatedIndividual.objects.exclude(related__alive=False, related__related_individual__isnull=True).order_by('related_id'),\n            [\n                RelatedIndividual.objects.get(related=i1),\n                RelatedIndividual.objects.get(related=i2),\n            ],\n        )\n\n"], "sample_964": ["def test_python_python_use_unqualified_type_names_no_module(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert '<span class=\"n\"><span class=\"pre\">Name</span></span>' in content\n    assert '<span class=\"n\"><span class=\"pre\">Age</span></span>' in content\n    assert '<p><strong>name</strong> (<span class=\"pre\">Name</span>) \u2013 blah blah</p>' in content\n    assert '<p><strong>age</strong> (<span class=\"pre\">Age</span>) \u2013 blah blah</p>' in content\n\n", "def test_info_field_list_no_type(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   :param name: blah blah\\n\")\n    doctree = restructuredtext.parse(app, text)\n\n    assert_node(doctree, (addnodes.index,\n                          [desc, (desc_signature,\n                                  [desc_content, nodes.field_list, nodes.field])]))\n\tassert_node(doctree[1][1][0][0], ([nodes.field_name, \"Parameters\"],\n                [nodes.field_body, nodes.paragraph]))\n\n    # :param name:\n    assert_node(doctree[1][1][0][0][1][0],\n                ([addnodes.literal_strong, \"name\"],\n                 \" -- \",\n                 \"blah blah\")) \n", "def test_py_deprecated(app):\n    text = (\n        \".. py:function:: foo()\\n\"\n        \"   :deprecated:\\n\"\n        \"       Use bar() instead.\\n\"\n    )\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc, ))\n    assert_node(doctree[1], desc, \n                desc_signature=[desc_name,\n                                desc_content,\n                                [\n                                  desc_addname, \n                                  desc_name, ]\n\n                                ],\n                desc_content=[nodes.paragraph,])\n    assert \"Use `bar()` instead.\" in doctree[1][1][0].astext()\n\n", "compilation error", "def test_param_type_unqualified_name(app, status, warning):\n    text = \".. py:function:: my_function\\n\" \\\n           \"   :param foo: blah\\n\" \\\n           \"   :type foo: MyType\\n\"\n    restructuredtext.parse(app, text)\n    domain = app.env.get_domain('py')\n    assert 'my_function' in domain.objects\n    assert domain.objects['my_function'][3] == [('param', 'foo (MyType) -- blah')]\n"], "sample_620": ["    def test_concat_merge_coords_with_same_name_different_dims(self) -> None:\n        \n        ds1 = Dataset({\"foo\": ((\"x\",), [1, 2, 3])}, coords={\"bar\": ((\"x\",), [10, 20, 30])})\n        ds2 = Dataset({\"foo\": ((\"y\",), [4, 5, 6])}, coords={\"bar\": ((\"y\",), [40, 50, 60])})\n\n        expected = Dataset(\n            {\"foo\": ((\"x\", \"y\"), [[1, 2, 3], [4, 5, 6]]),\n             \"bar\": ((\"x\", \"y\"), [[10, 20, 30], [40, 50, 60]])}\n        )\n\n        actual = concat([ds1, ds2], dim=\"y\")\n        assert_identical(actual, expected)\n", "compilation error", "    def test_concat_scalar_dims(self) -> None:\n        ds1 = Dataset({\"foo\": 1}, {\"x\": 0})\n        ds2 = Dataset({\"foo\": 2}, {\"x\": 1})\n        expected = Dataset({\"foo\": ((\"x\",), [1, 2])}, {\"x\": [0, 1]})\n        actual = concat([ds1, ds2], dim=\"x\")\n        assert_identical(actual, expected)\n", "compilation error", "    def test_concat_indexes_and_coords():\n        ds1 = Dataset(\n            {\"foo\": ((\"x\", \"y\"), [1, 2, 3, 4])},\n            coords={\"x\": IndexVariable(\"x\", [0, 1]), \"y\": [0, 1]},\n        )\n\n        ds2 = Dataset(\n            {\"foo\": ((\"x\", \"y\"), [5, 6, 7, 8])},\n            coords={\"x\": IndexVariable(\"x\", [0, 1]), \"y\": [0, 1]},\n        )\n\n        expected = Dataset(\n            {\"foo\": ((\"x\", \"y\"), [1, 2, 3, 4, 5, 6, 7, 8])},\n            coords={\"x\": IndexVariable(\"x\", [0, 1, 0, 1]), \"y\": [0, 1, 0, 1]},\n        )\n\n        actual = concat([ds1, ds2], dim=\"x\")\n        assert_identical(actual, expected)\n"], "sample_759": ["compilation error", "compilation error", "def test_ordinal_encoder_dtypes():\n    enc = OrdinalEncoder()\n    X = np.array([[1, 2], [3, 4]], dtype='int64')\n    exp = np.array([[0, 1], [1, 0]])\n    assert_array_equal(enc.fit_transform(X), exp)\n    assert enc.categories_[0].dtype == np.int64\n\n    X = np.array([[1, 'a'], [3, 'b']], dtype='object')\n    exp = np.array([[0, 1], [1, 0]])\n    assert_array_equal(enc.fit_transform(X), exp)\n    assert enc.categories_[0].dtype == np.object_\n\n", "compilation error", "def test_one_hot_encoder_invalid_handle_unknown():\n    X = np.array([[0, 2, 1], [1, 0, 3], [1, 0, 2]]).reshape(-1,3)\n    X2 = np.array([[4, 1, 1]])\n\n    # Test for invalid values of handle_unknown\n    oh = OneHotEncoder(handle_unknown='invalid')\n    msg = (r\"Invalid value for handle_unknown: 'invalid'. \"\n           r\"Possible values are 'error' or 'ignore'.\")\n    with pytest.raises(ValueError, match=msg):\n        oh.fit(X)\n"], "sample_751": ["    def test_decision_function_shape():\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        y = np.array([0, 1, 0])\n\n        clf = AdaBoostClassifier(DecisionTreeClassifier(), algorithm=\"SAMME\")\n        clf.fit(X, y)\n\n        decision_function = clf.decision_function(X)\n\n        assert decision_function.shape == (len(X), clf.n_classes_)\n", "    def test_warm_start():\n        # Test warm starting with AdaBoost.\n        X, y = datasets.make_classification(n_samples=100,\n                                            n_features=10,\n                                            random_state=0)\n\n        clf1 = AdaBoostClassifier(n_estimators=50).fit(X, y)\n        clf2 = AdaBoostClassifier(n_estimators=100, warm_start=True)\n        clf2.fit(X, y)\n\n        # The second classifier should have twice the number of estimators compared to the first.\n        assert len(clf1.estimators_) == 50\n        assert len(clf2.estimators_) == 100\n\n        # Check warm_start works with continuation\n        clf2.fit(X, y)\n\n        # Now the second classifier should have an additional 50 estimators\n        assert len(clf2.estimators_) == 150\n\n        clf3 = AdaBoostRegressor(n_estimators=50).fit(X, y)\n        clf4 = AdaBoostRegressor(n_estimators=100, warm_start=True)\n        clf4.fit(X, y)\n\n        # The second classifier should have twice the number of estimators compared to the first.\n        assert len(clf3.estimators_) == 50\n        assert len(clf4.estimators_) == 100\n\n        # Check warm_start works with continuation\n        clf4.fit(X, y)\n\n        # Now the second classifier should have an additional 50 estimators\n        assert len(clf4.estimators_) == 150\n", "    def test_warm_start():\n        # Check warm start functionality.\n        clf = AdaBoostClassifier(n_estimators=10)\n        clf.fit(X, y_class)\n        initial_score = clf.score(X, y_class)\n        clf.set_params(warm_start=True).fit(X, y_class)\n        second_score = clf.score(X, y_class)\n        assert second_score >= initial_score\n", "    def test_zero_weight_samples():\n        # Test AdaBoostRegressor with sample_weights containing zeros.\n\n        rng = np.random.RandomState(0)\n        n_samples = 10\n        n_features = 5\n        X = rng.randn(n_samples, n_features)\n        y = rng.randn(n_samples)\n        sample_weight = np.ones(n_samples)\n        sample_weight[2] = 0\n\n        reg = AdaBoostRegressor(random_state=0)\n        reg.fit(X, y, sample_weight=sample_weight)\n\n        # Check if the regressor can predict without crashing.\n        reg.predict(X)\n", "    def test_zero_weight():\n        # Test if AdaBoostRegressor handles zero sample weights correctly.\n        # Issue #18577\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        y = np.array([1, 2, 3])\n        sample_weight = np.array([1, 0, 1])\n\n        reg = AdaBoostRegressor(random_state=0).fit(X, y,\n                                                 sample_weight=sample_weight)\n        predictions = reg.predict(X)\n        assert len(predictions) == 3\n"], "sample_243": ["    def test_empty_query(self):\n        query = Query(Item)\n        where = query.build_where(Q() | Q())\n        self.assertEqual(where.connector, OR)\n        self.assertEqual(len(where.children), 0)\n", "    def test_values(self):\n        query = Query(Author)\n        query.set_values(['name', 'num'])\n        self.assertEqual(query.values_select, ('name', 'num'))\n\n", "    def test_join_promotion(self):\n        query = Query(Item)\n        query.add_filter(('creator__num__gt', 2))\n        query.add_filter(('note__content__contains', 'x'))\n        query.promote_joins({'creator': 1, 'note': 2})\n        self.assertEqual(query.join_list, [\n            ('creator', 'INNER', 'author_set'),\n            ('note', 'INNER', 'notes_set'),\n        ]) \n", "    def test_promote_joins(self):\n        query = Query(Ranking)\n        query.add_filter(Q(author__num__gt=2))\n        query.add_filter(Q(item__name='foo'))\n        query.add_order_by('author__name')\n        joins = query.promote_joins({'author': 2, 'item': 1})\n        self.assertEqual(joins, {'author': {'inner': 2}, 'item': {'inner': 1}})\n", "    def test_complex_or_query(self):\n        query = Query(Item)\n        where = query.build_where(\n            Q(name__startswith='foo')\n            | Q(created__lt=datetime(2017, 1, 1))\n            | Q(modified__gt=datetime(2018, 1, 1))\n        )\n        self.assertEqual(where.connector, OR)\n\n        startwith_lookup = where.children[0]\n        self.assertIsInstance(startwith_lookup, Exact)\n        self.assertEqual(startwith_lookup.lhs.target, Item._meta.get_field('name'))\n        self.assertEqual(startwith_lookup.rhs, 'foo')\n\n        lt_lookup = where.children[1]\n        self.assertIsInstance(lt_lookup, LessThan)\n        self.assertEqual(lt_lookup.rhs, datetime(2017, 1, 1))\n        self.assertEqual(lt_lookup.lhs.target, Item._meta.get_field('created'))\n\n        gt_lookup = where.children[2]\n        self.assertIsInstance(gt_lookup, GreaterThan)\n        self.assertEqual(gt_lookup.rhs, datetime(2018, 1, 1))\n        self.assertEqual(gt_lookup.lhs.target, Item._meta.get_field('modified'))\n"], "sample_328": ["    def test_jsonfield_nullable(self):\n        json_fields = [\n            JSONFieldNullable.objects.create(data='{\"key\": \"value\"}')\n            for _ in range(10)\n        ]\n        for field in json_fields:\n            field.data = '{\"updated\": \"true\"}'\n        JSONFieldNullable.objects.bulk_update(json_fields, ['data'])\n        self.assertCountEqual(JSONFieldNullable.objects.filter(data='{\"updated\": \"true\"}'), json_fields)\n\n", "    def test_jsonfield(self):\n        data = [{'key': 'value'}]\n        json_fields = [JSONFieldNullable.objects.create(json_data=data) for _ in range(10)]\n        for json_field in json_fields:\n            json_field.json_data = {'new_key': 'new_value'}\n        JSONFieldNullable.objects.bulk_update(json_fields, ['json_data'])\n        self.assertCountEqual(JSONFieldNullable.objects.filter(json_data={'new_key': 'new_value'}), json_fields)\n", "    def test_jsonfieldnullable(self):\n        json_field_nullable = JSONFieldNullable.objects.create(data=None)\n        json_field_nullable.data = {'key': 'value'}\n        JSONFieldNullable.objects.bulk_update([json_field_nullable], ['data'])\n        json_field_nullable.refresh_from_db()\n        self.assertEqual(json_field_nullable.data, {'key': 'value'})\n", "    def test_jsonfield(self):\n        data = [\n            {'key': str(i)}\n            for i in range(10)\n        ]\n        json_fields = JSONFieldNullable.objects.bulk_create([\n            JSONFieldNullable(data=data_entry)\n            for data_entry in data\n        ])\n        for obj, data_entry in zip(json_fields, data):\n            obj.data['new_key'] = 'new_value'\n        JSONFieldNullable.objects.bulk_update(json_fields, ['data'])\n        for obj, data_entry in zip(json_fields, data):\n            obj.refresh_from_db()\n            self.assertEqual(obj.data, {**data_entry, 'new_key': 'new_value'})\n", "    def test_jsonfield_nullable(self):\n        json_data = [\n            {'key1': i, 'key2': i**2}\n            for i in range(10)\n        ]\n        models = [JSONFieldNullable.objects.create(data=data) for data in json_data]\n\n        updated_data = [\n            {'key1': i + 10, 'key2': (i + 10)**2}\n            for i in range(10)\n        ]\n\n        for model, data in zip(models, updated_data):\n            model.data = data\n        JSONFieldNullable.objects.bulk_update(models, ['data'])\n        self.assertCountEqual(\n            JSONFieldNullable.objects.values_list('data', flat=True),\n            updated_data\n        )\n"], "sample_670": ["def test_invalid_idents(expr: str, column: int, message: str) -> None:\n    with pytest.raises(ParseError) as excinfo:\n        evaluate(expr, lambda ident: True)\n    assert excinfo.value.column == column\n    assert excinfo.value.message == message\n", "def test_unclosed_parentheses(expr: str, column: int, message: str) -> None:\n    with pytest.raises(ParseError) as excinfo:\n        evaluate(expr, lambda ident: True)\n    assert excinfo.value.column == column\n    assert excinfo.value.message == message \n", "    def test_nested_parentheses():\n        matcher = {\"true\": True, \"false\": False}.__getitem__\n        assert evaluate(\"((true and false) or (true))\", matcher) is True\n        assert evaluate(\"(((true and false) or true))\", matcher) is True\n        assert evaluate(\"((true and (false or true)))\", matcher) is True\n        assert evaluate(\"(((true) and (false or (true))))\", matcher) is True\n", "    def test_spacing(expr: str):\n        matcher = {\"a\": True, \"b\": True, \"c\": False}.__getitem__\n        assert evaluate(expr, matcher)\n", "def test_nested_parentheses(expr: str, expected: bool) -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(expr, matcher) is expected\n"], "sample_475": ["        def action2(modeladmin, request, queryset):\n            pass\n", "    def test_actions_valid_permission_method(self):\n        @admin.action(permissions=[\"custom\"])\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (custom_permission_action,)\n\n                return True\n\n        self.assertIsValid(BandAdmin, Band)\n", "    def test_actions_valid(self):\n        @admin.action\n            pass\n        \n        @admin.action\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (action1, action2)\n\n        self.assertIsValid(BandAdmin, Band)\n", "    def test_actions_with_conditional_permissions(self):\n        @admin.action(permissions=[\"add_song\"])\n            pass\n\n        @admin.action(permissions=[\"delete_song\"])\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (add_song, delete_song)\n\n        self.assertIsValid(BandAdmin, Band)\n", "        def action2(modeladmin, request, queryset):\n            pass\n"], "sample_103": ["    def test_aggregate_with_null_values(self):\n        Book.objects.create(isbn=\"abcde10\", name=\"None\", pages=10, rating=None,\n                           price=9999.98, contact=self.a1, publisher=self.p1, pubdate=timezone.now())\n\n        avg_rating = Book.objects.aggregate(Avg('rating'))\n        self.assertEqual(avg_rating['rating__avg'], Approximate(4.0, places=1))\n\n", "    def test_annotation_ordering(self):\n        qs = Book.objects.annotate(\n            double_pages=F('pages') * 2,\n        ).order_by('double_pages')\n        self.assertEqual(list(qs.values_list('title', 'double_pages'))[:3],\n                         [('The Definitive Guide to Django: Web Development Done Right', 528),\n             ('Artificial Intelligence: A Modern Approach', 488),\n             ('Python Tricks: A Buffet of Awesome Python Features', 406)])\n", "    def test_annotate_with_aggregate(self):\n        qs = Book.objects.annotate(\n            average_rating=Avg('rating'),\n            count_of_books=Count('*')\n        ).filter(average_rating__gt=4).order_by('publisher__name')\n        self.assertEqual(\n            list(qs), [\n                {\n                    \"pk\": self.b4.pk,\n                    \"isbn\": \"159059725\",\n                    \"name\": \"The Definitive Guide to Django: Web Development Done Right\",\n                    \"pages\": 447,\n                    \"rating\": 5.0,\n                    \"price\": Decimal(\"39.99\"),\n                    \"contact\": self.a1.pk,\n                    \"publisher\": self.p1.pk,\n                    \"pubdate\": self.b4.pubdate,\n                    \"average_rating\": Decimal(\"4.5\"),\n                    \"count_of_books\": 1\n                }\n            ]\n        )\n", "    def test_aggregate_with_empty_results(self):\n        with self.assertRaises(EmptyResultSet):\n            Book.objects.filter(pk=1000).aggregate(Avg('price'))\n", "    def test_annotation_with_subquery_and_aggregate(self):\n        qs = Book.objects.annotate(\n            latest_pubdate=Subquery(Book.objects.filter(\n                pk=OuterRef('pk')\n            ).values('pubdate').order_by('-pubdate')[:1]),\n        ).values('publisher__name', 'latest_pubdate').annotate(\n            d=Avg('latest_pubdate')\n        )\n        self.assertEqual(\n            list(qs.values_list('d', flat=True)),\n            [datetime.date(2008, 3, 3), datetime.date(2007, 12, 6), datetime.date(2008, 3, 3), datetime.date(2008, 1, 1),]\n        )\n\n"], "sample_538": ["    def test_transform_path_clipping():\n        path = Path([(0, 0), (10, 0), (10, 10), (0, 10), (0, 0)])\n        trans = mtransforms.Affine2D().translate(-5, -5)\n        \n        # Clip the path using the transform\n        clipped_path = trans.transform_path(path).clip(0, 0, 5, 5)\n        \n        # Verify the clipped path vertices\n        assert_allclose(clipped_path.vertices, [(0, 0), (5, 0), (5, 5), (0, 5), (0, 0)])\n", "compilation error", "    def test_transform_non_affine0(self):\n        # test that the affine part of stack1 & stack2 are equivalent\n        # (i.e. the optimization is working)\n        expected_result = (self.ta1 + (self.ta2 + self.ta3)).get_matrix()\n        result = (self.tn1 + self.stack2).get_matrix()\n        assert_array_equal(expected_result, result)\n", "    def test_transform_path_with_clipping():\n        # Create a path that goes outside the axes limits\n        points = [(0, 0), (2, 0), (2, 2), (0, 2), (0, 0)]\n        path = Path(points, closed=True)\n\n        fig, ax = plt.subplots()\n        ax.set_xlim(0, 1)\n        ax.set_ylim(0, 1)\n\n        # Create a Transform to clip the path to the axes limits\n        trans = mtransforms.BlendedGenericTransform(\n            ax.transData, scale.IdentityTransform()\n        )\n\n        # Apply the transform to the path\n        clipped_path = mtransforms.TransformedPath(path, trans)\n        \n\n        # Assert that the clipped path is within the axes limits\n        assert np.all(clipped_path.get_fully_transformed_path().vertices[:, 0] <= 1)\n        assert np.all(clipped_path.get_fully_transformed_path().vertices[:, 1] <= 1)\n        assert np.all(clipped_path.get_fully_transformed_path().vertices[:, 0] >= 0)\n        assert np.all(clipped_path.get_fully_transformed_path().vertices[:, 1] >= 0)\n\n", "compilation error"], "sample_1144": ["compilation error", "compilation error", "def test_requires_partial_unevaluated():\n    x, y = symbols('x y')\n    f = x*y\n    assert requires_partial(Derivative(f, x, evaluate=False)) is True\n    assert requires_partial(Derivative(f, y, evaluate=False)) is True\n", "compilation error", "compilation error"], "sample_449": ["    def test_close_connection_on_broken_pipe(self):\n            start_response(\"200 OK\", [])\n            yield b\"Hello, world!\"\n\n        rfile = BytesIO(b\"GET / HTTP/1.0\\r\\n\")\n        rfile.seek(0)\n\n        wfile = UnclosableBytesIO()\n\n            if mode == \"rb\":\n                return rfile\n            elif mode == \"wb\":\n                return wfile\n\n        request = Stub(makefile=makefile)\n        server = Stub(base_environ={}, get_app=lambda: test_app)\n\n        # Prevent logging from appearing in test output.\n        with captured_stderr() as stderr, self.assertLogs(\n            \"django.server\", \"INFO\"\n        ):\n            WSGIRequestHandler(request, \"192.168.0.2\", server)\n\n        self.assertIn(\n            \"- Broken pipe from ('192.168.0.2', 0)\", stderr.getvalue()\n        )\n", "    def test_content_length_not_set(self):\n            start_response(\"200 OK\", [])\n            yield b\"Hello World\"\n\n        rfile = BytesIO(b\"GET / HTTP/1.0\\r\\n\")\n        rfile.seek(0)\n\n        wfile = UnclosableBytesIO()\n\n            if mode == \"rb\":\n                return rfile\n            elif mode == \"wb\":\n                return wfile\n\n        request = Stub(makefile=makefile)\n        server = Stub(base_environ={}, get_app=lambda: test_app)\n\n        with self.assertLogs(\"django.server\", \"INFO\") as cm:\n            WSGIRequestHandler(request, \"192.168.0.2\", server)\n\n        wfile.seek(0)\n        lines = list(wfile.readlines())\n\n        self.assertIn(b\"Connection: close\\r\\n\", lines)\n\n", "    def test_close_connection_for_non_persistent_connections(self):\n        hello_world_body = b\"<!DOCTYPE html><html><body>Hello World</body></html>\"\n\n            \"\"\"A WSGI app that returns a hello world.\"\"\"\n            start_response(\"200 OK\", [])\n            return [hello_world_body]\n\n        rfile = BytesIO(b\"GET / HTTP/1.0\\r\\n\")\n        rfile.seek(0)\n\n        wfile = UnclosableBytesIO()\n\n            if mode == \"rb\":\n                return rfile\n            elif mode == \"wb\":\n                return wfile\n\n        request = Stub(makefile=makefile)\n        server = Stub(base_environ={}, get_app=lambda: test_app)\n\n        with captured_stderr() as stderr:\n            # Instantiating a handler runs the request as side effect.\n            WSGIRequestHandler(request, \"192.168.0.2\", server)\n\n        self.assertEqual(stderr.getvalue(), \"\")\n        wfile.seek(0)\n        lines = list(wfile.readlines())\n        self.assertIn(b\"Connection: close\\r\\n\", lines)\n\n", "    def test_close_connection(self):\n        rfile = BytesIO(b\"GET / HTTP/1.1\\r\\nConnection: close\\r\\n\\r\\n\")\n        rfile.seek(0)\n\n        wfile = UnclosableBytesIO()\n\n            if mode == \"rb\":\n                return rfile\n            elif mode == \"wb\":\n                return wfile\n\n        request = Stub(makefile=makefile)\n        server = Stub(base_environ={}, get_app=lambda: lambda *args: ([], None))\n\n        handler = WSGIRequestHandler(request, \"192.168.0.2\", server)\n        handler.handle_one_request()\n        self.assertTrue(handler.close_connection)\n\n", "    def test_close_connection_on_broken_pipe(self):\n            start_response(\"200 OK\", [])\n            yield b\"\"\n\n        rfile = BytesIO()\n        rfile.write(b\"GET / HTTP/1.1\\r\\n\")\n        rfile.write(b\"Host: example.com\\r\\n\")\n        rfile.write(b\"\\r\\n\")\n        rfile.seek(0)\n\n        wfile = UnclosableBytesIO()\n\n            if mode == \"rb\":\n                return rfile\n            elif mode == \"wb\":\n                return wfile\n\n        request = Stub(makefile=makefile)\n        server = Stub(base_environ={}, get_app=lambda: test_app)\n\n        with captured_stderr() as stderr:\n            # The client prematurely closes the connection.\n            WSGIRequestHandler(request, \"192.168.0.2\", server).handle()\n\n        self.assertEqual(\n            stderr.getvalue().splitlines()[-1], \"- Broken pipe from 192.168.0.2\"\n        )\n\n"], "sample_849": ["def test_leave_p_out_empty_trainset():\n    # LeavePOut expect at least 2 samples so no need to check\n    cv = LeavePOut(p=2)\n    X, y = [[1]], [0]  # 1 sample\n    with pytest.raises(\n            ValueError,\n            match='Cannot perform LeavePOut with n_samples=1'):\n        next(cv.split(X, y))\n", "def test_groupkfold_single_group():\n    X = np.ones(10)\n    y = np.ones(10)\n    groups = np.zeros(10, dtype=int)\n    kf = GroupKFold(n_splits=2)\n    with pytest.raises(ValueError, match=\"The groups array should have \"\n                                       \"at least 2 unique groups\"):\n        list(kf.split(X, y, groups))\n", "compilation error", "        def fit(self, X, y):\n            pass\n", "compilation error"], "sample_248": ["    def test_shell_with_bpython_not_installed(self, select):\n        select.return_value = ([], [], [])\n        with self.assertRaisesMessage(CommandError, \"Couldn't import bpython interface.\"):\n            call_command('shell', interface='bpython')\n", "    def test_shell_with_bpython_not_installed(self, select):\n        select.return_value = ([], [], [])\n        with self.assertRaisesMessage(CommandError, \"Couldn't import bpython interface.\"):\n            call_command('shell', interface='bpython')\n", "    def test_shell_with_bpython_not_installed(self, select):\n        select.return_value = ([], [], [])\n        with self.assertRaisesMessage(CommandError, \"Couldn't import bpython interface.\"):\n            call_command('shell', interface='bpython') \n", "    def test_shell_with_bpython_not_installed(self, select):\n        select.return_value = ([], [], [])\n        with self.assertRaisesMessage(CommandError, \"Couldn't import bpython interface.\"):\n            call_command('shell', interface='bpython')\n", "    def test_shell_with_bpython_not_installed(self, select):\n        select.return_value = ([], [], [])\n        with self.assertRaisesMessage(CommandError, \"Couldn't import bpython interface.\"):\n            call_command('shell', interface='bpython')\n"], "sample_1127": ["compilation error", "    def test_is_isomorphic(self):\n        G = PermutationGroup([Permutation(0, 1, 2), Permutation(0, 2, 1)])\n        H = PermutationGroup([Permutation(0, 1), Permutation(0, 2)])\n        assert G.is_isomorphic(H)\n        G = PermutationGroup([Permutation(0, 1, 2, 3), Permutation(0, 2)(1, 3)])\n        H = PermutationGroup([Permutation(0, 1), Permutation(0, 2)])\n        assert not G.is_isomorphic(H)\n", "def test_coset():\n    S = SymmetricGroup(4)\n    H = PermutationGroup(Permutation(1,2))\n    g = Permutation(0,1)\n    coset = Coset(g,H,S)\n    assert coset.representative == g\n    assert coset.subgroup == H\n    assert coset.parent == S\n", "    def test_is_primitive():\n        G = SymmetricGroup(4)\n        assert G.is_primitive == True\n        G = DihedralGroup(4)\n        assert G.is_primitive == False\n        G = AlternatingGroup(4)\n        assert G.is_primitive == True\n        G = PermutationGroup([Permutation(0, 1)(2, 3), Permutation(0, 2), Permutation(0, 3)])\n        assert G.is_primitive == False\n", "    def test_is_isomorphic():\n        G = PermutationGroup(Permutation(0, 1, 2), Permutation(0, 2, 3))\n        H = PermutationGroup(Permutation(0, 1, 3), Permutation(0, 3, 2))\n        assert G.is_isomorphic(H)\n"], "sample_117": ["    def test_custom_user_model(self):\n        class CustomUser(AbstractBaseUser):\n            username = models.CharField(max_length=150, unique=True)\n            email = models.EmailField(unique=True)\n            is_staff = models.BooleanField(default=False)\n            is_active = models.BooleanField(default=True)\n            objects = UserManager()\n\n            USERNAME_FIELD = 'username'\n            REQUIRED_FIELDS = ['email']\n\n        User = CustomUser\n        (user, username, email) = self.create_dummy_user()\n        data = {'password1': 'test123', 'password2': 'test123'}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertTrue(form.is_valid())\n        form.save(commit=False)\n        self.assertEqual(user.password, form.cleaned_data['password'])\n", "    def test_user_attribute_preservation(self):\n        user = User.objects.get(username='testclient')\n        original_first_name = user.first_name\n        original_last_name = user.last_name\n        data = {'password1': 'test123', 'password2': 'test123'}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertTrue(form.is_valid())\n        form.save()\n        user.refresh_from_db()\n        self.assertEqual(user.first_name, original_first_name)\n        self.assertEqual(user.last_name, original_last_name)\n\n", "    def test_custom_user_model(self):\n        class CustomUser(User):\n            class Meta:\n                managed = False\n\n        with mock.patch('django.contrib.auth.models.User', CustomUser):\n            user = CustomUser.objects.create_user('testuser', 'test@example.com', 'password')\n            data = {'password1': 'test123', 'password2': 'test123'}\n            form = AdminPasswordChangeForm(user, data)\n            self.assertTrue(form.is_valid())\n            form.save(commit=False)\n            self.assertTrue(user.check_password('test123'))\n", "    def test_short_password(self):\n        user = User.objects.get(username='testclient')\n        data = {'password1': '123', 'password2': '123'}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['password1'], [\n            _('Password must be at least 6 characters long.')\n        ])\n", "    def test_custom_user_model(self):\n        class CustomUser(AbstractUser):\n            pass\n\n        with override_settings(AUTH_USER_MODEL='auth_tests.CustomUser'):\n            user = CustomUser.objects.create_user('testuser', 'test@example.com', 'testpassword')\n            data = {'password1': 'newpassword', 'password2': 'newpassword'}\n            form = AdminPasswordChangeForm(user, data)\n            self.assertTrue(form.is_valid())\n            form.save()\n            self.assertTrue(user.check_password('newpassword'))\n"], "sample_207": ["    def test_key_transform_nested_lookups(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__baz__k__exact = 'b'\n            ),\n            [self.objs[7]],\n        )\n", "    def test_transform_lookup_with_subquery(self):\n        subquery = NullableJSONModel.objects.filter(value__c=14).values('pk')\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                pk__in=subquery,\n                value__d__contains=KeyTransform('a', subquery.first()),\n            ),\n            [self.objs[3]],\n        )\n", "    def test_custom_encoder_decoder(self):\n        class CustomEncoder(DjangoJSONEncoder):\n                if isinstance(obj, datetime.datetime):\n                    return obj.isoformat()\n                return super().default(obj)\n\n        class CustomDecoder(JSONDecoder):\n                return datetime.datetime.fromisoformat(s)\n\n        model = JSONModel.objects.create(value={'date': datetime.datetime(2023, 10, 26)})\n        obj = JSONModel.objects.get()\n        self.assertIsInstance(obj.value['date'], datetime.datetime)\n        self.assertEqual(obj.value['date'], datetime.datetime(2023, 10, 26))\n\n        model = JSONModel(value={'date': datetime.datetime(2023, 10, 27)})\n        model.save()\n        obj = JSONModel.objects.get(pk=model.pk)\n        self.assertIsInstance(obj.value['date'], datetime.datetime)\n        self.assertEqual(obj.value['date'], datetime.datetime(2023, 10, 27))\n", "    def test_key_transform_ordering(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__a__isnull=False).order_by(\n                'value__a', 'value__c', 'value__b'\n            ),\n            [self.objs[3], self.objs[4]],\n        )\n", "    def test_contained_by_with_none(self):\n        obj = NullableJSONModel.objects.create(value={'a': None, 'b': 2})\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__contained_by={'a': None, 'b': 2}),\n            [obj],\n        )\n"], "sample_880": ["compilation error", "compilation error", "    def test_ovr_decision_function():\n        # Test _ovr_decision_function output on a toy example\n        predictions = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n        confidences = np.array([[0.8, -0.2, -0.6], [-0.3, 0.9, -0.6], [-0.7, -0.4, 0.8]])\n        n_classes = 3\n\n        expected_result = np.array([[0.8, -0.2, 0.0], [-0.3, 0.9, 0.0], [-0.7, 0.0, 0.8]])\n        result = _ovr_decision_function(predictions, confidences, n_classes)\n        assert_allclose(result, expected_result)\n", "def test_ovr_decision_function():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([0, 1, 0])\n\n    # Mock classifier predictions and confidences\n    predictions = np.array([[0, 1], [1, 0], [0, 1]])\n    confidences = np.array([[0.8, 0.2], [0.3, 0.7], [0.6, 0.4]])\n\n    # Compute the OVR decision function\n    decision_function = _ovr_decision_function(predictions, confidences, 2)\n\n    # Expected decision function output\n    expected_decision_function = np.array(\n        [\n            [0.8 - 0.2, 0.3 - 0.7],\n            [0.2 - 0.8, 0.7 - 0.3],\n            [0.6 - 0.4, 0.4 - 0.6],\n        ]\n    )\n    assert_allclose(decision_function, expected_decision_function)\n", "    def test_ovr_decision_function():\n        # Test ovr_decision function with various inputs\n\n        # Binary classification\n        predictions = np.array([[1, 0], [0, 1]])\n        confidences = np.array([[0.8, -0.2], [-0.3, 0.6]])\n        n_classes = 2\n        expected_decision = np.array([[-0.2, 0.6], [0.8, -0.2]])\n        result = _ovr_decision_function(predictions, confidences, n_classes)\n        assert_allclose(result, expected_decision)\n\n        # Multiclass classification\n        predictions = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 1]])\n        confidences = np.array([[0.2, 0.6, -0.1], [-0.3, 0.8, 0.1], [0.1, -0.2, 0.7]])\n        n_classes = 3\n        expected_decision = np.array([[-0.1, 0.6, -0.6], [0.6, -0.3, 0.1], [-0.2, 0.1,\n        0.5]])\n        result = _ovr_decision_function(predictions, confidences, n_classes)\n        assert_allclose(result, expected_decision)\n"], "sample_833": ["    def test_LogisticRegressionCV_scoring(self):\n        # Test that LogisticRegressionCV works with custom scoring\n\n        X, y = make_classification(n_samples=100, n_classes=2, random_state=0)\n        # Use a custom scoring function that is just the accuracy\n        scoring = make_scorer(accuracy_score)\n\n        lrcv = LogisticRegressionCV(Cs=np.logspace(-4, 4, 5), scoring=scoring,\n                                     cv=5, random_state=0)\n\n        lrcv.fit(X, y)\n        assert hasattr(lrcv, 'best_score_')\n", "    def test_LogisticRegressionCV_n_jobs_multiclass(self):\n        # Test that LogisticRegressionCV correctly handles n_jobs for\n        # multi-class problems.\n        X, y = make_classification(n_samples=200, n_classes=3, random_state=0)\n        n_jobs = 2\n        lr = LogisticRegressionCV(n_jobs=n_jobs, cv=2, solver='saga',\n                                  multi_class='multinomial')\n        lr.fit(X, y)\n\n", "    def test_logistic_regression_cv_solver_liblinear_warning(self):\n        # Check warning when using solver='liblinear' with LogisticRegressionCV\n        X, y = make_classification(random_state=0)\n        lr_cv = LogisticRegressionCV(solver='liblinear', cv=5)\n\n        with pytest.warns(UserWarning,\n                          match=\"The 'l1_ratio' parameter is not supported for\"\n                               \" solver='liblinear'\"\n                               \". It will be ignored.\"):\n            lr_cv.fit(X, y)\n", "    def test_class_weight_balanced(self, solver):\n        # check that class_weight='balanced' gives same results as manual\n        # weighting\n        X, y = make_classification(n_samples=200, n_classes=3,\n                                   n_informative=2, random_state=0,\n                                   weights=[0.1, 0.4, 0.5])\n\n        lr = LogisticRegression(solver=solver, random_state=0)\n        lr.fit(X, y)\n        sample_weight = compute_sample_weight('balanced', y)\n        lr_weighted = LogisticRegression(solver=solver, random_state=0)\n        lr_weighted.fit(X, y, sample_weight=sample_weight)\n\n        assert_array_almost_equal(lr.coef_, lr_weighted.coef_, decimal=3)\n", "    def test_logistic_regression_path_sparse_input():\n        # check logistic_regression_path works with sparse input\n        X = csr_matrix(X)\n        Cs = [.00001, 1, 10000]\n        coefs, _, _ = _logistic_regression_path(X, y, penalty='l1', Cs=Cs,\n                                                 solver='saga', random_state=0,\n                                                 multi_class='ovr')\n        assert coefs.shape == (3, X.shape[1])\n\n"], "sample_770": ["def test_davies_bouldin_score():\n    assert_raises_on_only_one_label(davies_bouldin_score)\n\n    assert_raises_on_all_points_same_cluster(davies_bouldin_score)\n\n    # Assert the value is 0. when all samples are equals\n    assert_equal(0., davies_bouldin_score(np.ones((10, 2)),\n                                         [0] * 5 + [1] * 5))\n\n    # Assert the value is 0. when all the mean cluster are equal\n    assert_equal(0., davies_bouldin_score([[-1, -1], [1, 1]] * 10,\n                                         [0] * 10 + [1] * 10))\n\n    # General case (with non numpy arrays)\n    X = ([[0, 0], [1, 1]] * 5 + [[3, 3], [4, 4]] * 5 +\n         [[0, 4], [1, 3]] * 5 + [[3, 1], [4, 0]] * 5)\n    labels = [0] * 10 + [1] * 10 + [2] * 10 + [3] * 10\n    score = davies_bouldin_score(X, labels)\n    assert score > 0\n", "def test_davies_bouldin_score():\n    assert_raises_on_only_one_label(davies_bouldin_score)\n    assert_raises_on_all_points_same_cluster(davies_bouldin_score)\n    # Test case with two clusters\n    X = np.array([[-1, -1], [1, 1]] * 10)\n    labels = [0] * 10 + [1] * 10\n    score = davies_bouldin_score(X, labels)\n    assert score >= 0\n    # Test case with more than two clusters \n    X = np.array([[-1, -1], [1, 1], [3, 3], [4, 4]] * 5)\n    labels = [0] * 5 + [1] * 5 + [2] * 5 + [3] * 5\n    score = davies_bouldin_score(X, labels)\n    assert score >= 0\n", "def test_davies_bouldin_score():\n    assert_raises_on_only_one_label(davies_bouldin_score)\n\n    assert_raises_on_all_points_same_cluster(davies_bouldin_score)\n\n    # Assert the value is 0. when all samples are equals\n    assert_equal(0., davies_bouldin_score(np.ones((10, 2)),\n                                         [0] * 5 + [1] * 5))\n\n    # Assert the value is 0. when all the mean cluster are equal\n    assert_equal(0., davies_bouldin_score([[-1, -1], [1, 1]] * 10,\n                                         [0] * 10 + [1] * 10))\n\n    # General case (with non numpy arrays)\n    X = ([[0, 0], [1, 1]] * 5 + [[3, 3], [4, 4]] * 5 +\n         [[0, 4], [1, 3]] * 5 + [[3, 1], [4, 0]] * 5)\n    labels = [0] * 10 + [1] * 10 + [2] * 10 + [3] * 10\n    score = davies_bouldin_score(X, labels)\n    assert score > 0\n", "compilation error", "    def test_davies_bouldin_score():\n        assert_raises_on_only_one_label(davies_bouldin_score)\n        assert_raises_on_all_points_same_cluster(davies_bouldin_score)\n\n        # Assert the value is 0. when all samples are equals\n        assert_equal(0., davies_bouldin_score(np.ones((10, 2)),\n                                                     [0] * 5 + [1] * 5))\n\n        # Assert the value is big when the mean of each cluster are far apart\n        assert_greater(davies_bouldin_score([[-1, -1], [1, 1]] * 10,\n                                          [0] * 10 + [1] * 10),\n                       0.)\n\n        # General case (with non numpy arrays)\n        X = ([[0, 0], [1, 1]] * 5 + [[3, 3], [4, 4]] * 5 +\n             [[0, 4], [1, 3]] * 5 + [[3, 1], [4, 0]] * 5)\n        labels = [0] * 10 + [1] * 10 + [2] * 10 + [3] * 10\n        score = davies_bouldin_score(X, labels)\n        assert_greater(score, 0.) \n"], "sample_59": ["    def test_model_subclassing_with_auto_created_pk(self):\n        class MyModel(models.Model):\n            name = models.CharField(max_length=100)\n        \n        class SubModel(MyModel):\n            extra_field = models.CharField(max_length=50)\n\n        # Create instances of both models\n        MyModel.objects.create(name='MyModel Instance')\n        SubModel.objects.create(name='SubModel Instance', extra_field='Value')\n\n        # Check if the instances were created successfully\n        self.assertEqual(MyModel.objects.count(), 1)\n        self.assertEqual(SubModel.objects.count(), 1)\n", "    def test_m2m_ordering_through_model(self):\n        # Regression test for #25795: Ordering through models with ManyToManyField\n        \n        d1 = Department.objects.create(name=\"IT\")\n        d2 = Department.objects.create(name=\"Sales\")\n\n        w1 = Worker.objects.create(name=\"John Doe\", department=d1)\n        w2 = Worker.objects.create(name=\"Jane Doe\", department=d2)\n\n        e1 = Event.objects.create(description=\"Meeting\")\n        e2 = Event.objects.create(description=\"Presentation\")\n\n        e1.workers.add(w1, w2)\n        e2.workers.add(w1)\n\n        e1.workers.set_order([w2, w1])\n\n        ordered_workers = list(e1.workers.all())\n        self.assertEqual(ordered_workers[0], w2)\n        self.assertEqual(ordered_workers[1], w1)\n\n\n\n", "    def test_to_field_validation(self):\n        # Regression test for #20680: to_field validation when the field is not defined\n        with self.assertRaisesMessage(ValueError, \"The 'to_field' argument 'non_existent_field' on the FK 'Worker.department' does not exist on the related model 'Department'.\"):\n            class BrokenWorker(models.Model):\n                department = models.ForeignKey(Department, on_delete=models.CASCADE, to_field='non_existent_field')\n", "    def test_auto_pk_ordering(self):\n        # Regression test for #29875\n        Worker.objects.create(department=Department.objects.create(id=1, name='IT'), name='Alice')\n        Worker.objects.create(department=Department.objects.create(id=2, name='HR'), name='Bob')\n\n        results = Worker.objects.all().order_by('pk').values_list('name', flat=True)\n        self.assertListEqual(list(results), ['Alice', 'Bob'])\n\n", "    def test_model_deletion(self):\n        d = Department.objects.create(name=\"IT\")\n        w = Worker.objects.create(department=d, name=\"Full-time\")\n        d.delete()\n        with self.assertRaises(Worker.DoesNotExist):\n            Worker.objects.get(pk=w.pk)\n"], "sample_1104": ["compilation error", "def test_issue_14646():\n    A = Matrix([[1, 2], [3, 4]])\n    assert str(A.eigenvals()) == 'Eigenvalues({1, 2})'\n", "def test_str_issue_17149():\n    from sympy.core.numbers import Float\n    assert str(Float(\n        \"0.123456789012345678901234567890\", dps=25)) == '0.1234567890123456789012345'\n", "compilation error", "compilation error"], "sample_697": ["    def test_mk_tmp_numbered(self, tmp_path, tmp_path_factory: TempPathFactory):\n        path1 = tmp_path_factory.mktemp(\"hello\", numbered=True)\n        path2 = tmp_path_factory.mktemp(\"hello\", numbered=True)\n        assert path1 != path2\n        assert path1.name.startswith(\"hello0\")\n        assert path2.name.startswith(\"hello1\")\n", "    def test_tmp_path_factory_handles_long_basetemp(\n        tmp_path_factory: TempPathFactory, monkeypatch: MonkeyPatch", "def test_tmp_path_factory_create_directory_with_safe_permissions_symlink(tmp_path: Path, monkeypatch: MonkeyPatch) -> None:\n    \"\"\"Verify that pytest creates directories under a symlinked temproot with private permissions.\"\"\"\n    # Symlink /tmp to the test's tmp_path.\n    symlink_path = tmp_path / 'symlink-to-tmp'\n    attempt_symlink_to(symlink_path, str(tmp_path))\n    monkeypatch.setenv(\"PYTEST_DEBUG_TEMPROOT\", str(symlink_path))\n\n    tmp_factory = TempPathFactory(None, lambda *args: None, _ispytest=True)\n    basetemp = tmp_factory.getbasetemp()\n\n    # No world-readable permissions.\n    assert (basetemp.stat().st_mode & 0o077) == 0\n    # Parent too (pytest-of-foo).\n    assert (basetemp.parent.stat().st_mode & 0o077) == 0\n", "    def test_tmpdir_handles_symlinks(pytester) -> None:\n        basetemp = pytester.mkdir(\"basetemp\")\n        symlink = pytester.path.joinpath(\"symlinktemp\")\n        attempt_symlink_to(symlink, str(basetemp))\n        p = pytester.makepyfile(\n            \"\"\"\n                assert tmpdir.basename == \"tmpdir_\"\n        \"\"\"\n        )\n        result = pytester.runpytest('-s', p, \"--basetemp=%s/bt\" % symlink)\n        assert not result.ret\n", "    def test_tmpdirhandler_numbered_dir_with_symlink_loop(self, tmp_path) -> None:\n        \"\"\"Test that numbered directories created by the tmpdirhandler\n        don't create infinite symlink loops.\n\n        This can happen if the symlink resolution gets stuck in a cycle, for\n        example if symlinking to itself indirectly. (#7478)\n\n        \"\"\"\n        tmpdirhandler = TempdirFactory(\n            TempPathFactory(None, lambda *args: None, _ispytest=True), _ispytest=True\n        )\n        tmpdirhandler._basetemp = tmp_path\n        tmpdirhandler._trace = lambda *args: None\n\n        for i in range(10):\n            tmpdirhandler.mktemp(\"loop\")\n"], "sample_238": ["    def test_aggregation_subquery_annotation_related_field_no_match(self):\n        pub = Publisher.objects.create(name='Fake Pub', num_awards=2)\n        Book.objects.create(\n            isbn='159059999', name='Test book.', pages=819, rating=2.5,\n            price=Decimal('14.44'), contact=self.a9, publisher=pub,\n            pubdate=datetime.date(2019, 12, 6),\n        )\n        books_qs = Book.objects.annotate(\n            contact_publisher=Subquery(\n                Publisher.objects.filter(\n                    pk=OuterRef('publisher'),\n                    name=OuterRef('contact__name'),\n                ).values('name')[:1],\n            )\n        ).filter(\n            contact_publisher__isnull=False,\n        ).annotate(count=Count('authors'))\n        self.assertEqual(books_qs.count(), 0)\n\n", "    def test_aggregation_subquery_annotation_join(self):\n        books_qs = Book.objects.annotate(\n            latest_author_book=Subquery(\n                Author.objects.filter(\n                    pk=OuterRef('contact'),\n                    books__publisher=OuterRef('publisher')\n                ).values('books').order_by('-pubdate')[:1]\n            )\n        ).filter(\n            latest_author_book__isnull=False,\n        ).values('name', 'latest_author_book').order_by('name')\n        self.assertEqual(list(books_qs), [\n            {'name': 'The Definitive Guide to Django: Web Development Done Right',\n             'latest_author_book': [\n                 {'id': self.b4.pk, 'isbn': '159059999', 'name': 'Test book.',\n                  'pages': 819, 'rating': 2.5, 'price': '14.44', 'pubdate': datetime.date(2019, 12, 6)}\n             ]},\n            {'name': 'Practical Django Projects',\n             'latest_author_book': [\n                 {'id': self.b1.pk, 'isbn': '159059998', 'name': 'Python Tricks', 'pages': 250, 'rating': 5.0, 'price': '29.69', 'pubdate': datetime.date(2008, 1, 1)}\n             ]},\n        ])\n", "    def test_aggregation_subquery_annotation_related_field_isnull(self):\n        \"\"\"\n        Subquery annotations that are related fields and return NULL are handled\n        correctly.\n        \"\"\"\n        publisher = Publisher.objects.create(name=self.a9.name, num_awards=2)\n        book = Book.objects.create(\n            isbn='159059999', name='Test book.', pages=819, rating=2.5,\n            price=Decimal('14.44'), contact=self.a9, publisher=publisher,\n            pubdate=datetime.date(2019, 12, 6),\n        )\n\n        books_qs = Book.objects.annotate(\n            contact_publisher=Subquery(\n                Publisher.objects.filter(\n                    pk=OuterRef('publisher'),\n                    name=OuterRef('contact__name'),\n                ).values('name')[:1],\n            )\n        ).filter(Q(contact_publisher__isnull=True) | Q(contact_publisher__isnull=False))\n        self.assertQuerysetEqual(books_qs, [book])\n\n\n\n", "    def test_aggregation_subquery_annotation_conditional(self):\n        author = Author.objects.get(pk=self.a1.pk)\n        books_rating_qs = Book.objects.filter(\n            contact=author,\n            price=Decimal('29.69'),\n        ).values('rating')\n        publisher_qs = Publisher.objects.filter(\n            book__contact__age__gt=20,\n            name=self.p1.name,\n        ).annotate(\n            rating=Subquery(books_rating_qs, output_field=DecimalField()),\n        ).filter(rating__isnull=False).values('rating').annotate(total_count=Count('rating'))\n        self.assertEqual(list(publisher_qs), [{'rating': 4.0, 'total_count': 2}])\n", "    def test_aggregation_subquery_annotation_related_field_with_order_by(self):\n        publisher = Publisher.objects.create(name=self.a9.name, num_awards=2)\n        book = Book.objects.create(\n            isbn='159059999', name='Test book.', pages=819, rating=2.5,\n            price=Decimal('14.44'), contact=self.a9, publisher=publisher,\n            pubdate=datetime.date(2019, 12, 6),\n        )\n        book.authors.add(self.a5, self.a6, self.a7)\n        books_qs = Book.objects.annotate(\n            contact_publisher=Subquery(\n                Publisher.objects.filter(\n                    pk=OuterRef('publisher'),\n                    name=OuterRef('contact__name'),\n                ).values('name')[:1],\n            )\n        ).filter(\n            contact_publisher__isnull=False,\n        ).order_by('contact_publisher').annotate(count=Count('authors'))\n        self.assertSequenceEqual(books_qs, [book])\n"], "sample_1151": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_393": ["    def test_makemessages_no_new_strings(self):\n        \"\"\"PO files are unchanged when there are no new strings to extract.\"\"\"\n        # Replace the existing PO file with a copy of itself\n        shutil.copyfile(self.PO_FILE, self.PO_FILE + \".bak\")\n        management.call_command(\"makemessages\", locale=[LOCALE], verbosity=0)\n        # Compare the original and the updated PO file. They should be identical.\n        with open(self.PO_FILE, \"r\") as f_new, open(\n            self.PO_FILE + \".bak\", \"r\"\n        ) as f_original:\n            self.assertEqual(f_new.read(), f_original.read())\n", "    def test_ignore_non_existent_files(self):\n        ignore_patterns = [\n            \"templates/nonexistentfile.html\",\n            \"templates/subdir/also_nonexistent.html\",\n        ]\n        with mock.patch(\"django.core.management.commands.makemessages.os.path.exists\") as mock_exists:\n            mock_exists.side_effect = lambda path: False\n            management.call_command(\n                \"makemessages\", locale=[LOCALE], ignore_patterns=ignore_patterns, verbosity=0\n            )\n            mock_exists.assert_called()\n", "    def test_custom_pot_file_name(self):\n        \"\"\"Test a custom name for the .pot file used by makemessages.\"\"\"\n        with override_settings(POT_FILE_NAME=\"custom-django.pot\"):\n            management.call_command(\"makemessages\", locale=[LOCALE], verbosity=0)\n            self.assertTrue(os.path.exists(\"locale/custom-django.pot\"))\n            self.assertFalse(os.path.exists(\"locale/django.pot\"))\n", "    def test_ignore_files_with_extension(self):\n        with override_settings(\n            LOCALE_PATHS=[os.path.join(self.test_dir, \"locale\")],\n            INSTALLED_APPS=[\"app_with_locale\"],\n        ):\n            ignore_patter = \"*.txt\"\n            management.call_command(\n                \"makemessages\",\n                locale=[LOCALE],\n                verbosity=0,\n                ignore_patterns=[ignore_patter],\n            )\n            self.assertTrue(os.path.exists(self.PO_FILE))\n              \n            with open(self.PO_FILE) as fp:\n                po_contents = fp.read()\n                self.assertNotMsgId(\"This literal should be excluded\", po_contents)\n\n\n\n", "    def test_makemessages_with_empty_locale_dir(self):\n        \"\"\"\n        makemessages should create the locale directory if it doesn't exist.\n\n        \"\"\"\n        locale_dir = os.path.join(self.test_dir, \"locale\")\n        shutil.rmtree(locale_dir, ignore_errors=True)\n        management.call_command(\"makemessages\", locale=[LOCALE], verbosity=0)\n        self.assertTrue(os.path.exists(locale_dir))\n        self.assertTrue(os.path.exists(self.PO_FILE))\n"], "sample_857": ["    def test_min_samples_leaf_multilabel(self):\n        X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n        y = np.array([[0, 1], [1, 0], [0, 1], [1, 0]])\n\n        # Test that setting min_samples_leaf raises an error for multilabel\n        # classification\n        clf = DecisionTreeClassifier(min_samples_leaf=2)\n        with pytest.raises(ValueError, match=\"min_samples_leaf should be greater than or equal to 1\"):\n            clf.fit(X, y)\n", "    def test_max_depth_and_ccp_alpha():\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        y = np.array([0, 1, 0])\n        clf = DecisionTreeClassifier(random_state=0, max_depth=2, ccp_alpha=0.1)\n        clf.fit(X, y)\n        # Check that the tree is pruned to a depth of 1 when both max_depth\n        # and ccp_alpha are specified.\n        assert clf.tree_.max_depth == 1\n", "    def test_decision_tree_sparse_multioutput(self):\n        # Check if DecisionTreeRegressor works with sparse data and multi-output\n        X = csr_matrix([[1, 2], [3, 4], [5, 6]])\n        y = np.array([[1, 2], [3, 4], [5, 6]])\n        dt = DecisionTreeRegressor(random_state=0)\n        dt.fit(X, y)\n\n        assert dt.predict(X).shape == (3, 2)\n", "def test_sparse_input_empty_data(tree_type):\n    TreeEstimator = ALL_TREES[tree_type]\n\n    X = sparse_matrix((0, 10), dtype=np.float32)\n    y = np.array([])\n    with pytest.raises(ValueError):\n        TreeEstimator().fit(X, y)\n\n", "def test_decisiontree_max_depth_zero():\n    X, y = iris.data[:, :2], iris.target\n    for name in CLF_TREES:\n        est = ALL_TREES[name](max_depth=0)\n        est.fit(X, y)\n        assert est.tree_.max_depth == 0\n        assert est.tree_.node_count == 1\n"], "sample_179": ["    def test_check_exclude_constraint(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n            name = models.CharField(max_length=100)\n\n            class Meta:\n                constraints = [\n                    models.ExcludeConstraint(\n                        fields=['age', 'name'],\n                        name='exclude_age_name',\n                    ),\n                ]\n\n        errors = Model.check(databases=self.databases)\n        warn = Warning(\n            '%s does not support exclude constraints.' % connection.display_name,\n            hint=(\n                \"A constraint won't be created. Silence this warning if you \"\n                \"don't care about it.\"\n            ),\n            obj=Model,\n            id='models.W039',\n        )\n        expected = [] if connection.features.supports_exclude_constraints else [warn]\n        self.assertCountEqual(errors, expected)\n", "    def test_unique_constraint_with_expression(self):\n        class Model(models.Model):\n            name = models.CharField(max_length=100)\n            age = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        name='unique_name_age_expression',\n                        expression=models.F('name') + models.Value(' ') + models.F('age')\n                    ),\n                ]\n\n        errors = Model.check(databases=self.databases)\n        expected = [] if connection.features.supports_expression_in_constraint else [\n            Warning(\n                '%s does not support expressions in constraints.'\n                % connection.display_name,\n                hint=(\n                    \"A constraint won't be created. Silence this warning if \"\n                    \"you don't care about it.\"\n                ),\n                obj=Model,\n                id='models.W039',\n            ),\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_unique_constraint_with_expression(self):\n        class Model(models.Model):\n            name = models.CharField(max_length=10)\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        expression=models.F('name') + '_suffix',\n                        name='unique_expression',\n                    ),\n                ]\n\n        errors = Model.check(databases=self.databases)\n        expected = [] if connection.features.supports_expression_in_constraints else [\n            Warning(\n                '%s does not support expressions in unique constraints.' % connection.display_name,\n                hint=(\n                    \"A constraint won't be created. Silence this warning if \"\n                    \"you don't care about it.\"\n                ),\n                obj=Model,\n                id='models.W039',\n            ),\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_unique_constraint_with_condition_and_deferrable(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['age'],\n                        name='unique_age_gte_100_deferrable',\n                        condition=models.Q(age__gte=100),\n                        deferrable=models.Deferrable.DEFERRED,\n                    ),\n                ]\n\n        errors = Model.check(databases=self.databases)\n        expected = [] if (connection.features.supports_partial_indexes and\n                           connection.features.supports_deferrable_unique_constraints) else [\n            Warning(\n                '%s does not support unique constraints with conditions and deferrability.'\n                % connection.display_name,\n                hint=(\n                    \"A constraint won't be created. Silence this warning if \"\n                    \"you don't care about it.\"\n                ),\n                obj=Model,\n                id='models.W039',\n            ),\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_unique_constraint_with_deferrable_and_condition(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['age'],\n                        name='unique_age_deferrable_conditional',\n                        condition=models.Q(age__gte=100),\n                        deferrable=models.Deferrable.DEFERRED,\n                    ),\n                ]\n\n        errors = Model.check(databases=self.databases)\n        expected = [] if connection.features.supports_partial_indexes and connection.features.supports_deferrable_unique_constraints else [\n            Warning(\n                '%s does not support unique constraints with both conditions and deferrability.'\n                % connection.display_name,\n                hint=(\n                    \"A constraint won't be created. Silence this warning if you don't care about it.\"\n                ),\n                obj=Model,\n                id='models.W039',\n            ),\n        ]\n        self.assertEqual(errors, expected)\n"], "sample_689": ["def test_warning_captured_hook_is_deprecated(testdir: Testdir) -> None:\n    testdir.makepyfile(\n        test_warning_captured=\"\"\"\n        import pytest\n\n            with pytest.warns(UserWarning):\n                pass\n    \"\"\"\n    )\n\n    with pytest.warns(pytest.PytestDeprecationWarning, match=\"pytest_warning_captured\"):\n        testdir.runpytest(\"-v\", \"--capture=no\")\n", "def test_warning_captured_hook_is_deprecated(testdir: Testdir) -> None:\n    testdir.makepyfile(\n        test_warning_captured_hook=\"\"\"\n        import pytest\n\n            with pytest.raises(ValueError):\n                raise ValueError('xyz')\n    \"\"\"\n    )\n    result = testdir.runpytest_inprocess(\n        \"-p pytest-capturelog\", \"--capture=sys\", \"--strict-markers\"\n    )\n    result.stdout.fnmatch_lines(\n        [\n            \"*The pytest_warning_captured is deprecated and will be removed in a future release.*\",\n            \"*Please use pytest_warning_recorded instead.\",\n        ]\n    )\n\n\n", "def test_warning_captured_hook_is_deprecated(testdir) -> None:\n    testdir.makepyfile(\n        \"\"\"\n            pass\n        \"\"\"\n    )\n    result = testdir.runpytest(\"-p\", \"pytest_warning_captured\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*The pytest_warning_captured*deprecated*\",\n            \"*Please use pytest_warning_recorded*\",\n        ]\n    )\n", "def test_warning_captured_hook_is_deprecated(testdir) -> None:\n    testdir.makepyfile(\n        \"\"\"\n            pytest.warning_captured(RuntimeWarning)\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*The pytest_warning_captured is deprecated*\"])\n", "def test_warning_captured_hook_is_deprecated(testdir: Testdir) -> None:\n    testdir.makepyfile(\n        \"\"\"\n            import pytest\n            pytest.warning_captured()\n            assert False\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*The pytest_warning_captured is deprecated*removed in a future release*\",\n            \"*Please use pytest_warning_recorded instead*\",\n        ]\n    )\n\n"], "sample_132": ["    def test_sensitive_variables_with_nested_function(self):\n        @sensitive_variables\n                return password\n            return inner_function()\n\n        with self.settings(DEBUG=False):\n            response = self.client.get('/raises500/', HTTP_ACCEPT='application/json')\n            self.assertNotContains(response, 'password', status_code=500)\n", "    def test_sensitive_variables_with_nested_functions(self):\n        @sensitive_variables\n                return password\n            return inner_func()\n        \n        with self.settings(DEBUG=True):\n            response = self.client.get('/sensitive_variables_with_nested_functions/')\n            self.assertContains(response, 'XXXXXXXXXXXXXXXXXXXX', status_code=500)\n\n        with self.settings(DEBUG=False):\n            response = self.client.get('/sensitive_variables_with_nested_functions/')\n            self.assertNotContains(response, 'password', status_code=500)\n\n", "    def test_sensitive_method_view_class(self):\n        \"\"\"\n        The sensitive_variables decorator works with class methods.\n        \"\"\"\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(sensitive_method_view_class, check_for_POST_params=False)\n            self.verify_unsafe_email(sensitive_method_view_class, check_for_POST_params=False)\n\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(sensitive_method_view_class, check_for_POST_params=False)\n            self.verify_safe_email(sensitive_method_view_class, check_for_POST_params=False)\n\n", "    def test_sensitive_variables_with_nested_functions(self):\n        @sensitive_variables('password')\n                return password\n            return inner_func()\n\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(outer_func, check_for_vars=False)\n\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(outer_func, check_for_vars=False)\n", "    def test_sensitive_variables_with_arguments(self):\n        \"\"\"\n        The sensitive_variables decorator should work with functions\n        that take arguments other than just the sensitive ones.\n        \"\"\"\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(sensitive_args_function, check_for_POST_params=False)\n            self.verify_unsafe_email(sensitive_args_function, check_for_POST_params=False)\n\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(sensitive_args_function, check_for_POST_params=False)\n            self.verify_safe_email(sensitive_args_function, check_for_POST_params=False)\n"], "sample_921": ["    def test_signature_from_str_empty():\n        sig = inspect.signature_from_str('()')\n        assert sig.parameters == {}\n", "def test_signature_from_str_empty():\n    sig = inspect.signature_from_str('')\n    assert list(sig.parameters.keys()) == []\n    assert sig.return_annotation == Parameter.empty\n", "    def test_signature_from_str_empty_return():\n        signature = '() -> None'\n        sig = inspect.signature_from_str(signature)\n        assert sig.return_annotation == 'None'\n\n", "    def test_is_builtin_class_method(app):\n        from builtins import int\n\n        assert inspect.is_builtin_class_method(int.from_bytes) is True\n        assert inspect.is_builtin_class_method(MyInt.my_method) is False\n", "def test_signature_from_str_no_default_values():\n    signature = ('(a: int, *args: bytes, b: str, **kwargs: float)'\n                 )\n    sig = inspect.signature_from_str(signature)\n    assert sig.parameters['a'].default == Parameter.empty\n    assert sig.parameters['b'].default == Parameter.empty\n\n\n\n"], "sample_568": ["    def test_scatter_masked_color_single(fig_test, fig_ref):\n        x = [1, 2, 3]\n        y = [0, 0, 0]\n        z = [0, 0, np.nan]\n        colors = [\n            [0.0, 0.0, 0.0, 1],\n            [0.0, 0.0, 0.0, 1],\n            [0.0, 0.0, 0.0, 1],\n        ]\n\n        ax1 = fig_test.add_subplot(111, projection='3d')\n        path3d = ax1.scatter(x, y, z, color=colors)\n\n        ax2 = fig_ref.add_subplot(111, projection='3d')\n        ax2.scatter(x, y, z, color=colors)\n", "def test_surface3d_zsort_masked():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n\n    x, y = np.mgrid[-2:2:0.1, -2:2:0.1]\n    z = np.sin(x)**2 + np.cos(y)**2\n    z = np.ma.masked_where(x**2 + y**2 > 1, z)\n\n    ax.plot_surface(x, y, z, cmap='jet')\n    ax.view_init(elev=45, azim=145)\n", "def test_axis_panecolor_projection(projection):\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection=projection)\n    ax.set_proj_type(projection)\n    ax.xaxis.pane.set_color('r')\n    ax.yaxis.pane.set_color('g')\n    ax.zaxis.pane.set_color('b')\n\n", "def test_vertical_axis_label_order(vertical_axis, expected_label):\n    ax = plt.subplot(1, 1, 1, projection=\"3d\")\n    ax.view_init(vertical_axis=vertical_axis)\n    labels = [label.get_text() for label in ax.get_axis_labels()]\n    assert labels == expected_label\n", "def test_surface3d_zsort():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n\n    x, y = np.mgrid[-2:2:0.1, -2:2:0.1]\n    z = np.sin(x)**2 + np.cos(y)**2\n    z[x.shape[0] // 2:, x.shape[1] // 2:] = 10\n\n    ax.plot_surface(x, y, z, cmap='jet')\n    ax.view_init(elev=45, azim=145)\n"], "sample_23": ["    def test_angle_to_string_precision(cls):\n        a = cls(1.23456789, u.deg)\n        assert a.to_string(precision=2) == f\"1.23 deg\"\n        assert a.to_string(precision=5) == f\"1.23457 deg\"\n", "    def test_angle_str_with_positive_and_negative_signs(angle_value):\n        angle = Angle(angle_value)\n        assert angle.to_string() == angle_value\n", "def test_angle_with_zero_dimensionless(cls):\n    \"\"\"Regression test for #14282\n\n    Check that initializing an Angle with a dimensionless quantity of value zero\n    results in a zero angle with the expected unit.\n    \"\"\"\n    q = cls(0 * u.dimensionless_unscaled)\n    assert q.value == 0.0\n    assert q.unit == cls.get_default_unit()\n\n", "def test_angle_quantity_arithmetic(cls):\n    a = cls(1 * u.deg)\n    b = cls(2 * u.deg)\n    assert (a + b).unit == cls.default_unit\n    assert (a - b).unit == cls.default_unit\n    assert (a * b).unit == cls.default_unit * cls.default_unit\n    assert (a / b).unit == u.dimensionless_unscaled\n", "compilation error"], "sample_1199": ["def test_tensor_product_density():\n    rho = Density(Qubit('0'), Qubit('1'))\n    assert TensorProduct(rho, rho).doit() == rho*rho\n", "def test_tensor_product_trace():\n    assert Tr(TP(mat1,mat2)) == Tr(mat1)*Tr(mat2)\n    assert Tr(TP(mat1,mat2), indices=[0]) == Tr(mat1) * mat2\n\n", "compilation error", "compilation error", "def test_tensor_product_density():\n    rho = Density([0.5, 0.5])\n    assert TensorProduct(rho, rho) == rho*rho\n\n\n\n"], "sample_750": ["    def test_omp_cv_singular_matrix():\n        n_samples = 5\n        n_features = 5\n        n_targets = 1\n        rng = check_random_state(0)\n        X = rng.rand(n_samples, n_features)\n        # Make X singular\n        X[:, 2:] = X[:, :2]\n        y = rng.rand(n_samples, n_targets)\n\n        ompcv = OrthogonalMatchingPursuitCV(normalize=True,\n                                           fit_intercept=False,\n                                           max_iter=n_features,\n                                           cv=5)\n        with assert_warns(ConvergenceWarning):\n            ompcv.fit(X, y)\n", "    def test_omp_cv_multi_target():\n        ompcv = OrthogonalMatchingPursuitCV(normalize=True, fit_intercept=False,\n                                           max_iter=10, cv=5)\n        ompcv.fit(X, y)\n        assert_equal(ompcv.coef_.shape, (n_targets, n_features))\n        assert_true(np.all(ompcv.coef_ != 0))\n", "    def test_omp_cv_empty_data():\n        # Check if OrthogonalMatchingPursuitCV handles empty data correctly\n        X_empty = np.array([])\n        y_empty = np.array([])\n        ompcv = OrthogonalMatchingPursuitCV(cv=2)\n        assert_raises(ValueError, ompcv.fit, X_empty, y_empty)\n", "    def test_ompcv_n_jobs():\n        # Check if parallel execution works\n        from joblib import parallel_backend\n        n_jobs = 2\n        with parallel_backend('threading', n_jobs=n_jobs):\n            ompcv_parallel = OrthogonalMatchingPursuitCV(\n                normalize=True, fit_intercept=False,\n                max_iter=10, cv=5, n_jobs=n_jobs)\n            ompcv_parallel.fit(X, y_)\n\n        ompcv_serial = OrthogonalMatchingPursuitCV(\n            normalize=True, fit_intercept=False,\n            max_iter=10, cv=5, n_jobs=1)\n        ompcv_serial.fit(X, y_)\n\n        assert_array_almost_equal(ompcv_parallel.coef_, ompcv_serial.coef_)\n\n", "    def test_omp_cv_return_n_iter():\n        ompcv = OrthogonalMatchingPursuitCV(normalize=True, fit_intercept=False,\n                                           max_iter=10, cv=5,\n                                           return_n_iter=True)\n        ompcv.fit(X, y)\n        assert_equal(ompcv.n_iter_.shape, (n_targets, 5))\n"], "sample_908": ["    def test_unparse_arguments():\n        source = \"def my_func(a: int, b: str = 'default', *args, c: float, kwarg: dict = None, **kwargs): pass\"\n        module = ast.parse(source)\n        assert ast.unparse(module.body[0].args) == \"a: int, b: str = 'default', *args, c: float, kwarg: dict = None, **kwargs\"\n", "def test_unparse_function_definition(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0]) == expected\n", "def test_unparse_arguments(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0]) == expected\n", "def test_unparse_arguments():\n    source = \"def func(a: int, b: str, *, c: float = 3.14, d: bool = True, **kwargs): pass\"\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0].args) == \"a: int, b: str, *, c: float = 3.14, d: bool = True, **kwargs\"\n", "    def test_unparse_arguments():\n        source = \"def func(a: int, b: str = 'default', *args, c: float, **kwargs): pass\"\n        module = ast.parse(source)\n        assert ast.unparse(module.body[0].args) == \"a: int, b: str = 'default', *args, c: float, **kwargs\"\n"], "sample_1040": ["compilation error", "def test_print_function_with_no_arguments():\n    f = Function('f')\n    assert mathml(f(), printer='presentation') == '<mi>f</mi><mo>(</mo><mo>)</mo>'\n    assert mathml(f(), printer='content') == '<apply><f/></apply>' \n", "compilation error", "def test_presentation_mathml_fractions():\n    expr = (x + 1)/(2*x - 3)\n    mml = mpp._print(expr)\n    assert mml.childNodes[0].nodeName == 'mfrac'\n    assert mml.childNodes[0].childNodes[0].nodeName == 'mrow'\n    assert mml.childNodes[0].childNodes[1].nodeName == 'mrow'\n\n\n\n", "def test_print_matrix_element():\n    A = MatrixSymbol('A', 2, 2)\n    x, y = symbols('x y')\n    A[0,0] = x*y\n    A[0,1] = x\n    A[1,0] = y\n    A[1,1] = 1\n\n    assert mathml(A[0,0], printer='presentation') == '<msub><msub><mi>A</mi><mn>1</mn></msub><mn>1</mn></msub>'\n\n    assert mathml(A, printer='presentation') == '<mi>A</mi>'\n\n    assert mathml(A, printer='content') == '<matrix rowcount=\"2\" colcount=\"2\"><apply><eq/><ci>A</ci><matrix><matrixrow><ci>x*y</ci><ci>x</ci></matrixrow><matrixrow><ci>y</ci><cn>1</cn></matrixrow></matrix></apply></matrix>'\n\n"], "sample_188": ["    def test_expression_wrapper_output_field_mismatch(self):\n        with self.assertRaises(ValueError):\n            ExpressionWrapper(Value(True), output_field=IntegerField())\n", "    def test_expression_wrapper_with_alias(self):\n        alias = 'test_alias'\n        expr = ExpressionWrapper(F('field'), output_field=IntegerField(), alias=alias)\n        self.assertEqual(expr.alias, alias)\n", "    def test_expressionwrapper_with_alias(self):\n        expr = ExpressionWrapper(F('field') + F('other_field'), output_field=IntegerField(), alias='new_field')\n        self.assertEqual(expr.alias, 'new_field')\n", "    def test_expression_wrapper_with_alias(self):\n        expr = ExpressionWrapper(F('cost') + F('tax'), output_field=IntegerField(), alias='total_cost')\n        self.assertEqual(expr.alias, 'total_cost')\n", "    def test_expression_wrapper_output_field_none(self):\n        expr = ExpressionWrapper(F('col'), output_field=None)\n        self.assertIsNone(expr.output_field)\n"], "sample_638": ["def test_project_from_files_with_empty_input(mock_writer, mock_subprocess, capsys):\n    \"\"\"Test that pyreverse handles empty input correctly.\"\"\"\n    with pytest.raises(SystemExit) as wrapped_sysexit:\n        main.Run([])\n    # Check that the right info message is shown to the user\n    assert \"No package provided.\" in capsys.readouterr().out\n    # Check that the exit code is 1 to indicate an error\n    assert wrapped_sysexit.value.code == 1\n", "def test_graphviz_unsupported_image_format(mock_writer, mock_subprocess, capsys):\n    \"\"\"Test that Graphviz throws an error if the image format is not supported.\"\"\"\n    mock_subprocess.run.return_value.returncode = 1\n    with pytest.raises(SystemExit) as wrapped_sysexit:\n        # we have to catch the SystemExit so the test execution does not stop\n        main.Run([\"-o\", \"xyz\", TEST_DATA_DIR])\n    # Check that the right info message is shown to the user\n    assert (\n        \"Error: unable to generate diagram for format 'xyz' using Graphviz.\"\n        in capsys.readouterr().out\n    )\n    # Check that pyreverse actually made the call to create the diagram and we exit cleanly\n    mock_writer.DiagramWriter().write.assert_not_called()\n    assert wrapped_sysexit.value.code == 1\n\n", "def test_run_with_arguments(mock_writer, mock_subprocess, capsys):\n    \"\"\"Test that the Run class handles arguments correctly.\"\"\"\n    main.Run([\"-o\", \"dot\", TEST_DATA_DIR])\n    mock_writer.DiagramWriter().write.assert_called_once()\n", "def test_graphviz_unsupported_format(mock_writer, mock_subprocess, capsys):\n    \"\"\"Test that pyreverse raises an error if the image format is not supported.\"\"\"\n    mock_subprocess.run.return_value.stderr = (\n        'Format: \"XYZ\" not recognized. Use one of: '\n        \"bmp canon cgimage cmap cmapx cmapx_np dot dot_json eps exr fig gd \"\n        \"gd2 gif gv icns ico imap imap_np ismap jp2 jpe jpeg jpg json json0 \"\n        \"mp pct pdf pic pict plain plain-ext png pov ps ps2 psd sgi svg svgz \"\n        \"tga tif tiff tk vdx vml vmlz vrml wbmp webp xdot xdot1.2 xdot1.4 xdot_json\"\n    )\n    with pytest.raises(SystemExit) as wrapped_sysexit:\n        # we have to catch the SystemExit so the test execution does not stop\n        main.Run([\"-o\", \"XYZ\", TEST_DATA_DIR])\n    # Check that the right info message is shown to the user\n    assert (\n        \"Format XYZ is not supported natively. Pyreverse will try to generate it using Graphviz...\"\n        in capsys.readouterr().out\n    )\n    assert (\n        \"Unable to determine Graphviz supported output formats.\"\n        in capsys.readouterr().out\n    )\n    assert wrapped_sysexit.value.code == 1\n", "def test_no_output_format_specified(\n    mock_writer, mock_subprocess, capsys"], "sample_595": ["    def test_split():\n        values = xr.DataArray([\"a,b,c\", \"d;e;f\", \"g,h\"]).astype(dtype)\n\n        result = values.str.split(\",\")\n        expected = xr.DataArray([[\"a\", \"b\", \"c\"], [\"d;e;f\"], [\"g\", \"h\"]])\n        assert_equal(result, expected)\n", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_975": ["compilation error", "def test_unrad():\n    x = Symbol('x')\n    y = Symbol('y')\n    assert unrad(sqrt(x)*x**Rational(1, 3) + 2) is None\n\n", "compilation error", "compilation error", "compilation error"], "sample_847": ["def test_enet_cv_positive_constraint_multi_task():\n    X, y, X_test, y_test = build_dataset()\n    y = np.hstack((y[:, np.newaxis], y[:, np.newaxis]))\n    max_iter = 500\n\n    # Ensure the unconstrained fit has negative coefficients\n    enetcv_unconstrained = MultiTaskElasticNetCV(n_alphas=3, eps=1e-1,\n                                                 max_iter=max_iter,\n                                                 cv=2, n_jobs=1)\n    enetcv_unconstrained.fit(X, y)\n    assert np.any(enetcv_unconstrained.coef_ < 0)\n\n    # On same data, constrained fit has non-negative coefficients\n    enetcv_constrained = MultiTaskElasticNetCV(n_alphas=3, eps=1e-1,\n                                                 max_iter=max_iter,\n                                                 cv=2, positive=True, n_jobs=1)\n    enetcv_constrained.fit(X, y)\n    assert np.all(enetcv_constrained.coef_ >= 0)\n", "    def test_enet_cv_copy_X():\n        X, y, _, _ = build_dataset()\n        enet_cv = ElasticNetCV(copy_X=True)\n        X_copy = X.copy()\n        enet_cv.fit(X, y)\n        assert_array_equal(X, X_copy)\n", "def test_enet_warm_start_with_dtype(dtype):\n    X, y = np.random.randn(100, 50), np.random.randn(100)\n    model = ElasticNet(warm_start=True, alpha=1.0, random_state=42).fit(X, y)\n    coef0 = model.coef_.astype(dtype)\n    intercept0 = model.intercept_\n    # set the dtype of the model\n    model.coef_ = coef0.copy()\n    model.intercept_ = intercept0\n    model.fit(X, y)\n    assert model.coef_.dtype == dtype\n", "def test_enet_warm_start_sparse_dense_consistency():\n    # Test consistency between fitting with sparse and dense input when\n    # warm_start is True\n    X, y, _, _ = build_dataset(n_samples=50, n_features=20)\n    sparse_X = sparse.csr_matrix(X)\n\n    clf_dense = ElasticNet(warm_start=True, tol=1e-8)\n    clf_dense.fit(X, y)\n    coef_dense = clf_dense.coef_\n\n    clf_sparse = ElasticNet(warm_start=True, tol=1e-8)\n    clf_sparse.fit(sparse_X, y)\n    coef_sparse = clf_sparse.coef_\n\n    assert_array_almost_equal(coef_dense, coef_sparse)\n\n", "    def test_enet_cv_precompute_argument(precompute):\n        X, y, _, _ = build_dataset()\n        clf = ElasticNetCV(precompute=precompute)\n        clf.fit(X, y)\n        assert clf.steps[1][1].precompute == precompute\n"], "sample_966": ["compilation error", "def test_inheritance_diagram(app):\n    text = (\".. py:class:: A\\n\"\n            \"   :parents: object\\n\"\n            \"\\n\"\n            \".. py:class:: B(A)\\n\"\n            \"   :parents: A\\n\"\n            \"\\n\"\n            \".. py:class:: C(B)\\n\"\n            \"   :parents: B\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc, addnodes.index, desc, addnodes.index, desc))\n    assert 'A' in app.env.domains['py'].objects\n    assert 'B' in app.env.domains['py'].objects\n    assert 'C' in app.env.domains['py'].objects\n    assert app.env.domains['py'].objects['A']['inheritance'] == []\n    assert app.env.domains['py'].objects['B']['inheritance'] == ['A']\n    assert app.env.domains['py'].objects['C']['inheritance'] == ['B', 'A']\n\n\n", "compilation error", "def test_info_field_list_union(app):\n    text = (\".. py:module:: example\\n\"\n            \".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   :param age: blah blah\\n\"\n            \"   :type age: int | str | float\\n\")\n    doctree = restructuredtext.parse(app, text)\n\n    assert_node(doctree,\n                (nodes.target,\n                 addnodes.index,\n                 addnodes.index,\n                 [desc, ([desc_signature, ([desc_annotation, (\"class\", desc_sig_space)],\n                                           [desc_addname, \"example.\"],\n                                           [desc_name, \"Class\"])],\n                         [desc_content, nodes.field_list, nodes.field, (nodes.field_name,\n                                                                        nodes.field_body)])]))\n    assert_node(doctree[3][1][0][0][1],\n                ([nodes.paragraph, ([addnodes.literal_strong, \"age\"],\n                                    \" (\",\n                                    [pending_xref, addnodes.literal_emphasis, \"int\"],\n                                    [addnodes.literal_emphasis, \" | \"],\n                                    [pending_xref, addnodes.literal_emphasis, \"str\"],\n                                    [addnodes.literal_emphasis, \" | \"],\n                                    [pending_xref, addnodes.literal_emphasis, \"float\"],\n                                    \")\",\n                                    \" -- \",\n                                    \"blah blah\")],))\n    assert_node(doctree[3][1][0][0][1][0][2], pending_xref,\n                refdomain=\"py\", reftype=\"class\", reftarget=\"int\",\n                **{\"py:module\": \"example\", \"py:class\": \"Class\"})\n    assert_node(doctree[3][1][0][0][1][0][4], pending_xref,\n                refdomain=\"py\", reftype=\"class\", reftarget=\"str\",\n                **{\"py:module\": \"example\", \"py:class\": \"Class\"})\n    assert_node(doctree[3][1][0][0][1][0][6], pending_xref,\n                ", "def test_info_field_list_with_newline(app):\n    text = (\".. py:module:: example\\n\"\n            \".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   :param str name: blah\\n\"\n            \"                    blah\\n\"\n            \"\\n\"\n            \"   :param age: blah blah\\n\"\n            \"   :type age: int\\n\")\n    doctree = restructuredtext.parse(app, text)\n\n    assert_node(doctree,\n                (nodes.target,\n                 addnodes.index,\n                 addnodes.index,\n                 [desc, ([desc_signature, ([desc_annotation, (\"class\", desc_sig_space)],\n                                           [desc_addname, \"example.\"],\n                                           [desc_name, \"Class\"])],\n                         [desc_content, nodes.field_list, nodes.field, nodes.field])]))\n\n    assert_node(doctree[3][1][0][0][1][0],\n                ([nodes.paragraph, ([addnodes.literal_strong, \"name\"],\n                                    \" (\",\n                                    [pending_xref, addnodes.literal_emphasis, \"str\"],\n                                    \")\",\n                                    \" -- \",\n                                    \"blah\\n blah\")],))\n    assert_node(doctree[3][1][0][0][1][1],\n                ([nodes.paragraph, ([addnodes.literal_strong, \"age\"],\n                \" (\",\n                [pending_xref, addnodes.literal_emphasis, \"int\"],\n                \")\",\n                \" -- \",\n                \"blah blah\")],))\n"], "sample_757": ["compilation error", "    def test_ordinal_encoder_empty_input(self):\n        enc = OrdinalEncoder()\n        X = np.array([], dtype=object).reshape(0, 2)\n        assert_array_equal(enc.fit_transform(X), np.array([]).reshape(0, 2))\n", "compilation error", "compilation error", "compilation error"], "sample_747": ["compilation error", "compilation error", "def test_power_transformer_copy():\n    X = np.abs(X_2d)\n    pt = PowerTransformer(method='box-cox', copy=False)\n    \n    X_trans = pt.fit_transform(X.copy())\n    assert_array_equal(X, X_trans)\n", "compilation error", "compilation error"], "sample_17": ["    def test_iscomplexobj(self):\n        self.check(np.iscomplexobj, unit=None)\n", "    def test_masked_where(self):\n        # This is mostly to check that we don't crash.\n        with pytest.raises(TypeError):\n            np.ma.masked_where(self.q > 1, self.q)\n", "    def test_merge_arrays_nonquantities_with_units(self):\n        # Fails because cannot create quantity from structured array with units.\n        arr = rfn.merge_arrays((self.q_pv[\"p\"], self.q_pv.value), flatten=False)\n", "    def test_structured_to_unstructured_with_units(self):\n        # Test with structured units.\n        struct = np.zeros(\n            2, dtype=[(\"p\", \"f8\"), (\"v\", \"f8\")]\n        ) << u.StructuredUnit((u.m, u.m / u.s), (\"p\", \"v\"))\n        unstruct = rfn.structured_to_unstructured(struct)\n        assert unstruct.unit == u.StructuredUnit((u.m, u.m / u.s), (\"f0\", \"f1\"))\n\n\n", "    def test_merge_arrays_units_with_repeat_dtypes(self):\n        dtype = [((\"x\", \"y\"), float), ((\"a\", \"b\"), float)]\n        q1 = u.Quantity([(1, 2)], dtype=dtype, unit=u.m)\n        q2 = u.Quantity([(3, 4)], dtype=dtype, unit=u.s)\n        with pytest.raises(ValueError, match=\"Cannot merge arrays\"):\n            rfn.merge_arrays((q1, q2))\n"], "sample_115": ["    def test_paranoid_view_with_sensitive_variables_decorator(self):\n        \"\"\"\n        Paranoid view should not leak sensitive variables even if\n        they are decorated with @sensitive_variables.\n        \"\"\"\n        with self.settings(DEBUG=True):\n            self.verify_paranoid_response(paranoid_view_with_sensitive_variables, check_for_vars=False)\n\n        with self.settings(DEBUG=False):\n            self.verify_paranoid_response(paranoid_view_with_sensitive_variables, check_for_vars=False)\n\n", "    def test_sensitive_variables_with_keywords(self):\n        with self.settings(DEBUG=True):\n            request = self.rf.post('/some_url/', self.breakfast_data)\n            response = sensitive_keywords_function_caller(request)\n            self.assertContains(response, 'cooked_eggs', status_code=500)\n            self.assertContains(response, 'scrambled', status_code=500)\n            self.assertNotContains(response, 'sauce', status_code=500)\n            self.assertNotContains(response, 'worcestershire', status_code=500)\n\n", "    def test_sensitive_variables_with_non_string_keys(self):\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(sensitive_variables_with_non_string_keys, check_for_vars=False)\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(sensitive_variables_with_non_string_keys, check_for_vars=False)\n", "    def test_sensitive_variables_with_arguments(self):\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(sensitive_args_function_caller, check_for_POST_params=False)\n            self.verify_unsafe_email(sensitive_args_function_caller, check_for_POST_params=False)\n\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(sensitive_args_function_caller, check_for_POST_params=False)\n            self.verify_safe_email(sensitive_args_function_caller, check_for_POST_params=False)\n", "    def test_sensitive_variables_with_same_name(self):\n        @sensitive_variables('password', 'secret')\n            pass\n        reporter = ExceptionReporter(None, None, None, None)\n        reporter.get_traceback_text()\n \n        # Ensure all sensitive variables are handled correctly. \n        self.assertIn('password =', reporter.safe_vars)\n        self.assertIn('secret =', reporter.safe_vars)\n\n"], "sample_1147": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_380": ["def test_aggregation_default_in_subquery(self):\n    subquery = Publisher.objects.filter(\n        name__startswith='Apress'\n    ).annotate(\n        total_pages=Sum('book__pages'),\n        exists=Exists(Book.objects.none()),\n    )\n    result = subquery.aggregate(total_pages=Sum('total_pages', default=0))\n    self.assertEqual(result['total_pages'], 1452)\n", "    def test_aggregation_default_non_numeric_field(self):\n        result = Publisher.objects.aggregate(\n            value=Max('name', default='Zzzzzz'),\n        )\n        self.assertEqual(result['value'], 'Sams')\n", "    def test_aggregation_default_with_q_objects(self):\n        result = Book.objects.filter(\n            Q(rating__gt=4.0) | Q(rating__lt=2.0),\n        ).aggregate(\n            value=Sum('price', default=Decimal('10.00')),\n        )\n        self.assertEqual(result['value'], Decimal('39.98'))\n", "    def test_aggregation_default_not_in_aggregate_nested(self):\n        result = Publisher.objects.annotate(\n            double_awards=F('num_awards') * 2,\n            avg_rating=Avg('book__rating', default=2.5),\n        ).aggregate(Sum('double_awards'))\n        self.assertEqual(result['double_awards__sum'], 40)\n\n", "compilation error"], "sample_924": ["compilation error", "    def check(expected, role, target):\n        classes_ = classes(role, 'span')\n        assert classes_ == expected, f\"Expected classes {expected} for role '{role}' and target '{target}', got {classes_}\"\n", "    def check_xref_consistency(role, tag, expected_classes):\n        classes_found = classes(role, tag)\n        assert classes_found == expected_classes, (\n            f\"Unexpected classes found for role '{role}' and tag '{tag}'.\\n\"\n            f\"Expected: {expected_classes}\\n\"\n            f\"Found: {classes_found}\"\n        )\n", "    def check(actual, expected):\n        assert actual == expected, (\n            f\"Expected class names '{expected}' for target, but got '{actual}'\")\n", "compilation error"], "sample_186": ["    def test_duplicate_fields_in_nested_fieldsets(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            fieldsets = [\n                (None, {\n                    'fields': ['title', ('title', 'album')]\n                }),\n            ]\n\n        errors = MyModelAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"There are duplicate field(s) in 'fieldsets[0][1]'.\",\n                obj=MyModelAdmin,\n                id='admin.E012'\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_list_editable_with_relationship_model(self):\n        class AuthorAdmin(admin.ModelAdmin):\n            list_editable = ['name', 'books__title']\n\n        errors = AuthorAdmin(Author, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'list_editable[1]' refers to 'books__title', which \"\n                \"is not a field on 'admin_checks.Author'.\",\n                obj=AuthorAdmin,\n                id='admin.E121',\n            )\n        ]\n        self.assertEqual(errors, expected) \n", "    def test_invalid_inline_model(self):\n        class InvalidInline(admin.TabularInline):\n            model = 'NonExistentModel'\n\n        class MyAdmin(admin.ModelAdmin):\n            inlines = [InvalidInline]\n\n        errors = MyAdmin(Album, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'model' on 'InvalidInline' refers to 'NonExistentModel', \"\n                \"which is not a model.\",\n                obj=InvalidInline,\n                id='admin.E005',\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_repeated_fieldsets_with_sublists(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            fieldsets = [\n                (None, {\n                    'fields': ['title']\n                }),\n                (None, {\n                    'fields': ['album', ['title']]\n                }),\n            ]\n\n        errors = MyModelAdmin(Song, AdminSite()).check()\n\n        expected = [\n            checks.Error(\n                \"The value of 'fieldsets[1][1][\\\"fields\\\"]' contains the duplicate field 'title'.\",\n                obj=MyModelAdmin,\n                id='admin.E011'\n            )\n        ]\n        self.assertEqual(errors, expected)\n\n", "    def test_fieldsets_with_tuple_in_fields(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            fieldsets = [\n                (None, {\n                    'fields': ('title', 'album')\n                }),\n            ]\n\n        errors = MyModelAdmin(Song, AdminSite()).check()\n        self.assertEqual(errors, [])\n"], "sample_397": ["    def test_multiple_template_loaders(self):\n        templates = [\n            {\n                \"BACKEND\": \"django.template.backends.django.DjangoTemplates\",\n                \"OPTIONS\": {\n                    \"loaders\": [\n                        \"django.template.loaders.filesystem.Loader\",\n                        \"django.template.loaders.app_directories.Loader\",\n                    ],\n                },\n            }\n        ]\n        engines = EngineHandler(templates=templates)\n        engine = engines[\"django\"]\n        self.assertEqual(\n            engine.engine.loaders,\n            [\n                (\"django.template.loaders.filesystem.Loader\", []),\n                (\"django.template.loaders.app_directories.Loader\", []),\n            ],\n        )\n\n", "    def test_template_dirs_prioritized_over_app_dirs(self):\n        templates = [\n            {\n                \"BACKEND\": \"django.template.backends.django.DjangoTemplates\",\n                \"DIRS\": [str(Path(__file__).parent / \"templates\")],\n                \"APP_DIRS\": True,\n                \"OPTIONS\": {},\n            }\n        ]\n        engines = EngineHandler(templates=templates)\n        engine = engines[\"django\"]\n\n        # Template in DIRS should be found first.\n        engine.get_template(\"test_dirs.html\")\n", "    def test_template_loaders_custom_order(self):\n        templates = [\n            {\n                \"BACKEND\": \"django.template.backends.django.DjangoTemplates\",\n                \"OPTIONS\": {\n                    \"loaders\": [\n                        \"django.template.loaders.app_directories.Loader\",\n                        \"django.template.loaders.filesystem.Loader\"\n                    ],\n                },\n            }\n        ]\n        engines = EngineHandler(templates=templates)\n        engine = engines[\"django\"]\n\n        self.assertEqual(\n            engine.engine.loaders,\n            [\n                (\n                    \"django.template.loaders.app_directories.Loader\",\n                    [],\n                ),\n                (\"django.template.loaders.filesystem.Loader\", []),\n            ],\n        )\n", "    def test_loaders_are_list_of_tuples(self):\n        \"\"\"\n        Ensure that loaders configuration allows for a simple list of loaders\n        instead of always requiring a tuple of (loader_class, *args).\n        \"\"\"\n\n        engine = DjangoTemplates({\n            \"DIRS\": [],\n            \"APP_DIRS\": True,\n            \"NAME\": \"django\",\n            \"OPTIONS\": {\"loaders\": [\n                \"django.template.loaders.filesystem.Loader\",\n                \"django.template.loaders.app_directories.Loader\",\n            ]},\n        })\n        self.assertEqual(\n            engine.engine.loaders,\n            [\n                (\n                    \"django.template.loaders.filesystem.Loader\",\n                ),\n                (\n                    \"django.template.loaders.app_directories.Loader\",\n                ),\n            ],\n        )\n", "    def test_template_loader_order(self):\n        # Test that loaders are initialized in the correct order\n        engine = DjangoTemplates(\n            {\n                \"DIRS\": [],\n                \"APP_DIRS\": True,\n                \"NAME\": \"django\",\n                \"OPTIONS\": {\n                    \"loaders\": [\n                        \"django.template.loaders.app_directories.Loader\",\n                        \"django.template.loaders.filesystem.Loader\",\n                    ]\n                },\n            }\n        )\n        self.assertEqual(\n            engine.engine.loaders,\n            [\n                (\n                    \"django.template.loaders.cached.Loader\",\n                    [\n                        \"django.template.loaders.app_directories.Loader\",\n                        \"django.template.loaders.filesystem.Loader\",\n                    ],\n                )\n            ],\n        )\n"], "sample_825": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_706": ["def test_booleans_are_valid(expr: str) -> None:\n    matcher = lambda x: x in (\"True\", \"False\", \"None\")\n    assert evaluate(expr, matcher)\n", "def test_escaped_characters(expr: str, expected: bool) -> None:\n    matcher = {\"a + b\": True,\n               \"a - b\": True,\n               \"a . b\": True,\n               \"a / b\": True,\n               \"a [ b\": True,\n               \"a ] b\": True,\n               }.__getitem__\n    assert evaluate(expr, matcher) is expected\n\n", "def test_invalid_idents() -> None:\n    for ident in (\n        \"1\",\n        \"a 1\",\n        \"a b\",\n        \"+\",\n        \"-\",\n        \".\",\n        \"?\",\n        \"*\",\n        \"\\t\",\n        \" \",\n        \"\\n\",\n    ):\n        with pytest.raises(ParseError):\n            evaluate(ident, {})\n\n", "def test_keywords_as_identifiers(expr: str) -> None:\n    assert evaluate(expr, {expr: True}.__getitem__)\n", "def test_comparison_operators(expr: str, expected: bool) -> None:\n    matcher = {\"a\": 1, \"b\": 2}.__getitem__\n    assert evaluate(expr, matcher) is expected\n\n"], "sample_35": ["def test_minversion():\n\n    assert minversion(\"astropy\", \"4.0\") is True\n    assert minversion(\"astropy\", \"4.1\") is True\n    assert minversion(\"astropy\", \"4.1.1\") is True\n    assert minversion(\"astropy\", \"4.2\") is True\n    assert minversion(\"astropy\", \"5.0\") is True\n\n    assert minversion(\"astropy\", \"4.0\", inclusive=False) is False\n    assert minversion(\"astropy\", \"4.1\", inclusive=False) is True\n    assert minversion(\"astropy\", \"4.1.1\", inclusive=False) is True\n    assert minversion(\"astropy\", \"4.2\", inclusive=False) is True\n    assert minversion(\"astropy\", \"5.0\", inclusive=False) is True\n\n    assert minversion(\"astropy\", \"3.9\") is False\n    assert minversion(\"astropy\", \"3.9\", inclusive=False) is False\n    assert minversion(\"astropy\", \"3.10\") is False\n    assert minversion(\"astropy\", \"3.10\", inclusive=False) is False\n\n    with pytest.raises(ImportError):\n        minversion(\"nonexistent_module\", \"1.0\")\n", "def test_minversion():\n    assert minversion(astropy, '0.4.4')\n    assert minversion(astropy, '0.4.5', inclusive=False)\n    assert not minversion(astropy, '0.4.6')\n    assert not minversion(astropy, '0.4.4', inclusive=False)\n    assert minversion('astropy', '0.4.4')\n    assert minversion('astropy', '0.4.5', inclusive=False)\n    assert not minversion('astropy', '0.4.6')\n    assert not minversion('astropy', '0.4.4', inclusive=False)\n    with pytest.raises(ImportError):\n        minversion('fdasfdsasewrweriopunjlfiurrhujnkflgwhu', '0.4.4')\n\n    assert minversion('astropy', '0.4.dev0')\n    assert minversion('astropy', '0.4.rc1')\n\n    assert minversion('astropy', '0.4', version_path='__version__')\n    assert minversion('astropy', '0.4.1', version_path='__version__')\n", "def test_minversion():\n    assert minversion('astropy', '0.4.4') is True\n    assert minversion('astropy', '0.4.5') is True\n    assert minversion('astropy', '0.4.3') is False\n    assert minversion('astropy', '0.4.3', inclusive=False) is False\n    assert minversion('astropy', '0.4.4', inclusive=False) is True\n\n    assert minversion('astropy.utils', '1.0', version_path='__version__') is True\n\n    with pytest.raises(ImportError):\n        minversion('nonexistent_module', '1.0')\n", "def test_minversion():\n    import astropy\n    assert minversion(astropy, '0.4.4') is True\n    assert minversion(astropy, '0.4.5') is True\n    assert minversion(astropy, '0.4.3') is False\n    assert minversion('astropy', '0.4.3.post1') is False\n\n    assert minversion(astropy, '0.4.4', inclusive=False) is False\n    assert minversion(astropy, '0.4.5', inclusive=False) is True\n    assert minversion(astropy, '0.4.3', inclusive=False) is False\n\n    with pytest.raises(ImportError):\n        minversion('nonexistentmodule', '0.1')\n\n    assert minversion(astropy, '0.4.4', version_path='__version__') is True\n\n", "def test_minversion():\n    assert minversion('astropy', '0.4.4') is True\n    assert minversion('astropy', '0.4.5') is True\n    assert minversion('astropy', '0.4.3') is False\n    assert minversion('astropy', '0.4.3', inclusive=False) is False\n\n    with pytest.raises(ImportError):\n        minversion('not_a_module', '0.1.0')\n    assert minversion(introspection, '0.1.0') is True\n\n    # Test version path\n    assert minversion('astropy', '4.2', version_path='__version__') is True\n    assert minversion('astropy', '4.3', version_path='__version__') is True\n\n"], "sample_483": ["    def test_duplicate_fields_in_nested_fieldsets(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            fieldsets = (\n                (\"Header 1\", {\"fields\": [\"title\", \"album\"]}),\n                (\"Header 2\", {\"fields\": [\"title\", \"album\"]}),\n            )\n\n        errors = MyModelAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The fieldset 'Header 2' contains duplicate field(s): 'title', 'album'.\",\n                obj=MyModelAdmin,\n                id=\"admin.E012\",\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_duplicate_fieldsets_with_different_headlines(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            fieldsets = [\n                (\"Group 1\", {\"fields\": [\"title\"]}),\n                (\"Group 1\", {\"fields\": [\"album\"]}),\n            ]\n\n        errors = MyModelAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"Duplicate fieldset title 'Group 1'. Fieldset titles must be unique.\",\n                obj=MyModelAdmin,\n                id=\"admin.E016\",\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_invalid_fieldsets_type(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            fieldsets = \"invalid\"\n\n        errors = MyModelAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'fieldsets' must be a list or tuple.\",\n                obj=MyModelAdmin,\n                id=\"admin.E007\",\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_check_fieldset_sublists_for_duplicates_across_fieldsets(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            fieldsets = [\n                (None, {\"fields\": [\"title\", \"album\"]}),\n                (None, {\"fields\": [\"title\", \"album\"]}),\n            ]\n\n        errors = MyModelAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"There are duplicate field(s) across 'fieldsets'.\",\n                obj=MyModelAdmin,\n                id=\"admin.E016\",\n            )\n        ]\n        self.assertEqual(errors, expected)\n\n", "    def test_modelform_factory_inheritance(self):\n        class MyModelForm(forms.ModelForm):\n            class Meta:\n                model = Song\n                fields = \"__all__\"\n\n        class MyModelAdmin(admin.ModelAdmin):\n            form = MyModelForm\n\n        errors = MyModelAdmin(Song, AdminSite()).check()\n        self.assertEqual(errors, [])\n\n"], "sample_174": ["    def test_window_frame_range_start_end_unsupported(self):\n        with self.assertRaisesMessage(NotSupportedError, 'This backend only supports UNBOUNDED together with PRECEDING and FOLLOWING.'):\n            self.ops.window_frame_range_start_end(start=-1, end=1)\n\n", "    def test_sql_flush_with_tables(self):\n        Author.objects.create(name='Test author', age=30)\n        Book.objects.create(title='Test book', author=Author.objects.get(name='Test author'))\n        with transaction.atomic():\n            sql_list = connection.ops.sql_flush(no_style(), [Author._meta.db_table, Book._meta.db_table], reset_sequences=True)\n            self.assertNotEqual(sql_list, [])\n            # Assert that the tables are dropped\n            for sql in sql_list:\n                self.assertIn('DROP TABLE', sql.upper())\n", "    def test_sql_flush_with_tables(self):\n        with transaction.atomic():\n            Author.objects.create(name='Test Author')\n            Book.objects.create(title='Test Book', author=Author.objects.get(name='Test Author'))\n        sql_list = connection.ops.sql_flush(no_style(), [Author._meta.db_table, Book._meta.db_table])\n        self.assertGreater(len(sql_list), 0)\n        with connection.cursor() as cursor:\n            cursor.execute('SELECT COUNT(*) FROM %s' % Author._meta.db_table)\n            self.assertEqual(cursor.fetchone()[0], 0)\n            cursor.execute('SELECT COUNT(*) FROM %s' % Book._meta.db_table)\n            self.assertEqual(cursor.fetchone()[0], 0) \n", "    def test_integer_field_range(self):\n        self.assertEqual(self.ops.integer_field_range('PositiveIntegerField'), (1, None))\n", "    def test_sql_flush_with_tables(self):\n        with transaction.atomic():\n            Author.objects.create(name='Author 1')\n            Book.objects.create(title='Book 1', author=Author.objects.get(name='Author 1'))\n            sql = connection.ops.sql_flush(no_style(), [Author._meta.db_table, Book._meta.db_table])\n        self.assertGreater(len(sql), 0)\n"], "sample_997": ["def test_implicit_multiplication_application_fail():\n    inputs = ['sin 2x', 'cot z + csc z', 'sin(2x)']\n    transformations = standard_transformations + (implicit_multiplication_application,)\n    for text in inputs:\n        raises(TokenError, lambda: parse_expr(text, transformations=transformations))\n", "compilation error", "def test_issue_15148():\n    x = Symbol('x')\n    y = Symbol('y')\n    assert parse_expr('x**y**2', evaluate=False) == Pow(x, Pow(y, 2))\n", "compilation error", "    def test_issue_11119():\n        inputs = {\n            '{1 + 2}': '1 + 2',\n            '{1 * 2}': '1 * 2',\n            '{1 / 2}': '1 / 2',\n            '{1 - 2}': '1 - 2',\n            '{1 ** 2}': '1 ** 2',\n        }\n        for text, result in inputs.items():\n            assert parse_expr(text, evaluate=False) == parse_expr(result, evaluate=False)\n\n"], "sample_616": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_587": ["    def test_merge_indexes_override(self):\n        ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]},\n                         indexes={\"x\": xr.IndexVariable(\"x\", [0, 1])})\n        ds2 = xr.Dataset({\"b\": (\"x\", [3, 4]), \"x\": [1, 2]},\n                         indexes={\"x\": xr.IndexVariable(\"x\", [1, 2])})\n        expected = xr.Dataset({\"a\": (\"x\", [1, 2]), \"b\": (\"x\", [3, 4]),\n                               \"x\": [1, 2]},\n                             indexes={\"x\": xr.IndexVariable(\"x\", [1, 2])})\n\n        assert expected.identical(ds1.merge(ds2, join=\"override\"))\n", "    def test_merge_overwrite_vars(self):\n        data = create_test_data()\n        ds1 = data[[\"var1\", \"var2\"]]\n        ds2 = data[[\"var2\", \"var3\"]]\n        expected = data[[\"var1\", \"var2\", \"var3\"]]\n        actual = ds1.merge(ds2, overwrite_vars=[\"var2\"])\n        assert expected.identical(actual)\n\n        actual = ds1.merge(ds2, overwrite_vars=[\"var2\"])\n        assert expected.identical(actual)\n\n", "    def test_merge_overwrite_vars(self):\n        ds1 = xr.Dataset({\"a\": 0, \"b\": 1})\n        ds2 = xr.Dataset({\"a\": 2, \"c\": 3})\n        expected = xr.Dataset({\"a\": 2, \"b\": 1, \"c\": 3})\n\n        actual = ds1.merge(ds2, overwrite_vars=\"a\")\n        assert actual.identical(expected)\n\n        actual = ds1.merge(ds2, overwrite_vars=[\"a\"])\n        assert actual.identical(expected)\n\n        actual = ds1.merge(ds2, overwrite_vars={\"a\": ds2[\"a\"]})\n        assert actual.identical(expected)\n\n        with pytest.raises(ValueError):\n            ds1.merge(ds2, overwrite_vars={\"a\": 2})\n\n", "    def test_merge_identical_coords(self):\n        ds1 = xr.Dataset({\"x\": [1, 2], \"y\": (\"x\", [3, 4])})\n        ds2 = xr.Dataset({\"y\": (\"x\", [5, 6])})\n        with pytest.raises(xr.MergeError):\n            ds1.merge(ds2)\n", "    def test_merge_overwrite_vars(self):\n        ds1 = xr.Dataset({\"a\": 0, \"b\": 1})\n        ds2 = xr.Dataset({\"b\": 2, \"c\": 3})\n        expected = ds1.copy()\n        expected[\"b\"] = ds2[\"b\"]\n        expected[\"c\"] = ds2[\"c\"]\n        actual = ds1.merge(ds2, overwrite_vars=\"b\")\n        assert expected.identical(actual)\n\n        actual = ds1.merge(ds2, overwrite_vars=[\"b\"])\n        assert expected.identical(actual)\n\n        actual = ds1.merge(ds2, overwrite_vars={\"b\": \"b\"})\n        assert expected.identical(actual)\n\n        with pytest.raises(ValueError):\n            ds1.merge(ds2, overwrite_vars=\"d\")\n\n        with pytest.raises(TypeError):\n            ds1.merge(ds2, overwrite_vars=1)\n\n"], "sample_292": ["    def test_csrf_exempt_view(self):\n        \"\"\"\n        Views decorated with @csrf_exempt should bypass CSRF checks.\n        \"\"\"\n        req = self._get_POST_request_with_token()\n        req.COOKIES[settings.CSRF_COOKIE_NAME] = 'invalid'\n        mw = CsrfViewMiddleware(csrf_exempt_view)\n        response = mw.process_view(req, csrf_exempt_view, (), {})\n        self.assertIsNone(response)\n", "    def test_csrf_token_with_spaces_in_referer(self):\n        \"\"\"\n        A referer with spaces should be handled correctly.\n        \"\"\"\n        token = self._csrf_id\n        req = self._get_POST_request_with_token()\n        req._is_secure_override = True\n        req.META['HTTP_REFERER'] = f'https://www.example.com/ some path'\n        req.META['HTTP_HOST'] = 'www.example.com'\n        mw = CsrfViewMiddleware(post_form_view)\n        resp = mw.process_view(req, post_form_view, (), {})\n        self.assertIsNone(resp)\n", "    def test_bare_secret_accepted_and_replaced_with_session(self):\n        \"\"\"\n        The csrf token is reset from a bare secret, even when using sessions.\n        \"\"\"\n        req = self._get_POST_bare_secret_csrf_cookie_request_with_token()\n        mw = CsrfViewMiddleware(token_view)\n        mw.process_request(req)\n        resp = mw.process_view(req, token_view, (), {})\n        self.assertIsNone(resp)\n        resp = mw(req)\n        self.assertIn(settings.CSRF_COOKIE_NAME, resp.cookies, \"Cookie was not reset from bare secret\")\n        csrf_cookie = resp.cookies[settings.CSRF_COOKIE_NAME]\n        self.assertEqual(len(csrf_cookie.value), CSRF_TOKEN_LENGTH)\n        self._check_token_present(resp, csrf_id=csrf_cookie.value)\n", "    def test_csrf_cookie_secure_flag(self):\n        \"\"\"The CSRF cookie should have a secure flag if CSRF_COOKIE_SECURE is True.\"\"\"\n        with self.settings(CSRF_COOKIE_SECURE=True):\n            req = self._get_GET_no_csrf_cookie_request()\n            mw = CsrfViewMiddleware(token_view)\n            mw.process_view(req, token_view, (), {})\n            resp = mw(req)\n            self.assertTrue(resp.cookies[settings.CSRF_COOKIE_NAME]['secure']) \n", "    def test_csrf_cookie_not_secure(self):\n        \"\"\"\n        CSRF cookie should not be marked as secure if CSRF_COOKIE_SECURE is False.\n        \"\"\"\n        req = self._get_GET_no_csrf_cookie_request()\n        mw = CsrfViewMiddleware(ensure_csrf_cookie_view)\n        mw.process_view(req, ensure_csrf_cookie_view, (), {})\n        resp = mw(req)\n        csrf_cookie = resp.cookies.get(settings.CSRF_COOKIE_NAME)\n        self.assertFalse(csrf_cookie.get('secure'))\n\n"], "sample_1050": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_463": ["    def test_addfield_and_renamefield(self):\n        initial_author = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n            ],\n        )\n        author_new_constraints = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"new_name\", models.CharField(max_length=200)),\n                (\"age\", models.IntegerField()),\n            ],\n        )\n        changes = self.get_changes([initial_author], [author_new_constraints])\n\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(\n            changes,\n            \"testapp\",\n            0,\n            [\"AddField\", \"RenameField\"],\n        )\n        self.assertOperationAttributes(\n            changes,\n            \"testapp\",\n            0,\n            1,\n            model_name=\"author\",\n            old_name=\"name\",\n            new_name=\"new_name\",\n        )\n\n", "    def test_operation_with_empty_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.AddField(\n                    \"Person\", \"name\", field=models.CharField(max_length=200)\n                ),\n            ]\n\n        migration = Migration(\"0001_initial\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"person_name\")\n\n", "    def test_model_rename_and_index_together(self):\n        before = ModelState(\n            \"otherapp\",\n            \"Book\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n                (\"title\", models.CharField(max_length=200)),\n            ],\n            {\n                \"index_together\": {(\"author\", \"title\")},\n            },\n        )\n        after = ModelState(\n            \"otherapp\",\n            \"ReadingMaterial\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n                (\"title\", models.CharField(max_length=200)),\n            ],\n            {\n                \"index_together\": {(\"author\", \"title\")},\n            },\n        )\n        changes = self.get_changes([AutodetectorTests.author_empty, before],\n                                  [AutodetectorTests.author_empty, after],\n                                  MigrationQuestioner({\"ask_rename\": True}))\n        self.assertNumberMigrations(changes, \"otherapp\", 1)\n        self.assertOperationTypes(\n            changes,\n            \"otherapp\",\n            0,\n            [\"RenameModel\", \"AlterIndexTogether\"],\n        )\n        self.assertOperationAttributes(\n            changes,\n            \"otherapp\",\n            0,\n            0,\n            old_name=\"Book\",\n            new_name=\"ReadingMaterial\",\n        )\n        self.assertOperationAttributes(\n            changes,\n            \"otherapp\",\n            0,\n            1,\n            name=\"ReadingMaterial\",\n            index_together={(\"author\", \"title\")},\n        )\n", "    def test_multiple_fields_in_index_together(self):\n        book_index_together_multi = ModelState(\n            \"otherapp\",\n            \"Book\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n                (\"title\", models.CharField(max_length=200)),\n                (\"genre\", models.CharField(max_length=50)),\n            ],\n            {\n                \"index_together\": {(\"author\", \"title\", \"genre\")},\n            },\n        )\n        changes = self.get_changes(\n            [AutodetectorTests.author_empty, self.book_index_together],\n            [AutodetectorTests.author_empty, book_index_together_multi],\n        )\n        self.assertNumberMigrations(changes, \"otherapp\", 1)\n        self.assertOperationTypes(\n            changes,\n            \"otherapp\",\n            0,\n            [\"AlterIndexTogether\"],\n        )\n        self.assertOperationAttributes(\n            changes,\n            \"otherapp\",\n            0,\n            0,\n            name=\"book\",\n            index_together={(\"author\", \"title\", \"genre\")},\n        )\n", "    def test_renamemodel_with_related_fields(self):\n        before = [\n            ModelState(\n                \"testapp\",\n                \"Article\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"title\", models.CharField(max_length=200)),\n                ],\n            ),\n            ModelState(\n                \"testapp\",\n                \"Comment\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"article\", models.ForeignKey(\"testapp.Article\", models.CASCADE)),\n                    (\"text\", models.TextField()),\n                ],\n            ),\n        ]\n        after = [\n            ModelState(\n                \"testapp\",\n                \"Post\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"title\", models.CharField(max_length=200)),\n                ],\n            ),\n            ModelState(\n                \"testapp\",\n                \"Comment\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"post\", models.ForeignKey(\"testapp.Post\", models.CASCADE)),\n                    (\"text\", models.TextField()),\n                ],\n            ),\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(\n            changes,\n            \"testapp\",\n            0,\n            [\"RenameModel\", \"RenameField\"],\n        )\n\n"], "sample_438": ["    def test_get_prefetch_queryset_with_filter(self):\n        question = Question.objects.create(text=\"What is the meaning of life?\")\n        post = Post.objects.create(title=\"Answer\", parent=question)\n        Post.objects.create(title=\"Another Answer\")\n\n        qs = Answer.objects.prefetch_related(\"question\").filter(question__text=\"What is the meaning of life?\")\n        self.assertEqual(qs.first().question.text, \"What is the meaning of life?\")\n", "    def test_get_content_type_with_content_type_id(self):\n        answer = Answer.objects.create(text=\"Answer\", question_id=1, content_type_id=1)\n        content_type = answer.question.get_content_type(content_type_id=1)\n\n        self.assertEqual(content_type.pk, 1)\n", "    def test_get_prefetch_queryset_select_related(self):\n        post = Post.objects.create(title=\"Answer\", parent=Question.objects.create(text=\"Who?\"))\n        Answer.objects.create(text=\"Another Answer\", question=post.parent)\n\n        with self.assertNumQueries(3):\n            answers = Answer.objects.select_related(\"question\").all()\n            for answer in answers:\n                answer.question\n", "    def test_get_content_type_with_arguments(self):\n        question = Question.objects.create(text=\"Who?\")\n        post = Post.objects.create(title=\"Answer\", parent=question)\n        self.assertEqual(\n            post.question.get_content_type(instance=post),\n            ContentType.objects.get_for_model(question),\n        )\n", "    def test_get_prefetch_queryset_with_filter(self):\n        question = Question.objects.create(text=\"Test question\")\n        post = Post.objects.create(title=\"Test post\", parent=question)\n\n        with self.assertNumQueries(1):\n            # Prefetching should only execute one query\n            results = Answer.objects.prefetch_related(\"question\").all()\n        self.assertEqual(len(results), 0)\n"], "sample_989": ["def test_issue_10540():\n    x = Symbol('x', real=True)\n    assert (I*x).is_real is False\n    assert (I*x).is_imaginary is True\n    assert (2*I*x).is_real is False\n    assert (2*I*x).is_imaginary is True\n", "compilation error", "compilation error", "compilation error", "    def test_issue_10467():\n        x = Symbol('x', positive=True)\n        assert (x**2).is_positive == True\n        x = Symbol('x')\n        assert (x**2).is_positive is None\n        x = Symbol('x', negative=True)\n        assert (x**2).is_positive == True\n    \n"], "sample_760": ["    def test_scorer_multiclass_empty_labels():\n        # Test scorers handle empty label sets correctly.\n        X = np.zeros((10, 2))\n        y = np.array([], dtype=int)\n        clf = DecisionTreeClassifier()\n        clf.fit(X, y)\n\n        for scorer_name in ['accuracy', 'f1_micro', 'f1_macro', 'precision_micro',\n                           'precision_macro', 'recall_micro', 'recall_macro']:\n            scorer = get_scorer(scorer_name)\n            score = scorer(clf, X, y)\n            assert score == 0.0 or np.isnan(score)\n", "    def test_make_scorer_with_kwargs():\n        # Test if make_scorer can handle additional keyword arguments\n            return kwargs['my_arg']\n\n        scorer = make_scorer(my_scorer, my_arg=42)\n        score = scorer(None, None, y_true=None, y_pred=None)\n        assert score == 42\n", "def test_cluster_scorer_memmap_input(name):\n    # Non-regression test for #6147: some score functions would\n    # return singleton memmap when computed on memmap data instead of scalar\n    # float values.\n    check_scorer_memmap(name)\n", "compilation error", "    def test_cluster_scorers_memmap_input(name):\n        # Non-regression test for #6147: some score functions would\n        # return singleton memmap when computed on memmap data instead of scalar\n        # float values.\n        scorer, estimator = SCORERS[name], ESTIMATORS[name]\n        score = scorer(estimator, X_mm, y_mm)\n        assert isinstance(score, numbers.Number), name\n"], "sample_325": ["    def test_attribute_override(self):\n        class CustomForm(Form):\n            default_renderer = CustomRenderer()\n\n        form = CustomForm(renderer=DjangoTemplates())\n        self.assertEqual(form.renderer, DjangoTemplates())\n", "    def test_attribute_instance_override_kwarg(self):\n        class CustomForm(Form):\n            default_renderer = DjangoTemplates()\n\n        custom = CustomRenderer()\n        form = CustomForm(renderer=custom)\n        self.assertEqual(form.renderer, custom)\n", "    def test_attribute_class_instance(self):\n        class CustomForm(Form):\n            default_renderer = CustomRenderer()\n\n        form = CustomForm()\n        self.assertEqual(form.renderer, CustomForm.default_renderer)\n", "    def test_attribute_class_inheritance(self):\n        class ParentForm(Form):\n            default_renderer = CustomRenderer\n\n        class ChildForm(ParentForm):\n            pass\n\n        form = ChildForm()\n        self.assertIsInstance(form.renderer, ParentForm.default_renderer)\n", "    def test_attribute_subclassing(self):\n        class BaseForm(Form):\n            default_renderer = DjangoTemplates()\n\n        class CustomForm(BaseForm):\n            pass\n\n        form = CustomForm()\n        self.assertEqual(form.renderer, CustomForm.default_renderer)\n"], "sample_815": ["compilation error", "def test_balanced_accuracy_score_empty():\n    assert_equal(balanced_accuracy_score([], []), 0.0)\n    assert_equal(balanced_accuracy_score([0], []), 0.0)\n    assert_equal(balanced_accuracy_score([], [0]), 0.0)\n\n", "def test_balanced_accuracy_score_multiclass():\n    y_true = [0, 1, 2, 0, 1, 2]\n    y_pred = [0, 2, 1, 0, 0, 2]\n\n    balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n\n    assert balanced_accuracy == pytest.approx(0.33333333)\n", "def test_balanced_accuracy_score_empty():\n    assert balanced_accuracy_score([], []) == 0\n", "compilation error"], "sample_129": ["    def test_decimal_rounding(self):\n        with localcontext() as ctx:\n            ctx.prec = 3\n            self.assertEqual(floatformat(Decimal('1.2345'), 2), '1.23')\n            self.assertEqual(floatformat(Decimal('1.2355'), 2), '1.24')\n", "    def test_localcontext(self):\n        with localcontext() as ctx:\n            ctx.prec = 2\n            self.assertEqual(floatformat(Decimal('1.33333333'), 0), '1.33')\n", "    def test_floatformat_with_context(self):\n        with localcontext() as ctx:\n            ctx.prec = 28\n            self.assertEqual(floatformat(Decimal('1.1234567890123456789012345678'), 10), '1.1234567890')\n", "    def test_decimal_precision(self):\n        with localcontext() as ctx:\n            ctx.prec = 28\n            self.assertEqual(floatformat(Decimal('1.2345678901234567890123456789'), 10), '1.2345678901')\n", "    def test_floatformat_with_localcontext(self):\n        with localcontext() as ctx:\n            ctx.prec = 28\n            self.assertEqual(floatformat(Decimal('1.2345678901234567890123456789'), 2), '1.23')\n"], "sample_860": ["compilation error", "def test_check_array_dtype(dtype):\n    X = np.array([[1, 2], [3, 4]], dtype=dtype)\n    X_checked = check_array(X, dtype=dtype)\n    assert X_checked.dtype == dtype\n\n    # check that dtype is preserved if already correct\n    X_checked = check_array(X, dtype=None)\n    assert X_checked.dtype == dtype\n", "compilation error", "def test_check_array_dtypes():\n    X = np.array([[1, 2, 3], [4, 5, 6]], dtype='float32')\n    X_checked = check_array(X, dtype='numeric', force_all_finite=False)\n    assert X_checked.dtype == 'float32'\n\n    X_checked = check_array(X, dtype=np.float64, force_all_finite=False)\n    assert X_checked.dtype == 'float64'\n\n    X_checked = check_array(X, dtype='int', force_all_finite=False)\n    assert X_checked.dtype == 'int64'\n\n    with pytest.raises(TypeError):\n        check_array(X, dtype='object', force_all_finite=False) \n", "compilation error"], "sample_187": ["    def test_format_lazy_string(self):\n        self.assertEqual(format_lazy('Hello {}!', 'world'), 'Hello world!')\n        self.assertEqual(format_lazy('{a} {b} {c}', a='a', b='b', c='c'), 'a b c')\n        self.assertEqual(format_lazy(lazystr('Hello {}!'), 'world'), 'Hello world!')\n", "    def test_camel_case_to_spaces(self):\n        self.assertEqual(text.camel_case_to_spaces('thisIsATestString'), 'this is a test string')\n        self.assertEqual(text.camel_case_to_spaces('ThisIsATestString'), 'this is a test string')\n        self.assertEqual(text.camel_case_to_spaces('testString'), 'test string')\n        self.assertEqual(text.camel_case_to_spaces(''), '')\n        self.assertEqual(text.camel_case_to_spaces('singleWord'), 'single word')\n        self.assertEqual(text.camel_case_to_spaces('123Test'), '123 test')\n        self.assertEqual(text.camel_case_to_spaces(lazystr('thisIsATestString')), 'this is a test string')\n", "    def test_camel_case_to_spaces(self):\n        self.assertEqual(text.camel_case_to_spaces('camelCase'), 'camel case')\n        self.assertEqual(text.camel_case_to_spaces('already_spaced'), 'already spaced')\n        self.assertEqual(text.camel_case_to_spaces('SomeLongVariableName'), 'some long variable name')\n        self.assertEqual(text.camel_case_to_spaces('Acronym'), 'acronym')\n        self.assertEqual(text.camel_case_to_spaces('DoubleAcronym'), 'double acronym')\n\n\n", "    def test_camel_case_to_spaces(self):\n        self.assertEqual(text.camel_case_to_spaces('simpleTest'), 'simple test')\n        self.assertEqual(text.camel_case_to_spaces('someVariableName'), 'some variable name')\n        self.assertEqual(text.camel_case_to_spaces('alreadySpaces'), 'already spaces')\n        self.assertEqual(text.camel_case_to_spaces(''), '')\n        self.assertEqual(text.camel_case_to_spaces('Single'), 'single')\n        self.assertEqual(text.camel_case_to_spaces(lazystr('simpleTest')), 'simple test')\n\n", "    def test_camel_case_to_spaces(self):\n        self.assertEqual(text.camel_case_to_spaces('someCamelCaseString'), 'some camel case string')\n        self.assertEqual(text.camel_case_to_spaces('SomeCamelCaseString'), 'some camel case string')\n        self.assertEqual(text.camel_case_to_spaces('already spaces'), 'already spaces')\n        self.assertEqual(text.camel_case_to_spaces('some_snake_case'), 'some snake case')\n        self.assertEqual(text.camel_case_to_spaces('Some1Number'), 'some1 number')\n        self.assertEqual(text.camel_case_to_spaces('SomeCamelCaseStringWithNumbers123'),\n                         'some camel case string with numbers123')\n        self.assertEqual(text.camel_case_to_spaces('a'), 'a')\n        self.assertEqual(text.camel_case_to_spaces(''), '')\n"], "sample_678": ["def test_cleanup_numbered_dir_with_lock(tmp_path, monkeypatch):\n    \"\"\"Test that cleanup_numbered_dir correctly removes numbered directories\n    when a lock is present.\"\"\"\n    root = tmp_path / 'root'\n    root.mkdir()\n    p1 = root / 'numbered_dir_1'\n    p1.mkdir()\n\n        return True\n\n    monkeypatch.setattr(_pytest.pathlib, 'ensure_deletable', mock_ensure_deletable)\n    cleanup_numbered_dir(root, 'numbered_dir_', 1, 0)\n    assert not p1.exists()\n", "    def test_ensure_deletable_with_lock_permission(tmp_path, monkeypatch):\n        path = tmp_path / \"temp-1\"\n        path.mkdir()\n        lock_path = get_lock_path(path)\n        lock_path.touch()\n        os.chmod(str(lock_path), 0o000)\n        assert not ensure_deletable(path, 0)\n        monkeypatch.setattr(os, \"chmod\", lambda *args: None)\n\n        assert not ensure_deletable(path, 0)\n", "def test_ensure_deletable_with_permission_error(tmp_path, monkeypatch):\n    \"\"\"Ensures that ensure_deletable handles PermissionError correctly.\"\"\"\n    path = tmp_path / \"temp\"\n\n        return os.stat_result((os.S_IFREG | 0o444), 0, 0, 0, 0, 0, 0, 0, 0, 0)\n\n    monkeypatch.setattr(Path, \"stat\", mock_stat)\n    with monkeypatch.context() as m:\n        m.setattr(os, \"chmod\", lambda p, mode: None)\n        \n        lock_path = get_lock_path(path)\n\n        # Simulate lock creation\n        lock_path.touch()\n        assert not ensure_deletable(path, 0) \n", "def test_ensure_deletable_with_permission_error(tmp_path, monkeypatch):\n    path = tmp_path / \"temp-1\"\n    path.mkdir()\n    lock_path = get_lock_path(path)\n    lock_path.touch()\n    monkeypatch.setattr(Path, \"unlink\", lambda self: raise OSError(\"Permission denied\"))\n\n    assert not ensure_deletable(path, 0)\n    assert lock_path.exists()\n\n", "    def test_ensure_deletable_with_lock(tmp_path, monkeypatch):\n        path = tmp_path / \"subdir\"\n        path.mkdir()\n\n            return os.stat_result((123456789, 0, 0, 0, 0, 0, 0, 0, 0, 0))\n\n        monkeypatch.setattr(Path, \"stat\", mock_stat)\n        lock_path = get_lock_path(path)\n        lock_path.touch()\n\n        assert not ensure_deletable(path, time() - 10)\n        assert path.exists()\n\n"], "sample_276": ["    def test_simplify_regex(self):\n        self.assertEqual(simplify_regex(r'^myapp/items/(\\d+)/$'), '/myapp/items/<int:id>/')\n        self.assertEqual(simplify_regex(r'^myapp/items/(?P<item_id>\\d+)/edit/$'), '/myapp/items/<int:item_id>/edit/')\n        self.assertEqual(simplify_regex(r'^admin/blog/post/(?P<post_id>\\d+)/change/$'), '/admin/blog/post/<int:post_id>/change/')\n        self.assertEqual(simplify_regex(r'^myapp/articles/(?P<article_id>\\d+)/comments/(?P<comment_id>\\d+)/delete/$'), '/myapp/articles/<int:article_id>/comments/<int:comment_id>/delete/')\n\n", "    def test_model_field_display_order(self):\n        \"\"\"\n        Model fields should be displayed in the order they are defined in the model.\n        \"\"\"\n        response = self.client.get(reverse('django-admindocs-models-detail', args=['admin_docs', 'company']))\n        fields = response.context_data['fields']\n\n        self.assertEqual(fields[0]['name'], 'name')\n        self.assertEqual(fields[1]['name'], 'founded')\n        self.assertEqual(fields[2]['name'], 'website')\n", "    def test_model_with_abstract_base_class(self):\n        response = self.client.get(reverse('django-admindocs-models-detail', args=['admin_docs', 'AbstractPerson']))\n        self.assertContains(response, '<h1>admin_docs.AbstractPerson</h1>', html=True)\n        self.assertContains(response, '<td>first name</td>', html=True)\n        self.assertContains(response, '<td>last name</td>', html=True)\n", "    def test_model_detail_no_docstring(self):\n        response = self.client.get(reverse('django-admindocs-models-detail', args=['admin_docs', 'Family']))\n        self.assertContains(response, '<h1>admin_docs.Family</h1>', html=True)\n        self.assertNotContains(response, '<p>DESCRIPTION</p>', html=True)\n", "    def test_model_docstring_with_empty_description(self):\n        response = self.client.get(reverse('django-admindocs-models-detail', args=['admin_docs', 'empty_description_model']))\n        self.assertContains(response, '<h2 class=\"subhead\"><p> </p></h2>')\n"], "sample_1": ["    def model(x):\n        return x", "    def model_b(x):\n        return x + 1\n", "    def my_custom_model(x):\n        return x\n", "compilation error", "        def __init__(self, factor=1.0):\n            super().__init__()\n            self.factor = factor\n"], "sample_143": ["    def test_camel_case_to_spaces(self):\n        self.assertEqual(text.camel_case_to_spaces('ThisIsACamelCaseString'), 'this is a camel case string')\n        self.assertEqual(text.camel_case_to_spaces('already_spaced'), 'already spaced')\n        self.assertEqual(text.camel_case_to_spaces('ThisIs123'), 'this is 123') \n        self.assertEqual(text.camel_case_to_spaces('ThisIsA_CamelCaseString'), 'this is a camel case string')\n        self.assertEqual(text.camel_case_to_spaces(lazystr('ThisIsACamelCaseString')), 'this is a camel case string')\n\n", "    def test_camel_case_to_spaces(self):\n        self.assertEqual(text.camel_case_to_spaces('thisIsATest'), 'this is a test')\n        self.assertEqual(text.camel_case_to_spaces('ThisIsATest'), 'this is a test')\n        self.assertEqual(text.camel_case_to_spaces('test'), 'test')\n        self.assertEqual(text.camel_case_to_spaces(lazystr('thisIsATest')), 'this is a test')\n", "    def test_camel_case_to_spaces(self):\n        self.assertEqual(text.camel_case_to_spaces('ThisIsACamelCaseString'), 'this is a camel case string')\n        self.assertEqual(text.camel_case_to_spaces('already_spaced'), 'already spaced')\n        self.assertEqual(text.camel_case_to_spaces('ThisIsACamelCaseStringWithNumbers123'), 'this is a camel case string with numbers 123')\n        self.assertEqual(text.camel_case_to_spaces(lazystr('ThisIsACamelCaseString')), 'this is a camel case string')\n\n", "    def test_camel_case_to_spaces(self):\n        self.assertEqual(text.camel_case_to_spaces('camelCase'), 'camel case')\n        self.assertEqual(text.camel_case_to_spaces('SnakeCase'), 'snake case')\n        self.assertEqual(text.camel_case_to_spaces('already_spaced'), 'already spaced')\n        self.assertEqual(text.camel_case_to_spaces('ThisIsASentence'), 'this is a sentence')\n        self.assertEqual(text.camel_case_to_spaces('Acronym'), 'acronym')\n        self.assertEqual(text.camel_case_to_spaces('HTML'), 'html')\n\n", "    def test_camel_case_to_spaces(self):\n        self.assertEqual(text.camel_case_to_spaces('someRandomString'), 'some random string')\n        self.assertEqual(text.camel_case_to_spaces('SomeRandomString'), 'some random string')\n        self.assertEqual(text.camel_case_to_spaces('some_random_string'), 'some random string')\n        self.assertEqual(text.camel_case_to_spaces('someRandomStringWithNumbers123'), 'some random string with numbers 123')\n\n"], "sample_214": ["    def test_key_transform_order_by(self):\n        qs = NullableJSONModel.objects.filter(\n            value__isnull=False,\n        ).annotate(\n            key=KeyTransform('d', 'value'),\n        ).filter(key__isnull=False).order_by('key__0')\n        self.assertSequenceEqual(qs, [self.objs[4]])\n", "    def test_key_transform_contained_by(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__baz__contained_by={'a': 'b', 'c': 'd'},\n            ),\n            [self.objs[7]],\n        )\n", "    def test_key_transform_nested_key_exact(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__baz__a=KeyTransform('c', 'value')\n            ),\n            [self.objs[7]],\n        )\n", "    def test_key_transform_expression_with_f(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__d__0__isnull=False.\n            ).annotate(\n                key=KeyTransform('d', 'value'),\n                chain=F('key') + KeyTransform('0', 'key'),\n            ).filter(chain__contains=['e', 'e']),\n            [self.objs[4]],\n        )\n", "    def test_key_transform_field_lookup(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__d__0__isnull=False,\n                value__f=KeyTransform('f', KeyTransform('1', KeyTransform('d', 'value'))),\n            ),\n            [self.objs[4]],\n        )\n"], "sample_881": ["compilation error", "compilation error", "    def test_label_ranking_avg_precision_score_empty_y_true():\n        # Test that label_ranking_average_precision_score handles empty y_true arrays\n        y_true = np.array([])\n        y_score = np.array([])\n        with pytest.raises(ValueError, match=\"y_true contains no positive labels\"):\n            label_ranking_average_precision_score(y_true, y_score)\n", "compilation error", "compilation error"], "sample_205": ["    def test_update_error_dict(self):\n        error_dict = {}\n        error = ValidationError({'field1': 'message1'})\n\n        error.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['message1']})\n\n        error = ValidationError({'field1': ['message1', 'message2']})\n        error.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['message1', 'message2']})\n\n        error = ValidationError({'field2': 'message3'})\n\n        error.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['message1', 'message2'], 'field2': ['message3']})\n        \n", "    def test_empty_validationerror(self):\n        error = ValidationError()\n        self.assertEqual(error.messages, [])\n        self.assertEqual(str(error), '[]')\n        self.assertEqual(repr(error), 'ValidationError([])')\n", "    def test_update_error_dict(self):\n        error_dict = {}\n        exception = ValidationError(error_dict)\n        exception.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {})\n        error_dict['field1'] = ['E1', 'E2']\n        exception = ValidationError(error_dict)\n        exception.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['E1', 'E2']})\n        error_dict['field2'] = ['E3', 'E4']\n        exception = ValidationError(error_dict)\n        exception.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['E1', 'E2'], 'field2': ['E3', 'E4']})\n        self.assertEqual(exception.error_dict, error_dict)\n\n        error_dict2 = {}\n        exception = ValidationError('message')\n        exception.update_error_dict(error_dict2)\n        self.assertEqual(error_dict2, { NON_FIELD_ERRORS: ['message']})\n", "    def test_update_error_dict(self):\n        error = ValidationError({'field1': 'message1', 'field2': 'message2'})\n        error_dict = {}\n        error.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['message1'], 'field2': ['message2']})\n\n        error = ValidationError('message')\n        error.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'__all__': ['message']})\n\n        error = ValidationError([ValidationError('message1'), ValidationError('message2')])\n        error.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'__all__': ['message1', 'message2']})\n\n        error = ValidationError({'field1': ['error1']})\n        error.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ['error1']})\n", "    def test_error_dict(self):\n        error = ValidationError({'field1': ['E1', 'E2']})\n        self.assertEqual(error.error_dict, {'field1': ['E1', 'E2']})\n        error = ValidationError({'field1': ValidationError('E1'), 'field2': 'E2'})\n        self.assertEqual(error.error_dict, {'field1': ['E1'], 'field2': ['E2']})\n        error.update_error_dict({'field3': 'E3'})\n        self.assertEqual(error.error_dict, {'field1': ['E1'], 'field2': ['E2'], 'field3': ['E3']})\n\n"], "sample_841": ["    def test_ridge_regression_empty_data(solver):\n        X = np.array([])\n        y = np.array([])\n        assert_raises(ValueError, ridge_regression, X, y, solver=solver)\n", "    def test_ridge_cv_n_jobs(self):\n        X, y = self.make_data(n_samples=50, n_features=10, random_state=0)\n        ridge = RidgeCV(alphas=[0.1, 1.0, 10.0], cv=5, n_jobs=-1)\n        ridge.fit(X, y)\n        assert isinstance(ridge.best_score_, float)\n", "def test_ridge_regression_with_precomputed_kernel(solver):\n    rng = np.random.RandomState(42)\n    n_samples, n_features = 10, 5\n    X = rng.rand(n_samples, n_features)\n    y = rng.randn(n_samples)\n    kernel = np.dot(X, X.T)\n    coef = ridge_regression(kernel, y, alpha=1.0, solver=solver)\n    assert coef.shape == (n_samples,)\n\n", "    def test_ridge_cv_multioutput_regression(self):\n        rng = np.random.RandomState(42)\n        n_samples, n_features, n_targets = 10, 5, 2\n        X = rng.randn(n_samples, n_features)\n        y = rng.randn(n_samples, n_targets)\n        ridge_cv = RidgeCV(cv=5)\n        ridge_cv.fit(X, y)\n        assert ridge_cv.coef_.shape == (n_targets, n_features)\n", "    def test_ridgecv_sample_weight_empty(self):\n        # Test that RidgeCV handles empty sample weights\n        X, y = self.X_diabetes, self.y_diabetes\n        \n        ridgecv = RidgeCV(alphas=[1.0], cv=5)\n        sample_weight = np.array([]).astype(np.float64)\n        with pytest.warns(UserWarning, match=\"The sample weight array is empty\"):\n            ridgecv.fit(X, y, sample_weight=sample_weight)\n"], "sample_1112": ["compilation error", "def test_is_palindromic():\n    assert is_palindromic(121) is True\n    assert is_palindromic(12321) is True\n    assert is_palindromic(12345) is False\n    assert is_palindromic(-121) is True\n    assert is_palindromic(-12321) is True\n    assert is_palindromic(123456) is False\n    assert is_palindromic(0) is True\n    assert is_palindromic(0o121) is False\n    assert is_palindromic(0o121, 8) is True\n", "def test_is_palindromic():\n    assert is_palindromic(121) is True\n    assert is_palindromic(12321) is True\n    assert is_palindromic(123) is False\n    assert is_palindromic(121, 2) is True\n    assert is_palindromic(111, 3) is True\n    assert is_palindromic(12321, 10) is True\n    assert is_palindromic(0o121, 8) is False\n", "compilation error", "def test_is_palindromic():\n    assert is_palindromic(121) is True\n    assert is_palindromic(12321) is True\n    assert is_palindromic(12345) is False\n    assert is_palindromic(0) is True\n    assert is_palindromic(-121) is True\n    assert is_palindromic(-12321) is True\n    assert is_palindromic(0o121, 8) is True\n    assert is_palindromic(0o12321, 8) is True\n    assert is_palindromic(0o12345, 8) is False\n"], "sample_286": ["    def test_refresh_fk_and_update_fields(self):\n        s1 = SelfRef.objects.create()\n        s2 = SelfRef.objects.create()\n        s3 = SelfRef.objects.create(selfref=s1)\n        s3.selfref = s2\n        with self.assertNumQueries(2):\n            s3.refresh_from_db(update_fields=['selfref'])\n        self.assertEqual(s3.selfref, s2)\n", "    def test_refresh_many_to_many_field(self):\n        a1 = Article.objects.create(headline='Article 1')\n        a2 = Article.objects.create(headline='Article 2')\n        t1 = Tag.objects.create(name='Django')\n        t2 = Tag.objects.create(name='Python')\n        a1.tags.add(t1, t2)\n        a2.tags.add(t1)\n        a2.refresh_from_db()\n        self.assertEqual(len(a2.tags.all()), 1)\n", "    def test_refresh_instance_from_empty_queryset(self):\n        a = Article.objects.create(\n            headline='Parrot programs in Python',\n            pub_date=datetime(2005, 7, 28),\n        )\n        Article.objects.all().delete()\n        msg = \"Instance is not loaded\"\n        with self.assertRaisesMessage(ObjectDoesNotExist, msg):\n            a.refresh_from_db()\n\n", "    def test_refresh_prefetch_related(self):\n        a1 = Article.objects.create(headline='A1', pub_date=datetime.now())\n        a2 = Article.objects.create(headline='A2', pub_date=datetime.now())\n        s1 = SelfRef.objects.create(article=a1)\n        s2 = SelfRef.objects.create(article=a1)\n        s3 = SelfRef.objects.create(article=a2)\n        self.assertQuerysetEqual(s1.article.all(), [a1])\n        s1.refresh_from_db(fields=['selfref'])\n        self.assertEqual(s1.article_id, a1.pk)\n        # This should not trigger a new query because the related `article`\n        # has been prefetched during the previous query\n        s1.article.headline\n", "    def test_refresh_m2m(self):\n        a = Article.objects.create(pub_date=datetime.now())\n        t1 = Tag.objects.create(name='python')\n        t2 = Tag.objects.create(name='django')\n        a.tags.add(t1, t2)\n        a.refresh_from_db()\n        self.assertEqual(len(a.tags.all()), 2)\n        t3 = Tag.objects.create(name='testing')\n        a.tags.add(t3)\n        a.refresh_from_db()\n        self.assertEqual(len(a.tags.all()), 3)\n"], "sample_976": ["compilation error", "def test_var():\n    a, b = var('a b')\n    assert a.name == 'a'\n    assert b.name == 'b'\n    assert (a, b) == symbols('a b')\n\n    x, y, z = var('x y z', real=True)\n    assert x.is_real and y.is_real and z.is_real\n    assert (x, y, z) == symbols('x y z', real=True)\n\n    raises(TypeError, lambda: var())\n    raises(TypeError, lambda: var(1))\n", "    def test_symbols_cls():\n        x, y, z = symbols('x,y,z')\n        assert isinstance(x, Symbol)\n        assert isinstance(y, Symbol)\n        assert isinstance(z, Symbol)\n\n        f, g, h = symbols('f,g,h', cls=Function)\n        assert isinstance(f, Function)\n        assert isinstance(g, Function)\n        assert isinstance(h, Function)\n\n        \n        x, y, z = symbols('x,y,z', cls=Wild)\n        assert isinstance(x, Wild)\n        assert isinstance(y, Wild)\n        assert isinstance(z, Wild)\n\n", "compilation error", "def test_Symbol_hash():\n    a = Symbol(\"a\")\n    b = Symbol(\"b\")\n    assert hash(a) != hash(b)\n    assert hash(a) == hash(Symbol(\"a\"))\n    c = Symbol(\"a\", commutative=False)\n    assert hash(a) != hash(c)\n\n"], "sample_379": ["    def test_safe_string_concatenation(self):\n        s1 = mark_safe('a')\n        s2 = mark_safe('b')\n        s3 = 'c'\n        self.assertIsInstance(s1 + s2, SafeString)\n        self.assertIsInstance(s1 + s3, str)\n        self.assertEqual(s1 + s2, 'ab')\n        self.assertEqual(s1 + s3, 'ac')\n", "    def test_safe_string_addition_with_different_types(self):\n        self.assertEqual(SafeString('a') + 'b', 'ab')\n        self.assertEqual('a' + SafeString('b'), 'ab')\n        self.assertIsInstance(SafeString('a') + 'b', str)\n        self.assertIsInstance('a' + SafeString('b'), str)\n", "    def test_safe_string_concatenation_with_unsafe_string(self):\n        safe_str = SafeString('<p>Safe String</p>')\n        unsafe_str = '<script>alert(\"XSS\")</script>'\n        result = safe_str + unsafe_str\n        self.assertFalse(isinstance(result, SafeString))\n        self.assertIn('<script>alert(\"XSS\")</script>', result)\n\n", "    def test_safe_string_add_safe_string_and_string(self):\n        s = SafeString('a') + 'b'\n        self.assertEqual(s, 'ab')\n        self.assertIsInstance(s, SafeString)\n", "    def test_safetext_inheritance(self):\n        self.assertTrue(issubclass(SafeText, SafeString))\n"], "sample_300": ["    def test_exclude_conditional_join(self):\n        query = Query(Item)\n        filter_expr = ~Func('note__note', output_field=BooleanField())\n        msg = 'Joined field references are not permitted in this query'\n        with self.assertRaisesMessage(FieldError, msg):\n            query.build_where(filter_expr)\n\n", "    def test_add_filter_after_clone(self):\n        query = Query(Item)\n        query.add_filter(Q(name='test'))\n        clone = query.clone()\n        clone.add_filter(Q(created__lt=datetime(2023, 1, 1)))\n        self.assertEqual(len(query.where.children), 1)\n        self.assertEqual(len(clone.where.children), 2)\n\n", "    def test_filter_function_with_args(self):\n        query = Query(Item)\n        func = Func(F('name'), function='LENGTH', template='%(function)s(%(expressions)s)')\n        where = query.build_where(Q(name__length__gt=3))\n\n        self.assertEqual(where.children[0].lhs.func, func)\n        self.assertEqual(where.children[0].rhs, 3)\n", "    def test_filter_related_field_isnull(self):\n        query = Query(Item)\n        where = query.build_where(Q(creator__isnull=True))\n        isnull = where.children[0]\n        self.assertIsInstance(isnull, IsNull)\n        self.assertIsInstance(isnull.lhs, Col)\n        self.assertEqual(isnull.lhs.target, Item._meta.get_field('creator'))\n", "    def test_exclude_fields(self):\n        query = Query(Author)\n        query.defer('name')\n        self.assertEqual(query.deferred_loading,\n                         (frozenset({'name'}), True))\n"], "sample_94": ["    def test_create_permissions_with_custom_user_model(self):\n        @override_settings(AUTH_USER_MODEL='auth_tests.CustomUser')\n            custom_user_content_type = ContentType.objects.get_by_natural_key('auth_tests', 'customuser')\n            Permission._meta.permissions = [\n                ('my_custom_permission', 'Some permission'),\n            ]\n            create_permissions(self.app_config, verbosity=0)\n\n            # view/add/change/delete permission by default + custom permission\n            self.assertEqual(Permission.objects.filter(\n                content_type=custom_user_content_type,\n            ).count(), 5)\n\n        test(self)\n", "    def test_create_permissions_without_default_permissions(self):\n        permission_content_type = ContentType.objects.get_by_natural_key('auth', 'permission')\n        Permission._meta.default_permissions = []\n        create_permissions(self.app_config, verbosity=0)\n        self.assertEqual(Permission.objects.filter(\n            content_type=permission_content_type,\n        ).count(), 0)\n", "    def test_create_permissions_nonexistent_models(self):\n        with patch('django.apps.registry.Apps') as mock_apps:\n            mock_apps.return_value.get_models.return_value = []\n            create_permissions(self.app_config)\n", "    def test_usermodel_without_username(self):\n        new_io = StringIO()\n\n            with self.assertRaisesMessage(CommandError, 'Error: The provided User model does not have a username field.'):\n                call_command(\n                    'createsuperuser',\n                    interactive=False,\n                    stdout=new_io,\n                    stderr=new_io,\n                    email='joe@somewhere.org',\n                )\n\n        test(self)\n", "    def test_create_permissions_with_custom_user_model(self):\n        \"\"\"\n        Tests that create_permissions correctly creates permissions for a custom\n        user model.\n        \"\"\"\n        user_model = 'auth_tests.CustomUser'\n        with override_settings(AUTH_USER_MODEL=user_model):\n            create_permissions(self.app_config, verbosity=0)\n        self.assertTrue(\n            Permission.objects.filter(codename='add_customuser').exists()\n        )\n        self.assertTrue(\n            Permission.objects.filter(codename='change_customuser').exists()\n        )\n        self.assertTrue(\n            Permission.objects.filter(codename='delete_customuser').exists()\n        )\n        self.assertTrue(\n            Permission.objects.filter(codename='view_customuser').exists()\n        )\n"], "sample_49": ["    def test_media_inheritance_property_with_parent_media(self):\n        class MyWidget1(TextInput):\n            class Media:\n                css = {'all': ('path/to/css1', '/path/to/css2')}\n                js = ('/path/to/js1', 'http://media.other.com/path/to/js2', 'https://secure.other.com/path/to/js3')\n\n        class MyWidget7(MyWidget1):\n                return super().media + Media(css={'all': ('/other/path',)}, js=('/other/js',))\n            media = property(_media)\n\n        w7 = MyWidget7()\n        self.assertEqual(\n            str(w7.media),\n            \"\"\"<link href=\"http://media.example.com/static/path/to/css1\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">", "    def test_media_inheritance_from_property_duplicate(self):\n        # If a widget extends another but defines media, it extends the parents widget's media,\n        # even if the parent defined media using a property, and handles duplicates.\n        class MyWidget1(TextInput):\n            class Media:\n                css = {\n                    'all': ('path/to/css1', '/path/to/css2')\n                }\n                js = ('/path/to/js1', 'http://media.other.com/path/to/js2', 'https://secure.other.com/path/to/js3')\n\n        class MyWidget4(TextInput):\n                return Media(css={'all': ('/some/path',)}, js=('/some/js',))\n            media = property(_media)\n\n        class MyWidget9(MyWidget4):\n            class Media:\n                css = {\n                    'all': ('/other/path', 'path/to/css1')\n                }\n                js = ('/other/js', '/path/to/js1')\n\n        w9 = MyWidget9()\n        self.assertEqual(\n            str(w9.media),\n            \"\"\"<link href=\"/some/path\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">", "    def test_media_empty(self):\n        empty_media = Media()\n        self.assertEqual(str(empty_media), '')\n", "    def test_media_inheritance_property_override(self):\n        class MyWidget1(TextInput):\n            class Media:\n                css = {\n                    'all': ('path/to/css1', '/path/to/css2')\n                }\n                js = ('/path/to/js1', 'http://media.other.com/path/to/js2', 'https://secure.other.com/path/to/js3')\n\n        class MyWidget7(MyWidget1):\n                return Media(css={'all': ('/other/path',)}, js=('/other/js',))\n            media = property(_media)\n\n        w7 = MyWidget7()\n        self.assertEqual(\n            str(w7.media),\n            \"\"\"<link href=\"/other/path\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">", "    def test_media_inheritance_from_property_duplicate(self):\n        # If a widget extends another with properties and there are duplicate entries, they are\n        # merged correctly.\n\n        class MyWidget1(TextInput):\n            class Media:\n                css = {\n                    'all': ('path/to/css1', '/path/to/css2')\n                }\n                js = ('/path/to/js1', 'http://media.other.com/path/to/js2', 'https://secure.other.com/path/to/js3')\n\n        class MyWidget4(TextInput):\n                return Media(css={'all': ('/path/to/css2',)}, js=('/path/to/js1',))\n            media = property(_media)\n\n        class MyWidget9(MyWidget4):\n            class Media:\n                css = {\n                    'all': ('/other/path', '/path/to/css2')\n                }\n                js = ('/path/to/js1', '/other/js')\n\n        w9 = MyWidget9()\n        self.assertEqual(\n            str(w9.media),\n            \"\"\"<link href=\"/path/to/css2\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">"], "sample_987": ["compilation error", "compilation error", "compilation error", "    def test_issue_10323_2():\n        assert floor(sqrt(2**30 + 1)) == 2**15\n", "def test_evalf_with_subs_and_complex():\n    x = Symbol('x')\n    expr = x**2 + 2*I*x\n    assert expr.evalf(subs={x: 1 + 2*I}, n=2) == 3 + 2*4*I\n"], "sample_226": ["    def test_clone_test_db(self):\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db'):\n                creation.clone_test_db('clone1', verbosity=0, autoclobber=True)\n            # Migrations are not run when cloning.\n            self.assertFalse(mocked_migrate.called)\n\n        finally:\n            creation.destroy_test_db(old_database_name, verbosity=0)\n", "    def test_circular_reference_multiple(self):\n        # deserialize_db_from_string() handles multiple objects with circular references.\n        data = \"\"\"\n        [\n            {\n                \"model\": \"backends.objecta\",\n                \"pk\": 1,\n                \"fields\": {\"objectb\": 1}\n            },\n            {\n                \"model\": \"backends.objectb\",\n                \"pk\": 1,\n                \"fields\": {\"objecta\": 1}\n            }\n        ]\n        \"\"\"\n        connection.creation.deserialize_db_from_string(data)\n        objectA = CircularA.objects.get()\n        objectB = CircularB.objects.get()\n        self.assertEqual(objectA.objectb, objectB)\n        self.assertEqual(objectB.objecta, objectA)\n", "    def test_destroy_test_db_keepdb(self, mocked_sync_apps, mocked_migrate, *mocked_objects):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST']['MIGRATE'] = True\n        creation = test_connection.creation_class(test_connection)\n        if connection.vendor == 'oracle':\n            # Don't close connection on Oracle.\n            creation.connection.close = mock.Mock()\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db'):\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n            # Don't destroy db\n            creation.destroy_test_db(old_database_name, verbosity=0, keepdb=True)\n            # Assert _destroy_test_db isn't called\n            mocked_objects[0].assert_not_called()\n        finally:\n\n            creation.destroy_test_db(old_database_name, verbosity=0)\n\n", "    def test_clone_test_db(self):\n        # Test cloning a test database.\n        old_database_name = connection.settings_dict['NAME']\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n        test_database_name = test_connection.settings_dict['NAME']\n        cloning_suffix = 1\n\n        # Clone the test database.\n\n        try:\n            creation.clone_test_db(cloning_suffix, verbosity=0)\n            # Check that the clone name is correctly set.\n\n            clone_settings = creation.get_test_db_clone_settings(cloning_suffix)\n            self.assertEqual(clone_settings['NAME'], f'{test_database_name}_{cloning_suffix}')\n        finally:\n            # Destroy the cloned database.\n\n            creation.destroy_test_db(old_database_name, verbosity=0)\n", "    def test_serialize_deserialize_circular_references_with_multiple_objects(self):\n        # Test that circular references between multiple objects are handled correctly.\n        obj_a = CircularA.objects.create(name='A')\n        obj_b = CircularB.objects.create(name='B')\n        obj_a.b = obj_b\n        obj_b.a = obj_a\n        obj_a.save()\n        obj_b.save()\n\n        # Serialize and deserialize the objects\n        data = connection.creation.serialize_db_to_string()\n        connection.creation.deserialize_db_from_string(data)\n\n        # Check that the relationships are still intact\n        obj_a_reloaded = CircularA.objects.get(name='A')\n        obj_b_reloaded = CircularB.objects.get(name='B')\n        self.assertEqual(obj_a_reloaded.b, obj_b_reloaded)\n        self.assertEqual(obj_b_reloaded.a, obj_a_reloaded)\n"], "sample_574": ["    def test_tick_every(self, t, x):\n        n = 365\n        ax = mpl.figure.Figure().subplots()\n        Temporal().tick(every=n)._setup(t, Coordinate(), ax.xaxis)\n        locator = ax.xaxis.get_major_locator()\n        assert locator.interval == n\n", "    def test_tick_every(self, t, x):\n\n        d = 30\n        ax = mpl.figure.Figure().subplots()\n        Temporal().tick(every=d)._setup(t, Coordinate(), ax.xaxis)\n        locator = ax.xaxis.get_major_locator()\n        assert isinstance(locator, mpl.dates.DateLocator)\n        assert locator.intervald.days == d\n", "    def test_tick_every(self, t, x):\n\n        every = 300\n        ax = mpl.figure.Figure().subplots()\n        Temporal().tick(every=every)._setup(t, Coordinate(), ax.xaxis)\n        locator = ax.xaxis.get_major_locator()\n        assert set(locator.interval).issubset({every})\n", "    def test_label_base(self, t, x):\n\n        s = Temporal()\n        a = PseudoAxis(s._setup(t, Coordinate())._matplotlib_scale)\n        a.set_view_interval(100, 1000)\n        label, = a.major.formatter.format_ticks([100])\n        assert label == \"1970-01-01\"\n", "    def test_tick_every(self, t, x):\n\n        d = 365\n        ax = mpl.figure.Figure().subplots()\n        Temporal().tick(every=d)._setup(t, Coordinate(), ax.xaxis)\n        locator = ax.xaxis.get_major_locator()\n        assert np.allclose(np.diff(locator.locs), d)\n"], "sample_1192": ["compilation error", "def test_symbols_disambiguate():\n    x, y = symbols('x y')\n    assert disambiguate(x, y) == (x, y)\n    x1, x2 = symbols('x x')\n    assert disambiguate(x1, x2) == (x1, x2)\n    a, b = symbols('a b')\n\n    eqs = (a + b, a*b)\n    assert disambiguate(*eqs) == (a + b, a*b)\n    assert type(disambiguate(*eqs)[0]) == Add\n\n    x, y, z = symbols('x y z')\n    assert disambiguate(x*y, x*z) == (x*y, x*z_1)\n    assert disambiguate(x*y, x**2) == (x*y, x**2)\n\n    assert disambiguate(x*y, x*y) == (x*y, x*y)\n\n    x1, x2 = symbols('x x')\n    assert disambiguate(x1 + x2, x1 + x2) == (x1 + x2, x1 + x2)\n", "def test_disambiguate():\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n    a = Symbol('a')\n    b = Symbol('b')\n\n    assert disambiguate(x, y, z) == (x, y, z)\n    assert disambiguate(x, x, y) == (x_1, x_2, y)\n    assert disambiguate(x, y, x) == (x_1, y, x_2)\n    assert disambiguate(x, y, z, x) == (x_1, y, z, x_2)\n    assert disambiguate(x * y, y * z) == (x_1*y, y*z)\n    assert disambiguate(x + y, x + z, y + z) == (x_1+y, x_2+z, y_2+z)\n    assert disambiguate(x + y, 2*x + y, x + 2*y) == (x_1 + y, 2*x_2 + y, x + 2*y)\n\n    assert disambiguate(a, Dummy('a')) == (a, a_1)\n    assert disambiguate(a, Dummy('a'), Dummy('a')) == (a, a_1, a_2)\n", "compilation error", "compilation error"], "sample_391": ["    def test_create_model_add_field_unique_together(self):\n        \"\"\"\n        AddField should optimize into CreateModel if it's a FK to a model that's\n        between them (and there's no FK in the other direction), by changing\n        the order of the CreateModel operations.\n        \"\"\"\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    \"Foo\", [(\"name\", models.CharField(max_length=255))]\n                ),\n                migrations.CreateModel(\"Bar\", [(\"url\", models.TextField())]),\n                migrations.AddField(\"Foo\", \"bar\", models.ForeignKey(\"migrations.Bar\", models.CASCADE)),\n                migrations.AlterUniqueTogether(\"Foo\", [(\"name\", \"bar\")]),\n            ],\n            [\n                migrations.CreateModel(\"Bar\", [(\"url\", models.TextField())]),\n                migrations.CreateModel(\n                    \"Foo\",\n                    [\n                        (\"name\", models.CharField(max_length=255)),\n                        (\"bar\", models.ForeignKey(\"migrations.Bar\", models.CASCADE)),\n                    ],\n                ),\n                migrations.AlterUniqueTogether(\"Foo\", [(\"name\", \"bar\")]),\n            ],\n        )\n", "    def test_optimize_through_fields_remove_field(self):\n        \"\"\"\n        Test that removing a field that's referenced in a through model optimization\n        works correctly.\n\n        This tests a scenario where a through model is used, and one of the fields\n        referenced in the through model's ForeignKeys is later removed from one of\n        the main models. The optimization should handle this correctly without\n        causing errors.\n        \"\"\"\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    \"Author\",\n                    [\n                        (\"name\", models.CharField(max_length=255)),\n                    ],\n                ),\n                migrations.CreateModel(\n                    \"Book\",\n                    [\n                        (\"title\", models.CharField(max_length=255)),\n                    ],\n                ),\n                migrations.CreateModel(\n                    \"BookAuthor\",\n                    [\n                        (\"author\", models.ForeignKey(\"Author\", models.CASCADE)),\n                        (\"book\", models.ForeignKey(\"Book\", models.CASCADE)),\n                        (\"role\", models.CharField(max_length=255)),\n                    ],\n                ),\n                migrations.AddField(\"Book\", \"genre\", models.CharField(max_length=255)),\n                migrations.RemoveField(\"Book\", \"genre\"),\n            ],\n            [\n                migrations.CreateModel(\n                    \"Author\",\n                    [\n                        (\"name\", models.CharField(max_length=255)),\n                    ],\n                ),\n                migrations.CreateModel(\n                    \"Book\",\n                    [\n                        (\"title\", models.CharField(max_length=255)),\n                    ],\n                ),\n                migrations.CreateModel(\n                    \"BookAuthor\",\n                    [\n                        (\"author\", models.ForeignKey(\"Author\", models.CASCADE)),\n                        (\"book\", models.ForeignKey(\"Book\", models.CASCADE)),\n                        (\"role\", models.CharField(max_length=255)),\n                    ],\n                ),\n            ],\n        )\n", "    def test_create_model_rename_model_delete_model(self):\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    \"Foo\", [(\"name\", models.CharField(max_length=255))]\n                ),\n                migrations.RenameModel(\"Foo\", \"Bar\"),\n                migrations.DeleteModel(\"Bar\"),\n            ],\n            [],\n        )\n", "    def test_create_alter_delete_with_app_label(self):\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    \"Foo\",\n                    [(\"name\", models.CharField(max_length=255))],\n                    app_label=\"testapp\",\n                ),\n                migrations.AlterField(\n                    \"testapp.Foo\", \"name\", models.CharField(max_length=100)\n                ),\n                migrations.DeleteModel(\n                    \"testapp.Foo\",\n                ),\n            ],\n            [],\n            app_label=\"testapp\",\n        )\n", "compilation error"], "sample_986": ["compilation error", "    def test_issue_10845():\n        assert nsimplify(0.125, tolerance=1e-12) == Rational(1, 8)\n", "compilation error", "compilation error", "compilation error"], "sample_1029": ["    def test_FunctionClass():\n        sT(Function('f'), \"Function('f')\")\n", "def test_Sum2():\n    i, a, b = x, 1, 5\n    sT(Sum2(sin(x), (i, a, b)), \"Sum2(sin(Symbol('x')), (Symbol('i'), Integer(1), Integer(5)))\")\n\n", "def test_Sum2():\n    x = Symbol('x')\n    f = x**2\n    i = Symbol('i', integer=True)\n    a = 1\n    b = 3\n\n    sT(Sum2(f, (i, a, b)), \"Sum2(Pow(Symbol('x'), Integer(2)), (Symbol('i'), Integer(1), Integer(3)))\")\n", "def test_Sum2():\n    sT(Sum2(x**2, (x, 1, 5)), \"Sum2(Pow(Symbol('x'), Integer(2)), (Symbol(x), Integer(1), Integer(5)))\")\n\n", "    def test_Sum2():\n        sT(Sum2(x**2, (x, 1, 3)), \"Sum2(Pow(Symbol('x'), Integer(2)), Symbol('x'), Integer(1), Integer(3))\")\n"], "sample_662": ["     def test_outcome_properties(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n        \"\"\"\n        )\n        reprec = testdir.inline_run()\n        reports = reprec.getreports(\"pytest_runtest_logreport\")\n        assert len(reports) == 3\n        pass_report = reports[1]\n        fail_report = reports[2]\n        assert pass_report.passed\n        assert not pass_report.failed\n        assert not pass_report.skipped\n        assert not fail_report.passed\n        assert fail_report.failed\n        assert not fail_report.skipped\n", "    def test_xdist_report_location(self, testdir, pytestconfig):\n        testdir.makepyfile(\n            \"\"\"\n        \"\"\"\n        )\n        reprec = testdir.inline_run()\n        reports = reprec.getreports(\"pytest_runtest_logreport\")\n        assert len(reports) == 3\n        rep = reports[1]\n        d = rep._to_json()\n        a = TestReport._from_json(d)\n        assert a.location == rep.location\n", "    def test_failure_repr_location(self, testdir, pytestconfig):\n        testdir.makepyfile(\n            \"\"\"\n                assert 0, \"This is a failure message\"\n        \"\"\"\n        )\n        reprec = testdir.inline_run()\n        reports = reprec.getreports(\"pytest_runtest_logreport\")\n        assert len(reports) == 3\n        report = reports[1]\n        data = pytestconfig.hook.pytest_report_to_serializable(\n            config=pytestconfig, report=report\n        )\n        assert data[\"longrepr\"][\"reprcrash\"][\"path\"] == str(testdir.tmpdir.join(\"test_a.py\"))\n        assert data[\"longrepr\"][\"reprcrash\"][\"lineno\"] == 3\n", "    def test_report_sections(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.parametrize(\"value\", [1, 2, 3])\n                print(f'captured value: {value}')\n            \n        \"\"\"\n        )\n        reprec = testdir.inline_run()\n        reports = reprec.getreports(\"pytest_runtest_logreport\")\n        assert len(reports) == 4\n        test_report = reports[1]\n        assert test_report.sections == (\n            (\"Captured stdout\", \"captured value: 1\\n\"),\n        )\n\n", "    def test_report_outcome_serialization(self, testdir, pytestconfig):\n        testdir.makepyfile(\n            \"\"\"\n        \"\"\"\n        )\n        reprec = testdir.inline_run()\n        reports = reprec.getreports(\"pytest_runtest_logreport\")\n        assert len(reports) == 6\n\n        for rep in reports:\n            data = pytestconfig.hook.pytest_report_to_serializable(\n                config=pytestconfig, report=rep\n            )\n            assert data[\"outcome\"] in (\"passed\", \"failed\", \"skipped\")\n            new_rep = pytestconfig.hook.pytest_report_from_serializable(\n                config=pytestconfig, data=data\n            )\n            assert new_rep.outcome == rep.outcome\n"], "sample_296": ["    def test_multiple_messages_with_safedata(self):\n        \"\"\"\n        Multiple messages, some containing SafeData, are correctly handled\n        when stored and retrieved from the cookie storage.\n        \"\"\"\n        storage = self.get_storage()\n        response = self.get_response()\n\n        messages = [\n            Message(constants.INFO, 'Regular message'),\n            Message(constants.WARNING, mark_safe('<b>SafeData message</b>')),\n            Message(constants.ERROR, 'Another regular message'),\n            Message(constants.DEBUG, mark_safe('<script>UnsafeData</script>')),\n        ]", "    def test_empty_cookie(self):\n        storage = self.storage_class(self.get_request())\n        set_cookie_data(storage, [],encode_empty=True)\n        self.assertEqual(list(storage), [])\n", "    def test_empty_cookie(self):\n        storage = self.get_storage()\n        response = self.get_response()\n        # Test when there are no messages to store\n        storage.update(response)\n        self.assertEqual(stored_cookie_messages_count(storage, response), 0)\n", "    def test_empty_cookie(self):\n        storage = self.storage_class(self.get_request())\n        set_cookie_data(storage, [], encode_empty=True)\n        self.assertEqual(list(storage), [])\n", "    def test_empty_cookie_encoding(self):\n        \"\"\"\n        Test encoding and decoding of an empty cookie.\n        \"\"\"\n        storage = self.get_storage()\n        encoded = storage._encode([], encode_empty=True)\n        decoded = storage._decode(encoded)\n        self.assertEqual(decoded, [])\n"], "sample_535": ["def test_table_bbox():\n    fig, ax = plt.subplots()\n    ax.axis('off')\n\n    # Test bbox without tight layout\n    table = ax.table(cellText='Test', loc='center', bbox=Bbox([[0.2, 0.2], [0.8, 0.8]]))\n\n    # Test bbox with tight layout\n    fig2, ax2 = plt.subplots()\n    ax2.axis('off')\n    table2 = ax2.table(cellText='Test', loc='center', bbox=Bbox([[0.2, 0.2], [0.8, 0.8]]))\n    fig2.tight_layout()\n", "def test_bbox():\n    fig, ax = plt.subplots()\n    ax.axis('off')\n    table = ax.table([[1, 2], [3, 4]], bbox=[0.2, 0.2, 0.6, 0.6])\n    table.auto_set_column_width(list(range(2)))\n", "def test_bbox():\n    fig, ax = plt.subplots()\n\n    bbox = Bbox.from_bounds(0.2, 0.2, 0.6, 0.6)\n    table = ax.table(cellText=[[1, 2], [3, 4]], bbox=bbox)\n    ax.add_patch(plt.Rectangle((0.2, 0.2), 0.6, 0.6, fill=False))\n\n", "def test_table_bbox():\n    fig, ax = plt.subplots()\n    bbox = Bbox.from_bounds(0.2, 0.2, 0.4, 0.4)\n    ax.table(cellText=[['1', '2'], ['3', '4']], bbox=bbox)\n    ax.set_xlim([0, 1])\n    ax.set_ylim([0, 1])\n", "def test_table_bbox():\n    fig, ax = plt.subplots()\n    bbox = Bbox.from_extents(0.2, 0.2, 0.8, 0.8)\n    ax.table(cellText=[['1', '2'], ['3', '4']], bbox=bbox)\n"], "sample_875": ["compilation error", "def test_balanced_accuracy_score_empty():\n    assert balanced_accuracy_score([], []) == 0.0\n    assert balanced_accuracy_score([0], []) == 0.0\n    assert balanced_accuracy_score([], [0]) == 0.0\n\n", "compilation error", "def test_balanced_accuracy_score_multilabel():\n    y_true = np.array([[0, 1], [1, 0], [0, 0]])\n    y_pred = np.array([[0, 1], [1, 1], [1, 0]])\n\n    with pytest.raises(ValueError):\n        balanced_accuracy_score(y_true, y_pred)\n", "compilation error"], "sample_934": ["    def assert_consistent(self, other):\n        \"\"\"Assert that another RoleClasses instance has the same classes.\"\"\"\n        assert self.classes == other.classes, (\n            f\"Classes mismatch for role '{self.name}':\\n\"\n            f\"  Expected: {sorted(self.classes)}\\n\"\n            f\"   Actual: {sorted(other.classes)}\"\n        )\n        for tag, classes in self.content_classes.items():\n            assert classes == other.content_classes.get(tag, set()), (\n                f\"Classes mismatch for role '{self.name}' with tag '{tag}':\\n\"\n                f\"  Expected: {sorted(classes)}\\n\"\n                f\"   Actual: {sorted(other.content_classes.get(tag, set()))}\"\n            )\n", "    def check_role(role, root, contents):\n        role_classes = RoleClasses(role, root, contents)\n        for tag in contents:\n            assert role_classes.classes.issubset(role_classes.content_classes[tag]), (\n                f\"Role '{role}' content tag '{tag}' missing classes: \"\n                f\"{role_classes.classes - role_classes.content_classes[tag]}\"\n            )\n", "compilation error", "    def assert_classes_equal(self, other):\n        assert self.name == other.name, f\"Roles don't match: {self.name} != {other.name}\"\n        assert self.classes == other.classes, f\"Classes for {self.name} differ: {self.classes} != {other.classes}\"\n        for tag in self.content_classes:\n            assert tag in other.content_classes, f\"Content tag {tag} missing in {other.name}\"\n            assert self.content_classes[tag] == other.content_classes[tag], f\"Classes for {self.name}::{tag} differ: {self.content_classes[tag]} != {other.content_classes[tag]}\"\n", "compilation error"], "sample_442": ["    def test_signer_init_with_kwargs(self):\n        signer = signing.Signer(key=\"predictable-secret\", sep=\";\", salt=\"extra-salt\")\n        self.assertEqual(signer.key, \"predictable-secret\")\n        self.assertEqual(signer.sep, \";\")\n        self.assertEqual(signer.salt, \"extra-salt.Signer\")\n", "    def test_sign_unsign_with_fallback_keys_in_settings(self):\n        signer = signing.Signer()\n        signed = signer.sign(\"abc\")\n        self.assertEqual(signer.unsign(signed), \"abc\")\n", "    def test_signer_constructor_with_only_key_arg(self):\n        signer = signing.Signer(key=\"predictable-secret\")\n        self.assertEqual(signer.key, \"predictable-secret\")\n        self.assertEqual(signer.sep, \":\")\n        self.assertEqual(signer.salt, \"%s.%s\" % (\n            signer.__class__.__module__,\n            signer.__class__.__name__,\n        ))\n        self.assertEqual(signer.algorithm, \"sha256\")\n        self.assertIsNone(signer.fallback_keys)\n", "    def test_signer_fallback_keys_deprecation(self):\n        signer = signing.Signer(\n            key=\"secret\", fallback_keys=[\"oldsecret\"], sep=\"!\"\n        )\n        signed = signer.sign(\"abc\")\n        self.assertEqual(signer.unsign(signed), \"abc\")\n", "    def test_sign_object_with_compress(self):\n        signer = signing.Signer(key=\"predictable-secret\")\n        obj = list(range(1000))\n        signed_obj = signer.sign_object(obj, compress=True)\n        self.assertNotEqual(obj, signed_obj)\n        self.assertEqual(obj, signer.unsign_object(signed_obj))\n"], "sample_810": ["compilation error", "    def test_make_pipeline_empty():\n        pipeline = make_pipeline()\n        assert len(pipeline.steps) == 0\n        assert pipeline.memory is None\n", "compilation error", "    def test_feature_union_with_no_features():\n        # Test that FeatureUnion works correctly when some transformers\n        # produce empty arrays.\n        # This can happen when dealing with sparse data, for example.\n        X = np.array([[1, 2, 3], [4, 5, 6]])\n        \n        class EmptyTransformer(TransformerMixin):\n                return self\n\n                return np.array([])\n\n        union = FeatureUnion([\n            ('passthrough', 'passthrough'),\n            ('empty', EmptyTransformer())\n        ])\n        Xt = union.fit_transform(X)\n        assert Xt.shape == (2, 3)\n", "compilation error"], "sample_69": ["    def test_notify_file_changed_ignores_missing_files(self):\n        self.reloader.notify_file_changed(self.nonexistent_file)\n        self.assertFalse(self.reloader._should_restart)\n", "    def test_file_changed_ignores_symlinks(self):\n        symlink_path = self.tempdir / 'test.symlink'\n        os.symlink(self.existing_file, symlink_path)\n        self.reloader.watch_file(symlink_path)\n        with self.tick_twice():\n            self.increment_mtime(self.existing_file)\n\n        self.assertEqual(self.reloader.notify_file_changed.call_count, 0)\n", "    def test_notify_file_changed_removes_from_changed_files(self):\n        self.reloader.watch_file(self.existing_file)\n        with self.tick_twice():\n            self.increment_mtime(self.existing_file)\n        self.assertIn(self.existing_file, self.reloader._changed_files)\n        self.reloader.notify_file_changed(self.existing_file)\n        self.assertNotIn(self.existing_file, self.reloader._changed_files)\n", "    def test_notify_file_changed_with_invalid_file(self):\n        with self.assertRaises(FileNotFoundError):\n            self.reloader.notify_file_changed(self.nonexistent_file)\n", "    def test_file_changed_returns_true_when_file_is_modified(self):\n        self.reloader.watch_file(self.existing_file)\n        self.assertFalse(self.reloader.file_changed(self.existing_file))\n        self.increment_mtime(self.existing_file)\n        self.assertTrue(self.reloader.file_changed(self.existing_file))\n"], "sample_416": ["    def test_runshell(self):\n        with mock.patch(\"subprocess.run\") as mock_subprocess_run:\n            client = DatabaseClient(connection.settings_dict.copy())\n            client.runshell([])\n            mock_subprocess_run.assert_called_once()\n            args, kwargs = mock_subprocess_run.call_args\n            self.assertEqual(kwargs[\"check\"], True)\n", "    def test_runshell_calls_super(self):\n        with mock.patch(\"subprocess.run\") as mock_run:\n            client = DatabaseClient(\n                {\"NAME\": \"dbname\"},\n                mock.MagicMock(),\n            )\n            client.runshell([\"--help\"])\n            mock_run.assert_called_once()\n\n", "    def test_runshell(self):\n        with mock.patch(\"subprocess.Popen\") as mock_subprocess_popen, mock.patch(\n            \"django.db.backends.postgresql.client.DatabaseClient.settings_to_cmd_args_env\"\n        ) as mock_settings_to_cmd_args_env:\n            mock_settings_to_cmd_args_env.return_value = (\n                [\"psql\", \"--help\"],\n                None,\n            )\n            client = DatabaseClient(None)\n            client.runshell([\"--help\"])\n            mock_subprocess_popen.assert_called_once_with(\n                [\"psql\", \"--help\"],\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                stdin=subprocess.PIPE,\n                env=None,\n                close_fds=True,\n            )\n", "    def test_runshell(self):\n        with mock.patch('subprocess.Popen') as mock_popen:\n            client = DatabaseClient(connection.settings_dict)\n            client.runshell(['-c', 'SELECT 1'])\n            mock_popen.assert_called_once()\n            args, kwargs = mock_popen.call_args\n\n            # Check SIGINT handling\n            self.assertIn(\n                'preexec_fn', kwargs, 'SIGINT handling function not passed to subprocess'\n            )\n            preexec_fn = kwargs['preexec_fn']\n            original_handler = signal.getsignal(signal.SIGINT)\n            preexec_fn()\n            self.assertEqual(signal.getsignal(signal.SIGINT), signal.SIG_IGN)\n            signal.signal(signal.SIGINT, original_handler)\n\n\n", "    def test_runshell_sigint(self):\n        \"\"\"Test SIGINT handling in runshell.\"\"\"\n        with mock.patch(\"subprocess.run\") as mock_run:\n                raise SystemExit(\"SIGINT received\")\n\n            # Install our mock handler temporarily.\n            original_handler = signal.signal(signal.SIGINT, sigint_handler)\n\n            try:\n                client = DatabaseClient(connection.settings_dict)\n                client.runshell([])\n\n            except SystemExit as e:\n                self.assertEqual(str(e), \"SIGINT received\")\n            finally:\n                # Restore the original handler.\n                signal.signal(signal.SIGINT, original_handler)\n\n\n\n"], "sample_498": ["def test_legend_labelspacing():\n    # Test setting labelspacing in legend\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], label='line1')\n    ax.plot([4, 5, 6], label='line2')\n    leg = ax.legend()\n    leg.set_labelspacing(0.5)\n    assert leg.get_labelspacing() == 0.5\n", "compilation error", "    def test_legend_bbox_to_anchor():\n        # Test bbox_to_anchor with numerical input for legend placement\n        fig, ax = plt.subplots()\n        ax.plot([1, 2], [3, 4], label='line')\n        leg = ax.legend(bbox_to_anchor=(0.5, 0.5))\n        assert leg.bbox.anchored_bbox.get_bbox().x0 == 0.5\n        assert leg.bbox.anchored_bbox.get_bbox().y0 == 0.5\n", "def test_legend_custom_handler():\n    fig, ax = plt.subplots()\n    ax.scatter([0, 1], [0, 1], label='scatter', c='r')\n    class CustomHandler(HandlerScatter):\n                           xdescent, ydescent, width, height, fontsize,\n                           trans):\n\n            artists = super().create_artists(legend, orig_handle, xdescent,\n                                            ydescent, width, height, fontsize,\n                                            trans)\n\n            # Add a rectangle around the scatter marker\n            rect = mpl.patches.Rectangle((0, 0), width / 2, height / 2,\n                                        edgecolor='black', facecolor='none')\n            return artists + [rect]\n\n    leg = ax.legend(handler_map={type(ax.collections[0]): CustomHandler()})\n\n    assert len(leg.get_patches()) == 2\n", "def test_legend_title_fontproperties():\n    # test setting title font properties through FontProperties object\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [1, 2], label='Test Line')\n    font = FontProperties(size=16, weight='bold', style='italic')\n    leg = ax.legend(title='Legend Title', title_fontproperties=font)\n    assert leg.get_title().get_fontsize() == 16\n    assert leg.get_title().get_weight() == 'bold'\n    assert leg.get_title().get_style() == 'italic'\n"], "sample_719": ["    def test_tfidfvectorizer_vocabulary_setter():\n        vect = TfidfVectorizer()\n        vocab = ['hello', 'world']\n        vect.vocabulary_ = {word: i for i, word in enumerate(vocab)}\n        assert_equal(vect.vocabulary_, {word: i for i, word in enumerate(vocab)})\n        assert_equal(vect.get_feature_names_out(), vocab)\n\n\n", "def test_tfidfvectorizer_empty_vocabulary():\n    vect = TfidfVectorizer()\n    with pytest.raises(ValueError, match=\"Empty vocabulary.\"):\n        vect.transform(JUNK_FOOD_DOCS)\n", "    def test_tfidf_transformer_with_prefitted_vectorizer():\n        vectorizer = CountVectorizer().fit(JUNK_FOOD_DOCS)\n        transformer = TfidfTransformer()\n        X_count = vectorizer.transform(JUNK_FOOD_DOCS)\n        X_tfidf = transformer.fit_transform(X_count)\n        X_tfidf_direct = TfidfVectorizer(\n            vocabulary=vectorizer.vocabulary_,\n            use_idf=True\n        ).fit_transform(JUNK_FOOD_DOCS)\n        assert_allclose_dense_sparse(X_tfidf, X_tfidf_direct)\n", "compilation error", "    def test_tfidf_vectorizer_stop_words_with_empty_vocabulary(self):\n        vect = TfidfVectorizer(stop_words='english', vocabulary=[])\n        with pytest.raises(ValueError):\n            vect.fit_transform([\"hello world\"])\n"], "sample_704": ["compilation error", "compilation error", "def test_node_repr_failure_full_trace(pytester: Pytester) -> None:\n    \"\"\"Test repr_failure when full trace is enabled.\"\"\"\n    pytester.runpytest(\"--fulltrace\", \"-v\")\n    resultlog = pytester.stdout.getvalue()\n    assert \"Traceback (most recent call last):\" in resultlog\n\n", "    def test_node_keywords(pytester: Pytester) -> None:\n        items = pytester.getitems(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.skip(reason=\"skip it\")\n                pass\n\n                pass\n        \"\"\"\n        )\n        assert items[0].keywords[\"skip\"] == \"skip it\"\n        assert \"skip\" not in items[1].keywords\n\n", "compilation error"], "sample_262": ["    def test_lazy_object_pickle(self):\n        original_object = 'Lazy translation text'\n        lazy_obj = lazy(lambda: original_object, str)\n        deserialized_obj = pickle.loads(pickle.dumps(lazy_obj))\n        self.assertEqual(lazy_obj(), deserialized_obj())\n", "    def test_keep_lazy_text(self):\n        \"\"\"\n        keep_lazy_text should correctly evaluate lazy text arguments.\n        \"\"\"\n\n            return a + b\n\n        lazy_a = lazystr(\"hello\")\n        lazy_b = lazystr(\" world\")\n        result = keep_lazy_text(func)(lazy_a, lazy_b)\n        self.assertEqual(result, \"hello world\")\n", "    def test_lazy_pickling(self):\n        obj = lazy(lambda: [1, 2, 3], list)\n        import pickle\n        pickled = pickle.dumps(obj)\n        unpickled = pickle.loads(pickled)\n        self.assertEqual(unpickled(), [1, 2, 3])\n", "    def test_lazystr(self):\n        lazy_string = lazystr(\"hello world\")\n        self.assertEqual(lazy_string(), \"hello world\")\n        self.assertIsInstance(lazy_string(), str)\n", "    def test_lazy_empty_args(self):\n            return []\n\n        lazy_func = lazy(empty_func, list)\n        self.assertEqual(lazy_func(), [])\n"], "sample_253": ["    def test_notify_file_changed_ignores_missing_files(self):\n        with mock.patch.object(self.reloader, '_should_restart') as mocked_restart:\n            self.reloader.notify_file_changed(self.nonexistent_file)\n            mocked_restart.assert_not_called()\n", "    def test_watch_dir_with_empty_glob(self):\n        self.reloader.watch_dir(self.tempdir, '')\n        watched_files = list(self.reloader.watched_files())\n        self.assertEqual(watched_files, [])\n", "    def test_tick_file_modified_twice(self):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[self.existing_file]):\n            ticker = self.reloader.tick()\n            next(ticker)\n            self.increment_mtime(self.existing_file)\n            next(ticker)\n            self.increment_mtime(self.existing_file)\n            next(ticker)\n", "    def test_should_stop(self):\n        self.reloader.should_stop = True\n        self.assertTrue(self.reloader.should_stop())\n        self.reloader.should_stop = False\n        self.assertFalse(self.reloader.should_stop())\n", "    def test_notify_file_changed_called_with_correct_arguments(self):\n        with mock.patch('django.utils.autoreload.BaseReloader.notify_file_changed') as mocked_notify:\n            self.reloader.notify_file_changed(self.existing_file)\n            mocked_notify.assert_called_once_with(self.existing_file)\n\n"], "sample_819": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_22": ["compilation error", "compilation error", "    def test_matrix_product():\n        m1 = rotation_matrix(30 * u.deg, \"x\")\n        m2 = rotation_matrix(45 * u.deg, \"y\")\n        m3 = rotation_matrix(60 * u.deg, \"z\")\n\n        assert_allclose(matrix_product(m1, m2), np.dot(m1, m2), atol=1e-12)\n        assert_allclose(\n            matrix_product(m1, m2, m3), np.dot(np.dot(m1, m2), m3), atol=1e-12\n        )\n", "def test_matrix_product():\n    m1 = rotation_matrix(30 * u.deg, \"x\")\n    m2 = rotation_matrix(45 * u.deg, \"y\")\n    assert_allclose(matrix_product(m1, m2), np.dot(m1, m2), atol=1e-12)\n\n    m3 = rotation_matrix(60 * u.deg, \"z\")\n    assert_allclose(\n        matrix_product(m1, m2, m3), np.dot(np.dot(m1, m2), m3), atol=1e-12\n    )\n", "compilation error"], "sample_640": ["def test_get_node_first_ancestor_of_type() -> None:\n    code = astroid.extract_node(\n        \"\"\"\n    class C(object):\n            try:\n                pass\n            except ZeroDivisionError as e:\n                print(a)\n            finally:\n                print(\"always\")\n    \"\"\"\n    )\n    ancestor = utils.get_node_first_ancestor_of_type(code[0].body[0].orelse[0], (nodes.TryExcept, nodes.TryFinally))\n    assert isinstance(ancestor, nodes.TryExcept)\n    assert ancestor.parent == code[0].body[0]\n\n", "    def test_get_node_first_ancestor_of_type_and_its_child():\n        code = astroid.extract_node(\n            \"\"\"\n                try:\n                    pass\n                except Exception as e:\n                    print(e)\n            \"\"\"\n        )\n        ancestor, child = utils.get_node_first_ancestor_of_type_and_its_child(\n            code[-1], (nodes.TryExcept, nodes.TryFinally)\n        )\n        assert isinstance(ancestor, nodes.TryExcept)\n        assert isinstance(child, nodes.Print)\n", "def test_is_node_in_guarded_import_block() -> None:\n    code = astroid.extract_node(\n        \"\"\"\n    if True:\n        from typing import List \n    \n    if TYPE_CHECKING:\n        from typing_extensions import Literal\n    \n        from .module import MyClass\n\n        \n    \"\"\"\n    )\n    assert utils.is_node_in_guarded_import_block(code[0])\n    assert utils.is_node_in_guarded_import_block(code[1])\n    assert not utils.is_node_in_guarded_import_block(code[2])\n\n", "def test_is_node_in_typing_guarded_import_block() -> None:\n    code = astroid.extract_node(\n        \"\"\"\n    from typing import TYPE_CHECKING\n    if TYPE_CHECKING:\n      import some_module\n    \"\"\"\n    )\n    assert utils.is_node_in_typing_guarded_import_block(code[2])\n    assert not utils.is_node_in_typing_guarded_import_block(code[0])\n    assert not utils.is_node_in_typing_guarded_import_block(code[1])\n\n\n", "def test_is_node_in_guarded_import_block() -> None:\n    code = astroid.extract_node(\n        \"\"\"\n    from typing import TYPE_CHECKING\n    if TYPE_CHECKING:\n        from xyz import a\n    \"\"\"\n    )\n    assert utils.is_node_in_guarded_import_block(code[1])\n    assert not utils.is_node_in_guarded_import_block(code[0])\n\n"], "sample_801": ["compilation error", "def test_repr_with_custom_estimator_name():\n    class CustomEstimator(BaseEstimator):\n            self.param1 = param1\n            self.param2 = param2\n\n            return self\n\n    estimator = CustomEstimator(param1=2, param2=\"world\")\n    expected = \"\"\"", "def test_feature_names_out():\n    # Test for estimators with feature_names_out_ attribute\n\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    \n    vectorizer = TfidfVectorizer()\n    # fit and check\n    vectorizer.fit([\"hello world\", \"goodbye world\"])\n    assert vectorizer.get_feature_names_out().tolist() == ['goodbye', 'hello', 'world']\n\n", "def test_repr_with_empty_params():\n    # Test the repr function when the estimator has no parameters.\n    class EmptyEstimator(BaseEstimator):\n        pass\n\n    estimator = EmptyEstimator()\n    expected = \"EmptyEstimator()\"\n    assert repr(estimator) == expected \n", "def test_nested_objects():\n    # Render a deeply nested estimator with objects inside\n    class CustomTransformer(BaseEstimator, TransformerMixin):\n            self.param1 = param1\n            self.param2 = param2\n\n            return self\n\n            return X\n\n    pipeline = make_pipeline(\n        CustomTransformer(param1=2, param2='b'),\n        LogisticRegression(C=999)\n    )\n    expected = \"\"\""], "sample_1187": ["    def test_point_in_hull():\n        points = [(1, 1), (2, 2), (3, 1), (2, 0)]\n        hull = ConvexHull(points)\n        assert point_in_hull((1.5, 1.5), hull) is True\n        assert point_in_hull((4, 0), hull) is False\n        assert point_in_hull((2.5, 1), hull) is True\n", "    def test_get_monomial_values():\n        triangle = Polygon(Point(0, 3), Point(5, 3), Point(1, 1))\n        facets = triangle.sides\n        a, b = hyperplane_parameters(triangle)[0]\n        x0 = facets[0].points[0]\n        monomial_values = [[0, 0, 0, 0], [1, 0, 0, 5],\n                           [y, 0, 1, 15], [x, 1, 0, None]]\n\n        assert get_monomial_values(facets, 0, a, b, x, 1, (x, y), 1,\n                                   0, 1, x0, monomial_values, 3) == \\\n            [[0, 0, 0, 0], [1, 0, 0, 5], [y, 0, 1, 15], [x, 1, 0, 2.5]]\n\n\n\n", "    def test_hyperplane_parameters():\n        triangle = Polygon(Point(0, 3), Point(5, 3), Point(1, 1))\n        assert hyperplane_parameters(triangle) == [(1, 0, -3), (0, 1, -3), (-1, -1, 2)]\n", "def test_main_integrate3d_with_empty_polygon():\n    faces = []\n    vertices = []\n    hp_params = []\n    assert main_integrate3d(1, faces, vertices, hp_params) == 0\n    assert main_integrate3d(x**2 + y**2 + z**2, faces, vertices, hp_params, max_degree=1) == {0: 0} \n", "def test_hyperplane_parameters():\n    triangle = Polygon(Point(0, 3), Point(5, 3), Point(1, 1))\n    facets = triangle.sides\n    expected_params = [(S(1), S(-3)), (S(-1), S(3)), (S(0), S(-1))]\n    assert hyperplane_parameters(facets, triangle.vertices) == expected_params\n\n"], "sample_1058": ["compilation error", "compilation error", "compilation error", "compilation error", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n    assert p.doprint(sign(x)) == 'sympy.sign(x)'\n    A = MatrixSymbol(\"A\", 2, 2)\n    assert p.doprint(A**(-1)) == 'A**(-1)'\n    assert p.doprint(A**5) == 'A**5'\n    assert p.doprint(x**-1) == 'x**(-1)'\n    assert p.doprint(x**-2) == 'x**(-2)'\n"], "sample_152": ["    def test_fast_delete_related_name(self):\n        a = Avatar.objects.create()\n        u = User.objects.create(avatar=a)\n        # 1 query to delete the avatar\n        # 1 query to fast-delete the user using the related name \"avatar_set\"\n        self.assertNumQueries(2, a.delete)\n        self.assertFalse(User.objects.exists())\n        self.assertFalse(Avatar.objects.exists())\n", "    def test_fast_delete_cascade_circular_fk(self):\n        a = A.objects.create()\n        b = B.objects.create(a=a)\n        a.b_set.add(b)\n        # 1 for a, 1 for b\n        with self.assertNumQueries(2):\n            a.delete()\n        self.assertFalse(A.objects.exists())\n        self.assertFalse(B.objects.exists())\n", "    def test_fast_delete_related_queryset_delete(self):\n        a = Avatar.objects.create()\n        u = User.objects.create(avatar=a)\n        # 1 for self, 1 for fast delete of User\n        self.assertNumQueries(2, a.delete)\n        self.assertFalse(Avatar.objects.exists())\n        self.assertFalse(User.objects.exists())\n", "    def test_fast_delete_signals_dont_affect_queryset_count(self):\n        u = User.objects.create()\n        s = signals.Signal(providing_args=['instance'])\n\n            pass\n\n        s.connect(receiver, sender=User)\n        # 1 query to delete the User\n        self.assertNumQueries(1, u.delete)\n        # Make sure the User count is correct after deletion\n        self.assertEqual(User.objects.count(), 0)\n        s.disconnect(receiver, sender=User)\n", "    def test_fast_delete_m2m_related_field_cascade(self):\n        d1 = Document.objects.create()\n        d2 = Document.objects.create()\n        t1 = Tag.objects.create(name='tag1')\n        t2 = Tag.objects.create(name='tag2')\n\n        d1.tags.add(t1, t2)\n        d2.tags.add(t1)\n\n        # We are deleting d1, which should cascade to the\n        # intermediate table.\n        self.assertNumQueries(2, d1.delete)\n        self.assertFalse(Document.objects.exists())\n\n        self.assertEqual(Tag.objects.count(), 2)\n"], "sample_75": ["    def test_prefetch_related_on_empty_queryset(self):\n        qs = AuthorWithAge.objects.filter(age__gt=100)\n        with self.assertNumQueries(1):\n            list(qs.prefetch_related('favorite_authors'))\n", "    def test_prefetch_related_empty_querysets(self):\n        with self.assertNumQueries(2):\n            books = Book.objects.filter(title='nonexistent').prefetch_related(\n                'first_time_authors',\n                'authors',\n            )\n        self.assertEqual(len(list(books)), 0)\n\n", "    def test_prefetch_related_empty_queryset(self):\n        with self.assertNumQueries(1):\n            Author.objects.prefetch_related('books').filter(id__in=[])\n", "    def test_invalidation_on_instance_modification(self):\n        with self.assertNumQueries(3):\n            books = Book.objects.filter(pk__in=[self.book1.pk, self.book2.pk]).prefetch_related('authors')\n        book1 = books.get(pk=self.book1.pk)\n        with self.assertNumQueries(0):\n            self.assertEqual(len(book1.authors.all()), 1)\n        book1.title = 'New Title'\n        book1.save()\n        with self.assertNumQueries(1):\n            self.assertEqual(len(book1.authors.all()), 1)\n", "    def test_prefetch_related_empty_queryset(self):\n        with self.assertNumQueries(1):\n            books = Book.objects.none().prefetch_related('authors')\n        self.assertEqual(len(books), 0)\n"], "sample_222": ["    def test_lock_exclusive_windows(self):\n        \"\"\"Test exclusive locking behavior on Windows.\"\"\"\n        if os.name != 'nt':\n            self.skipTest(\"This test only runs on Windows.\")\n        file_path = Path(__file__).parent / 'test.png'\n        with open(file_path, 'rb') as f1, open(file_path, 'rb') as f2:\n            self.assertTrue(locks.lock(f1, locks.LOCK_EX))\n            self.assertFalse(locks.lock(f2, locks.LOCK_EX | locks.LOCK_NB))\n            self.assertFalse(locks.lock(f2, locks.LOCK_SH | locks.LOCK_NB))\n            self.assertTrue(locks.unlock(f1))\n", "    def test_file_move_safe_no_overwrite(self):\n        handle_a, self.file_a = tempfile.mkstemp()\n        handle_b, self.file_b = tempfile.mkstemp()\n\n        self.assertIsNone(file_move_safe(self.file_a, self.file_b, allow_overwrite=False))\n\n        os.close(handle_a)\n        os.close(handle_b)\n", "    def test_file_move_safe_no_overwrite(self):\n        handle_a, self.file_a = tempfile.mkstemp()\n        handle_b, self.file_b = tempfile.mkstemp()\n\n        # Create a file at the destination to simulate an existing file.\n        with open(self.file_b, 'w') as f:\n            f.write('Existing content')\n\n        with self.assertRaises(FileExistsError):\n            file_move_safe(self.file_a, self.file_b, allow_overwrite=False)\n\n        os.close(handle_a)\n        os.close(handle_b)\n", "    def test_file_move_safe_nonexistent_source(self):\n        handle_b, self.file_b = tempfile.mkstemp()\n        with self.assertRaises(FileNotFoundError):\n            file_move_safe(\"nonexistent_file\", self.file_b)\n        os.close(handle_b)\n", "    def test_file_iteration_binary(self):\n        f = File(BytesIO(b'one\\ntwo\\nthree'))\n\n        # Ensure the file's content can be treated as bytes\n        for line in f:\n            self.assertIsInstance(line, bytes)\n"], "sample_783": ["    def test_missing_indicator_sparse_format(self, sparse_matrix):\n        X = sparse.csr_matrix([[1, 0], [0, 1]])\n        indicator = MissingIndicator(sparse=True)\n        X_trans = indicator.fit_transform(X.tocoo())\n        assert X_trans.format == 'csc'\n        # Test with different sparse input formats\n        for fmt in ['csr', 'csc', 'coo']:\n            X_fmt = X.toformat(fmt)\n            X_trans = indicator.fit_transform(X_fmt)\n            assert X_trans.format == 'csc'\n\n", "    def test_missingindicator_sparse_fit_transform_error(self, missing_values, dtype, arr_type):\n        X = arr_type([[missing_values, 1], [1, missing_values]]).astype(dtype)\n        indicator = MissingIndicator(missing_values=missing_values, sparse=True, features='missing-only')\n        with pytest.raises(ValueError, match=\"The sparse matrix must be in\"):\n            indicator.fit_transform(X)\n", "def test_missing_indicator_errors(missing_values, arr_type, dtype):\n    # Test that error are raised when incorrect input is given\n    X = arr_type([[missing_values, 1], [1, missing_values]], dtype=dtype)\n    indicator = MissingIndicator(missing_values=missing_values)\n    with pytest.raises(ValueError, match=\"Invalid value for 'features'\"):\n        indicator.fit_transform(X, features='invalid')\n    with pytest.raises(ValueError, match=\"Invalid value for 'sparse'\"):\n        indicator.fit_transform(X, sparse='invalid')\n", "    def test_missing_indicator_sparse_features_all(self, arr_type):\n        X_fit = arr_type([[np.nan, 1, np.nan],\n                          [4, np.nan, 2]])\n        X_trans = arr_type([[np.nan, 1, np.nan],\n                           [4, 12, 10]])\n        indicator = MissingIndicator(missing_values=np.nan,\n                                     features='all', sparse=True)\n        X_fit_mask = indicator.fit_transform(X_fit)\n        X_trans_mask = indicator.transform(X_trans)\n\n        assert X_fit_mask.format == 'csc'\n        assert X_trans_mask.format == 'csc'\n\n        X_fit_mask_expected = np.array([[1, 0, 1],\n                                       [0, 1, 0]])\n        X_trans_mask_expected = np.array([[1, 0, 1],\n                                         [0, 0, 0]])\n        assert_allclose(X_fit_mask.toarray(), X_fit_mask_expected)\n        assert_allclose(X_trans_mask.toarray(), X_trans_mask_expected)\n", "    def test_missing_indicator_error_empty_array():\n        X_fit = np.array([])\n        X_trans = np.array([])\n        indicator = MissingIndicator()\n        with pytest.raises(ValueError, match=\"Input X must not be empty.\"):\n            indicator.fit(X_fit).transform(X_trans)\n"], "sample_897": ["    def test_partial_dependence_display_n_lines(\n        self, pyplot, clf_diabetes, diabetes, features, expected_n_lines", "    def test_partial_dependence_display_kind_no_interaction(\n        pyplot,\n        kind,\n        clf_diabetes,\n        diabetes,", "    def test_partial_dependence_display_feature_names(\n        self, pyplot, clf_diabetes, diabetes, feature_names, expected_labels", "    def test_partial_dependence_display_axes(\n        self, pyplot, clf_diabetes, diabetes, features, expected_ax_shape", "    def test_partial_dependence_display_kind_centered_interaction_with_categorical(\n        pyplot,\n        kind,\n        clf_diabetes,\n        diabetes,"], "sample_14": ["def test_angle_with_mixed_units():\n    with pytest.raises(TypeError):\n        Angle([1*u.deg, 1*u.hour], unit=u.deg)\n", "def test_angle_units_conversion(cls):\n    q = cls(10, unit=u.deg)\n    assert q.to(u.hourangle).value == 10 / 15\n    assert q.to(u.rad).value == np.deg2rad(10)\n    assert q.to(u.arcmin).value == 10 * 60\n    assert q.to(u.arcsec).value == 10 * 3600\n", "compilation error", "    def test_invalid_wrap_angle(cls):\n        \"\"\"\n        Regression test for issue #1423\n\n        Ensure that invalid wrap_angles raise a TypeError.\n        \"\"\"\n        with pytest.raises(TypeError):\n            cls(10, unit='deg', wrap_angle='invalid')\n", "    def test_angle_to_string_always_include_degrees(angle):\n        \"\"\"\n        Test that angle.to_string() always includes degrees sign.\n\n        This ensures user-friendly output for angles regardless of input\n        format.\n        \"\"\"\n        assert 'd' in angle.to_string()\n"], "sample_6": ["def test_angle_wrap_at():\n    a1 = Angle([10, 370, -10], unit=u.deg)\n    a2 = a1.wrap_at(180 * u.deg)\n    assert_allclose(a2.degree, [10, -10, 170])\n\n    a3 = Angle([10, 370, -10], unit=u.deg)\n    a3.wrap_at(180 * u.deg, inplace=True)\n    assert_allclose(a3.degree, [10, -10, 170])\n\n    a4 = Angle([-10, -370, 10], unit=u.deg)\n    a4 = a4.wrap_at('0d')\n    assert_allclose(a4.degree, [350, 20, 10])\n\n", "def test_angle_formats():\n    a1 = Angle(\"12h34m56.7s\")\n    assert a1.unit == u.hour\n    assert a1.to(u.degree).value == pytest.approx(189.72133333)\n\n    a2 = Angle(\"12 degrees 34 arcminutes 56.7 arcseconds\")\n    assert a2.unit == u.degree\n    assert a2.to(u.hour).value == pytest.approx(189.72133333 / 15)\n\n    a3 = Angle(\"12d34m\")\n    assert a3.unit == u.degree\n    assert a3.to(u.hour).value == pytest.approx(12.56666667)\n\n    with pytest.raises(ValueError):\n        Angle(\"12h34m56.7s degrees\")\n", "    def test_angle_wrap_at(self):\n        a = Angle([10, 350, 720], unit=u.deg)\n\n        a_wrapped = a.wrap_at(180 * u.deg)\n        npt.assert_allclose(a_wrapped.degree, [10, -10, 180])\n\n        a_wrapped_inplace = a.copy()\n        a_wrapped_inplace.wrap_at(180 * u.deg, inplace=True)\n        npt.assert_allclose(a_wrapped_inplace.degree, [10, -10, 180])\n        \n", "compilation error", "    def test_angle_wrap_at(self):\n        a = Angle([350, 360, 370, 400], unit=u.deg)\n        a.wrap_at(180 * u.deg, inplace=True)\n        assert_allclose(a.degree, [-10, 0, 10, 160])\n\n        a2 = Angle([-1, 181], unit=u.deg).wrap_at(0 * u.deg)\n        assert_allclose(a2.degree, [359, -1])\n\n        a3 = Angle([[-1, 181], [359, 361]], unit=u.deg)\n        a3.wrap_at(0 * u.deg, inplace=True)\n        assert_allclose(a3.degree, [[359, -1], [-1, 1]])\n\n        with pytest.raises(ValueError):\n            Angle([180, 360], unit=u.deg).wrap_at(100 * u.deg)\n"], "sample_675": ["def test_log_cli_level_dynamic(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            plugin.log_cli_handler.setLevel(logging.DEBUG)\n            logging.getLogger('catchlog').debug(\"This log message should be shown\")\n            print('PASSED')\n    \"\"\"\n    )\n\n    result = testdir.runpytest(\"-s\")\n\n    # fnmatch_lines does an assertion internally\n    result.stdout.fnmatch_lines(\n        [\n            \"*test_log_cli_dynamic.py* This log message should be shown\",\n            \"PASSED\", \n        ]\n    )\n\n    # make sure that that we get a '0' exit code for the testsuite\n    assert result.ret == 0\n", "def test_log_file_rotation(testdir, tmpdir):\n    log_file = tmpdir.join(\"pytest.log\")\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file={}\n        log_file_level = INFO\n        log_cli=true\n        log_file_rotation=100\n        \"\"\".format(\n            log_file\n        )\n    )\n    testdir.makepyfile(\n        \"\"\"\n            import logging\n            import time\n            logger = logging.getLogger(__name__)\n\n                for i in range(200):\n                    logger.info(f\"message {i}\")\n                    time.sleep(0.01)\n\n        \"\"\"\n    )\n    testdir.runpytest()\n\n    assert log_file.exists()\n    assert log_file.size() > 0\n\n    # Check if there are additional rotated log files (pytest.log.1, pytest.log.2, etc.)\n    rotated_files = [f for f in tmpdir.listdir() if f.basename.startswith(str(log_file))]\n\n    assert len(rotated_files) > 0\n", "def test_log_cli_level_during_test_execution(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import pytest\n\n        @pytest.fixture\n            logging.getLogger().setLevel(logging.DEBUG)\n            logging.debug('debug log from fixture')\n\n            logging.info('info log from test')\n            assert True\n    \"\"\"\n    )\n\n    result = testdir.runpytest(\"--log-cli-level=WARNING\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*test_log_cli_level_during_test_execution.py*\",\n            \"*info log from test*\",\n            \"PASSED\",\n        ]\n    )\n", "    def test_log_cli_level_set_during_test(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import logging\n            import pytest\n\n                plugin = request.config.pluginmanager.getplugin('logging-plugin')\n                assert plugin.log_cli_handler.level == logging.WARNING\n                logging.getLogger('catchlog').debug(\"This log message won't be shown\")\n                logging.getLogger('catchlog').info(\"This log message will be shown\")\n                plugin.set_log_cli_level(logging.DEBUG)\n                assert plugin.log_cli_handler.level == logging.DEBUG\n                logging.getLogger('catchlog').debug(\"This log message will now be shown\")\n                print('PASSED')\n        \"\"\"\n        )\n        result = testdir.runpytest(\"-s\")\n        result.stdout.fnmatch_lines(\n            [\n                \"*test_log_cli_level_set_during_test.py*\",\n                \"*-- live log call --*\",\n                \"*This log message will be shown*\",\n                \"*-- live log call --*\",\n                \"*This log message will now be shown*\",\n                \"PASSED\",\n            ]\n        )\n", "def test_log_file_handler_encoding(testdir):\n    log_file = testdir.tmpdir.join(\"pytest.log\").strpath\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file={}\n        log_file_level=INFO\n        log_file_encoding=latin-1\n        \"\"\".format(\n            log_file\n        )\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n            logging.getLogger('catchlog').info(\"\u00e4\u00f6\u00fc\")\n            print('PASSED')\n    \"\"\"\n    )\n\n    result = testdir.runpytest(\"-s\")\n\n    # fnmatch_lines does an assertion internally\n    result.stdout.fnmatch_lines([\"test_log_file_handler_encoding.py PASSED\"])\n\n    # make sure that that we get a '0' exit code for the testsuite\n    assert result.ret == 0\n    assert os.path.isfile(log_file)\n    with open(log_file, \"rb\") as rfh:\n        contents = rfh.read()\n        assert contents == b\"\u00e4\u00f6\u00fc\\n\".encode(\"latin-1\") \n"], "sample_92": ["    def test_get_user_model_from_settings(self):\n        # Verify that get_user_model() returns the correct model.\n        self.assertEqual(get_user_model(), CustomPermissionsUser)\n", "    def test_authenticate_superusers(self):\n        \"\"\"\n        Superusers should bypass permission checks when using a custom backend.\n\n        This test verifies that superusers can authenticate even if\n        a custom backend returns False for has_perm.\n        \"\"\"\n        self.superuser.is_active = False\n        self.superuser.save()\n        self.assertTrue(self.client.login(username=self.superuser.username, password=self.superuser.password))\n\n", "    def test_authenticate_custom_permissions_user(self):\n        test_user = CustomPermissionsUser._default_manager.create_user(\n            email='test@example.com',\n            password='test',\n            date_of_birth=date(2006, 4, 25)\n        )\n        authenticated_user = authenticate(email='test@example.com', password='test')\n        self.assertEqual(test_user, authenticated_user)\n", "    def test_authenticate_custom_permissions_user(self):\n        test_user = CustomPermissionsUser.objects.create_user(\n            email='test@example.com',\n            password='test',\n            date_of_birth=date(2006, 4, 25)\n        )\n        authenticated_user = authenticate(email='test@example.com', password='test')\n        self.assertEqual(test_user, authenticated_user)\n", "    def test_authentication_with_deprecated_hasher(self):\n        self.user.set_password('test')\n        self.user.save()\n\n        with self.assertNumQueries(1):\n            authenticated_user = authenticate(**self.user_credentials)\n        self.assertEqual(authenticated_user, self.user)\n\n"], "sample_767": ["compilation error", "compilation error", "compilation error", "def test_column_transformer_callable_specifier_pandas_multiindex():\n    pd = pytest.importorskip('pandas')\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    X_df = pd.DataFrame(X_array, columns=pd.MultiIndex.from_tuples([('A', 'a'),\n                                                                 ('A', 'b'),\n                                                                 ('B', 'c')]))\n    X_res_first = np.array([[0, 1, 2]]).T\n\n        assert isinstance(X.columns, pd.MultiIndex)\n        assert_array_equal(X.columns.values, X_df.columns.values)\n        assert_array_equal(X.values, X_df.values)\n        return [('A', 'a')]\n\n    ct = ColumnTransformer([('trans', Trans(), func)],\n                           remainder='drop')\n    assert_array_equal(ct.fit_transform(X_df), X_res_first)\n    assert_array_equal(ct.fit(X_df).transform(X_df), X_res_first)\n    assert callable(ct.transformers[0][2])\n    assert ct.transformers_[0][2] == [('A', 'a')]\n", "    def func(X):\n        return None\n"], "sample_679": ["    def test_pytest_param_id_allows_none_or_string(self, testdir):\n        p = testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.parametrize(\"x\", [1, 2], ids=[\"a\", None])\n                pass\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines([\"test_func[a]\", \"test_func[None]\"])\n", "    def test_pytest_param_id_allows_none_or_string_in_parametrize(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.parametrize(\"input, expected\", [\n                (None, \"None\"),\n                (\"hello world\", \"hello world\"),\n            ])\n                assert pytest.param(input, id=input).id == expected\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.assert_outcomes(passed=2)\n", "    def test_parametrize_with_empty_marker_name(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.parametrize(\"arg\", [1, 2],\n            ids=[\"\", \"two\"])\n                pass\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.assert_outcomes(passed=2)\n", "    def test_pytest_param_id_with_extra_kwargs(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n                print(request.param.id)\n\n            @pytest.mark.parametrize(\n                \"arg, extra\",\n                [\n                    pytest.param(1, 2, id=\"int\"),\n                    pytest.param(\"hello\", 3, id=\"string\"),\n                ],\n            )\n                pass\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines([\"*int*\", \"*string*\"])\n", "    def test_pytest_param_id_none_gets_default(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n                pass\n            pytest.mark.parametrize('x', [1, 2], ids=[None, 'two'])\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines([\"*test_func[1] *\", \"*test_func[two] *\"])\n"], "sample_251": ["    def test_values_only_alias(self):\n        qs = Book.objects.alias(rating_alias=F('rating') - 1).values('rating_alias')\n        self.assertEqual(len(qs.first()), 1)\n", "    def test_select_related_alias(self):\n        qs = Book.objects.alias(\n            rating_alias=F('rating') - 1,\n        ).select_related('contact').filter(contact__name='Adrian Holovaty', rating_alias=3.5)\n        self.assertSequenceEqual(qs, [self.b1])\n\n", "    def test_select_related_alias(self):\n        qs = Book.objects.alias(rating_alias=F('rating')).select_related('publisher').filter(\n            rating_alias=F('publisher__num_awards')\n        )\n        self.assertEqual(qs.count(), 0)\n", "    def test_select_related_alias(self):\n        qs = Book.objects.select_related('publisher').alias(\n            publisher_alias=F('publisher__name'),\n        )\n        self.assertEqual(qs.first().publisher_alias, 'Apress')\n", "    def test_alias_with_model_field(self):\n        qs = Author.objects.alias(full_name=Concat(F('name'), Value(', '), F('age')))\n        self.assertIs(hasattr(qs.first(), 'full_name'), False)\n        expected = [\n            'Adrian Holovaty, 34',\n            'Jacob Kaplan-Moss, 35',\n            'James Bennett, 34',\n            'Peter Norvig, 57',\n            'Stuart Russell, 46',\n        ]\n        for author, expected_full_name in zip(qs, expected):\n            self.assertEqual(author.name + ', ' + str(author.age), expected_full_name)\n"], "sample_872": ["    def test_label_ranking_avg_precision_score_perfect_score():\n        y_true = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n        y_score = np.array([[0.9, 0.1, 0], [0, 0.8, 0.2], [0, 0, 1]])\n        result = label_ranking_average_precision_score(y_true, y_score)\n        assert result == pytest.approx(1) \n", "    def test_label_ranking_average_precision_score_empty_dataset():\n        y_true = np.array([])\n        y_score = np.array([])\n        with pytest.raises(ValueError, match=r\"y_true and y_score must not be empty\"):\n            label_ranking_average_precision_score(y_true, y_score)\n", "    def test_label_ranking_avg_precision_score_empty_prediction():\n        y_true = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n        y_score = np.zeros((3, 3))\n        with pytest.warns(UndefinedMetricWarning, match=\"No positive labels found in y_score\"):\n            result = label_ranking_average_precision_score(y_true, y_score)\n        assert result == 0\n", "    def test_label_ranking_avg_precision_score_empty_label_set():\n        # Test when no labels are present in y_true for a specific sample\n        y_true = np.array([[0, 0, 0], [0, 1, 0]])\n        y_score = np.array([[0.5, 0.9, 0.6], [0, 0, 1]])\n        result = label_ranking_average_precision_score(y_true, y_score)\n        assert result == pytest.approx(0.5)\n", "    def test_top_k_accuracy_score_with_ties(y_true, y_score, k, expected_score):\n        assert top_k_accuracy_score(y_true, y_score, k=k) == pytest.approx(expected_score)\n\n"], "sample_948": ["    def test_cpp_xref_role_target_not_found(app, warning):\n        text = (\n            \".. cpp:function:: func_not_defined()\\n\"\n            \"\\n\"\n            \"See :cpp:func:`func_not_defined` for more details.\"\n        )\n        restructuredtext.parse(app, text)\n        ws = warning.getvalue().split(\"\\n\")\n        assert len(ws) == 1\n        assert \"WARNING: cpp:function reference target not found: func_not_defined\" in ws[0]\n", "    def test_noindexentry_in_domain(app):\n        text = (\".. cpp:namespace:: Ns\\n\"\n                \"   .. cpp:function:: void f()\\n\"\n                \"      :noindexentry:\\n\")\n        doctree = restructuredtext.parse(app, text)\n        assert_node(doctree, (addnodes.index, desc, addnodes.index, desc))\n        assert_node(doctree[0], addnodes.index, entries=[('single', 'Ns (C++ namespace)', '_CPPv42Ns', '', None)])\n        assert_node(doctree[2], addnodes.index, entries=[])\n", "def test_cpp_any_noindexentry(app, warning):\n    text = (\n        \".. cpp:any:: void f()\\n\"\n        \"   :noindexentry:\\n\"\n        \".. cpp:function:: void g()\\n\"\n    )\n    restructuredtext.parse(app, text)\n    ws = warning.getvalue().split(\"\\n\")\n    assert len(ws) == 2\n    assert \"index.rst:1: WARNING: Duplicate C++ declaration\" in ws[0]\n    assert \"Declaration is '.. cpp:any:: void f()'.\" in ws[1]\n\n", "    def test_decl_duplicate_different_files(app, warning, tmpdir):\n        # Issue 8270\n        (tmpdir / 'a.rst').write_text(\n            \".. cpp:struct:: A\\n\"\n            \".. cpp:function:: void A()\\n\"\n        )\n        (tmpdir / 'b.rst').write_text(\n            \".. cpp:struct:: A\\n\"\n        )\n        app.builder.build([str(tmpdir / 'a.rst'), str(tmpdir / 'b.rst')])\n        ws = warning.getvalue().split(\"\\n\")\n        assert len(ws) == 3\n        assert \"WARNING: Duplicate C++ declaration, also defined at 'b.rst':2.\" in ws[0]\n        assert \"Declaration is '.. cpp:struct:: A'.\" in ws[1]\n        assert ws[2] == \"\"\n", "    def test_cpp_member_with_default_value_and_initializer(app, warning):\n        # Issue 10152\n        text = (\n            \".. cpp:class:: MyClass\\n\"\n            \"   :members:\\n\"\n            \"   \\n\"\n            \"   .. cpp:member:: int myVar = 42\\n\"\n            \"   \\n\"\n            \"   .. cpp:member:: int myOtherVar{}\\n\"\n        )\n        restructuredtext.parse(app, text)\n        ws = warning.getvalue().split(\"\\n\")\n        assert len(ws) == 0\n\n"], "sample_665": ["compilation error", "def test_collect_only_with_testpaths(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            pass\n        \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        testpaths = tests\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-o\", \"python_files=*\", \"--collect-only\")\n    result.stdout.fnmatch_lines([\"collected 1 item\"])\n", "    def test_collect_symlink_to_init(testdir):\n        sub = testdir.mkdir(\"sub\")\n        sub.ensure(\"__init__.py\").write(\"def test_func(): pass\")\n        symlink = testdir.tmpdir.join(\"symlink\")\n        symlink.mksymlinkto(sub)\n        result = testdir.runpytest(symlink)\n        result.stdout.fnmatch_lines([\"sub/__init__.py::test_func PASSED*\", \"*1 passed in*\"])\n", "    def test_collect_symlink_in_conftest_skip(testdir):\n        \"\"\"Test that symlinks in conftest.py are skipped (#4787).\"\"\"\n        real = testdir.makepyfile(\n            real=\"\"\"\n                pass\n            \"\"\"\n        )\n        symlink = testdir.tmpdir.join(\"conftest.py.symlink\")\n        symlink.mksymlinkto(real)\n        result = testdir.runpytest_subprocess(str(symlink))\n        assert result.ret == ExitCode.NO_TESTS_COLLECTED\n", "compilation error"], "sample_18": ["    def test_view(self):\n        q_view = self.q.view(np.ndarray)\n        assert_no_info(q_view)\n\n", "    def test_field_access(self):\n        q_p = self.q['p'].copy()\n        assert_info_equal(q_p, self.q)\n        q_v = self.q['v'].copy()\n        assert_info_equal(q_v, self.q)\n", "    def test_view(self):\n        q = self.q.view(u.Quantity)\n        assert_info_equal(q, self.q, ignore={\"unit\"})\n", "    def test_views_retain_info(self):\n        q_view = self.q[0]\n        assert_info_equal(q_view, self.q, ignore={\"unit\"})\n", "    def test_view(self):\n        q = self.q.view(np.ndarray)\n        assert_no_info(q)\n"], "sample_523": ["compilation error", "def test_legend_handler_get_legend_handles():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1], label='line')\n    ax.scatter([0, 1], [0, 1], label='scatter')\n\n    handler_map = {\n        'line': mlines.Line2D,\n        'scatter': mcollections.PathCollection\n    }\n\n    handles = [h for h in ax.get_legend_handles_labels()[0]\n               if isinstance(h, (mlines.Line2D, mcollections.PathCollection))]\n    labels = [l for l in ax.get_legend_handles_labels()[1]\n              if isinstance(h, (mlines.Line2D, mcollections.PathCollection))]\n\n    assert len(handles) == len(labels) == 2\n\n    for i in range(2):\n        h = handles[i]\n        l = labels[i]\n        handler = mlegend.LegendHandler(handler_map.get(l, None))\n\n        assert handler.get_legend_handles(h) == [h]\n\n", "    def test_legend_with_empty_labels():\n        fig, ax = plt.subplots()\n        ax.plot([1, 2], [2, 3], label='')\n        ax.plot([2, 3], [3, 4], label='')\n        ax.legend()\n", "    def test_legend_with_empty_string_label():\n        fig, ax = plt.subplots()\n        ax.plot([1, 2], [1, 2], label='')\n        with pytest.warns(UserWarning) as record:\n            ax.legend()\n        assert len(record) == 1\n        assert \"empty label\" in str(record[0].message)\n", "    def test_legend_title_fontdict():\n        # Test setting title font properties via fontdict\n        fig, ax = plt.subplots()\n        ax.plot(range(10), label='test')\n        fontdict = {'family': 'serif', 'size': 16, 'weight': 'bold'}\n        leg = ax.legend(title='Aardvark', title_fontdict=fontdict)\n        title_text = leg.get_title()\n        assert title_text.get_fontfamily()[0] == 'serif'\n        assert title_text.get_fontsize() == 16\n        assert title_text.get_fontweight() == 'bold'\n"], "sample_682": ["def test_skipif_condition_with_globals(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        x = 3\n        @pytest.mark.skipif(\"x == 3\")\n            pass\n        @pytest.mark.xfail(\"x != 3\")\n            assert 0\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-rsx\")\n    result.stdout.fnmatch_lines([\"*SKIP*x == 3*\", \"*XFAIL*test_boolean*\", \"*x != 3*\"])\n", "def test_skipif_condition_with_globals(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        x = 3\n        @pytest.mark.skipif(\"x == 3\")\n            pass\n        @pytest.mark.skipif(\"x == 4\")\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-rs\")\n    result.stdout.fnmatch_lines([\"*SKIP*test_skip1*\", \"*x == 3*\"])\n    result.stdout.fnmatch_lines([\"*1 skipped*1 passed*\"])\n", "    def test_importorskip_with_reason(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n            pytest.importorskip(\"doesnotexist\", reason=\"missing library\")\n\n                assert True\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines(\n            [\n                \"*SKIPPED*missing library*\",\n                \"*1 skipped*\",\n            ]\n        )\n", "    def test_importorskip_with_module_path(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n                pytest.importorskip(\"os.path\")\n                assert hasattr(os.path, 'exists')\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        result.assert_outcomes(passed=1)\n", "    def test_importorskip_with_reason(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n                pytest.importorskip(\"doesnotexist\", reason=\"Missing\")\n                assert 0\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines([f\"*SKIP*Missing*\", \"*1 skipped*\"])\n"], "sample_311": ["    def test_known_url_missing_slash_404_if_not_authenticated_without_final_catch_all_view(self):\n        known_url = reverse('admin10:admin_views_article_changelist')\n        response = self.client.get(known_url[:-1])\n        self.assertEqual(response.status_code, 404)\n", "    def test_missing_slash_append_slash_true_non_staff_user_without_final_catch_all_view(self):\n        user = User.objects.create_user(\n            username='user',\n            password='secret',\n            email='user@example.com',\n            is_staff=False,\n        )\n        self.client.force_login(user)\n        known_url = reverse('admin10:admin_views_article_changelist')\n        response = self.client.get(known_url[:-1])\n        self.assertRedirects(\n            response,\n            '/test_admin/admin10/login/?next=/test_admin/admin10/admin_views/article/',\n        )\n", "    def test_trailing_slash_on_known_url_without_final_catch_all_view(self):\n        superuser = User.objects.create_user(\n            username='staff',\n            password='secret',\n            email='staff@example.com',\n            is_staff=True,\n        )\n        self.client.force_login(superuser)\n        known_url = reverse('admin10:admin_views_article_changelist')\n        response = self.client.get(known_url + '/')\n        self.assertEqual(response.status_code, 200)\n", "    def test_known_url_missing_slash_redirects_with_slash(self):\n        superuser = User.objects.create_superuser(\n            username='super',\n            password='secret',\n            email='super@example.com',\n        )\n        self.client.force_login(superuser)\n        known_url = reverse('admin:admin_views_article_changelist')\n        response = self.client.get(known_url[:-1])\n        self.assertRedirects(response, known_url, status_code=301, fetch_redirect_response=False)\n\n", "    def test_missing_slash_append_slash_true_non_staff_user_without_final_catch_all_view(self):\n        user = User.objects.create_user(\n            username='user',\n            password='secret',\n            email='user@example.com',\n            is_staff=False,\n        )\n        self.client.force_login(user)\n        known_url = reverse('admin10:admin_views_article_changelist')\n        response = self.client.get(known_url[:-1])\n        self.assertRedirects(response, '/test_admin/admin10/login/?next=/test_admin/admin10/admin_views/article')\n\n"], "sample_273": ["    def test_abstract_model_with_no_pk(self):\n        class AbstractModel(models.Model):\n            class Meta:\n                abstract = True\n\n        class ConcreteModel(AbstractModel):\n            pass\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [\n            Warning(self.msg, hint=self.hint, obj=AbstractModel, id='models.W042'),\n        ])\n", "    def test_abstract_base_class(self):\n        class AbstractBase(models.Model):\n            class Meta:\n                abstract = True\n\n        class ConcreteModel(AbstractBase):\n            pass\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [\n            Warning(self.msg, hint=self.hint, obj=AbstractBase, id='models.W042'),\n        ])\n\n", "    def test_no_warning_for_proxy_model(self):\n        class Parent(models.Model):\n            pass\n\n        class ProxyModel(Parent):\n            class Meta:\n                proxy = True\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [])\n\n", "    def test_custom_default_auto_field(self):\n        with mock.patch('django.db.models.base.DEFAULT_AUTO_FIELD', 'check_framework.test_model_checks.CustomAutoField'):\n            class Model(models.Model):\n                pass\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [])\n", "def test_proxy_model_inherited_pk(self):\n    class Parent(models.Model):\n        pass\n\n    class Proxy(Parent):\n        class Meta:\n            proxy = True\n\n    self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [\n        Warning(self.msg, hint=self.hint, obj=Parent, id='models.W042'),\n    ])\n"], "sample_893": ["    def test_plot_tree_regression(pyplot):\n        # mostly smoke tests\n        # Check correctness of export_graphviz for regression\n\n        reg = DecisionTreeRegressor(\n            max_depth=3, min_samples_split=2, random_state=2\n        )\n        reg.fit(X, y)\n        # Test export code\n        feature_names = [\"first feat\", \"sepal_width\"]\n        nodes = plot_tree(reg, feature_names=feature_names)\n        assert len(nodes) == 3\n        assert nodes[0].get_text() == (\n            \"first feat <= 0.0\\n\"\n            \"squared_error = 1.0\\n\"\n            \"samples = 6\\n\"\n            \"value = -0.0\"\n        )\n        assert nodes[1].get_text() == (\n            \"squared_error = 0.0\\n\" \"samples = 3\\n\" \"value = -1.0\"\n        )\n        assert nodes[2].get_text() == (\n            \"squared_error = 0.0\\n\" \"samples = 3\\n\" \"value = 1.0\"\n        )\n", "    def test_plot_tree_multioutput(pyplot):\n        # Test plot_tree with multi-output regression\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        y = np.array([[1, 2], [3, 4], [5, 6]])\n        reg = DecisionTreeRegressor(random_state=0)\n        reg.fit(X, y)\n        nodes = plot_tree(reg)\n        assert len(nodes) == 3\n\n", "    def test_plot_tree_regression(pyplot):\n        # Check correctness of plot_tree for regression\n        clf = DecisionTreeRegressor(\n            max_depth=3, min_samples_split=2, criterion=\"squared_error\", random_state=2\n        )\n        clf.fit(X, y)\n\n        # Test export code\n        nodes = plot_tree(clf)\n        assert len(nodes) == 3\n        assert nodes[0].get_text() == \"x[0] <= 0.0\\nsquared_error = 1.0\\nsamples = 6\\nvalue = 0.0\"\n        assert nodes[1].get_text() == \"squared_error = 0.0\\nsamples = 3\\nvalue = -1.0\"\n        assert nodes[2].get_text() == \"squared_error = 0.0\\nsamples = 3\\nvalue = 1.0\" \n\n", "    def test_plot_tree_regressor(pyplot):\n        # Check correctness of plot_tree for regression\n        reg = DecisionTreeRegressor(\n            max_depth=3, min_samples_split=2, random_state=2\n        )\n        reg.fit(X, y)\n\n        # Test export code\n        nodes = plot_tree(reg, feature_names=[\"first feat\", \"sepal_width\"])\n        assert len(nodes) == 3\n        assert (\n            nodes[0].get_text()\n            == \"first feat <= 0.0\\nvalue = 0.0\\nsamples = 6\"\n        )\n        assert nodes[1].get_text() == \"value = -1.0\\nsamples = 3\"\n        assert nodes[2].get_text() == \"value = 1.0\\nsamples = 3\"\n", "    def test_plot_tree_friedman_mse(pyplot):\n        # mostly smoke tests\n        # Check correctness of export_graphviz for criterion = friedman_mse\n        clf = DecisionTreeRegressor(\n            max_depth=3, min_samples_split=2, criterion=\"friedman_mse\", random_state=2\n        )\n        clf.fit(X, y)\n\n        # Test export code\n        feature_names = [\"first feat\", \"sepal_width\"]\n        nodes = plot_tree(clf, feature_names=feature_names)\n        assert len(nodes) == 3\n        assert (\n            nodes[0].get_text()\n            == \"first feat <= 0.0\\nfriedman_mse = 1.0\\nsamples = 6\\nvalue = [0.0]\"\n        )\n        assert nodes[1].get_text() == \"friedman_mse = 0.0\\nsamples = 3\\nvalue = [ -1.0]\"\n        assert nodes[2].get_text() == \"friedman_mse = 0.0\\nsamples = 3\\nvalue = [1.0]\"\n"], "sample_245": ["    def test_custom_locale_paths_with_symlinks(self):\n        symlink_target = os.path.join(self.test_dir, 'app_with_locale', 'locale')\n        symlink_path = os.path.join(self.test_dir, 'locale', 'symlinked_locale')\n        try:\n            os.symlink(symlink_target, symlink_path)\n\n            with override_settings(LOCALE_PATHS=[symlink_path]):\n                management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n                app_de_locale = os.path.join(\n                    self.test_dir, 'app_with_locale', 'locale', 'de', 'LC_MESSAGES', 'django.po')\n                self.assertTrue(os.path.exists(app_de_locale))\n\n                with open(app_de_locale) as fp:\n                    po_contents = fp.read()\n                    self.assertMsgId('This app has a locale directory', po_contents)\n\n        finally:\n            if os.path.exists(symlink_path):\n                os.unlink(symlink_path)\n", "    def test_extract_from_subdirectories(self):\n        \"\"\"\n        Test that makemessages extracts strings from subdirectories correctly.\n        \"\"\"\n        management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertMsgId(\"This literal should be included from a subdirectory.\", po_contents)\n", "    def test_domain(self):\n        with override_settings(LOCALE_PATHS=[self._test_dir_path('project_locale')]):\n            management.call_command('makemessages', locale=[LOCALE], domain='custom_domain', verbosity=0)\n            custom_de_locale = os.path.join(\n                self._test_dir_path('project_locale'), 'de', 'LC_MESSAGES', 'custom_domain.po')\n            self.assertTrue(os.path.exists(custom_de_locale))\n\n            with open(custom_de_locale) as fp:\n                po_contents = fp.read()\n                self.assertMsgId('This string should be in custom_domain.po', po_contents)\n", "    def test_custom_default_locale_path(self):\n        with override_settings(DEFAULT_LOCALE_PATH='project_locale'):\n            management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n            project_de_locale = os.path.join(\n                self.test_dir, 'project_locale', 'de', 'LC_MESSAGES', 'django.po')\n            self.assertTrue(os.path.exists(project_de_locale))\n", "    def test_project_locale_paths_different_project_locale(self):\n        with override_settings(LOCALE_PATHS=[\n                os.path.join(self.test_dir, 'another_project_locale')]):\n            management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n            project_de_locale = os.path.join(\n                self.test_dir, 'another_project_locale', 'de', 'LC_MESSAGES', 'django.po')\n            self.assertTrue(os.path.exists(project_de_locale))\n"], "sample_0": ["def test_conversion_to_from_variance_not_supported(UncertClass):\n    uncert = np.arange(1, 11).reshape(2, 5) * u.adu\n    start_uncert = UncertClass(uncert)\n    with pytest.raises(NotImplementedError):\n        start_uncert.represent_as(VarianceUncertainty)\n", "def test_conversion_to_from_variance_not_supported(UncertClass):\n    uncert = np.arange(1, 11).reshape(2, 5) * u.adu\n    start_uncert = UncertClass(uncert)\n    with pytest.raises(NotImplementedError):\n        start_uncert.represent_as(VarianceUncertainty)\n", "def test_conversion_to_variance_not_supported(UncertClass):\n    uncert = np.arange(1, 11).reshape(2, 5) * u.adu\n    start_uncert = UncertClass(uncert)\n    with pytest.raises(NotImplementedError):\n        start_uncert.represent_as(VarianceUncertainty)\n", "def test_conversion_to_variance_unsupported(UncertClass):\n    uncert = np.arange(1, 11).reshape(2, 5) * u.adu\n    start_uncert = UncertClass(uncert)\n    with pytest.raises(NotImplementedError):\n        start_uncert.represent_as(VarianceUncertainty)\n\n", "def test_conversion_to_from_variance_not_supported(UncertClass):\n    uncert = np.arange(1, 11).reshape(2, 5) * u.adu\n    start_uncert = UncertClass(uncert)\n    with pytest.raises(NotImplementedError):\n        var_uncert = start_uncert.represent_as(VarianceUncertainty)\n"], "sample_321": ["    def test_csrf_cookie_age_zero(self):\n        \"\"\"\n        CSRF cookie age can be set to 0, effectively making them session-based\n        cookies.\n        \"\"\"\n        req = self._get_GET_no_csrf_cookie_request()\n\n        MAX_AGE = 0\n        with self.settings(CSRF_COOKIE_NAME='csrfcookie',\n                           CSRF_COOKIE_DOMAIN='.example.com',\n                           CSRF_COOKIE_AGE=MAX_AGE,\n                           CSRF_COOKIE_PATH='/test/',\n                           CSRF_COOKIE_SECURE=True,\n                           CSRF_COOKIE_HTTPONLY=True):\n            # token_view calls get_token() indirectly\n            mw = CsrfViewMiddleware(token_view)\n            mw.process_view(req, token_view, (), {})\n            resp = mw(req)\n            max_age = resp.cookies.get('csrfcookie').get('max-age')\n            self.assertEqual(max_age, '0')\n", "    def test_bad_csrf_cookie_missing_session_key(self):\n        \"\"\"\n        If the CSRF cookie doesn't have the session key, it should be rejected.\n        \"\"\"\n        req = self._get_GET_no_csrf_cookie_request()\n        req.COOKIES[settings.CSRF_COOKIE_NAME] = self._csrf_id_cookie\n        mw = CsrfViewMiddleware(token_view)\n        with self.assertRaises(ImproperlyConfigured):\n            mw.process_view(req, token_view, (), {})\n", "    def test_bare_secret_rejected(self):\n        \"\"\"\n        The csrf token is rejected when only a bare secret is present.\n\n        \"\"\"\n        req = self._get_POST_bare_secret_csrf_cookie_request()\n        mw = CsrfViewMiddleware(post_form_view)\n        with self.assertRaises(Http403):\n            mw.process_view(req, post_form_view, (), {})\n", "    def test_session_cookie_domain_reject_cross_domain(self):\n        \"\"\"\n        A POST HTTPS request from a domain that doesn't match SESSION_COOKIE_DOMAIN\n        should be rejected.\n        \"\"\"\n        req = self._get_POST_request_with_token()\n        req._is_secure_override = True\n        req.META['HTTP_REFERER'] = 'https://otherdomain.com/'\n        req.META['SERVER_PORT'] = '443'\n        mw = CsrfViewMiddleware(post_form_view)\n        response = mw.process_view(req, post_form_view, (), {})\n        self.assertContains(\n            response,\n            'Referer checking failed - Referer host mismatch.',\n            status_code=403,\n        )\n", "    def test_https_good_referer_with_port(self):\n        \"\"\"\n        A POST HTTPS request with a good referer should be accepted when the referer includes the port.\n        \"\"\"\n        req = self._get_POST_request_with_token()\n        req._is_secure_override = True\n        req.META['HTTP_REFERER'] = 'https://www.example.com:443/'\n        req.META['SERVER_PORT'] = '443'\n        mw = CsrfViewMiddleware(post_form_view)\n        response = mw.process_view(req, post_form_view, (), {})\n        self.assertIsNone(response)\n"], "sample_633": ["def test_empty_file() -> None:\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([str(INPUT / \"empty_file.py\"), str(INPUT / \"empty_file.py\")])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == \"\"\"", "def test_different_number_of_lines() -> None:\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([SIMILAR1, SIMILAR4])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == (\n            f\"\"\"", "def test_similar_lines_with_comments() -> None:\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([SIMILAR_CLS_A, SIMILAR_CLS_B])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue().strip()\n        == (\n            f'''", "def test_empty_file() -> None:\n    output = StringIO()\n    with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n        similar.Run([str(INPUT / \"empty_file\")])\n    assert ex.value.code == 0\n    assert (\n        output.getvalue()\n        == (\n            f\"\"\"", "    def test_empty_file() -> None:\n        output = StringIO()\n        with redirect_stdout(output), pytest.raises(SystemExit) as ex:\n            similar.Run([str(INPUT / \"empty.py\"), str(INPUT / \"similar_lines_a.py\")])\n        assert ex.value.code == 0\n        assert \"TOTAL lines=106 duplicates=0 percent=0.00\" in output.getvalue()\n"], "sample_491": ["    def test_empty_data_files_multi_value_dict(self):\n        p = Person()\n        self.assertIsInstance(p.data, MultiValueDict)\n        self.assertIsInstance(p.files, MultiValueDict)\n", "    def test_error_list_with_auto_id(self):\n        class MyForm(Form):\n            name = CharField(max_length=10)\n            email = EmailField()\n\n        form = MyForm({\"name\": \"Too long\", \"email\": \"invalid\"}, auto_id=True)\n        self.assertHTMLEqual(\n            form.as_ul,\n            '<ul class=\"errorlist\">'\n            '<li>Ensure this value has at most 10 characters (it has 8).</li>'\n            '<li>Enter a valid email address.</li>'\n            '</ul>',\n        )\n", "    def test_empty_data_files_multi_value_dict(self):\n        p = Person()\n        self.assertIsInstance(p.data, MultiValueDict)\n        self.assertIsInstance(p.files, MultiValueDict)\n", "    def test_empty_data_multi_value_dict(self):\n        p = Person()\n        self.assertIsInstance(p.data, MultiValueDict)\n        self.assertIsInstance(p.files, MultiValueDict)\n", "    def test_as_p_with_errors(self):\n        class CommentForm(Form):\n            name = CharField(max_length=50, required=False)\n            email = EmailField()\n            comment = CharField()\n\n        data = {\"email\": \"invalid\"}\n        f = CommentForm(data)\n        self.assertHTMLEqual(\n            f.as_p(),\n            '<p>Name: <input type=\"text\" name=\"name\" maxlength=\"50\"></p>'\n            '<p><ul class=\"errorlist\"><li>Enter a valid email address.</li></ul>Email: '\n            '<input type=\"email\" name=\"email\" value=\"invalid\" maxlength=\"320\" required>'\n            '</p><p>Comment: <input type=\"text\" name=\"comment\" required></p>'\n        )\n"], "sample_546": ["    def test_toolmanager_trigger_tool():\n        with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n            plt.rcParams['toolbar'] = 'toolmanager'\n        fig = plt.gcf()\n        tool = fig.canvas.manager.toolmanager.get_tool('zoom')\n        assert isinstance(tool, NavigationToolbar2.Zoom)\n        fig.canvas.manager.toolmanager.trigger_tool('zoom')\n        assert fig.canvas.manager.toolmanager.active_tool.tool == 'zoom'\n        fig.canvas.manager.toolmanager.trigger_tool('pan')\n        assert fig.canvas.manager.toolmanager.active_tool.tool == 'pan'\n        fig.canvas.manager.toolmanager.trigger_tool('forward')\n        assert fig.canvas.manager.toolmanager.active_tool is None\n", "def test_figurecanvas_draw_event():\n    fig, ax = plt.subplots()\n    called = []\n        called.append(event)\n    fig.canvas.mpl_connect('draw_event', on_draw)\n    fig.canvas.draw()\n    assert called\n    assert isinstance(called[0], FigureCanvasBase.DrawEvent)\n", "compilation error", "def test_toolmanager_add_tool():\n    with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n        plt.rcParams['toolbar'] = 'toolmanager'\n    fig = plt.gcf()\n    initial_len = len(fig.canvas.manager.toolmanager.tools)\n    class MyTool(ToolBase):\n        name = 'mytool'\n            pass\n    fig.canvas.manager.toolmanager.add_tool(MyTool)\n    assert len(fig.canvas.manager.toolmanager.tools) == initial_len + 1\n    assert 'mytool' in fig.canvas.manager.toolmanager.tools\n\n", "    def test_rubberband_draw(self):\n        fig, ax = plt.subplots()\n        ax.plot([0, 1], [0, 1])\n        rubberband = RubberbandBase(fig.canvas, 'draw')\n        rubberband.press(fig.canvas, Event(x=0.1, y=0.2))\n        rubberband.release(fig.canvas, Event(x=0.5, y=0.8))\n        assert len(ax.artists) == 1\n        assert isinstance(ax.artists[0], mpl.patches.Rectangle)\n        assert ax.artists[0].get_width() == 0.4\n        assert ax.artists[0].get_height() == 0.6\n\n"], "sample_749": ["compilation error", "    def test_column_transformer_single_transformer_remainder(self, remainder, spars):\n        X = np.array([[0, 1], [2, 3]])\n        if spars:\n            X = sparse.csr_matrix(X)\n        ct = ColumnTransformer(\n            [('trans', Trans(), [0])], remainder=remainder)\n        ct.fit(X)\n        \n        if isinstance(remainder, str):\n            exp_n_outputs = 1 \n        else:\n            exp_n_outputs = 2  \n\n        assert len(ct.transformers_) == exp_n_outputs \n", "compilation error", "compilation error", "compilation error"], "sample_707": ["def test_nodeid_with_double_colons(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.parametrize(\"x\", [1, 2])\n            pass\n    \"\"\"\n    )\n    items = pytester.runpytest(\"-v\")\n    assert items.nodeid == \"test_func[1]\"\n", "def test_node_from_parent_with_fspath_and_path(pytester: Pytester) -> None:\n    item = nodes.File.from_parent(\n        pytester.session,\n        fspath=\"some_path.py\",\n        path=Path(\"another/path.py\"),\n    )\n    assert item.path == Path(\"another/path.py\")\n    assert item.fspath == \"some_path.py\"\n", "    def test__check_initialpaths_for_relpath_with_multiple_initialpaths():\n        cwd = Path.cwd()\n        alternative_path = cwd / \"other_dir\"\n\n        class FakeSession:\n            _initialpaths = frozenset({cwd, alternative_path})\n\n        session = cast(pytest.Session, FakeSession)\n\n        assert nodes._check_initialpaths_for_relpath(session, cwd) == \"\"\n        assert nodes._check_initialpaths_for_relpath(session, cwd / \"subfile\") == \"subfile\"\n\n        assert nodes._check_initialpaths_for_relpath(\n            session, alternative_path / \"subfile\"\n        ) == \"other_dir/subfile\"\n\n", "def test_nodeid_from_parent_with_different_types_of_parents(pytester: Pytester) -> None:\n    item = pytester.getitem(\"def test_func(): pass\")\n    collector = item.parent\n\n    # Test nodeid creation when parent is a File node\n    file_node = nodes.File.from_parent(collector.parent, fspath=collector.fspath)\n    assert file_node.nodeid == collector.nodeid\n\n    # Test nodeid creation when parent is a Session node\n    session_node = nodes.Session()\n    session_node.path = collector.session.config.rootpath\n    session_node.fspath = str(session_node.path)\n    with pytest.raises(TypeError):\n        nodes.Node.from_parent(session_node, name=\"test\", parent=collector)\n\n", "def test_node_repr_failure_tbstyle(pytester, monkeypatch) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.parametrize(\"x\", [1, 2, 3])\n            raise ValueError(\"something went wrong\")\n    \"\"\"\n    )\n    monkeypatch.setenv(\"PYTEST_TBSTYLE\", \"short\")\n    result = pytester.runpytest()\n    assert result.ret == 1  \n    result.stdout.fnmatch_lines(\n        [\n            \"*test_something[1*]*\",\n            \"*ValueError: something went wrong*\",\n        ]\n    )\n"], "sample_935": ["    def test_role_classes_consistency():\n        role_classes = {\n            'class': RoleClasses('class', 'p', ['code']),\n            'struct': RoleClasses('struct', 'p', ['code']),\n            'union': RoleClasses('union', 'p', ['code']),\n            'func': RoleClasses('func', 'p', ['code']),\n            'member': RoleClasses('member', 'p', ['code']),\n            'var': RoleClasses('var', 'p', ['code']),\n            'type': RoleClasses('type', 'p', ['code']),\n            'concept': RoleClasses('concept', 'p', ['code']),\n            'enum': RoleClasses('enum', 'p', ['code']),\n            'enumerator': RoleClasses('enumerator', 'p', ['code']),\n        }\n        for role_name, role in role_classes.items():\n            assert role.classes == {'xref', role_name}\n            for content_role, content_classes in role.content_classes.items():\n                assert content_classes == {'xref', 'docutils', 'literal',\n                                          content_role}\n", "    def check_consistency(role, root, contents):\n        roleClasses = RoleClasses(role, root, contents)\n\n        # Check that all content tags share the same set of classes\n        for tag1, classes1 in roleClasses.content_classes.items():\n            for tag2, classes2 in roleClasses.content_classes.items():\n                if tag1 != tag2:\n                    assert classes1 == classes2, (\n                        f\"Inconsistent classes for role '{role}' between tags '{tag1}' and '{tag2}': \"\n                        f\"{classes1} != {classes2}\"\n                        )\n        # Check that the root tag includes the classes of all content tags\n        for tag, classes in roleClasses.content_classes.items():\n            assert roleClasses.classes.issuperset(classes), (\n                f\"Missing classes for role '{role}' in root tag '{root}' compared to content tag '{tag}': \"\n                f\"{roleClasses.classes} !><= {classes}\"\n                )\n", "compilation error", "    def test_role_class_consistency(self):\n        roles = [\n            RoleClasses('func', 'a', ['p', 'span']),\n            RoleClasses('member', 'a', ['p', 'span']),\n            RoleClasses('class', 'a', ['p', 'span']),\n            RoleClasses('type', 'a', ['p', 'span']),\n        ]\n        for i in range(len(roles)):\n            for j in range(i + 1, len(roles)):\n                assert roles[i].classes == roles[j].classes, \\\n                    f\"Classes for roles '{roles[i].name}' and '{roles[j].name}' are not consistent: '{roles[i].classes}' != '{roles[j].classes}'\"\n                for tag in roles[i].content_classes:\n                    assert roles[i].content_classes[tag] == roles[j].content_classes.get(tag, set()), \\\n                        f\"Content classes for tag '{tag}' in roles '{roles[i].name}' and '{roles[j].name}' are not consistent: '{roles[i].content_classes[tag]}' != '{roles[j].content_classes.get(tag, set())}'\"\n", "    def __eq__(self, other):\n        if not isinstance(other, RoleClasses):\n            return False\n        return (self.name == other.name and\n                self.classes == other.classes and \n                self.content_classes == other.content_classes)\n"], "sample_612": ["    def test_ds_resample_apply_func_kwargs(self):\n            return arg1.mean(\"time\") + arg2 + arg3\n\n        times = pd.date_range(\"2000\", freq=\"D\", periods=3)\n        ds = xr.Dataset({\"foo\": (\"time\", [1.0, 1.0, 1.0]), \"time\": times})\n        expected = xr.Dataset({\"foo\": (\"time\", [3.0, 3.0, 3.0]), \"time\": times})\n        actual = ds.resample(time=\"D\").map(func, arg2=1.0, arg3=1.0)\n        assert_identical(expected, actual)\n", "    def test_resample_nearest_upsampling(self):\n        times = pd.date_range(\"2000-01-01\", freq=\"6H\", periods=5)\n        array = DataArray(np.arange(5), [(\"time\", times)])\n        actual = array.resample(time=\"1H\").nearest()\n        expected_times = pd.date_range(\"2000-01-01\", freq=\"1H\", periods=24)\n        expected = DataArray(np.tile(np.arange(5), 4), [(\"time\", expected_times)])\n        assert_identical(expected, actual)\n", "    def test_ds_resample_apply_func_kwargs(self):\n            return arg1.sum('time') + arg2 + arg3\n\n        times = pd.date_range(\"2000\", freq=\"D\", periods=3)\n        ds = xr.Dataset({\"foo\": (\"time\", [1.0, 1.0, 1.0]), \"time\": times})\n        expected = xr.Dataset({\"foo\": (\"time\", [3.0, 3.0, 3.0]), \"time\": times})\n        actual = ds.resample(time=\"D\").map(func, arg2=1.0, arg3=1.0)\n        assert_identical(expected, actual) \n", "    def test_resample_ds_apply_func_kwargs(self):\n            return arg1.mean(\"time\") + arg2 + kwargs[\"arg3\"]\n\n        times = pd.date_range(\"2000\", freq=\"D\", periods=3)\n        ds = xr.Dataset({\"foo\": (\"time\", [1.0, 1.0, 1.0]), \"time\": times})\n        expected = xr.Dataset({\"foo\": (\"time\", [3.0, 3.0, 3.0]), \"time\": times})\n        actual = ds.resample(time=\"D\").map(func, args=(1.0,), arg3=1.0)\n        assert_identical(expected, actual)\n", "    def test_resample_da_with_empty_result(self):\n        times = pd.date_range(\"2000-01-01\", freq=\"D\", periods=5)\n        da = xr.DataArray([1, 2, 3, 4, 5], dims=[\"time\"], coords={\"time\": times})\n        actual = da.resample(time=\"2D\").sum()\n        expected = xr.DataArray([3, 9], dims=[\"time\"], coords={\"time\": pd.DatetimeIndex([\"2000-01-02\", \"2000-01-04\"])})\n        assert_identical(actual, expected)\n"], "sample_621": ["    def test_to_unstructured_dict(self, indexes) -> None:\n        actual = indexes.to_unstructured_dict()\n        assert actual == {\n            \"x\": indexes.variables[\"x\"].values,\n            \"y\": indexes.variables[\"y\"].values,\n            \"z\": indexes.variables[\"z\"].values,\n            \"one\": indexes.variables[\"one\"].values,\n            \"two\": indexes.variables[\"two\"].values,\n        }\n", "    def test_isel(self, indexes):\n        expected = indexes[\"x\"].isel({\"x\": [0, 2]})\n        actual = indexes.isel({\"x\": [0, 2]})\n        assert actual[\"x\"].equals(expected)\n", "    def test_from_list_of_arrays(self) -> None:\n        x_data = np.arange(3)\n        y_data = np.array([\"a\", \"b\", \"c\"])\n        z_data = [\n            np.array([1, 2]),\n            np.array([3, 4]),\n            np.array([5, 6]),\n        ]\n        indexes = Indexes.from_list_of_arrays([\"x\", \"y\", \"z\"], [x_data, y_data, z_data])\n\n        assert len(indexes) == 3\n        assert indexes[\"x\"].equals(PandasIndex(x_data, \"x\"))\n        assert indexes[\"y\"].equals(PandasIndex(y_data, \"y\"))\n        assert indexes[\"z\"].equals(PandasMultiIndex.from_arrays(z_data, \"z\"))\n", "    def test_to_pandas_indexes_empty(self, indexes):\n        empty_indexes = Indexes({}, {})\n        pd_indexes = empty_indexes.to_pandas_indexes()\n        assert isinstance(pd_indexes, Indexes)\n        assert len(pd_indexes) == 0\n        assert pd_indexes.variables == {}\n", "    def test_to_pandas_indexes_multi_index(self, indexes):\n        pd_multi_index = indexes.to_pandas_indexes()['z']\n        assert isinstance(pd_multi_index, pd.MultiIndex)\n"], "sample_398": ["    def test_user_change_password_with_staff_permission(self):\n        user = User.objects.create_user(\n            username=\"staff_user\",\n            email=\"staff_user@example.com\",\n            password=\"password\",\n        )\n        user.save()\n        user.is_staff = False\n        user.save()\n        self.u1.user_permissions.add(get_perm(User, \"change_user\"))\n        self.client.login(username=\"testclient\", password=\"password\")\n        response = self.client.get(\n            reverse(\"auth_test_admin:auth_user_change\", args=(user.pk,))\n        )\n        self.assertEqual(response.status_code, 200)\n        response = self.client.post(\n            reverse(\"auth_test_admin:auth_user_password_change\", args=(user.pk,)),\n            {\"password1\": \"newpassword\", \"password2\": \"newpassword\"},\n        )\n        self.assertRedirects(\n            response, reverse(\"auth_test_admin:auth_user_change\", args=(user.pk,))\n        )\n        user = User.objects.get(pk=user.pk)\n        self.assertTrue(user.check_password(\"newpassword\"))\n", "    def test_user_changelist_filter_by_date_joined(self):\n        # Create a new user\n        user = User.objects.create_user(\n            username=\"newuser\",\n            password=\"password\",\n            email=\"newuser@example.com\",\n        )\n        # Get the current date\n        today = date.today()\n        # Filter users by date joined today\n        response = self.client.get(\n            reverse(\"auth_test_admin:auth_user_changelist\") + \"?date_joined__day=\" + str(today.day) + \"&date_joined__month=\" + str(today.month) + \"&date_joined__year=\" + str(today.year)\n        )\n        # Check that only the new user is in the changelist\n        self.assertContains(response, \"newuser\")\n        self.assertNotContains(response, \"testclient\")\n\n", "    def test_changelist_filter_by_is_active(self):\n        # Filter by active users.\n        response = self.client.get(\n            reverse(\"auth_test_admin:auth_user_changelist\") + \"?is_active=1\",\n        )\n        self.assertEqual(response.status_code, 200)\n        user_list = User.objects.filter(is_active=True)\n        self.assertQuerysetEqual(response.context[\"object_list\"], user_list)\n\n        # Filter by inactive users.\n        response = self.client.get(\n            reverse(\"auth_test_admin:auth_user_changelist\") + \"?is_active=0\",\n        )\n        self.assertEqual(response.status_code, 200)\n        user_list = User.objects.filter(is_active=False)\n        self.assertQuerysetEqual(response.context[\"object_list\"], user_list)\n", "    def test_view_user_password_is_readonly_superuser(self):\n        u = User.objects.get(username=\"testclient\")\n        u.is_superuser = True\n        u.save()\n        original_password = u.password\n        u.user_permissions.add(get_perm(User, \"view_user\"))\n        response = self.client.get(\n            reverse(\"auth_test_admin:auth_user_change\", args=(u.pk,)),\n        )\n        algo, salt, hash_string = u.password.split(\"$\")\n        self.assertContains(response, '<div class=\"readonly\">testclient</div>')\n        # Superusers can see the full hash.\n        self.assertContains(\n            response,\n            \"<strong>algorithm</strong>: %s\\n\\n\"\n            \"<strong>salt</strong>: %s\\n\\n\"\n            \"<strong>hash</strong>: %s\\n\\n\"\n            % (algo, salt, hash_string),\n            html=True,\n        )\n        # Value in POST data is ignored.\n        data = self.get_user_data(u)\n        data[\"password\"] = \"shouldnotchange\"\n        change_url = reverse(\"auth_test_admin:auth_user_change\", args=(u.pk,))\n        response = self.client.post(change_url, data)\n        self.assertEqual(response.status_code, 403)\n        u.refresh_from_db()\n        self.assertEqual(u.password, original_password)\n\n", "    def test_user_change_list_filter_by_is_active(self):\n        \n        # Create an inactive user\n        inactive_user = User.objects.create_user(\n            username=\"inactive_user\", password=\"password\", is_active=False\n        )\n        \n        # Check that the inactive user is not visible in the changelist\n        response = self.client.get(reverse(\"auth_test_admin:auth_user_changelist\"))\n        self.assertNotContains(response, inactive_user.username)\n\n        # Filter by active users\n        response = self.client.get(\n            reverse(\"auth_test_admin:auth_user_changelist\") + \"?is_active=True\"\n        )\n        self.assertContains(response, self.admin.username)\n        self.assertNotContains(response, inactive_user.username)\n\n        # Filter by inactive users\n        response = self.client.get(\n            reverse(\"auth_test_admin:auth_user_changelist\") + \"?is_active=False\"\n        )\n        self.assertNotContains(response, self.admin.username)\n        self.assertContains(response, inactive_user.username)\n"], "sample_1171": ["compilation error", "compilation error", "compilation error", "compilation error", "def test_ComplexRegion_empty_intersection():\n    c1 = ComplexRegion(Interval(0, 1)*Interval(0, pi/2), polar=True)\n    c2 = ComplexRegion(Interval(1, 2)*Interval(pi/2, pi), polar=True)\n    assert c1.intersection(c2) is S.EmptySet\n"], "sample_1023": ["def test_totient():\n    assert totient(1) == 1\n    assert totient(2) == 1\n    assert totient(3) == 2\n    assert totient(4) == 2\n    assert totient(5) == 4\n    assert totient(6) == 2\n    assert totient(10) == 4\n    assert totient(12) == 4\n    assert totient(20) == 8\n    assert totient(100) == 40\n    assert totient(1000) == 400\n    assert totient(10000) == 4000\n    assert totient(10**8) == 4 * 10**7\n    assert totient(2**8) == 2**7\n    assert totient(3**4) == 3**3 * 2\n    assert totient(5**3) == 5**2 * 4\n    assert totient(7**2) == 7 * 6\n    assert totient(2**3 * 3**2) == 2**2 * 3 * 2\n    assert totient(2**2 * 5 * 7) == 2 * 5 * 6\n\n", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_882": ["    def test_mlp_regressor_predict_returns_vector(self):\n        mlp = MLPRegressor()\n        mlp.fit(X_digits[:100], y_digits[:100])\n        y_pred = mlp.predict(X_digits[100:110])\n        assert isinstance(y_pred, np.ndarray)\n        assert y_pred.ndim == 1\n", "    def test_mlp_validation_fraction_zero(MLPEstimator):\n        \"\"\"Check that validation fraction of zero does not raise an error.\"\"\"\n        mlp = MLPEstimator(validation_fraction=0, early_stopping=True)\n        mlp.fit(X_iris, y_iris)\n", "def test_mlp_coefs_readonly(MLPEstimator):\n    # Check if the coefs_ attribute is readonly after fitting\n    mlp = MLPEstimator(random_state=0)\n    mlp.fit(X_iris, y_iris)\n    with pytest.raises(AttributeError):\n        mlp.coefs_[0][0] = 10\n", "    def test_mlp_solver_convergence_warning(self):\n        mlp = MLPClassifier(solver=\"sgd\", max_iter=5)\n\n        with warnings.catch_warnings(record=True) as w:\n            mlp.fit(X_digits[:100], y_digits[:100])\n            assert len(w) == 1\n            assert \"ConvergenceWarning\" in str(w[0].category)\n", "    def test_mlp_activation_functions(activation):\n        mlp = MLPClassifier(hidden_layer_sizes=(5,), activation=activation, solver='sgd')\n        mlp.fit(X_digits[:100], y_digits[:100])\n        predictions = mlp.predict(X_digits[100:])\n\n        assert len(predictions) == len(y_digits[100:])\n                                \n            \n\n"], "sample_580": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_351": ["    def test_to_field_name_not_available(self):\n        class AuthorForm(forms.ModelForm):\n            class Meta:\n                model = Author\n                fields = ['name']\n\n        # This field is on the Author model, not a related model.\n        form = AuthorForm({}, initial={'name': 'Test Author'})\n        field = form['name']\n        try:\n            field.to_field_name\n        except AttributeError:\n            assert True\n\n\n\n", "    def test_modelchoicefield_to_field_name_with_custom_model_pk(self):\n        class CustomModel(models.Model):\n            custom_id = models.CharField(max_length=10, primary_key=True)\n            name = models.CharField(max_length=100)\n\n        form = forms.ModelForm(\n            model=CustomModel,\n            fields=['name'],\n        )\n        field = form.fields['name']\n        self.assertEqual(field.to_field_name, 'custom_id')\n", "    def test_modelmultiplechoicefield_initial_model_instances(self):\n        class ArticleForm(forms.ModelForm):\n            categories = forms.ModelMultipleChoiceField(Category.objects.all(), required=False)\n\n            class Meta:\n                model = Article\n                fields = ['categories']\n\n        category1 = Category.objects.create(name='cat1')\n        category2 = Category.objects.create(name='cat2')\n        article = Article.objects.create(\n            pub_date=datetime.date(1988, 1, 4),\n            writer=Writer.objects.create(name='Test writer'),\n        )\n        article.categories.set([category1.pk, category2.pk])\n\n        form = ArticleForm(instance=article)\n        self.assertEqual([x.pk for x in form.cleaned_data['categories']], [category1.pk, category2.pk])\n\n", "    def test_modelmultiplechoicefield_initial_queryset_none(self):\n        class ModelMultipleChoiceForm(forms.Form):\n            categories = forms.ModelMultipleChoiceField(queryset=None, initial=[self.c1.pk, self.c2.pk])\n\n                super().__init__(*args, **kwargs)\n                self.fields['categories'].queryset = Category.objects.filter(pk__in=[self.c1.pk, self.c2.pk])\n\n        form = ModelMultipleChoiceForm()\n        self.assertEqual(form.initial['categories'], [self.c1.pk, self.c2.pk])\n        self.assertEqual([x.pk for x in form.cleaned_data['categories']], [self.c1.pk, self.c2.pk])\n\n", "    def test_custom_label_from_instance(self):\n        class CustomModelChoiceField(forms.ModelChoiceField):\n                return f\"Category {obj.slug}\"\n\n        field = CustomModelChoiceField(Category.objects.all())\n        self.assertEqual(\n            list(field.choices),\n            [\n                ('', '---------'),\n                (self.c1.pk, 'Category entertainment'),\n                (self.c2.pk, 'Category test'),\n                (self.c3.pk, 'Category third-test'),\n            ],\n        )\n"], "sample_1052": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1079": ["    def test_Point2D_issue_13574():\n        p1 = Point2D(1, 2)\n        p2 = Point2D(p1)\n        assert p1 == p2\n", "compilation error", "    def test_Point3D_direction_cosine_and_ratio():\n        p1 = Point3D(1, 2, 3)\n        p2 = Point3D(4, 5, 6)\n\n        # Test direction cosine\n        assert p1.direction_cosine(p2) == [\n            sqrt(3) / 3, sqrt(3) / 3, sqrt(3) / 3]\n\n        # Test direction ratio\n        assert p1.direction_ratio(p2) == [3, 3, 3]\n", "    def test_Point3D_are_coplanar():\n        p1 = Point3D(1, 2, 3)\n        p2 = Point3D(4, 5, 6)\n        p3 = Point3D(7, 8, 9)\n        p4 = Point3D(10, 11, 12)\n        assert Point3D.are_coplanar(p1, p2, p3, p4) is False\n        \n", "    def test_point2d_direction_ratio():\n        p1 = Point2D(1, 1)\n        p2 = Point2D(4, 2)\n        assert p1.direction_ratio(p2) == [3, 1]\n        \n"], "sample_282": ["    def test_partially_required_field_empty(self):\n        form = PartiallyRequiredForm({'f': ''})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['f'], ['This field is required.'])\n", "    def test_partially_required_field_errors(self):\n        form = PartiallyRequiredForm({'f_0': 'abc', 'f_1': ''})\n        self.assertEqual(form.errors, {'f': ['Only a required field']} )\n", "    def test_partially_required_form(self):\n        form = PartiallyRequiredForm({\n            'f_0': 'x',\n            'f_1': ''\n        })\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['f'], 'x')\n\n", "    def test_partially_required_field(self):\n        form = PartiallyRequiredForm({\n            'f_0': 'value',\n            'f_1': '',\n        })\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['f'], 'value,')\n", "    def test_partially_required_field(self):\n        form = PartiallyRequiredForm({'f_0': 'some text', 'f_1': ''})\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['f'], 'some text,')\n"], "sample_500": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_859": ["    def test_enetcv_precompute_warning(monkeypatch):\n        X, y, _, _ = build_dataset()\n        # Mock the ElasticNetWarning to check if it is raised\n        warn_calls = 0\n        \n            nonlocal warn_calls\n            warn_calls += 1\n\n        monkeypatch.setattr(warnings, 'warn', mock_warn)\n        \n        clf = ElasticNetCV(precompute='auto')\n        clf.fit(X, y)\n        assert warn_calls == 0\n", "compilation error", "    def test_enet_path_sparse_input_convergence(monkeypatch):\n        X, y, _, _ = build_dataset(n_samples=100, n_features=100)\n        X_sparse = sparse.csr_matrix(X)\n        n_alphas = 10\n        alphas = np.linspace(1, 0, n_alphas).astype(np.float64)\n        coefs = []\n        with pytest.warns(ConvergenceWarning):\n            for alpha in alphas:\n                coefs.append(ElasticNet(alpha=alpha, max_iter=1, tol=0).\n                              fit(X_sparse, y).coef_)\n", "    def test_enet_with_sparse_matrix_and_zero_alpha(self):\n        X, y, _, _ = build_dataset(n_samples=20, n_features=10)\n        X_sparse = sparse.csr_matrix(X)\n        enet = ElasticNet(alpha=0, random_state=42)\n        enet.fit(X_sparse, y)\n        assert_array_equal(enet.coef_, np.zeros(X.shape[1]))\n", "    def test_enetCV_does_not_set_precompute(monkeypatch, precompute,\n                                          inner_precompute):\n        X, y, _, _ = build_dataset()\n        calls = 0\n\n        class ElasticNetMock(ElasticNet):\n                super().fit(X, y)\n                nonlocal calls\n                calls += 1\n                assert self.precompute == inner_precompute\n\n        monkeypatch.setattr(ElasticNetCV, \"ElasticNet\", ElasticNetMock)\n\n        clf = ElasticNetCV(precompute=precompute)\n        clf.fit(X, y)\n        assert calls == 1\n"], "sample_210": ["    def test_redirect_view_with_custom_template_name(self):\n        response = RedirectView.as_view(url='/bar/', template_name='generic_views/redirect.html')(self.rf.get('/foo/'))\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response.url, '/bar/')\n", "    def test_redirect_view_with_custom_content_type(self):\n        response = RedirectView.as_view(url='/bar/', content_type='text/plain')(self.rf.get('/foo/'))\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response['Content-Type'], 'text/plain')\n", "    def test_get_template_names_empty_list(self):\n        \"\"\"\n        Test that get_template_names returns an empty list if there is no\n        template_name or get_template_names() implementation.\n\n        \"\"\"\n        view = views.TemplateResponseWithoutTemplate()\n        self.assertEqual(view.get_template_names(), [])\n", "    def test_redirect_view_with_template_name(self):\n        \"\"\"\n        Test that RedirectView accepts a template_name argument.\n        \"\"\"\n        response = RedirectView.as_view(\n            url='/bar/', template_name='generic_views/about.html'\n        )(self.rf.get('/foo/'))\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response.url, '/bar/')\n\n        # Make sure the template_name is properly stored in the view instance\n        self.assertEqual(response._wrapped_view.template_name, 'generic_views/about.html')\n", "    def test_redirect_view_passes_kwargs_to_context(self):\n        response = self.client.get('/redirect/kwarg/some_value/')\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response.context['kwarg'], 'some_value')\n"], "sample_963": ["def test_stringify_type_hints_NoneType():\n    assert stringify(NoneType) == \"NoneType\"\n    assert stringify(Optional[NoneType]) == \"Optional[NoneType]\"\n", "    def test_stringify_type_hints_NewType():\n        MyInt = NewType('MyInt', int)\n        assert stringify(MyInt) == 'tests.test_util_typing.MyInt'\n", "def test_restify_type_hints_NewType():\n    NT = NewType('NT', int)\n    assert restify(NT) == ':py:class:`tests.test_util_typing.NT`'\n    assert restify(NT[1]) == ':py:class:`tests.test_util_typing.NT`\\\\ [:py:class:`int`]'\n", "    def test_stringify_type_hints_paramspec():\n        from typing import ParamSpec  # type: ignore\n        P = ParamSpec('P')\n        assert stringify(Callable[P, int]) == \"Callable[P, int]\"\n\n", "def test_restify_type_hints_NewType():\n    MyInt = NewType('MyInt', int)\n    assert restify(MyInt) == \":py:class:`tests.test_util_typing.MyInt`\"\n\n\n"], "sample_89": ["    def test_watch_files_with_recursive_glob_nested_directories(self):\n        inner_dir = self.ensure_file(self.tempdir / 'dir1' / 'dir2')\n        self.ensure_file(inner_dir / 'test.py')\n        self.reloader.watch_dir(self.tempdir, '**/*.py')\n        watched_files = list(self.reloader.watched_files())\n        self.assertIn(self.existing_file, watched_files)\n        self.assertIn(inner_dir / 'test.py', watched_files)\n", "    def test_should_stop_when_reloaded(self):\n        self.reloader.should_stop = True\n        with self.tick_twice():\n            self.increment_mtime(self.existing_file)\n        self.assertFalse(self.notify_mock.called)\n", "    def test_should_stop_after_multiple_errors(self):\n        self.reloader.should_stop = False\n        self.reloader._error_files.append(Exception)\n        self.reloader._error_files.append(Exception)\n        self.reloader._error_files.append(Exception)\n        with self.assertRaises(SystemExit):\n            self.reloader.run_loop()\n        self.assertTrue(self.reloader.should_stop)\n\n", "    def test_should_stop_returns_true_if_error_is_not_none(self):\n        self.reloader._exception = Exception()\n        self.assertTrue(self.reloader.should_stop())\n", "    def test_watches_with_relative_files(self, mocked_modules, notify_mock):\n        relative_file = Path('./test.py')\n        self.ensure_file(relative_file)\n        self.reloader.watch_file(relative_file)\n        with self.tick_twice():\n            self.increment_mtime(relative_file)\n        self.assertEqual(notify_mock.call_count, 1)\n        self.assertCountEqual(notify_mock.call_args[0], [relative_file])\n"], "sample_505": ["compilation error", "def test_date_formatter():\n    date_formatter = mdates.DateFormatter('%Y-%m-%d')\n    dfmt = date_formatter\n    dates = [datetime.datetime(2023, 10, 26),\n             datetime.datetime(2023, 10, 27)]\n    formatted_dates = [dfmt(date) for date in dates]\n    assert formatted_dates == ['2023-10-26', '2023-10-27']\n", "def test_date2num_leapyear_midnight():\n    dt = datetime.datetime(2024, 2, 29, 0, 0)\n    expected = 738956.0  \n    assert mdates.date2num(dt) == expected\n", "compilation error", "    def test_date2num_tz_aware_datetime_range(self):\n        dates = pd.date_range('2023-01-01', periods=5, tz='US/Eastern')\n        num_dates = mdates.date2num(dates)\n        assert np.all(np.isfinite(num_dates))\n"], "sample_417": ["    def test_decimal_formatting(self):\n        with localcontext() as ctx:\n            ctx.prec = 28\n            self.assertEqual(floatformat(Decimal(\"123456789012345678901234567890\"), 2), \"123456789012345678901234567890.00\")\n", "    def test_localcontext(self):\n        with localcontext() as ctx:\n            ctx.prec = 28\n            self.assertEqual(floatformat(Decimal(\"123456789012345678901234567890\"), 20), \"12345678901234567890.1234567890\")\n", "    def test_scientific_notation(self):\n        self.assertEqual(floatformat(12345678901234567890.0, 'g'), '1.2345678901234567e+19')\n        self.assertEqual(floatformat(12345678901234567890.0, 'g2'), '1.23e+19')\n        self.assertEqual(floatformat(0.00000000000000012345, 'g'), '1.2345e-16')\n        self.assertEqual(floatformat(0.00000000000000012345, 'g2'), '1.23e-16')\n", "    def test_decimal_precision(self):\n        with localcontext() as ctx:\n            ctx.prec = 28\n            self.assertEqual(floatformat(Decimal(\"12345678901234567890123456789\"), 20), \"12345678901234567890.1234567890\")\n            self.assertEqual(floatformat(Decimal(\"12345678901234567890123456789\"), -2), \"12345678901234567890123456789\")\n", "    def test_localcontext(self):\n        with localcontext() as ctx:\n            ctx.prec = 20\n            self.assertEqual(floatformat(Decimal(\"123456.123456789012345678901\")), \n                             \"123456.123456789012345678901\")\n"], "sample_696": ["def test_warning_captured_hook_is_deprecated(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            pass\n\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: The pytest_warning_captured is deprecated*\",\n        ]\n    )\n\n", "def test_argument_type_str_choice_deprecated(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n            request.addoption(\n                \"--foo\", type=\"str\", choices=[\"option1\", \"option2\"], help=\"foo help\"\n            )\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([\"*choices this is optional and can be omitted*\"])\n", "def test_argument_percent_default_is_deprecated(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        pytest.addoption(\n            \"--myopt\",\n            action=\"store\",\n            default=\"default value\",\n            help=\"%default\",\n        )\n\n            assert pytestconfig.getoption(\"--myopt\") == \"default value\"\n\n        \"\"\"\n\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\"*PytestDeprecationWarning: pytest now uses argparse. ']'*\"]\n    )\n", "def test_warning_captured_hook_is_deprecated(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        pytest.hookimpl(tryfirst=True, hookwrapper=True)\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: The pytest_warning_captured*\",\n        ]\n    )\n\n", "def test_argument_type_str_choice_is_deprecated(pytester: Pytester) -> None:\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        addopts = -p no:warnings\n    \"\"\"\n    )\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n\n            parser.addoption('--myoption', type='str', choices=['a', 'b'],\n                             help='My option')\n        \"\"\"\n    )\n    result = pytester.runpytest(\"-h\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*--myoption*type='str', choices=['a', 'b']*\",\n            \"*The `type` argument to addoption() is the string*\",\n        ]\n    )\n\n"], "sample_122": ["    def test_cache_key_with_etag(self):\n        request = self.factory.get(self.path)\n        template = engines['django'].from_string(\"This is a test\")\n        response = TemplateResponse(HttpRequest(), template)\n        response['ETag'] = 'some-etag'\n        learn_cache_key(request, response)\n        self.assertEqual(\n            get_cache_key(request),\n            'views.decorators.cache.cache_page.settingsprefix.GET.'\n            '58a0a05c8a5620f813686ff969c26853.some-etag'\n        ) \n", "    def test_cache_middleware_with_cache_control_no_store(self):\n        view = cache_page(3)(hello_world_view)\n        request = self.factory.get('/view/')\n        response = view(request, '1')\n        response['Cache-Control'] = 'no-store'\n\n        cached_response = self.other_cache.get(get_cache_key(request))\n        self.assertIsNone(cached_response)\n", "    def test_cache_middleware_empty_response(self):\n            translation.activate(lang)\n            response = HttpResponse()\n            response.content = msg\n            return UpdateCacheMiddleware().process_response(request, response)\n\n        request = self.factory.get(self.path)\n        request._cache_update_cache = True\n        response = HttpResponse()\n        response.status_code = 204\n        set_cache(request, 'en', '')\n\n        get_cache_data = FetchFromCacheMiddleware().process_request(request)\n        self.assertIsNone(get_cache_data)\n", "    def test_never_cache_for_special_views(self):\n        \"\"\"\n        Special views like login, logout should never be cached.\n        \n        Note: There's actually no foolproof way to determine which views should\n        be excluded from caching. This test ensures that a special view\n        implementation is not cached.\n        \"\"\"\n        cache_middleware = CacheMiddleware()\n        request = self.factory.get('/view/')\n\n        class SpecialView(APIView):\n                return HttpResponse('Special View')\n\n        view_instance = SpecialView()\n\n        response = cache_middleware.process_response(request, view_instance.get(request))\n        self.assertIsNone(cache_middleware.process_request(request))\n\n        response = cache_middleware.process_response(request, view_instance.get(request))\n        self.assertIsNone(cache_middleware.process_request(request))\n", "    def test_invalid_backend(self):\n        \"\"\"Test that an invalid backend raises an ImproperlyConfigured error.\"\"\"\n        with self.assertRaises(ImproperlyConfigured):\n            caches['invalid_backend'] = {'BACKEND': 'nonexistent.backend'}\n"], "sample_890": ["def test_n_jobs(direction):\n    X, y = make_regression(n_features=10, random_state=0)\n    sfs = SequentialFeatureSelector(\n        LinearRegression(),\n        n_features_to_select=\"auto\",\n        direction=direction,\n        n_jobs=2,\n        cv=LeaveOneGroupOut(),\n    )\n    groups = np.arange(X.shape[0])\n    with pytest.warns(UserWarning, match=\"n_jobs will not be used\"):\n        sfs.fit(X, y, groups=groups)\n", "def test_groups(direction):\n    # Test that SequentialFeatureSelector respects group information when\n    # using LeaveOneGroupOut\n\n    rng = np.random.RandomState(0)\n    n_samples, n_features = 40, 4\n    X, y = make_regression(n_samples, n_features, random_state=0)\n    groups = rng.randint(0, 5, size=n_samples)\n\n    sfs = SequentialFeatureSelector(\n        LinearRegression(),\n        n_features_to_select=2,\n        direction=direction,\n        cv=LeaveOneGroupOut()\n    )\n    sfs.fit(X, y, groups=groups)\n    sfs.transform(X)\n", "def test_groups_parameter():\n    n_samples = 20\n    n_features = 5\n    n_groups = 4\n    groups = np.repeat(np.arange(n_groups), n_samples // n_groups)\n\n    X, y = make_classification(n_samples=n_samples, n_features=n_features, random_state=0)\n    \n    sfs = SequentialFeatureSelector(\n        KNeighborsClassifier(), n_features_to_select=3, cv=LeaveOneGroupOut()\n    )\n    sfs.fit(X, y, groups=groups)\n    \n", "    def test_groups_support():\n        # Test SFS with groups\n\n        rng = np.random.RandomState(0)\n        n_samples = 20\n        n_features = 10\n        n_groups = 3\n\n        X, y = make_regression(n_samples=n_samples, n_features=n_features, random_state=0)\n        groups = rng.randint(0, n_groups, n_samples)\n\n        sfs = SequentialFeatureSelector(\n            LinearRegression(), n_features_to_select=\"auto\", cv=LeaveOneGroupOut()\n        )\n        sfs.fit(X, y, groups=groups)\n\n        assert sfs.n_features_to_select_ <= n_features\n\n", "def test_leave_one_group_out_cv(n_features_to_select, direction):\n    \"\"\"\n    Check that SequentialFeatureSelector works with LeaveOneGroupOut cv.\n    \"\"\"\n    n_samples, n_features = 40, 5\n    X, y = make_classification(n_samples=n_samples, n_features=n_features, random_state=0)\n    groups = np.arange(n_samples) // 10\n    sfs = SequentialFeatureSelector(\n        KNeighborsClassifier(),\n        n_features_to_select=n_features_to_select,\n        direction=direction,\n        cv=LeaveOneGroupOut(),\n    )\n    sfs.fit(X, y, groups=groups)\n    assert sfs.transform(X).shape[1] <= n_features\n\n\n\n"], "sample_1184": ["    def test_conjugate_gauss_beams_f():\n        l, w_i, w_o = symbols('l w_i w_o')\n        s_in, s_out, f = conjugate_gauss_beams(1e-6, 1e-3, 1e-4, f=0.1)\n        assert streq(s_in, '0.1*(1 - sqrt(1000000/100000000 - 1000000*pi**2/(0.01*l**2)))')\n        assert streq(s_out, '0.1*10000/1000000*(1000000/100000000 - sqrt(1000000/100000000 - 1000000*pi**2/(0.01*l**2))))')\n        assert streq(f, '0.1')\n\n", "compilation error", "compilation error", "    def test_conjugate_gauss_beams(self):\n        l, w_i, w_o, f = symbols('l w_i w_o f')\n        s_in, s_out, f_ = conjugate_gauss_beams(l, w_i, w_o, f=f)\n        self.assertTrue(streq(s_in, f*(1 - sqrt(w_i**2/w_o**2 - pi**2*w_i**4/(f**2*l**2)))))\n        self.assertTrue(streq(factor(s_out), factor(f*w_o**2*(w_i**2/w_o**2 - sqrt(w_i**2/w_o**2 - pi**2*w_i**4/(f**2*l**2)))/w_i**2)))\n        self.assertTrue(streq(f_, f))\n", "    def test_conjugate_gauss_beams_f(self):\n        l = symbols('l', real=True, positive=True)\n        wi = symbols('wi', real=True, positive=True)\n        wo = symbols('wo', real=True, positive=True)\n        f = symbols('f', real=True, positive=True)\n        s_in, s_out, f_test = conjugate_gauss_beams(l, wi, wo, f=f)\n        assert streq(s_in, f*(1 - sqrt(wi**2/wo**2 - pi**2*wi**4/(f**2*l**2))))\n        assert streq(s_out, f*wo**2*(wi**2/wo**2 - sqrt(wi**2/wo**2 - pi**2*wi**4/(f**2*l**2)))/wi**2)\n        assert streq(f_test, f)\n"], "sample_488": ["    def test_namespace_package_app_config(self):\n        \"\"\"\n        Test that AppConfig subclass works correctly within a namespace package.\n        \"\"\"\n        with extend_sys_path(self.base_location):\n            with self.settings(INSTALLED_APPS=[\"nsapp\"]):\n                app_config = apps.get_app_config(\"nsapp\")\n                self.assertEqual(type(app_config).__name__, \"MyAppConfig\")\n                self.assertEqual(app_config.name, \"nsapp\")\n", "    def test_nonexistent_namespace_package(self):\n        \"\"\"\n        A nonexistent namespace package should raise an ImproperlyConfigured\n        exception when it's listed in INSTALLED_APPS.\n        \"\"\"\n        with self.assertRaisesMessage(\n            ImproperlyConfigured,\n            \"Unable to import 'nonexistent_nsapp'. Check that the package \"\n            \"exists and is spelled correctly.\",\n        ):\n            with self.settings(INSTALLED_APPS=[\"nonexistent_nsapp\"]):\n                apps.get_app_config(\"nonexistent_nsapp\")\n", "    def test_namespace_package_with_init_py(self):\n        \"\"\"\n        A Py3.3+ namespace package with an __init__.py in one of its\n        locations can be an app, and the path should point to the location\n        with the __init__.py.\n        \"\"\"\n        # Create a temporary file inside the 'other_location' directory to\n        # simulate an __init__.py file.\n        init_py_path = os.path.join(self.other_location, \"nsapp\", \"__init__.py\")\n        with open(init_py_path, \"w\") as f:\n            f.write(\"\")\n\n        with extend_sys_path(self.base_location, self.other_location):\n            with self.settings(INSTALLED_APPS=[\"nsapp\"]):\n                app_config = apps.get_app_config(\"nsapp\")\n\n                # The path should point to the 'other_location' directory\n                # where the __init__.py file is located.\n                self.assertEqual(app_config.path, os.path.join(self.other_location, \"nsapp\"))\n\n", "    def test_namespace_package_no_init(self):\n        \"\"\"\n        A namespace package without __init__.py files is not a valid app.\n        \"\"\"\n        with extend_sys_path(self.base_location):\n            with self.assertRaises(ImproperlyConfigured):\n                with self.settings(INSTALLED_APPS=[\"nsapp_no_init\"]):\n                    pass\n", "    def test_duplicate_model_names(self):\n        \"\"\"\n        Ensure models with conflicting names but different app labels\n        are registered correctly.\n        \"\"\"\n        LogEntry = apps.get_model(\"admin\", \"LogEntry\")\n        self.assertEqual(LogEntry._meta.app_label, \"admin\")\n        TotallyNormal = apps.get_model(\"apps\", \"TotallyNormal\")\n\n        with self.assertRaises(LookupError):\n            apps.get_model(\"apps\", \"LogEntry\")\n"], "sample_123": ["    def test_escape_leading_slashes(self):\n        self.assertEqual(escape_leading_slashes('/path'), '/%2Fpath')\n        self.assertEqual(escape_leading_slashes('//example.com/path'), '/%2Fexample.com/path')\n        self.assertEqual(escape_leading_slashes('http://example.com/path'), 'http://example.com/path')\n", "    def test_escape_leading_slashes(self):\n        self.assertEqual(escape_leading_slashes('/path/to/resource'), '/%2Fpath/to/resource')\n        self.assertEqual(escape_leading_slashes('//example.com/path'), '/%2Fexample.com/path')\n        self.assertEqual(escape_leading_slashes('https://example.com/path'), 'https://example.com/path')\n", "    def test_is_same_domain_empty_hostnames(self):\n        self.assertIs(is_same_domain('', ''), False)\n        self.assertIs(is_same_domain('', 'example.com'), False)\n        self.assertIs(is_same_domain('example.com', ''), False)\n", "    def test_escape_leading_slashes(self):\n        self.assertEqual(escape_leading_slashes('/view/?param=http://example.com'), '/%2Fview/?param=http://example.com')\n        self.assertEqual(escape_leading_slashes('//example.com'), '/%2Fexample.com')\n        self.assertEqual(escape_leading_slashes('https://example.com'), 'https://example.com')\n", "    def test_escape_leading_slashes(self):\n        self.assertEqual(escape_leading_slashes('//example.com/path'), '/%2Fexample.com/path')\n        self.assertEqual(escape_leading_slashes('/example.com/path'), '/example.com/path')\n"], "sample_105": ["    def test_redirect_view_with_kwargs(self):\n        response = RedirectView.as_view(url='/bar/%(pk)d/', permanent=True)(self.rf.get('/foo/'), pk=42)\n        self.assertEqual(response.status_code, 301)\n        self.assertEqual(response['Location'], '/bar/42/')\n", "    def test_template_params_with_override(self):\n        \"\"\"\n        Test that kwargs passed to get_context_data override those passed to the view.\n        \"\"\"\n        response = self.client.get('/template/override/bar/')\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.context['foo'], 'baz')\n\n", "    def test_template_context_data_with_get_queryset(self):\n        \"\"\"A generic view's get_queryset() method can be used to inject data into context.\"\"\"\n        response = self.client.get('/template/queryset/data/')\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.context['object_list'], ['data_point_1', 'data_point_2'])\n\n", "    def test_template_view_context_data(self):\n        \"\"\"Test if get_context_data is called correctly.\"\"\"\n        response = self.client.get('/template/context/')\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.context['custom'], 'context')\n        self.assertIsInstance(response.context['view'], View)\n", "    def test_redirect_to_named_url_with_args(self):\n        response = RedirectView.as_view(pattern_name='artist_detail', args=['pk'])(self.rf.get('/foo/'), pk=1)\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response['Location'], '/detail/artist/1/')\n"], "sample_1155": ["def test_golden_ratio():\n    assert construct_domain([GoldenRatio, 1]) == (QQ.algebraic_field(GoldenRatio),\n                                             [QQ.algebraic_field(GoldenRatio).convert(GoldenRatio),\n                                              QQ.algebraic_field(GoldenRatio).convert(1)])\n", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1028": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_34": ["def test_equivalent_units_with_prefixes():\n    assert u.km.find_equivalent_units() == [u.km, u.kilometer, u.kmetra]\n", "def test_composite_unit_equivalencies():\n    # Issue #3782\n    cm_per_inch = 2.54\n    inch = u.cm / cm_per_inch\n    meter = (inch * 39.37007874)\n    assert meter.equiv(u.m)\n\n    cm_per_mile = cm_per_inch * 12 * 5280\n    mile = u.cm / cm_per_mile\n    assert (1 * mile).to(u.km) == 1.60934 * u.km\n\n", "    def test_compose_with_custom_unit(self):\n        class MyUnit(u.UnitBase):\n                self.value = value\n\n                return self.value * other.value\n\n        custom_unit = MyUnit(2)\n        composed = u.m.compose([custom_unit])\n        assert len(composed) == 1\n        assert composed[0].scale == 2\n        assert composed[0].bases == [u.m]\n", "compilation error", "compilation error"], "sample_820": ["compilation error", "compilation error", "compilation error", "    def test_voting_classifier_error_handling():\n        clf1 = LogisticRegression(random_state=123)\n        clf2 = RandomForestClassifier(random_state=123)\n        \n        # Test for error if 'voting' is not 'hard' or 'soft'\n        eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)],\n                                voting='invalid')\n        msg = (\"Voting must be 'soft' or 'hard'; got (voting='invalid')\")\n        with pytest.raises(ValueError, match=msg):\n            eclf.fit(X, y)\n        \n        # Test for error if estimators are not fitted\n        eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)], \n                                voting='soft')\n        msg = (\"This VotingClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.\")\n        with pytest.raises(NotFittedError, match=msg):\n            eclf.predict(X)\n    \n", "compilation error"], "sample_591": ["    def test_merge_noncoord_data_variables(self):\n        ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]})\n        ds2 = xr.Dataset({\"b\": (\"x\", [3, 4]), \"x\": [1, 2]})\n        ds1 = ds1.rename({\"x\": \"x1\"})\n        ds2 = ds2.rename({\"x\": \"x2\"})\n        expected = xr.Dataset({\"a\": (\"x1\", [1, 2]),\n                               \"b\": (\"x2\", [3, 4]),\n                               \"x1\": [0, 1], \"x2\": [1, 2]})\n        actual = ds1.merge(ds2)\n        assert expected.identical(actual)\n\n", "    def test_merge_empty_objects(self):\n        ds1 = xr.Dataset()\n        ds2 = xr.Dataset({\"x\": (\"y\", [1, 2]), \"y\": [0, 1]})\n        expected = ds2.copy()\n        actual = ds1.merge(ds2)\n        assert expected.identical(actual)\n\n        actual = ds2.merge(ds1)\n        assert expected.identical(actual)\n", "    def test_merge_override(self):\n        ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]})\n        ds2 = xr.Dataset({\"a\": (\"x\", [2, 3]), \"x\": [1, 2]})\n        expected = xr.Dataset({\"a\": (\"x\", [2, 3]), \"x\": [1, 2]})\n        assert expected.identical(ds1.merge(ds2, compat=\"override\"))\n\n", "    def test_merge_coords(self):\n        ds1 = xr.Dataset({\"x\": 0, \"y\": 1})\n        ds2 = xr.Dataset({\"x\": 0, \"z\": 2})\n        expected = xr.merge([ds1, ds2])\n        assert expected.identical(ds1.merge(ds2))\n        assert expected.identical(ds2.merge(ds1))\n\n", "    def test_merge_override(self):\n        ds1 = xr.Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]})\n        ds2 = xr.Dataset({\"a\": (\"x\", [2, 3]), \"x\": [1, 2]})\n        expected = ds2.copy()\n        assert_identical(expected, ds1.merge(ds2, compat=\"override\"))\n"], "sample_846": ["compilation error", "compilation error", "def test_column_transformer_remainder_with_callable():\n    X = np.array([[0, 1, 2], [2, 4, 6], [8, 6, 4]]).T\n    X_df = pd.DataFrame(X, columns=['first', 'second', 'third'])\n\n        return X['second']\n\n    ct = ColumnTransformer([('trans1', Trans(), ['first', 'third'])],\n                           remainder=remainder_func)\n    result = ct.fit_transform(X_df)\n    expected = np.hstack((Trans().fit_transform(X_df[['first', 'third']]),\n                          X_df[['second']]))\n\n    assert_array_equal(result, expected)\n\n", "    def test_column_transformer_missing_column():\n        pd = pytest.importorskip('pandas')\n        X = pd.DataFrame({'a': [1, 2], 'b': [3, 4]})\n        tf = ColumnTransformer([('bycol', Trans(), ['c'])])\n        with pytest.raises(LookupError, match=\"Column 'c' not found\"):\n            tf.fit(X)\n", "compilation error"], "sample_512": ["    def test_figure_subplot_reuse():\n        fig, ax1 = plt.subplots()\n        ax2 = plt.subplot(111)\n        assert ax1 is ax2\n        fig, ax3 = plt.subplots()\n        assert ax1 is not ax3\n", "    def test_subplot_kwarg_collision_twice():\n        plt.figure()\n        ax1 = plt.subplot(projection='polar', theta_offset=0)\n        ax2 = plt.subplot(projection='polar', theta_offset=0)\n        assert ax1 is ax2\n        ax1.remove()\n        ax3 = plt.subplot(projection='polar', theta_offset=1)\n        assert ax1 is not ax3\n        assert ax1 not in plt.gcf().axes\n        ax3.remove()\n        ax4 = plt.subplot(projection='polar', theta_offset=0)\n        assert ax3 is not ax4\n\n", "def test_subplot_kwarg_handling():\n    fig = plt.figure()\n\n    # Test passing kwargs directly to subplot()\n    ax1 = plt.subplot(projection='polar', theta_offset=np.pi / 2)\n    assert ax1.name == 'polar'\n    assert ax1.theta_offset == np.pi / 2\n    \n\n    ax2 = plt.subplot(111, projection='polar', theta_offset=np.pi)\n    assert ax2 is not ax1\n    assert ax2.name == 'polar'\n    assert ax2.theta_offset == np.pi\n", "def test_subplot_kwarg_collision_projection():\n    plt.figure()\n    ax1 = plt.subplot(projection='polar', theta_offset=0)\n    ax2 = plt.subplot(projection='polar', theta_offset=1)\n    assert ax1 is not ax2\n    plt.close()\n", "compilation error"], "sample_736": ["    def test_logreg_predict_proba_ovr_binary():\n        # Test that predict_proba for ovr works for binary classification\n        X, y = make_classification(n_samples=10, n_features=20, random_state=0,\n                                   n_classes=2)\n\n        clf_ovr = LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\")\n        clf_ovr.fit(X, y)\n\n        prob = clf_ovr.predict_proba(X)\n        assert_equal(prob.shape, (len(X), 2))\n        assert_array_almost_equal(prob.sum(axis=1), np.ones(len(X)))\n\n", "    def test_predict_proba_consistency():\n        # Test that predict_proba is consistent with predict for binary case\n        X, y = make_classification(n_samples=100, n_features=20, random_state=0)\n        clf = LogisticRegression(solver='lbfgs', random_state=42)\n        clf.fit(X, y)\n\n        y_proba = clf.predict_proba(X)\n        y_pred = clf.predict(X)\n\n        assert_array_equal(np.argmax(y_proba, axis=1), y_pred)\n", "    def test_saga_sample_weights():\n        iris = load_iris()\n        X, y = iris.data, iris.target\n        X = np.concatenate([X] * 10)\n        y = np.concatenate([y] * 10)\n        sample_weight = np.random.RandomState(42).rand(X.shape[0])\n        n_samples = X.shape[0]\n        for penalty in ['l1', 'l2']:\n            for alpha in np.logspace(-1, 1, 3):\n                saga = LogisticRegression(\n                    C=1. / (n_samples * alpha),\n                    solver='saga',\n                    multi_class='ovr',\n                    max_iter=200,\n                    fit_intercept=False,\n                    penalty=penalty, random_state=0, tol=1e-24)\n\n                saga.fit(X, y, sample_weight=sample_weight)\n                # Check if results are consistent\n                saga_coef = saga.coef_.copy()\n                saga.fit(X, y, sample_weight=sample_weight)\n                assert_array_almost_equal(saga.coef_, saga_coef)\n", "    def test_class_weight_sparse():\n        # Test class_weight with sparse input\n        X, y = make_classification(n_samples=100, n_features=20, random_state=0)\n        X_sparse = sparse.csr_matrix(X)\n        class_weight = {0: 0.5, 1: 1.}\n        clf = LogisticRegression(class_weight=class_weight, solver='lbfgs')\n        clf.fit(X_sparse, y)\n", "    def test_logreg_l1_sparse_data_empty_features():\n        # Test that the l1 penalty works correctly with sparse data\n        # containing empty features.\n\n        rng = np.random.RandomState(42)\n        n_samples = 50\n        X, y = make_classification(n_samples=n_samples, n_features=20,\n                                   random_state=0)\n        X_noise = rng.normal(scale=0.1, size=(n_samples, 3))\n        X_empty = np.zeros(shape=(n_samples, 2))\n        X = np.concatenate((X, X_noise, X_empty), axis=1)\n        X[X < 1] = 0\n        X = sparse.csr_matrix(X)\n\n        lr_liblinear = LogisticRegression(penalty=\"l1\", C=1.0, solver='liblinear',\n                                         fit_intercept=False,\n                                         tol=1e-10)\n        lr_liblinear.fit(X, y)\n\n        lr_saga = LogisticRegression(penalty=\"l1\", C=1.0, solver='saga',\n                                     fit_intercept=False,\n                                     max_iter=1000, tol=1e-10)\n        lr_saga.fit(X, y)\n        assert_array_almost_equal(lr_saga.coef_, lr_liblinear.coef_)\n        # Weights corresponding to empty features should be zero\n        assert_array_equal(lr_saga.coef_[:, -2:], np.zeros((lr_saga.coef_.shape[0], 2)))\n        assert_array_equal(lr_liblinear.coef_[:, -2:], np.zeros((lr_liblinear.coef_.shape[0], 2)))\n"], "sample_955": ["def test_unparse_function_def(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0], source) == expected\n", "def test_parse_type_comments(source, expected, typed_comments):\n    if typed_comments and sys.version_info >= (3, 8):\n        module = ast.parse(source, type_comments=True)\n        assert ast.unparse(module) == expected\n    else:\n        with pytest.raises(SyntaxError):\n            ast.parse(source, type_comments=True)\n", "    def test_unparse_with_type_comments():\n        code = \"\"\"", "    def test_unparse_arguments():\n        source = \"def func(a: int, b: str = 'default', *args, c: float, **kwargs): pass\"\n        module = ast.parse(source)\n        assert ast.unparse(module.body[0].args) == \"a: int, b: str = 'default', *args, c: float, **kwargs\"\n", "def test_unparse_function_def(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0], source) == expected\n"], "sample_717": ["    def test_load_fake_lfw_pairs():\n        lfw_pairs = fetch_lfw_pairs(subset='train', data_home=SCIKIT_LEARN_DATA,\n                                    download_if_missing=False)\n\n        assert_equal(lfw_pairs.data.shape, (10, 2914 * 2))\n        assert_equal(lfw_pairs.pairs.shape, (10, 2, 62, 47))\n        assert_array_equal(lfw_pairs.target,\n                          [1, 0, 1, 0, 1, 0, 1, 0, 1, 0])\n        assert_array_equal(lfw_pairs.target_names,\n                           ['Different persons', 'Same person'])\n", "def test_load_fake_lfw_pairs():\n    lfw_pairs = fetch_lfw_pairs(subset='train', data_home=SCIKIT_LEARN_DATA,\n                                download_if_missing=False)\n\n    # The data is flatten pair wise but is reshape-able to the original size.\n    assert_equal(lfw_pairs.data.shape, (10, 2914 * 2))\n    assert_equal(lfw_pairs.pairs.shape, (10, 2, 62, 47))\n    assert_array_equal(lfw_pairs.target, [1, 0, 0, 1, 0, 0, 1, 1, 0, 1])\n    assert_array_equal(lfw_pairs.target_names, ['Different persons', 'Same person'])\n", "def test_load_fake_lfw_pairs():\n    lfw_pairs = fetch_lfw_pairs(data_home=SCIKIT_LEARN_DATA,\n                                subset='train', download_if_missing=False)\n    assert_equal(lfw_pairs.pairs.shape, (10, 2, 62, 47))\n    assert_equal(lfw_pairs.data.shape, (10, 2914*2))\n    assert_array_equal(lfw_pairs.target, [1, 0, 1, 0, 1, 0, 1, 0, 1, 0])\n    assert_array_equal(lfw_pairs.target_names, ['Different persons', 'Same person'])\n", "    def test_load_fake_lfw_pairs():\n        lfw_pairs = fetch_lfw_pairs(subset='train',\n                                     data_home=SCIKIT_LEARN_DATA,\n                                     download_if_missing=False)\n\n        assert_equal(lfw_pairs.pairs.shape, (10, 2, 62, 47))\n        assert_equal(lfw_pairs.data.shape, (10, 2 * 2914))\n        assert_array_equal(lfw_pairs.target, [1, 0, 1, 0, 1, 0, 1, 0, 1, 0])\n", "    def test_load_fake_lfw_pairs():\n        lfw_pairs = fetch_lfw_pairs(subset='train', data_home=SCIKIT_LEARN_DATA,\n                                     download_if_missing=False)\n\n        assert_equal(lfw_pairs.data.shape, (10, 2914 * 2))\n        assert_equal(lfw_pairs.pairs.shape, (10, 2, 62, 47))\n        assert_array_equal(lfw_pairs.target, [1, 0, 0, 1, 1, 0, 0, 1, 0, 0])\n        assert_array_equal(lfw_pairs.target_names,\n                           ['Different persons', 'Same person'])\n"], "sample_454": ["    def test_validate_condition_with_exclude(self):\n        p1 = UniqueConstraintConditionProduct.objects.create(name=\"p1\")\n        constraint = UniqueConstraintConditionProduct._meta.constraints[0]\n        msg = \"Constraint \u201cname_without_color_uniq\u201d is violated.\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            constraint.validate(\n                UniqueConstraintConditionProduct,\n                UniqueConstraintConditionProduct(name=p1.name, color=None),\n                exclude={\"name\"},\n            )\n", "    def test_check_constraint_creation(self):\n        constraint = models.CheckConstraint(\n            check=models.Q(price__gt=models.F(\"discount\")), name=\"price_gt_discount\"\n        )\n        Product._meta.constraints.append(constraint)\n        with self.assertRaises(IntegrityError):\n            Product.objects.create(price=5, discount=10)\n\n", "    def test_expressions_with_include(self):\n        msg = (\n            \"UniqueConstraint.include cannot be used with expressions. Use \"\n            \"django.db.models.F() within the expression instead.\"\n        )\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                Lower(\"field\"), name=\"test_func_include\", include=[\"other_field\"]\n            )\n", "    def test_include_field_in_covering_index(self):\n        UniqueConstraintInclude.objects.create(name=\"p1\", color=\"red\")\n        with connection.cursor() as cursor:\n            cursor.execute(\n                \"SELECT indexname FROM pg_indexes WHERE tablename = %s AND indexname = %s\",\n                [\n                    UniqueConstraintInclude._meta.db_table,\n                    \"unique_constraint_include_name_color_idx\",\n                ],\n            )\n            self.assertEqual(cursor.fetchone(), (\"unique_constraint_include_name_color_idx\",))\n", "    def test_include_covering_index_creation(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\n                \"SELECT COUNT(*) FROM pg_indexes WHERE indexname = 'unique_constraint_include_covering_idx'\"\n            )\n            self.assertEqual(cursor.fetchone()[0], 0)\n        UniqueConstraintInclude.objects.create(name=\"p1\", color=\"red\")\n        with connection.cursor() as cursor:\n            cursor.execute(\n                \"SELECT COUNT(*) FROM pg_indexes WHERE indexname = 'unique_constraint_include_covering_idx'\"\n            )\n            self.assertEqual(cursor.fetchone()[0], 1)\n"], "sample_125": ["    def test_delete_cookie_with_path(self):\n        response = HttpResponse()\n        response.set_cookie('c', path='/test')\n        response.delete_cookie('c', path='/test')\n        cookie = response.cookies['c']\n        self.assertEqual(cookie['expires'], 'Thu, 01 Jan 1970 00:00:00 GMT')\n        self.assertEqual(cookie['max-age'], 0)\n        self.assertEqual(cookie['path'], '/test')\n", "    def test_delete_cookie_with_path_and_domain(self):\n        response = HttpResponse()\n        response.delete_cookie('c', path='/path', domain='example.com')\n        cookie = response.cookies['c']\n        self.assertEqual(cookie['expires'], 'Thu, 01 Jan 1970 00:00:00 GMT')\n        self.assertEqual(cookie['max-age'], 0)\n        self.assertEqual(cookie['path'], '/path')\n        self.assertEqual(cookie['domain'], 'example.com')\n", "    def test_secure_cookie(self):\n        response = HttpResponse()\n        response.delete_cookie('c', secure=True)\n        cookie = response.cookies['c']\n        self.assertEqual(cookie['expires'], 'Thu, 01 Jan 1970 00:00:00 GMT')\n        self.assertEqual(cookie['max-age'], 0)\n        self.assertEqual(cookie['path'], '/')\n        self.assertEqual(cookie['secure'], True)\n        self.assertEqual(cookie['domain'], '')\n\n", "    def test_secure_cookie(self):\n        response = HttpResponse()\n        response.set_cookie('secure', 'value', secure=True)\n        self.assertIn('; Secure', str(response.cookies['secure']))\n", "    def test_delete_cookie_with_path_and_domain(self):\n        response = HttpResponse()\n        response.delete_cookie('c', path='/mypath', domain='example.com')\n        cookie = response.cookies['c']\n        self.assertEqual(cookie['expires'], 'Thu, 01 Jan 1970 00:00:00 GMT')\n        self.assertEqual(cookie['max-age'], 0)\n        self.assertEqual(cookie['path'], '/mypath')\n        self.assertEqual(cookie['secure'], '')\n        self.assertEqual(cookie['domain'], 'example.com')\n"], "sample_1174": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_863": ["compilation error", "    def test_pipeline_fit_params_no_fit_method():\n        transformer = NoFitTransformer()\n        pipeline = Pipeline([('transformer', transformer),\n                             ('classifier', LogisticRegression())])\n        X = [[1, 2], [3, 4]]\n        y = [0, 1]\n        with pytest.raises(AttributeError,\n                           match='Transformer transformer '\n                                 '(type NoFitTransformer) does not '\n                                 'provide a fit method.'):\n            pipeline.fit(X, y, transformer__param='value')\n", "    def test_pipeline_empty_steps():\n        # Test Pipeline with empty steps\n        pipe = Pipeline([])\n        assert len(pipe.steps) == 0\n        assert pipe.named_steps == {}\n\n        X = [[1, 2, 3]]\n        assert_array_equal(pipe.fit_transform(X, [0]), X)\n        assert_array_equal(pipe.transform(X), X)\n        assert_array_equal(pipe.predict(X), [0])  # Predict on empty pipeline\n", "    def test_pipeline_fit_params_passed_to_steps():\n        class DummyEstimator(BaseEstimator):\n                self.fit_param = fit_param\n                self.fit_param = fit_params.get('fit_param', self.fit_param)\n                return self\n\n        transf = DummyEstimator()\n        clf = DummyEstimator()\n        pipe = Pipeline([('transf', transf), ('clf', clf)])\n\n        X = [[1, 2], [3, 4]]\n        y = [0, 1]\n\n        pipe.fit(X, y, fit_params={'transf__fit_param': 1, 'clf__fit_param': 2})\n\n        assert transf.fit_param == 1\n        assert clf.fit_param == 2\n\n", "    def test_pipeline_empty_steps():\n        empty_pipe = Pipeline([])\n        assert empty_pipe.steps == []\n        assert len(empty_pipe) == 0\n\n        empty_pipe.fit([[1]], [1])\n        assert_array_equal([[1]], empty_pipe.transform([[1]]))\n        assert_array_equal([[1]], empty_pipe.fit_transform([[1]], [1]))\n\n        # Test with FeatureUnion\n        empty_union = FeatureUnion([])\n        assert empty_union.transformer_list == []\n        assert len(empty_union) == 0\n\n        empty_union.fit([[1]], [1])\n        assert_array_equal([[]], empty_union.transform([[1]]))\n        assert_array_equal([[]], empty_union.fit_transform([[1]], [1]))\n"], "sample_605": ["compilation error", "    def test_groupby_bins_empty_bins(array):\n        bins = [0, 1, 2, 3]\n        actual = array.groupby_bins(\"x\", bins=bins).sum()\n        expected = xr.DataArray(\n            np.nan, dims=[\"x_bins\"], coords={\"x_bins\": [Interval(0, 1, closed='right'), Interval(1, 2, closed='right'), Interval(2, 3, closed='right')]}\n        ).to_dataset(name=\"foo\")\n        assert_identical(actual, expected)\n\n", "compilation error", "compilation error", "def test_groupby_reduce_empty_dataset():\n    ds = xr.Dataset(data_vars={\"foo\": ((\"x\", \"y\"), [])}, coords={\"x\": [], \"y\": []})\n    actual = ds.groupby(\"x\").mean()\n    expected = xr.Dataset(\n        data_vars={\"foo\": ((\"x\",), [])}, coords={\"x\": []}\n    )\n    assert_identical(actual, expected)\n"], "sample_234": ["    def test_annotate_after_union(self):\n        qs1 = Number.objects.filter(num__lte=1).annotate(\n            count=Value(0, IntegerField()),\n        )\n        qs2 = Number.objects.filter(num__gte=2, num__lte=3).annotate(\n            count=Count('id'),\n        )\n        self.assertEqual(\n            list(qs1.union(qs2).values_list('num', 'count')),\n            [(0, 0), (1, 0), (2, 1), (3, 1)],\n        )\n", "    def test_union_with_complex_annotation(self):\n        qs1 = Number.objects.annotate(\n            sum_of_itself_and_num=F('num') + F('num'),\n        )\n        qs2 = Number.objects.annotate(\n            sum_of_itself_and_num=F('num') * 2,\n        )\n        self.assertQuerysetEqual(\n            qs1.union(qs2).filter(sum_of_itself_and_num=4),\n            [Number(num=2)],\n            operator.attrgetter('num'),\n        )\n", "    def test_union_with_select_related(self):\n        qs1 = Number.objects.select_related('other_num').filter(num=1)\n        qs2 = Number.objects.select_related('other_num').filter(num=2)\n        self.assertCountEqual(qs1.union(qs2).values_list('num', 'other_num'), [(1, 9), (2, 8)])\n", "    def test_union_with_annotation_on_different_fields(self):\n        qs1 = Number.objects.annotate(double_num=F('num') * 2)\n        qs2 = Number.objects.annotate(half_num=F('num') / 2)\n        self.assertQuerysetEqual(\n            qs1.union(qs2).order_by('num'),\n            [\n                {'num': 0, 'double_num': 0, 'half_num': None},\n                {'num': 1, 'double_num': 2, 'half_num': 0.5},\n                {'num': 2, 'double_num': 4, 'half_num': 1.0},\n                {'num': 3, 'double_num': 6, 'half_num': 1.5},\n                {'num': 4, 'double_num': 8, 'half_num': 2.0},\n                {'num': 5, 'double_num': 10, 'half_num': 2.5},\n                {'num': 6, 'double_num': 12, 'half_num': 3.0},\n                {'num': 7, 'double_num': 14, 'half_num': 3.5},\n                {'num': 8, 'double_num': 16, 'half_num': 4.0},\n                {'num': 9, 'double_num': 18, 'half_num': 4.5},\n            ],\n            operator.itemgetter('num', 'double_num', 'half_num'),\n        )\n", "    def test_union_with_distinct(self):\n        qs1 = Number.objects.filter(num__lte=3).values('num').distinct()\n        qs2 = Number.objects.filter(num__gte=6).values('num').distinct()\n        self.assertNumbersEqual(qs1.union(qs2), [0, 1, 2, 3, 6, 7, 8, 9], ordered=False)\n\n"], "sample_903": ["    def test_small_data_exact(self):\n        # Test the exact method on a very small dataset to ensure it doesn't fail\n        random_state = check_random_state(0)\n        X = random_state.randn(5, 2)\n        tsne = TSNE(n_components=2, method='exact', init=\"random\",\n                    random_state=0, n_iter=500, perplexity=5)\n        tsne.fit_transform(X)\n", "def test_early_exaggeration_iterations():\n    # Ensure that the correct number of early exaggeration iterations are\n    # performed.\n    random_state = check_random_state(0)\n    X = random_state.randn(50, 2)\n    tsne = TSNE(n_components=2, perplexity=10, early_exaggeration=12,\n                n_iter_without_progress=50, random_state=0,\n                init=\"random\", method='exact')\n    tsne.fit_transform(X)\n\n    assert tsne.n_iter_ == tsne._EXPLORATION_N_ITER + tsne._N_ITER_CHECK\n\n", "    def test_negative_perplexity():\n        # Check if the code raises a ValueError when perplexity is negative\n        tsne = TSNE(perplexity=-1)\n        with pytest.raises(ValueError) as excinfo:\n            tsne.fit_transform(np.random.randn(10, 2))\n        assert \"perplexity\" in str(excinfo.value)\n", "def test_perplexity_validation():\n    # Test that the perplexity parameter is validated correctly\n    tsne = TSNE(perplexity=-1)\n    assert_raises(ValueError, tsne.fit_transform, np.array([[0.0], [1.0]]))\n", "    def test_negative_perplexity():\n        # Check if a ValueError is raised when perplexity is negative.\n        random_state = check_random_state(0)\n        X = random_state.randn(100, 10)\n\n        for method in [\"barnes_hut\", \"exact\"]:\n            tsne = TSNE(perplexity=-1, random_state=0, method=method)\n            assert_raises_regexp(ValueError, \"perplexity must be greater than 0\",\n                                 tsne.fit_transform, X)\n"], "sample_1173": ["    def test_issue_12832():\n        x = Symbol('x')\n        y = Symbol('y')\n        assert parse_expr('x + y + 1', evaluate=False) == parse_expr('x + y + 1') == Add(x, y, 1)\n        assert parse_expr('x - y - 1', evaluate=False) == parse_expr('x - y - 1') == Add(x, -y, -1)\n\n", "    def test_issue_13081():\n        # Test implicit multiplication involving parentheses when applying\n        # the implicit_multiplication_application transformation.\n        transformations = standard_transformations + (implicit_multiplication_application,)\n        x = Symbol('x')\n        assert parse_expr('(x+1)(x-1)', transformations=transformations) == (x+1)*(x-1)\n        assert parse_expr('(x+1)(2)', transformations=transformations) == (x+1)*2\n\n", "def test_issue_11029():\n    x = Symbol('x')\n    y = Symbol('y')\n    assert parse_expr('2**(x + y)', evaluate=False) == 2**(x + y)\n    assert parse_expr('2**(x+y)', evaluate=False) == 2**(x + y)\n", "compilation error", "compilation error"], "sample_656": ["def test_multicapture_with_live_logging(testdir):\n    # Issue 3819\n    # capture should work with live cli logging\n\n    testdir.makeconftest(\n        \"\"\"\n            if \"test_multi\" in report.nodeid:\n                if report.when == \"teardown\":\n                    with open(\"caplog\", \"w\") as f:\n                        f.write(report.caplog)\n                    with open(\"capstdout\", \"w\") as f:\n                        f.write(report.capstdout)\n        \"\"\"\n    )\n\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import sys\n        import pytest\n\n        logger = logging.getLogger(__name__)\n\n        @pytest.fixture\n            print(\"fix setup\")\n            logging.info(\"fix setup\")\n            yield\n            logging.info(\"fix teardown\")\n            print(\"fix teardown\")\n\n            print(\"begin test\")\n            logging.info(\"something in test\")\n            print(\"end test\")\n    \"\"\"\n    )\n    result = testdir.runpytest_subprocess(\"--log-cli-level=INFO\")\n    assert result.ret == 0\n\n    with open(\"caplog\", \"r\") as f:\n        caplog = f.read()\n\n    assert \"fix setup\" in caplog\n    assert \"something in test\" in caplog\n    assert \"fix teardown\" in caplog\n\n    with open(\"capstdout\", \"r\") as f:\n        capstdout = f.read()\n\n    assert \"fix setup\" in capstdout\n    assert \"begin test\" in capstdout\n    assert \"end test\" in capstdout\n    assert \"fix teardown\" in capstdout\n\n", "    def test_capture_with_live_logging_and_no_capture(testdir):\n        # Issue 3819\n        # capture should work with live cli logging\n        # and when no capture fixture is used\n\n        testdir.makepyfile(\n            \"\"\"\n            import logging\n            import sys\n\n            logger = logging.getLogger(__name__)\n\n                print(\"hello\")\n                sys.stderr.write(\"world\\\\n\")\n\n                logging.info(\"something\")\n                print(\"next\")\n                logging.info(\"something\")\n            \"\"\"\n        )\n\n        result = testdir.runpytest_subprocess(\"--log-cli-level=INFO\")\n        assert result.ret == 0\n", "def test_capture_with_live_logging_and_errors(testdir, capsys):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import sys\n\n        logger = logging.getLogger(__name__)\n\n            try:\n                raise ValueError(\"Some error\")\n            except ValueError:\n                logging.error(\"Some error occurred\", exc_info=True)\n            print(\"hello\")\n            sys.stderr.write(\"world\\\\n\")\n            captured = capsys.readouterr()\n            assert captured.out == \"hello\\\\n\"\n            assert captured.err == \"world\\\\n\"\n        \"\"\"\n    )\n\n    result = testdir.runpytest_subprocess(\"--log-cli-level=ERROR\")\n    assert result.ret == 0\n", "    def test_capture_with_live_logging_and_error(testdir, capsys):\n        testdir.makepyfile(\n            \"\"\"\n            import logging\n            import sys\n\n            logger = logging.getLogger(__name__)\n\n                print(\"hello\")\n                sys.stderr.write(\"world\\\\n\")\n                captured = capsys.readouterr()\n                assert captured.out == \"hello\\\\n\"\n                assert captured.err == \"world\\\\n\"\n\n                logging.info(\"something\")\n                print(\"next\")\n                raise ValueError(\"intentional error\")\n                logging.info(\"something\")\n\n            \"\"\"\n        )\n\n        result = testdir.runpytest_subprocess(\"--log-cli-level=INFO\")\n        assert result.ret == 1\n        assert \"ValueError\" in result.stdout.str()\n", "def test_capture_during_logging_setup(testdir, capsys):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import pytest\n        \n        logging.basicConfig(level=\"INFO\")\n\n        @pytest.fixture\n            logging.info(\"fix setup\")\n            yield\n            logging.info(\"fix teardown\")\n\n            print(\"hello\")\n            captured = capsys.readouterr()\n            assert captured.out == \"hello\\n\"\n            logging.info(\"something\")\n\n            captured = capsys.readouterr()\n            assert captured.out == \"\"\n        \"\"\"\n    )\n\n    result = testdir.runpytest_subprocess()\n    assert result.ret == 0\n"], "sample_585": ["def test_groupby_empty_dataset():\n    empty_dataset = xr.Dataset()\n    with pytest.raises(ValueError):\n        empty_dataset.groupby('dim')\n", "    def test_groupby_empty_dataset():\n        ds = xr.Dataset({\n            'a': ('x', []),\n            'b': ('y', [1, 2, 3]),\n        })\n        with pytest.raises(ValueError):\n            ds.groupby('a')\n", "    def test_da_groupby_nonexistent_dim():\n        array = xr.DataArray([1, 2, 3], dims=['x'])\n        with pytest.raises(ValueError):\n            array.groupby('y')\n\n", "def test_groupby_reduce_empty_dimension():\n    # Regression test for GH1885\n    # Ensure correct handling of empty dimensions during groupby reduction\n\n    data = xr.DataArray([1, 2], dims=['x'])\n    expected = xr.DataArray([1, 2], dims=['x'])\n    actual = data.groupby('x').sum()\n\n    assert_identical(expected, actual)\n", "compilation error"], "sample_479": ["    def test_rename_model_with_relation(self):\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    \"Author\", [(\"name\", models.CharField(max_length=255))]\n                ),\n                migrations.CreateModel(\n                    \"Book\",\n                    [(\"title\", models.CharField(max_length=255))],\n                    bases=(\"testapp.Author\",),\n                ),\n                migrations.RenameModel(\"Author\", \"Writer\"),\n            ],\n            [\n                migrations.CreateModel(\n                    \"Writer\", [(\"name\", models.CharField(max_length=255))]\n                ),\n                migrations.CreateModel(\n                    \"Book\",\n                    [\n                        (\"title\", models.CharField(max_length=255)),\n                    ],\n                    bases=(\"testapp.Writer\",),\n                ),\n            ],\n        )\n\n", "    def test_optimize_through_multi_index_fields(self):\n        \"\"\"\n        Test that optimizations work with multi-column indexes that are renamed and\n        altered during migration.\n        \"\"\"\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    \"Foo\",\n                    [\n                        (\"a\", models.IntegerField()),\n                        (\"b\", models.IntegerField()),\n                        (\"c\", models.IntegerField()),\n                    ],\n                ),\n                migrations.CreateModel(\n                    \"Bar\", [(\"size\", models.IntegerField()), (\"weight\", models.IntegerField())]\n                ),\n                migrations.AddIndex(\n                    model_name=\"foo\",\n                    fields=[\"a\", \"b\"],\n                    name=\"idx_foo_ab\",\n                ),\n                migrations.AddIndex(\n                    model_name=\"bar\", fields=[\"size\", \"weight\"], name=\"idx_bar_sw\"\n                ),\n                migrations.RenameModel(\"Foo\", \"Phou\"),\n                migrations.RenameIndex(\n                    \"Phou\", new_name=\"idx_phou_ba\", old_fields=[\"a\", \"b\"]\n                ),\n                migrations.RenameIndex(\n                    \"Bar\", new_name=\"idx_bar_ws\", old_name=\"idx_bar_sw\"\n                ),\n                migrations.RenameField(\"Phou\", \"a\", \"x\"),\n                migrations.RenameField(\"Phou\", \"b\", \"y\"),\n                migrations.DeleteModel(\"Phou\"),\n            ],\n            [\n                migrations.CreateModel(\n                    \"Bar\",\n                    [\n                        (\"size\", models.IntegerField()),\n                        (\"weight\", models.IntegerField()),\n                    ],\n                ),\n                migrations.AddIndex(\n                    model_name=\"bar\", fields=[\"size\", \"weight\"], name=\"idx_bar_ws\"\n                ),\n            ],\n        )\n", "    def test_drop_and_add_field(self):\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    \"Foo\",\n                    [\n                        (\"name\", models.CharField(max_length=255)),\n                        (\"age\", models.IntegerField()),\n                    ],\n                ),\n                migrations.RemoveField(\"Foo\", \"age\"),\n                migrations.AddField(\"Foo\", \"age\", models.FloatField()),\n            ],\n            [\n                migrations.CreateModel(\n                    \"Foo\",\n                    [\n                        (\"name\", models.CharField(max_length=255)),\n                        (\"age\", models.FloatField()),\n                    ],\n                ),\n            ],\n        )\n", "    def test_optimize_through_delete_model(self):\n        \"\"\"\n        When a model is deleted and has relations, related operations should be\n        optimized too.\n        \"\"\"\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    \"Foo\",\n                    [\n                        (\"name\", models.CharField(max_length=255)),\n                        (\"age\", models.IntegerField()),\n                    ],\n                ),\n                migrations.CreateModel(\n                    \"Bar\",\n                    [\n                        (\"name\", models.CharField(max_length=255)),\n                        (\"foo\", models.ForeignKey(\"Foo\", models.CASCADE)),\n                    ],\n                ),\n                migrations.AddField(\"Bar\", \"value\", models.IntegerField()),\n                migrations.DeleteModel(\"Bar\"),\n                migrations.DeleteModel(\"Foo\"),\n            ],\n            [],\n        )\n", "    def test_create_model_add_field_with_default(self):\n        managers = [(\"objects\", EmptyManager())]\n        self.assertOptimizesTo(\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[(\"name\", models.CharField(max_length=255))],\n                    options={\"verbose_name\": \"Foo\"},\n                    bases=(UnicodeModel,),\n                    managers=managers,\n                ),\n                migrations.AddField(\n                    \"Foo\", \"age\", models.IntegerField(default=10)\n                ),\n            ],\n            [\n                migrations.CreateModel(\n                    name=\"Foo\",\n                    fields=[\n                        (\"name\", models.CharField(max_length=255)),\n                        (\"age\", models.IntegerField(default=10)),\n                    ],\n                    options={\"verbose_name\": \"Foo\"},\n                    bases=(UnicodeModel,),\n                    managers=managers,\n                ),\n            ],\n        )\n"], "sample_1088": ["compilation error", "compilation error", "compilation error", "compilation error", "def test_viete():\n    assert viete(x**2 + 1, [a, b], x) == [(a + b, -1)]\n    assert viete(x**2 + 2*x + 1, [a, b], x) == [(a + b, 1), (a*b, 1)]\n    assert viete(x**3 + x**2 + x + 1, [a, b, c], x) == [(a + b + c, -1), (a*b + a*c + b*c, 0), (a*b*c, 1)]\n    assert viete(2*x**2 - 4*x + 2, [a, b], x) == [(a + b, 2), (a*b, 1)]\n    assert viete(a*x**2 + b*x + c, [a, b], x) == [(a + b, -b/a), (a*b, c/a)]\n    assert viete(x**2 + 2*x + 1, roots=[a, b], x) == [(a + b, 1), (a*b, 1)]\n    raises(ValueError, lambda: viete(x + 1, [a, b], x))\n    raises(MultivariatePolynomialError, lambda: viete(x**2 + y**2, x,y))\n"], "sample_549": ["compilation error", "compilation error", "def test_safe_first_element_empty():\n    assert cbook._safe_first_finite([]) is None\n", "    def test_contiguous_regions_empty():\n        assert cbook.contiguous_regions([]) == []\n", "    def test_reshape2d_empty(self):\n        # Test reshaping an empty list\n        x = []\n        xnew = cbook._reshape_2D(x, 'x')\n        assert np.shape(xnew) == (0, 0)\n"], "sample_730": ["compilation error", "    def test_enet_cv_l1_ratio_grid(self):\n        X, y, _, _ = build_dataset(n_samples=20, n_features=10)\n        # Test that providing a list of l1_ratio values works\n        l1_ratios = [0.1, 0.5, 0.9]\n        clf = ElasticNetCV(l1_ratios=l1_ratios, cv=5, random_state=42)\n        clf.fit(X, y)\n        assert_equal(len(clf.l1_ratio_), len(l1_ratios))\n        assert_array_equal(clf.l1_ratio_, l1_ratios)\n\n        # Test that providing a list of l1_ratio values works for\n        # MultiTaskElasticNetCV\n        l1_ratios = [0.1, 0.5, 0.9]\n        clf = MultiTaskElasticNetCV(l1_ratios=l1_ratios, cv=5, random_state=42)\n        clf.fit(X, y[:, np.newaxis])\n        assert_equal(len(clf.l1_ratio_), len(l1_ratios))\n        assert_array_equal(clf.l1_ratio_, l1_ratios)\n", "compilation error", "compilation error", "    def test_enet_sample_weights(self):\n\n        X, y, _, _ = build_dataset(n_samples=20, n_features=10)\n        sample_weight = np.ones(20)\n        sample_weight[:5] = 2.\n\n        clf = ElasticNet(alpha=0.5, max_iter=100)\n        clf.fit(X, y, sample_weight=sample_weight)\n        # test predict\n        prediction = clf.predict(X)\n        # test score\n        score = clf.score(X, y, sample_weight=sample_weight)\n"], "sample_534": ["    def test_contour_with_empty_arraylike():\n        fig, ax = plt.subplots()\n        with pytest.raises(ValueError):\n            ax.contour(np.array([]))\n\n        with pytest.raises(ValueError):\n            ax.contour([], [])\n\n", "compilation error", "def test_contour_linewidth_after_remove():\n    fig, ax = plt.subplots()\n    cs = ax.contour([[1, 2], [3, 4]])\n    linewidths = [1, 2]\n    cs.set_linewidths(linewidths)\n    assert all(\n        line.get_linewidth() == lw for line, lw in zip(cs.collections, linewidths)\n    )\n    cs.remove()\n    # After removing, linewidths should not have changed\n    assert all(\n        line.get_linewidth() == lw for line, lw in zip(cs.collections, linewidths)\n    )\n\n", "def test_contour_linewidth_scaling():\n    fig, ax = plt.subplots()\n    x = np.arange(0, 10, 0.1)\n    y = np.sin(x)\n    ax.contour(x, y, np.ones_like(y))\n    # Test that the linewidths scale correctly when zoomed in\n    ax.set_xlim(0, 1)\n    ax.set_ylim(-1, 1)\n    assert len(ax.lines) > 0  \n", "compilation error"], "sample_772": ["def check_n_jobs_oob_score(name):\n    # Test that n_jobs is correctly handled when oob_score is True\n    X, y = hastie_X, hastie_y\n    ForestEstimator = FOREST_ESTIMATORS[name]\n    # Use 15 estimators to avoid 'some inputs do not have OOB scores' warning.\n    est = ForestEstimator(n_estimators=15, max_depth=3, oob_score=True,\n                          random_state=1, bootstrap=True)\n\n    # Check that the oob score is calculated with n_jobs > 1\n    for n_jobs in [2, -1]:\n        est.set_params(n_jobs=n_jobs)\n        est.fit(X, y)\n        assert hasattr(est, 'oob_score_')\n        assert est.oob_score_ is not None\n\n    # Check that the oob score is not calculated with n_jobs = 1\n    est.set_params(n_jobs=1)\n    est.fit(X, y)\n    assert not hasattr(est, 'oob_score_')\n\n", "def test_oob_score_early_stopping():\n    # Test out-of-bag score with early stopping\n    X, y = make_classification(n_samples=100, n_features=10,\n                               random_state=0)\n    est = RandomForestClassifier(n_estimators=10, oob_score=True,\n                                 random_state=0)\n    est.fit(X, y)\n    assert hasattr(est, 'oob_score_')\n", "    def check_oob_score_error(name):\n        # Test that an error is raised when oob_score is True and bootstrap\n        # is False, for all estimators except for RandomTreesEmbedding\n        ForestEstimator = FOREST_ESTIMATORS[name]\n\n        if name == \"RandomTreesEmbedding\":\n            return\n\n        clf = ForestEstimator(n_estimators=10, oob_score = True,\n                              bootstrap=False, random_state=0)\n        assert_raises(ValueError, clf.fit, X, y)\n", "def test_n_jobs_backend():\n    # Test that joblib backend can be set and used correctly\n    X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)\n\n    # Test default backend\n    est = RandomForestClassifier(n_estimators=5, n_jobs=2)\n    with assert_no_warnings():\n        est.fit(X, y)\n\n    # Test custom backend\n    backend = MyBackend()\n    est = RandomForestClassifier(n_estimators=5, n_jobs=2,\n                               random_state=0, backend=backend)\n    with assert_no_warnings():\n        est.fit(X, y)\n    assert backend.count == est.n_estimators\n\n", "def test_joblib_backend():\n    # Test if the joblib backend is correctly used\n    X, y = hastie_X, hastie_y\n    ForestEstimator = RandomForestClassifier\n\n    backend_instance = MyBackend()\n    with joblib.parallel_backend(backend='my_backend',\n                                 n_jobs=1,\n                                 backend_options={'backend': backend_instance}):\n        est = ForestEstimator(n_estimators=10, random_state=0)\n        est.fit(X, y)\n\n    assert_equal(backend_instance.count, 1)\n"], "sample_784": ["    def test_calibration_error():\n        # Test that calibration raises ValueError on inappropriate input\n        clf = LinearSVC()\n        calibrated_clf = CalibratedClassifierCV(clf, method='isotonic')\n\n        # Test if calibration raises an error with non-binary input\n        X, y = make_classification(n_samples=20, n_classes=3, random_state=42)\n        assert_raises(ValueError, calibrated_clf.fit, X, y)\n\n        # Test if calibration raises an error with non-numeric input\n        X = np.array[['a', 'b'], ['c', 'd']])\n        y = np.array([0, 1])\n        assert_raises(ValueError, calibrated_clf.fit, X, y)\n", "    def test_calibration_with_sparse_data():\n        # Test calibration works with sparse input\n        X, y = make_classification(n_samples=100, n_features=20,\n                                   random_state=42)\n        X_sparse = sparse.csr_matrix(X)\n        clf = LinearSVC(random_state=42)\n        clf_calibrated = CalibratedClassifierCV(clf, method='sigmoid', cv=2)\n        clf_calibrated.fit(X_sparse, y)\n        clf_calibrated.predict_proba(X_sparse)\n", "    def test_calibration_pipeline(self):\n        \"\"\"Test calibration within a Pipeline.\"\"\"\n        X, y = make_classification(n_samples=50, n_features=10, random_state=42)\n\n        pipe = Pipeline([\n            ('clf', LogisticRegression(random_state=42)),\n            ('cal', CalibratedClassifierCV(method='sigmoid', cv=5))\n        ])\n\n        pipe.fit(X, y)\n        y_prob = pipe.predict_proba(X)\n        assert_array_almost_equal(np.sum(y_prob, axis=1), np.ones(len(X)))\n", "    def test_calibration_empty_classes():\n        # Test that calibration handles cases with empty classes during cv\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        y = np.array([0, 1, 1])  # class 0 has only one sample\n        clf = MultinomialNB()\n        \n        clf_c = CalibratedClassifierCV(clf, method='isotonic', cv=2)\n        \n        with pytest.warns(UserWarning):\n            clf_c.fit(X, y)\n", "def test_calibration_with_precomputed_probability():\n    \"\"\"Test calibration with precomputed probability.\"\"\"\n    num_samples = 100\n    X, y = make_classification(n_samples=num_samples, n_features=5,\n                               random_state=42)\n    clf = LinearSVC(C=1.0)\n    clf.fit(X, y)\n    probas = clf.decision_function(X)\n    # Simulate precomputed probabilities, normalize for demonstration\n    # purposes (should not be necessary for CalibratedClassifierCV)\n    probas = (probas - probas.min()) / (probas.max() - probas.min())\n    probas = np.stack((1 - probas, probas), axis=1)\n\n    clf_calibrated = CalibratedClassifierCV(\n        base_estimator=clf, method=\"sigmoid\", cv=5\n    )\n    clf_calibrated.fit(X, y, sample_weight=None)\n    calibrated_probas = clf_calibrated.predict_proba(X)\n\n    assert_greater(\n        brier_score_loss(y, probas[:, 1]), brier_score_loss(y, calibrated_probas[:, 1])\n    )\n\n"], "sample_745": ["def test_lambda_function():\n    X = np.arange(10).reshape((5, 2))\n    F = FunctionTransformer(\n        lambda x: x * 2\n    )\n    assert_array_equal(F.transform(X), X * 2)\n    assert_not_callable(F.func)\n", "def test_sparse_matrix_handling():\n    X_sparse = sparse.csr_matrix(np.arange(10).reshape((5, 2)))\n    F_sparse = FunctionTransformer(np.square, accept_sparse=True)\n\n    X_transformed = F_sparse.transform(X_sparse)\n    assert isinstance(X_transformed, sparse.csr_matrix)\n\n    F_dense = FunctionTransformer(np.square)\n    with pytest.raises(TypeError):\n        F_dense.transform(X_sparse)\n", "def test_lambda_function():\n    X = np.arange(10).reshape((5, 2))\n\n    # Test that a lambda function can be used\n    F = FunctionTransformer(lambda x: x + 1)\n    assert_array_equal(F.transform(X), X + 1)\n\n    # Test that the transformer is not pickleable\n    import pickle\n    with pytest.raises(AttributeError):\n        pickle.dumps(F)\n\n", "def test_sparse_input():\n    X_sparse = sparse.csr_matrix(np.arange(10).reshape((5, 2)))\n    transformer = FunctionTransformer(np.square, accept_sparse=True)\n\n    assert_sparse_matrix_equal(transformer.transform(X_sparse),\n                               sparse.csr_matrix(np.square(X_sparse.toarray())))\n\n    transformer = FunctionTransformer(np.square)\n    with pytest.raises(TypeError):\n        transformer.transform(X_sparse)\n", "def test_sparse_matrices():\n    X_sparse = sparse.csr_matrix(np.array([1, 4, 9, 16]).reshape((2, 2)))\n    transformer = FunctionTransformer(func=np.sqrt, accept_sparse=True)\n    Xt = transformer.fit_transform(X_sparse)\n    assert isinstance(Xt, sparse.csr_matrix)\n    assert_allclose_dense_sparse(transformer.inverse_transform(Xt), X_sparse)\n\n"], "sample_645": ["def test_log_file_level(pytester: Pytester, tmpdir) -> None:\n    log_file = tmpdir / \"pytest.log\"\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n        logger = logging.getLogger(__name__)\n            logger.warning(\"This is a warning\")\n            logger.info(\"This is an info\")\n    \"\"\"\n    )\n    pytester.makeini(\n        f\"\"\"\n        [pytest]\n        log_file = {log_file}\n        log_file_level = WARNING\n    \"\"\"\n    )\n\n    result = pytester.runpytest()\n    assert result.ret == 0\n    assert log_file.read() == \"WARNING root: This is a warning\\n\"\n", "def test_log_records_access(caplog):\n    caplog.set_level(logging.INFO)\n    logger.info(\"boo %s\", \"arg\")\n    assert caplog.records[0].levelname == \"INFO\"\n    assert caplog.records[0].msg == \"boo %s\"\n    assert caplog.records[0].args == (\"arg\",)\n\n\n\n", "def test_teardown_capturing(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n    import pytest\n    import logging\n        logger = logging.getLogger(__name__)\n        logger.info(\"message before teardown\")\n        assert caplog.records\n        yield\n        logger.info(\"message after teardown\")\n        assert not caplog.records\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    assert result.ret == 0\n", "    def test_caplog_captures_despite_exception_with_set_level(pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n            import logging\n                logger = logging.getLogger('catchlog')\n                caplog.set_level(logging.DEBUG, logger.name)\n\n                logger.error(\"ERROR message \" + \"will be shown\")\n\n                try:\n                    logger.debug(\"DEBUG message \" + \"will be shown\")\n                    raise Exception()\n                except Exception:\n                    logger.warning(\"WARNING message \" + \"will be shown\")\n        \"\"\"\n        )\n        result = pytester.runpytest()\n        result.stdout.fnmatch_lines([\"*ERROR message will be shown*\", \"*DEBUG message will be shown*\", \"*WARNING message will be shown*\"])\n        assert result.ret == 1\n\n\n\n", "    def test_caplog_with_capture_manager(pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n            import logging\n\n            @pytest.fixture\n                return \"fixture value\"\n\n                logger = logging.getLogger('catchlog')\n                logger.info(f\"info log with fixture: {my_fixture}\")\n                assert \"fixture value\" in caplog.text\n        \"\"\"\n        )\n\n        result = pytester.runpytest()\n        assert result.ret == 0\n"], "sample_58": ["    def test_attribute_inheritance(self):\n        class BaseForm(Form):\n            default_renderer = DjangoTemplates()\n\n        class CustomForm(BaseForm):\n            pass\n\n        form = CustomForm()\n        self.assertEqual(form.renderer, BaseForm.default_renderer)\n\n", "    def test_attribute_callable(self):\n        class CustomForm(Form):\n                return CustomRenderer()\n            default_renderer = set_renderer\n\n        form = CustomForm()\n        self.assertTrue(isinstance(form.renderer, CustomRenderer))\n", "    def test_hidden_field_in_form_with_errors(self):\n        class MyForm(Form):\n            name = CharField(max_length=10)\n            hidden_field = CharField(widget=HiddenInput)\n\n        form = MyForm({'name': 'John', 'hidden_field': 'secret'})\n        form.full_clean()\n        self.assertEqual(form.errors, {'name': ['This field is required.']})\n        self.assertHTMLEqual(\n            form.as_p(),\n            '<p><ul class=\"errorlist\"><li>This field is required.</li></ul>'\n            '<label for=\"id_name\">Name:</label> <input type=\"text\" name=\"name\" value=\"John\" id=\"id_name\" required>'\n            '<input type=\"hidden\" name=\"hidden_field\" value=\"secret\" id=\"id_hidden_field\"></p>'\n        )\n\n", "    def test_attribute_class_inheritance(self):\n        class BaseForm(Form):\n            default_renderer = CustomRenderer\n\n        class DerivedForm(BaseForm):\n            pass\n\n        form = DerivedForm()\n        self.assertTrue(isinstance(form.renderer, CustomRenderer))\n", "    def test_attribute_instance_override(self):\n        class CustomForm(Form):\n            default_renderer = DjangoTemplates()\n\n        form = CustomForm(renderer=CustomRenderer())\n        self.assertTrue(isinstance(form.renderer, CustomRenderer))\n\n"], "sample_353": ["    def test_createsuperuser_command_with_user_model_no_permissions_field(self):\n        \"\"\"\n        Test createsuperuser command works with a custom user model\n        that doesn't have a permissions field.\n        \"\"\"\n        @mock_inputs({'password': 'nopasswd', 'username': 'testuser', 'email': 'test@example.com'})\n            call_command(\n                'createsuperuser',\n                interactive=True,\n                stdin=MockTTY(),\n                verbosity=0,\n                app_label='auth_tests',\n                model='CustomUserWithoutPermissions',\n            )\n            user = CustomUserWithoutPermissions._default_manager.get(username='testuser')\n            self.assertTrue(user.is_superuser)\n\n        test(self)\n", "    def test_createsuperuser_command_with_custom_user_model_and_no_password(self):\n        # Test that createsuperuser works with a custom user model that doesn't have a password field\n        @override_settings(AUTH_USER_MODEL='auth_tests.NoPasswordUser')\n            new_io = StringIO()\n            call_command(\n                'createsuperuser',\n                interactive=False,\n                username='testuser',\n                email='test@example.com',\n                stdout=new_io,\n            )\n            command_output = new_io.getvalue().strip()\n            self.assertEqual(command_output, 'Superuser created successfully.')\n            user = get_user_model().objects.get(username='testuser')\n            self.assertEqual(user.email, 'test@example.com')\n\n        test(self)\n", "    def test_createsuperuser_interactive_with_existing_user(self):\n        User.objects.create_superuser('joe', 'joe@example.com', 'password')\n        self.assertEqual(User.objects.count(), 1)\n\n            call_command(\n                'createsuperuser',\n                interactive=True,\n                stdin=MockTTY(),\n                stdout=StringIO(),\n                stderr=StringIO(),\n            )\n\n        with self.assertRaisesMessage(CommandError, 'Error: That username is already taken.'):\n            test(self)\n        self.assertEqual(User.objects.count(), 1)\n", "    def test_create_permissions_database_router(self):\n        \"\"\"\n        Permissions should be created for the database defined by the router.\n\n        This test requires a database router to be configured. It assumes a\n        'custom' database and checks that permissions are created on that\n        database.\n\n        \"\"\"\n        router = get_router()\n        if not router:\n            return  # Skip test if no router configured\n\n        if 'custom' not in router.db_for_read(None, **{'model': 'auth.permission'}):\n            self.skipTest(\"No 'custom' database configured for the router.\")\n        Permission._meta.permissions = [\n            ('my_custom_permission', 'Some permission'),\n        ]\n        create_permissions(self.app_config, verbosity=0)\n\n        # Check if permissions exist in the 'custom' database\n        self.assertEqual(Permission.objects.using('custom').filter(\n            codename='my_custom_permission',\n        ).count(), 1)\n\n\n\n", "    def test_createsuperuser_command_with_custom_user_model_and_email(self):\n        @override_settings(AUTH_USER_MODEL='auth_tests.CustomUserWithEmail')\n            new_io = StringIO()\n            email = 'test@example.com'\n            call_command(\n                'createsuperuser',\n                interactive=False,\n                username='test_user',\n                email=email,\n                stdout=new_io,\n            )\n            command_output = new_io.getvalue().strip()\n            self.assertEqual(command_output, 'Superuser created successfully.')\n            user = CustomUserWithEmail._default_manager.get(username='test_user')\n            self.assertEqual(user.email, email)\n        test(self)\n\n"], "sample_62": ["    def test_unregister_model(self):\n        self.site.register(Person)\n        self.assertTrue(self.site.is_registered(Person))\n        self.site.unregister(Person)\n        self.assertFalse(self.site.is_registered(Person))\n", "    def test_unregister(self):\n        self.site.register(Person, NameAdmin)\n        self.site.unregister(Person)\n        with self.assertRaises(admin.sites.NotRegistered):\n            self.site._registry[Person]\n\n", "    def test_register_with_empty_name(self):\n        class EmptyNameAdmin(admin.ModelAdmin):\n            pass\n        with self.assertRaisesMessage(ValueError, 'ModelAdmin class must have a name'):\n            register('')(EmptyNameAdmin)\n", "    def test_unregister(self):\n        self.site.register(Person)\n        self.assertTrue(self.site.is_registered(Person))\n        self.site.unregister(Person)\n        self.assertFalse(self.site.is_registered(Person))\n", "    def test_register_with_callable(self):\n            return NameAdmin\n        register(Person)(get_admin_class)\n        self.assertIsInstance(self.default_site._registry[Person], admin.ModelAdmin)\n        self.default_site.unregister(Person)\n"], "sample_876": ["def test_mlp_regressor_partial_fit_with_validation_data():\n    # Test partial_fit on MLPRegressor with validation data.\n    X = X_reg[:50]\n    y = y_reg[:50]\n    X_val = X_reg[50:100]\n    y_val = y_reg[50:100]\n    mlp = MLPRegressor(\n        solver=\"sgd\",\n        hidden_layer_sizes=(5,),\n        max_iter=100,\n        validation_fraction=0.2,\n        random_state=1,\n        batch_size=X.shape[0],\n    )\n\n    with warnings.catch_warnings(record=True):\n        # catch convergence warning\n        mlp.fit(X, y)\n    assert mlp.validation_scores_ is not None\n    assert len(mlp.validation_scores_) > 0\n\n    mlp = MLPRegressor(\n        solver=\"sgd\",\n        hidden_layer_sizes=(5,),\n        max_iter=100,\n        random_state=1,\n        batch_size=X.shape[0],\n    )\n    for i in range(10):\n        mlp.partial_fit(X, y)\n\n    assert mlp.validation_scores_ is None\n\n    mlp = MLPRegressor(\n        solver=\"sgd\",\n        hidden_layer_sizes=(5,),\n        max_iter=100,\n        validation_fraction=0.2,\n        random_state=1,\n        batch_size=X.shape[0],\n    )\n    for i in range(10):\n        mlp.partial_fit(X, y)\n    assert mlp.validation_scores_ is not None\n    assert len(mlp.validation_scores_) > 0\n\n", "def test_mlp_sample_weights():\n    # Test that sample_weights are correctly used in fitting\n\n    X = X_digits[:100]\n    y = y_digits[:100]\n    sample_weights = np.ones(100)\n    sample_weights[::2] = 2\n\n    mlp = MLPClassifier(hidden_layer_sizes=5, random_state=1)\n    mlp.fit(X, y, sample_weight=sample_weights)\n    pred = mlp.predict(X)\n\n    # Compare to a baseline model without sample weights\n    mlp_no_weights = MLPClassifier(hidden_layer_sizes=5, random_state=1)\n    mlp_no_weights.fit(X, y)\n    pred_no_weights = mlp_no_weights.predict(X)\n\n    assert not np.array_equal(pred, pred_no_weights)\n\n", "    def test_mlp_warm_start_invalid_input(MLPEstimator):\n        mlp = MLPEstimator(warm_start=True)\n        mlp.fit(X_iris[:10], y_iris[:10])\n        with pytest.raises(ValueError):\n            mlp.fit(X_digits[:10], y_digits[:10])\n", "compilation error", "compilation error"], "sample_1205": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_991": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_346": ["    def test_cache_page(self):\n        @cache_page(60 * 15)\n            return HttpResponse()\n        r = a_view(HttpRequest())\n        self.assertEqual(r.get('Cache-Control'), 'max-age=900')\n\n        # Check that the cached response doesn't have ETag\n        self.assertIsNone(r.get('ETag'))\n", "    def test_cache_control_decorator_with_dict(self):\n        @cache_control({'a':'b', 'c':'d'})\n            return HttpResponse()\n        r = a_view(HttpRequest())\n        self.assertEqual(r.headers['Cache-Control'], 'a=\"b\", c=\"d\"')\n", "    def test_cache_control_decorator_with_request(self):\n        @cache_control(private=True)\n            return HttpResponse()\n        req = HttpRequest()\n        resp = a_view(req)\n        self.assertEqual(resp.headers['Cache-Control'], 'private')\n", "    def test_cache_control_decorator_no_http_request(self):\n        @cache_control(max_age=100)\n            return HttpResponse()\n        r = a_view()\n        self.assertEqual(r.headers['Cache-Control'], 'max-age=100')\n\n", "    def test_cache_control_decorator_with_request(self):\n        @cache_control(public=True, max_age=3600)\n            return HttpResponse()\n        r = a_view(HttpRequest())\n        self.assertEqual(r.headers['Cache-Control'], 'public, max-age=3600')\n"], "sample_350": ["    def test_difference_with_distinct(self):\n        qs1 = Number.objects.all()\n        qs2 = Number.objects.all()\n        self.assertEqual(len(qs1.difference(qs2, distinct=False)), 0)\n        self.assertEqual(len(qs1.difference(qs2, distinct=True)), 0)\n", "    def test_union_with_none(self):\n        qs1 = Number.objects.filter(num=1)\n        qs2 = None\n        self.assertNumbersEqual(qs1.union(qs2), [1])\n", "    def test_union_with_values_list_and_order_by_f_expression(self):\n\n        qs1 = Number.objects.filter(num__gte=6).values_list('num', flat=True)\n        qs2 = Number.objects.filter(num__lte=5).values_list('num', flat=True)\n        self.assertSequenceEqual(\n            qs1.union(qs2).order_by(F('num').desc()).values_list('num', flat=True),\n            [9, 8, 7, 6, 5, 4, 3, 2, 1, 0],\n        )\n\n", "    def test_union_with_empty_qs_and_different_models(self):\n        qs1 = Number.objects.all()\n        qs2 = Celebrity.objects.none()\n        self.assertEqual(len(qs1.union(qs2)), 10)\n        self.assertEqual(len(qs2.union(qs1)), 10)\n", "    def test_union_with_empty_qs_and_filters(self):\n        qs1 = Number.objects.filter(num__lte=1)\n        qs2 = Number.objects.none().filter(num__gt=2)\n        self.assertEqual(len(list(qs1.union(qs2))), 2)\n"], "sample_110": ["    def test_pickle_queryset_with_sliced_subquery(self):\n        group = Group.objects.create(name='group')\n        Event.objects.create(title='event', group=group)\n        subquery = Event.objects.filter(group=group)[:1]\n        groups = Group.objects.filter(event__in=subquery)\n        groups2 = pickle.loads(pickle.dumps(groups))\n        self.assertSequenceEqual(groups2, [group])\n\n", "    def test_pickle_annotation_with_complex_conditional_expression(self):\n        from django.db.models import F, Q\n\n        qs = Group.objects.annotate(\n            has_event_with_specific_title=\n            Q(event__title__icontains='a') &\n            Q(event__title__isnull=False)\n        ).filter(has_event_with_specific_title=True)\n\n        self.assert_pickles(qs)\n\n\n\n", "    def test_pickle_queryset_with_aggregate(self):\n        group = Group.objects.create(name='group')\n        Event.objects.create(title='event', group=group)\n        qs = Group.objects.annotate(event_count=models.Count('event_set')).filter(event_count__gt=0)\n        self.assert_pickles(qs)\n", "    def test_pickle_select_related_with_subquery(self):\n        group = Group.objects.create(name='group')\n        Event.objects.create(title='event', group=group)\n        qs = Group.objects.select_related('event_set__group').annotate(\n            event_title=models.Subquery(\n                Event.objects.filter(group_id=models.OuterRef('id')).values('title'),\n            ),\n        )\n        self.assert_pickles(qs)\n", "    def test_pickle_prefetch_related_with_empty_queryset(self):\n        group = Group.objects.create(name='group')\n        Event.objects.create(title='event')\n        groups = Group.objects.prefetch_related('event_set').filter(id__gt=1)\n        groups_from_pickle = pickle.loads(pickle.dumps(groups))\n        self.assertSequenceEqual(groups_from_pickle, [])\n"], "sample_593": ["def test_summarize_coord_array_with_unsafe_name(dataset):\n   coord = dataset.coords[\"<IA>\"].to_index_variable()\n   formatted = fh.summarize_variable(\"<IA>\", coord)\n   assert \"&lt;IA&gt;\" in formatted\n", "def test_summarize_variable_with_unsafe_name():\n    var = xr.Variable([\"<x>\", \"y\"], [1, 2])\n    formatted = fh.summarize_variable(\"<x>\", var)\n    assert \"&lt;x&gt;\" in formatted\n", "def test_summarize_coord_multiindex_coord_with_none_level_names(multiindex):\n    idx = multiindex.x.variable.to_index_variable()\n    idx.level_names = None\n    formatted = fh._summarize_coord_multiindex(\"foo\", idx)\n    assert \"(None)\" in formatted\n    assert \"MultiIndex\" in formatted\n    assert \"<span class='xr-has-index'>foo</span>\" in formatted\n", "def test_summarize_variable_dtype(dataarray):\n    dataarray = dataarray.astype(\"int32\")\n    formatted = fh.summarize_variable(\"var\", dataarray, dtype=\"int32\")\n    assert \"int32\" in formatted\n", "def test_summarize_variable_dtype_with_unsafe_characters():\n    var = xr.Variable(\n        [\"x\"], np.array([1, 2, 3]), {\"dtype\": \"<U4\"}, attrs={\"foo\": \"<bar>\"}\n    )\n    formatted = fh.summarize_variable(\"x\", var)\n    assert '&lt;U4' in formatted\n    assert '&lt;bar&gt;' in formatted\n"], "sample_598": ["    def test_format_item_with_numpy_scalar():\n        cases = [\n            (np.float64(1), \"1.0\"),\n            (np.int32(1), \"1\"),\n            (np.complex128(1 + 2j), \"1+2j\"),\n        ]\n        for item, expected in cases:\n            actual = formatting.format_item(item)\n            assert expected == actual\n\n", "    def test_limit_lines():\n\n        long_string = \"This is a very long string that needs to be truncated\\n\" * 50\n\n        result = formatting.limit_lines(long_string, limit=2)\n        expected = \"This is a very long string that needs to be truncated\\nThis is a very long string that needs to be truncated\\n...\"\n        assert result == expected\n", "compilation error", "    def test_format_item_datetime64_with_timezone(self):\n        dt = np.datetime64(\"2023-10-26T12:00:00-0500\")\n        expected = \"2023-10-26T12:00:00-05:00\"\n        actual = formatting.format_item(dt)\n        assert actual == expected\n", "    def test_format_array_flat_empty():\n        actual = formatting.format_array_flat(np.array([]), 5)\n        expected = \"\"\n        assert actual == expected\n"], "sample_373": ["    def test_view_detail_with_middleware(self):\n        # Make sure middleware is accounted for in the view detail.\n        response = self.client.get(reverse('django-admindocs-views-detail', args=['django.contrib.auth.views.login']))\n        self.assertContains(response, 'MIDDLEWARE')\n", "    def test_model_detail_with_proxy(self):\n        response = self.client.get(reverse('django-admindocs-models-detail', args=['admin_docs', 'ProxyPerson']))\n        self.assertContains(response, '<h1>admin_docs.ProxyPerson</h1>', html=True)\n        self.assertContains(response, 'Proxy model for Person')\n", "    def test_model_with_abstract_base_class(self):\n        response = self.client.get(reverse('django-admindocs-models-detail', args=['admin_docs', 'abstractbaseclass']))\n        self.assertContains(response, '<h1>admin_docs.AbstractBaseClass</h1>', html=True)\n        self.assertContains(response, '<h2 id=\"fields\">Fields</h2>', html=True)\n", "    def test_model_with_custom_fields(self):\n        response = self.client.get(reverse('django-admindocs-models-detail', args=['admin_docs', 'Family']))\n        self.assertContains(response, 'description=&#x27;A custom field type&#x27;')\n", "    def test_simplify_regex(self):\n        self.assertEqual(simplify_regex('/^company/(?P<company_id>\\d+)/$/'), '/company/<company_id>/')\n        self.assertEqual(simplify_regex('/^company\\/(?P<company_id>\\d+)\\/$/'), '/company/<company_id>/')\n        self.assertEqual(simplify_regex('/articles/(?P<year>\\d{4})/(?P<month>\\d{2})/(?P<day>\\d{2})/(?P<slug>[\\w-]+)/'), '/articles/<year>/<month>/<day>/<slug>/')\n"], "sample_667": ["    def test_tmpdir_factory_scope(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n            @pytest.fixture(scope='session')\n                return tmpdir_factory.mktemp('data', numbered=False)\n                assert session_dir.isdir()\n                assert session_dir.isdir()\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.assertoutcome(passed=2)\n", "def test_tmpdir_factory_with_existing_basetemp(testdir):\n    \"\"\"ensure that providing an existing directory as basetemp works\"\"\"\n    existing_dir = testdir.tmpdir.mkdir(\"existing_basetemp\")\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n            tmpdir_factory.getbasetemp().ensure(\"hello\")\n    \"\"\"\n    )\n    testdir.runpytest(p, \"--basetemp=\" + str(existing_dir))\n    existing_dir.join(\"hello\").check()\n", "def test_tmpdir_fixture_twice(testdir):\n    p = testdir.copy_example(\"tmpdir/tmpdir_fixture.py\")\n    results = testdir.runpytest(p)\n    assert results.ret == 0\n    results = testdir.runpytest(p)\n    assert results.ret == 0\n\n", "    def test_tmpdir_factory_session_fixture_different_basetemp(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n            @pytest.fixture(scope='session')\n                return tmpdir_factory.mktemp('data', numbered=False)\n\n                assert session_dir.isdir()\n        \"\"\"\n        )\n        result = testdir.runpytest(\"--basetemp=othertemp\")\n        result.assertoutcome(passed=1)\n", "    def test_tmpdir_factory_with_custom_basetemp(testdir, monkeypatch):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n            @pytest.fixture(scope='session')\n                return tmpdir_factory.mktemp('data', numbered=False)\n                assert session_dir.isdir()\n        \"\"\"\n        )\n        monkeypatch.setenv(\"PYTEST_DEBUG_TEMPROOT\", str(testdir.tmpdir))\n        reprec = testdir.inline_run()\n        reprec.assertoutcome(passed=1)\n\n"], "sample_1201": ["def test_cgs_gauss_conversions():\n    assert convert_to(statvolt, volt, cgs_gauss) == 10**(-6)*speed_of_light*volt\n    assert convert_to(volt, statvolt, cgs_gauss) == volt/(10**(-6)*speed_of_light)\n    assert convert_to(tesla, gauss, cgs_gauss) == 10**(-4)*gauss\n    assert convert_to(gauss, tesla, cgs_gauss) == 10**(4)*tesla\n", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_206": ["    def test_file_object_unchanged(self):\n        d = Document.objects.create(myfile='something.txt')\n        self.assertEqual(d.myfile.file, None)\n\n", "    def test_file_descriptor_initializes_correctly(self):\n        document = Document.objects.create(myfile='something.txt')\n        document_field = document._meta.get_field('myfile')\n        self.assertIsNotNone(document_field.descriptor.instance)\n        self.assertEqual(document_field.descriptor.instance, document)\n", "    def test_file_field_descriptor(self):\n        d = Document()\n        # Accessing the attribute off the instance should return a FieldFile\n        self.assertIsInstance(d.myfile, Document.myfile.attr_class)\n\n", "    def test_custom_storage(self):\n        class MockStorage(object):\n                self.base_url = base_url\n                return os.path.join(self.base_url, name)\n                return os.path.join(self.base_url, name)\n        # Use a mock storage backend\n        custom_storage = MockStorage('/custom-storage')\n        Document.objects.create(myfile='something.txt', storage=custom_storage)\n\n        # Assert that the url is correctly generated using custom storage\n        d = Document.objects.get(myfile='something.txt')\n        self.assertEqual(d.myfile.url, '/custom-storage/something.txt')\n", "    def test_file_closed_on_instance_deletion(self):\n        d = Document.objects.create(myfile='something.txt')\n        self.assertFalse(d.myfile.closed)\n        d.delete()\n        self.assertTrue(d.myfile.closed)\n"], "sample_307": ["    def test_z_format_year_before_1000(self):\n        self.assertEqual(dateformat.format(datetime(1, 1, 1), 'z'), '0')\n        self.assertEqual(dateformat.format(datetime(999, 12, 31), 'z'), '364')\n\n\n", "    def test_leap_year_february(self):\n        self.assertEqual(dateformat.format(datetime(2024, 2, 29), 'j'), '29')\n        self.assertEqual(dateformat.format(datetime(2023, 2, 28), 'j'), '28')\n", "    def test_z_format_year_before_1000(self):\n        self.assertEqual(dateformat.format(datetime(1, 1, 1), 'z'), '1')\n        self.assertEqual(dateformat.format(datetime(999, 12, 31), 'z'), '365')\n\n", "    def test_timezone_aware_datetime_with_dst(self):\n        dt = datetime(2009, 5, 16, 5, 30, 30, tzinfo=get_fixed_timezone(-240))\n        self.assertEqual(dateformat.format(dt, 'Z'), '-14400')\n", "    def test_format_with_incorrect_timezone(self):\n        dt = datetime(2023, 10, 26, 10, 0, 0)\n        with self.assertRaises(ValueError):\n            dateformat.format(dt, 'T', timezone='invalid/timezone')\n"], "sample_272": ["    def test_minimize_rollbacks_circular(self):\n        \"\"\"\n        Minimize rollbacks when target is part of a circular dependency.\n\n        a: 1 <-> b: 1\n                      ^\n                      |\n                      2\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        b2_impl = FakeMigration('b2')\n        b2 = ('b', '2')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_node(b2, b2_impl)\n        graph.add_dependency(None, b1, a1)\n        graph.add_dependency(None, a1, b1)\n        graph.add_dependency(None, b2, b1)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            b1: b1_impl,\n            b2: b2_impl,\n        })\n\n        plan = executor.migration_plan({a1})\n\n        should_be_rolled_back = [b2_impl, b1_impl]\n        exp = [(m, True) for m in should_be_rolled_back]\n        self.assertEqual(plan, exp)\n", "    def test_minimize_rollbacks_empty_target(self):\n        r\"\"\"\n        Minimize rollbacks when target has no in-app migrations\n        \n        a: 1 <---- 3\n        b:\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a3_impl = FakeMigration('a3')\n        a3 = ('a', '3')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a3, a3_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_dependency(None, a3, a1)\n        graph.add_dependency(None, b1, a1)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            a3: a3_impl,\n            b1: b1_impl,\n        })\n\n        plan = executor.migration_plan({'b'})\n\n        self.assertEqual(plan, [])\n", "    def test_detect_soft_applied_inconsistent_dependencies(self):\n        \"\"\"\n        Detection of soft applied migration should consider inconsistent dependencies\n\n        Scenario:\n        - Migration A depends on Migration B\n        - Migration A is soft-applied but Migration B isn't\n\n        Expected behavior:\n        - Detect_soft_applied should return False for Migration A as dependencies aren't met\n\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_dependency(b1, a1)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {b1: b1_impl})\n\n        self.assertFalse(executor.detect_soft_applied(None, a1_impl)[0])\n", "    def test_unapplied_dependent_migrations_are_rolled_back(self):\n        \"\"\"\n        Ensure unapplied dependent migrations are rolled back when a migration is\n        unapplied.\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_dependency(None, b1, a1)\n        graph.add_dependency(None, a2, a1)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            b1: b1_impl,\n        })\n\n        # Unapply a1, which should also unapply b1 (its dependent)\n        plan = executor.migration_plan({a1}, backwards=True)\n        self.assertEqual(plan, [(a2_impl, True), (b1_impl, True), (a1_impl, True)])\n", "    def test_minimize_rollbacks_multiple_apps_same_migration(self):\n        r\"\"\"\n        Minimize rollbacks when target application has multiple migrations\n        at the same level.\n\n        a: 1 <--- 2\n        b: 1 <--- 2\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        b2_impl = FakeMigration('b2')\n        b2 = ('b', '2')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_node(b2, b2_impl)\n        graph.add_dependency(None, a2, a1)\n        graph.add_dependency(None, b2, b1)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            b1: b1_impl,\n            a2: a2_impl,\n            b2: b2_impl,\n        })\n\n        plan = executor.migration_plan({a1, b1})\n\n        self.assertEqual(plan, [(a2_impl, True), (b2_impl, True)])\n\n"], "sample_1038": ["compilation error", "def test_matpow_negative():\n    A = MatrixSymbol('A', 2, 2)\n    raises(ValueError, lambda: A**-3)\n\n", "def test_trace():\n    A = MatrixSymbol('A', 2, 2)\n    assert A.trace() == A[0, 0] + A[1, 1]\n    assert (2*A).trace() == 2*A.trace()\n    assert (A + A).trace() == 2*A.trace()\n    assert (A.T).trace() == A.trace()\n    assert (A**2).trace() == A.trace()**2\n\n", "compilation error", "def test_issue_12135():\n    A = MatrixSymbol(\"A\", 2, 2)\n    B = MatrixSymbol(\"B\", 2, 2)\n    assert (A + B).T == A.T + B.T\n"], "sample_695": ["def test_node_get_closest_marker_default(pytester: Pytester) -> None:\n    item = pytester.getnode(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    assert item.get_closest_marker(\"something\", default=\"default_marker\") == \"default_marker\"\n", "def test_node_getparent_returns_none_when_not_found(pytester: Pytester) -> None:\n    item = pytester.getitem(\"\"\"\n            pass\n    \"\"\")\n    assert item.getparent(nodes.Session) is None\n", "def test_node_repr_failure_custom_exception(pytester: Pytester) -> None:\n    \"\"\"Test Repr failure for custom exception.\"\"\"\n    class MyException(Exception):\n        pass\n\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            raise MyException(\"Something bad happened!\")\n    \"\"\"\n    )\n    result = pytester.runpytest('-s')\n    assert result.ret == 1\n    result.stdout.fnmatch_lines(\n        [\"*fixture_raising_exception*\", \"*MyException: Something bad happened!*\"]\n    )\n", "def test_node_keywords_extra_keyword_matches(pytester: Pytester) -> None:\n    items = pytester.getitems(\n        \"\"\"\n            request.node.add_marker(\"extra\")\n            request.node.extra_keyword_matches.add(\"extra\")\n    \"\"\"\n    )\n    assert items[0].keywords.get(\"extra\") is not None\n    assert \"extra\" in items[0].listextrakeywords()\n\n", "compilation error"], "sample_280": ["    def test_aggregation_default_using_integerfield_from_python(self):\n        result = Publisher.objects.filter(num_awards__gt=3).aggregate(\n            value=Sum('num_awards', default=0),\n        )\n        self.assertEqual(result['value'], 0)\n", "    def test_aggregation_default_unsupported_by_count_distinct(self):\n        msg = 'Count distinct does not allow default.'\n        with self.assertRaisesMessage(TypeError, msg):\n            Count('age', distinct=True, default=0)\n\n", "    def test_nested_subquery_with_aggregate_and_annotate(self):\n        qs = Book.objects.annotate(\n            avg_price_by_publisher=Subquery(\n                Book.objects.filter(\n                    publisher=OuterRef('publisher'),\n                ).values('publisher').annotate(avg_price=Avg('price'))\n                .values('avg_price')[:1],\n                output_field=DecimalField(),\n            ),\n        )\n        self.assertSequenceEqual(\n            qs.values_list('name', 'avg_price_by_publisher'),\n            [\n                ('Practical Django Projects', Decimal('19.99')),\n                ('The Definitive Guide to Django: Web Development Done Right', Decimal('39.99')),\n                ('Sams Teach Yourself Django in 24 Hours', Decimal('29.99')),\n                ('Python Web Development with Django', Decimal('29.99')),\n                ('Django for Beginners', Decimal('29.69')),\n                ('Introduction to Django', Decimal('29.99')),\n            ],\n        )\n", "    def test_coalesce_empty_result_set_with_filter(self):\n        with self.assertNumQueries(0):\n            self.assertEqual(\n                Book.objects.none().filter(\n                    rating=OuterRef('publisher__num_awards')\n                ).aggregate(\n                    sum_awards=Coalesce(Sum('rating'), 0),\n                )['sum_awards'],\n                0,\n            )\n", "    def test_aggregation_default_using_integer_field_from_database(self):\n        result = Book.objects.filter(rating__lt=3.0).aggregate(\n            value=Sum('pages', default=Now().year),\n        )\n        self.assertEqual(result['value'], datetime.datetime.now().year)\n\n"], "sample_525": ["    def test_draw_without_rendering(fig_test, fig_ref):\n        fig_test.add_subplot(111).plot([1, 2, 3], [4, 5, 6])\n        fig_test.draw_without_rendering()\n        fig_ref.add_subplot(111).plot([1, 2, 3], [4, 5, 6])\n        fig_ref.draw_without_rendering()\n\n", "def test_subfigure_projection():\n    fig = plt.figure()\n    gs = fig.add_gridspec(1, 2)\n    ax0 = fig.add_subplot(gs[0, 0], projection='polar')\n    ax0.plot(np.arange(0, 2 * np.pi, 0.01), np.sin(np.arange(0, 2 * np.pi, 0.01)))\n    ax1 = fig.add_subplot(gs[0, 1])\n    ax1.plot(np.arange(0, 10, 0.1), np.sin(np.arange(0, 10, 0.1)))\n", "    def test_add_subplot_projection_collision():\n        fig, ax = plt.subplots(projection='3d')\n        with pytest.raises(ValueError, match='Projection already set'):\n            fig.add_subplot(111, projection='polar')\n", "compilation error", "def test_figure_facecolor_axes_facecolor():\n    fig_test, ax_test = plt.subplots(facecolor='lightgrey')\n\n    fig_ref, ax_ref = plt.subplots()\n    ax_ref.set_facecolor('lightgrey')\n\n    ax_test.plot([0, 1], [2, 3])\n    ax_ref.plot([0, 1], [2, 3])\n"], "sample_829": ["    def test_incremental_pca_partial_fit_stability():\n        # Test that IncrementalPCA is stable over multiple partial_fits\n        rng = np.random.RandomState(1999)\n        n_samples = 100\n        n_features = 20\n        X = rng.randn(n_samples, n_features)\n        ipca = IncrementalPCA(n_components=10)\n        ipca.partial_fit(X[:50, :])\n        ipca.partial_fit(X[50:, :])\n        components_1 = ipca.components_\n\n        ipca = IncrementalPCA(n_components=10)\n        ipca.partial_fit(X)\n        components_2 = ipca.components_\n\n        assert_almost_equal(components_1, components_2, decimal=3)\n", "    def test_incremental_pca_partial_fit_with_whiten(self):", "    def test_incremental_pca_get_covariance(self):\n        # Test that get_covariance outputs the correct covariance matrix.\n        rng = np.random.RandomState(1999)\n        n_samples = 100\n        n_features = 3\n        X = rng.randn(n_samples, n_features)\n\n        ipca = IncrementalPCA(n_components=2, batch_size=25).fit(X)\n        cov = ipca.get_covariance()\n        assert_almost_equal(cov, np.cov(X.T), decimal=3)\n", "    def test_incremental_pca_partial_fit_update():\n        # Test IncrementalPCA updates correctly after calls to partial_fit\n        rng = np.random.RandomState(1999)\n        n_samples, n_features = 50, 3\n        X = rng.randn(n_samples, n_features)\n        X[:, 1] *= .00001  # make middle component relatively small\n\n        # Fit with all data, get initial components\n        ipca = IncrementalPCA(n_components=2, batch_size=10)\n        ipca.fit(X)\n        initial_components = ipca.components_\n\n        # Partially fit with new data, ensure components are updated\n        X_new = rng.randn(10, n_features)\n        ipca.partial_fit(X_new)\n        updated_components = ipca.components_\n\n        # Components should have changed after partial_fit\n        assert not np.allclose(initial_components, updated_components)\n", "    def test_partial_fit_with_precomputed_components(self):\n        # Test that partial_fit works with precomputed components.\n\n        rng = np.random.RandomState(0)\n        n_samples = 100\n        n_features = 20\n        X = rng.randn(n_samples, n_features)\n\n        ipca = IncrementalPCA(n_components=5, batch_size=10)\n        ipca.fit(X[:50])\n\n        # Manually compute the first SVD components.\n        U, S, V = np.linalg.svd(X[:50].T, full_matrices=False)\n        precomputed_components = V[:5]\n\n        ipca.partial_fit(X[50:], components=precomputed_components)\n        # Ensure the components are updated correctly.\n        assert_almost_equal(ipca.components_, V, decimal=3)\n"], "sample_604": ["def test_pretty_print_utf8():\n    assert formatting.pretty_print(\"\u00df\"*10, 10) == \"\u00df\u00df\u00df\u00df\u00df\u00df...\"\n", "    def test_dim_summary_empty_dims(self):\n        data = xr.Dataset()\n        expected = \"()\"\n        result = formatting.dim_summary(data)\n        assert result == expected\n", "    def test_summarize_coord(self):\n        coord = xr.DataArray([1, 2, 3], dims='x', attrs={'desc': 'coordinate'})\n        expected = \"    x: [1 2 3] (desc: coordinate)\"\n        assert formatting.summarize_coord('x', coord) == expected\n\n", "    def test_format_items_empty_array():\n        array = np.array([], dtype=\"int64\")\n        actual = formatting.format_items(array)\n        expected = []\n        assert actual == expected\n", "    def test_short_data_repr_empty_array(self):\n        da = xr.DataArray(np.empty((0, 5, 1)))\n        actual = formatting.short_data_repr(da)\n        expected = \"array([], shape=(0, 5, 1), dtype=float64)\"\n        assert actual == expected\n"], "sample_7": ["def test_column_inplace_addition_with_scalar(Column):\n    c = Column([1, 2, 3])\n    c += 4\n    assert (c == [5, 6, 7]).all()\n", "def test_masked_column_set_mask_broadcast():\n    mc = table.MaskedColumn([1., 2., 3.], mask=[True, False, True])\n    mc.mask = np.array([True, True, False])\n    assert np.all(mc.mask == [True, True, False])\n", "def test_column_meta_copy_on_view():\n    \"\"\"\n    Test that metadata on a Column is properly copied when a view of the Column is created.\n    \"\"\"\n    c = table.Column([1, 2, 3], meta={'key': 'value'})\n    c_view = c[1:]\n    assert c_view.meta == c.meta\n    c_view.meta['key'] = 'new_value'\n    assert c_view.meta['key'] == 'new_value'\n    assert c.meta['key'] == 'value'\n", "def test_column_data_comparison():\n    \"\"\"Test comparison operations between Column and NumPy arrays.\"\"\"\n    c = table.Column([1, 2, 3], name='a')\n    arr = np.array([1, 2, 3])\n\n    assert np.all(c == arr)\n    assert np.all(arr == c)\n    assert not np.all(c != arr)\n    assert not np.all(arr != c)\n", "compilation error"], "sample_765": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_930": ["def test_create_index_with_category_key(app):\n    text = (\".. index:: single: docutils\\n\"\n            \"   :category: Tools\\n\"\n            \".. index:: single: Python\\n\"\n            \"   :category: Languages\\n\"\n            \".. index:: Sphinx\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n\n    # check index is created correctly\n    assert len(index) == 3\n    assert index[0] == ('Languages', [('Python', [[('', '#index-1')], [], None])])\n    assert index[1] == ('S', [('Sphinx', [[('', '#index-2')], [], None])])\n    assert index[2] == ('Tools', [('docutils', [[('', '#index-0')], [], None])])\n", "def test_create_index_with_category_key(app):\n    text = (\".. index:: pair: docutils; reStructuredText :category: Tools\\n\"\n            \".. index:: pair: Python; interpreter :category: Languages\\n\"\n            \".. index:: Sphinx\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n    assert len(index) == 3\n    assert index[0] == ('Languages', [('Python', [[], [('interpreter', [('', '#index-1')])],\n                                          None])])\n    assert index[1] == ('Tools', [('docutils', [[], [('reStructuredText', [('', '#index-0')])],\n                                         None])\n    assert index[2] == ('S', [('Sphinx', [[('', '#index-2')], [], None])])\n", "def test_create_index_with_category_key(app):\n    text = (\".. index:: single: docutils\\n\"\n            \"   :category: Docs\\n\"\n            \".. index:: single: Python\\n\"\n            \"   :category: Programming\\n\"\n            \".. index:: Sphinx\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n    assert len(index) == 3\n    assert index[0] == ('Docs', [('docutils', [[('', '#index-0')], [], 'Docs'])])\n    assert index[1] == ('Programming', [('Python', [[('', '#index-1')], [], 'Programming'])])\n    assert index[2] == ('S', [('Sphinx', [[('', '#index-2')], [], None])])\n", "def test_create_index_grouping(app):\n    text = (\".. index:: pair: foo; bar\\n\"\n            \".. index:: pair: foo; baz\\n\"\n            \".. index:: pair: bar; baz\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n    assert len(index) == 3\n    assert index[0] == ('B', [('bar', [[], [('baz', [('', '#index-2')]),\n                                              ('foo', [('', '#index-0')])], None])])\n    assert index[1] == ('F', [('foo', [[], [('bar', [('', '#index-0')]),\n                                              ('baz', [('', '#index-1')])], None])])\n    assert index[2] == ('Z', [('baz', [[], [('foo', [('', '#index-1')]),\n                                              ('bar', [('', '#index-2')])], None])])\n", "def test_fix_index_entries(app):\n    text = \"\"\"\n    .. index::\n        single: function() (in module foo)\n        single: function() (in module bar)\n    \"\"\"\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n    assert len(index) == 1\n    assert index[0] == ('F',\n                        [('function()',\n                          [[('', '#index-0'),\n                            ('', '#index-1')],\n                           {}, None])])\n\n"], "sample_562": ["def test_set_linewidth():\n    fig, ax = plt.subplots()\n    line, = ax.plot([1, 2, 3, 4], [1, 4, 9, 16])\n    with pytest.raises(ValueError):\n        line.set_linewidth('invalid_value')\n    assert line.get_linewidth() == 1.0\n    line.set_linewidth(3.0)\n    assert line.get_linewidth() == 3.0\n", "    def test_set_pickradius_after_creation():\n        fig, ax = plt.subplots()\n        l, = ax.plot([0, 1], [0, 1], picker=False)\n        l.set_pickradius(10)\n        assert l.get_pickradius() == 10\n        l.set_picker(True)\n\n        mouse_event = SimpleNamespace(x=fig.bbox.width // 2,\n                                      y=fig.bbox.height // 2 + 15)\n\n        found, indices = l.contains(mouse_event)\n        assert found\n", "    def test_markersize(self, markersize):\n        fig, ax = plt.subplots()\n        lines = ax.plot([1, 2, 3], [4, 5, 6], marker='o', markersize=markersize)\n        assert lines[0].get_markersize() == markersize\n", "def test_set_marker_after_plot():\n    fig, ax = plt.subplots()\n\n    line, = ax.plot([1, 2, 3, 4], [5, 6, 7, 8])\n    assert line.get_marker() is None\n\n    line.set_marker('o')\n    assert line.get_marker() == 'o'\n    fig.canvas.draw()\n\n", "def test_linedata_mutation(fig_test, fig_ref):\n    x = np.arange(10)\n    y = np.arange(10)**2\n\n    ax_test = fig_test.add_subplot()\n    line_test, = ax_test.plot(x, y)\n    line_test.set_xdata(np.arange(20))\n    line_test.set_ydata(np.arange(20)**2)\n\n    ax_ref = fig_ref.add_subplot()\n    ax_ref.plot(np.arange(20), np.arange(20)**2)\n"], "sample_283": ["    def test_runshell(self):\n        with mock.patch('subprocess.Popen') as mock_popen:\n            mock_process = mock.MagicMock()\n            mock_process.communicate.return_value = (b'', b'')\n            mock_popen.return_value = mock_process\n            client = DatabaseClient(connection.settings_dict)\n            client.runshell(['some_parameter'])\n        mock_popen.assert_called_once_with(\n            ['psql', 'some_parameter'],\n            stdin=subprocess.PIPE,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            env={'PGPASSWORD': str(connection.settings_dict['PASSWORD'])},\n        )\n", "    def test_default_db(self):\n        self.assertEqual(\n            self.settings_to_cmd_args_env({'HOST': 'somehost', 'PORT': '444'}),\n            (['psql', '-h', 'somehost', '-p', '444', 'postgres'], {}),\n        )\n", "    def test_runshell(self):\n        with mock.patch('django.db.backends.postgresql.client.subprocess.run') as mock_run:\n            DatabaseClient().runshell(['some', 'parameters'])\n            mock_run.assert_called_once()\n            args, kwargs = mock_run.call_args\n            self.assertEqual(kwargs['capture_output'], True)\n", "    def test_default_db(self):\n        self.assertEqual(\n            self.settings_to_cmd_args_env({'USER': 'someuser', 'HOST': 'somehost', 'PORT': '444'}),\n            (['psql', '-U', 'someuser', '-h', 'somehost', '-p', '444', 'postgres'], {}),\n        )\n", "    def test_runshell(self):\n        with mock.patch('subprocess.run') as mock_subprocess_run:\n            client = DatabaseClient(connection.settings_dict)\n            client.runshell([])\n\n            mock_subprocess_run.assert_called_once()\n            args, kwargs = mock_subprocess_run.call_args\n            self.assertIn('-c', args[1:])\n"], "sample_1102": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_76": ["    def test_language_settings_consistent(self):\n        msg = 'You have provided a value for the LANGUAGE_CODE setting that is not in the LANGUAGES setting.'\n        for invalid_tag in self.invalid_tags:\n            with self.subTest(f'invalid_tag={invalid_tag}'):\n                with self.settings(LANGUAGE_CODE=invalid_tag, LANGUAGES=[('en', 'en')]):\n                    self.assertEqual(check_language_settings_consistent(None), [\n                        Error(msg, id='translation.E004'),\n                    ])\n        for valid_tag in self.valid_tags:\n            with self.subTest(f'valid_tag={valid_tag}'):\n                with self.settings(LANGUAGE_CODE=valid_tag, LANGUAGES=[(valid_tag, valid_tag)]):\n                    self.assertEqual(check_language_settings_consistent(None), [])\n", "    def test_language_settings_consistent(self):\n        for tag in self.valid_tags:\n            with self.subTest(tag), self.settings(LANGUAGE_CODE=tag, LANGUAGES=[(tag, tag)]):\n                self.assertEqual(check_language_settings_consistent(None), [])\n\n        with self.settings(LANGUAGE_CODE='invalid', LANGUAGES=[('en', 'en')]):\n            self.assertEqual(check_language_settings_consistent(None), [Error('You have provided a value for the LANGUAGE_CODE setting that is not in the LANGUAGES setting.', id='translation.E004')])\n", "    def test_language_settings_consistent(self):\n        for tag in self.valid_tags:\n            with self.subTest(tag), self.settings(LANGUAGE_CODE=tag, LANGUAGES=[(tag, tag)]):\n                self.assertEqual(check_language_settings_consistent(None), [])\n\n        with self.subTest(\"invalid LANGUAGE_CODE\"), self.settings(LANGUAGE_CODE='invalid', LANGUAGES=[('en', 'en')]):\n            self.assertEqual(check_language_settings_consistent(None), [Error(\n            'You have provided a value for the LANGUAGE_CODE setting that is not in the LANGUAGES setting.',\n            id='translation.E004')])\n\n\n\n", "    def test_inconsistent_language_settings(self):\n        for invalid_tag in self.invalid_tags:\n            with self.subTest(invalid_tag), self.settings(\n                LANGUAGE_CODE=invalid_tag, LANGUAGES=[('en', 'English')]\n            ):\n                self.assertEqual(check_language_settings_consistent(None), [Error(\n                    'You have provided an invalid value for the LANGUAGE_CODE setting: %s.' % invalid_tag, id='translation.E001' \n                )])\n", "    def test_inconsistent_language_settings(self):\n        with self.settings(LANGUAGE_CODE='foo', LANGUAGES=[('en', 'English')]):\n            self.assertEqual(check_language_settings_consistent(None), [\n                Error('You have provided a value for the LANGUAGE_CODE setting that is not in the LANGUAGES setting.', id='translation.E004'),\n            ])\n"], "sample_1118": ["def test_power():\n    assert MatPow(C, 2).shape == (n, n)\n    assert MatPow(A, 2).shape == (n, m)\n    assert MatPow(C, 0).doit() == Identity(n)\n    assert MatPow(A, -1).doit() == Inverse(A).doit()\n    assert MatPow(C, 2).doit() == C*C\n    assert MatPow(ZeroMatrix(n, n), 2).doit() == ZeroMatrix(n, n)\n    assert MatPow(C, -1).doit(inv_expand=False) == Inverse(C)\n\n\n", "def test_matpow():\n    assert MatPow(C, 2).shape == (n, n)\n    assert MatPow(C, 2).base == C\n    assert MatPow(C, 2).exp == 2\n    assert MatPow(A, 2).doit() == A*A\n    assert MatPow(C, 0).doit() == Identity(n)\n    assert MatPow(C, -1).doit() == Inverse(C)\n    assert MatPow(ZeroMatrix(n, n), 2).doit() == ZeroMatrix(n, n)\n    raises(NonInvertibleMatrixError, lambda: MatPow(ZeroMatrix(n, n), -1).doit())\n    raises(NonSquareMatrixError, lambda: MatPow(A, 2))\n\n", "def test_matpow():\n    assert MatPow(C, 2).shape == (n, n)\n    assert MatPow(A, 3).shape == (n, l)\n\n    assert MatPow(C, 0) == Identity(n)\n    assert MatPow(C, 1) == C\n    assert MatPow(C, -1) == Inverse(C)\n\n    assert MatPow(C, 2).doit() == C*C\n    assert MatPow(C, -2).doit() == Inverse(C)*Inverse(C)\n\n    raises(NonSquareMatrixError, lambda: MatPow(A, 2))\n    raises(ValueError, lambda: MatPow(C, 'a'))\n\n    assert MatPow(Identity(n), 3).doit() == Identity(n)\n\n    assert MatPow(ZeroMatrix(n, n), 2).doit() == ZeroMatrix(n, n)\n\n    assert MatPow(C, 0).doit() == Identity(n)\n    assert MatPow(C, 1).doit() == C\n    assert MatPow(C, -1).doit() == Inverse(C)\n", "def test_matpow():\n    assert MatPow(C, 2).shape == (n, n)\n    assert MatPow(A, 2).shape == (n, m)\n    assert MatPow(C, 0).doit() == Identity(n)\n    assert MatPow(C, 1).doit() == C\n    raises(NonSquareMatrixError, lambda: MatPow(A, 2))\n    raises(NonInvertibleMatrixError, lambda: MatPow(ZeroMatrix(n, n), -1))\n    assert MatPow(C, -1).doit() == Inverse(C)\n    assert MatPow(C, 3).doit() == C*C*C\n", "def test_MatPow():\n    assert MatPow(C, 2).shape == (n, n)\n    assert MatPow(C, 2).base == C\n    assert MatPow(C, 2).exp == 2\n    assert isinstance(MatPow(C, 2).doit(), MatPow)\n    assert MatPow(C, 0).doit() == Identity(n)\n    assert MatPow(ZeroMatrix(n, n), 2).doit() == ZeroMatrix(n, n)\n    raises(NonInvertibleMatrixError, lambda: MatPow(ZeroMatrix(n, n), -2).doit())\n    assert MatPow(C, -1).doit() == Inverse(C)\n    assert MatPow(C, S.Half).doit() == C**S.Half\n"], "sample_602": ["    def test_to_zarr_append_existing_data(tmp_path):\n        ds = xr.Dataset(\n            {\"a\": (\"x\", np.arange(5))}, coords={\"x\": np.arange(5)}\n        )\n        zarr_path = tmp_path / \"test.zarr\"\n        ds.to_zarr(zarr_path, mode=\"w\")\n        ds_append = xr.Dataset({\"a\": (\"x\", np.arange(5, 10))}, coords={\"x\": np.arange(5, 10)})\n        ds_append.to_zarr(zarr_path, mode=\"a\")\n        ds_expected = xr.concat([ds, ds_append], dim=\"x\")\n        ds_loaded = xr.open_zarr(zarr_path)\n        assert_identical(ds_expected, ds_loaded)\n", "    def test_open_mfdataset_multifile_with_pathlib_objects(tmp_path):\n        ds1 = xr.Dataset({\"a\": ([\"x\"], [1, 2])}, coords={\"x\": [0, 1]})\n        ds2 = xr.Dataset({\"a\": ([\"x\"], [3, 4])}, coords={\"x\": [0, 1]})\n        \n        file_path1 = tmp_path / \"file1.nc\"\n        file_path2 = tmp_path / \"file2.nc\"\n        \n        ds1.to_netcdf(file_path1)\n        ds2.to_netcdf(file_path2)\n        \n        combined = xr.open_mfdataset([file_path1, file_path2], parallel=True)\n\n        assert_identical(xr.concat([ds1, ds2], dim=\"x\"), combined)\n\n\n\n", "compilation error", "compilation error", "def test_to_netcdf_encoding():\n    ds = xr.Dataset({'a': ('x', np.arange(5))})\n    expected = xr.Dataset({'a': ('x', np.arange(5))}).astype('int16')\n\n    with xr.backends.NetCDF4DataStore.open(\"test.nc\", 'w') as store:\n        ds.to_netcdf(\"test.nc\", encoding={'a': {'dtype': 'int16'}})\n        actual = xr.open_dataset(\"test.nc\").copy()\n    assert_identical(expected, actual)\n"], "sample_1160": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_427": ["    def test_invalid_management_form_data_with_extra(self):\n        data = {\n            \"choices-TOTAL_FORMS\": \"two\",\n            \"choices-INITIAL_FORMS\": \"one\",\n            \"choices-0-choice\": \"Zero\",\n            \"choices-0-votes\": \"0\",\n            \"choices-1-choice\": \"One\",\n            \"choices-1-votes\": \"1\",\n        }\n        ChoiceFormSet = formset_factory(Choice, extra=1)\n        formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(\n            formset.non_form_errors(),\n            [\n                \"ManagementForm data is missing or has been tampered with. \"\n                \"Missing fields: form-TOTAL_FORMS, form-INITIAL_FORMS. \"\n                \"You may need to file a bug report if the issue persists.\",\n            ],\n        )\n\n", "    def test_empty_formset_with_initial_data(self):\n        data = {\n            \"form-TOTAL_FORMS\": \"0\",\n            \"form-INITIAL_FORMS\": \"0\",\n        }\n        formset = ArticleFormSet(data)\n        self.assertFalse(formset.is_valid())\n\n", "    def test_empty_formset_cleaned_data(self):\n        ChoiceFormSet = formset_factory(Choice, extra=0)\n        formset = ChoiceFormSet()\n        self.assertEqual(formset.cleaned_data, [])\n", "    def test_formset_management_form_extra(self):\n        \"\"\"\n        The management form should always be included, even when extra is 0.\n        \"\"\"\n        ChoiceFormSet = formset_factory(Choice, extra=0)\n        formset = ChoiceFormSet()\n        self.assertIn('form-TOTAL_FORMS', formset.management_form.as_p())\n        self.assertIn('form-INITIAL_FORMS', formset.management_form.as_p())\n", "    def test_formset_empty_data(self):\n        data = {'form-TOTAL_FORMS': '0', 'form-INITIAL_FORMS': '0'}\n        formset = ArticleFormSet(data)\n        self.assertTrue(formset.is_valid())\n        self.assertEqual(formset.cleaned_data, [])\n"], "sample_1078": ["def test_IndexedBase_expand():\n    i, j = symbols('i j', integer=True)\n    a, b = symbols('a b')\n    A = IndexedBase('A')\n    assert (A[i] + A[j])**2.expand() == A[i]**2 + A[j]**2 + 2*A[i]*A[j]\n    assert (A[i] * b + a * A[j]).expand() == a * A[j] + b * A[i]\n", "def test_IndexedBase_shape_error():\n    i, j = symbols('i j', integer=True)\n    raises(ValueError, lambda: IndexedBase('A', shape=(i, i + 1)))\n", "compilation error", "def test_issue_19290():\n    i = Idx('i', (1, 5))\n    A = IndexedBase('A')\n    assert A[i].free_symbols == {A, i}\n\n", "compilation error"], "sample_1139": ["compilation error", "def test_ComplexRegion_empty():\n    assert ComplexRegion(EmptySet()).is_empty\n    assert ComplexRegion(FiniteSet()).is_empty\n", "compilation error", "compilation error", "compilation error"], "sample_929": ["compilation error", "compilation error", "def test_pyobject_signature_highlighting(app):\n    text = \".. py:function:: hello(name: str) -> str\"\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n\n    assert_node(doctree[1][0][1], desc_signature,  \n                [desc_name, desc_parameterlist, desc_returns])\n    assert_node(doctree[1][0][1][1][0], desc_parameter, [desc_sig_name, desc_sig_punctuation, 'str'])\n", "def test_pydata_signature_with_default_value(app):\n    text = (\".. py:data:: var\\n\"\n            \"   :type: int\\n\"\n            \"   :value: 1\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"var\"],\n                                                    [desc_annotation, (\": \",\n                                                                       [pending_xref, \"int\"])],\n                                                    [desc_annotation, \" = 1\"])],\n                                  [desc_content, ()])]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"data\",\n                domain=\"py\", objtype=\"data\", noindex=False)\n", "def test_pyobject_prefix_with_alias(app):\n    text = (\".. py:class:: Foo\\n\"\n            \"\\n\"\n            \"   .. py:method:: Foo.say\\n\"\n            \"\\n\"\n            \".. py:class:: Bar\\n\"\n            \"\\n\"\n            \"   .. py:method:: Bar.say\\n\"\n            \"      :alias: say_something\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Foo\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert doctree[1][1][0].astext().strip() == 'Foo'\n    assert doctree[1][1][3].astext().strip() == 'say()'\n\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Bar\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert doctree[3][1][0].astext().strip() == 'Bar'\n    assert doctree[3][1][3].astext().strip() == 'say() / say_something()'\n"], "sample_315": ["    def test_i18n_patterns_used_with_script_prefix(self):\n        prefix = '/script_prefix'\n        with override_script_prefix(prefix):\n            response = self.client.get('/nl/prefixed/', HTTP_ACCEPT_LANGUAGE='nl', SCRIPT_NAME=prefix)\n            self.assertEqual(response.status_code, 200)\n            self.assertEqual(response.headers['content-language'], 'nl')\n", "    def test_language_code_not_in_languages(self):\n        with translation.override('fr'):\n            response = self.client.get('/fr/account/register/')\n            self.assertEqual(response.status_code, 404)\n", "    def test_request_urlconf_not_considered(self):\n        request = RequestFactory().get('/nl/')\n        middleware = LocaleMiddleware(lambda req: HttpResponse())\n        with translation.override('nl'):\n            middleware.process_request(request)\n        self.assertEqual(request.LANGUAGE_CODE, settings.LANGUAGE_CODE)\n\n", "    def test_url_with_query_params(self):\n        with translation.override('en'):\n            response = self.client.get('/en/account/register/?next=/en/some/page/')\n            self.assertEqual(response.status_code, 200)\n            self.assertEqual(response.headers['content-language'], 'en')\n            self.assertEqual(response.context['LANGUAGE_CODE'], 'en')\n        with translation.override('nl'):\n            response = self.client.get('/nl/profiel/registreren/?next=/nl/some/page/')\n            self.assertEqual(response.status_code, 200)\n            self.assertEqual(response.headers['content-language'], 'nl')\n            self.assertEqual(response.context['LANGUAGE_CODE'], 'nl') \n", "    def test_language_not_in_settings(self):\n        with self.assertRaises(ImproperlyConfigured):\n            with translation.override('fr'):\n                reverse('prefixed')\n\n"], "sample_96": ["    def test_actions_have_valid_signatures(self):\n            pass\n        class BandAdmin(ModelAdmin):\n            actions = (bad_action,)\n        self.assertIsInvalid(\n            BandAdmin, Band,\n            'bad_action defined in <class \\'modeladmin.test_checks.'\n            'ActionsCheckTests.test_actions_have_valid_signatures.'\n            '<locals>.BandAdmin\\'> must have a signature of '\n            '(modeladmin, request, queryset).',\n            id='admin.E131'\n\n        )\n", "        def action2(modeladmin, request, queryset):\n            pass\n", "    def test_actions_not_callable(self):\n        class BandAdmin(ModelAdmin):\n            actions = 'not_a_callable'\n\n        self.assertIsInvalid(\n            BandAdmin, Band,\n            \"The value of 'actions' must be a list or tuple of callables.\",\n            id='admin.E131',\n        )\n", "        def action2(modeladmin, request, queryset):\n            pass\n", "        def action2(modeladmin, request, queryset):\n            pass\n"], "sample_1033": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_261": ["    def test_iso8601_duration(self):\n        test_values = (\n            ('P1DT2H3M4S', timedelta(days=1, hours=2, minutes=3, seconds=4)),\n            ('PT2H3M4S', timedelta(hours=2, minutes=3, seconds=4)),\n            ('P1D', timedelta(days=1)),\n            ('-P1D', timedelta(days=-1)),\n            ('P1D+2H3M4S', timedelta(days=1, hours=2, minutes=3, seconds=4)),\n            ('P1DT2H3M4.5S', timedelta(days=1, hours=2, minutes=3, seconds=4, microseconds=500000)),\n            ('P1DT2H3M4,5S', timedelta(days=1, hours=2, minutes=3, seconds=4, microseconds=500000)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n\n", "    def test_iso8601_duration(self):\n        test_values = (\n            ('P5DT12H30M', timedelta(days=5, hours=12, minutes=30)),\n            ('P1W', timedelta(days=7)),\n            ('PT12H30M', timedelta(hours=12, minutes=30)),\n            ('P1D', timedelta(days=1)),\n            ('PT1H', timedelta(hours=1)),\n            ('PT1M', timedelta(minutes=1)),\n            ('PT1S', timedelta(seconds=1)),\n            ('-P1D', timedelta(days=-1)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n", "    def test_iso8601_duration(self):\n        test_values = (\n            ('P1DT2H3M4.5S', timedelta(days=1, hours=2, minutes=3, seconds=4, milliseconds=500)),\n            ('PT2H3M4.5S', timedelta(hours=2, minutes=3, seconds=4, milliseconds=500)),\n            ('P1D', timedelta(days=1)),\n            ('PT12H', timedelta(hours=12)),\n            ('PT30M', timedelta(minutes=30)),\n            ('PT60S', timedelta(seconds=60)),\n            ('-P1DT2H3M4S', timedelta(days=-1, hours=-2, minutes=-3, seconds=-4)),\n            ('-PT2H3M4S', timedelta(hours=-2, minutes=-3, seconds=-4)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n", "    def test_iso8601_duration(self):\n        test_values = (\n            ('P1DT2H3M4.5S', timedelta(days=1, hours=2, minutes=3, seconds=4, milliseconds=500)),\n            ('PT2H3M4.5S', timedelta(hours=2, minutes=3, seconds=4, milliseconds=500)),\n            ('P1D', timedelta(days=1)),\n            ('PT1H', timedelta(hours=1)),\n            ('PT1M', timedelta(minutes=1)),\n            ('PT1S', timedelta(seconds=1)),\n            ('P-1DT2H3M4.5S', timedelta(days=-1, hours=2, minutes=3, seconds=4, milliseconds=500)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n\n", "    def test_iso8601_duration_format(self):\n        test_values = (\n            ('PT15M', timedelta(minutes=15)),\n            ('-PT15M', timedelta(minutes=-15)),\n            ('P2DT10H30M', timedelta(days=2, hours=10, minutes=30)),\n            ('P3D', timedelta(days=3)),\n            ('PT10.5S', timedelta(seconds=10.5)),\n            ('PT10,5S', timedelta(seconds=10.5)),\n            ('PT10.5H', timedelta(hours=10.5)),\n            \n            \n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n\n"], "sample_533": ["    def test_contourf_add_lines():\n        fig, ax = plt.subplots()\n        x = np.linspace(-3, 3, 100)\n        y = np.linspace(-3, 3, 100)\n        X, Y = np.meshgrid(x, y)\n        Z = np.sin(np.sqrt(X**2 + Y**2))\n        contourf = ax.contourf(X, Y, Z, levels=10)\n        contour = ax.contour(X, Y, Z, levels=5, colors='k')\n        ax.clabel(contour)\n        # Add contour lines to the contourf plot\n        contourf.add_lines(contour)\n\n", "compilation error", "def test_contour_linewidth():\n    # Test different linewidths for contour lines.\n    x = np.linspace(-2, 2, 100)\n    y = np.linspace(-2, 2, 100)\n    X, Y = np.meshgrid(x, y)\n    Z = np.sin(X**2 + Y**2)\n\n    fig, axs = plt.subplots(2, 2)\n    linewidths = [0.5, 1, 2, 3]\n\n    for ax, lw in zip(axs.ravel(), linewidths):\n        CS = ax.contour(X, Y, Z, levels=[0.2, 0.4, 0.6, 0.8],\n                       linewidths=lw)\n        ax.clabel(CS, inline=True, fontsize=8)\n        ax.set_title(f\"Linewidth: {lw}\")\n", "    def test_contour_filled_remove():\n        ax = plt.figure().add_subplot()\n        orig_children = ax.get_children()\n        cf = ax.contourf(np.arange(16).reshape((4, 4)))\n        assert ax.get_children() != orig_children\n        cf.remove()\n        assert ax.get_children() == orig_children\n", "def test_contourf_negative_linestyles_colors():\n    delta = 0.025\n    x = np.arange(-3.0, 3.0, delta)\n    y = np.arange(-2.0, 2.0, delta)\n    X, Y = np.meshgrid(x, y)\n    Z1 = np.exp(-X**2 - Y**2)\n    Z2 = np.exp(-(X - 1)**2 - (Y - 1)**2)\n    Z = (Z1 - Z2) * 2\n\n    fig, ax = plt.subplots()\n    CS = ax.contourf(X, Y, Z, 6, cmap='viridis',\n                     negative_linestyles='dashed',\n                     negative_colors='red')\n    ax.clabel(CS, fontsize=9, inline=True)\n    ax.set_title(\n        'Filled contours - negative contours dashed and red')\n"], "sample_894": ["    def test_set_params_with_readonly_buffer():\n        X, y = make_classification(n_samples=100, n_features=200)\n        X_csr = csr_matrix(X, copy=True)\n\n        clf = RandomForestClassifier(random_state=0)\n        clf.fit(X_csr, y)\n\n        # Modify the input data after fitting\n        X_csr[0, 0] = 10\n\n        # Make sure the model can set new parameters without errors,\n        # even if the input data is read-only.\n        clf.set_params(max_depth=2)\n", "    def test_oob_score_multiclass_consistency(name):\n        # Test that oob_score is consistent for multiclass problems\n        # regardless of the number of estimators.\n        X, y = make_classification(n_samples=100, n_features=5, n_informative=3,\n                                   n_classes=3, random_state=0)\n\n        clf1 = FOREST_CLASSIFIERS[name](n_estimators=10, oob_score=True, bootstrap=True, \n                                       random_state=0)\n        clf2 = FOREST_CLASSIFIERS[name](n_estimators=100, oob_score=True, bootstrap=True,\n                                       random_state=0)\n\n        clf1.fit(X, y)\n        clf2.fit(X, y)\n\n        assert clf1.oob_score_ == pytest.approx(clf2.oob_score_, rel=0.1)\n", "def test_oob_score_deprecation(name):\n    X = np.random.rand(50, 10)\n    y = np.random.rand(50)\n    est = FOREST_CLASSIFIERS_REGRESSORS[name](oob_score=True, random_state=0)\n    warn_msg = \"`oob_score` is deprecated and will be removed in 1.4.\"\n    with pytest.warns(FutureWarning, match=warn_msg):\n        est.fit(X, y)\n", "compilation error", "def test_oob_score_memory_leak():\n    # Check that oob_score computation does not lead to a memory leak\n    n_estimators = 100\n    X, y = datasets.make_classification(n_samples=100, n_features=10, random_state=0)\n    est = RandomForestClassifier(\n        n_estimators=n_estimators, bootstrap=True, oob_score=True, random_state=0\n    )\n\n    gc.collect()\n    before_fit = used_memory()\n    est.fit(X, y)\n\n    gc.collect()\n    after_fit = used_memory()\n\n    # OOB calculation can be complex and lead to a temporary increase in memory usage.\n    # We allow for a small increase, but it should not grow linearly with the\n    # number of estimators.\n    assert (after_fit - before_fit) / n_estimators < 100\n"], "sample_270": ["    def test_unique_constraint_include_pointing_to_fk(self):\n        class Target(models.Model):\n            pass\n\n        class Model(models.Model):\n            fk_1 = models.ForeignKey(Target, models.CASCADE, related_name='target_1')\n            fk_2 = models.ForeignKey(Target, models.CASCADE, related_name='target_2')\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['fk_2'],\n                        include=['fk_1_id'],\n                        name='name',\n                    ),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [])\n", "    def test_unique_constraint_include_pointing_to_fk(self):\n        class Target(models.Model):\n            pass\n\n        class Model(models.Model):\n            fk = models.ForeignKey(Target, models.CASCADE, related_name='target')\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['fk'],\n                        include=['id'],\n                        name='name',\n                    ),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [])\n", "    def test_unique_constraint_include_pointing_to_fk(self):\n        class Target(models.Model):\n            pass\n\n        class Model(models.Model):\n            fk_1 = models.ForeignKey(Target, models.CASCADE, related_name='target_1')\n            fk_2 = models.ForeignKey(Target, models.CASCADE, related_name='target_2')\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['fk_2'],\n                        include=['fk_1_id'],\n                        name='name',\n                    ),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [])\n", "    def test_unique_constraint_include_pointing_to_fk(self):\n        class Target(models.Model):\n            pass\n\n        class Model(models.Model):\n            fk_1 = models.ForeignKey(Target, models.CASCADE, related_name='target_1')\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['id'],\n                        include=['fk_1'],\n                        name='name',\n                    ),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [])\n", "    def test_unique_constraint_include_pointing_to_fk(self):\n        class Target(models.Model):\n            pass\n\n        class Model(models.Model):\n            fk = models.ForeignKey(Target, models.CASCADE, related_name='target')\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['id'],\n                        include=['fk'],\n                        name='name',\n                    ),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [])\n"], "sample_654": ["    def test_fixture_function_closure(testdir):\n        testdir.makepyfile(\n            \"\"\"\n\n                @pytest.fixture\n                    return 42\n                return inner_fixture\n\n                assert inner_fixture == 42\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.assertoutcomes(passed=1)\n", "    def test_fixture_closure_with_non_fixture_function(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n                return 1\n\n            @pytest.fixture\n                return my_function()\n            \n\n                pass\n        \"\"\"\n        )\n        items, _ = testdir.inline_genitems()\n        request = FixtureRequest(items[0])\n        assert request.fixturenames == \"my_fixture\".split()\n", "    def test_fixture_call_direct(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.fixture\n                return 1\n\n                print(f())\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines([\"*TypeError*\"])\n", "    def test_fixture_depends_on_module_fixture_error(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.fixture(scope=\"module\")\n                pass\n\n            @pytest.fixture\n                pass\n\n                pass\n        \"\"\"\n        )\n\n        result = testdir.runpytest()\n        assert result.ret == 0\n", "    def test_call_fixture_function_error(testdir):\n        testdir.makepyfile(\n            \"\"\"\n        import pytest\n\n        @pytest.fixture\n            return 1\n\n            assert fix() == 1\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        assert result.ret == 0\n\n"], "sample_701": ["    def test_argument_percent_default(self, pytester: Pytester) -> None:\n        p = pytester.makepyfile(\n            \"\"\"\n            import argparse\n            import pytest\n\n                parser = argparse.ArgumentParser()\n                parser.add_argument(\"--myopt\", type=int, default=42, help=\"%default\")\n                args, _ = parser.parse_known_args()\n                assert args.myopt == 42 \n            \"\"\"\n        )\n        result = pytester.runpytest(p)\n        result.stdout.fnmatch_lines(\n            [\n                \"*pytest now uses argparse. '%default' should be changed to '%(default)s'\",\n            ]\n        )\n        result.assert_outcomes(passed=1, warnings=1)\n", "    def test_deprecate_argument_percent_default(self, pytester: Pytester) -> None:\n        p = pytester.makepyfile(\n            \"\"\"\n            import pytest\n\n                pytest.mark.\n\n                options = ['--myoption=%default']\n                pytest.main(options)\n            \"\"\"\n        )\n        result = pytester.runpytest(p)\n        result.stdout.fnmatch_lines(\n            [\n                \"*PytestDeprecationWarning: pytest now uses argparse. '%default' should be changed to '%(default)s'\",\n            ]\n        )\n\n", "def test_argument_type_str_choice_is_deprecated(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            pytest.addoption('--myoption', type=str, choices=['a', 'b', 'c'])\n            request.addfinalizer(lambda: request.config.pop('myoption', None))\n        \"\"\"\n    )\n\n    result = pytester.runpytest('-p no:warnings', \"--myoption=a\")\n    result.stdout.fnmatch_lines(\n        [\n            '*PytestDeprecationWarning: `type` argument to addoption() is the string '\n            \"'str'. For choices this is optional and can be omitted, but when supplied \"\n            \"should be a type (for example `str` or `int`)\"\n            \". (options: ['myoption'])\"\n        ]\n    )\n\n\n\n", "    def test_argument_percent_default_is_deprecated(self, pytester: Pytester) -> None:\n        p = pytester.makepyfile(\n            \"\"\"\n            import pytest\n            \n                pytest.addoption('--myoption', default=50, type=int, help='%default')\n            \"\"\"\n        )\n        result = pytester.runpytest(p)\n        result.stdout.fnmatch_lines([\"*PytestDeprecationWarning: pytest now uses argparse. '%default' should be changed to '%(default)s'\"])\n        result.assert_outcomes(warnings=1)        \n", "def test_argument_percent_default_is_deprecated(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            option = pytestconfig.getoption('--myoption')\n            assert option == 'value'\n        \"\"\",\n        myplugin=\"\"\"\n        import pytest\n\n            parser.addoption('--myoption', default='value', help='My option',\n                             type='string', \"%default is deprecated\")\n        \"\"\",\n    )\n    result = pytester.runpytest_subprocess(\"--myoption\", \"--myplugin\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: pytest now uses argparse. '%default\",\n        ]\n    )\n"], "sample_913": ["def test_pyclass_module_index(app):\n    text = (\".. py:class:: docutils.nodes.Node\\n\"\n            \".. py:class:: sphinx.util.docutils. sphinx.util.docutils\\n\")\n    restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    assert index.generate() == (\n        [('d', [IndexEntry('docutils', 0, 'index', 'module-docutils', '', '', '')]),\n         ('s', [IndexEntry('sphinx', 1, 'index', 'module-sphinx', '', '', ''),\n                IndexEntry('sphinx.util', 2, 'index', 'module-sphinx.util', '', '', '')])],\n        False\n    )\n", "def test_pymodule_with_version(app):\n    text = \".. py:module:: package.module; version=1.2.3\\n\"\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_addname, \"package.\"],\n                                                    [desc_name, \"module\"])],\n                                  [desc_content, ()])]))\n    assert 'package.module' in domain.objects\n    assert domain.objects['package.module'] == ('index', 'package.module', 'module')\n", "   def test_pyexception_no_module(app):\n        text = \".. py:exception:: IOError\"\n        domain = app.env.get_domain('py')\n        doctree = restructuredtext.parse(app, text)\n        assert_node(doctree, (addnodes.index,\n                              [desc, ([desc_signature, ([desc_annotation, \"exception \"],\n                                                        [desc_name, \"IOError\"])],\n                                      desc_content)]))\n        assert_node(doctree[1], desc, desctype=\"exception\",\n                    domain=\"py\", objtype=\"exception\", noindex=False)\n        assert 'IOError' in domain.objects\n        assert domain.objects['IOError'] == ('index', 'IOError', 'exception')\n", "def test_pyExceptionEvent(app):\n    text = (\".. py:exception:: ExceptionEvent\\n\"\n            \"   :module: myModule\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"exception \"],\n                                                    [desc_addname, \"myModule.\"],\n                                                    [desc_name, \"ExceptionEvent\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"exception\",\n                domain=\"py\", objtype=\"exception\", noindex=False)\n\n    assert 'myModule.ExceptionEvent' in domain.objects\n    assert domain.objects['myModule.ExceptionEvent'] == ('index', 'myModule.ExceptionEvent', 'exception')\n", "    def test_pyattribute_inheritance(app):\n        text = (\".. py:class:: Parent\\n\"\n                \"   .. py:attribute:: attr\\n\"\n                \"\\n\"\n                \".. py:class:: Child(Parent)\\n\"\n                \"   .. py:attribute:: attr\\n\")\n        domain = app.env.get_domain('py')\n        doctree = restructuredtext.parse(app, text)\n        assert_node(doctree, (addnodes.index,\n                              [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                        [desc_name, \"Parent\"])],\n                                      [desc_content, (addnodes.index,\n                                                      desc)])]))\n        assert_node(doctree[1][1][0], addnodes.index,\n                    entries=[('single', 'attr (Parent attribute)', 'Parent.attr', '', None)])\n"], "sample_434": ["    def test_view_setup(self):\n        class TestView(View):\n                self.request = request\n                self.args = args\n                self.kwargs = kwargs\n                self.setup_called = True\n\n        request_factory = RequestFactory()\n        view = TestView.as_view()\n        instance = view(request_factory.get(\"/\"))\n        self.assertTrue(instance.setup_called)\n        self.assertIs(instance.request, request_factory.get(\"/\"))\n        self.assertEqual(instance.args, ())\n        self.assertEqual(instance.kwargs, {})\n\n", "compilation error", "    def test_redirectview_get_redirect_url(self):\n        view = RedirectView()\n        view.url = '/foo/'\n        self.assertEqual(view.get_redirect_url(), '/foo/')\n\n        view.url = '/foo/{id}/'\n        self.assertEqual(view.get_redirect_url(id=1), '/foo/1/')\n\n        view.pattern_name = 'my_pattern'\n        view.url = None\n        with mock.patch('django.urls.reverse') as mock_reverse:\n            mock_reverse.return_value = '/bar/'\n            self.assertEqual(view.get_redirect_url(id=1), '/bar/')\n\n", "    def test_view_as_view_raises_error_on_invalid_kwargs(self):\n        class MyView(View):\n            pass\n\n        with self.assertRaisesMessage(TypeError, \"The method name unknown is not accepted as a keyword argument to MyView().\"):\n            MyView.as_view(unknown=\"something\")\n\n        with self.assertRaisesMessage(TypeError, \"MyView() received an invalid keyword 'unknown'. as_view only accepts arguments that are already attributes of the class.\"):\n            MyView.as_view(unknown=\"something\")\n", "    def test_redirectview_permanent(self):\n        view = RedirectView(permanent=True, url='/redirected/')\n        request = RequestFactory().get('/')\n        response = view.get(request)\n        self.assertEqual(response.status_code, 308)\n        self.assertEqual(response[\"Location\"], '/redirected/')\n"], "sample_318": ["    def test_invalid_reverse(self):\n        with self.assertRaises(NoReverseMatch):\n            reverse('lookahead-positive', kwargs={'city': 'not-a-city'})\n        with self.assertRaises(NoReverseMatch):\n            reverse('lookahead-negative', kwargs={'city': 'not-a-city'})\n        with self.assertRaises(NoReverseMatch):\n            reverse('lookbehind-positive', kwargs={'city': 'not-a-city'})\n        with self.assertRaises(NoReverseMatch):\n            reverse('lookbehind-negative', kwargs={'city': 'not-a-city'})\n", "    def test_empty_path_pattern(self):\n        with self.assertRaisesMessage(ImproperlyConfigured, \"The pattern '' must have a trailing '$'\"):\n            resolve('')\n", "    def test_reverse_with_defaults(self):\n        url = reverse('lookahead-positive', kwargs={'city': 'default'})\n        self.assertEqual(url, '/lookahead+/default/')\n\n", "    def test_named_group_lookaround(self):\n        \"\"\"Test named groups with lookaround assertions.\"\"\"\n        self.assertEqual(\n            reverse('named_lookaround-positive', kwargs={'city': 'a-city'}),\n            '/named_lookaround+/a-city/'\n        )\n        self.assertEqual(\n            reverse('named_lookaround-negative', kwargs={'city': 'a-city'}),\n            '/named_lookaround-/a-city/'\n        )\n", "    def test_include_urls_with_leading_slash(self):\n        urls = [\n            path('inner/', views.empty_view, name='urlobject-view'),\n        ]\n        self.assertEqual(include(urls, namespace='test'), (urls, 'test', 'test'))\n        self.assertEqual(include(['/inner/'], namespace='test'), (urls, 'test', 'test'))\n"], "sample_1065": ["compilation error", "def test_binomial_series():\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True)\n\n    assert binomial(n, k).series(n, 0, 3) == \\\n        k**(n) - (k*(k - 1)*n)/2 + (k*(k - 1)*(k - 2)*n**2)/6 + O(n**3)\n", "compilation error", "compilation error", "def test_binomial_evalf():\n    assert binomial(10, 5).evalf() == 252.0\n    assert binomial(100, 50).evalf() == \\\n        2.1192007168025376e+29\n    assert binomial(1000, 500).evalf() == \\\n        1.3797741162071843e+299\n    assert binomial(10000, 5000).evalf() == \\\n        5.860329658501257e+2999\n    assert binomial(0.5, 0.2).evalf() == 1.1547005383792515\n    assert binomial(-1, 2).evalf() == 1\n\n\n\n"], "sample_439": ["    def test_form_with_hidden_field_errors(self):\n        class HiddenFormField(Form):\n            hidden_field = HiddenInput()\n            other_field = CharField()\n\n        f = HiddenFormField({\"hidden_field\": \"invalid\", \"other_field\": \"some_data\"})\n        f.fields[\"hidden_field\"].error_messages = {\"invalid\": \"Hidden field error.\"}\n        self.assertFalse(f.is_valid())\n        self.assertIn(\"Hidden field error\", str(f.errors))\n", "    def test_form_errors_access(self):\n        class MyForm(Form):\n            name = CharField()\n            email = EmailField()\n\n        f = MyForm({'name': 'John Doe', 'email': 'invalid'})\n\n        self.assertEqual(f.errors, f.non_field_errors())\n        self.assertEqual(list(f.errors.keys()), [])\n        self.assertEqual(f.get_errors(), {})\n", "    def test_hidden_fields_in_template(self):\n        class HiddenForm(Form):\n            data = IntegerField(widget=HiddenInput)\n\n        f = HiddenForm({})\n        t = Template(\"{{ form.as_p }}\")\n        self.assertHTMLEqual(\n            t.render(Context({\"form\": f})),\n            '<p><input type=\"hidden\" name=\"data\" id=\"id_data\"></p>'\n        )\n", "    def test_empty_initial_data(self):\n        class MyForm(Form):\n            name = CharField(max_length=50)\n            age = IntegerField()\n\n        form = MyForm(initial={})\n        self.assertEqual(form.initial, {})\n        self.assertFalse(form.is_bound)\n", "    def test_hidden_fields_in_initial(self):\n        class Person(Form):\n            first_name = CharField(\n                widget=HiddenInput,\n                initial='John'\n            )\n            last_name = CharField()\n        f = Person()\n        self.assertEqual(f[\"first_name\"].value(), 'John')\n\n"], "sample_756": ["    def test_different_metrics(self, metric):\n        # Test that OPTICS works with different metrics\n        clust = OPTICS(min_samples=5, metric=metric).fit(X)\n        assert clust.labels_.size == X.shape[0]\n        assert clust.reachability_.size == X.shape[0]\n        assert clust.core_distances_.size == X.shape[0]\n", "def test_cluster_tree_empty_input():\n    # Test empty input array\n    X = np.array([])\n    clust = OPTICS().fit(X)\n    assert_array_equal(clust.ordering_, np.array([]))\n    assert_array_equal(clust.reachability_, np.array([]))\n    assert_array_equal(clust.labels_, np.array([]))\n    assert clust.core_sample_indices_.size == 0\n", "compilation error", "compilation error", "    def test_cluster_optics_small_dataset():\n        # Test OPTICS with a very small dataset to ensure it works correctly\n        X = np.array([[1, 1], [2, 2], [1.5, 1.5]])\n\n        clust = OPTICS().fit(X)\n        assert len(clust.labels_) == 3\n        assert len(clust.ordering_) == 3\n        assert len(clust.reachability_) == 3\n        assert np.isfinite(clust.reachability_).all()\n"], "sample_478": ["    def test_actions_valid_permissions(self):\n        @admin.action(permissions=[\"can_edit\"])\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (edit_action,)\n            \n                return True\n\n        self.assertIsValid(BandAdmin, Band) \n", "        def action2(modeladmin, request, queryset):\n            pass\n", "    def test_actions_valid(self):\n        @admin.action\n            pass\n\n        @admin.action\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (action1, action2)\n\n        self.assertIsValid(BandAdmin, Band)\n", "    def test_actions_valid(self):\n        @admin.action\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (action,)\n\n        self.assertIsValid(BandAdmin, Band)\n", "    def test_actions_custom_permission_missing_permission_string(self):\n        @admin.action\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (custom_permission_action,)\n\n        self.assertIsInvalid(\n            BandAdmin,\n            Band,\n            \"custom_permission_action action in BandAdmin must have \"\n            \"a 'permissions' attribute.\",\n            id=\"admin.E131\",\n        )\n"], "sample_776": ["def test_lars_path_gracefull_handling_of_col_zero_var():\n    # Test that lars_path handles columns with zero variance gracefully.\n\n    # Generate data with one column of zeros\n    X = np.array([[1, 0],\n                  [2, 0],\n                  [3, 0]])\n    y = np.array([1, 2, 3])\n\n    # Check if lars_path raises an error\n    alphas, _, coefs = linear_model.lars_path(X, y, method='lars')\n\n    # Ensure no error occurs and the coefficients for the zero-variance column\n    # are set to zero\n    assert np.all(coefs[1] == 0)\n\n", "def test_lasso_lars_memory_leak():\n    rng = np.random.RandomState(0)\n    X = rng.rand(1000, 10)\n    y = rng.rand(1000)\n\n    # Perform multiple fits to check for memory leaks\n    for _ in range(50):\n        lasso_lars = linear_model.LassoLars()\n        lasso_lars.fit(X, y)\n\n\n", "    def test_lars_path_returns_empty_coef_path(self):\n        # Test case for issue 18257\n        # When all features are zero, lars path should return empty coef_path.\n\n        X = np.zeros((10, 5))\n        y = np.ones(10)\n        _, _, coef_path = linear_model.lars_path(X, y, method='lasso',\n                return_path=True)\n        assert coef_path.shape[1] == 0\n\n", "compilation error", "def test_lars_path_empty_data():\n    # Test that lars_path handles empty input data gracefully.\n    X = np.array([])\n    y = np.array([])\n    with pytest.raises(ValueError):\n        linear_model.lars_path(X, y)\n"], "sample_1122": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_684": ["    def test_repr_func_args_empty(self, tw_mock):\n        r = ReprFuncArgs([])\n        r.toterminal(tw_mock)\n        assert tw_mock.lines == ['']\n\n", "    def test_reprfunc_args_empty(self, tw_mock):\n        args = []\n        r = ReprFuncArgs(args)\n        r.toterminal(tw_mock)\n\n        assert not tw_mock.lines\n", "    def test_reprfuncargs_empty_list(self, tw_mock):\n        args = []\n        r = ReprFuncArgs(args)\n        r.toterminal(tw_mock)\n        assert tw_mock.lines == [] \n", "    def test_reprfunc_args_empty(self, tw_mock) -> None:\n        r = ReprFuncArgs([])\n        r.toterminal(tw_mock)\n        assert tw_mock.lines == ['']\n", "    def test_reprfuncArgs_empty(self, tw_mock):\n        args = []\n\n        r = ReprFuncArgs(args)\n        r.toterminal(tw_mock)\n\n        assert tw_mock.lines == []\n"], "sample_215": ["    def test_sensitive_variables_with_arguments(self):\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(sensitive_variables_with_arguments_view, check_for_POST_params=False)\n\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(sensitive_variables_with_arguments_view, check_for_POST_params=False)\n", "    def test_sensitive_variables_decorator_with_kwargs(self):\n        with self.settings(DEBUG=True):\n            self.verify_safe_response(sensitive_kwargs_view)\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(sensitive_kwargs_view, check_for_POST_params=False)\n", "    def test_sensitive_variables_with_custom_filter(self):\n        with self.settings(DEBUG=True, DEFAULT_EXCEPTION_REPORTER_FILTER='%s.CustomExceptionReporterFilter' % __name__):\n            self.verify_unsafe_response(sensitive_view_with_custom_filter, check_for_vars=False)\n", "    def test_sensitive_variables_with_default_filter(self):\n        with self.settings(DEBUG=True, DEFAULT_EXCEPTION_REPORTER_FILTER='django.views.debug.SafeExceptionReporterFilter'):\n            response = self.client.post('/sensitive_variables_view/', data={'password': 'secret'})\n            self.assertContains(response, \"password\", status_code=500)\n            \n", "    def test_sensitive_variables_without_args(self):\n        @sensitive_variables\n            cooked_eggs = 'cooked'\n            return cooked_eggs\n\n        with self.settings(DEBUG=True):\n            response = self.client.get('/test_sensitive_view/')\n            self.assertEqual(response.status_code, 500)\n            self.assertIn('cooked_eggs', str(response.content))\n\n        with self.settings(DEBUG=False):\n            response = self.client.get('/test_sensitive_view/')\n            self.assertEqual(response.status_code, 500)\n            self.assertNotIn('cooked_eggs', str(response.content))\n"], "sample_465": ["    def test_get_list_per_page(self):\n        ma = ModelAdmin(Band, self.site)\n        self.assertEqual(ma.get_list_per_page(request), 100)\n        request = MockRequest()\n        request.GET = {\"per_page\": \"50\"}\n        self.assertEqual(ma.get_list_per_page(request), 50)\n\n        class MyBandAdmin(ModelAdmin):\n            list_per_page = 25\n\n        ma = MyBandAdmin(Band, self.site)\n\n        request = MockRequest()\n        self.assertEqual(ma.get_list_per_page(request), 25)\n\n        request = MockRequest()\n        request.GET = {\"per_page\": \"10\"}\n        self.assertEqual(ma.get_list_per_page(request), 10)\n", "    def test_has_module_permission(self):\n        ma = ModelAdmin(Band, AdminSite())\n        request = MockRequest()\n        request.user = MockUser()\n        self.assertTrue(ma.has_module_permission(request))\n        request.user = self.MockViewUser()\n        self.assertTrue(ma.has_module_permission(request))\n", "    def test_has_delete_permission_with_custom_get_actions(self):\n        class TestModelAdmin(ModelAdmin):\n                return {'delete_selected': lambda: None} \n\n        class MockDeleteUser(self.MockUser):\n                return perm == \"modeladmin.delete_band\"\n\n        ma = TestModelAdmin(Band, AdminSite())\n        request = MockRequest()\n        request.user = self.MockDeleteUser()\n        self.assertTrue(ma.has_delete_permission(request))\n", "    def test_get_readonly_fields(self):\n        class BandAdmin(ModelAdmin):\n            readonly_fields = [\"name\", \"sign_date\"]\n\n        ma = BandAdmin(Band, self.site)\n        self.assertEqual(\n            ma.get_readonly_fields(request),\n            [\"name\", \"sign_date\"],\n        )\n\n        class FooAdmin(ModelAdmin):\n                return [\"bio\"]\n\n        ma = FooAdmin(Band, self.site)\n        self.assertEqual(\n            ma.get_readonly_fields(request),\n            [\"bio\"],\n        )\n", "    def test_get_actions(self):\n        class TestModelAdmin(ModelAdmin):\n            actions = ['delete_selected']\n\n        ma = TestModelAdmin(Band, self.site)\n        self.assertEqual(ma.get_actions(request), ['delete_selected'])\n"], "sample_447": ["    def test_alias_with_select_related(self):\n        qs = Book.objects.alias(rating_alias=F(\"rating\") + 1).select_related(\"publisher\")\n        book = qs.first()\n        self.assertIs(hasattr(book, \"rating_alias\"), False)\n        self.assertIs(hasattr(book, \"publisher\"), True)\n", "    def test_alias_multiple_levels(self):\n        qs = Book.objects.alias(\n            pubdate_year_alias=F(\"pubdate__year\"),\n        ).alias(\n            pubdate_month_alias=F(\"pubdate_year_alias\")\n        ).filter(pubdate_month_alias=1991)\n        self.assertSequenceEqual(qs, [self.b4])\n\n", "    def test_values_list_alias(self):\n        qs = Book.objects.alias(rating_alias=F(\"rating\") - 1)\n        msg = \"Cannot select the 'rating_alias' alias. Use annotate() to promote it.\"\n        with self.assertRaisesMessage(FieldError, msg):\n            qs.values_list(\"rating_alias\")\n", "    def test_alias_subquery_in_filter(self):\n        qs = (\n            Book.objects.alias(\n                is_popular=Subquery(\n                    Book.objects.filter(rating__gt=4).values(\"id\"), output_field=ExistsField()\n                )\n            )\n            .filter(is_popular=True)\n        )\n        self.assertCountEqual(\n            qs, [self.b1],\n        )\n\n", "    def test_alias_f_transform_aggregation(self):\n        qs = (\n            Book.objects.filter(\n                pubdate__year__in=[2007, 2008],\n            )\n            .alias(\n                pubdate_year_alias=ExtractYear(\"pubdate\"),\n            )\n            .values(\"pubdate_year_alias\")\n            .annotate(\n                max_rating=Max(\"rating\"),\n            )\n        )\n\n        self.assertCountEqual(\n            qs,\n            [\n                {\"pubdate_year_alias\": 2007, \"max_rating\": 4.5},\n                {\"pubdate_year_alias\": 2008, \"max_rating\": 4.0},\n            ],\n        )\n"], "sample_495": ["    def test_paginator_with_custom_page_class(self):\n        class CustomPage(Page):\n                super().__init__(object_list, number, paginator)\n                self.custom_attribute = 'this is custom'\n\n        paginator = Paginator(Article.objects.all(), 5, page_class=CustomPage)\n        page = paginator.page(1)\n\n        self.assertIsInstance(page, CustomPage)\n        self.assertEqual(page.custom_attribute, 'this is custom')\n", "    def test_empty_paginator(self):\n        paginator = Paginator([], 5)\n        self.assertEqual(paginator.num_pages, 0)\n        self.assertEqual(len(list(paginator.page_range)), 0)\n        with self.assertRaises(EmptyPage):\n            paginator.page(1)\n\n", "    def test_empty_paginator_has_next_previous(self):\n        paginator = Paginator([], per_page=2)\n        self.assertFalse(paginator.page(1).has_next())\n        self.assertFalse(paginator.page(1).has_previous())\n", "    def test_get_page_number_out_of_range(self):\n        paginator = Paginator(Article.objects.order_by('id'), 5)\n        with self.assertRaises(EmptyPage):\n            paginator.get_page(3)\n", "    def test_empty_paginator_with_allow_empty_first_page_false(self):\n        paginator = Paginator([], 2, allow_empty_first_page=False)\n        with self.assertRaises(EmptyPage):\n            paginator.page(1)\n"], "sample_78": ["    def test_normalize_path_patterns(self):\n        self.assertListEqual(normalize_path_patterns(['foo/bar/baz', 'baz']), ['foo/bar/baz', 'baz'])\n        self.assertListEqual(normalize_path_patterns(['*/baz'], True), ['*/baz'])\n", "    def test_normalize_path_patterns(self):\n        self.assertEqual(normalize_path_patterns('foo/bar/baz'), ['foo/bar/baz'])\n        self.assertEqual(normalize_path_patterns(['foo/bar/baz']), ['foo/bar/baz'])\n        self.assertEqual(normalize_path_patterns('foo/bar/baz', 'qux'), ['foo/bar/baz', 'qux'])\n        self.assertEqual(normalize_path_patterns(['foo/bar/baz', 'qux']), ['foo/bar/baz', 'qux'])\n", "    def test_find_commands_with_empty_path(self):\n        \"\"\"\n        find_commands should return an empty list if the given path is empty.\n        \"\"\"\n        self.assertEqual(find_commands(\"\"), [])\n", "    def test_normalize_path_patterns(self):\n        self.assertEqual(normalize_path_patterns(['foo/bar/baz', 'baz']), ['foo/bar/baz', 'baz'])\n        self.assertEqual(normalize_path_patterns(['*/baz']), ['*/baz'])\n        self.assertEqual(normalize_path_patterns(['foo*']), ['foo*'])\n        self.assertEqual(normalize_path_patterns(['foo\\\\*']), ['foo*'])\n", "    def test_normalize_path_patterns_absolute(self):\n        patterns = ['/foo/bar/baz', '/foo/bar/bat']\n        normalized_patterns = normalize_path_patterns(patterns)\n        self.assertEqual(normalized_patterns, ['foo/bar/baz', 'foo/bar/bat'])\n"], "sample_1176": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_312": ["    def test_add_squash_same_connector(self):\n        node = Node([('a', 1), ('b', 2)], 'AND')\n        node2 = Node([('c', 3), ('d', 4)], 'AND')\n        combined = node.add(node2, 'AND')\n        self.assertEqual(len(combined), 4)\n        self.assertEqual(combined.children, [('a', 1), ('b', 2), ('c', 3), ('d', 4)])\n\n", "    def test_add_squash(self):\n        node1 = Node([('a', 1), ('b', 2)], 'AND')\n        node2 = Node([('c', 3), ('d', 4)], 'AND')\n        # Squashing AND nodes\n        self.assertEqual(node1.add(node2, 'AND'), node2)\n        self.assertEqual(len(node1), 4)\n\n", "    def test_add_squash(self):\n        node1 = Node([('a', 1), ('b', 2)], 'AND')\n        node2 = Node([('c', 3), ('d', 4)], 'AND')\n        node3 = Node([('a', 1), ('b', 2)], 'AND')\n        node3.add(node2, 'AND')\n        expected = Node([('a', 1), ('b', 2), ('c', 3), ('d', 4)], 'AND')\n        self.assertEqual(node3, expected)\n\n\n", "    def test_add_squash_same_connector(self):\n        node1 = Node([('a', 1), ('b', 2)], 'AND')\n        node2 = Node([('c', 3), ('d', 4)], 'AND')\n        expected_node = Node([('a', 1), ('b', 2), ('c', 3), ('d', 4)], 'AND')\n        self.assertEqual(node1.add(node2, 'AND'), node2)\n        self.assertEqual(node1, expected_node)\n", "    def test_add_squash(self):\n        node1 = Node([('a', 1), ('b', 2)], 'AND')\n        node2 = Node([('c', 3), ('d', 4)], 'AND')\n        node3 = Node([node1, node2], 'AND')\n        self.assertEqual(len(node3), 4)\n        self.assertEqual(str(node3), \"(AND: (AND: ('a', 1), ('b', 2)), (AND: ('c', 3), ('d', 4)))\")\n        node3.add(node2, 'AND')\n        self.assertEqual(len(node3), 4)\n        self.assertEqual(str(node3), \"(AND: ('a', 1), ('b', 2), ('c', 3), ('d', 4))\")\n"], "sample_436": ["    def test_completions(self):\n        args = [\"completions\", \"--shell=bash\", \"--settings=test_project.settings\"]\n        out, err = self.run_django_admin(args)\n        self.assertNoOutput(err)\n        self.assertOutput(\n            out,\n            \"load_command_completions() { local SETTINGS_MODULE=${1}; python -m django completions --shell=bash --settings=$SETTINGS_MODULE; };\",\n\n        )\n", "    def test_invalid_help_subcommand(self):\n        out, err = self.run_django_admin([\"help\", \"invalid_subcommand\"])\n        self.assertNoOutput(out)\n        self.assertOutput(err, \"Unknown command: 'invalid_subcommand'\")\n", "    def test_missing_settings_file(self):\n        \"\"\"\n        Run django-admin with missing settings file should show a helpful error,\n        not a traceback\n        \"\"\"\n        args = [\"runserver\"]\n        out, err = self.run_django_admin(args)\n        self.assertNoOutput(out)\n        self.assertIn(\n            \"Error: The settings file 'settings.py' doesn't exist.\", err\n        )\n", "    def test_template_with_non_ascii_chars(self):\n        \"\"\"Test template rendering with non-ascii characters.\"\"\"\n        template_path = os.path.join(custom_templates_dir, \"project_template\")\n        args = [\n            \"startproject\",\n            \"--template\",\n            template_path,\n            \"--extension=txt\",\n            \"customtestproject\",\n        ]\n        testproject_dir = os.path.join(self.test_dir, \"customtestproject\")\n\n        out, err = self.run_django_admin(args)\n        self.assertNoOutput(err)\n        self.assertTrue(os.path.isdir(testproject_dir))\n        path = os.path.join(testproject_dir, \"ticket-18091-non-ascii-template.txt\")\n        with open(path, encoding=\"utf-8\") as f:\n            self.assertEqual(\n                f.read().splitlines(False),\n                [\"Some non-ASCII text for testing ticket #18091:\", \"\u00fc\u00e4\u00f6 \u20ac\"],\n            )\n", "    def test_invalid_settings_path(self):\n        args = [\"startapp\", \"new_app\", \"--settings=invalid_path\"]\n        out, err = self.run_django_admin(args)\n        self.assertOutput(\n            err,\n            \"Error: Settings path 'invalid_path' does not exist.\",\n        )\n"], "sample_136": ["    def test_getlist_multiple_values(self):\n        environ = {\n            'HTTP_COOKIE': 'name1=value1; name2=value2',\n        }\n        headers = HttpHeaders(environ)\n        self.assertEqual(headers.getlist('Cookie'), ['name1=value1', 'name2=value2'])\n", "    def test_getlist(self):\n        environ = {\n            'HTTP_COOKIE': 'foo=bar; baz=qux',\n            'HTTP_ACCEPT': 'text/html,application/xhtml+xml',\n        }\n        headers = HttpHeaders(environ)\n        self.assertEqual(headers.getlist('Cookie'), ['foo=bar; baz=qux'])\n        self.assertEqual(headers.getlist('Accept'), ['text/html', 'application/xhtml+xml'])\n", "    def test_getlist(self):\n        environ = {\n            'COOKIE': 'chocolate=chip; milk=whole',\n            'HTTP_ACCEPT_LANGUAGE': 'en-US,en;q=0.5',\n        }\n        headers = HttpHeaders(environ)\n        self.assertEqual(headers.getlist('Cookie'), ['chocolate=chip', 'milk=whole'])\n        self.assertEqual(headers.getlist('Accept-Language'), ['en-US', 'en'])\n", "    def test_getlist(self):\n        environ = {\n            'CONTENT_TYPE': 'text/html',\n            'CONTENT_LENGTH': '100',\n            'HTTP_COOKIE': 'foo=bar; baz=quux',\n        }\n        headers = HttpHeaders(environ)\n        self.assertEqual(headers.getlist('Cookie'), ['foo=bar; baz=quux'])\n", "    def test_getlist(self):\n        environ = {\n            'CONTENT_TYPE': 'text/html',\n            'CONTENT_LENGTH': '100',\n            'HTTP_HOST': 'example.com',\n            'HTTP_COOKIE': 'foo=bar; another=cookie',\n        }\n        headers = HttpHeaders(environ)\n        self.assertEqual(list(headers.getlist('Cookie')), ['foo=bar; another=cookie'])\n        self.assertEqual(list(headers.getlist('Set-Cookie')), [])\n"], "sample_579": ["    def test_row_col_colors_df_empty(self):\n        kws = self.default_kws.copy()\n        kws['row_colors'] = pd.DataFrame({'row_annot': []},\n                index=pd.Index([]))\n        kws['col_colors'] = pd.DataFrame({'col_annot': []},\n                index=pd.Index([]))\n\n        cm = mat.clustermap(self.df_norm, **kws)\n\n        assert cm.row_color_labels == ['row_annot']\n        assert cm.col_color_labels == ['col_annot']\n        assert len(cm.ax_row_colors.collections) == 0\n        assert len(cm.ax_col_colors.collections) == 0\n\n", "    def test_colors_list_input_to_matrix_and_cmap(self):\n        matrix, cmap = mat.ClusterGrid.color_list_to_matrix_and_cmap(\n            self.col_colors, self.x_norm_leaves, axis=0\n        )\n        for i, leaf in enumerate(self.x_norm_leaves):\n            color = self.col_colors[leaf]\n            assert_colors_equal(cmap(matrix[i, 0]), color)\n", "    def test_dendrogram_kwargs(self):\n        kws = self.default_kws.copy()\n        kws[\"dendrogram_kws\"] = {\"color\": \"green\", \"linewidth\": 2}\n\n        g = mat.clustermap(self.df_norm, **kws)\n        for ax in [g.ax_col_dendrogram, g.ax_row_dendrogram]:\n            for line in ax.get_lines():\n                assert line.get_color() == \"green\"\n                assert line.get_linewidth() == 2\n", "    def test_pass_axis_labels(self):\n        kws = self.default_kws.copy()\n        kws['row_cluster'] = False\n        kws['col_cluster'] = False\n\n        g = mat.clustermap(self.df_norm, **kws)\n\n        g = mat.clustermap(\n            self.df_norm,\n            row_cluster=False,\n            col_cluster=False,\n            col_linkage='complete',\n            row_linkage='average',\n            xticklabels=list(self.df_norm.columns),\n            yticklabels=list(self.df_norm.index),\n        )\n\n        xtl_actual = [t.get_text() for t in g.ax_heatmap.get_xticklabels()]\n        ytl_actual = [t.get_text() for t in g.ax_heatmap.get_yticklabels()]\n\n        assert xtl_actual == list(self.df_norm.columns)\n        assert ytl_actual == list(self.df_norm.index)\n", "compilation error"], "sample_98": ["    def test_broken_pipe_handling(self):\n        conn = HTTPConnection(LiveServerViews.server_thread.host, LiveServerViews.server_thread.port)\n        try:\n            conn.request('GET', '/example_view/')\n            conn.close()  # Simulate a broken pipe\n            # Ensure the server doesn't crash and can handle further requests\n            conn = HTTPConnection(LiveServerViews.server_thread.host, LiveServerViews.server_thread.port)\n            conn.request('GET', '/example_view/')\n            response = conn.getresponse()\n            self.assertEqual(response.status, 200)\n            self.assertEqual(response.read(), b'example view')\n\n        finally:\n            conn.close()\n\n", "    def test_multiple_requests_concurrent(self):\n        from threading import Thread\n\n            with self.urlopen(url) as f:\n                return f.read()\n\n        threads = [\n            Thread(target=lambda: fetch('/example_view/')),\n            Thread(target=lambda: fetch('/media/example_media_file.txt')),\n        ]\n        for thread in threads:\n            thread.start()\n        for thread in threads:\n            thread.join()\n", "    def test_threaded_view_calls_subview(self):\n        url = '/threaded/subview_calling_view/?%s' % urlencode({'url': self.live_server_url})\n        with self.urlopen(url) as f:\n            self.assertEqual(f.read(), b'subview calling view: subview')\n\n", "    def test_handle_broken_pipe(self):\n        conn = HTTPConnection(LiveServerViews.server_thread.host, LiveServerViews.server_thread.port)\n        try:\n            conn.request('GET', '/example_view/')\n            conn.close()\n            with self.assertRaises(socket.error) as err:\n                conn.getresponse()\n            self.assertEqual(err.exception.errno, errno.EPIPE)\n        finally:\n            conn.close()\n", "    def test_broken_pipe_error_handled(self):\n        \"\"\"\n        Test if broken pipe errors are silently ignored.\n        \"\"\"\n        conn = HTTPConnection(LiveServerViews.server_thread.host, LiveServerViews.server_thread.port)\n        try:\n            conn.request('GET', '/example_view/')\n            conn.close()\n            conn.getresponse()  # This should raise a BrokenPipeError\n        except socket.error as e:\n            self.assertEqual(type(e), socket.error)\n            self.assertEqual(e.errno, errno.EPIPE)\n        except Exception as e:\n            self.fail(f\"Unexpected error: {e}\") \n"], "sample_294": ["    def test_csrf_cookie_samesite_strict_to_none(self):\n        req = self._get_GET_no_csrf_cookie_request()\n        with self.settings(CSRF_COOKIE_NAME='csrfcookie', CSRF_COOKIE_SAMESITE='Strict'):\n            mw = CsrfViewMiddleware(token_view)\n            mw.process_view(req, token_view, (), {})\n            resp = mw(req)\n            csrf_cookie = resp.cookies['csrfcookie']\n            self.assertEqual(csrf_cookie['samesite'], 'Strict')\n            \n        with self.settings(CSRF_COOKIE_NAME='csrfcookie', CSRF_COOKIE_SAMESITE=None):\n            req = self._get_GET_no_csrf_cookie_request()\n            mw = CsrfViewMiddleware(token_view)\n            mw.process_view(req, token_view, (), {})\n            resp = mw(req)\n            csrf_cookie = resp.cookies['csrfcookie']\n            self.assertFalse('samesite' in csrf_cookie)   \n", "    def test_good_origin_trusted_origins_wildcard(self):\n        \"\"\"\n        A POST HTTPS request with a good origin containing a wildcard is accepted if the request origin matches.\n        \"\"\"\n        req = self._get_POST_request_with_token()\n        req._is_secure_override = True\n        req.META['HTTP_HOST'] = 'www.example.com'\n        req.META['HTTP_ORIGIN'] = 'https://subdomain.example.com'\n        mw = CsrfViewMiddleware(post_form_view)\n        self.assertIs(mw._origin_verified(req), True)\n        resp = mw.process_view(req, post_form_view, (), {})\n        self.assertIsNone(resp)\n", "    def test_csrf_cookie_age_is_set_correctly(self):\n        req = self._get_GET_no_csrf_cookie_request()\n        mw = CsrfViewMiddleware(token_view)\n        mw.process_view(req, token_view, (), {})\n        resp = mw(req)\n        csrf_cookie = resp.cookies.get(settings.CSRF_COOKIE_NAME)\n        self.assertEqual(csrf_cookie['max-age'], 60)\n", "    def test_csrf_cookie_secure_unsecure_session_cookie(self):\n        \"\"\"\n        CSRF cookie is secure even if session cookie is not.\n        \"\"\"\n        req = self._get_GET_no_csrf_cookie_request()\n        mw = CsrfViewMiddleware(ensure_csrf_cookie_view)\n        mw.process_view(req, ensure_csrf_cookie_view, (), {})\n        resp = mw(req)\n        self.assertTrue(resp.cookies.get(settings.CSRF_COOKIE_NAME, False)['secure'])\n        self.assertFalse('secure' in resp.cookies.get(SESSION_COOKIE_NAME, {}))\n\n", "    def test_session_cookie_domain_mismatch(self):\n        \"\"\"\n        A request with a session cookie from a different domain should be rejected.\n        \"\"\"\n        req = self._get_GET_no_csrf_cookie_request()\n        req.session[CSRF_SESSION_KEY] = self._csrf_id_cookie\n        req.COOKIES[settings.SESSION_COOKIE_NAME] = 'session_id'\n        with self.assertLogs('django.security.csrf', 'WARNING') as cm:\n            mw = CsrfViewMiddleware(token_view)\n            response = mw.process_view(req, token_view, (), {})\n        self.assertEqual(response.status_code, 403)\n        self.assertEqual(cm.records[0].getMessage(), 'Forbidden: %s' % REASON_CSRF_TOKEN_MISSING)\n\n"], "sample_255": ["    def test_get_environ(self):\n        request = self.request_factory.get('/')\n        handler = WSGIRequestHandler(request, '192.168.0.2', None)\n        environ = handler.get_environ()\n        request.META['HTTP_X_SOME_HEADER'] = 'value'\n        request.META['HTTP_X_SOME_HEADER_WITH_UNDERSCORE'] = 'value'\n\n        self.assertNotIn('HTTP_X_SOME_HEADER_WITH_UNDERSCORE', environ)\n        self.assertIn('HTTP_X_SOME_HEADER', environ)\n", "    def test_handle(self):\n        request = WSGIRequest(self.request_factory.get('/').environ)\n        request.makefile = lambda *args, **kwargs: BytesIO()\n        handler = WSGIRequestHandler(request, '192.168.0.2', None)\n        handler.rfile = BytesIO(b\"GET / HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n\")\n        handler.close_connection = False\n        handler.handle_one_request()\n        self.assertTrue(handler.close_connection)\n", "    def test_get_environ_forwards_connection_close(self):\n        request = self.request_factory.get('/')\n        request.META['Connection'] = 'close'\n        handler = WSGIRequestHandler(Stub(), '192.168.0.2', None)\n        environ = handler.get_environ()\n        self.assertEqual(environ.get('HTTP_CONNECTION'), 'close')\n", "    def test_handle_one_request(self):\n        request = WSGIRequest(self.request_factory.get('/').environ)\n        request.makefile = lambda *args, **kwargs: BytesIO()\n        handler = WSGIRequestHandler(request, '192.168.0.2', None)\n        handler.close_connection = True\n        handler.server = Stub(get_app=lambda: lambda *args: ('Hello World', []))\n        with captured_stderr() as stderr:\n            handler.handle_one_request()\n        self.assertEqual(stderr.getvalue(), '')\n        self.assertEqual(request.read(), b'Hello World')\n", "    def test_get_environ(self):\n        request = self.request_factory.get('/')\n        request.META['HTTP_X_FOO_BAR'] = 'baz_qux'\n        request.META['HTTP_X_FOO_UNDERSCORE'] = 'should_be_stripped'\n\n        handler = WSGIRequestHandler(Stub(), '192.168.0.2', None)\n        environ = handler.get_environ(request)\n\n        self.assertIn('HTTP_X_FOO_BAR', environ)\n        self.assertEqual(environ['HTTP_X_FOO_BAR'], 'baz_qux')\n        self.assertNotIn('HTTP_X_FOO_UNDERSCORE', environ)\n"], "sample_185": ["    def test_watch_for_translation_changes_new_locale_path(self):\n        mocked_sender = mock.MagicMock()\n        watch_for_translation_changes(mocked_sender)\n        mocked_sender.watch_dir.assert_any_call(Path(os.path.join(here, 'locale')), '**/*.mo')\n", "    def test_doesnt_crash_on_missing_po_files(self):\n        gettext_module._translations = {'foo': 'bar'}\n        trans_real._translations = {'foo': 'bar'}\n\n        with mock.patch('django.utils.translation.gettext_module.find', return_value=None):\n            path = Path('test.mo')\n            self.assertIs(translation_file_changed(None, path), True)\n", "    def test_ignores_changes_in_ignored_directories(self):\n        gettext_module._translations = {'foo': 'bar'}\n        trans_real._translations = {'foo': 'bar'}\n        path = Path('test/locale/en/LC_MESSAGES/django.mo')\n        self.assertIsNone(translation_file_changed(None, path))\n        self.assertEqual(gettext_module._translations, {'foo': 'bar'})\n        self.assertEqual(trans_real._translations, {'foo': 'bar'})\n", "    def test_locale_paths_updated(self):\n        mocked_sender = mock.MagicMock()\n        locale_dir = Path(os.path.join(here, 'locale'))\n        with tempfile.TemporaryDirectory() as temp_dir:\n            with self.settings(LOCALE_PATHS=[temp_dir]):\n                watch_for_translation_changes(mocked_sender)\n                self.assertEqual(mocked_sender.watch_dir.call_count, 1)\n                mocked_sender.watch_dir.assert_called_once_with(Path(temp_dir), '**/*.mo')\n", "    def test_no_reloading_on_same_hash(self):\n        # Simulate a translation file with a specific hash\n        hash = 'abcdef1234567890' \n        path = Path('test.mo')\n            return hash\n        with mock.patch('django.utils.translation.get_file_hash', mock_get_file_hash):\n            translation_file_changed(None, path)\n\n            # Assuming a reload has already occurred, the cache is populated\n            self.assertIn('test', gettext_module._translations)\n\n            # Simulate a second change, but with the same hash\n            with mock.patch('django.utils.translation.get_file_hash', mock_get_file_hash):\n                reload_happened = translation_file_changed(None, path)\n            \n            self.assertFalse(reload_happened)\n            self.assertIn('test', gettext_module._translations)  \n"], "sample_649": ["    def test_disable_loggers_with_wildcard(testdir):\n        testdir.makepyfile(\n            \"\"\"\n        import logging\n\n        logger1 = logging.getLogger('logger1')\n        logger2 = logging.getLogger('logger2')\n\n            with caplog.at_level(logging.DEBUG):\n                logger1.warning(\"log message from logger1\")\n                logger2.warning(\"log message from logger2\")\n                assert len(caplog.record_tuples) == 1\n                assert caplog.record_tuples[0][0] == 'logger2'\n        \"\"\"\n        )\n\n        result = testdir.runpytest(\"--log-disable=logger*\", \"-s\")\n        assert result.ret == ExitCode.OK\n        assert not result.stderr.lines\n", "    def test_disable_loggers_with_wildcard_does_not_propagate(testdir):\n        testdir.makepyfile(\n            \"\"\"\n        import logging\n        import os\n\n        parent_logger = logging.getLogger(\"parent\")\n        child_logger = parent_logger.getChild(\"child\")\n\n                with caplog.at_level(logging.DEBUG):\n                    parent_logger.warning(\"some parent logger message\")\n                    child_logger.warning(\"some child logger message\")\n                    assert len(caplog.record_tuples) == 1\n                    assert caplog.record_tuples[0][0] == \"parent\"\n                    assert caplog.record_tuples[0][2] == \"some parent logger message\"\n        \"\"\"\n        )\n\n        result = testdir.runpytest(\"--log-disable=parent.*\", \"-s\")\n        assert result.ret == ExitCode.OK\n        assert not result.stderr.lines\n", "def test_log_file_level_with_hierarchy(pytester):\n    log_file = str(pytester.path.joinpath(\"pytest.log\"))\n\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_file={}\n        log_file_level = INFO\n        log_cli=true\n        \"\"\".format(\n            log_file\n        )\n    )\n    pytester.makeconftest(\n        \"\"\"\n        import logging\n        logger = logging.getLogger(__name__)\n\n            logger.info(\"logreport\")\n    \"\"\"\n    )\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n        logger = logging.getLogger(__name__)\n\n            logger.debug(\"debug message in test_first\")\n            logger.info(\"info message in test_first\")\n            assert True\n        \"\"\"\n    )\n    pytester.runpytest()\n    with open(log_file) as rfh:\n        contents = rfh.read()\n    assert \"debug message in test_first\" not in contents\n    assert \"info message in test_first\" in contents\n", "def test_disable_loggers_wildcard(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import os\n\n        logger_a = logging.getLogger(\"a\")\n        logger_b = logging.getLogger(\"b\")\n\n            with caplog.at_level(logging.DEBUG):\n                logger_a.warning(\"message from a\")\n                logger_b.warning(\"message from b\")\n                assert len(caplog.record_tuples) == 1\n                assert caplog.record_tuples[0][0] == \"b\"\n                assert caplog.record_tuples[0][2] == \"message from b\"\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--log-disable=a*\", \"-s\")\n    assert result.ret == ExitCode.OK\n", "    def test_disable_loggers_with_wildcard(testdir):\n        testdir.makepyfile(\n            \"\"\"\n        import logging\n        import os\n        log1 = logging.getLogger('test.log1')\n        log2 = logging.getLogger('test.log2')\n\n            with caplog.at_level(logging.DEBUG):\n                log1.warning(\"log1 message\")\n                log2.warning(\"log2 message\")\n                assert len(caplog.record_tuples) == 0\n         \"\"\"\n        )\n        result = testdir.runpytest(\"--log-disable=test.log*\", \"-s\")\n        assert result.ret == ExitCode.OK\n        assert not result.stderr.lines\n"], "sample_344": ["    def test_proxy_fk(self):\n        A = self.create_model(\"A\", foreign_keys=[models.ForeignKey('B', models.CASCADE)])\n        B = self.create_model(\"B\", bases=(A,), proxy=True)\n        C = self.create_model(\"C\", bases=(B,), proxy=True)\n        self.assertRelated(A, [B, C])\n        self.assertRelated(B, [A, C])\n        self.assertRelated(C, [A, B])\n", "    def test_proxy_fk_through_base(self):\n        A = self.create_model(\"A\", foreign_keys=[models.ForeignKey('B', models.CASCADE)])\n        B = self.create_model(\"B\")\n        C = self.create_model(\"C\", bases=(B,), proxy=True)\n        self.assertRelated(A, [B, C])\n        self.assertRelated(B, [A, C])\n        self.assertRelated(C, [A, B])\n", "    def test_proxy_fk(self):\n        A = self.create_model(\"A\")\n        B = self.create_model(\"B\", bases=(A,), proxy=True, foreign_keys=[models.ForeignKey('C', models.CASCADE)])\n        C = self.create_model(\"C\")\n        self.assertRelated(A, [B, C])\n        self.assertRelated(B, [C])\n        self.assertRelated(C, [A, B])\n", "    def test_proxy_fk_through_concrete(self):\n        A = self.create_model(\"A\")\n        B = self.create_model(\"B\", bases=(A,), proxy=True)\n        C = self.create_model(\"C\", foreign_keys=[models.ForeignKey('B', models.CASCADE)])\n        self.assertRelated(A, [B, C])\n        self.assertRelated(B, [C])\n        self.assertRelated(C, [A, B])\n", "    def test_proxy_fk(self):\n        A = self.create_model(\"A\", foreign_keys=[models.ForeignKey('B', models.CASCADE)])\n        B = self.create_model(\"B\", bases=(models.Model,), proxy=True)\n        self.assertRelated(A, [B])\n        self.assertRelated(B, [A])\n"], "sample_444": ["    def test_manifest_file_updated(self):\n        self.run_collectstatic()\n        manifest_path = os.path.join(settings.STATIC_ROOT, storage.staticfiles_storage.manifest_name)\n        original_mtime = os.path.getmtime(manifest_path)\n\n        # Touch a file to trigger a change\n        with open(self._get_filename_path(\"styles.css\"), 'a') as f:\n            f.write('')\n\n        self.run_collectstatic()\n        updated_mtime = os.path.getmtime(manifest_path)\n\n        self.assertNotEqual(original_mtime, updated_mtime)\n\n", "    def test_manifest_file_updates(self):\n        file_path = self._get_filename_path(\"file.css\")\n        with open(file_path, \"w\") as f:\n            f.write(\"\")\n\n        # Let the manifest be built the first time.\n        self.run_collectstatic()\n\n        # Make a change to the file.\n        with open(file_path, \"a\") as f:\n            f.write(\"body { background-color: red; }\")\n\n        # Collect again, the manifest should update.\n        self.run_collectstatic()\n        \n        manifest_file = storage.staticfiles_storage.path(\n            storage.staticfiles_storage.manifest_name\n        )\n        with open(manifest_file, \"r\") as f:\n            manifest_content = json.load(f)\n        hashed_filename = storage.staticfiles_storage.hashed_name(\"file.css\") \n\n        self.assertIn(hashed_filename, manifest_content) \n", "    def test_hashed_files_cache(self):\n        self.run_collectstatic()\n        hashed_files = storage.staticfiles_storage.hashed_files\n        # Check if hashed_files were populated correctly.\n        self.assertGreater(len(hashed_files), 0)\n\n        # Modify a static file to trigger a rehash.\n        with open(self._get_filename_path(\"test.css\"), \"a\") as f:\n            f.write(\"\\n/* Modified */\")\n\n        # Run collectstatic again to rehash the file.\n        self.run_collectstatic()\n\n        # Check if the cache was cleared and repopulated.\n        self.assertEqual(hashed_files, storage.staticfiles_storage.hashed_files)\n        self.assertIn(\n            \"test.css\", storage.staticfiles_storage.hashed_files\n        )\n\n        # Verify that the hashed name reflects the modification.\n        self.assertNotEqual(\n            self.hashed_file_path(\"test.css\"),\n            self.hashed_file_path(\"test.css\"),\n        )\n\n", "def test_manifest_version(self):\n    self.run_collectstatic()\n    manifest, _ = storage.staticfiles_storage.load_manifest()\n    self.assertEqual(manifest[\"version\"], storage.staticfiles_storage.manifest_version)\n", "    def test_manifest_strict_mode(self):\n        # Create a file that's not part of the manifest but exists on disk.\n        os.makedirs(os.path.join(self._temp_dir, \"test\", \"extra\"))\n        with open(\n            os.path.join(self._temp_dir, \"test\", \"extra\", \"extra_file.txt\"), \"w\"\n        ) as f:\n            f.write(\"Content\")\n\n        self.run_collectstatic()\n\n        # Try to access the file when strict mode is enabled.\n        storage.staticfiles_storage.manifest_strict = True\n        with self.assertRaises(ValueError) as e:\n            self.hashed_file_path(\"extra/extra_file.txt\")\n        self.assertIn(\n            \"Missing staticfiles manifest entry for 'extra/extra_file.txt'\",\n            str(e.exception),\n        )\n\n        # Try to access the file when strict mode is disabled.\n        storage.staticfiles_storage.manifest_strict = False\n        self.assertEqual(\n            self.hashed_file_path(\"extra/extra_file.txt\"),\n            \"extra/extra_file.txt\",\n        )\n"], "sample_170": ["    def test_sensitive_variables_with_arguments(self):\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(sensitive_variables_with_arguments, check_for_POST_params=False)\n\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(sensitive_variables_with_arguments, check_for_POST_params=False)\n", "    def test_sensitive_variables_multiple_decorators(self):\n        \"\"\"\n        It's possible to use both @sensitive_variables and\n        @sensitive_post_parameters decorators on the same function.\n        \"\"\"\n        with self.settings(DEBUG=True):\n            self.verify_safe_response(multiple_decorators, check_for_vars=False)\n\n        with self.settings(DEBUG=False):\n            self.verify_paranoid_response(multiple_decorators, check_for_vars=False)\n", "    def test_sensitive_variables_with_positional_args(self):\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(sensitive_variables_with_positional_args_view, check_for_vars=False)\n\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(sensitive_variables_with_positional_args_view, check_for_vars=False)\n", "    def test_sensitive_variables_with_kwargs(self):\n        with self.settings(DEBUG=True):\n            response = self.client.get('/sensitive-variables-kwargs/')\n            self.assertEqual(response.status_code, 500)\n            self.assertContains(response, 'cooked_eggs', status_code=500)\n            self.assertNotContains(response, 'worcestershire', status_code=500)\n", "    def test_sensitive_variables_decorator_with_kwargs(self):\n        with self.settings(DEBUG=True):\n            self.verify_safe_response(sensitive_args_kwargs_view, check_for_POST_params=False)\n"], "sample_1093": ["    def test_SymPyPrinter_piecewise():\n        from sympy import Piecewise\n\n        p = SymPyPrinter()\n\n        expr = Piecewise((1, Eq(x, 0)), (2, x > 0), (3, True))\n        assert p.doprint(expr) == 'Piecewise((1, Eq(x, 0)), (2, x > 0), (3, True))'\n\n", "def test_SymPyPrinter():\n    p = SymPyPrinter()\n\n    assert p.doprint(S.Exp1) == 'sympy.E'\n    assert p.doprint(S.Pi) == 'sympy.pi'\n    assert p.doprint(S.GoldenRatio) == 'sympy.GoldenRatio'\n    assert p.doprint(S.EulerGamma) == 'sympy.EulerGamma'\n    assert p.doprint(S.NaN) == 'sympy.nan'\n    assert p.doprint(S.Infinity) == 'sympy.oo'\n    assert p.doprint(S.NegativeInfinity) == '-sympy.oo'\n", "    def test_SymPyPrinter_print_piecewise():\n        p = SymPyPrinter()\n        expr = Piecewise((1, Eq(x, 0)), (2, x > 6))\n        assert p.doprint(expr) == 'Piecewise((1, Eq(x, 0)), (2, x > 6))'\n", "compilation error", "def test_SymPyPrinter_print_seq():\n    s = SymPyPrinter()\n\n    assert s._print_seq(range(2)) == '(0, 1)'\n"], "sample_723": ["def test_imputation_empty_array():\n    # Test imputation with an empty array.\n    X = np.array([])\n    X = X.reshape((0, 10))\n\n    imputer = SimpleImputer(missing_values=0, strategy=\"mean\")\n    imputed_X = imputer.fit_transform(X)\n    assert_array_equal(imputed_X, np.array([]).reshape((0, 10)))\n\n    imputer = SimpleImputer(missing_values=0, strategy=\"median\")\n    imputed_X = imputer.fit_transform(X)\n    assert_array_equal(imputed_X, np.array([]).reshape((0, 10)))\n\n    imputer = SimpleImputer(missing_values=0, strategy=\"most_frequent\")\n    imputed_X = imputer.fit_transform(X)\n    assert_array_equal(imputed_X, np.array([]).reshape((0, 10)))\n\n\n\n", "def test_imputation_sparse_zero_columns():\n    # Test imputation on a sparse matrix with zero-filled columns.\n    X = sparse.csr_matrix(np.array([\n        [0, 0, 1],\n        [0, 0, 2],\n        [0, 0, 3],\n    ]))\n    X_true = sparse.csr_matrix(np.array([\n        [0, 0, 1],\n        [0, 0, 2],\n        [0, 0, 3],\n    ]))\n\n    for strategy in [\"mean\", \"median\", \"most_frequent\"]:\n        _check_statistics(X, X_true, strategy,\n                          [np.nan, np.nan, 1], 0)\n", "def test_imputation_empty_array():\n    # Test imputation on empty arrays.\n\n    for strategy in [\"mean\", \"median\", \"most_frequent\"]:\n        for axis in [0, 1]:\n            imputer = SimpleImputer(strategy=strategy, axis=axis)\n            X_empty = np.array([])\n\n            if axis == 0:\n                X_empty = X_empty.reshape((0, 1))\n\n            assert_raises(ValueError, imputer.fit, X_empty)\n", "    def test_imputation_sparse_zero_strategy():\n        # Test imputation with a sparse matrix for the \"zero\" strategy,\n        # where missing values are 0. This should result in no change to the\n        # matrix.\n\n        X = sparse.csc_matrix([[1, 0, 0],\n                               [0, 2, 0],\n                               [0, 0, 3]])\n\n        imputer = SimpleImputer(missing_values=0, strategy='most_frequent')\n        X_transformed = imputer.fit_transform(X.copy())\n\n        assert_array_equal(X.toarray(), X_transformed)\n", "def test_imputation_missing_values_integer():\n    # Test imputation with missing_values as an integer.\n    X = np.array([[1, 2, 3],\n                  [4, 0, 6],\n                  [7, 8, 9],])\n\n    X[1, 1] = -1\n\n    X_true = np.array([[1, 2, 3],\n                       [4, 5, 6],\n                       [7, 8, 9],])\n\n    imputer = SimpleImputer(missing_values=-1, strategy=\"mean\", axis=0)\n    X_trans = imputer.fit_transform(X)\n    assert_array_almost_equal(X_trans, X_true)\n"], "sample_252": ["    def test_key_transform_with_annotation(self):\n        qs = NullableJSONModel.objects.annotate(\n            baz_a=KeyTransform('a', 'value__baz'),\n        ).filter(baz_a='b')\n        self.assertEqual(qs.get(), self.objs[7])\n", "    def test_key_transform_lookup_with_null_value(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__k__l=None).get(),\n            self.objs[4],\n        )\n", "    def test_key_transform_with_annotate(self):\n        qs = NullableJSONModel.objects.annotate(\n            foo_value=KeyTransform('foo', 'value'),\n        ).filter(foo_value='bar')\n        self.assertSequenceEqual(qs, [self.objs[7]])\n", "    def test_key_transform_lookup_nested_key_transform(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__baz__has_key=KeyTransform('c', KeyTransform('baz', 'value')),\n            ),\n            [self.objs[7]],\n        )\n", "    def test_key_transform_with_isnull(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__k__isnull=False,\n                value__k__l__isnull=True,\n            ),\n            [self.objs[4]],\n        )\n"], "sample_990": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_556": ["def test_constrained_layout_tight():\n    fig, axs = plt.subplots(2, 2, constrained_layout=True)\n    axs[0, 0].set_xlabel(\"xlabel1\")\n    axs[1, 1].set_ylabel(\"ylabel2\")\n    fig.tight_layout()\n    \n", "compilation error", "compilation error", "compilation error", "def test_subfigure_with_colorbar():\n    fig = plt.figure()\n    subfig = fig.add_subfigure()\n    ax = subfig.add_subplot()\n    pc = ax.pcolormesh(np.random.randn(30, 30))\n    subfig.colorbar(pc, ax=ax)\n\n"], "sample_700": ["    def test_importorskip_exists(pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n            import sys\n\n                pytest.importorskip(sys.modules.get \"__main__\")\n                assert True\n        \"\"\"\n        )\n        result = pytester.runpytest()\n        result.stdout.fnmatch_lines([\"*1 passed*\"])\n", "    def test_importorskip_with_reason(pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n\n                pytest.importorskip(\"sys\", reason=\"Need sys module for this test\")\n                sys.exit(0)\n        \"\"\"\n        )\n        result = pytester.runpytest()\n        result.stdout.fnmatch_lines(\n            [\n                \"*IMPORT SKIP:*test_importorskip_with_reason.py:Need sys module for this test*\",\n            ]\n        )\n", "    def test_skipif_condition_import(pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.skipif(\"sys.platform == 'win32'\", reason=\"Requires non-windows platform\")\n                assert True\n        \"\"\"\n        )\n\n        result = pytester.runpytest()\n        if sys.platform == 'win32':\n            result.stdout.fnmatch_lines([\"*SKIP*test_not_on_windows*Requires non-windows platform*\"])\n        else:\n            result.stdout.fnmatch_lines([\"*1 passed*\"])\n", "    def test_importorskip_with_reason(pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n\n                with pytest.raises(pytest.skip.Exception):\n                    pytest.importorskip(\"doesnotexist\", reason=\"module should not exist\")\n                assert True\n        \"\"\"\n        )\n        result = pytester.runpytest()\n        result.stdout.fnmatch_lines([\"*SKIP*module should not exist*\"])\n", "    def test_importorskip_success(pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n\n                pytest.importorskip('os')\n                assert os.path.exists('/')\n        \"\"\"\n        )\n        result = pytester.runpytest()\n        result.assert_outcomes(passed=1)\n"], "sample_646": ["    def test_unittest_test_case_inheritance(pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import unittest\n\n            class BaseTestCase(unittest.TestCase):\n                pass\n\n            class MyTestCase(BaseTestCase):\n                    assert True\n            \"\"\"\n        )\n        result = pytester.runpytest()\n        result.assert_outcomes(passed=1)\n", "            def test_setup_method_exception(self):\n                raise Exception(\"Setup method exception\")\n", "    def test_do_cleanups_on_failure(pytester: Pytester) -> None:\n        testpath = pytester.makepyfile(\n            \"\"\"\n            import unittest\n            class MyTestCase(unittest.TestCase):\n                values = []\n                        self.values.append(1)\n                    self.addCleanup(cleanup)\n                    assert False\n                assert MyTestCase.values == [1]\n        \"\"\"\n        )\n        reprec = pytester.inline_run(testpath)\n        passed, skipped, failed = reprec.countoutcomes()\n        assert failed == 1\n        assert passed == 1\n", "    def test_subclassing_unittest_case(pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import unittest\n\n            class MyBaseTestCase(unittest.TestCase):\n                    self.base_setup = True\n\n            class MyTestCase(MyBaseTestCase):\n                    self.assertTrue(self.base_setup)\n        \"\"\"\n        )\n        reprec = pytester.inline_run()\n        reprec.assertoutcome(passed=1)\n", "    def test_do_class_cleanups_on_teardown_failure_multiple(pytester: Pytester) -> None:\n        testpath = pytester.makepyfile(\n            \"\"\"\n            import unittest\n            class MyTestCase(unittest.TestCase):\n                values = []\n                @classmethod\n                        cls.values.append(1)\n                    cls.addClassCleanup(cleanup)\n                @classmethod\n                    assert False\n                assert MyTestCase.values == [1]\n            pass\n            pass\n        \"\"\"\n        )\n        reprec = pytester.inline_run(testpath)\n        passed, skipped, failed = reprec.countoutcomes()\n        assert passed == 1\n        assert failed == 1\n"], "sample_1196": ["def test_Contains_evaluate():\n    x = Symbol('x')\n    s = FiniteSet(1, 2, 3)\n    c = Contains(x, s)\n    assert c.evalf() == Contains(x, s)\n    assert c.subs(x, 1) == S.true\n    assert c.subs(x, 4) == S.false\n", "def test_contains_evaluate():\n    x = Symbol('x')\n    s = FiniteSet(1, 2, 3)\n    assert Contains(x, s, evaluate=False) == Contains(x, s)\n    assert Contains(x, s).subs(x, 1) is S.true\n    assert Contains(x, s).subs(x, 4) is S.false\n", "def test_contains_evaluate():\n    assert Contains(1, S.Naturals,evaluate=True) is S.true\n    assert Contains(-1, S.Naturals, evaluate=True) is S.false\n", "def test_contains_evaluate():\n    x = Symbol('x')\n    assert Contains(x, S.Naturals, evaluate=False) == Contains(x, S.Naturals)\n    assert Contains(2, S.Naturals, evaluate=False).evaluate() is S.true\n", "def test_contains_eval():\n    x = Symbol('x')\n    S1 = FiniteSet(1, 2, 3)\n    S2 = Interval(0, 1)\n    assert Contains(x, S1).eval(x=1) == S.true\n    assert Contains(x, S1).eval(x=4) == S.false\n    assert Contains(x, S2).eval(x=0.5) == S.true\n    assert Contains(x, S2).eval(x=2) == S.false\n"], "sample_590": ["    def test_concat_empty_dims(self):\n        ds1 = Dataset({\"a\": ((\"x\", \"y\"), [[1, 2, 3], [4, 5, 6]])})\n        ds2 = Dataset({\"a\": ((\"x\", \"y\"), [[7, 8, 9], [10, 11, 12]])})\n        ds3 = Dataset({\"a\": ((\"x\", \"y\"), [])})\n        actual = concat([ds1, ds2, ds3], dim=\"z\")\n        expected = Dataset({\"a\": ((\"z\", \"x\", \"y\"), [[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]], []])})\n        assert_identical(actual, expected)\n", "    def test_concat_dim_name_existing_coord(self):\n        ds1 = Dataset({\"foo\": (\"x\", [1, 2])}, coords={\"x\": [0, 1]})\n        ds2 = Dataset({\"foo\": (\"x\", [1, 2])}, coords={\"x\": [2, 3]})\n        actual = concat([ds1, ds2], dim=\"x\")\n        expected = Dataset({\"foo\": (\"x\", [1, 2, 1, 2]), \"x\": [0, 1, 2, 3]})\n        assert_identical(actual, expected)\n", "    def test_concat_empty_dataset(self):\n        ds = Dataset()\n        actual = concat([ds, ds], dim=\"x\")\n        assert_identical(ds, actual)\n", "    def test_concat_empty_arrays(self):\n        empty = DataArray(np.array([]), dims=[\"x\"])\n        foo = DataArray([1, 2], coords=[(\"x\", [1, 2])])\n        actual = concat([empty, foo], dim=\"x\")\n        expected = DataArray([np.nan, 1, 2], coords=[(\"x\", [np.nan, 1, 2])])\n        assert_identical(actual, expected)\n", "    def test_concat_empty_dataset(self):\n        ds1 = Dataset()\n        ds2 = Dataset({\"foo\": (\"x\", [1, 2])})\n        actual = concat([ds1, ds2], dim=\"x\")\n        expected = ds2.copy()\n        assert_identical(expected, actual)\n"], "sample_843": ["    def test_kernel_gradient_with_anisotropic():", "def test_kernel_combination():\n    # Test combination of kernels using +, *\n    k1 = RBF(length_scale=1.0)\n    k2 = WhiteKernel(noise_level=1.0)\n    k_sum = k1 + k2\n    k_prod = k1 * k2\n    x = np.random.randn(10, 2)\n    \n    k_sum_eval = k_sum(x)\n    k_prod_eval = k_prod(x)\n    \n    assert k_sum_eval.shape == (10, 10)\n    assert k_prod_eval.shape == (10, 10)\n\n", "def test_kernel_anisotropic_set_params():\n    # Test setting anisotropic kernel parameters via set_params.\n    kernel = RBF([0.5, 2.0])\n    kernel.set_params(length_scale=[1.0, 3.0])\n\n    assert_array_almost_equal(kernel.k2.length_scale, [1.0, 3.0])\n", "def test_kernel_hyperparameter_bounds(kernel_class):\n    # Test that hyperparameters have sensible defaults and bounds.\n    kernel = kernel_class()\n    for hyperparameter in kernel.hyperparameters:\n        assert hyperparameter.bounds is not None\n        if isinstance(hyperparameter.bounds, tuple): \n            assert hyperparameter.bounds[0] <= hyperparameter.bounds[1]\n", "    def test_kernel_operator_commutativity():\n        # Adding kernels and multiplying kernels should be commutative.\n        # Check addition\n        assert_almost_equal((RBF(2.0) + 1.0)(X),\n                            (1.0 + RBF(2.0))(X))\n\n        # Check multiplication\n        assert_almost_equal((3.0 * RBF(2.0))(X),\n                            (RBF(2.0) * 3.0)(X))\n"], "sample_199": ["    def test_annotate_subquery_with_exists(self):\n        qs = Publisher.objects.annotate(\n            has_long_books=Exists(Book.objects.filter(\n                publisher=OuterRef('pk'), pages__gt=400\n            ))\n        ).filter(has_long_books=True).values('name')\n        self.assertCountEqual(qs, [{'name': 'Sams'}, {'name': 'Morgan Kaufmann'}])\n", "    def test_annotation_with_aggregate_filter(self):\n        qs = Book.objects.annotate(avg_rating=Avg('rating')).filter(avg_rating__gt=4)\n        self.assertCountEqual(qs.values('isbn'), [{'isbn': '159059725'}, {'isbn': '013235613'}])\n", "    def test_annotation_with_case_when(self):\n        qs = Book.objects.annotate(\n            price_category=Case(\n                When(price__gt=50, then=Value('Expensive')),\n                When(price__gt=30, then=Value('Moderate')),\n                default=Value('Cheap'),\n                output_field=CharField(),\n            ),\n        ).order_by('price')\n        self.assertQuerysetEqual(\n            qs, [\n                ('155860191', 'Cheap'),\n                ('159059725', 'Moderate'),\n                ('013235613', 'Expensive'),\n                ('013790395', 'Expensive'),\n            ],\n            lambda b: (b.isbn, b.price_category)\n        )\n\n", "    def test_annotation_exists_aggregate_values_filtering(self):\n        qs = Book.objects.values('publisher').annotate(\n            has_authors=Exists(Book.authors.through.objects.filter(book=OuterRef('pk'))),\n            max_pubdate=Max('pubdate'),\n        ).filter(has_authors=True, max_pubdate__gt=datetime.date(2007, 1, 1)).values_list('max_pubdate', flat=True).order_by('max_pubdate')\n        self.assertCountEqual(qs, [\n            datetime.date(2008, 3, 3),\n            datetime.date(2008, 6, 23),\n            datetime.date(2008, 11, 3),\n        ])\n", "    def test_annotation_aggregate_with_f_expression_on_same_field(self):\n        qs = Book.objects.annotate(\n            rating_sum=Sum('rating'),\n            avg_rating=F('rating_sum') / Count('pk'),\n        ).filter(avg_rating__gt=4)\n        self.assertQuerysetEqual(qs.values('name', 'avg_rating'), [\n            {'name': 'Practical Django Projects', 'avg_rating': Decimal('4.5')},\n            {'name': 'Python Web Development with Django', 'avg_rating': Decimal('4.5')},\n        ], lambda b: (b['name'], b['avg_rating']))\n"], "sample_204": ["    def test_circular_dependency(self):\n        \"\"\"\n        Tests handling of circular dependencies between migrations.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        msg = \"Cannot resolve dependencies for migrations:\"\n        with self.assertRaisesMessage(RuntimeError, msg):\n            loader.build_graph()\n\n", "    def test_loading_replace_squashed(self):\n        \"\"\"\n        Tests loading migrations where a replacing migration depends on a squashed migration.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        recorder = MigrationRecorder(connection)\n        self.addCleanup(recorder.flush)\n\n        # Load with nothing applied: both migrations squashed.\n        loader.build_graph()\n        plan = set(loader.graph.forwards_plan(('migrations', '4_auto')))\n        plan = plan - loader.applied_migrations.keys()\n        expected_plan = {\n            ('migrations', '1_auto'),\n            ('migrations', '2_squashed_3'),\n            ('migrations', '4_auto'),\n        }\n        self.assertEqual(plan, expected_plan)\n\n        # Fake-apply first migration of the squashed\n        recorder.record_applied('migrations', '1_auto')\n        loader.build_graph()\n\n        # Now the plan should be:\n        plan = set(loader.graph.forwards_plan(('migrations', '4_auto')))\n        plan = plan - loader.applied_migrations.keys()\n        expected_plan = {\n            ('migrations', '2_squashed_3'),\n            ('migrations', '4_auto'),\n        }\n        self.assertEqual(plan, expected_plan)\n", "    def test_loading_empty_namespace_package(self):\n        \"\"\"\n        MigrationLoader handles packages with no migrations correctly.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        loader.load_disk()\n        self.assertEqual(loader.disk_migrations, set())\n", "    def test_conflicting_migrations(self):\n        \"\"\"\n        Tests that the loader detects conflicting migrations.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        with self.assertRaises(ValueError) as exc:\n            loader.build_graph()\n        self.assertIn(\"Conflicting migrations found\", str(exc.exception))\n        self.assertIn(\"migrations.0001_initial and migrations.0001_conflicting_initial\", str(exc.exception))\n", "    def test_loading_multiple_apps(self):\n        \"\"\"\n        MigrationLoader should correctly load migrations from multiple apps.\n        \"\"\"\n        loader = MigrationLoader(connection)\n        loader.build_graph()\n\n        plan = set(loader.graph.forwards_plan(('migrations', '0002_second')))\n        expected_plan = {\n            ('migrations', '0001_initial'),\n            ('migrations', '0002_second'),\n            ('another_app', '0001_initial'),\n        }\n        self.assertEqual(plan, expected_plan)\n"], "sample_576": ["    def test_legend_title_from_scale(self, xy):\n\n        s = pd.Series([\"a\", \"b\", \"a\", \"c\"], name=\"s\")\n        p = Plot(**xy, color=s).scale(color=\"category\").add(MockMark()).plot()\n        legend, = p._figure.legends\n        assert legend.get_title().get_text() == \"s\"\n\n", "    def test_legend_title_from_kwargs(self, xy):\n\n        p = Plot(**xy).add(MockMark(), color=[\"a\", \"b\", \"c\", \"d\"],\n                          legend_title=\"My Legend\").plot()\n        legend, = p._figure.legends\n        assert legend.get_title().get_text() == \"My Legend\" \n", "    def test_legend_title(self, xy):\n\n        p = Plot(**xy).add(MockMark(), color=[\"a\", \"b\"]).label(color=\"MyColors\").plot()\n        legend, = p._figure.legends\n        assert legend.get_title().get_text() == \"MyColors\"\n", "    def test_legend_title_from_variable_with_name(self, xy):\n\n        s = pd.Series([\"a\", \"b\", \"a\", \"c\"], name=\"my_colors\")\n        p = Plot(**xy, color=s).add(MockMark()).plot()\n        legend, = p._figure.legends\n        assert legend.get_title().get_text() == \"my_colors\"\n", "    def test_legend_title_from_mark(self, xy):\n\n        class TitledMark(MockMark):\n                super().__init__()\n                self.title = title\n\n                return mpl.lines.Line2D([], []), self.title\n\n        p = Plot(**xy).add(TitledMark(\"Custom Title\")).plot()\n        legend, = p._figure.legends\n        assert legend.get_title().get_text() == \"Custom Title\"\n"], "sample_807": ["    def test_calibration_empty_bins():\n        \"\"\"Test calibration with empty bins.\"\"\"\n        y_true = np.array([0, 0, 1, 1])\n        y_pred = np.array([0.1, 0.2, 0.8, 0.9])\n        # Force empty bin by setting a very coarse binning strategy\n        prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=2,\n                                                strategy='quantile')\n\n        assert len(prob_true) == 2\n        assert len(prob_pred) == 2\n        assert all(np.isnan(prob_pred))\n", "def test_calibration_with_missing_labels():\n    n_samples = 10\n    X, y = make_classification(n_samples=n_samples, n_features=2,\n                               random_state=42, n_classes=3)\n\n    # Remove labels to simulate missing data\n    y[::2] = -1\n\n    clf = MultinomialNB()\n    calibrated_clf = CalibratedClassifierCV(clf, method=\"sigmoid\", cv=2)\n    with pytest.raises(ValueError):\n      calibrated_clf.fit(X, y)\n", "def test_calibration_multiclass_partial_fit():\n    \"\"\"Test calibration for multiclass classifiers with partial_fit\"\"\"\n    # Test multiclass calibration with partial_fit, using a LinearSVC as an\n    # example.\n    X = np.random.randn(100, 5)\n    y = np.random.randint(0, 3, size=100)\n\n    clf = LinearSVC()\n    cal_clf = CalibratedClassifierCV(clf, method='sigmoid', cv=2)\n\n    # Perform partial training on the first half of the data\n    cal_clf.partial_fit(X[:50], y[:50])\n\n    # Check if predict_proba can be called after partial_fit\n    with pytest.raises(RuntimeError):\n        cal_clf.predict_proba(X[50:])\n\n    # Train on the remaining data\n    cal_clf.partial_fit(X[50:], y[50:])\n\n    probas = cal_clf.predict_proba(X)\n    assert_array_almost_equal(np.sum(probas, axis=1), np.ones(len(X)))\n\n    # Test that log-loss of calibrated classifier is smaller than\n    # log-loss of naively turned OvR decision function to probabilities\n    # via softmax\n        e = np.exp(-y_pred)\n        return e / e.sum(axis=1).reshape(-1, 1)\n\n    uncalibrated_log_loss = log_loss(y, softmax(clf.decision_function(X)))\n    calibrated_log_loss = log_loss(y, probas)\n    assert calibrated_log_loss <= uncalibrated_log_loss\n\n", "    def test_calibration_empty_classes():\n        # Test calibration handles cases where a class is never seen in training set\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        y = np.array([0, 0, 1])\n        clf = LinearSVC(C=1.0)\n\n        # For LeaveOneOut, at least one fold will have only class 0\n        cal_clf = CalibratedClassifierCV(clf, method=\"sigmoid\", cv=LeaveOneOut())\n        cal_clf.fit(X, y)\n\n        # Predict probabilities for unseen class\n        X_test = np.array([[7, 8]])\n        y_prob = cal_clf.predict_proba(X_test)\n\n        # Check that probability for unseen class is 0\n        assert_array_almost_equal(y_prob[0, 1], 0)\n", "def test_calibration_with_weights():\n    \"\"\"Test calibration with sample weights.\"\"\"\n    X, y = make_classification(n_samples=100, n_features=6, random_state=42)\n    sample_weight = np.random.RandomState(seed=42).uniform(size=len(y))\n    \n    clf = LogisticRegression()\n    calibrated_clf = CalibratedClassifierCV(clf, method='sigmoid', cv=5)\n    calibrated_clf.fit(X, y, sample_weight=sample_weight)\n\n    # Check that predictions are the same with and without sample weights\n    y_pred_weighted = calibrated_clf.predict(X)\n    calibrated_clf.fit(X, y)\n    y_pred_unweighted = calibrated_clf.predict(X)\n    assert_array_equal(y_pred_weighted, y_pred_unweighted)\n"], "sample_67": ["    def test_model_form_with_custom_metaclass_and_callable_fields(self):\n        class MyMetaclass(ModelFormMetaclass):\n                new = super().__new__(cls, name, bases, attrs)\n                new.base_fields = {}\n                return new\n\n        class MyModelForm(forms.ModelForm, metaclass=MyMetaclass):\n                return self.cleaned_data['name'].upper()\n\n            class Meta:\n                model = Person\n                fields = '__all__'\n\n        form = MyModelForm({'name': 'test'})\n\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['name'], 'TEST')\n", "    def test_modelform_factory_with_exclude(self):\n        \"\"\"\n        Test that modelform_factory can exclude fields\n        \"\"\"\n        form = modelform_factory(Person, exclude=['name'])\n        self.assertEqual(list(form.base_fields), [])\n", "    def test_setattr_raises_validation_error_no_fields(self):\n        form_class = modelform_factory(model=StrictAssignmentAll, fields=[])\n        form = form_class(data={}, files=None)\n        form.instance._should_error = True\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors, {\n            '__all__': ['Cannot set attribute'],\n        })\n\n", "    def test_modelform_factory_with_custom_error_class(self):\n        \"\"\"\n        Test that modelform_factory can take a custom error class.\n        \"\"\"\n        class CustomValidationError(ValidationError):\n            pass\n\n        class CustomFormBase(forms.ModelForm):\n            class Meta:\n                model = Person\n                fields = '__all__'\n\n        Form = modelform_factory(Person, form=CustomFormBase, error_class=CustomValidationError)\n        form = Form({'name': 'test'})\n        self.assertTrue(isinstance(form.errors['name'][0], CustomValidationError))\n", "    def test_model_form_custom_error_message_with_html(self):\n        data = {'name1': 'FORBIDDEN_VALUE', 'name2': 'ABC'}\n        form = CustomErrorMessageForm(data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['name1'], '<ul class=\"errorlist\"><li>Model.clean() error messages with &lt;b&gt;HTML&lt;/b&gt;.</li></ul>')\n"], "sample_1080": ["compilation error", "compilation error", "compilation error", "compilation error", "def test_refine_Pow_complex():\n    assert refine(Pow(x, 2), Q.complex(x)) == Pow(x, 2)\n\n\n"], "sample_936": ["def test_stringify_broken_type():\n    assert stringify(BrokenType()) == 'BrokenType'\n", "def test_stringify_broken_type():\n    assert stringify(BrokenType()) == 'BrokenType'\n", "def test_stringify_type_hints_broken():\n    with pytest.raises(TypeError):\n        stringify(BrokenType())\n\n", "    def test_stringify_broken_args():\n        with pytest.raises(TypeError):\n            stringify(BrokenType())\n", "def test_stringify_broken_type():\n    with pytest.raises(TypeError):\n        stringify(BrokenType())\n"], "sample_1057": ["    def test_render_as_module_fully_qualified():\n        code = Print(1 + 2)\n        expected = \"\"\"", "    def test_render_as_module_fully_qualified(self):\n        code = Print(1 + 2)\n        expected = \"import sympy\\n\\nprint(sympy.Integer(3))\"\n        with self.settings_for_test(fully_qualified_modules=True):\n            self.assertEqual(render_as_module(code), expected)\n", "    def test_render_as_module_with_imports():\n        expr = Print(1 + 2)\n        code = render_as_module(expr)\n        assert 'from sympy.codegen.ast import Print' in code\n", "    def test_render_as_module_fully_qualified():\n        code = Print(1 + 2)\n        module_str = render_as_module(code, standard='python3')\n        assert 'from sympy.codegen.ast import Print' in module_str\n        assert 'print(1 + 2)' in module_str\n\n", "    def test_render_as_module_fully_qualified():\n        content = [Print(1)]\n        expected = \"import sympy\\n\\nprint(1)\"\n        assert render_as_module(content, standard='python3', fully_qualified_modules=True) == expected\n"], "sample_732": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_63": ["    def test_find_template_loader_invalid(self):\n        with self.assertRaises(ImproperlyConfigured):\n            engine = Engine(loaders=['invalid_loader'])\n", "    def test_get_template_builtins(self):\n        engine = Engine(builtins=['django.template.defaulttags', 'myproject.templatetags.mytags'])\n        self.assertIn('django.template.defaulttags', [b.__module__ for b in engine.template_builtins])\n        self.assertIn('myproject.templatetags.mytags', [b.__module__ for b in engine.template_builtins])\n", "    def test_loaders_with_app_dirs(self):\n        loaders = [\n            'django.template.loaders.filesystem.Loader'\n        ]\n        engine = Engine(dirs=[OTHER_DIR, TEMPLATE_DIR], loaders=loaders, app_dirs=True)\n        template = engine.get_template('priority/foo.html')\n        self.assertEqual(template.render(Context()), 'priority\\n')\n\n", "    def test_find_template_loader_with_args(self):\n        loader_class = mock.MagicMock()\n        loader_class.return_value = mock.MagicMock()\n        with patch('django.template.engine.import_string', return_value=loader_class):\n            engine = Engine(loaders=[('django.template.loaders.filesystem.Loader', {})])\n            engine.find_template_loader(('django.template.loaders.filesystem.Loader', {}))\n            loader_class.assert_called_once_with(engine, {}) \n", "    def test_loader_cache(self):\n        engine = Engine(dirs=[TEMPLATE_DIR], debug=False, loaders=[\n            'django.template.loaders.filesystem.Loader',\n        ])\n        template1 = engine.get_template('test_context.html')\n        template2 = engine.get_template('test_context.html')\n        self.assertIs(template1, template2)\n"], "sample_918": ["def test_pyexception_no_module(app):\n    text = \".. py:exception:: IOError\"\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"exception \"],\n                                                    [desc_name, \"IOError\"])],\n                                  [desc_content, ()])]))\n    assert_node(doctree[1], desc, desctype=\"exception\",\n                domain=\"py\", objtype=\"exception\", noindex=False)\n    assert 'IOError' in domain.objects\n    assert domain.objects['IOError'] == ('index', 'IOError', 'exception')\n", "    def test_pyobject_module_index(app):\n        text = (\".. py:module:: package\\n\"\n                \"\\n\"\n                \".. py:class:: package.Class1\\n\"\n                \"\\n\"\n                \".. py:function:: package.function1\\n\"\n                \"\\n\"\n                \".. py:exception:: package.Exception1\\n\")\n        restructuredtext.parse(app, text)\n        index = PythonModuleIndex(app.env.get_domain('py'))\n        assert index.generate() == (\n            [('p', [IndexEntry('package', 0, 'index', 'module-package', '', '', '') ,\n                    IndexEntry('package.Class1', 2, 'index', 'package.Class1', '', '', ''),\n                    IndexEntry('package.Exception1', 2, 'index', 'package.Exception1', '', '', ''),\n                    IndexEntry('package.function1', 2, 'index', 'package.function1', '', '', '')])],\n            False\n        )\n", "def test_pyattribute_without_type(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"      :value: ''\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'attr (Class attribute)', 'Class.attr', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_name, \"attr\"],\n                                                     [desc_annotation, \" = ''\"])],\n                                   [desc_content, ()]))\n    assert 'Class.attr' in domain.objects\n    assert domain.objects['Class.attr'] == ('index', 'Class.attr', 'attribute')\n", "    def test_pyfunction_signature_no_parameterlist(app):\n        text = \".. py:function:: hello\"\n        doctree = restructuredtext.parse(app, text)\n        assert_node(doctree, (addnodes.index,\n                              [desc, ([desc_signature, ([desc_name, \"hello\"],\n                              [desc_parameterlist, ()])],\n                              desc_content)]))\n", "    def test_pyobject_no_explicit_role(app):\n        text = (\".. py:function:: function_name\\n\"\n                \".. py:class:: ClassName\\n\"\n                \".. py:data:: VariableName\\n\"\n                \".. py:exception:: ExceptionName\")\n        domain = app.env.get_domain('py')\n        doctree = restructuredtext.parse(app, text)\n        assert 'function_name' in domain.objects\n        assert domain.objects['function_name'] == ('index', 'function_name', 'function')\n        assert 'ClassName' in domain.objects\n        assert domain.objects['ClassName'] == ('index', 'ClassName', 'class')\n        assert 'VariableName' in domain.objects\n        assert domain.objects['VariableName'] == ('index', 'VariableName', 'data')\n        assert 'ExceptionName' in domain.objects\n        assert domain.objects['ExceptionName'] == ('index', 'ExceptionName', 'exception')\n"], "sample_493": ["    def test_nested_aggregate_annotation_pruned(self):\n        with CaptureQueriesContext(connection) as ctx:\n            Book.objects.annotate(\n                avg_rating=Avg(\"book__rating\"),\n            ).annotate(\n                count=Count(\"*\"),\n            ).aggregate(\n                Avg(\"avg_rating\"),\n            )\n        sql = ctx.captured_queries[0][\"sql\"].lower()\n        self.assertEqual(sql.count(\"select\"), 2)\n        self.assertNotIn(\"avg_rating\", sql)\n", "    def test_aggregate_coalesce_with_null_values(self):\n        Book.objects.create(\n            name=\"b3\",\n            publisher=self.p1,\n            pages=100,\n            rating=None,\n            price=10,\n            contact=self.a1,\n            pubdate=datetime.date.today(),\n        )\n\n        result = Book.objects.aggregate(avg_rating=Coalesce(Avg(\"rating\"), 0))\n        self.assertEqual(result[\"avg_rating\"], 3.85)\n\n", "    def test_conditional_aggregate_annotation(self):\n        with CaptureQueriesContext(connection) as ctx:\n            Book.objects.annotate(\n                price_if_over_4=Case(When(price__gt=4, then=Sum(\"price\")), default=0)\n            ).aggregate(Avg(\"price_if_over_4\"))\n        sql = ctx.captured_queries[0][\"sql\"].lower()\n        self.assertEqual(sql.count(\"select\"), 2, \"Subquery wrapping required\")\n        self.assertIn(\"price_if_over_4\", sql)\n", "    def test_aggregate_annotation_with_transform(self):\n        with CaptureQueriesContext(connection) as ctx:\n            queryset = (\n                Book.objects.annotate(\n                    pages_over_100=Case(\n                        When(pages__gt=100, then=ExpressionWrapper(F(\"pages\") - 100, output_field=IntegerField())),\n                        default=0,\n                    )\n                )\n                .values(\"publisher__name\")\n                .annotate(avg_pages_over_100=Avg(\"pages_over_100\"))\n                .order_by(\"publisher__name\")\n            )\n            \n            self.assertQuerySetEqual(\n                queryset,\n                [\n                    {\"publisher__name\": \"Apress\", \"avg_pages_over_100\": 0},\n                    {\"publisher__name\": \"Jonno's House of Books\", \"avg_pages_over_100\": 0},\n                    {\"publisher__name\": \"Morgan Kaufmann\", \"avg_pages_over_100\": 850},\n                    {\"publisher__name\": \"Prentice Hall\", \"avg_pages_over_100\": 0},\n                    {\"publisher__name\": \"Sams\", \"avg_pages_over_100\": 0},\n                ],\n                lambda a, b: a[\"publisher__name\"] == b[\"publisher__name\"] and a[\"avg_pages_over_100\"] == b[\"avg_pages_over_100\"],\n            )\n\n", "    def test_aggregate_with_unreferenced_subquery(self):\n        queryset = (\n            Book.objects.annotate(\n                total_authors=Subquery(\n                    Author.objects.filter(book__isbn=OuterRef(\"isbn\")).count()\n                )\n            )\n            .filter(total_authors__gt=1)\n            .aggregate(Avg(\"total_authors\"))\n        )\n        self.assertEqual(queryset[\"total_authors__avg\"], 2)\n\n"], "sample_194": ["    def test_database_constraint_with_include(self):\n        UniqueConstraintInclude.objects.create(name='p1', color='red')\n        with self.assertRaises(IntegrityError):\n            UniqueConstraintInclude.objects.create(name='p1', color='blue')\n", "    def test_database_constraint_opclasses(self):\n        UniqueConstraintProduct.objects.create(price=10, discounted_price=5)\n        with self.assertRaises(IntegrityError):\n            UniqueConstraintProduct.objects.create(price=10, discounted_price=20)\n\n\n", "    def test_opclasses_database_constraint(self):\n        UniqueConstraintProduct.objects.create(name='p1', price=10)\n        with self.assertRaises(IntegrityError):\n            UniqueConstraintProduct.objects.create(name='p1', price=10)\n", "    def test_opclasses_database_constraint(self):\n        UniqueConstraintProduct.objects.create(name='p1', price=10)\n        with self.assertRaises(IntegrityError):\n            UniqueConstraintProduct.objects.create(name='p1', price=10)\n", "    def test_opclasses_database_constraint(self):\n        UniqueConstraintProduct.objects.create(name='p1', color='red')\n\n        with self.assertRaises(IntegrityError):\n            UniqueConstraintProduct.objects.create(name='p1', color='blue')\n"], "sample_1085": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_916": ["compilation error", "compilation error", "    def check_xref_consistency(role, tag1, tag2):\n        classes1 = classes(role, tag1)\n        classes2 = classes(role, tag2)\n        assert classes1 == classes2, (\n            f\"Mismatch in classes for '{role}' role between tags '{tag1}' and '{tag2}':\"\n            f\"\\n{classes1}\\n{classes2}\"\n        )\n", "    def check_xref_consistency(role, tag, expected_classes):\n        found_classes = classes(role, tag)\n        assert found_classes == expected_classes, (\n            f\"Expected classes {expected_classes} for {role} with tag {tag}, \"\n            f\"but found {found_classes}\"\n        )\n", "    def check_xref_classes(role, tag, expected_classes):\n        classes_set = classes(role, tag)\n        assert classes_set == expected_classes\n"], "sample_157": ["    def test_serialize_empty_db(self):\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n\n        # Ensure there are no existing objects\n        Object.objects.all().delete()\n        ObjectReference.objects.all().delete()\n\n        serialized_data = creation.serialize_db_to_string()\n        self.assertEqual(serialized_data, '[]')\n", "    def test_test_db_signature_with_port(self):\n        # Test signature when port is defined.\n        test_name = 'hodor'\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['NAME'] = 'foo'\n        test_connection.settings_dict['PORT'] = 123\n        test_connection.settings_dict['TEST'] = {'NAME': test_name}\n        signature = BaseDatabaseCreation(test_connection).test_db_signature()\n        self.assertEqual(signature[3], test_name)\n        self.assertEqual(signature[1], 123)\n\n", "    def test_keepdb_setting(self, mocked_migrate, mocked_ensure_connection):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST']['MIGRATE'] = True\n        creation = test_connection.creation_class(test_connection)\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db') as mocked_create_test_db:\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False, keepdb=True)\n                mocked_create_test_db.assert_called_once_with(\n                    verbosity=0, autoclobber=True, keepdb=True\n                )\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0)\n\n", "    def test_destroy_test_db_keepdb_true(self, mocked_migrate, mocked_ensure_connection):\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        old_database_name = test_connection.settings_dict['NAME']\n        with mock.patch.object(creation, '_destroy_test_db') as mock_destroy:\n            creation.destroy_test_db(old_database_name, verbosity=0, keepdb=True)\n            mock_destroy.assert_not_called()\n", "    def test_serialize_deserialize_data(self):\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n            Object.objects.create(data='foo')\n            ObjectReference.objects.create(name='bar', object=Object.objects.get(data='foo'))\n            serialized_data = creation.serialize_db_to_string()\n            creation.destroy_test_db(old_database_name, verbosity=0)\n            creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n            creation.deserialize_db_from_string(serialized_data)\n\n            self.assertEqual(Object.objects.count(), 1)\n            self.assertEqual(ObjectReference.objects.count(), 1)\n        finally:\n            creation._destroy_test_db(old_database_name, verbosity=0)\n\n"], "sample_514": ["compilation error", "compilation error", "compilation error", "    def test_colorbar_ticks_after_setting_norm(self):\n        fig, ax = plt.subplots()\n        pcm = ax.pcolormesh(np.random.rand(10, 10))\n        cb = fig.colorbar(pcm)\n        ticks = cb.get_ticks()\n        pcm.set_norm(mcolors.LogNorm())\n        new_ticks = cb.get_ticks()\n\n        assert new_ticks is not ticks\n\n", "compilation error"], "sample_994": ["compilation error", "def test_issue_10369():\n    assert Rational(1, 2).evalf(2) == .50\n", "    def test_issue_11521():", "compilation error", "compilation error"], "sample_390": ["    def test_was_modified_since_invalid_header(self):\n        self.assertTrue(was_modified_since(header=\"invalid\"))\n", "    def test_was_modified_since_length(self):\n        \"\"\"\n        Header with length mismatch is considered modified (#24814).\n        \"\"\"\n        header = \"Wed, 21 Oct 2020 07:28:58 GMT; length=100\"\n        mtime = 1603303738\n        size = 200\n        self.assertTrue(was_modified_since(header, mtime, size))\n", "    def test_was_modified_since_invalid_header(self):\n        \"\"\"\n        was_modified_since should handle malformed If-Modified-Since headers\n        gracefully.\n\n        \"\"\"\n        with self.assertRaises(ValueError):\n            was_modified_since(header=\"garbage\")\n", "    def test_was_modified_since_different_sizes(self):\n        \"\"\"\n        If the Content-Length in the If-Modified-Since header doesn't match\n        the current size, the file should be considered modified.\n        \"\"\"\n\n        mtime = 1343416141\n        header = \"Wed, 12 Sep 2012 17:15:41 GMT; length=12345\"\n\n        self.assertTrue(was_modified_since(\n          header, mtime, 54321\n        ))\n", "    def test_was_modified_since_header_length(self):\n        \"\"\"\n        Test that was_modified_since correctly handles the length parameter in the\n        If-Modified-Since header.\n        \"\"\"\n        mtime = 1343416141\n        header = f\"Wed, 07 Nov 2012 15:55:41 GMT; length=12345\"\n        self.assertFalse(was_modified_since(header, mtime, 12345))\n        self.assertTrue(was_modified_since(header, mtime, 12346))\n"], "sample_134": ["    def test_serialize_custom_field_with_args(self):\n        class CustomField(models.Field):\n                super().__init__(*args, **kwargs)\n                self.arg1 = arg1\n                self.arg2 = arg2\n\n        field = CustomField(arg1='value1', arg2='value2')\n        self.assertSerializedEqual(field)\n", "    def test_serialize_decimal_with_context(self):\n        decimal_value = decimal.Decimal('12.345')\n        with decimal.localcontext(decimal.Context(prec=4)):\n            serialized_value, imports = MigrationWriter.serialize(decimal_value)\n        self.assertEqual(serialized_value, \"decimal.Decimal('12.345')\")\n        self.assertEqual(imports, {'import decimal'})\n", "    def test_serialize_database_operations(self):\n        migration = type(\"Migration\", (migrations.Migration,), {\n            \"operations\": [\n                migrations.RunSQL(\"SELECT * FROM some_table\"),\n                migrations.AlterModelTable('Model', 'new_table_name'),\n                migrations.AlterDatabaseAlias('old_alias', 'new_alias'),\n            ]\n        })\n        writer = MigrationWriter(migration)\n        output = writer.as_string()\n        result = self.safe_exec(output)\n        self.assertIn(\"migrations.RunSQL\", output)\n        self.assertIn(\"migrations.AlterModelTable\", output)\n        self.assertIn(\"migrations.AlterDatabaseAlias\", output)\n", "    def test_serialize_custom_field(self):\n        class CustomField(models.Field):\n                super().__init__(*args, **kwargs)\n                self.custom_argument = kwargs.pop('custom_argument', None)\n\n                name, path, args, kwargs = super().deconstruct()\n                kwargs['custom_argument'] = self.custom_argument\n                return name, path, args, kwargs\n\n        field = CustomField(custom_argument='hello')\n\n        string, imports = MigrationWriter.serialize(field)\n        self.assertEqual(string, \"migrations.test_writer.CustomField(custom_argument='hello')\")\n        self.assertEqual(imports, {'import migrations.test_writer'})\n", "    def test_serialize_custom_field_with_args(self):\n        class CustomField(models.Field):\n                super().__init__(*args, **kwargs)\n                self.args = args\n                self.kwargs = kwargs\n\n                return (\n                    self.__class__.__name__,\n                    self.args,\n                    self.kwargs\n                )\n        migration = type(\"Migration\", (migrations.Migration,), {\n            \"operations\": [\n                migrations.AddField(\"mymodel\", \"myfield\", CustomField(default=42, choices=[(1, 'One'), (2, 'Two')])),\n            ]\n        })\n        writer = MigrationWriter(migration)\n        output = writer.as_string()\n        result = self.safe_exec(output)\n        self.assertIn(\"migrations.test_writer.CustomField\", result)\n"], "sample_104": ["    def test_manifest_hash_collision(self):\n        \"\"\"\n        Ensure that files with the same hash don't overwrite each other in\n        the manifest.\n        \"\"\"\n        # Create two files with the same hash.\n        with open(self._get_filename_path('file1.txt'), 'w') as f:\n            f.write('content')\n        with open(self._get_filename_path('file2.txt'), 'w') as f:\n            f.write('content')\n\n        self.run_collectstatic()\n        manifest = storage.staticfiles_storage.load_manifest()\n\n        self.assertIn('file1.txt', manifest)\n        self.assertIn('file2.txt', manifest)\n        self.assertNotEqual(manifest['file1.txt'], manifest['file2.txt'])\n\n", "    def test_manifest_updates_on_file_change(self):\n        filename = 'test/file.txt'\n        filepath = self._get_filename_path(filename)\n        with open(filepath, 'w') as f:\n            f.write('Initial content')\n        self.run_collectstatic()\n        hashed_filename = self.hashed_file_path(filename)\n        original_content = self.get_staticfile_content(hashed_filename)\n\n        with open(filepath, 'w') as f:\n            f.write('Updated content')\n        self.run_collectstatic()\n        updated_content = self.get_staticfile_content(hashed_filename)\n\n        self.assertNotEqual(original_content, updated_content)\n", "    def test_manifest_cache_invalidation(self):\n        # Create a file that will be added to the manifest\n        filename = 'test_file.css'\n        content = b'/* This is a test CSS file */'\n        with open(self._get_filename_path(filename), 'wb') as f:\n            f.write(content)\n\n        # Run collectstatic to generate the initial manifest\n        self.run_collectstatic()\n\n        # Get the initial hashed filename from the manifest\n        hashed_filename = self.hashed_file_path(filename)\n        manifest_path = storage.staticfiles_storage.manifest_name\n        manifest = storage.staticfiles_storage.load_manifest()\n        self.assertIn(\n            hashed_filename, manifest\n        )\n\n        # Modify the original file\n        content = b'/* This is a modified test CSS file */'\n        with open(self._get_filename_path(filename), 'wb') as f:\n            f.write(content)\n\n        # Run collectstatic again to update the manifest\n        self.run_collectstatic()\n        updated_manifest = storage.staticfiles_storage.load_manifest()\n\n        # The hashed filename should have changed in the manifest\n        self.assertNotEqual(manifest, updated_manifest)\n        self.assertIn(\n            self.hashed_file_path(filename), updated_manifest\n        )\n\n", "    def test_cached_file_references_hashed(self):\n        filename = 'test.css'\n        with open(self._get_filename_path(filename), 'w') as f:\n            f.write('@import url(\"other.css\");')\n\n        # Create the referenced file \n        other_filename = 'other.css'\n        with open(self._get_filename_path(other_filename), 'w') as f:\n            f.write('body { color: red; }')\n\n        self.run_collectstatic()\n\n        # Verify that the hashed name is used in the CSS file\n        css_file_path = self.hashed_file_path(filename)\n        with storage.staticfiles_storage.open(css_file_path) as f:\n            css_content = f.read().decode()\n\n        other_hashed_filename = self.hashed_file_path(other_filename)\n        self.assertIn(other_hashed_filename, css_content)\n", "    def test_hashed_files_cache_invalidates_on_file_modification(self):\n        # Create a test file\n        filename = 'test.css'\n        filepath = self._get_filename_path(filename)\n        with open(filepath, 'w') as f:\n            f.write('body { background-color: red; }')\n\n        # Trigger hashed file generation\n        self.hashed_file_path(filename)\n\n        # Modify the original file\n        with open(filepath, 'a') as f:\n            f.write('\\n h1 { color: blue; }')\n\n        # Force re-processing of the file and check new hash\n        self.run_collectstatic(clear=True)\n        self.assertNotEqual(self.hashed_file_path(filename), self.hashed_file_path(filename))\n"], "sample_399": ["    def test_aggregation_default_with_subquery(self):\n        result = (\n            Book.objects.annotate(\n                highest_rating=Subquery(\n                    Author.objects.filter(pk=OuterRef(\"authors\")).values(\"rating\").order_by(\"-rating\")[:1]\n                )\n            )\n            .filter(highest_rating__isnull=False)\n            .aggregate(avg_rating=Avg(\"highest_rating\", default=2.5))\n        )\n        self.assertEqual(result[\"avg_rating\"], Decimal(\"4.25\"))\n", "    def test_aggregation_default_with_filter_and_subquery(self):\n        author_ids = [self.a1.pk, self.a2.pk]\n        result = Book.objects.annotate(\n            avg_rating=Avg(\"rating\", default=2.5),\n        ).filter(\n            author__in=author_ids,\n        ).aggregate(\n            total_pages=Sum(\"pages\"),\n            avg_pages=Avg(\"pages\"),\n        )\n        self.assertEqual(result[\"total_pages__sum\"], 1000)\n        self.assertEqual(result[\"avg_pages__avg\"], 500)\n\n", "    def test_aggregation_default_with_transform_expression(self):\n        qs = (\n            Book.objects.annotate(\n                pages_above_avg=Case(\n                    When(pages__gt=Avg(\"pages\"), then=F(\"pages\")), default=0,\n                ),\n            )\n            .aggregate(total_pages=Sum(\"pages_above_avg\", default=0))\n        )\n        self.assertEqual(qs[\"total_pages\"], 2061)\n\n", "    def test_aggregation_order_by_default_behavior(self):\n        queryset = Publisher.objects.annotate(\n            book_count=Count(\"book\")\n        ).order_by(\"book_count\")\n        self.assertEqual(list(queryset.values_list(\"name\", \"book_count\")), [\n            (\"Jonno's House of Books\", 0),\n            (\"Morgan Kaufmann\", 1),\n            (\"Sams\", 1),\n            (\"Apress\", 3),\n            (\"Prentice Hall\", 3),\n        ])\n\n", "    def test_aggregation_default_on_function_call_with_aggregate(self):\n        result = Publisher.objects.annotate(\n            avg_rating=Avg(\"book__rating\", default=2.5),\n        ).aggregate(\n            value=Sum(F(\"avg_rating\") * 2, default=0)\n        )\n        self.assertEqual(result[\"value\"], 46.0)\n"], "sample_685": ["def test_log_format_specified_via_ini(testdir, monkeypatch):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n            logging.info(\"test message\")\n            assert 'test message' in caplog.text\n\n    \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_format = %(asctime)s - %(name)s - %(levelname)s - %(message)s\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    assert result.ret == 0\n    # Assert that the log message follows the specified format\n    result.stdout.fnmatch_lines([\"* - * - INFO - test message\"])\n", "def test_log_capture_handler_level_persists(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n            logger = logging.getLogger('catchlog')\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_level == logging.WARNING\n\n            caplog.set_level(logging.DEBUG, logger.name)\n            logger.debug(\"DEBUG message will be shown\")\n            assert caplog.handler.level == logging.DEBUG\n            \n            # Test that the level persists across test function boundaries\n\n                logger.debug(\"DEBUG message in inner function will be shown\")\n            \n            inner_test()\n            assert caplog.handler.level == logging.DEBUG \n\n            assert \"DEBUG message will be shown\" in caplog.text\n            assert \"DEBUG message in inner function will be shown\" in caplog.text\n\n    \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_level=WARNING\n    \"\"\"\n    )\n    result = testdir.runpytest()\n\n    assert result.ret == 0\n", "def test_capturing_from_different_loggers(caplog):\n    logger1 = logging.getLogger(__name__ + '.test1')\n    logger2 = logging.getLogger(__name__ + '.test2')\n    caplog.set_level(logging.INFO)\n    logger1.info(\"Message from logger1\")\n    logger2.info(\"Message from logger2\")\n    assert 'logger1' in caplog.text\n    assert 'logger2' in caplog.text\n", "def test_log_file_level(testdir: Testdir) -> None:\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n            logger = logging.getLogger(__name__)\n            logger.debug(\"debug message\")\n            logger.info(\"info message\")\n            logger.warning(\"warning message\")\n            logger.error(\"error message\")\n            logger.critical(\"critical message\")\n        \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file_level=WARNING\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    content = result.stdout.raw\n    for msg in [\"debug message\", \"info message\", \"warning message\", \"error message\",\n                \"critical message\"]:\n        if \"WARNING\" in msg:\n            assert msg in content\n        else:\n            assert msg not in content\n", "    def test_changing_log_level_twice(caplog):\n        caplog.set_level(logging.INFO)\n        logger.info(\"first INFO\")\n\n        caplog.set_level(logging.DEBUG)\n        logger.info(\"second INFO\")\n        logger.debug(\"second DEBUG\")\n\n        caplog.set_level(logging.INFO)\n        logger.info(\"third INFO\")\n\n        assert \"first INFO\" in caplog.text\n        assert \"second INFO\" in caplog.text\n        assert \"second DEBUG\" in caplog.text\n        assert \"third INFO\" in caplog.text\n"], "sample_1194": ["def test_julia_custom_functions():\n    from sympy import Function, Symbol\n    f = Function('f')\n    g = Function('g')\n    x = Symbol('x')\n    assert julia_code(f(x) + g(x), user_functions={\n        'f': 'existing_julia_fcn_1',\n        'g': lambda x: 'existing_julia_fcn_2(%s)' % x\n    }) == \"existing_julia_fcn_1(x) + existing_julia_fcn_2(x)\"\n\n", "    def test_julia_matrix_assign_to_more():\n        A = Matrix([[1, 2, 3]])\n        B = MatrixSymbol('B', 1, 3)\n        C = MatrixSymbol('C', 2, 3)\n        assert julia_code(A, assign_to=B) == \"B = [1 2 3]\"\n        raises(ValueError, lambda: julia_code(A, assign_to=x))\n        raises(ValueError, lambda: julia_code(A, assign_to=C))\n", "compilation error", "compilation error", "compilation error"], "sample_520": ["def test_bar3d_stacked():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    xpos = np.arange(4)\n    ypos = np.arange(4)\n    xpos = [x - 0.2 for x in xpos]\n    dx = 0.4\n    dy = 0.4\n    zpos = np.array([0, 1, 2, 3])\n\n    ax.bar3d(xpos, ypos, zpos, dx, dy, np.random.rand(4), shade=True)\n    ax.bar3d(xpos, ypos, zpos, dx, dy, np.random.rand(4), shade=True, bottom=np.random.rand(4))\n    \n", "def test_Poly3DCollection_set_facecolor():\n    # Test setting the facecolor of a Poly3DCollection\n    y, x = np.ogrid[1:10:100j, 1:10:100j]\n    z2 = np.cos(x) ** 3 - np.sin(y) ** 2\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    r = ax.plot_surface(x, y, z2, cmap='hot')\n    r.set_facecolor('blue')\n    assert r.get_facecolor() == 'blue'\n", "def test_axis_limits_after_rotation(rotation):\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n\n    # Create some data\n    x = np.arange(-5, 5, 0.25)\n    y = np.arange(-5, 5, 0.25)\n    X, Y = np.meshgrid(x, y)\n    Z = np.sin(np.sqrt(X**2 + Y**2))\n\n    # Plot the data\n    surf = ax.plot_surface(X, Y, Z)\n\n    # Rotate the axes\n    ax.view_init(*rotation)\n\n    # Assert that the axis limits are still correct\n    assert np.allclose(ax.get_xlim3d(), (-5, 5))\n    assert np.allclose(ax.get_ylim3d(), (-5, 5))\n    assert np.allclose(ax.get_zlim3d(), (-1, 1))\n", "def test_contour3d():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    X, Y, Z = axes3d.get_test_data(0.05)\n    ax.contour3D(X, Y, Z, 50, cmap='viridis')\n    \n", "def test_axes3d_set_aspect():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection=\"3d\")\n    ax.set_aspect('equal')\n    assert ax.get_aspect() == 'equal'\n    ax.set_aspect('auto')\n    assert ax.get_aspect() == 'auto'\n\n    with pytest.raises(ValueError):\n        ax.set_aspect('invalid')\n"], "sample_16": ["    def test_merge_arrays_units(self):\n        with pytest.raises(u.UnitsError, match=\"'km', 'm/s'\"):\n            rfn.merge_arrays((self.q_pv['p'], self.q_pv_t['t']))\n", "    def test_structured_to_unstructured_dtype(self):\n        # Test that dtype is preserved correctly\n        dtype = [(\"f1\", float), (\"f2\", float)]\n        struct = np.array([(1, 2)], dtype=dtype) * u.m\n        unstruct = rfn.structured_to_unstructured(struct)\n        assert unstruct.dtype == np.dtype(float)\n        assert unstruct.shape == (2,)\n", "    def test_broadcast_arrays(self):\n        q1 = np.array([1, 2, 3]) * u.m\n        q2 = np.array([4, 5]) * u.kg\n        with pytest.raises(ValueError, match=\"Cannot broadcast\"):\n           np.broadcast_arrays(q1, q2)\n\n", "    def test_all_quantities(self):\n        for func_name in FUNCTION_HELPERS:\n            func = FUNCTION_HELPERS[func_name]\n            if func_name not in UNSUPPORTED_FUNCTIONS and func_name not in SUBCLASS_SAFE_FUNCTIONS:\n                with pytest.raises(TypeError):\n                    func(np.arange(10) * u.m)\n", "    def test_linalg_eigvals_complex(self):\n        q = np.array([[1j, 2j], [0, 3j]]) * u.m\n        w = np.linalg.eigvals(q)\n        wx = np.linalg.eigvals(q.value) << u.m\n        assert_array_equal(w, wx)\n"], "sample_1084": ["compilation error", "compilation error", "compilation error", "compilation error", "def test_ComplexRegion_intersection_empty_set():\n    empty_set = ComplexRegion(FiniteSet(), False)\n    c1 = ComplexRegion(Interval(0, 1)*Interval(0, 2*S.Pi), polar=True)\n    assert c1.intersection(empty_set) == empty_set\n    assert empty_set.intersection(c1) == empty_set\n"], "sample_200": ["    def test_local_host_raises_exception(self):\n        with self.assertRaises(ConnectionRefusedError):\n            mail.send_mail('Subject', 'Content', 'from@example.com', ['to@example.com']) \n", "    def test_error_handling_during_send(self):\n        \"\"\"\n        SMTP connection errors are properly caught and re-raised.\n        \"\"\"\n        backend = smtp.EmailBackend()\n        backend.connection = Mock(side_effect=socket.error('Connection Error'))\n        email = EmailMessage('Subject', 'Content', 'from@example.com', ['to@example.com'])\n        with self.assertRaises(smtp.SMTPException):\n            backend.send_messages([email])\n", "    def test_send_messages_after_server_stopped(self):\n        \"\"\"\n        Sending messages after the SMTP server has stopped raises\n        a SMTPException.\n        \"\"\"\n        self.assertRaises(SMTPException, self.backend.send_messages, [EmailMessage('Subject', 'Content', 'from@example.com', ['to@example.com'])])\n", "    def test_send_messages_with_invalid_email_address(self):\n        \"\"\"\n        Sending a message with an invalid email address should not crash the\n        backend.\n        \"\"\"\n        backend = smtp.EmailBackend()\n        backend.connection = True\n        email = EmailMessage('Subject', 'Content', 'from@example.com', ['invalid-email'])\n        with self.assertRaises(BadHeaderError):\n          backend.send_messages([email])\n", "    def test_smtp_connection_error_retry(self):\n        \"\"\"Make sure that we retry on temporary SMTP connection errors.\"\"\"\n        backend = smtp.EmailBackend(username='', password='')\n        with patch('smtplib.SMTP.connect', side_effect=socket.timeout):\n            with self.assertRaises(smtplib.SMTPException):\n                backend.open()\n"], "sample_1022": ["compilation error", "def test_convert_equals_signs():\n    cases = {\n        '1=2' : 'Eq(1, 2)',\n        '1*2=x' : 'Eq(2, x)',\n        '(1*2=x)=False' : 'Eq(Eq(2, x), False)',\n        'a=b=c' : 'Eq(a,Eq(b,c))'\n    }\n    transformations = standard_transformations + (convert_xor,\n                                                 convert_equals_signs)\n   \n    for case, expected in cases.items():\n        assert(parse_expr(case, transformations=transformations) == parse_expr(expected))\n", "compilation error", "    def test_repeated_decimals():\n        cases = {\n            '0.2[1]': '19/90',\n            '1.2[34]': '1234/990',\n            '0.[12345]': '12345/99999',\n            '.[123]': '123/999',\n            '3.14[159265]': '314159265/100000000',\n\n        }\n        transformations = standard_transformations + (convert_xor,\n                                                     repeated_decimals,\n                                                     implicit_multiplication_application)\n        for case, expected in cases.items():\n            assert(parse_expr(case, transformations=transformations) ==\n                   parse_expr(expected))\n\n\n", "compilation error"], "sample_726": ["compilation error", "def test_label_binarize_empty_input():\n    y = np.array([])\n    classes = [0, 1]\n    pos_label = 1\n    neg_label = 0\n    expected = np.array([])\n    for sparse_output in [True, False]:\n        binarized = label_binarize(y, classes, neg_label=neg_label,\n                                pos_label=pos_label,\n                                sparse_output=sparse_output)\n        assert_array_equal(toarray(binarized), expected)\n        assert_equal(issparse(binarized), sparse_output)\n\n    lb = LabelBinarizer(neg_label=neg_label, pos_label=pos_label,\n                        sparse_output=sparse_output)\n    binarized = lb.fit_transform(y)\n    assert_array_equal(toarray(binarized), expected)\n    assert_equal(issparse(binarized), sparse_output)\n\n", "def test_label_binarize_empty_array():\n    y = []\n    classes = [0, 1, 2]\n    pos_label = 2\n    neg_label = 0\n    expected = np.array([])\n    yield check_binarized_results, y, classes, pos_label, neg_label, expected\n\n\n\n", "compilation error", "compilation error"], "sample_1162": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_694": ["def test_deprecation_kwargs_in_addoption(pytester: Pytester):\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            pass\n\n        @pytest.mark.parametrize(\"arg\", [1, 2], ids=[\"one\", \"two\"])\n            pass\n\n        @pytest.mark.parametrize(\n            \"arg\", [1, 2], ids=[\"one\", \"two\"], scope=\"function\"\n        )\n            pass\n\n            pass\n\n        @pytest.fixture\n            return request.param\n\n        @pytest.mark.parametrize(\"arg\", [1, 2], ids=[\"one\", \"two\"])\n            pass\n\n    \"\"\"\n    )\n\n    # Ensure pytest.addoption() warnings are correctly emitted.\n    for kwargs in [\n        {\"type\": str},\n        {\"type\": \"int\"},\n        {\"type\": int(1)},\n    ]:\n        with pytest.warns(\n            pytest.PytestDeprecationWarning,\n            match=r\"`type` argument to addoption() is the string .* \",\n        ) as recorder:\n            pytester.parseconfig(\"-p\", \"pytester\")\n            pytester.makepyfile(\n                \"\"\"\n                import pytest\n\n                    pass\n\n                pytest.addoption(\"--myoption\", **kwargs)\n            \"\"\"\n            )\n\n        assert len(recorder) == 1\n\n\n", "def test_hookspec_deprecated_param_names(pytester: Pytester, monkeypatch):\n    from _pytest.config import PytestPluginManager\n\n    pm = PytestPluginManager()\n\n    class DeprecatedHookSpec:\n            parser.addoption(\n                \"--foo\", action=\"store\", type=\"int\", help=\"foo help\",\n            )\n\n        pytest_addoption = pytest.hookspec(historic=False)  # type: ignore[attr-defined]\n        pytest_addoption.historic = False\n\n    with pytest.warns(\n        PytestDeprecationWarning,\n        match=r\"Please use the pytest\\.hookspec\\(historic=False\\) decorator\",\n    ) as recorder:\n        pm.add_hookspecs(DeprecatedHookSpec)\n    (record,) = recorder\n    assert (\n        record.lineno\n        == DeprecatedHookSpec.pytest_addoption.__code__.co_firstlineno\n    )\n    assert record.filename == __file__\n\n", "def test_hookimpl_tryfirst_deprecated(pytester: Pytester) -> None:\n    from _pytest import hookspec\n\n    plugin = hookspec.pytest_plugin_register\n    config = pytester.parseconfig()\n    plugin.register(MyPlugin(config))\n\n    with pytest.warns(\n        PytestDeprecationWarning,\n        match=r\"Please use the pytest.hookimpl\\(tryfirst=True\\)\",\n    ):\n        config.hook.pytest_runtest_protocol(item=pytester.Item(\"test.py\"))\n", "    def test_nose_deprecated_with_nose_method(pytester: Pytester) -> None:\n        pytest.importorskip(\"nose\")\n        pytester.makepyfile(\n            \"\"\"\n            from nose.tools import raises\n\n            @raises(ZeroDivisionError)\n                1 / 0\n            \"\"\"\n        )\n        output = pytester.runpytest(\"-Wdefault::pytest.PytestRemovedIn8Warning\")\n        message = [\n            \"*PytestRemovedIn8Warning: Support for nose tests is deprecated and will be removed in a future release.\",\n            \"*test_raises is using nose-specific method: `raises()`\",\n        ]\n        output.stdout.fnmatch_lines(message)\n        output.assert_outcomes(passed=1)\n\n\n\n", "    def test_private_is_deprecated_with_positional(self):\n        class PrivateInit:\n                deprecated.check_ispytest(_ispytest)\n\n        with pytest.warns(\n            pytest.PytestDeprecationWarning, match=\"private pytest class or function\"\n        ):\n            PrivateInit(10, bar=20)\n"], "sample_1049": ["def test_plane_contains():\n    x, y, z, u, v = symbols('x y z u v', real=True)\n    p1 = Point3D(0, 0, 0)\n    p2 = Point3D(1, 1, 1)\n    p3 = Point3D(1, 2, 3)\n    pl3 = Plane(p1, p2, p3)\n    assert pl3.contains(Point3D(0,0,0)) is True\n    assert pl3.contains(Point3D(1,1,1)) is True \n    assert pl3.contains(Point3D(1,2,3)) is True\n    assert pl3.contains(Point3D(2,3,4)) is False\n    assert pl3.contains(pl3.arbitrary_point(u,v)) is True\n", "def test_plane_contains():\n    p1 = Point3D(1, 0, 0)\n    p2 = Point3D(0, 1, 0)\n    p3 = Point3D(0, 0, 1)\n    pl = Plane(p1, p2, p3)\n\n    assert pl.contains(p1) is True\n    assert pl.contains(p2) is True\n    assert pl.contains(p3) is True\n    assert pl.contains(Point3D(0.5, 0.5, 0)) is True\n    assert pl.contains(Point3D(0, 0, 0)) is True\n    assert pl.contains(Point3D(1, 1, 1)) is False\n    assert pl.contains(Point3D(-1, -1, -1)) is False\n\n", "compilation error", "compilation error", "compilation error"], "sample_790": ["def test_kernel_pca_non_positive_eigenvalues():\n    # Test case for handling non-positive eigenvalues in KernelPCA.\n\n    # Create a synthetic dataset with eigenvalues including zeros and negatives.\n    X = np.array([[1, 0], [0, 1], [-1, 0], [0, -1]])\n\n    kpca = KernelPCA(n_components=2)\n\n    # Fit the KernelPCA model.\n    kpca.fit(X)\n\n    # Assert that the number of components is correctly determined.\n    assert_equal(kpca.lambdas_.shape[0], 2)\n\n    # Assert that the eigenvalues are sorted in descending order.\n    assert_greater(kpca.lambdas_[0], kpca.lambdas_[1])\n", "    def test_kernel_pca_copy_X(self):\n        rng = np.random.RandomState(0)\n        X_fit = rng.random_sample((5, 4))\n\n        # test copy_X=True\n        kpca_copy = KernelPCA(copy_X=True).fit(X_fit)\n        assert_equal(kpca_copy.X_fit_, X_fit)\n        X_fit[0,0] = 666\n        assert_not_equal(kpca_copy.X_fit_[0,0], 666)\n\n        # test copy_X=False\n        kpca_no_copy = KernelPCA(copy_X=False).fit(X_fit)\n        assert_equal(kpca_no_copy.X_fit_, X_fit)\n        X_fit[0,0] = 666\n        assert_equal(kpca_no_copy.X_fit_[0,0], 666)\n", "def test_kernel_pca_n_components_and_remove_zero_eig_interaction():\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((5, 4))\n\n    # Case 1: n_components > number of non-zero eigenvalues\n    kpca = KernelPCA(n_components=4, remove_zero_eig=True)\n    Xt = kpca.fit_transform(X)\n    assert_less(Xt.shape[1], kpca.n_components)\n\n    # Case 2: n_components <= number of non-zero eigenvalues\n    kpca = KernelPCA(n_components=2, remove_zero_eig=True)\n    Xt = kpca.fit_transform(X)\n    assert_equal(Xt.shape[1], kpca.n_components)\n", "    def test_kernel_pca_copy_x():\n        rng = np.random.RandomState(0)\n        X = rng.random_sample((10, 5))\n        kpca = KernelPCA(copy_X=True)\n        kpca.fit(X)\n        assert(kpca.X_fit_ is not X)\n        \n        kpca = KernelPCA(copy_X=False)\n        kpca.fit(X)\n        assert(kpca.X_fit_ is X)\n", "    def test_kernel_pca_n_features_in():\n        kpca = KernelPCA()\n        X = [[1, 2], [3, 4], [5, 6]]\n        kpca.fit(X)\n        assert_equal(kpca.n_features_in_, 2)\n"], "sample_1061": ["compilation error", "compilation error", "compilation error", "    def test_Integer_bit_length():\n        assert Integer(0).bit_length() == 0\n        assert Integer(1).bit_length() == 1\n        assert Integer(-1).bit_length() == 1\n        assert Integer(2).bit_length() == 2\n        assert Integer(-2).bit_length() == 2\n        assert Integer(1234567890).bit_length() == 31\n", "compilation error"], "sample_88": ["    def test_connect_timeout(self):\n        \"\"\"\n        Test that connect() raises a ConnectTimeoutError if the server is\n        unreachable.\n        \"\"\"\n        server = smtplib.SMTP(\"nonexistent.host.name\", 25)\n        with self.assertRaises(smtplib.ConnectTimeoutError):\n            server.connect()\n", "    def test_send_messages_connection_error(self):\n        \"\"\"\n        Test that send_messages raises an exception if the connection fails.\n        \"\"\"\n        self.backend.connection = mock.Mock(side_effect=socket.error('Connection error'))\n        email = EmailMessage('Subject', 'Content', 'from@example.com', ['to@example.com'])\n        with self.assertRaises(smtp.SMTPException):\n            self.backend.send_messages([email])\n", "    def test_server_stopped_send_messages(self):\n        with self.assertRaises(ConnectionRefusedError):\n            self.backend.send_messages([EmailMessage('Subject', 'Content', 'from@example.com', ['to@example.com'])])\n", "    def test_smtp_connection_failure(self):\n        \"\"\"\n        Test that the backend handles SMTP connection failures gracefully.\n        \"\"\"\n        self.server.stop()\n        with self.assertRaises(smtplib.SMTPConnectError):\n            with self.backend:\n                pass\n", "    def test_send_messages_after_server_stopped(self):\n        \"\"\"\n        A connection that has been closed raises a ConnectionRefusedError\n        when the server is stopped.\n        \"\"\"\n        self.assertRaises(smtplib.SMTPConnectError, self.backend.open)\n        self.assertRaises(smtplib.SMTPConnectError, self.backend.send_messages, [EmailMessage('Subject', 'Content', 'from@example.com', ['to@example.com'])])\n\n"], "sample_1073": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_169": ["    def test_deep_lookup_or(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(Q(value__d__0__isnull=False) | Q(value__k__l='m')),\n            [self.objs[4]],\n        )\n", "    def test_key_lookup_on_empty_json(self):\n        obj = NullableJSONModel.objects.create(value={})\n        self.assertFalse(NullableJSONModel.objects.filter(value__nonexistent_key=1).exists())\n        self.assertFalse(NullableJSONModel.objects.filter(value__nonexistent_key__isnull=True).exists())\n        self.assertTrue(NullableJSONModel.objects.filter(value__nonexistent_key__isnull=False).exists())\n", "    def test_key_transform_expression_null(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__isnull=False).annotate(\n                key=KeyTransform('d', 'value'),\n                expr=KeyTransform('0', Cast('key', models.JSONField())),\n            ).filter(expr__isnull=True),\n            [self.objs[4]],\n        )\n", "    def test_key_isnull_and_contains(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__j__isnull=True, value__a='b'),\n            [self.objs[3]],\n        )\n", "    def test_key_lookup_with_special_characters(self):\n        obj = NullableJSONModel.objects.create(value={'key.with.dots': 'value'})\n        self.assertEqual(NullableJSONModel.objects.filter(value__key__with__dots='value').get(), obj)\n"], "sample_851": ["compilation error", "def test_regression_metrics_with_inf():\n    y_true = np.array([1, 2, 3])\n    y_pred = np.array([1, np.inf, 3])\n    \n    assert np.isinf(mean_squared_error(y_true, y_pred))\n\n    y_pred = np.array([1, -np.inf, 3])\n    assert np.isinf(mean_squared_error(y_true, y_pred))\n    \n    with pytest.raises(ValueError):\n        mean_squared_log_error(y_true, y_pred)\n    \n    assert np.isnan(mean_absolute_error(y_true, y_pred))\n    assert np.isnan(median_absolute_error(y_true, y_pred))\n    assert np.isnan(r2_score(y_true, y_pred))\n", "compilation error", "def test_mean_tweedie_deviance_raises_value_error_for_invalid_p():\n    with pytest.raises(ValueError, match=\n                       \"Tweedie deviance is only defined for power<=0 and power>=1.\"\n                       ):\n        mean_tweedie_deviance([0], [0], power=0.5)\n", "compilation error"], "sample_446": ["    def test_decimal_localecontext(self):\n        with localcontext() as ctx:\n            ctx.prec = 28\n            self.assertEqual(floatformat(Decimal(\"123456.123456789012345678901\"), 21),\n                             \"123456.123456789012345678901\")\n", "    def test_localcontext(self):\n        with localcontext() as ctx:\n            ctx.prec = 28\n            self.assertEqual(floatformat(Decimal(\"1.12345678901234567890123456789\"), 20),\n                             \"1.12345678901234567890\")\n", "    def test_unicode(self):\n        with localcontext() as ctx:\n            ctx.prec = 28\n            value = Decimal(\"1.2345678901234567890123456789\")\n            self.assertEqual(floatformat(value), \"1.234567890123456789012345679\")\n", "    def test_decimal_context(self):\n        with localcontext() as ctx:\n            ctx.prec = 5\n            self.assertEqual(floatformat(Decimal(\"1.23456789\")), \"1.23457\")\n            ctx.prec = 2\n            self.assertEqual(floatformat(Decimal(\"1.23456789\")), \"1.23\")\n", "compilation error"], "sample_842": ["compilation error", "    def test_kernel_hyperparameter_bounds(self):\n        # Check that bounds are correctly applied to hyperparameters.\n        kernel = RBF(length_scale=1.0, length_scale_bounds=(0.1, 10.0))\n        assert kernel.length_scale_bounds == (0.1, 10.0)\n        assert kernel.get_params()['length_scale_bounds'] == (0.1, 10.0)\n\n        # Set new bounds using set_params\n        kernel.set_params(length_scale_bounds=(0.5, 5.0))\n        assert kernel.length_scale_bounds == (0.5, 5.0)\n        assert kernel.get_params()['length_scale_bounds'] == (0.5, 5.0)\n        \n        kernel = RBF(length_scale=1.0)\n        with pytest.raises(ValueError):\n            kernel.set_params(length_scale_bounds=(10.0, 0.1))\n\n        # Test setting bounds for isotropic and anisotropic kernels\n        anisotropic_kernel = RBF([1.0, 2.0], length_scale_bounds=[(0.1, 10.0), (0.5, 5.0)])\n        assert anisotropic_kernel.length_scale_bounds == [(0.1, 10.0), (0.5, 5.0)]\n        anisotropic_kernel.set_params(length_scale_bounds=[(0.2, 10.0), (0.6, 5.0)])\n        assert anisotropic_kernel.length_scale_bounds == [(0.2, 10.0), (0.6, 5.0)]\n", "    def test_kernel_negative_length_scales(self):\n        kernel = RBF(length_scale=-1)\n        with pytest.warns(UserWarning):\n            kernel(X)\n", "def test_kernel_bounds(kernel):\n    # Test that setting parameters outside of bounds raises a ValueError.\n    for hyperparameter in kernel.hyperparameters:\n        if isinstance(hyperparameter.bounds, tuple):\n            lower, upper = hyperparameter.bounds\n            # Test lower bound\n            with pytest.raises(ValueError):\n                    kernel.set_params(**{hyperparameter.name: lower - 1})\n            # Test upper bound\n            with pytest.raises(ValueError):\n                    kernel.set_params(**{hyperparameter.name: upper + 1})\n\n", "def test_kernel_clone_with_fixed_hyperparameters():\n    kernel = RBF(length_scale=2.0, length_scale_bounds=\"fixed\")\n    kernel_cloned = clone(kernel)\n    assert kernel.get_params() == kernel_cloned.get_params()\n\n    assert kernel.length_scale == kernel_cloned.length_scale\n    assert kernel.length_scale_bounds == kernel_cloned.length_scale_bounds\n"], "sample_1068": ["compilation error", "compilation error", "def test_specfun_issue_14846():\n    assert octave_code(Ci(x)) == 'cosint(x)'\n    assert octave_code(Shi(x)) == 'sinhint(x)'\n    assert octave_code(Chi(x)) == 'coshint(x)'\n", "compilation error", "compilation error"], "sample_1166": ["compilation error", "def test_monomial_divides_exact_quotient():\n    assert ExactQuotientFailed((1, 2, 3), (1, 2, 4)) is not None\n", "compilation error", "compilation error", "compilation error"], "sample_499": ["def test_legend_title_fontproperties_update():\n    # test updating fontproperties of legend title\n    plt.plot(range(10))\n    leg = plt.legend(title='Aardvark',\n                     title_fontproperties=FontProperties(size=20, weight='bold'))\n    assert leg.get_title().get_size() == 20\n    assert leg.get_title().get_weight() == 'bold'\n    leg.get_title().set_fontproperties(FontProperties(size=12, style='italic'))\n    assert leg.get_title().get_size() == 12\n    assert leg.get_title().get_style() == 'italic'\n", "def test_legend_handlerlinewidth():\n    # Test linewidth consistency for monolithic Line2D legend handler.\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1], linewidth=5)\n    handles = [mlines.Line2D([0], [0], linewidth=5)]\n    leg = ax.legend(handles, [\"Aardvark\"])\n    assert handles[0].get_linewidth() == leg.legendHandles[0].get_linewidth()\n", "    def test_legend_text_color_consistency():\n        fig, ax = plt.subplots()\n        ax.plot([1, 2], [3, 4], label='Testing', color='blue')\n        leg = ax.legend()\n        assert leg.get_texts()[0].get_color() == mpl.colors.to_rgb('blue')\n", "def test_legend_scatter_markersize():\n    # Test setting markersize for Scatter legend entries.\n    fig, ax = plt.subplots()\n    ax.scatter([0], [0], s=100, label='Aardvark')\n    leg = ax.legend()\n    assert leg.legendHandles[0].get_markersize() == 100\n\n    leg.legendHandles[0].set_markersize(50)\n    assert leg.legendHandles[0].get_markersize() == 50\n", "    def test_legend_title_fontsize_units():\n        plt.plot([1, 2], [1, 2], label='Data')\n        with pytest.raises(ValueError):\n            plt.legend(title='Aardvark', title_fontsize='invalid')\n\n        leg = plt.legend(title='Aardvark', title_fontsize=12)\n        assert leg.get_title().get_fontsize() == 12\n\n        leg = plt.legend(title='Aardvark', title_fontsize='small')\n        assert leg.get_title().get_fontsize() == mpl.rcParams['font.size'] * 0.8\n\n"], "sample_156": ["    def test_attribute_factory(self):\n        class CustomForm(Form):\n            default_renderer = lambda: CustomRenderer()\n\n        form = CustomForm()\n        self.assertIsInstance(form.renderer, CustomRenderer)\n", "    def test_attribute_class_instance(self):\n        class CustomForm(Form):\n            default_renderer = CustomRenderer() \n\n        form = CustomForm()\n        self.assertEqual(form.renderer, CustomForm.default_renderer)\n", "    def test_attribute_dynamic_class(self):\n        class CustomForm(Form):\n            default_renderer = lambda self: CustomRenderer()\n\n        form = CustomForm()\n        self.assertIsInstance(form.renderer, CustomRenderer)\n", "    def test_attribute_subclassing(self):\n        class CustomForm(Form):\n            default_renderer = DjangoTemplates()\n\n        class SubCustomForm(CustomForm):\n            pass\n\n        form = SubCustomForm()\n        self.assertEqual(form.renderer, CustomForm.default_renderer)\n", "    def test_attribute_callable(self):\n        class CustomForm(Form):\n            default_renderer = lambda self: DjangoTemplates()\n\n        form = CustomForm()\n        self.assertIsInstance(form.renderer, DjangoTemplates)\n"], "sample_410": ["    def test_group_permissions_through_related_manager(self):\n        group = Group.objects.create(name=\"test_group\")\n        permission = Permission.objects.create(\n            codename=\"test_permission\", name=\"Test Permission\", content_type=ContentType.objects.get_for_model(User)\n        )\n        group.permissions.add(permission)\n        self.assertIn(permission, group.permissions.all())\n", "    def test_group_permissions(self):\n        group = Group.objects.create(name=\"test_group\")\n        permission = Permission.objects.create(\n            codename='test_permission', name='Test Permission', content_type=ContentType.objects.get_for_model(User)\n        )\n        group.permissions.add(permission)\n        self.assertIn(permission, group.permissions.all())\n\n", "    def test_group_permissions_cached(self):\n        g = Group.objects.create(name=\"test_group\")\n        p1 = Permission.objects.get(codename=\"add_post\")\n        p2 = Permission.objects.get(codename=\"delete_post\")\n        g.permissions.add(p1)\n        self.assertEqual(set(g.permissions.all()), {p1})\n        g.permissions.add(p2)\n        self.assertEqual(set(g.permissions.all()), {p1, p2})\n", "    def test_get_permissions(self):\n        group1 = Group.objects.create(name=\"group 1\")\n        permission1 = Permission.objects.create(\n            name=\"permission 1\", codename=\"perm1\", content_type=ContentType.objects.get(model=\"user\")\n        )\n        permission2 = Permission.objects.create(\n            name=\"permission 2\", codename=\"perm2\", content_type=ContentType.objects.get(model=\"user\")\n        )\n        group1.permissions.add(permission1)\n        user = User.objects.create(username=\"testuser\")\n        user.groups.add(group1)\n\n        self.assertCountEqual(user.get_all_permissions(), {permission1})\n", "    def test_group_permissions_through_permissions(self):\n        permission = Permission.objects.create(\n            name=\"can edit widget\", codename=\"can_edit_widget\", content_type=ContentType.objects.get_for_model(User)\n        )\n        group = Group.objects.create(name=\"widget editors\")\n        group.permissions.add(permission)\n        self.assertIn(permission, group.permissions.all())\n        group.permissions.remove(permission)\n        self.assertNotIn(permission, group.permissions.all())\n"], "sample_840": ["compilation error", "compilation error", "compilation error", "    def test_n_components_error():\n        d = load_linnerud()\n        X = d.data\n        Y = d.target\n        for clf in [pls_.PLSCanonical(), pls_.PLSRegression(),\n                    pls_.PLSSVD()]:\n            clf.n_components = -1\n            assert_raise_message(ValueError,\n                                 \"n_components should be greater than or equal to 1\",\n                                 clf.fit, X, Y)\n\n", "compilation error"], "sample_521": ["    def test_scatter_marker(self):\n        fig = plt.figure()\n        ax = fig.add_subplot(projection='3d')\n        x = np.arange(10)\n        y = np.arange(10)\n        z = np.arange(10)\n        ax.scatter(x, y, z, marker='o')\n        fig.canvas.draw()\n", "    def test_quiver3D_empty(self):\n        fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n        with pytest.raises(ValueError):\n            ax.quiver3D([], [], [], [], [])\n", "compilation error", "    def test_projection_argument(self, projection, expected_projection):\n        fig = plt.figure()\n        ax = fig.add_subplot(projection=projection)\n        assert ax.get_projection() == expected_projection\n", "def test_scatter_marker_facecolor():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n\n    # Test different marker face colors by setting 'facecolor'\n\n    ax.scatter([1, 2, 3], [4, 5, 6], [7, 8, 9], marker='o', facecolor='red')\n    ax.scatter([1, 2, 3], [1, 2, 3], [1, 2, 3], marker='^', facecolor='blue')\n    ax.scatter([1, 2, 3], [4, 5, 6], [7, 8, 9], marker='s', facecolor='green')\n\n    # Test setting facecolor to 'none' for transparent markers\n\n    ax.scatter([1, 2, 3], [1, 2, 3], [1, 2, 3], marker='o', facecolor='none', s=100)\n"], "sample_370": ["    def test_prefetch_related_flattened_and_nested_prefetch(self):\n        with self.assertNumQueries(3):\n            authors = AuthorWithAge.objects.filter(pk__in=[self.author1.pk, self.author2.pk]).prefetch_related(\n                Prefetch(\n                    'author',\n                    queryset=Author.objects.prefetch_related(\n                        Prefetch('favorite_authors', to_attr='fav_authors'),\n                        Prefetch('favorite_authors__first_book', to_attr='fav_books'),\n                    ),\n                ),\n            )\n        with self.assertNumQueries(0):\n            self.assertEqual(authors[0].author.fav_authors[0].first_book.title, 'Candide')\n            self.assertEqual(authors[1].author.fav_books[0].title, 'Candide')\n", "    def test_nested_prefetch_related(self):\n        rooms = Room.objects.filter(house__name='Big house').prefetch_related(\n            Prefetch('house', queryset=House.objects.prefetch_related('rooms'))\n        )\n        with self.assertNumQueries(1):\n            for room in rooms:\n                self.assertEqual(room.house.rooms.count(), 1)\n", "    def test_nested_prefetch_m2m(self):\n        house = House.objects.create(name='Big house', address='123 Main St')\n        room1 = Room.objects.create(name='Kitchen', house=house)\n        room2 = Room.objects.create(name='Living room', house=house)\n        person1 = Person.objects.create(name='Alice')\n        person2 = Person.objects.create(name='Bob')\n        room1.occupants.add(person1, person2)\n        room2.occupants.add(person1)\n        persons = Person.objects.prefetch_related('rooms__house')\n        with self.assertNumQueries(3):\n            self.assertEqual(persons.count(), 2)\n            self.assertEqual(persons[0].rooms.count(), 2)\n            self.assertEqual(persons[0].rooms[0].house.name, 'Big house')\n            self.assertEqual(persons[0].rooms[1].house.name, 'Big house')\n            self.assertEqual(persons[1].rooms.count(), 1)\n            self.assertEqual(persons[1].rooms[0].house.name, 'Big house')\n", "    def test_nested_prefetch_with_through_model(self):\n        FavoriteAuthors.objects.create(author=self.author1, likes_author=self.author2)\n        authors = AuthorWithAge.objects.prefetch_related(\n            Prefetch(\n                'author',\n                queryset=Author.objects.prefetch_related(\n                    Prefetch('favorite_authors', queryset=FavoriteAuthors.objects.select_related('author')),\n                ),\n            ),\n        )\n        with self.assertNumQueries(3):\n            for author in authors:\n                for fav in author.author.favorite_authors.all():\n                    self.assertEqual(fav.author, self.author2)\n", "    def test_prefetch_related_empty_result(self):\n        with self.assertNumQueries(2):\n            qs = Book.objects.prefetch_related('authors')\n            books = list(qs)\n        self.assertEqual(books, [self.book1, self.book2])\n"], "sample_1019": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_308": ["    def test_format_with_invalid_date(self):\n        with self.assertRaises(ValueError):\n            dateformat.format(datetime(2024, 2, 30), '%d')\n", "    def test_RFC5322_tzinfo(self):\n        dt = datetime(2009, 5, 16, 5, 30, 30)\n        dt_aware = make_aware(dt, get_default_timezone())\n        self.assertEqual(dateformat.format(dt, 'r'), dt_aware.strftime('%a, %d %b %Y %H:%M:%S +0000'))\n        self.assertEqual(dateformat.format(dt_aware, 'r'), dt_aware.strftime('%a, %d %b %Y %H:%M:%S %z'))\n", "    def test_time_format_with_non_en_locale(self):\n        # Changing the locale doesn't change the \"r\" format.\n        dt = datetime(1979, 7, 8, 22, 00)\n        with translation.override('fr'):\n            self.assertEqual(\n                dateformat._format(dt, 'r'),\n                'Sun, 08 Jul 1979 22:00:00 +0100',\n            )\n", "    def test_f_format_with_zero_minute(self):\n        dt = datetime(2023, 10, 26, 14, 00)\n        self.assertEqual(dateformat.format(dt, 'f'), '2')\n", "    def test_format_with_timezone_aware_datetime_objects(self):\n        \"\"\"\n        Test that format() handles timezone-aware datetime objects correctly.\n        \"\"\"\n        aware_dt = datetime(2023, 11, 15, 10, 30, tzinfo=utc)\n        formats = ['Y-m-d H:i:s', 'r', 'P', 'T', 'O', 'e']\n\n        for format_str in formats:\n            with self.subTest(format_str=format_str):\n                result = dateformat.format(aware_dt, format_str)\n                # Assert that the result is not empty.\n                self.assertTrue(result)\n"], "sample_883": ["def test_bayesian_ridge_ard_alpha_lambda_init():\n    # Test BayesianRidge and ARDRegression initialisation with alpha_init and\n    # lambda_init set to various values.\n    n_samples = 10\n    n_features = 5\n    random_state = check_random_state(42)\n    X = random_state.random_sample((n_samples, n_features))\n    y = random_state.random_sample(n_samples)\n\n    # Test with various initial values\n    alpha_inits = [1e-6, 1e-3, 1, 10]\n    lambda_inits = [1e-6, 1e-3, 1, 10]\n    for alpha_init in alpha_inits:\n        for lambda_init in lambda_inits:\n            for clf in [BayesianRidge(), ARDRegression()]:\n                clf = clf.set_params(alpha_init=alpha_init, lambda_init=lambda_init)\n                clf.fit(X, y)\n                assert clf.alpha_ == pytest.approx(alpha_init, rel=1e-5)\n                assert clf.lambda_  == pytest.approx(lambda_init, rel=1e-5)\n", "    def test_no_intercept_model():\n        # Test that it is possible to create both BayesianRidge and ARDRegression\n        # models without intercept terms\n        X = np.array([[1, 1], [3, 4], [5, 7], [4, 1], [2, 6], [3, 10], [3, 2]])\n        y = np.array([1, 2, 3, 2, 0, 4, 5]).T\n        for Estimator in [BayesianRidge, ARDRegression]:\n            model = Estimator(fit_intercept=False)\n            model.fit(X, y)\n", "    def test_bayesian_ridge_ard_with_empty_features(self):\n        X = np.array([[], []])\n        y = np.array([])\n        for clf in [BayesianRidge(), ARDRegression()]:\n            with pytest.raises(ValueError):\n                clf.fit(X, y)\n", "    def test_bayesian_ridge_ard_with_sparse_data(self):\n        # Test BayesianRidge and ARDRegression with sparse input data\n        rng = np.random.RandomState(42)\n        n_samples = 50\n        n_features = 100\n        X = rng.rand(n_samples, n_features)\n        X_sparse = csr_matrix(X)\n        y = rng.rand(n_samples)\n\n        for clf in [BayesianRidge(), ARDRegression()]:\n            clf.fit(X, y)\n            clf_sparse = clf.__class__()\n            clf_sparse.fit(X_sparse, y)\n            assert_allclose(clf.coef_, clf_sparse.coef_, rtol=1e-5)\n            assert_allclose(clf.intercept_, clf_sparse.intercept_, rtol=1e-5)\n", "    def test_bayesian_ridge_ard_alpha_lambda_deprecation(global_random_seed):\n        # check deprecation warnings for alpha_1, alpha_2, lambda_1, lambda_2\n\n        X = np.random.RandomState(global_random_seed).randn(100, 5)\n        y = np.random.RandomState(global_random_seed).randn(100)\n\n        for estimator in [BayesianRidge, ARDRegression]:\n            with pytest.warns(FutureWarning, match=\"The parameters alpha_1, alpha_2, lambda_1, lambda_2\"):\n                estimator(alpha_1=1, alpha_2=2, lambda_1=3, lambda_2=4).fit(X, y)\n"], "sample_332": ["    def test_formset_non_field_errors_with_empty_formset(self):\n        data = {'form-INITIAL_FORMS': '0', 'form-TOTAL_FORMS': '0'}\n        formset = ChoiceFormSet(data, formset=BaseCustomFormSet)\n\n        self.assertEqual(formset.non_form_errors(), ['This is a non-form error'])\n", "    def test_all_valid_with_empty_formsets(self):\n        data = {}\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset1, formset2)), True)\n", "    def test_formset_errors_as_dict(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '',\n        }\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertFalse(formset.is_valid())\n        expected_errors = [[], {'votes': ['This field is required.']}]\n        self.assertEqual(formset.errors_as_dict(), expected_errors)\n", "    def test_formset_with_nested_forms(self):\n        class NestedForm(Form):\n            name = CharField()\n        class OuterForm(Form):\n            nested_form = formset_factory(NestedForm, extra=1)\n        data = {\n            'nested_form-TOTAL_FORMS': '2',\n            'nested_form-INITIAL_FORMS': '0',\n            'nested_form-0-name': 'Nested One',\n            'nested_form-1-name': 'Nested Two',\n        }\n        form = OuterForm(data)\n        self.assertTrue(form.is_valid())\n        self.assertEqual(len(form.cleaned_data['nested_form']), 2)\n", "    def test_formset_invalid_data(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '',\n            'choices-1-choice': '',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertFalse(formset.is_valid())\n        self.assertFalse(all_valid((formset,)))\n"], "sample_888": ["    def test_iforest_get_feature_names_out(self):\n        \"\"\"Check get_feature_names_out for IsolationForest.\"\"\"\n        X = [[1, 2], [3, 4]]\n        names = ['feature1', 'feature2']\n        clf = IsolationForest().fit(X)\n\n        # Check default behavior (None input)\n        assert_array_equal(clf.get_feature_names_out(), names)\n        \n        # Check with input array\n        assert_array_equal(clf.get_feature_names_out(input_features=names), names)\n", "    def test_iforest_with_empty_features():\n        \"\"\"Test IsolationForest with empty features.\"\"\"\n        X = np.zeros((10, 0))\n        clf = IsolationForest()\n        with pytest.raises(ValueError):\n            clf.fit(X)\n", "def test_iforest_max_samples_with_random_state():\n    random_state = 42\n    X = np.random.randn(100, 10)\n    clf1 = IsolationForest(max_samples=50, random_state=random_state).fit(X)\n    clf2 = IsolationForest(max_samples=50, random_state=random_state).fit(X)\n    assert_array_equal(\n        clf1.estimators_features_, clf2.estimators_features_\n    )\n", "    def test_iforest_same_results_with_same_seed(self, global_random_seed):\n        \"\"\"Check that IsolationForest produces same results with same seed.\"\"\"\n        X = iris.data[:100]\n        clf1 = IsolationForest(random_state=global_random_seed).fit(X)\n        clf2 = IsolationForest(random_state=global_random_seed).fit(X)\n        assert_array_equal(clf1.predict(X), clf2.predict(X))\n", "def test_iforest_outlier_score():\n    # Test if outlier score is consistently higher for outliers\n    rng = check_random_state(0)\n    X_inliers = rng.randn(100, 2)\n    X_outliers = rng.uniform(low=-10, high=10, size=(10, 2))\n    X = np.vstack((X_inliers, X_outliers))\n    clf = IsolationForest(contamination=0.1).fit(X) \n    inlier_scores = clf.score_samples(X_inliers)\n    outlier_scores = clf.score_samples(X_outliers)\n    assert np.mean(outlier_scores) < np.mean(inlier_scores) \n"], "sample_1075": ["    def test_beta_diff_x(self):\n        x = Symbol('x')\n        y = Symbol('y')\n        assert diff(beta(x, y), x) == beta(x, y) * (digamma(x) - digamma(x + y))\n", "    def test_beta_diff_x():\n        x = Symbol('x')\n        y = Symbol('y')\n        assert diff(beta(x,y), x) == beta(x, y)*(digamma(x) - digamma(x + y))\n", "    def test_beta_rewrite_as_gamma():\n        x = Symbol('x')\n        y = Symbol('y')\n        assert beta(x, y).rewrite(gamma) == gamma(x) * gamma(y) / gamma(x + y)\n", "def test_beta_eval_rewrite_as_gamma():\n    x = Symbol('x')\n    y = Symbol('y')\n    assert beta(x, y)._eval_rewrite_as_gamma(x, y) == gamma(x)*gamma(y)/gamma(x + y)\n", "    def test_beta_eval_rewrite_as_gamma(self):\n        x = Symbol('x', real=True)\n        y = Symbol('y', real=True)\n        assert beta(x, y).rewrite(gamma) == gamma(x) * gamma(y) / gamma(x + y)\n"], "sample_37": ["    def test_to_fits_2():\n        \"\"\"\n        Test to_fits() with no distortion.\n        \"\"\"\n        w = wcs.WCS(naxis=2)\n        w.wcs.crpix = [100, 100]\n        w.wcs.crval = [10, 20]\n        w.wcs.cdelt = [0.1, 0.1]\n        w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\"]\n        wfits = w.to_fits()\n        assert isinstance(wfits, fits.HDUList)\n        assert isinstance(wfits[0], fits.PrimaryHDU)\n        assert isinstance(wfits[1], fits.ImageHDU)\n\n", "    def test_wcs_to_header():\n        \"\"\"\n        Test converting a WCS object to a header.\n        \"\"\"\n        w = wcs.WCS(naxis=2)\n        w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\"]\n        w.wcs.crval = [10, 20]\n        w.wcs.cdelt = [0.1, 0.05]\n        w.wcs.crpix = [100, 200]\n\n        header = w.to_header()\n\n        assert header['CTYPE1'] == 'RA---TAN'\n        assert header['CTYPE2'] == 'DEC--TAN'\n        assert header['CRVAL1'] == 10\n        assert header['CRVAL2'] == 20\n        assert header['CDELT1'] == 0.1\n        assert header['CDELT2'] == 0.05\n        assert header['CRPIX1'] == 100\n        assert header['CRPIX2'] == 200\n\n", "def test_table_sip():\n    \"\"\"\n    Test that SIP coefficients are properly read from a table.\n    \"\"\"\n    f = fits.open(get_pkg_data_filename(\"data/table-sip.fits\"))\n     w = wcs.WCS(f[1].header)\n    assert isinstance(w.sip, wcs.Sip)\n\n", "    def test_to_fits_2():\n        \"\"\"\n        Test to_fits() with SIP distortion and multiple extensions.\n        \"\"\"\n        fits_name = get_pkg_data_filename('data/sip.fits')\n        w = wcs.WCS(fits_name)\n        wfits = w.to_fits()\n        assert isinstance(wfits, fits.HDUList)\n        assert isinstance(wfits[0], fits.PrimaryHDU)\n        assert isinstance(wfits[1], fits.ImageHDU)\n\n", "def test_sip_with_special_projection():\n    \"\"\"\n    Test reading SIP with special projection (e.g. 'GLON-CAR').\n    \"\"\"\n    hdr = get_pkg_data_contents(\"data/sip-gloncar.hdr\")\n    w = wcs.WCS(hdr)\n    assert isinstance(w.sip, wcs.Sip)\n    assert w.wcs.ctype[0] == 'GLON-CAR'\n    assert w.wcs.ctype[1] == 'GLAT-CAR'\n"], "sample_836": ["def test_ovr_decision_function():\n    random_state = np.random.RandomState(0)\n    n_samples = 20\n    n_classes = 3\n    predictions = random_state.randint(0, 2, size=(n_samples, 3))\n    confidences = random_state.rand(n_samples, 3)\n    \n    decision_function = _ovr_decision_function(predictions, confidences, n_classes)\n    assert decision_function.shape == (n_samples, n_classes)\n", "    def test_ovr_decision_function():\n        # Test _ovr_decision_function\n\n        n_classes = 3\n        n_samples = 4\n\n        predictions = np.array([[0, 1, 1], [1, 0, 1], [1, 1, 0], [0, 0, 1]])\n        confidences = np.array([[0.1, 0.9, 0.2], [0.8, 0.1, 0.3],\n                                 [0.7, 0.2, 0.6], [0.3, 0.2, 0.8]])\n\n        expected_decision_function = np.array([[-0.1, 0.9, 0.4],\n                                               [0.9, -0.1, 0.5],\n                                               [0.9, 0.4, -0.1],\n                                               [-0.1, -0.1, 0.9]])\n        decision_function = _ovr_decision_function(predictions, confidences,\n                                                   n_classes)\n\n        assert_allclose(decision_function, expected_decision_function)\n", "def test_ovr_decision_function():\n    n_samples = 10\n    n_classes = 3\n    predictions = np.array([\n        [0, 1, 0],\n        [1, 0, 0],\n        [1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1],\n        [0, 1, 0],\n        [1, 0, 0],\n        [0, 0, 1],\n        [0, 1, 0],\n        [1, 0, 0],\n    ])\n    confidences = np.array([\n        [-0.5, 0.8, -0.3],\n        [0.9, -0.6, -0.2],\n        [0.7, -0.4, -0.1],\n        [-0.3, 0.9, -0.6],\n        [-0.1, -0.2, 0.9],\n        [-0.4, 0.6, -0.2],\n        [0.8, -0.5, -0.3],\n        [-0.2, -0.3, 0.8],\n        [-0.6, 0.7, -0.1],\n        [0.6, -0.8, -0.2],\n    ])\n\n    expected_decision_function = np.array([\n        [-0.33, 0.83, -0.5],\n        [0.9, -0.6, -0.2],\n        [0.7, -0.4, -0.1],\n        [-0.33, 0.9, -0.6],\n        [-0.1, -0.2, 0.9],\n        [-0.4, 0.6, -0.2],\n        [0.8, -0.5, -0.3],\n        [-0.2, -0.3, 0.8],\n        [-0.6, 0.7, -0.1],\n        [0.6, -0.8,", "def test_ovr_decision_function():\n    # Test _ovr_decision_function on a simple example\n\n    predictions = np.array([[0, 1], [1, 0], [0, 1]])\n    confidences = np.array([[0.8, -0.2], [-0.3, 0.7], [0.6, -0.4]])\n    n_classes = 3\n    expected_decision_function = np.array([[ -0.2, 0.5],\n                                         [ 0.4, -0.5],\n                                         [ 0.2,  0.1]])\n    decision_function = _ovr_decision_function(predictions, confidences,\n                                               n_classes)\n    assert_allclose(decision_function, expected_decision_function)\n\n", "def test_ovr_decision_function():\n    n_samples = 5\n    n_classes = 3\n    predictions = np.array([[0, 1, 2], [1, 0, 2], [2, 0, 1],\n                           [0, 2, 1], [1, 2, 0]])\n    confidences = np.array([[0.9, -0.5, -0.4], [-0.2, 0.7, -0.5],\n                           [-0.4, -0.3, 0.7], [0.8, -0.6, -0.2],\n                           [-0.1, 0.6, -0.5]])\n    expected_decision_function = np.array([[0., 1., -1.],\n                                          [1., 0., -1.],\n                                          [-1., 0., 1.],\n                                          [0., -1., 1.],\n                                          [1., -1., 0.]])\n    decision_function = _ovr_decision_function(predictions, confidences,\n                                                n_classes)\n    assert_array_almost_equal(decision_function, expected_decision_function)\n"], "sample_782": ["compilation error", "compilation error", "    def func_err(X):\n        return 'invalid'", "compilation error", "    def func(X):\n        return [0]\n"], "sample_762": ["    def test_get_params_with_set_params_nested():\n        # Test interaction between set_params and get_params with nested\n        # pipelines\n\n        clf = Pipeline([(\"svc\", SVC())])\n        clf.set_params(svc__C=10)\n        params = clf.get_params()\n        assert params['svc__C'] == 10\n", "    def test_clone_with_none_params():\n        # Test cloning an estimator with None as parameter value\n        clf = MyEstimator(empty=None)\n        clf2 = clone(clf)\n        assert clf2.empty is None\n", "    def test_clone_sparse_matrix_empty():\n        sparse_matrix = sp.csr_matrix(np.array([]))\n        clf = MyEstimator(empty=sparse_matrix)\n        clf2 = clone(clf)\n        assert clf.empty.__class__ is clf2.empty.__class__\n        assert_array_equal(clf.empty.toarray(), clf2.empty.toarray())\n", "    def test_get_params_with_empty_dict():\n        estimator = MyEstimator()\n        assert_dict_equal(estimator.get_params(), {})\n", "    def test_get_params_on_inherited_class():\n        estimator = MultiInheritanceEstimator()\n        params = estimator.get_params()\n        assert \"attribute_pickled\" in params\n        assert \"_attribute_not_pickled\" not in params\n"], "sample_686": ["def test_warning_captured_hook_is_deprecated(testdir) -> None:\n    testdir.makepyfile(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    with pytest.warns(\n        DeprecationWarning,\n        match=\"The pytest_warning_captured is deprecated and will be removed\",\n    ):\n        testdir.runpytest(\"--capture=no\")\n\n", "def test_pytest_collect_module_deprecated(name):\n    with pytest.warns(DeprecationWarning, match=name):\n        getattr(pytest.collect, f\"pytest_collect_{name.lower()}\")\n\n", "def test_pytest_collect_module_deprecated(name):\n    with pytest.warns(\n        pytest.PytestDeprecationWarning,\n        match=f\"pytest.collect.{name} was moved to pytest.{name}\",\n    ):\n        getattr(pytest.collect, name)\n", "def test_pytest_collect_module_deprecated2(name):\n    with pytest.warns(DeprecationWarning, match=f\"pytest.collect.{name}\"):\n        getattr(pytest, name)\n", "def test_warning_captured_hook_deprecated(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    with pytest.warns(DeprecationWarning) as record:\n        testdir.runpytest(\"-p\", \"pytest_warning_captured\")\n        assert \"pytest_warning_captured is deprecated and will be removed in a future release.\" in str(record[0].message)\n\n\n"], "sample_718": ["compilation error", "        def fit(self, X, y):\n            return self\n", "compilation error", "compilation error", "compilation error"], "sample_1137": ["    def test_quantity_simplify():\n        q1 = Quantity('q1')\n        q2 = Quantity('q2')\n        q1.set_global_relative_scale_factor(10, meter)\n        q2.set_global_relative_scale_factor(100, centimeter)\n        assert quantity_simplify(q1 + q2) == 110*centimeter\n\n", "def test_quantity_simplify_with_prefixes():\n    q = kilo*meter\n\n    assert quantity_simplify(q) == 1000*meter\n    assert quantity_simplify(q*meter) == 1000*meter**2\n    assert quantity_simplify(q/meter) == 1000\n    assert quantity_simplify(q*k*meter) == 1000**2*meter**2\n\n", "compilation error", "compilation error", "compilation error"], "sample_648": ["    def test_mark_expressions_with_spaces(pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.skipif(\"sys.platform == 'win32'\")\n                pass\n\n            @pytest.mark.xfail(\"True\")\n                pass\n        \"\"\"\n        )\n\n        reprec = pytester.inline_run(\"-m\", \"skipif\")\n        passed, skipped, failed = reprec.countoutcomes()\n        assert skipped == 1\n        assert passed == failed == 0\n\n        reprec = pytester.inline_run(\"-m\", \"xfail\")\n        passed, skipped, failed = reprec.countoutcomes()\n        assert skipped == 1\n        assert passed == failed == 0\n", "def test_marker_expr_eval_failure_handling_with_warning(pytester: Pytester) -> None:\n    foo = pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.internal_err\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest(foo, \"-m\", \"NOT internal_err\")\n    result.stderr.fnmatch_lines([\"*WARNING: Could not evaluate expression 'NOT internal_err'*\"])\n    assert result.ret == ExitCode.OK\n", "    def test_marker_expression_and(pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.slow\n            @pytest.mark.db\n                pass\n\n            @pytest.mark.slow\n                pass\n\n            @pytest.mark.db\n                pass\n        \"\"\"\n        )\n        reprec = pytester.inline_run(\"-m\", \"slow and db\")\n        assert len(reprec.getcalls(\"pytest_collection_modifyitems\")) == 1\n        passed, skipped, failed = reprec.countoutcomes()\n        assert passed == 1\n        assert skipped == failed == 0\n", "    def test_parametrize_duplicate_marks(pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.foo\n            @pytest.mark.foo\n                pass\n            \"\"\"\n        )\n        result = pytester.runpytest()\n        result.assert_outcomes(passed=1)\n", "    def test_marker_expr_eval_failure_handling_with_quotes(pytester: Pytester) -> None:\n        foo = pytester.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.internal_err\n                pass\n        \"\"\"\n        )\n        result = pytester.runpytest(foo, \"-m\", \"'NOT internal_err'\")\n        result.stderr.fnmatch_lines([f\"ERROR: Wrong expression passed to '-m': 'NOT internal_err': *\"])\n        assert result.ret == ExitCode.USAGE_ERROR\n"], "sample_42": ["def test_equivalent_units_errors():\n    with pytest.raises(TypeError):\n        u.m.to(u.kg, equivalencies=[(u.m,)])\n    with pytest.raises(TypeError):\n        u.m.to(u.kg, equivalencies=[(u.m, 5)])\n    with pytest.raises(ValueError):\n        u.m.to(u.kg, equivalencies=[(u.m, u.kg, u.s)])\n\n", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_871": ["def test_silhouette_samples_precomputed_empty():\n    # Test silhouette_samples with empty distance matrix and labels\n    X = np.array([])\n    labels = np.array([])\n    with pytest.raises(ValueError, match=\"Distance matrix contains no samples\"):\n        silhouette_samples(X, labels, metric=\"precomputed\")\n", "    def test_silhouette_samples_empty_clusters():\n        X = np.array([[0.2, 0.1, 0.1, 0.2, 0.1, 1.6, 0.2, 0.1]], dtype=np.float32).T\n        y = [0, 0, 0, 0, 1, 1, 1, 1]\n\n        # Introduce an empty cluster\n        y[0] = 2\n\n        with pytest.raises(ValueError, match=\"Empty cluster\"):\n            silhouette_samples(X, y)\n\n", "compilation error", "def test_silhouette_score_empty_cluster():\n    \"\"\"Test silhouette score when an empty cluster exists.\"\"\"\n    X = np.array([[1, 2], [2, 1], [1, 1], [3, 3]])\n    labels = np.array([0, 0, 0, 1])\n\n    # This should NOT raise an error\n    score = silhouette_score(X, labels)\n    assert np.isnan(score)\n", "compilation error"], "sample_395": ["    def test_watch_for_template_changes_no_dirs(self, mock_get_dirs):\n        mock_get_dirs.return_value = set()\n        mock_reloader = mock.MagicMock()\n        autoreload.watch_for_template_changes(mock_reloader)\n        mock_get_dirs.assert_called_once()\n        self.assertEqual(mock_reloader.watch_dir.call_count, 0)\n", "    def test_template_changed_with_single_loader(self):\n        template_path = Path(__file__).parent / \"templates\" / \"index.html\"\n        self.assertTrue(autoreload.template_changed(None, template_path))\n", "    def test_watch_for_template_changes_calls_get_template_directories(self, mock_get_dirs):\n        mock_reloader = mock.MagicMock()\n        mock_get_dirs.return_value = {Path(__file__).parent / \"templates\"}\n        autoreload.watch_for_template_changes(mock_reloader)\n        mock_get_dirs.assert_called_once()\n", "    def test_watch_for_template_changes_no_directories(self, mock_get_directories):\n        mock_get_directories.return_value = set()\n        mock_reloader = mock.MagicMock()\n        autoreload.watch_for_template_changes(mock_reloader)\n        self.assertEqual(mock_reloader.watch_dir.call_count, 0)\n", "    def test_watch_for_template_changes_no_templates(self, mock_reset_loaders,\n                                               mock_get_template_directories):\n        mock_get_template_directories.return_value = set()\n        mock_reloader = mock.MagicMock()\n        autoreload.watch_for_template_changes(mock_reloader)\n        self.assertEqual(mock_reset_loaders.call_count, 0)\n        self.assertEqual(mock_reloader.watch_dir.call_count, 0)\n\n"], "sample_326": ["    def test_urlize_with_trim_url_limit(self):\n        text1 = 'This is a long url: https://www.example.com/very/very/very/very/long/path/to/something?param1=value1&param2=value2'\n        expected1 = 'This is a long url: <a href=\"https://www.example.com/very/very/very/very/long/path/to/something?param1=value1&param2=value2\">https://www.example.com/very/\u2026</a>'\n        text2 = 'This is a long url: https://www.example.com/very/very/very/very/long/path/to/something?param1=value1&param2=value2'\n        expected2 = 'This is a long url: <a href=\"https://www.example.com/very/very/very/very/long/path/to/something?param1=value1&param2=value2\">https://www.example.com/very/very/very/very/long/path/to/something?param1=value1&param2=value2</a>'\n\n        self.assertEqual(urlize(text1, trim_url_limit=20), expected1)\n        self.assertEqual(urlize(text2, trim_url_limit=None), expected2)\n\n", "    def test_urlize_long_urls(self):\n        long_url = 'https://www.example.com/very/long/path/with/lots/of/segments?query_string=with+lots+of+parameters&another=value#fragment'\n        expected = '<a href=\"%s\">%s\u2026</a>' % (\n            long_url,\n            long_url[:max(0, 20)],\n        )\n        self.assertEqual(urlize(long_url, trim_url_limit=20), expected)\n", "    def test_urlize_trim_url_limit(self):\n        self.assertEqual(\n            urlize('This is a very long URL: https://www.example.com/very/long/path/to/a/resource?query=parameters#fragment', trim_url_limit=20),\n            'This is a very long URL: <a href=\"https://www.example.com/very/long/path/to/a/resource?query=parameters#fragment\">https://www.example.com/very/long/path\u2026</a>'\n        )\n", "    def test_urlize_trim_url_limit(self):\n        self.assertEqual(\n            urlize(\"Visit https://www.example.com/very/long/url/path\", trim_url_limit=20),\n            \"Visit <a href=\\\"https://www.example.com/very/long/url/path\\\">https://www.example.com/very/long/url\u2026</a>\"\n        )\n", "    def test_urlize_trim_url_limit(self):\n        url = 'https://www.example.com/very/long/path/with/many/segments'\n        expected = (\n            'Search for <a href=\"%s\">%s</a> and see.' % (url, 'https://www.example.com/very/long/path/\u2026')\n        )\n        self.assertEqual(urlize('Search for %s and see.' % url, trim_url_limit=30), expected)\n"], "sample_407": ["    def test_fk_to_small_autofield_with_custom_pk(self):\n        us = Country.objects.create(name=\"United States\", id=100)\n        City.objects.create(country=us, name=\"Chicago\")\n\n        uk = Country.objects.create(name=\"United Kingdom\", id=200)\n        City.objects.create(country=uk, name=\"London\")\n        City.objects.create(country=uk, name=\"Edinburgh\")\n", "    def test_create_with_custom_pk_related_object_not_saved(self):\n        p = ParentStringPrimaryKey()\n        c = ChildStringPrimaryKeyParent(parent=p)\n        with self.assertRaises(ValueError):\n            c.save()\n", "    def test_reverse_to_field_relation_creation_update(self):\n        parent = Parent.objects.create(name=\"jeff\")\n        child = ToFieldChild.objects.create(parent_id=parent.name)\n        child.parent.name = \"jane\"\n        child.save()\n        with self.assertNumQueries(0):\n            self.assertEqual(child.parent.name, \"jane\")\n", "    def test_cached_relation_invalidated_on_delete(self):\n        \"\"\"\n        Model.delete() invalidates stale ForeignKey relations.\n        \"\"\"\n        self.assertEqual(self.a.reporter, self.r)  # caches a.reporter\n        self.r.delete()\n        with self.assertRaises(models.ObjectDoesNotExist):\n            self.a.reporter\n", "    def test_to_field_related_manager_filtering(self):\n        p1 = Parent.objects.create(name=\"Parent 1\")\n        p2 = Parent.objects.create(name=\"Parent 2\")\n        ToFieldChild.objects.create(parent=p1, parent_id=\"p1\")\n        ToFieldChild.objects.create(parent=p2, parent_id=\"p2\")\n\n        self.assertSequenceEqual(\n            Parent.objects.filter(to_field_children__parent_id=\"p1\"), [p1]\n        )\n"], "sample_524": ["compilation error", "compilation error", "compilation error", "compilation error", "    def test_colorbar_update_ticks():\n        fig, ax = plt.subplots()\n        pc = ax.pcolormesh(np.random.randn(10, 10))\n        cb = fig.colorbar(pc)\n        cb.set_ticks([0.5, 1, 1.5])\n        assert np.allclose(cb.get_ticks(), [0.5, 1, 1.5])\n        cb.set_ticks([])\n        assert len(cb.get_ticks()) == 0\n"], "sample_443": ["    def test_cache_middleware_with_custom_cache_alias(self):\n        cache_alias = \"custom_cache\"\n        Middleware = CacheMiddleware(\n            hello_world_view, cache_alias=cache_alias\n        )\n\n        request = self.factory.get(\"/view/\")\n        Middleware.process_request(request)\n        Middleware.cache = caches[\"invalid_cache\"]\n\n        with self.assertRaisesMessage(InvalidCacheBackendError, msg):\n           Middleware.process_response(request, hello_world_view(request, \"1\"))\n\n", "    def test_get_cache_key_with_post_request(self):\n        request = self.factory.post(self.path, {\"test\": 1})\n        template = engines[\"django\"].from_string(\"This is a test\")\n        response = TemplateResponse(HttpRequest(), template)\n        # Expect None if no headers have been set yet.\n        self.assertIsNone(get_cache_key(request))\n        # Set headers to an empty list.\n\n        learn_cache_key(request, response)\n        # POST requests shouldn't be cached\n        self.assertIsNone( get_cache_key(request))\n", "    def test_cache_handler_with_custom_cache_timeout(self):\n        test_caches = CacheHandler(\n            {\n                \"custom_timeout\": {\n                    \"BACKEND\": \"django.core.cache.backends.locmem.LocMemCache\",\n                    \"TIMEOUT\": 10,\n                },\n            }\n        )\n        self.assertEqual(test_caches[\"custom_timeout\"].default_timeout, 10)\n", "    def test_cache_middleware_with_custom_cache_alias(self):\n        \"\"\"\n        Test that CacheMiddleware can be configured with a custom cache alias.\n        \"\"\"\n        middleware = CacheMiddleware(hello_world_view, cache_alias=\"other\")\n        request = self.factory.get(\"/view/\")\n\n        # Put the request through the request middleware\n        result = middleware.process_request(request)\n        self.assertIsNone(result)\n\n        # Verify that the correct cache is used\n        response = hello_world_view(request, \"1\")\n        response = middleware.process_response(request, response)\n\n        # Repeating the request should result in a cache hit\n        result = middleware.process_request(request)\n        self.assertIsNotNone(result)\n        self.assertEqual(result.content, b\"Hello World 1\")\n", "    def test_cache_backend_options(self):\n        test_caches = CacheHandler(\n            {\n                \"test_backend\": {\n                    \"BACKEND\": \"django.core.cache.backends.locmem.LocMemCache\",\n                    \"PARAM1\": \"value1\",\n                    \"PARAM2\": \"value2\",\n                },\n            }\n        )\n        backend = test_caches[\"test_backend\"]\n        self.assertEqual(backend.param1, \"value1\")\n        self.assertEqual(backend.param2, \"value2\")\n"], "sample_265": ["    def test_template_debug_information(self):\n        \"\"\"\n        Test if template debug information is preserved when reraised\n        TemplateDoesNotExist exceptions.\n        \"\"\"\n        engine = DjangoTemplates({\n            'DIRS': [],\n            'APP_DIRS': False,\n            'NAME': 'django',\n            'OPTIONS': {},\n        })\n\n        try:\n            engine.get_template('nonexistent.html')\n        except TemplateDoesNotExist as exc:\n            # Assert template debug information is present\n            self.assertTrue(hasattr(exc, 'template_debug'))\n        \n            # Reraise the exception\n            reraise(exc, engine)\n", "    def test_get_installed_libraries(self):\n        with override_settings(INSTALLED_APPS=['template_backends.apps.good']):\n            libraries = get_installed_libraries()\n            self.assertIn('good_tags', libraries)\n            self.assertIn('subpackage.tags', libraries)\n", "    def test_get_installed_libraries_empty_app(self):\n        \"\"\"\n        Test to ensure that empty templatetags directory in an installed\n        application does not cause issues.\n        \"\"\"\n        with override_settings(INSTALLED_APPS=['template_backends.apps.emptyapp']):\n            engine = DjangoTemplates({\n                'DIRS': [],\n                'APP_DIRS': False,\n                'NAME': 'django',\n                'OPTIONS': {},\n            })\n            self.assertEqual(\n                engine.get_templatetag_libraries({}),\n                {},\n\n            )\n\n", "    def test_get_templatetag_libraries_empty(self):\n        engine = DjangoTemplates({\n            'DIRS': [],\n            'APP_DIRS': False,\n            'NAME': 'django',\n            'OPTIONS': {\n            },\n        })\n\n        self.assertEqual(\n            engine.get_templatetag_libraries({}),\n        )\n", "    def test_get_templatetag_libraries_empty(self):\n        engine = DjangoTemplates({\n            'DIRS': [],\n            'APP_DIRS': False,\n            'NAME': 'django',\n            'OPTIONS': {\n                'libraries': {},\n            },\n        })\n\n        self.assertEqual(engine.get_templatetag_libraries({}), {})\n\n"], "sample_802": ["def test_pipeline_memory_fit_params():\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    cachedir = mkdtemp()\n    try:\n        if LooseVersion(joblib_version) < LooseVersion('0.12'):\n            # Deal with change of API in joblib\n            memory = Memory(cachedir=cachedir, verbose=10)\n        else:\n            memory = Memory(location=cachedir, verbose=10)\n        # Test with Transformer + SVC and fit_params\n        clf = SVC(gamma='scale', probability=True, random_state=0)\n        transf = DummyTransf()\n        pipe = Pipeline([('transf', clone(transf)), ('svc', clf)])\n        cached_pipe = Pipeline([('transf', transf), ('svc', clf)],\n                               memory=memory)\n\n        # Memoize the transformer at the first fit\n        cached_pipe.fit(X, y, sample_weight=[1] * len(y))\n        pipe.fit(X, y, sample_weight=[1] * len(y))\n        # Get the time stamp of the transformer in the cached pipeline\n        ts = cached_pipe.named_steps['transf'].timestamp_\n        # Check that cached_pipe and pipe yield identical results\n        assert_array_equal(pipe.predict(X), cached_pipe.predict(X))\n        assert_array_equal(pipe.predict_proba(X), cached_pipe.predict_proba(X))\n        assert_array_equal(pipe.predict_log_proba(X),\n                           cached_pipe.predict_log_proba(X))\n        assert_array_equal(pipe.score(X, y), cached_pipe.score(X, y))\n        assert_array_equal(pipe.named_steps['transf'].means_,\n                           cached_pipe.named_steps['transf'].means_)\n        assert not hasattr(transf, 'means_')\n        # Check that we are reading the cache while fitting\n        # a second time with different sample_weight\n        cached_pipe.fit(X, y, sample_weight=[2] * len(y))\n        # Check that cached_pipe and pipe yield", "compilation error", "    def test_pipeline_memory_with_n_jobs(self):\n        iris = load_iris()\n        X = iris.data\n        y = iris.target\n        cachedir = mkdtemp()\n        try:\n            if LooseVersion(joblib_version) < LooseVersion('0.12'):\n                # Deal with change of API in joblib\n                memory = Memory(cachedir=cachedir, verbose=10)\n            else:\n                memory = Memory(location=cachedir, verbose=10)\n            # Test with Transformer + SVC\n            clf = SVC(gamma='scale', probability=True, random_state=0)\n            transf = DummyTransf()\n            pipe = Pipeline([('transf', clone(transf)), ('svc', clf)])\n            cached_pipe = Pipeline([('transf', transf), ('svc', clf)],\n                                   memory=memory, n_jobs=2)\n\n            # Memoize the transformer at the first fit\n            cached_pipe.fit(X, y)\n            pipe.fit(X, y)\n\n            # Check that cached_pipe and pipe yield identical results\n            assert_array_equal(pipe.predict(X), cached_pipe.predict(X))\n            assert_array_equal(pipe.predict_proba(X),\n                               cached_pipe.predict_proba(X))\n            assert_array_equal(pipe.predict_log_proba(X),\n                               cached_pipe.predict_log_proba(X))\n            assert_array_equal(pipe.score(X, y), cached_pipe.score(X, y))\n            assert_array_equal(pipe.named_steps['transf'].means_,\n                               cached_pipe.named_steps['transf'].means_)\n            \n            # Check that we are reading the cache while fitting\n            # a second time\n            cached_pipe.fit(X, y)\n\n            # Check that cached_pipe and pipe yield identical results\n            assert_array_equal(pipe.predict(X), cached_pipe.predict(X))\n            assert_array_equal(pipe.predict_proba(X),\n                               cached_pipe.predict_proba(X))\n            assert_array_equal(pipe.predict_log_proba(X),", "compilation error", "def test_pipeline_memory_location():\n\n    from sklearn.exceptions import NotFittedError\n\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    cachedir = mkdtemp()\n\n    try:\n\n        # Test with Transformer + SVC\n        clf = SVC(gamma='scale', probability=True, random_state=0)\n        transf = DummyTransf()\n        pipe = Pipeline([('transf', clone(transf)), ('svc', clf)])\n        cached_pipe = Pipeline([('transf', transf), ('svc', clf)],\n                               memory=cachedir)\n\n        # Memoize the transformer at the first fit\n        cached_pipe.fit(X, y)\n        # Check that the cache directory exists\n        assert os.path.isdir(cachedir)\n\n        # Remove the cache directory and check that refit raises NotFittedError\n        shutil.rmtree(cachedir)\n        assert_raises(NotFittedError, cached_pipe.predict, X)\n\n    finally:\n        shutil.rmtree(cachedir)\n"], "sample_744": ["compilation error", "def test_power_transformer_yeo_johnson_1d():\n    X = np.abs(X_1col)\n    for standardize in [True, False]:\n        pt = PowerTransformer(method='yeo-johnson', standardize=standardize)\n\n        X_trans = pt.fit_transform(X)\n        X_trans_func = power_transform(X, method='yeo-johnson',\n                                       standardize=standardize)\n\n        X_expected, lambda_expected = stats.yeo_johnson(X.flatten())\n\n        if standardize:\n            X_expected = scale(X_expected)\n\n        assert_almost_equal(X_expected.reshape(-1, 1), X_trans)\n        assert_almost_equal(X_expected.reshape(-1, 1), X_trans_func)\n\n        assert_array_almost_equal(X, pt.inverse_transform(X_trans))\n        assert_almost_equal(lambda_expected, pt.lambdas_[0])\n\n        assert len(pt.lambdas_) == X.shape[1]\n        assert isinstance(pt.lambdas_, np.ndarray)\n", "def test_power_transformer_yeo_johnson_standardize():\n    X = np.array([[-1, 0, 1], [2, 4, 6], [-3, -6, -9]])\n\n    pt = PowerTransformer(method='yeo-johnson', standardize=True)\n    X_trans = pt.fit_transform(X)\n    assert np.allclose(np.mean(X_trans, axis=0), 0)\n    assert np.allclose(np.std(X_trans, axis=0), 1)\n\n    pt = PowerTransformer(method='yeo-johnson', standardize=False)\n    X_trans = pt.fit_transform(X)\n    assert not np.allclose(np.mean(X_trans, axis=0), 0)\n    assert not np.allclose(np.std(X_trans, axis=0), 1)\n", "compilation error", "compilation error"], "sample_359": ["    def test_field_operation_with_unique_together(self):\n        operation = migrations.AddField(\n            'Model',\n            'field',\n            models.CharField(max_length=100),\n        )\n        operation.unique_together = [('field', 'other_field')]\n        self.assertEqual(\n            operation.describe(),\n            \"Add field field to Model and make it unique together with other_field\",\n        )\n", "    def test_references_field_by_related_name(self):\n        operation = FieldOperation('Model', 'field', models.ForeignKey('Other', models.CASCADE, related_name='related_to_field'))\n        self.assertIs(operation.references_field('Other', 'related_to_field', 'migrations'), True)\n        self.assertIs(operation.references_field('Other', 'whatever', 'migrations'), False)\n        self.assertIs(operation.references_field('Model', 'related_to_field', 'migrations'), False)\n\n", "    def test_field_operation_with_custom_field(self):\n        class CustomField(models.Field):\n            pass\n\n        operation = FieldOperation('Model', 'field', CustomField())\n\n        self.assertIsInstance(operation.field, CustomField)\n", "    def test_remove_field_ignore_swapped(self):\n        \"\"\"\n        Tests the RemoveField operation ignores swapped models.\n        \"\"\"\n        operation = migrations.RemoveField('Pony', 'pink')\n        project_state, new_state = self.make_test_state(\"test_rmfligsw\", operation)\n        # Test the database alteration\n        self.assertTableNotExists(\"test_rmfligsw_pony\")\n        with connection.schema_editor() as editor:\n            operation.database_forwards(\"test_rmfligsw\", editor, project_state, new_state)\n        self.assertTableNotExists(\"test_rmfligsw_pony\")\n        # And test reversal\n        with connection.schema_editor() as editor:\n            operation.database_backwards(\"test_rmfligsw\", editor, new_state, project_state)\n        self.assertTableNotExists(\"test_rmfligsw_pony\")\n", "    def test_references_m2m_through_field(self):\n        operation = FieldOperation(\n            'Model', 'field', models.ManyToManyField('Other', through='Through', through_fields=('through_field', 'other_field'))\n        )\n        self.assertIs(operation.references_field('Through', 'through_field', 'migrations'), True)\n        self.assertIs(operation.references_field('Other', 'other_field', 'migrations'), True)\n"], "sample_263": ["    def test_loaddata_with_file_zip_invalid_format(self):\n        msg = \"Unsupported file extension (.zip). Fixtures saved in 'dumpdata.json'.\"\n        with self.assertWarnsMessage(RuntimeWarning, msg):\n            self._dumpdata_assert(\n                ['fixtures'],\n                '[{\"pk\": 1, \"model\": \"fixtures.category\", \"fields\": {\"description\": \"Latest news stories\", \"title\": '\n                '\"News Stories\"}}, {\"pk\": 2, \"model\": \"fixtures.article\", \"fields\": {\"headline\": \"Poker has no place '\n                'on ESPN\", \"pub_date\": \"2006-06-16T12:00:00\"}}, {\"pk\": 3, \"model\": \"fixtures.article\", \"fields\": '\n                '{\"headline\": \"Time to reform copyright\", \"pub_date\": \"2006-06-16T13:00:00\"}}]',\n                filename='dumpdata.json.zip',\n            )\n", "    def test_loaddata_with_skip_errors(self):\n        # Create a fixture with an error\n        with open('fixtures/fixture_with_error.json', 'w') as f:\n            f.write(\n                '[{ \"model\": \"fixtures.article\", \"pk\": 1, \"fields\": {\"headline\": \"This will cause an error\" } },'\n                ' { \"model\": \"fixtures.article\", \"pk\": 2, \"fields\": {\"headline\": \"This will cause an error\" } }]'\n            )\n\n        # Try to load the fixture with skip_errors\n        with self.assertRaises(SystemExit):\n            management.call_command('loaddata', 'fixture_with_error', skip_errors=True, verbosity=0)\n        self.assertEqual(Article.objects.count(), 0)\n\n        # Now try to load the fixture without skip_errors\n        with self.assertRaisesMessage(\n            IntegrityError,\n            'This will cause an error'\n        ):\n            management.call_command('loaddata', 'fixture_with_error', verbosity=0)\n        self.assertEqual(Article.objects.count(), 0)\n", "    def test_loaddata_with_invalid_compressed_format(self):\n        # Load fixture 5 (compressed), using an invalid format\n        msg = \"Unsupported file format for compressed fixture: 'invalid_format.json.gz'.\"\n        with self.assertRaisesMessage(management.CommandError, msg):\n            management.call_command('loaddata', 'invalid_format.json.gz', verbosity=0)\n", "    def test_loading_using_empty_fixture(self):\n        # Load an empty fixture file.\n        management.call_command('loaddata', 'empty_fixture.json', verbosity=0)\n        self.assertEqual(Article.objects.count(), 0)\n\n", "    def test_loaddata_with_replacements(self):\n        management.call_command('loaddata', 'fixture1.json', verbosity=0)\n        original_headline = Article.objects.get(headline='Time to reform copyright').headline\n        management.call_command('loaddata', 'fixture1.json', replace=True, verbosity=0)\n        self.assertEqual(Article.objects.get(headline='Time to reform copyright').headline, original_headline)\n"], "sample_177": ["    def test_related_with_swappable(self):\n        new_apps = Apps(['migrations'])\n\n        class Author(models.Model):\n            name = models.CharField(max_length=255)\n            bio = models.TextField()\n            age = models.IntegerField(blank=True, null=True)\n\n            class Meta:\n                app_label = 'migrations'\n                apps = new_apps\n                swappable = 'TEST_SWAPPABLE_MODEL'\n\n        class Book(models.Model):\n            title = models.CharField(max_length=200)\n            author = models.ForeignKey(Author, models.CASCADE, related_name='books')\n            \n\n            class Meta:\n                app_label = 'migrations'\n                apps = new_apps\n\n        author_state = ModelState.from_model(Author)\n        book_state = ModelState.from_model(Book)\n        self.assertRelated(book_state, [author_state])\n", "    def test_proxy_fk(self):\n        A = self.create_model(\"A\", foreign_keys=[models.ForeignKey('B', models.CASCADE)])\n        B = self.create_model(\"B\", proxy=True)\n        C = self.create_model(\"C\", bases=(B,), proxy=True)\n        self.assertRelated(A, [B, C])\n        self.assertRelated(B, [A])\n        self.assertRelated(C, [A])\n", "    def test_proxy_fk(self):\n        A = self.create_model(\"A\")\n        B = self.create_model(\"B\", bases=(A,), proxy=True)\n        C = self.create_model(\"C\", foreign_keys=[models.ForeignKey('B', models.CASCADE)])\n        self.assertRelated(A, [B, C])\n        self.assertRelated(B, [A, C])\n        self.assertRelated(C, [A, B])\n", "    def test_abstract_and_concrete_fk(self):\n        A = self.create_model(\"A\", abstract=True, foreign_keys=[models.ForeignKey('B', models.CASCADE)])\n        B = self.create_model(\"B\")\n        C = self.create_model(\"C\", bases=(A,))\n        self.assertRelated(A, [B])\n        self.assertRelated(B, [A])\n        self.assertRelated(C, [B])\n", "    def test_proxy_fk(self):\n        A = self.create_model(\"A\")\n        B = self.create_model(\"B\", bases=(A,), proxy=True)\n        C = self.create_model(\"C\", foreign_keys=[models.ForeignKey('B', models.CASCADE)])\n        self.assertRelated(A, [B, C])\n        self.assertRelated(B, [A, C])\n        self.assertRelated(C, [A, B])\n"], "sample_1181": ["def test_scipy_sparse_matrix():\n    from scipy.sparse import coo_matrix\n    if not np:\n        skip(\"NumPy not installed\")\n\n    M = MatrixSymbol(\"M\", 3, 3)\n    printer = SciPyPrinter()\n\n    # test printing of a sparse matrix\n    sparse_mat = coo_matrix([[1, 0, 0], [0, 2, 0], [0, 0, 3]])\n    f = lambdify((M,), M, 'scipy.sparse')\n    assert printer.doprint(f(sparse_mat)) == repr(sparse_mat)\n", "    def test_scipy_piecewise(self):\n        from sympy.functions.piecewise import Piecewise\n        prntr = SciPyPrinter()\n        x = Symbol('x')\n        piecewise_func = Piecewise((1, x<0), (0, True))\n        result = prntr.doprint(piecewise_func)\n        assert result == 'scipy.piecewise((1, x < 0), (0, True))))'\n", "    def test_issue_17961():\n        if not np:\n            skip(\"NumPy not installed\")\n\n        M = MatrixSymbol('M', 3, 3)\n        expr = M.transpose() * M\n        f = lambdify(M, expr)\n        ma = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n        assert np.allclose(f(ma), ma.T@ma)\n", "def test_scipy_print_methods():\n    prntr = SciPyPrinter()\n    assert hasattr(prntr, '_print_betainc')\n    assert hasattr(prntr, '_print_airyai')\n    assert hasattr(prntr, '_print_loggamma')\n", "    def test_issue_20448():\n        if not np:\n            skip(\"NumPy not installed\")\n\n        M = eye(3)\n        N = MatrixSymbol('N', 3, 3)\n        expr = M + N\n        f = lambdify(N, expr, 'numpy')\n        assert (f(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])) == np.array([[2, 2, 3], [4, 6, 6], [7, 8, 10]])).all()\n"], "sample_435": ["    def test_html_autocomplete_attributes(self):\n        user = User.objects.get(username=\"testclient\")\n        form = AdminPasswordChangeForm(user)\n        self.assertEqual(\n            form.fields[\"password1\"].widget.attrs[\"autocomplete\"], \"new-password\"\n        )\n", "    def test_blank_password_fields(self):\n        user = User.objects.get(username=\"testclient\")\n        form = AdminPasswordChangeForm(user, {\"password1\": \"\", \"password2\": \"\"})\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data, {\"password1\": \"\", \"password2\": \"\"})\n        self.assertEqual(form.changed_data, [])  \n", "    def test_existing_password_not_changed(self):\n        user = User.objects.get(username=\"testclient\")\n        data = {\"password1\": user.password, \"password2\": user.password}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.changed_data, [])\n", "    def test_password_length_validation(self):\n        user = User.objects.get(username=\"testclient\")\n        data = {\"password1\": \"test\", \"password2\": \"test\"}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertEqual(\n            form.errors[\"password1\"],\n            [\n                \"Ensure this field has at least 8 characters.\",\n            ],\n        )\n        self.assertEqual(form.errors[\"password2\"],  # Same error for password2\n            [\n                \"Ensure this field has at least 8 characters.\",\n            ],\n        )\n        self.assertEqual(form.changed_data, [\"password\"])\n", "    def test_custom_user_model(self):\n        class CustomUser(User):\n            field = models.CharField(max_length=100)\n\n        CustomUser.objects.create_user(\"testuser\", \"test@example.com\", \"test\")\n        user = CustomUser.objects.get(username=\"testuser\")\n        data = {\"password1\": \"test123\", \"password2\": \"test123\"}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertTrue(form.is_valid())\n        form.save(commit=False)\n        self.assertEqual(form.changed_data, [\"password\"])\n\n"], "sample_137": ["    def test_replace_named_groups(self):\n        self.assertEqual(replace_named_groups(r'(?P<a>\\w+)/b/(\\w+)'), r'<a>/b/(\\w+)')\n        self.assertEqual(replace_named_groups(r'(?P<a>\\w+)/b/(?P<c>\\w+)'), r'<a>/b/<c>')\n        self.assertEqual(replace_named_groups(r'(?P<a>\\w+)/(?P<b>\\w+)/(?P<c>\\w+)'), r'<a>/<b>/<c>')\n        self.assertEqual(replace_named_groups(r'^((?P<a>\\w+))/(?P<b>\\w+)/(?P<c>\\w+)$'), \n                         r'^<a>/<b>/<c>$')\n        self.assertEqual(replace_named_groups(r'^((?P<a>\\w+))/b/(?P<c>\\w+)$'), r'^<a>/b/<c>$')\n\n", "    def test_replace_named_groups(self):\n        self.assertEqual(replace_named_groups('^(?P<a>\\w+)/b/(\\w+)$'), r'^<a>/b/(\\w+)$')\n        self.assertEqual(replace_named_groups('(?P<a>\\w+)/b/(?P<c>\\w+)$'), r'^<a>/b/<c>$')\n        self.assertEqual(replace_named_groups('^(?P<a>\\w+)/b/(\\w+)$'), r'^<a>/b/(\\w+)$')\n        self.assertEqual(replace_named_groups('^(?P<a>\\w+)/b/(?P<c>\\w+)$'), r'^<a>/b/<c>$')\n        self.assertEqual(replace_named_groups('^(?P<a>(x|y))/b/(\\w+)$'), r'^<a>/b/(\\w+)$')\n\n", "    def test_replace_named_groups(self):\n        self.assertEqual(replace_named_groups(r'^(?P<a>\\w+)/(?P<b>\\w+)$'), r'^<a>/<b>/$')\n        self.assertEqual(replace_named_groups(r'^(?P<a>\\w+)/b/(?P<c>\\w+)$'), r'^<a>/b/<c>/$')\n        self.assertEqual(replace_named_groups(r'^(?P<a>\\w+)/b/(\\w+)$'), r'^<a>/b/(\\w+)$')\n        self.assertEqual(replace_named_groups(r'^(?P<a>\\w+)/b/(?P<c>\\w+)$'), r'^<a>/b/<c>$')\n\n", "    def test_replace_named_groups(self):\n        self.assertEqual(replace_named_groups('^(?P<a>\\w+)/b/(\\w+)$'), '^(<a>)/b/(\\w+)$')\n        self.assertEqual(replace_named_groups('^(?P<a>\\w+)/b/(?P<c>\\w+)/$'), '^(<a>)/b/<c>/$')\n        self.assertEqual(replace_named_groups('^(?P<a>\\w+)/b/(\\w+)$'), '^(<a>)/b/(\\w+)$')\n        self.assertEqual(replace_named_groups('^(?P<a>\\w+)/b/(?P<c>\\w+)$'), '^(<a>)/b/<c>$')\n        self.assertEqual(replace_named_groups('^(?P<a>(x|y))/b/(\\w+)$'), '^(<a>)/b/(\\w+)$')\n        self.assertEqual(replace_named_groups('^(?P<a>(x|y))/b/(?P<c>\\w+)$'), '^(<a>)/b/<c>$')\n\n", "    def test_replace_named_groups(self):\n        self.assertEqual(replace_named_groups('^(?P<a>\\w+)/b/(\\w+)$'), r'^<a>/b/(\\w+)$')\n        self.assertEqual(replace_named_groups('^(?P<a>\\w+)/b/(?P<c>\\w+)$'), r'^<a>/b/<c>$')\n        self.assertEqual(replace_named_groups('^(?P<a>\\w+)/b/(\\w+)$'), r'^<a>/b/(\\w+)$')\n        self.assertEqual(replace_named_groups('^(?P<a>\\w+)/b/(?P<c>\\w+)'), r'^<a>/b/<c>')\n"], "sample_957": ["def test_stringify_type_hints_Union_with_None():\n    assert stringify(Union[str, None]) == \"Optional[str]\"\n", "def test_stringify_type_union_nested():\n    assert stringify(Union[int, Union[str, float]]) == \"Union[int, str, float]\"  # type: ignore\n", "def test_restify_type_union_operator():\n    assert restify(int | None) == \":obj:`~typing.Optional`\\\\ [:class:`int`]\"  # type: ignore\n    assert restify(int | str) == \":class:`int` | :class:`str`\"  # type: ignore\n    assert restify(int | str | None) == \":obj:`~typing.Optional`\\\\ [:class:`int` | :class:`str`]\"  # type: ignore\n", "    def test_stringify_type_hints_union_operator_nested():\n        assert stringify((int | None) | str) == \"Optional[int] | str\"  # type: ignore\n", "    def test_stringify_type_hints_ellipsis():\n        assert stringify(Ellipsis) == \"...\"\n"], "sample_414": ["    def test_many_to_many_with_existing_values(self):\n        from selenium.webdriver.common.by import By\n        event = Event.objects.create(name=\"Test Event\")\n        event.supporting_bands.add(Band.objects.get(id=42))\n        event.supporting_bands.add(Band.objects.get(id=98))\n        self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n        self.selenium.get(\n            self.live_server_url + reverse(\n                \"admin:admin_widgets_event_change\", args=(event.id,)\n            )\n        )\n        main_window = self.selenium.current_window_handle\n\n        # Existing values are pre-selected\n        self.assertEqual(\n            self.selenium.find_element(By.ID, \"id_supporting_bands\").get_attribute(\n                \"value\"\n            ),\n            \"42,98\",\n        )\n\n        # Open the popup window and click on one of the pre-selected bands\n        self.selenium.find_element(By.ID, \"lookup_id_supporting_bands\").click()\n        self.wait_for_and_switch_to_popup()\n        link = self.selenium.find_element(By.LINK_TEXT, \"Bogey Blues\")\n        self.assertIn(\"/band/42/\", link.get_attribute(\"href\"))\n        link.click()\n\n        # The field still contains the same ids\n        self.selenium.switch_to.window(main_window)\n        self.wait_for_value(\"#id_supporting_bands\", \"42,98\")\n\n", "    def test_raw_id_empty(self):\n        from selenium.webdriver.common.by import By\n\n        self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_widgets_event_add\")\n        )\n\n        # Empty raw id field should not raise error when saving.\n        self.selenium.find_element(By.NAME, \"_save\").click()\n        self.wait_for_page_loaded()\n\n        self.assertContains(\n            self.selenium.page_source, \"This field is required.\"\n        ) \n", "    def test_many_to_many_remove(self):\n        from selenium.webdriver.common.by import By\n\n        self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_widgets_event_add\")\n        )\n        main_window = self.selenium.current_window_handle\n\n        event = Event.objects.create(name=\"Test Event\")\n        event.supporting_bands.add(self.band_bop, self.band_green)\n\n        self.selenium.get(\n\n            self.live_server_url\n            + reverse(\"admin:admin_widgets_event_change\", args=(event.id,))\n        )\n\n        band_bop_selected = self.selenium.find_element(By.CSS_SELECTOR, 'option[value=\"1\"]')\n        self.assertTrue(band_bop_selected.is_selected())\n\n        band_green_selected = self.selenium.find_element(By.CSS_SELECTOR, 'option[value=\"2\"]')\n        self.assertTrue(band_green_selected.is_selected())\n\n        remove_button = self.selenium.find_element(By.ID, \"changelist-form-0-remove\")\n        remove_button.click()\n\n        self.wait_for_value(\"#id_supporting_bands\", \"2\")\n\n", "    def test_filter_vertical_select_all(self):\n        from selenium.webdriver.common.by import By\n        from selenium.webdriver.common.keys import Keys\n\n        self.school.students.set([self.lisa, self.peter])\n        self.school.alumni.set([self.lisa, self.peter])\n\n        self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n        self.selenium.get(\n            self.live_server_url\n            + reverse(\"admin:admin_widgets_school_change\", args=(self.school.id,))\n        )\n\n        self.wait_page_ready()\n\n        # Select all options in the 'from' box.\n\n        from_box = \"#id_students_from\"\n        choose_all_link = \"id_students_add_all_link\"\n        self.selenium.find_element(By.ID, choose_all_link).click()\n\n        selected_options = [\n            option.get_attribute(\"value\")\n            for option in self.selenium.find_elements(\n                By.CSS_SELECTOR, from_box + \" > option\"\n            )\n        ]\n        self.assertEqual(\n            selected_options,\n            [\n                str(self.arthur.id),\n                str(self.bob.id),\n                str(self.cliff.id),\n                str(self.jason.id),\n                str(self.jenny.id),\n                str(self.john.id),\n            ],\n\n        )\n\n        # Remove them.\n\n        remove_all_link = \"id_students_remove_all_link\"\n        self.selenium.find_element(By.ID, remove_all_link).click()\n\n        selected_options = [\n            option.get_attribute(\"value\")\n            for option in self.selenium.find_elements(\n                By.CSS_SELECTOR, from_box + \" > option\"\n            )\n        ]\n        self.assertEqual(selected_options, [])\n\n", "    def test_related_lookup(self):\n        from selenium.webdriver.common.by import By\n\n        self.admin_login(username=\"super\", password=\"secret\", login_url=\"/\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_widgets_album_add\")\n        )\n        main_window = self.selenium.current_window_handle\n\n        # No value has been selected yet\n        self.assertEqual(\n            self.selenium.find_element(By.ID, \"id_band\").get_attribute(\"value\"), \"\"\n        )\n\n        # Open the popup window and click on a band\n        self.selenium.find_element(By.ID, \"lookup_id_band\").click()\n        self.wait_for_and_switch_to_popup()\n        link = self.selenium.find_element(By.LINK_TEXT, \"Bogey Blues\")\n        self.assertIn(\"/band/42/\", link.get_attribute(\"href\"))\n        link.click()\n\n        # The field now contains the selected band's id\n        self.selenium.switch_to.window(main_window)\n        self.wait_for_value(\"#id_band\", \"42\")\n"], "sample_720": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_828": ["def test_haversine_distances_empty_input():\n    # Check haversine distance with empty input\n\n    X = np.array([])\n    Y = np.array([])\n    assert_raises(ValueError, haversine_distances, X, Y)\n", "def test_check_sparse_array_with_dense():\n    # Ensures that when XB is dense and XA is sparse, a sparse array\n    # is returned for XB.\n\n    rng = np.random.RandomState(0)\n    XA = rng.random_sample((5, 4))\n    XA_sparse = csr_matrix(XA)\n    XB = rng.random_sample((5, 4))\n    XA_checked, XB_checked = check_pairwise_arrays(XA_sparse, XB)\n    assert issparse(XA_checked)\n    assert issparse(XB_checked)\n", "compilation error", "compilation error", "compilation error"], "sample_586": ["    def test_concat_empty_dataset(self):\n        empty_ds = Dataset()\n        non_empty_ds = Dataset({\"foo\": (\"x\", [1, 2])})\n        actual = concat([empty_ds, non_empty_ds], dim=\"x\")\n        assert_identical(actual, non_empty_ds)\n        actual = concat([non_empty_ds, empty_ds], dim=\"x\")\n        assert_identical(actual, non_empty_ds)\n", "    def test_concat_empty_dataset(self):\n        empty_ds = Dataset()\n        assert_identical(empty_ds, concat([empty_ds, empty_ds], \"new_dim\"))\n", "    def test_concat_dask_array(self):\n        import dask.array as da\n\n        arrays = [\n            DataArray(da.from_array(np.zeros((3, 3)), 3), dims=[\"x\", \"y\"])\n            for _ in range(2)\n        ]\n        combined = concat(arrays, dim=\"z\")\n        assert isinstance(combined.data, da.Array)\n        assert combined.shape == (2, 3, 3)\n        assert combined.dims == (\"z\", \"x\", \"y\")\n", "    def test_concat_empty_dataset():\n        ds_empty = Dataset()\n        ds = Dataset({\"foo\": 0})\n        actual = concat([ds_empty, ds], dim=\"x\")\n        assert_identical(ds, actual)\n        \n", "    def test_concat_empty_datasets(self):\n        empty_ds = Dataset()\n        full_ds = Dataset({'a': (('x',), [1, 2])})\n        actual = concat([empty_ds, full_ds], dim='x')\n        assert_identical(actual, full_ds)\n"], "sample_290": ["    def test_operation_with_empty_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('', fields=[]),\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertIs(migration.suggest_name().startswith('auto_'), True)\n\n", "    def test_auto_naming_for_empty_migration(self):\n        class Migration(migrations.Migration):\n            operations = []\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'initial')\n\n", "    def test_suggest_name_with_dependency(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Person', fields=[]),\n            ]\n            dependencies = [('myapp', '0001_initial')]\n\n        migration = Migration('0002_person', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'person')\n\n", "    def test_operation_name_with_underscores(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('person_with_underscores', fields=[]),\n            ]\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'person_with_underscores')\n\n", "    def test_operation_with_custom_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    'Person',\n                    fields=[],\n                    name='MyCustomPersonModel',  \n                ),\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'mycustompersonmodel')\n"], "sample_832": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1141": ["compilation error", "compilation error", "    def test_MatrixSet_contains():\n        M = MatrixSet(2, 2, set=S.Reals)\n        X = Matrix([[1, 2], [3, 4]])\n        assert X in M\n        X = ZeroMatrix(2, 2)\n        assert X in M\n        raises(TypeError, lambda: A in M)\n        raises(TypeError, lambda: 1 in M)\n", "compilation error", "compilation error"], "sample_1165": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_211": ["    def test_get_context_data_with_paginate_by(self):\n        test_view = views.CustomMultipleObjectMixinView()\n        test_view.paginate_by = 2\n        response = test_view.get(self.rf.get('/'))\n        context = response.context\n        self.assertEqual(context['paginator'].per_page, 2)\n", "    def test_empty_queryset(self):\n        test_view = views.CustomMultipleObjectMixinView()\n        test_view.queryset = []\n        context = test_view.get_context_data()\n        self.assertEqual(context['object_list'], [])\n", "    def test_get_context_data_with_paginator(self):\n        test_view = views.CustomMultipleObjectMixinView()\n        test_view.paginate_by = 1\n        test_view.get(self.rf.get('/'))\n\n        context = test_view.get_context_data()\n        self.assertIn('paginator', context)\n        self.assertIn('page_obj', context)\n", "    def test_get_queryset_from_get(self):\n        test_view = views.CustomMultipleObjectMixinView()\n        request = self.rf.get('/')\n        test_view.request = request\n        queryset = test_view.get_queryset()\n        self.assertEqual(queryset, test_view.model.objects.all())\n", "    def test_get_context_data_with_paginate_by(self):\n        test_view = views.CustomMultipleObjectMixinView()\n        test_view.paginate_by = 2\n        context = test_view.get_context_data()\n        self.assertIn('paginator', context)\n        self.assertIn('page_obj', context)\n        self.assertEqual(context['paginator'].per_page, 2)\n"], "sample_15": ["        def test_jv_invalid_units(self, function):\n            with pytest.raises(u.UnitsError):\n                function(1.0 * u.m, 2.0 * u.s)\n            with pytest.raises(u.UnitsError):\n                function(1.0 * u.m / (2.0 * u.kg), 2.0 * u.s)\n", "        def test_spherical_jvn(self, function):\n            q = function(2.0 * u.m / (2.0 * u.m), 3.0 * u.m / (6.0 * u.m), 0.0 * u.radian)\n            assert q.unit == u.dimensionless_unscaled\n            assert q.value.shape == (4,)\n\n            q2 = function(np.ones(3) * u.m / (1.0 * u.m), np.array([2.0, 3.0, 6.0]) * u.m / (6.0 * u.m), np.array([0.0, np.pi/2, np.pi]) * u.radian)\n            assert q2.unit == u.dimensionless_unscaled\n            assert q2.value.shape == (3, 4)\n\n", "        def test_jv_invalid_units(self):\n            with pytest.raises(u.UnitsError):\n                sps.jv(1.0 * u.m, 1.0 * u.s)\n            with pytest.raises(u.UnitsError):\n                sps.jv(1.0 * u.m, 1.0 * u.kg)\n\n", "    def test_erf_inplace_array(self, function):\n        arr = np.arange(5.0) * u.dimensionless_unscaled\n        result = np.empty_like(arr)\n        function(arr, out=result)\n        expected = function(arr.value) * u.dimensionless_unscaled\n        assert np.all(result == expected)\n", "        def test_erf_inplace(self, function):\n            q = u.Quantity([1.0, 2.0, 3.0], u.dimensionless_unscaled)\n            q_copy = q.copy()\n            function(q, out=q)\n            assert_allclose(q.value, function(q_copy.value))\n            assert q.unit == u.dimensionless_unscaled\n"], "sample_1178": ["compilation error", "compilation error", "compilation error", "def test_PointerDereference():\n    i = symbols('i', integer=True)\n    x = symbols('x', real=True)\n    ptr = Pointer(x)\n    deref = ptr[i]\n    assert deref.symbol is ptr.symbol\n    assert deref.indices == (i,)\n    assert deref.func(*deref.args) == deref\n\n", "compilation error"], "sample_33": ["def test_dtype_bytes_or_chars():\n    assert misc.dtype_bytes_or_chars(np.dtype('<U5')) == 5\n    assert misc.dtype_bytes_or_chars(np.dtype('<f8')) == 8\n    assert misc.dtype_bytes_or_chars(np.dtype('int32')) is None\n", "compilation error", "def test_dtype_bytes_or_chars():\n    assert misc.dtype_bytes_or_chars(np.dtype('<U5')) == 5\n    assert misc.dtype_bytes_or_chars(np.dtype('<f8')) == 8\n    assert misc.dtype_bytes_or_chars(np.dtype('int32')) is None\n", "compilation error", "compilation error"], "sample_464": ["    def test_unicode_attachment_no_filename_from_buffer(self):\n        response = FileResponse(\n            io.BytesIO(b\"binary content\"),\n            as_attachment=True,\n            filename=\"\u795d\u60a8\u5e73\u5b89.odt\",\n            content_type=\"application/vnd.oasis.opendocument.text\",\n        )\n        self.assertEqual(\n            response.headers[\"Content-Type\"],\n            \"application/vnd.oasis.opendocument.text\",\n        )\n        self.assertEqual(\n            response.headers[\"Content-Disposition\"],\n            \"attachment; filename*=utf-8''%E7%A5%9D%E6%82%A8%E5%B9%B3%E5%AE%89.odt\",\n        )\n\n", "    def test_file_with_invalid_filename(self):\n        with tempfile.NamedTemporaryFile(suffix=\".txt\") as tmp:\n            tmp.write(b\"content\")\n            tmp.flush()\n            os.rename(tmp.name, \"invalid:filename.txt\")\n\n            with self.assertRaises(ValueError):\n                FileResponse(open(\"invalid:filename.txt\", \"rb\"))\n", "    def test_file_response_with_content_type(self):\n        with tempfile.NamedTemporaryFile(suffix=\".txt\") as tmp:\n            tmp.write(b\"Hello World!\")\n            tmp.flush()\n\n            response = FileResponse(tmp.name, content_type=\"text/plain\")\n            self.assertEqual(response.headers[\"Content-Type\"], \"text/plain\")\n", "    def test_file_like_with_tell(self):\n        class FileLike:\n                self._data = b\"binary content\"\n                self._position = 0\n\n                if size == -1:\n                    ret = self._data[self._position:]\n                    self._position = len(self._data)\n                else:\n                    ret = self._data[self._position: self._position + size]\n                    self._position += size\n                return ret\n\n                if whence == io.SEEK_SET:\n                    self._position = offset\n                elif whence == io.SEEK_CUR:\n                    self._position += offset\n                elif whence == io.SEEK_END:\n                    self._position = len(self._data) + offset\n                else:\n                    raise ValueError(\"Invalid whence value\")\n\n                return self._position\n\n        file_like = FileLike()\n        response = FileResponse(file_like)\n        self.assertEqual(list(response), [b\"binary content\"])\n        self.assertEqual(file_like.tell(), len(file_like._data))\n\n", "    def test_file_not_seekable(self):\n        with open(__file__, \"rb\") as f:\n            filelike = io.BytesIO(f.read())\n            filelike.seekable = lambda: False\n            response = FileResponse(filelike)\n            self.assertEqual(response.headers[\"Content-Length\"], str(os.path.getsize(__file__)))\n\n"], "sample_153": ["    def test_database_checks_called_with_specific_databases(self):\n        with mock.patch('django.db.backends.base.validation.BaseDatabaseValidation.check') as mocked_check:\n            check_database_backends(databases={'default'})\n            mocked_check.assert_called_once_with('default')\n\n", "    def test_database_checks_router(self, mocked_check):\n        from django.db import router\n        mocked_router_allow_migrate_model = mock.patch.object(router, 'allow_migrate_model').start()\n        mocked_router_allow_migrate_model.side_effect = lambda db, model: db == 'default'\n        check_database_backends(databases=self.databases)\n\n        mocked_router_allow_migrate_model.assert_any_call('default', mock.ANY)\n        mocked_router_allow_migrate_model.assert_any_call('other', mock.ANY)\n\n        mocked_check.assert_called_once_with('default')\n\n", "    def test_check_constraints_supports_check_constraints(self):\n        class MyModel(models.Model):\n            field = models.IntegerField(\n                constraints=[models.CheckConstraint(check=models.Q(field__gt=0), name='positive_field')]\n            )\n\n        errors = MyModel._check_constraints(databases=['default'])\n        self.assertEqual(len(errors), 0)\n", "    def test_model_unpickle(self):\n        with mock.patch('django.apps.apps.get_model', return_value=MockModel):\n            unpickled_model = model_unpickle(('app_name', 'MockModel'))\n        self.assertEqual(type(unpickled_model), MockModel)\n", "    def test_database_checks_ignore_invalid_databases(self):\n        mocked_check = mock.MagicMock()\n        with mock.patch('django.db.backends.base.validation.BaseDatabaseValidation.check', mocked_check):\n            check_database_backends(databases={'invalid_database'})\n        self.assertFalse(mocked_check.called)\n\n"], "sample_856": ["compilation error", "compilation error", "def test_time_series_split_empty():\n    X = []\n    tscv = TimeSeriesSplit(2)\n    with pytest.raises(ValueError, match=\"Cannot have number of splits.*greater\"):\n        next(tscv.split(X))\n", "compilation error", "compilation error"], "sample_51": ["    def test_iso8601_duration(self):\n        test_values = (\n            ('P1DT2H30M45S', timedelta(days=1, hours=2, minutes=30, seconds=45)),\n            ('PT2H30M45S', timedelta(hours=2, minutes=30, seconds=45)),\n            ('P1D', timedelta(days=1)),\n            ('PT1H', timedelta(hours=1)),\n            ('PT30M', timedelta(minutes=30)),\n            ('PT45S', timedelta(seconds=45)),\n            ('P1DT2H30M45.123S', timedelta(days=1, hours=2, minutes=30, seconds=45, microseconds=123000)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n\n", "    def test_iso8601_format(self):\n        test_values = (\n            ('PT1H30M', timedelta(hours=1, minutes=30)),\n            ('PT1H', timedelta(hours=1)),\n            ('PT30M', timedelta(minutes=30)),\n            ('P1D', timedelta(days=1)),\n            ('P1DT2H30M', timedelta(days=1, hours=2, minutes=30)),\n            ('P1DT2H30M10.5S', timedelta(days=1, hours=2, minutes=30, seconds=10.5)),\n            ('-P1DT2H30M10.5S', timedelta(days=-1, hours=-2, minutes=-30, seconds=-10.5)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n\n", "    def test_iso8601(self):\n        test_values = (\n            ('P1D', timedelta(days=1)),\n            ('PT1H', timedelta(hours=1)),\n            ('PT1M', timedelta(minutes=1)),\n            ('PT1S', timedelta(seconds=1)),\n            ('P1DT1H1M1S', timedelta(days=1, hours=1, minutes=1, seconds=1)),\n            ('P-1DT1H1M1S', timedelta(days=-1, hours=1, minutes=1, seconds=1)),\n            ('PT1.5S', timedelta(seconds=1.5)),\n            ('PT1M30.5S', timedelta(minutes=1, seconds=30.5)),\n            ('P1DT1H30.5M', timedelta(days=1, hours=1, minutes=30.5)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n", "    def test_iso_8601(self):\n        test_values = (\n            ('P1DT10H30M', timedelta(days=1, hours=10, minutes=30)),\n            ('P1D', timedelta(days=1)),\n            ('PT10H30M', timedelta(hours=10, minutes=30)),\n            ('PT3H', timedelta(hours=3)),\n            ('PT30S', timedelta(seconds=30)),\n            ('P2DT5H30M10S', timedelta(days=2, hours=5, minutes=30, seconds=10)),\n            ('P1D-0:00:01', timedelta(days=1, seconds=-1)),\n            ('-P1D-0:00:01', timedelta(days=-1, seconds=-1)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n\n", "    def test_parse_iso_8601_duration(self):\n        test_values = [\n            ('P1DT12H30M', timedelta(days=1, seconds=45 * 3600)),\n            ('PT12H30M', timedelta(seconds=12 * 3600 + 30 * 60)),\n            ('P20DT12H30M45.678S', timedelta(days=20, seconds=12 * 3600 + 30 * 60 + 45.678)),\n        ]\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n"], "sample_1209": ["def test_prefix_creation():\n    # Test creating prefixes with different bases\n    dodeca = Prefix('dodeca', 'dd', 1, base=12)\n    assert dodeca.name == 'dodeca'\n    assert dodeca.abbrev == 'dd'\n    assert dodeca.scale_factor == 12\n    assert dodeca.base == 12\n\n    binary_kilo = Prefix('kilobyte', 'Ki', 10, base=2)\n    assert binary_kilo.name == 'kilobyte'\n    assert binary_kilo.abbrev == 'Ki'\n    assert binary_kilo.scale_factor == 1024\n    assert binary_kilo.base == 2\n\n", "def test_prefix_latex():\n    assert Prefix('test', 't', 1, latex_repr=r'\\text{test}')._latex(None) == r'\\text{test}'\n    assert Prefix('test', 't', 1)._latex(None) == r'\\text{t}'\n", "def test_prefix_latex():\n    assert Prefix('test', 't', 10)._latex({}) == r'\\text{t}'\n    assert Prefix('test', 't', 10, latex_repr=r'\\text{tst}')._latex({}) == r'\\text{tst}'\n", "def test_prefix_latex():\n    m = PREFIXES['m']\n    k = PREFIXES['k']\n    assert m._latex(None) == r'\\text{m}'\n    assert k._latex(None) == r'\\text{k}'\n    assert Prefix('test', 'te', 1, latex_repr=r't_e')._latex(None) == r't_e'\n", "    def test_prefix_latex():\n        assert PREFIXES[\"mu\"]._latex(None) == r'\\text{mu}'\n        assert PREFIXES[\"m\"]._latex(None) == r'\\text{m}'\n"], "sample_1039": ["def test_print_Piecewise():\n    pw = Piecewise((x, x < 1), (x**2, x >= 1))\n    assert mpp.doprint(pw) == '<mrow><mfenced open=\"{\" close=\"\"><mtable><mtr><mtd><mi>x</mi></mtd><mtd><mi>if</mi><mi>\\ </mi><mi>x</mi><mi>&lt;</mi><mn>1</mn></mtd></mtr><mtr><mtd><msup><mi>x</mi><mn>2</mn></msup></mtd><mtd><mi>if</mi><mi>\\ </mi><mi>x</mi><mi>\u2265</mi><mn>1</mn></mtd></mtr></mtable></mfenced></mrow>'\n\n", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_411": ["    def test_find_command_with_app_labels(self):\n        with mock.patch(\n            \"django.core.management.base.find_command\", return_value=\"command_name\"\n        ) as mocked_find_command:\n            management.call_command(\"app_command\", \"myapp\")\n        mocked_find_command.assert_called_once_with(\"app_command\", \"myapp\")\n", "    def test_call_command_command_option(self):\n        \"\"\"Test passing a command-line option directly to a management command.\"\"\"\n        out = StringIO()\n        management.call_command(\"command_option\", option=\"value\", stdout=out)\n        self.assertIn(\"option=value\", out.getvalue())\n", "    def test_call_command_with_invalid_option_type(self):\n        msg = (\n            \"Error: option 'bad_option' requires a string \"\n            \"(received int)\"\n        )\n        with self.assertRaisesMessage(TypeError, msg):\n            management.call_command(\"dance\", bad_option=1)\n", "    def test_normalize_path_patterns_without_trailing_slash(self):\n        patterns = [\n            \"foo/bar/baz\",\n            \"baz\",\n            \"foo/bar/baz*\",\n            \"*baz\",\n            \"*\",\n            \"b?z\",\n            \"[abc]az\",\n            \"*/ba[!z]/baz\",\n        ]\n        expected_patterns = [\n            \"foo/bar/baz/\",\n            \"baz/\",\n            \"foo/bar/baz*\",\n            \"*baz/\",\n            \"*/\",\n            \"b?z/\",\n            \"[abc]az/\",\n            \"*/ba[!z]/baz/\",\n        ]\n        self.assertEqual(normalize_path_patterns(patterns), expected_patterns)\n\n\n", "    def test_normalize_path_patterns(self):\n        self.assertEqual(\n            normalize_path_patterns([\"foo/bar/baz\", \"*/baz\", \"foo/*\"]),\n            [\"foo/bar/baz\", \"*/baz\", \"foo/*\"],\n        )\n        self.assertEqual(\n            normalize_path_patterns([\"*/baz\", \"foo/bar/*\"]), [\"*/baz\", \"foo/bar/*\"]\n        )\n"], "sample_1108": ["    def test__af_inv():\n        assert _af_inv([1, 0, 3, 2], [1, 2, 1, 0]) == (\n            [0, 1, 2, 3], [1, 0, 3, 2])\n\n", "    def test_multiset_permutations_empty():\n        assert list(multiset_permutations(multiset())) == [[]]\n\n", "def test_kbins():\n    assert len(list(kbins('1123', 2, ordered=1))) == 24\n    assert len(list(kbins('1123', 2, ordered=11))) == 36\n    assert len(list(kbins('1123', 2, ordered=10))) == 10\n    assert len(list(kbins('1123', 2, ordered=0))) == 5\n    assert len(list(kbins('1123', 2, ordered=None))) == 3\n\n        for orderedval in [None, 0, 1, 10, 11]:\n            print('ordered =', orderedval)\n            for p in kbins([0, 0, 1], 2, ordered=orderedval):\n                print('   ', p)\n    assert capture(lambda : test1()) == dedent('''\\\n        ordered = None\n            [[0], [0, 1]]\n            [[0, 0], [1]]\n        ordered = 0\n            [[0, 0], [1]]\n            [[0, 1], [0]]\n        ordered = 1\n            [[0], [0, 1]]\n            [[0], [1, 0]]\n            [[1], [0, 0]]\n        ordered = 10\n            [[0, 0], [1]]\n            [[1", "compilation error", "    def test_is_permutation():\n        assert is_permutation([1, 2, 3]) is True\n        assert is_permutation([1, 3, 2]) is True\n        assert is_permutation([3, 2, 1]) is True\n        assert is_permutation([1, 2, 2]) is False\n        assert is_permutation([1, 2]) is False\n        assert is_permutation('abc') is True\n        assert is_permutation('cab') is True\n        assert is_permutation('bac') is True\n        assert is_permutation('acb') is True\n        assert is_permutation('cba') is True\n        assert is_permutation('bca') is True\n\n"], "sample_889": ["    def test_calibration_with_unseen_classes(data):\n        X, y = data\n        y = np.array([0, 1, 2] * 10)\n        estimator = LogisticRegression()\n        calibrated_clf = CalibratedClassifierCV(estimator)\n        with pytest.raises(ValueError, match=r\"Classes in `y`\"):\n            calibrated_clf.fit(X, y)\n\n", "    def test_calibration_with_sparse_input(pyplot):\n        # Check that the CalibratedClassifierCV can handle sparse input\n        X, y = make_classification(n_samples=100, n_features=10, random_state=42)\n        X_sparse = csr_matrix(X)\n\n        clf = LogisticRegression()\n        calibrated_clf = CalibratedClassifierCV(clf)\n\n        calibrated_clf.fit(X_sparse, y)\n\n        # smoke test: should not raise an error\n        calibrated_clf.predict_proba(X_sparse)\n\n        viz = CalibrationDisplay.from_estimator(calibrated_clf, X_sparse, y)\n        viz.plot()\n", "    def test_calibration_with_different_n_classes(data):\n        \"\"\"Test that CalibrationClassifierCV works correctly when the number of classes\n        in the training data differs from the number of classes in the predictions.\"\"\"\n\n        X, y = data\n\n        # Create a classifier that predicts only two classes\n        binary_clf = LogisticRegression()\n\n        # Train the calibrator on the multi-class data\n        calibrated_clf = CalibratedClassifierCV(binary_clf)\n        calibrated_clf.fit(X, y)\n\n        # Generate predictions on data with three classes\n        y_pred = calibrated_clf.predict_proba(X)\n\n        # Check that the output shape matches the number of classes in the predictions\n        assert y_pred.shape[1] == 2\n", "def test_calibration_with_missing_predict_proba(data):\n    \"\"\"Check that CalibratedClassifierCV raises an error if the base estimator\n    does not implement predict_proba.\"\"\"\n    class DummyClassifier:\n            return self\n            return np.zeros(len(X))\n\n    with pytest.raises(AttributeError, match=\"predict_proba\"):\n        CalibratedClassifierCV(estimator=DummyClassifier()).fit(*data)\n\n", "    def test_calibration_display_error_y_prob_shape(self):\n        y_true = np.array([0, 1, 1, 0])\n        y_prob = np.array([[0.1, 0.9], [0.6, 0.4], [0.8, 0.2], [0.3, 0.7]])\n        \n        with pytest.raises(ValueError, match=\"y_prob shape mismatch\"):\n            CalibrationDisplay(y_true, y_prob)\n"], "sample_66": ["    def test_getlist(self):\n        environ = {\n            'HTTP_COOKIE': 'foo=bar; baz=quux',\n        }\n        headers = HttpHeaders(environ)\n        self.assertEqual(headers.getlist('Cookie'), ['foo=bar; baz=quux'])\n", "    def test_getlist(self):\n        environ = {\n            'CONTENT_TYPE': 'text/html',\n            'CONTENT_LENGTH': '100',\n            'HTTP_HOST': 'example.com',\n            'HTTP_COOKIE': 'A=B; C=D',\n        }\n        headers = HttpHeaders(environ)\n        self.assertEqual(headers.getlist('Cookie'), ['A=B; C=D'])\n", "    def test_getlist(self):\n        environ = {\n            'HTTP_ACCEPT': 'text/plain, text/html',\n            'HTTP_COOKIE': 'name1=value1; name2=value2',\n        }\n        headers = HttpHeaders(environ)\n        self.assertEqual(headers.getlist('Accept'), ['text/plain', 'text/html'])\n        self.assertEqual(headers.getlist('Cookie'), ['name1=value1; name2=value2'])\n        self.assertEqual(headers.getlist('NonExistent'), [])\n", "    def test_header_case_insensitivity(self):\n        environ = {\n            'content-type': 'text/html',\n            'CONTENT-LENGTH': '100',\n            'http_host': 'example.com',\n        }\n        headers = HttpHeaders(environ)\n        self.assertEqual(headers['content-type'], 'text/html')\n        self.assertEqual(headers['Content-Length'], '100')\n        self.assertEqual(headers['HOST'], 'example.com')\n", "    def test_getlist(self):\n        environ = {\n            'HTTP_COOKIE': 'chocolate=chip; mint=flavor',\n            'HTTP_ACCEPT': 'text/html,application/xhtml+xml',\n        }\n        headers = HttpHeaders(environ)\n        self.assertEqual(headers.getlist('Cookie'), ['chocolate=chip', 'mint=flavor'])\n        self.assertEqual(headers.getlist('Accept'), ['text/html', 'application/xhtml+xml'])\n"], "sample_650": ["    def test_log_date_format_log_cli_level(pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import logging\n\n            logger = logging.getLogger(__name__)\n\n                logger.warning('text')\n                assert False\n            \"\"\"\n        )\n        pytester.makeini(\n            \"\"\"\n            [pytest]\n            log_format=%(asctime)s; %(levelname)s; %(message)s\n            log_date_format=%Y-%m-%d %H:%M:%S.%f\n            log_cli_level=DEBUG\n            \"\"\"\n        )\n        result = pytester.runpytest()\n        assert result.ret == 1\n        result.stdout.re_match_lines([r\"^[0-9-]{10} [0-9:]{8}.[0-9]{6}; WARNING; text\"])\n", "    def test_log_date_format_with_custom_format(pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import logging\n\n            logger = logging.getLogger(__name__)\n\n                logger.warning('text')\n                assert False\n            \"\"\"\n        )\n        pytester.makeini(\n            \"\"\"\n            [pytest]\n            log_format=%(asctime)s - %(levelname)s - %(name)s - %(message)s\n            log_date_format=%d-%m-%Y %H:%M:%S\n            \"\"\"\n        )\n        result = pytester.runpytest()\n        assert result.ret == 1\n        result.stdout.re_match_lines([r\"^[0-9]{2}-[0-9]{2}-[0-9]{4} [0-9:]{8} - WARNING - __main__ - text\"])\n\n", "def test_log_date_format_custom_format_string(pytester: Pytester) -> None:\n    \"\"\"Check that custom log_date_format works as expected.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.warning('text')\n            assert False\n        \"\"\"\n    )\n    pytester.makeini(\n        \"\"\"\n        [pytest]\n        log_format=%(asctime)s; %(levelname)s; %(message)s\n        log_date_format=My-%Y-%m-%d-%H-%M-%S\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    assert result.ret == 1\n    result.stdout.re_match_lines([r\"^My-[0-9-]{10}-[0-9]{2}-[0-9]{2}-[0-9]{2}; WARNING; text\"])\n", "    def test_log_level_override(pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import logging\n            logger = logging.getLogger(__name__)\n\n                logger.debug('debug message')\n                logger.info('info message')\n                logger.warning('warning message')\n                assert caplog.record_tuples == [\n                    ('__main__', 20, 'debug message'),\n                    ('__main__', 30, 'info message'),\n                    ('__main__', 40, 'warning message'),\n                ]\n        \"\"\"\n        )\n        pytester.makeini(\n            \"\"\"\n            [pytest]\n            log_level=DEBUG\n        \"\"\"\n        )\n        result = pytester.runpytest()\n        assert result.ret == 0\n", "    def test_log_file_level_with_log_cli_level(pytester: PyTester) -> None:\n        \"\"\"Test that log_file_level is respected even if log_cli_level is set.\"\"\"\n        pytester.makepyfile(\n            \"\"\"\n            import logging\n            logger = logging.getLogger(__name__)\n                logger.debug(\"debug message\")\n                logger.info(\"info message\")\n        \"\"\"\n        )\n        pytester.makeini(\n            \"\"\"\n            [pytest]\n            log_file=pytest.log\n            log_file_level=INFO\n            log_cli_level=DEBUG\n        \"\"\"\n        )\n\n        result = pytester.runpytest(\"-s\")\n\n        result.stdout.fnmatch_lines(\n            [\n                \"*test_log_levels*\",\n                \"*DEBUG    *debug message*\",\n                \"*INFO     *info message*\",\n            ]\n        )\n\n        with open(\"pytest.log\") as rfh:\n            contents = rfh.read()\n            assert \"debug message\" not in contents\n            assert \"info message\" in contents\n"], "sample_369": ["    def test_create_model_with_unique_together_constraint(self):\n        book = ModelState('testapp', 'Book', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200)),\n            ('author', models.ForeignKey('testapp.Author', models.CASCADE)),\n        ],\n        options={'unique_together': [('title', 'author')]}\n        )\n        changes = self.get_changes(\n            [self.author_empty],\n            [self.author_empty, book],\n        )\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['CreateModel'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='Book')\n", "    def test_custom_name(self):\n        class Migration(migrations.Migration):\n            operations = [migrations.CreateModel('Person', fields=[])]\n            name = 'custom_name'\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'custom_name')\n\n", "    def test_field_rename_suggest_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.RenameField(\n                    model_name='Person',\n                    old_name='old_name',\n                    new_name='new_name',\n                ),\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'rename_person_old_name_to_new_name')\n", "    def test_migration_suggest_name_with_existing_tables(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Person', fields=[\n                    ('id', models.AutoField(primary_key=True)),\n                    ('name', models.CharField(max_length=100)),\n                ]),\n            ]\n\n        migration = Migration('0001_initial', 'test_app')\n        with mock.patch('django.db.migrations.Migration.detect_existing_tables',\n                       return_value=['Person']):\n            self.assertEqual(migration.suggest_name(), 'person')\n\n", "    def test_mti_inheritance_model_removal_with_fk(self):\n        Animal = ModelState('app', 'Animal', [\n            (\"id\", models.AutoField(primary_key=True)),\n        ])\n        Dog = ModelState('app', 'Dog', [\n            ('owner', models.ForeignKey('auth.User', models.CASCADE)),\n        ], bases=('app.Animal',))\n        changes = self.get_changes([Animal, Dog], [Animal])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['DeleteModel'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='Dog')\n\n"], "sample_547": ["def test_annotationbbox_update_position(fig_test, fig_ref):\n    ax_ref = fig_ref.add_subplot()\n    ab = AnnotationBbox(DrawingArea(20, 20, 0, 0, clip=True), (0.5, 0.5),\n                        xybox=(-0.2, 0.4), xycoords='data',\n                        boxcoords='axes fraction')\n    ax_ref.add_artist(ab)\n\n    ax_test = fig_test.add_subplot()\n    ab = AnnotationBbox(DrawingArea(20, 20, 0, 0, clip=True), (0.5, 0.5),\n                        xybox=(-0.2, 0.4), xycoords='data',\n                        boxcoords='axes fraction')\n    ax_test.add_artist(ab)\n\n    ab.xybox = (0.1, 0.8)\n    fig_test.canvas.draw()\n", "    def test_draggable_annotation(self):\n        fig, ax = plt.subplots()\n        text = ax.text(0.5, 0.5, 'Draggable Text', ha='center', va='center')\n        annotation = DraggableAnnotation(text)\n        fig.canvas.draw()\n\n        # Simulate a drag event\n        event = MouseEvent('button_press_event',\n                           fig.canvas, 0.5, 0.5, button=1)\n        annotation.on_pick(event)\n        # ...rest of the test logic to simulate dragging and assert\n        # the final position of the text.\n", "def test_annotationbbox_box_alignment(box_align):\n    fig, ax = plt.subplots()\n    da = DrawingArea(20, 20, 0, 0, clip=True)\n    ab = AnnotationBbox(da, (0.5, 0.5),\n                        xybox=(-0.2, 0.5), xycoords='data',\n                        boxcoords=\"axes fraction\", box_alignment=box_align)\n    ax.add_artist(ab)\n    fig.canvas.draw()\n", "def test_paddedbox_alignment(fig_test, fig_ref):\n    ax_ref = fig_ref.add_subplot()\n    ta = TextArea(\"foo\")\n    pb = PaddedBox(ta, pad=5, patch_attrs={'facecolor': 'r'},\n                   draw_frame=True, align=\"bottom\")\n    ab = AnchoredOffsetbox('upper left', child=pb)\n    ax_ref.add_artist(ab)\n\n    ax_test = fig_test.add_subplot()\n    ta = TextArea(\"foo\")\n    pb = PaddedBox(ta, pad=5, patch_attrs={'facecolor': 'r'},\n                   draw_frame=True, align=\"center\")\n    ab = AnchoredOffsetbox('upper left', child=pb)\n    ax_test.add_artist(ab)\n", "def test_anchoredtext_clip():\n    fig, ax = plt.subplots()\n\n    text0 = AnchoredText(\"test\\ntest long text\", loc=\"center left\",\n                         pad=0.2, prop={\"ha\": \"left\"}, clip_on=True)\n    ax.add_artist(text0)\n    ax.set_xlim(0, 0.5)\n    ax.set_ylim(0, 0.5)\n"], "sample_112": ["    def test_prepopulated_fields_js(self):\n        request = self.factory.get(reverse('admin:admin_views_article_change', args=[1]))\n        request.user = self.superuser\n        admin = ArticleAdmin(Article, site)\n        article = Article.objects.create(title='Test', content='Content', pub_date=datetime.datetime.now())\n        question = Question.objects.create(article=article, question_text='What is the meaning of life?')\n        article.save()\n        extra_context = {'adminform': admin.get_form(request, article, 'edit'), 'inline_admin_formsets': [admin.get_inline_formsets(request, article)]}\n        response = admin.change_view(request, str(article.pk), extra_context=extra_context)\n        response.render()\n        context = response.context_data\n        self.assertIn('prepopulated_fields', context)\n        self.assertIn('prepopulated_fields_json', context)\n", "    def test_prepopulated_fields_js(self):\n        \"\"\"\n        Test the prepopulated_fields_js template tag.\n        \"\"\"\n        request = self.factory.get(reverse('admin:admin_views_article_change', args=[1]))\n        request.user = self.superuser\n\n        # Create a mock admin and some prepopulated fields\n        class MockArticleAdmin(ModelAdmin):\n                return [\n                    {\n                        'field': 'title',\n                        'dependencies': [],\n                        'maxLength': 255,\n                        'allowUnicode': True,\n                    },\n                    {\n                        'field': 'slug',\n                        'dependencies': ['title'],\n                        'maxLength': 255,\n                        'allowUnicode': True,\n                    },\n                ]\n\n        admin = MockArticleAdmin(Article, site)\n        response = admin.change_view(request, '1')\n\n        # Check if the prepopulated fields are correctly rendered in the context\n        self.assertIn('prepopulated_fields', response.context_data)\n        self.assertIn('prepopulated_fields_json', response.context_data)\n\n        prepopulated_fields = response.context_data['prepopulated_fields']\n        prepopulated_fields_json = response.context_data['prepopulated_fields_json']\n\n        self.assertEqual(len(prepopulated_fields), 2)\n        self.assertEqual(len(prepopulated_fields_json), 2)\n\n        self.assertEqual(prepopulated_fields[0]['field'].name, 'title')\n        self.assertEqual(prepopulated_fields[1]['field'].name, 'slug')\n\n        self.assertEqual(prepopulated_fields_json[1]['dependency_ids'][0], '#id_title')\n", "    def test_prepopulated_fields_js(self):\n        request = self.factory.get(reverse('admin:admin_views_article_change', args=[1]))\n        request.user = self.superuser\n        admin = ArticleAdmin(Article, site)\n        article = Article.objects.create(title='Test Article', content='This is a test', publish_date=datetime.date.today())\n        response = admin.change_view(request, str(article.pk))\n        response.render()\n        self.assertIn('prepopulated_fields', response.context_data)\n        self.assertIn('prepopulated_fields_json', response.context_data)\n", "    def test_prepopulated_fields_js(self):\n        request = self.factory.get(reverse('admin:admin_views_article_change', args=[1]))\n        request.user = self.superuser\n        admin = ArticleAdmin(Article, site)\n        article = Article.objects.create(title='Test', content='Test content', author=self.superuser)\n        response = admin.change_view(request, str(article.pk))\n        response.render()\n        self.assertIn('\"id\": \"#id_content\"', response.content.decode())\n        self.assertIn('\"dependency_ids\": [\"#id_title\"]', response.content.decode())\n", "    def test_cell_count(self):\n        request = self.factory.get(reverse('admin:admin_views_article_changelist'))\n        request.user = self.superuser\n        admin = ArticleAdmin(Article, site)\n        response = admin.changelist_view(request)\n        response.render()\n        inline_admin_formset = response.context_data['inline_admin_formsets'][0]\n        self.assertEqual(cell_count(inline_admin_formset), 3)\n\n"], "sample_133": ["    def test_javascript_gettext_context(self):\n        self.selenium.get(self.live_server_url + '/jsi18n_context/')\n\n        elem = self.selenium.find_element_by_id(\"gettext_context\")\n        self.assertEqual(elem.text, \"This is contextual translation\")\n", "    def test_jsi18n_with_missing_en_files_empty_catalog(self):\n        \"\"\"\n        The javascript_catalog should return an empty catalog if translations are\n        missing for the requested language and its fallback language is English,\n        which also lacks translations.\n        \"\"\"\n        with self.settings(LANGUAGE_CODE='en-gb'), override('en'):\n            response = self.client.get('/jsi18n/')\n            self.assertEqual(response.context['catalog'], {})\n", "    def test_javascript_gettext_with_context(self):\n        self.selenium.get(self.live_server_url + '/jsi18n_template/')\n\n        elem = self.selenium.find_element_by_id(\"pgettext_ctx\")\n        self.assertEqual(elem.text, \"Aprobado\")\n", "    def test_javascript_gettext_with_context(self):\n        self.selenium.get(self.live_server_url + '/jsi18n_template_context/')\n\n        get_text_elem = self.selenium.find_element_by_id(\"gettext_context\")\n        self.assertEqual(get_text_elem.text, \"This is my translated text\")\n", "    def test_i18n_plural_with_context(self):\n        with override('fr'):\n            response = self.client.get('/jsi18n/')\n            self.assertContains(response, '\"month name\\x04May\": \"mai\"', 1)\n"], "sample_208": ["    def test_add_field_with_default(self):\n        before = [\n            ModelState('app', 'Person', [\n                ('id', models.AutoField(primary_key=True)),\n            ]),\n        ]\n        after = [\n            ModelState('app', 'Person', [\n                ('id', models.AutoField(primary_key=True)),\n                ('name', models.CharField(max_length=100, default='John Doe')),\n            ]),\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['AddField'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, name='name',\n                                      field=models.CharField(max_length=100, default='John Doe'))\n", "    def test_multiple_operations_same_model(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.AddField('Person', name='age', field=models.IntegerField()),\n                migrations.AlterField('Person', name='name', field=models.CharField(max_length=50)),\n            ]\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'alter_person')\n\n", "    def test_alter_field_name_with_initial_true(self):\n        class Migration(migrations.Migration):\n            initial = True\n            operations = [\n                migrations.RenameField('Person', 'old_name', 'new_name'),\n            ]\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'rename_person_old_name')\n", "    def test_complex_operations_suggest_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Author', fields=[\n                    ('id', models.AutoField(primary_key=True)),\n                    ('name', models.CharField(max_length=200)),\n                ]),\n                migrations.CreateModel('Book', fields=[\n                    ('id', models.AutoField(primary_key=True)),\n                    ('title', models.CharField(max_length=200)),\n                    ('author', models.ForeignKey('Author', models.CASCADE)),\n                ]),\n                migrations.AddField('Author', name='age', field=models.IntegerField()),\n            ]\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'author_book')\n", "    def test_rename_model(self):\n        old_model = ModelState('app', 'OldName', [\n            ('id', models.AutoField(primary_key=True)),\n        ])\n        new_model = ModelState('app', 'NewName', [\n            ('id', models.AutoField(primary_key=True)),\n        ], bases=old_model.bases)\n        changes = self.get_changes([old_model], [new_model])\n        self.assertNumberMigrations(changes, 'app', 1)\n        self.assertOperationTypes(changes, 'app', 0, ['RenameModel'])\n        self.assertOperationAttributes(changes, 'app', 0, 0, old_name='OldName', new_name='NewName')\n"], "sample_912": ["    def test_pyattribute_type_annotation(app):\n        text = (\".. py:class:: Class\\n\"\n                \"\\n\"\n                \"   .. py:attribute:: attr\\n\"\n                \"      :type: List[str]\\n\")\n        domain = app.env.get_domain('py')\n        doctree = restructuredtext.parse(app, text)\n        assert_node(doctree, (addnodes.index,\n                              [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                        [desc_name, \"Class\"])],\n                                      [desc_content, (addnodes.index,\n                                                      desc)])]))\n        assert_node(doctree[1][1][0], addnodes.index,\n                    entries=[('single', 'attr (Class attribute)', 'class.attr', '', None)])\n        assert_node(doctree[1][1][1], ([desc_signature, ([desc_name, \"attr\"],\n                                                          [desc_annotation, \": List[str]\"])],\n                                      [desc_content, ()]))\n        assert 'Class.attr' in domain.objects\n        assert domain.objects['Class.attr'] == ('index', 'class.attr', 'attribute')\n", "compilation error", "def test_pyfunction_signature_with_return_type(app):\n    text = \".. py:function:: hello(name: str) -> int\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"hello\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"int\"])],\n                                  desc_content)]))\n", "def test_pyfunction_signature_return_annotation(app):\n    text = \".. py:function:: func(arg: int) -> List[str]\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"func\"],\n                                                    desc_parameterlist,\n                                                    [desc_returns, pending_xref, \"List[str]\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], addnodes.desc, desctype=\"function\",\n                domain=\"py\", objtype=\"function\", noindex=False)\n    assert_node(doctree[1][0][1],\n                [desc_parameterlist, ([desc_parameter, ([desc_sig_name, \"arg\"],\n                                                        [desc_sig_punctuation, \":\"],\n                                                        \" \",\n                                                        [nodes.inline, \"int\"])]) ,\n                [desc_returns, pending_xref, \"List[str]\"]])\n\n", "def test_pyexception_nested_class(app):\n    text = \".. py:exception:: exceptions.FooError.BarError\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"exception \"],\n                                                    [desc_addname, \"exceptions.\"],\n                                                    [desc_name, \"FooError.\"],\n                                                    [desc_name, \"BarError\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], desc, desctype=\"exception\",\n                domain=\"py\", objtype=\"exception\", noindex=False)\n"], "sample_518": ["compilation error", "    def test_default_joinstyle():\n        patch = Patch()\n        assert patch.get_joinstyle() == 'miter'\n", "    def test_default_joinstyle():\n        patch = Patch()\n        assert patch.get_joinstyle() == 'miter'\n", "compilation error", "def test_default_joinstyle():\n    patch = Patch()\n    assert patch.get_joinstyle() == 'miter'\n"], "sample_421": ["    def test_when_constructor_arguments(self):\n        with self.assertRaises(TypeError):\n            When(condition=Q(pk__in=[]), then=Value(1), else_=Value(2))\n        with self.assertRaises(TypeError):\n            When(condition=Q(pk__in=[]), then=1)\n", "    def test_case_when_ordering(self):\n        qs = CaseTestModel.objects.values_list(\"integer\", \"integer2\").annotate(\n            test=Case(\n                When(integer=1, then=Value(1)),\n                When(integer2=1, then=Value(2)),\n                default=Value(3),\n                output_field=IntegerField(),\n            )\n        )\n        self.assertQuerysetEqual(\n            qs.order_by(\"test\"),\n            [(1, 2, 1), (2, 1, 2), (3, 2, 3), (1, 1, 1), (2, 2, 3), (3, 3, 3)],\n            lambda x: x,\n        )\n", "    def test_case_with_f_expression(self):\n        self.assertQuerysetEqual(\n            CaseTestModel.objects.annotate(\n                test=Case(\n                    When(F(\"integer\") > 2, then=Value(\"greater than 2\")),\n                    default=Value(\"less than or equal to 2\"),\n                ),\n            ).order_by(\"pk\"),\n            [\n                (1, \"less than or equal to 2\"),\n                (2, \"less than or equal to 2\"),\n                (3, \"greater than 2\"),\n                (2, \"less than or equal to 2\"),\n                (3, \"greater than 2\"),\n                (3, \"greater than 2\"),\n                (4, \"greater than 2\"),\n            ],\n            transform=attrgetter(\"integer\", \"test\"),\n        )\n", "    def test_empty_when_list(self):\n        with self.assertRaises(ValueError):\n            Case(When())\n", "    def test_when_argument_types(self):\n        with self.assertRaisesMessage(TypeError, \"Case() takes at most 1 positional argument.\"):\n            Case(When(Q(pk=1)), When(Q(pk=2)))\n        with self.assertRaisesMessage(TypeError, \"Case() takes at most 1 positional argument.\"):\n            Case(When(Q(pk=1)), default=2, When(Q(pk=2)))\n"], "sample_419": ["    def test_formset_extra_with_initial(self):\n        data = {\n            \"form-TOTAL_FORMS\": \"2\",\n            \"form-INITIAL_FORMS\": \"1\",\n            \"form-0-title\": \"Test Article 1\",\n            \"form-0-pub_date\": \"2023-10-26\",\n        }\n        initial = [{\"title\": \"Initial Article\", \"pub_date\": \"2023-10-25\"}]\n        formset = ArticleFormSet(data, initial=initial)\n        self.assertEqual(formset.initial_form_count(), 1)\n        self.assertEqual(formset.total_form_count(), 2)\n        self.assertEqual(formset.cleaned_data[0][\"title\"], \"Initial Article\")\n        self.assertEqual(formset.cleaned_data[0][\"pub_date\"], \"2023-10-25\")\n        self.assertEqual(formset.cleaned_data[1][\"title\"], \"Test Article 1\")\n        self.assertEqual(formset.cleaned_data[1][\"pub_date\"], \"2023-10-26\")\n", "    def test_formset_with_initial_data(self):\n        data = {\n            \"choices-TOTAL_FORMS\": \"2\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"Zero\",\n            \"choices-0-votes\": \"0\",\n            \"choices-1-choice\": \"One\",\n            \"choices-1-votes\": \"1\",\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        initial_data = [\n            {\"choice\": \"Initial Zero\", \"votes\": 10},\n            {\"choice\": \"Initial One\", \"votes\": 20},\n        ]\n        formset = ChoiceFormSet(data, initial=initial_data, auto_id=False, prefix=\"choices\")\n        self.assertTrue(formset.is_valid())\n        self.assertEqual(formset.cleaned_data, [\n            {\"choice\": \"Zero\", \"votes\": 0},\n            {\"choice\": \"One\", \"votes\": 1},\n        ])\n", "    def test_management_form_data_missing_with_initial_forms_greater_than_total_forms(self):\n        data = {\n            \"form-TOTAL_FORMS\": \"1\",\n            \"form-INITIAL_FORMS\": \"2\",\n        }\n        formset = ArticleFormSet(data)\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(\n            formset.non_form_errors(),\n            [\n                \"ManagementForm data is missing or has been tampered with. \"\n                \"You may need to file a bug report if the issue persists.\"\n            ],\n        )\n", "    def test_formset_initial_data_with_empty_strings(self):\n        data = {\n            \"choices-TOTAL_FORMS\": \"2\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"\",\n            \"choices-0-votes\": \"0\",\n            \"choices-1-choice\": \"\",\n            \"choices-1-votes\": \"1\",\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        self.assertTrue(formset.is_valid())\n        self.assertEqual(formset.cleaned_data, [{'choice': '', 'votes': 0}, {'choice': '', 'votes': 1}])\n", "    def test_formset_data_is_not_mutated(self):\n        data = {\n            \"choices-TOTAL_FORMS\": \"2\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"Zero\",\n            \"choices-0-votes\": \"0\",\n            \"choices-1-choice\": \"One\",\n            \"choices-1-votes\": \"1\",\n        }\n        data_copy = data.copy()\n        ChoiceFormSet = formset_factory(Choice)\n        formset = ChoiceFormSet(data)\n        self.assertEqual(data, data_copy)\n"], "sample_371": ["def test_setting_allows_custom_subclass(self):\n        self.assertIsInstance(\n            get_default_exception_reporter_filter(),\n            CustomExceptionReporterFilter,\n        )\n", "    def test_sensitive_variables_on_method(self):\n        class MyClass:\n            @sensitive_variables('password')\n                return HttpResponse()\n\n        obj = MyClass()\n        obj.a_view('super_secret')\n", "    def test_sensitive_variables_with_arguments(self):\n        @sensitive_variables('password', 'secret_key')\n            pass\n\n        reporter_filter = SafeExceptionReporterFilter()\n        with self.settings(DEBUG=True):\n            tb = traceback.format_exc()\n            for arg in ['password', 'secret_key']:\n                self.assertNotIn(arg, tb)\n", "    def test_sensitive_variables_with_nested_functions(self):\n        @sensitive_variables\n                return password\n            return inner_func()\n\n        with self.assertRaises(RuntimeError):\n            outer_func('secret')\n\n", "    def test_sensitive_variables_decorator_on_class_method(self):\n        class MyClass:\n            @sensitive_variables\n            @method_decorator\n\n                return HttpResponse()\n\n\n        response = self.client.post('/test_view/', data={'key': 'value'})\n        self.assertEqual(response.status_code, 200)\n"], "sample_1113": ["def test_block_diag():\n    X = MatrixSymbol('X', n, n)\n    Y = MatrixSymbol('Y', m, m)\n    Z = MatrixSymbol('Z', l, l)\n    BD = BlockDiagMatrix(X, Y, Z)\n    assert BD[0, 0] == X[0, 0]\n    assert BD[n, n] == Y[0, 0]\n    assert BD[n+m, n+m] == Z[0, 0]\n", "def test_block_add_index():\n    A1 = MatrixSymbol('A1', n, k)\n    A2 = MatrixSymbol('A2', n, l)\n    A3 = MatrixSymbol('A3', m, k)\n    A4 = MatrixSymbol('A4', m, l)\n    A = BlockMatrix([[A1, A2], [A3, A4]])\n    B1 = MatrixSymbol('B1', n, k)\n    B2 = MatrixSymbol('B2', n, l)\n    B3 = MatrixSymbol('B3', m, k)\n    B4 = MatrixSymbol('B4', m, l)\n    B = BlockMatrix([[B1, B2], [B3, B4]])\n    C = A + B\n    assert C[0,0] == A1 + B1\n    assert C[n + i, k + j] == A4[i, j] + B4[i, j]\n\n", "def test_block_transpose():\n    A1 = MatrixSymbol('A1', n, k)\n    A2 = MatrixSymbol('A2', n, l)\n    A3 = MatrixSymbol('A3', m, k)\n    A4 = MatrixSymbol('A4', m, l)\n    A = BlockMatrix([[A1, A2], [A3, A4]])\n    At = A.transpose()\n    assert At[0,0] == A1.transpose()\n    assert At[0,1] == A2.transpose()\n    assert At[1,0] == A3.transpose()\n    assert At[1,1] == A4.transpose()\n\n\n", "def test_blockdiagmatrix():\n    X = MatrixSymbol('X', 2, 2)\n    Y = MatrixSymbol('Y', 3, 3)\n    B = BlockDiagMatrix(X, Y)\n    assert B[0, 0] == X[0, 0]\n    assert B[1, 1] == Y[0, 0]\n    assert B[0, 1] == 0\n    assert B[2, 2] == Y[1, 1]\n", "def test_block_inverse_2x2_symbolic():\n    n, m = symbols('n m', integer=True)\n    A1 = MatrixSymbol('A1', n, n)\n    A2 = MatrixSymbol('A2', n, m)\n    A3 = MatrixSymbol('A3', m, n)\n    A4 = MatrixSymbol('A4', m, m)\n    B = BlockMatrix([[A1, A2], [A3, A4]])\n\n    BI = B.inverse()\n    assert BI[0, 0].has(A1, A2, A3, A4)\n"], "sample_147": ["    def test_empty_union(self):\n        qs1 = Number.objects.none()\n        qs2 = Number.objects.filter(num=5)\n        self.assertNumbersEqual(qs1.union(qs2), [5], ordered=False)\n", "    def test_intersection_with_duplicate_values(self):\n        Number.objects.bulk_create(Number(num=i) for i in range(5))\n        qs1 = Number.objects.all()\n        qs2 = Number.objects.filter(num__in=[0, 1, 2, 0, 1])\n        self.assertNumbersEqual(qs1.intersection(qs2), [0, 1, 2])\n\n", "    def test_union_with_select_related(self):\n        ReservedName.objects.create(name='rn1', order=1)\n        Number.objects.create(num=1, reserved_name=ReservedName.objects.get(name='rn1'))\n        qs1 = Number.objects.filter(num=1).select_related('reserved_name')\n        qs2 = Number.objects.filter(num=2)\n        self.assertEqual(qs1.union(qs2).count(), 2)\n", "    def test_union_of_empty_and_non_empty_qs(self):\n        qs1 = Number.objects.none()\n        qs2 = Number.objects.all()\n        self.assertEqual(len(qs1.union(qs2)), 10)\n        self.assertEqual(len(qs2.union(qs1)), 10)\n", "    def test_union_same_model_with_distinct(self):\n        qs1 = Number.objects.all().order_by('num')\n        qs2 = Number.objects.filter(num__gte=5).order_by('num')\n        self.assertEqual(len(list(qs1.union(qs2, all=True))), 10)\n        self.assertEqual(len(list(qs1.union(qs2))), 10)\n\n"], "sample_919": ["    def check(role, tag, cls):\n        classes_set = classes(role, tag)\n        assert cls.issubset(classes_set), (\n            f\"Expected classes {cls} not found in the classes set \"\n            f\"{classes_set} for role '{role}' and tag '{tag}'\")\n", "def test_build_domain_cpp_custom_attributes(app, status, warning):\n    app.builder.build_all()\n    f = 'custom_attributes.html'\n    t = (app.outdir / f).read_text()\n    assert 'class=\"id_attr\"' in t\n    assert 'class=\"paren_attr\"' in t\n\n", "    def check(target, role, tag, classes_expected):\n        pattern = (r'{role}-role:.*?'\n                   r'<(?P<tag>{tag}) .*?>'\n                   r'.*{target}.*?'\n                   r'</(?P=tag)>').format(role=role, tag=tag, target=target)\n        result = re.search(pattern, output)\n        expect = '''\\", "compilation error", "compilation error"], "sample_778": ["    def test_nmf_sparse_input_dtype():\n        # Test that sparse matrices of different dtypes are accepted as input\n        from scipy.sparse import csc_matrix\n\n        rng = np.random.mtrand.RandomState(42)\n        A = np.abs(rng.randn(10, 10))\n        A[:, 2 * np.arange(5)] = 0\n\n        for dtype in ['float32', 'float64']:\n            A_sparse = csc_matrix(A.astype(dtype))\n            for solver in ('cd', 'mu'):\n                est = NMF(solver=solver, random_state=0, n_components=5,\n                          max_iter=400)\n                A_fit_tr = est.fit_transform(A_sparse)\n                assert A_fit_tr.dtype == dtype\n", "    def test_nmf_empty_components():\n        # Test NMF with initial components containing empty rows or columns\n        rng = np.random.mtrand.RandomState(42)\n        n_samples = 10\n        n_features = 5\n        n_components = 3\n        X = rng.randn(n_samples, n_features)\n        np.abs(X, X)\n\n        # Initialize with empty rows in W\n        W = np.zeros((n_samples, n_components))\n        H = rng.randn(n_components, n_features)\n        model = NMF(n_components=n_components, solver='mu', random_state=42)\n        model.fit_transform(X, W=W, H=H)\n        assert not np.all(model.components_ == 0)\n\n        # Initialize with empty columns in H\n        W = rng.randn(n_samples, n_components)\n        H = np.zeros((n_components, n_features))\n        model = NMF(n_components=n_components, solver='mu', random_state=42)\n        model.fit_transform(X, W=W, H=H)\n        assert not np.all(model.components_ == 0)\n", "def test_nmf_sparse_input_empty_rows():\n    # Test that NMF handles sparse matrices with empty rows correctly.\n    rng = np.random.mtrand.RandomState(42)\n    X = np.array([[1, 2], [0, 0], [3, 4]])\n    X_sparse = csc_matrix(X)\n\n    for solver in ('cd', 'mu'):\n        est = NMF(solver=solver, random_state=0, n_components=2)\n        W = est.fit_transform(X_sparse)\n        assert W.shape == (3, 2)\n        assert not np.any(np.isnan(W))\n", "    def test_nmf_sparse_input_zero_components():\n        # Test NonNegativeFactorization with a sparse matrix input\n        # containing zero columns\n\n        rng = np.random.mtrand.RandomState(42)\n        X = np.array([[1, 2, 0], [3, 4, 0], [5, 6, 0]])\n        X_sparse = sp.csr_matrix(X)\n        n_components = 2\n\n        # Test with both 'cd' and 'mu' solvers\n        for solver in ['cd', 'mu']:\n            model = nmf.NMF(n_components=n_components, solver=solver,\n                            random_state=0, max_iter=1000).fit(X_sparse)\n            assert_array_equal(model.components_.shape, (n_components, 3))\n            assert (model.components_[:, 2] == 0).all()\n\n            transformed_data = model.transform(X_sparse)\n            assert_array_equal(transformed_data.shape, (3, n_components))\n", "    def test_nmf_sparse_input_empty_rows(self):\n        # Test that NMF handles sparse matrices with empty rows.\n\n        rng = np.random.mtrand.RandomState(42)\n        X = csc_matrix(rng.randn(10, 5))\n\n        # Make the first row empty\n        X[0, :] = 0\n\n        for solver in ('cd', 'mu'):\n            est = NMF(solver=solver, random_state=0, n_components=2,\n                      max_iter=400)\n            est.fit(X)\n            # No assertion here, we just want to make sure the code runs\n            # without crashing.\n"], "sample_355": ["    def test_get_user_model(self):\n        with self.settings(AUTH_USER_MODEL='auth_tests.CustomPermissionsUser'):\n            self.assertEqual(get_user_model(), CustomPermissionsUser)\n\n", "    def test_authenticate_returns_none(self):\n        \"\"\"\n        If authenticate() returns None, the backend is skipped.\n        \"\"\"\n        class StubBackend:\n                return None\n\n        self.user = User.objects.create_user('test', 'test@example.com', 'test')\n        with self.settings(AUTHENTICATION_BACKENDS=[\n                'auth_tests.test_auth_backends.StubBackend',\n                'django.contrib.auth.backends.ModelBackend']):\n            self.assertIsNone(authenticate(username='test', password='test'))\n", "    def test_get_user_model(self):\n        with self.settings(AUTH_USER_MODEL='auth_tests.CustomPermissionsUser'):\n            self.assertEqual(get_user_model(), CustomPermissionsUser)\n\n", "    def test_backend_path_login_with_invalid_backend(self):\n        user = User.objects.create_user(self.username, 'email', self.password)\n        expected_message = (\n            f'Invalid backend specified: \"invalid_backend\". '\n            f'Available backends: {self.backend}, {self.other_backend}'\n        )\n        with self.assertRaisesMessage(ValueError, expected_message):\n            self.client._login(user, backend=\"invalid_backend\")\n", "    def test_custom_user_model_with_custom_backend(self):\n        self.user = CustomPermissionsUser._default_manager.create_user(\n            email='test@example.com',\n            password='test',\n            date_of_birth=date(2006, 4, 25)\n        )\n        # Test that the custom backend can authenticate the custom user model\n        self.assertEqual(authenticate(email='test@example.com', password='test'), self.user)\n"], "sample_1115": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_636": ["    def test_duplicate_code_raw_strings_ignore_comments(self) -> None:\n        \"\"\"Tests that comments are ignored.\"\"\"\n        path = join(DATA, \"raw_strings_ignore_comments\")\n        expected_output = \"Similar lines in 2 files\"\n        self._test_output(\n            [path, \"--disable=all\", \"--enable=duplicate-code\"],\n            expected_output=expected_output,\n        )\n\n", "    def test_duplicate_code_raw_strings_disable_inline_comment(self) -> None:\n        \"\"\"Tests disabling duplicate-code at a line with an inline comment.\"\"\"\n        path = join(DATA, \"raw_strings_disable_inline_comment\")\n        expected_output = \"Similar lines in 2 files\"\n        self._test_output(\n            [path, \"--disable=all\", \"--enable=duplicate-code\"],\n            expected_output=expected_output,\n        )\n\n", "    def test_duplicate_code_raw_strings_ignore_comments(self) -> None:\n        \"\"\"Tests ignoring comments in the duplicate code detection.\"\"\"\n        path = join(DATA, \"raw_strings_ignore_comments\")\n        expected_output = \"Similar lines in 2 files\"\n        self._test_output(\n            [path, \"--disable=all\", \"--enable=duplicate-code\", \"--ignore-comments\"],\n            expected_output=expected_output,\n        )\n", "    def test_duplicate_code_raw_strings_ignore_docstrings(self) -> None:\n        \"\"\"Tests if disabling duplicate lines in docstrings works.\"\"\"\n        path = join(DATA, \"raw_strings_ignore_docstrings\")\n        expected_output = \"Similar lines in 2 files\"\n        self._test_output(\n            [path, \"--disable=all\", \"--enable=duplicate-code\", \"--ignore-docstrings\"],\n            expected_output=expected_output,\n        )\n", "    def test_ignoring_signatures(self):\n        path = join(DATA, \"signatures\")\n        expected_output = \"Similar lines in 2 files\"\n        self._test_output(\n            [path, \"--disable=all\", \"--enable=duplicate-code\", \"--ignore-signatures\"],\n            expected_output=expected_output,\n        )\n"], "sample_628": ["    def test_docstring_multi_line_with_hash_in_code(self):\n        stmt = astroid.extract_node(\n            '''def f():", "    def test_ignore_list(self):\n        self.checker.ignore_list = ['coment']\n        stmt = astroid.extract_node('class ComentAbc(object):\\n   \"\"\"ComentAbc with a bad coment\"\"\"\\n   pass')\n        self.checker.visit_classdef(stmt)\n        assert self.linter.release_messages() == []\n", "    def test_ignore_words_specified_in_pylintrc(self):\n        self.checker.ignore_list = [\"coment\"]\n        stmt = astroid.extract_node(\n            'class ComentAbc(object):\\n   \"\"\"ComentAbc with a bad coment\"\"\"\\n   pass'\n        )\n        with self.assertAddsMessages():\n            self.checker.visit_classdef(stmt)\n", "    def test_issue_3956(self):\n        stmt = astroid.extract_node(\n            '''def f(", "    def test_ignore_words_in_config_file(self):\n        self.checker.config.spelling_ignore_words = [\"coment\"]\n        self.checker.process_tokens(_tokenize_str(\"# bad coment\"))\n        assert self.linter.release_messages() == []\n"], "sample_983": ["compilation error", "compilation error", "compilation error", "    def test_sparse_row_col_ops():\n        m = SparseMatrix(((1, 2, 3), (4, 5, 6)))\n        assert m.row_del(0) == SparseMatrix(((4, 5, 6)))\n        assert m.col_del(1) == SparseMatrix(((1, 3), (4, 6)))\n        assert m.row_swap(0, 1) == SparseMatrix(((4, 5, 6), (1, 2, 3)))\n", "def test_sparse_solve_singular():\n    A = SparseMatrix([[1, 2], [2, 4]])\n    raises(LinAlgError, lambda: A.inv())\n    raises(LinAlgError, lambda: A.solve(SparseMatrix([1, 2])))\n"], "sample_743": ["    def test_radius_neighbors_empty_input():\n        # Test radius_neighbors_graph with empty input data\n\n        nn = neighbors.NearestNeighbors()\n        nn.fit(np.empty((0, 2)))\n\n        dist, ind = nn.radius_neighbors(np.empty((0, 2)), radius=1.0)\n        assert_equal(len(dist), 0)\n        assert_equal(len(ind), 0)\n\n        graph = nn.radius_neighbors_graph(np.empty((0, 2)), radius=1.0)\n        assert_equal(graph.shape, (0, 0))\n\n        # Test that it works correctly with non-empty query data\n        dist, ind = nn.radius_neighbors([[0, 0]], radius=1.0)\n        assert_equal(len(dist), 1)\n        assert_equal(len(ind), 1)\n\n        graph = nn.radius_neighbors_graph([[0, 0]], radius=1.0)\n        assert_equal(graph.shape, (1, 0))\n", "    def test_radius_neighbors_empty_radius():\n        # Test radius_neighbors_graph when radius is 0\n        X = np.array([[0, 1], [2, 3], [4, 5]])\n        n_neighbors = 2\n\n        nn = neighbors.NearestNeighbors(radius=0)\n        nn.fit(X)\n\n        # Test radius_neighbors\n        dist, ind = nn.radius_neighbors(X)\n        assert_array_equal(dist, [[] for _ in range(len(X))])\n        assert_array_equals(ind, [[] for _ in range(len(X))])\n\n        # Test radius_neighbors_graph\n        graph = nn.radius_neighbors_graph(X).toarray()\n        assert_array_equal(graph, np.zeros((len(X), len(X))))\n", "    def test_radius_neighbors_graph_empty_input():\n        # Test radius_neighbors_graph with empty input\n        nn = neighbors.NearestNeighbors(radius=1.0)\n        X = np.array([]).reshape(0, 2)\n        nn.fit(X)\n        graph = nn.radius_neighbors_graph([[0, 0]])\n        assert_array_equal(graph.A, [[0.0]])\n        graph = nn.radius_neighbors_graph(X)\n        assert_array_equal(graph.A, np.array([]).reshape(0, 0))\n", "compilation error", "    def test_kneighbors_graph_empty_query():\n        # Test kneighbors_graph when query is empty\n        for algorithm in ALGORITHMS:\n            nn = neighbors.NearestNeighbors(n_neighbors=1, algorithm=algorithm)\n            X = [[0], [1]]\n            nn.fit(X)\n            result = nn.kneighbors_graph([], mode='connectivity').toarray()\n            assert_array_equal(result, np.array([]).reshape((0, 2)))\n"], "sample_289": ["    def test_len(self):\n        self.assertEqual(len(self.dict1), 2)\n", "    def test_setitem(self):\n        with self.assertRaisesMessage(TypeError, \"'CaseInsensitiveMapping' object does not support item assignment\"):\n            self.dict1['Accept'] = 'text/html'\n", "    def test_caseinsensitivemapping_len(self):\n        self.assertEqual(len(self.dict1), 2)\n", "    def test_len(self):\n        self.assertEqual(len(self.dict1), 2)\n", "    def test_len(self):\n        self.assertEqual(len(self.dict1), 2)\n"], "sample_384": ["    def test_bulk_update_with_empty_values(self):\n        notes = [Note.objects.create(note=\"test-%s\" % i, misc=\"test-%s\" %i) for i in range(10)]\n        for note in notes:\n            note.note = ''\n        Note.objects.bulk_update(notes, [\"note\"])\n        self.assertCountEqual(\n            Note.objects.filter(note__exact=\"\"), notes\n        ) \n\n", "    def test_bulk_update_with_duplicate_pks(self):\n        objs = [Note.objects.create(note=str(i), misc=str(i)) for i in range(3)]\n        duplicate = objs[0]\n        objs.append(duplicate)\n        with self.assertRaisesMessage(IntegrityError, \"UNIQUE constraint failed\"):\n            Note.objects.bulk_update(objs, [\"note\"])\n\n", "    def test_unique_constraint_violation(self):\n        notes = [Note.objects.create(note=\"test-%s\" % i, misc=\"test\") for i in range(10)]\n        for note in notes:\n            note.note = \"duplicate\"\n        with self.assertRaises(IntegrityError):\n            Note.objects.bulk_update(notes, [\"note\"])\n\n", "    def test_empty_values_to_null(self):\n        RelatedObject.objects.create(single_id=1)\n        objs = RelatedObject.objects.all()\n        for obj in objs:\n            obj.single = None\n        RelatedObject.objects.bulk_update(objs, [\"single\"])\n        self.assertCountEqual(RelatedObject.objects.filter(single__isnull=True), objs)\n\n", "    def test_bulk_update_with_same_pk_from_different_objects(self):\n        obj1 = Note.objects.create(note=\"test1\", misc=\"test1\")\n        obj2 = Note.objects.create(note=\"test2\", misc=\"test2\")\n        obj1.pk = obj2.pk\n        with self.assertRaises(IntegrityError):\n            Note.objects.bulk_update([obj1, obj2], fields=[\"note\"])\n\n"], "sample_705": ["def test_pytester_assert_outcomes_with_empty_list(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(passed=1)\n", "def test_pytester_run_no_timeout_with_timeout_arg(pytester: Pytester) -> None:\n    testfile = pytester.makepyfile(\"def test_no_timeout(): pass\")\n    with pytest.raises(ValueError, match=\"cannot specify both timeout and no_timeout\"):\n        pytester.runpytest_subprocess(testfile, timeout=1, no_timeout=True)\n", "compilation error", "compilation error", "def test_pytester_assert_outcomes_with_skip(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.skip\n            pass\n\n            pass\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(skipped=1, passed=1)\n"], "sample_93": ["    def test_group_by_subquery_annotation_with_filter(self):\n        \"\"\"\n        Subquery annotations are included in the GROUP BY even when filtered.\n        \"\"\"\n        long_books_count_qs = Book.objects.filter(\n            publisher=OuterRef('pk'),\n            pages__gt=400,\n        ).values(\n            'publisher'\n        ).annotate(count=Count('pk')).values('count').filter(count__gt=1)\n        long_books_count_breakdown = Publisher.objects.values_list(\n            Subquery(long_books_count_qs, IntegerField()),\n        ).annotate(total=Count('*'))\n        self.assertEqual(dict(long_books_count_breakdown), {None: 1, 1: 1})\n\n", "    def test_annotation_with_aggregate_over_subquery(self):\n        Book.objects.create(\n            isbn=\"abcde11\", name=\"none\", pages=10, rating=4.0,\n            price=9999.98, contact=self.a1, publisher=self.p1, pubdate=timezone.now()\n        )\n\n        qs = Publisher.objects.annotate(\n            max_price=Max(Subquery(Book.objects.filter(publisher=OuterRef('pk')).values('price')))\n        ).filter(max_price__gt=10).order_by('max_price')\n        self.assertQuerysetEqual(qs, [self.p1, ], lambda v: v.pk)\n        self.assertEqual(qs[0].max_price, Decimal('9999.98'))\n", "    def test_aggregation_expression_on_related_model(self):\n        # test that aggregation on a related field works correctly\n        qs = Publisher.objects.annotate(avg_book_rating=Avg('book__rating')).order_by('avg_book_rating')\n        self.assertQuerysetEqual(\n            qs,\n            [self.p4, self.p3, self.p1, self.p2],\n            lambda v: v.pk)\n\n        qs = Publisher.objects.annotate(\n            avg_book_rating=Avg(F('book__rating') * F('book__price'))\n            ).order_by('avg_book_rating')\n        self.assertQuerysetEqual(\n            qs,\n            [self.p4, self.p3, self.p1, self.p2],\n            lambda v: v.pk)\n", "    def test_complex_annotation_with_subquery_aggregate(self):\n        # Test using a subquery with a complex annotation\n        latest_book_pubdate_qs = Book.objects.filter(\n            publisher=OuterRef('pk')\n        ).order_by('-pubdate').values('pubdate')[:1]\n        publisher_qs = Publisher.objects.annotate(\n            latest_book_pubdate=Subquery(latest_book_pubdate_qs),\n            count=Count('book')\n        ).annotate(\n            avg_rating_since_latest=Avg(\n                F('book__rating')\n                * F('book__price')\n                / (\n                    Func(F('latest_book_pubdate'), function='YEAR')\n                    - Extracted('book__pubdate', year=True)\n                )\n            )\n        ).filter(avg_rating_since_latest__gt=10)\n        with self.assertNumQueries(1) as ctx:\n            list(publisher_qs)\n        self.assertEqual(ctx[0]['sql'].count('SELECT'), 2)\n\n", "    def test_annotation_different_output_fields(self):\n        with self.assertRaisesMessage(\n            FieldError, 'Incompatible output fields: IntegerField and FloatField'\n        ):\n            Book.objects.annotate(\n                combined_field=Sum(F('rating') + F('price'), output_field=IntegerField()\n                )\n            ).annotate(\n                combined_field=Avg(F('combined_field'), output_field=FloatField())\n            )\n\n"], "sample_798": ["def test_ridge_regression_sparse_solver_error():\n     rng = check_random_state(42)\n    X = rng.rand(1000, 3)\n    true_coefs = [1, 2, 0.1]\n    y = np.dot(X, true_coefs)\n    X_sparse = sp.csr_matrix(X)\n    alpha = 1e-3\n    solver = 'cholesky'\n    assert_raises_regex(ValueError,\n                        \"Solver 'cholesky' does not support sparse input.\",\n                        ridge_regression, X_sparse, y,\n                        alpha=alpha, solver=solver)\n", "    def test_ridge_intercept_sparse_vs_dense(self):\n        rng = np.random.RandomState(42)\n        n_samples, n_features = 100, 50\n        X_dense = rng.randn(n_samples, n_features)\n        X_sparse = sp.csr_matrix(X_dense)\n\n        y = rng.randn(n_samples)\n        alpha = 1.0\n\n        for solver in ['sparse_cg', 'cholesky', 'lsqr', 'sag', 'saga']:\n            ridge_dense = Ridge(alpha=alpha, solver=solver, fit_intercept=True)\n            ridge_sparse = Ridge(alpha=alpha, solver=solver, fit_intercept=True)\n            ridge_dense.fit(X_dense, y)\n            ridge_sparse.fit(X_sparse, y)\n\n            assert_allclose(ridge_dense.coef_, ridge_sparse.coef_, rtol=1e-5)\n            assert_allclose(ridge_dense.intercept_, ridge_sparse.intercept_, rtol=1e-5)\n\n", "def test_ridge_classifier_predict_proba():\n    X, y = make_classification(n_samples=100, n_features=10, random_state=0)\n    clf = RidgeClassifierCV()\n    clf.fit(X, y)\n    proba = clf.predict_proba(X)\n    assert proba.shape == (100, 2)\n    assert np.all(np.isclose(proba.sum(axis=1), 1.0))\n", "    def test_ridge_cv_warm_start():\n        rng = np.random.RandomState(0)\n\n        n_samples, n_features = 10, 5\n        X = rng.randn(n_samples, n_features)\n        y = rng.randn(n_samples)\n\n        alphas = [1e-3, 1e-2, 1e-1, 1]\n\n        ridge_cv = RidgeCV(alphas=alphas, cv=5)\n\n        # Fit first time\n        ridge_cv.fit(X, y)\n\n        # Store the best estimator's coefficient\n        initial_coef = ridge_cv.best_estimator_.coef_\n\n        # Fit again using warm start\n        ridge_cv.fit(X, y, warm_start=True)\n\n        # The coefficients should be the same as the previous fit\n        assert_array_almost_equal(ridge_cv.best_estimator_.coef_,\n                                 initial_coef)\n\n", "def test_ridge_intercept_with_no_fit_intercept():\n    # Test that the intercept is set to 0 when fit_intercept is False\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([1, 2, 3])\n\n    # Test Ridge with fit_intercept=False\n    ridge = Ridge(fit_intercept=False)\n    ridge.fit(X, y)\n    assert ridge.intercept_ == 0\n\n    # Test RidgeClassifier with fit_intercept=False\n    ridge_classifier = RidgeClassifier(fit_intercept=False)\n    ridge_classifier.fit(X, y)\n    assert ridge_classifier.intercept_ == 0\n\n"], "sample_905": ["compilation error", "def test_getdoc_decorated_classmethod():\n    class Foo:\n        @classmethod\n        @functools.lru_cache()\n            \"\"\"\n            docstring\n                indented text\n            \"\"\"\n    assert inspect.getdoc(Foo.meth, getattr, False, Foo, \"meth\") == Foo.meth.__doc__\n    assert inspect.getdoc(Foo.meth, getattr, True, Foo, \"meth\") == Foo.meth.__doc__\n", "compilation error", "    def test_is_builtin_class_method():\n        assert inspect.is_builtin_class_method(int.from_bytes) is True\n        assert inspect.is_builtin_class_method(str.join) is False\n", "compilation error"], "sample_352": ["    def test_ticket_24889(self):\n        # Test that applying a filter with a subquery using F() expression\n        # doesn't lead to an incorrect query.\n        a1 = Action.objects.create(name=\"a1\")\n        a2 = Action.objects.create(name=\"a2\")\n        p1 = Permission.objects.create(action=a1, name=\"p1\")\n        p2 = Permission.objects.create(action=a2, name=\"p2\")\n        Role.objects.create(permissions=p1)\n\n        qs = Action.objects.filter(permission__in=Permission.objects.filter(\n            role__permissions__action=F('id')\n        )).order_by('name')\n        self.assertSequenceEqual(qs, [a1])\n\n", "    def test_ticket_24723(self):\n        \"\"\"\n        Regression test for ticket #24723: ensure that JOINs are properly generated\n        for subqueries that are used as the basis for a filter.\n        \"\"\"\n        a1 = Article.objects.create(title=\"Article 1\")\n        a2 = Article.objects.create(title=\"Article 2\")\n        a3 = Article.objects.create(title=\"Article 3\")\n\n        c1 = Comment.objects.create(article=a1, text=\"Comment 1\")\n        c2 = Comment.objects.create(article=a2, text=\"Comment 2\")\n        c3 = Comment.objects.create(article=a3, text=\"Comment 3\")\n\n        # Use a subquery in the filter to check if there are any comments\n        # with the text \"Comment 1\".\n        qs = Article.objects.filter(comment__text=\"Comment 1\").order_by('pk')\n        self.assertSequenceEqual(qs, [a1])\n", "    def test_ticket_24605_negated_subquery(self):\n        i1 = Individual.objects.create(alive=True)\n        RelatedIndividual.objects.create(related=i1)\n        i2 = Individual.objects.create(alive=False)\n        RelatedIndividual.objects.create(related=i2)\n        i3 = Individual.objects.create(alive=True)\n        i4 = Individual.objects.create(alive=False)\n\n        self.assertSequenceEqual(Individual.objects.filter(~Q(related_individual__isnull=True), Q(alive=False)), [i2])\n", "    def test_ticket_24761(self):\n        p1 = Parent.objects.create()\n        c1 = Child.objects.create(parent=p1)\n        c2 = Child.objects.create(parent=p1)\n        c3 = Child.objects.create()\n        qs = Parent.objects.filter(child__in=[c1, c2, c3])\n        self.assertSequenceEqual(qs, [p1])\n", "    def test_ticket_24605_fk_isnull(self):\n        i1 = Individual.objects.create(alive=True)\n        RelatedIndividual.objects.create(related=i1)\n        i2 = Individual.objects.create(alive=False)\n        RelatedIndividual.objects.create(related=i2)\n        i3 = Individual.objects.create(alive=True)\n        i4 = Individual.objects.create(alive=False)\n\n        self.assertSequenceEqual(Individual.objects.filter(related_individual__isnull=True), [i3, i4])\n\n"], "sample_1051": ["def test_dotprint_empty_expression():\n    text = dotprint(Basic())\n    assert \"digraph\" in text\n    assert not any(x in text for x in ['Add', 'Symbol', 'Integer', 'Pow'])\n\n", "def test_dotprint_empty_expr():\n    text = dotprint(Basic())\n    assert dotnode(Basic()) in text\n    assert not dotedges(Basic())\n", "def test_dotprint_Atom():\n    atom = lambda x: isinstance(x, Integer)\n    text = dotprint(x + 2, atom=atom)\n    assert dotnode(x) not in text\n    assert dotnode(Integer(2)) in text\n", "def test_dotprint_empty_expr():\n    assert dotprint(Symbol('x')*0) == dotprint(Integer(0))\n", "def test_dotprint_styles():\n    styles = [(Basic, {'color': 'red', 'shape': 'box'}),\n              (Expr,  {'color': 'green', 'shape': 'ellipse'})]\n    text = dotprint(x + 2, styles=styles)\n    assert '\"color\"=\"red\"' in text\n    assert '\"shape\"=\"box\"' in text\n    assert '\"color\"=\"green\"' in text\n    assert '\"shape\"=\"ellipse\"' in text\n"], "sample_158": ["    def test_inherited_m2m_through(self):\n        class Base(models.Model):\n            name = models.CharField(max_length=100)\n\n        class Child(Base):\n            tags = models.ManyToManyField(\"Tag\", through=\"ChildTag\")\n\n        class Tag(models.Model):\n            name = models.CharField(max_length=100)\n\n        class ChildTag(models.Model):\n            child = models.ForeignKey(Child, on_delete=models.CASCADE)\n            tag = models.ForeignKey(Tag, on_delete=models.CASCADE)\n\n        self.assertEqual(Child.check(), [])\n\n", "    def test_clash_with_related_name(self):\n        class Target(models.Model):\n            clash = models.CharField(max_length=10)\n\n        class Model(models.Model):\n            rel = models.ManyToManyField(Target, related_name='clash', symmetrical=False)\n\n        self.assertEqual(Model.check(), [\n            Error(\n                \"Reverse accessor for 'Model.rel' clashes with field name 'Target.clash'.\",\n                hint=(\n                    \"Rename field 'Target.clash', or add/change a related_name \"\n                    \"argument to the definition for field 'Model.rel'.\"\n                ),\n                obj=Model._meta.get_field('rel'),\n                id='fields.E302',\n            ),\n            Error(\n                \"Reverse query name for 'Model.rel' clashes with field name 'Target.clash'.\",\n                hint=(\n                    \"Rename field 'Target.clash', or add/change a related_name \"\n                    \"argument to the definition for field 'Model.rel'.\"\n                ),\n                obj=Model._meta.get_field('rel'),\n                id='fields.E303',\n            ),\n\n        ])\n", "    def test_foreign_object_to_through_model(self):\n        \"\"\"\n        A ForeignObject field can't point to an intermediary model\n        used in a ManyToManyField with through_fields.\n\n        \"\"\"\n        class Fan(models.Model):\n            pass\n\n        class Event(models.Model):\n            invitees = models.ManyToManyField(Fan, through='Invitation', through_fields=('invitee', 'event'))\n\n        class Invitation(models.Model):\n            event = models.ForeignKey(Event, models.CASCADE)\n            invitee = models.ForeignKey(Fan, models.CASCADE)\n            inviter = models.ForeignKey(Fan, models.CASCADE, related_name='+')\n            some_other_field = models.CharField(max_length=255)\n\n        class Attender(models.Model):\n            invitation = models.ForeignObject(Invitation, on_delete=models.CASCADE, related_name='attenders')\n\n        field = Attender._meta.get_field('invitation')\n        self.assertEqual(field.check(from_model=Attender), [\n            Error(\n                \"ForeignObject field cannot point to an intermediary model 'Invitation' \"\n                \"that is used in a ManyToManyField with 'through_fields'.\",\n                hint=\"Either remove the 'through_fields' argument from the ManyToManyField or \"\n                \"choose a different model for the ForeignObject field.\",\n                obj=field,\n                id='fields.E340',\n            ),\n        ])\n", "    def test_non_unique_foreign_object(self):\n        class Parent(models.Model):\n            a = models.PositiveIntegerField()\n            b = models.PositiveIntegerField()\n\n        class Child(models.Model):\n            parent = models.ForeignObject(\n                Parent,\n                on_delete=models.SET_NULL,\n                from_fields=('a', ),\n                to_fields=('a',),\n                related_name='children',\n            )\n        field = Child._meta.get_field('parent')\n        self.assertEqual(field.check(from_model=Child), [\n            Error(\n                \"No unique field or set of fields on model 'Parent' matches the \"\n                \"from_fields ('a',).\",\n                hint=(\n                    'Mark a single field as unique=True or add a set of '\n                    'fields to a unique constraint (via unique_together or a '\n                    'UniqueConstraint (without condition) in the model '\n                    'Meta.constraints).'\n                ),\n                obj=field,\n                id='fields.E310',\n            ),\n        ])\n", "    def test_hidden_multi_foreign_objects(self):\n        class Parent(models.Model):\n            a = models.PositiveIntegerField()\n            b = models.PositiveIntegerField()\n            c = models.PositiveIntegerField()\n\n            class Meta:\n                unique_together = (('a', 'b', 'c'),)\n\n        class Child(models.Model):\n            a = models.PositiveIntegerField()\n            b = models.PositiveIntegerField()\n            value = models.CharField(max_length=255)\n            parent = models.ForeignObject(\n                Parent,\n                on_delete=models.SET_NULL,\n                from_fields=('a', 'b'),\n                to_fields=('a', 'b'),\n                related_name='+',\n            )\n\n        field = Child._meta.get_field('parent')\n        self.assertEqual(field.check(from_model=Child), [])\n"], "sample_374": ["    def test_empty_nested_prefetch(self):\n        with self.assertNumQueries(2):\n            empty_books = Book.objects.filter(title='empty').prefetch_related(\n                Prefetch('authors', Prefetch('addresses', AuthorAddress.objects.filter(address='empty')))\n            )\n        self.assertEqual(list(empty_books), [])\n", "    def test_nested_prefetch_with_isnull(self):\n        with self.assertNumQueries(3):\n            rooms = Room.objects.prefetch_related(\n                Prefetch('house', queryset=House.objects.filter(name__isnull=False)),\n                Prefetch('house__occupants', queryset=AuthorWithAge.objects.filter(age__gt=60)),\n            ).all()\n        self.assertEqual(rooms.count(), 1)\n        self.assertEqual(rooms[0].house.name, 'Big house')\n        self.assertEqual(rooms[0].house.occupants.count(), 0)\n", "    def test_prefetch_related_empty_related_objects(self):\n        empty_house = House.objects.create(name='Empty house', address='456 Side St')\n        with self.assertNumQueries(2):\n            rooms = Room.objects.prefetch_related('house').filter(house=empty_house)\n        self.assertEqual(len(list(rooms)), 0)\n", "    def test_prefetch_related_on_reversed_foreign_key_with_prefetch_related(self):\n        authors = AuthorWithAge.objects.prefetch_related(\n            Prefetch(\n                'favorite_authors',\n                queryset=Author.objects.prefetch_related('first_book'),\n            ),\n        )\n        with self.assertNumQueries(3):\n            self.assertSequenceEqual(authors, [self.author1, self.author2])\n", "    def test_nested_prefetch_with_null_relation(self):\n        house = House.objects.create(name='Big house', address='123 Main St')\n        room1 = Room.objects.create(name='Kitchen', house=house)\n        room2 = Room.objects.create(name='Living room', house=house)\n        room1.occupants.add(self.author1)\n\n        with self.assertNumQueries(3):\n            rooms = Room.objects.filter(pk__in=[room1.pk, room2.pk]).prefetch_related(\n                Prefetch('occupants', queryset=Author.objects.prefetch_related('favorite_books'))\n            )\n\n        self.assertEqual(len(rooms), 2)\n        self.assertEqual(rooms[0].occupants.all(), [self.author1])\n        self.assertEqual(rooms[1].occupants.all(), [])\n"], "sample_510": ["    def test_subplot_projection_reuse_with_kwargs():\n        # create an Axes\n        ax1 = plt.subplot(111, facecolor=\"red\")\n        # check that it is current\n        assert ax1 is plt.gca()\n        # make sure we get it back if we ask again without kwargs\n        assert ax1 is plt.subplot(111)\n        # remove it\n        ax1.remove()\n        # create a polar plot\n        ax2 = plt.subplot(111, projection='polar', facecolor=\"blue\")\n        assert ax2 is plt.gca()\n        # this should have deleted the first axes\n        assert ax1 not in plt.gcf().axes\n        # assert we get it back if no extra parameters passed\n        assert ax2 is plt.subplot(111)\n", "    def test_subplot_projection_reuse():", "compilation error", "compilation error", "compilation error"], "sample_951": ["compilation error", "compilation error", "def test_getdoc_decorated_property():\n    class Foo:\n        @property\n        @functools.lru_cache()\n            \"\"\"\n            docstring\n                indented text\n            \"\"\"\n            return 42\n\n    assert inspect.getdoc(Foo.my_property) == Foo.my_property.__doc__\n", "compilation error", "compilation error"], "sample_652": ["    def test_fixture_factory_with_params(self, testdir, monkeypatch):\n        monkeypatch.setattr(\n            pytest, \"FIXTURE_ORDER\", [], raising=False\n        )\n\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            FIXTURE_ORDER = pytest.FIXTURE_ORDER\n\n            @pytest.fixture\n                FIXTURE_ORDER.append(f\"my_fixture_factory_{a}_{b}\")\n                return a + b\n\n            @pytest.fixture\n                FIXTURE_ORDER.append(\"arg1\")\n                return 1\n            @pytest.fixture\n                FIXTURE_ORDER.append(\"arg2\")\n                return 2\n            \n                pass\n\n        \"\"\"\n        )\n        items, _ = testdir.inline_genitems()\n        request = FixtureRequest(items[0])\n        assert request.fixturenames == \"arg1 arg2 my_fixture_factory\".split()\n\n        testdir.runpytest()\n        assert pytest.FIXTURE_ORDER == [\n            \"arg1\",\n            \"arg2\",\n            \"my_fixture_factory_1_2\",\n        ]\n", "    def test_fixture_call_from_another_fixture(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.fixture(scope=\"module\")\n                return 1\n\n            @pytest.fixture\n                return my_fixture * 2\n            \n                assert nested_fixture == 2\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.assertoutcome(passed=1)\n", "    def test_fixture_finalizer_order(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.fixture(scope='module', autouse=True)\n                print(\"Setting up m_fixture\")\n                yield\n                print(\"Finalizing m_fixture\")\n\n            @pytest.fixture(scope='function')\n                print(\"Setting up f_fixture\")\n                yield\n                print(\"Finalizing f_fixture\")\n\n                print(\"Running test_func\")\n        \"\"\"\n        )\n        result = testdir.runpytest(\"-s\")\n        result.stdout.fnmatch_lines(\n            [\n                \"Setting up m_fixture\",\n                \"Setting up f_fixture\",\n                \"Running test_func\",\n                \"Finalizing f_fixture\",\n                \"Finalizing m_fixture\",\n            ]\n        )\n\n", "    def test_fixture_request_getfixturevalue_error(self, testdir):\n\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.fixture\n                return 1\n            \n                request.getfixturevalue('fix')\n                request.getfixturevalue('nonexistent_fixture') \n            \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines(\n            [\"*LookupError: Fixture not found: 'nonexistent_fixture'\"]\n        )\n\n", "def test_missing_scope(testdir):\n    testdir.makepyfile(\n        \"\"\"\n\n        import pytest\n\n        @pytest.fixture\n            pass\n\n            pass\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(failed=1)\n    result.stdout.fnmatch_lines(\n        [\n            \"*fixture 'myfixture' doesn't have a declared scope*\",\n            \"*Consider adding 'scope' argument to the fixture definition*\",\n        ]\n    )\n"], "sample_954": ["def test_desc_signatures(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'sphinxtests.1').read_text()\n    assert '.RS 4\\n.IP \"function(arg1, arg2) -> int\"' in content\n    assert '.IP \"class MyClass(baseclass1, baseclass2) \" ' in content\n", "def test_toctree(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'sphinxtests.1').read_text()\n    assert 'submodule' in content\n", "def test_image_directive(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'sphinx.1').read_text()\n    assert '[image: sphinx.png]' in content\n    assert '[image]' in content\n", "def test_download_reference(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'sphinxtests.1').read_text()\n    assert 'Download: \\\\fI[linktext]\\\\fP' in content\n", "def test_man_output_section_number(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'sphinxtests.1').read_text()\n\n    assert '.SH 1. First Section\\n' in content\n    assert '.SH 2. Second Section\\n' in content\n"], "sample_171": ["    def test_squashmigrations_app_name_specified_as_label(self):\n        with self.assertRaisesMessage(CommandError, self.did_you_mean_auth_error):\n            call_command('squashmigrations', 'django.contrib.auth', '0002', interactive=False)\n", "    def test_squashmigrations_app_name_specified_as_label(self):\n        with self.assertRaisesMessage(CommandError, self.did_you_mean_auth_error):\n            call_command('squashmigrations', 'django.contrib.auth', '0002')\n", "    def test_squashmigrations_app_name_specified_as_label(self):\n        with self.assertRaisesMessage(CommandError, self.did_you_mean_auth_error):\n            call_command('squashmigrations', 'django.contrib.auth', '0002')\n", "    def test_squashmigrations_app_name_specified_as_label(self):\n        with self.assertRaisesMessage(CommandError, self.did_you_mean_auth_error):\n            call_command('squashmigrations', 'django.contrib.auth', '0002')\n\n", "    def test_squashmigrations_app_name_specified_as_label(self):\n        with self.assertRaisesMessage(CommandError, self.did_you_mean_auth_error):\n            call_command('squashmigrations', 'django.contrib.auth', '0002')\n"], "sample_288": ["    def test_key_transform_with_different_field_names(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                bax__k__l=F('value__k__l'),\n            ).values_list('id'),\n            [self.objs[4]],\n        )\n\n", "    def test_key_transform_on_subquery(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                id__in=NullableJSONModel.objects.filter(\n                    pk=OuterRef('pk'), value__c=KeyTransform('c', 'value')\n                ),\n            ),\n            self.objs[3:5],\n        )\n", "    def test_key_transform_with_null_values(self):\n        obj = NullableJSONModel.objects.create(value={'a': None, 'b': 1})\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__a=KeyTransform('a', 'value'),\n            ),\n            [obj],\n        )\n", "    def test_deep_key_transform_ordering(self):\n        qs = NullableJSONModel.objects.annotate(\n            nested_key=KeyTransform('c', KeyTransform('baz', 'value')),\n        ).order_by('nested_key')\n        self.assertSequenceEqual(\n            qs,\n            [self.objs[4]],\n        )\n", "    def test_contains_with_expression_wrapper(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__contains=ExpressionWrapper({'a': 'b'}, output_field=JSONField()),\n            ),\n            [self.objs[3], self.objs[4]],\n        )\n"], "sample_764": ["    def func(X):\n        return ['first']\n    ", "    def test_column_transformer_multi_output_transformer():\n        X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n\n        class MultiOutputTransformer(TransformerMixin, BaseEstimator):\n                return self\n\n                return np.array([X[:, 0], X[:, 1]]).T\n\n        ct = ColumnTransformer([('trans', MultiOutputTransformer(), [0])],\n                               remainder='passthrough')\n\n        expected_output = np.array([[0, 1, 2], [2, 4, 6],\n                                    [0, 2, 2], [2, 4, 6]]).T\n        assert_array_equal(ct.fit_transform(X_array), expected_output)\n", "    def func(X):\n        return ['first']\n", "compilation error", "compilation error"], "sample_1116": ["def test_inverse_determinant():\n    assert (Inverse(C).determinant() == 1/C.det()) \n    assert (Inverse(A*B).determinant() == 1/(A.det()*B.det()))\n", "def test_derivative():\n    from sympy import diff\n    X = MatrixSymbol('X', 2, 2)\n    Y = MatrixSymbol('Y', 2, 2)\n\n    assert diff(X.I, X) == -X.I*X.T*X.I\n    assert diff(Inverse(X*Y), X) == -Inverse(X*Y)*Y.T*Inverse(X*Y)\n", "compilation error", "def test_derivative():\n    A = MatrixSymbol('A', 3, 3)\n    x = symbols('x')\n    A_inv = Inverse(A)\n    assert A_inv.diff(x) == -A.T.diff(x)*A_inv**2\n", "def test_determinant():\n    assert Inverse(C)._eval_determinant() == 1 / C.det()\n    assert Inverse(C).T._eval_determinant() == (1 / C.det())\n    assert Inverse(C * D)._eval_determinant() == (1 / (C * D).det())\n\n\n\n"], "sample_516": ["    def test_pdf_font_weight(self):\n        fig, ax = plt.subplots()\n        ax.text(0.5, 0.5, 'Hello, world!', fontweight='bold')\n        buf = io.BytesIO()\n        fig.savefig(buf, format='pdf')\n        pdf_content = buf.getvalue()\n        assert b'/FontWeight 700' in pdf_content\n", "def test_embed_font_subset(fontname, fontfile):\n    if Path(findfont(FontProperties(family=[fontname]))).name != fontfile:\n        pytest.skip(f'Font {fontname!r} may be missing')\n\n    rcParams['pdf.fonttype'] = 42\n    fig, ax = plt.subplots()\n    ax.set_title(f'Test {fontname} subset embedding', font=fontname)\n    fig.savefig(io.BytesIO(), format='pdf')\n\n\n", "def test_pdf_empty_page():\n    with PdfPages(io.BytesIO()) as pdf:\n        pdf.savefig(figure=None)\n        assert pdf._file.pageList\n\n", "def test_font_size_rounding(font_size):\n    fig, ax = plt.subplots()\n    ax.text(0.5, 0.5, 'Test Text', fontsize=font_size)\n    buf = io.BytesIO()\n    fig.savefig(buf, format='pdf')\n    with pikepdf.Pdf.open(buf) as pdf:\n        font_size_from_pdf = pdf.pages[0].Annots[0].Rect[3]\n    assert font_size_from_pdf == font_size\n    \n", "    def test_glyph_cache(self):\n        # Test that glyphs are cached correctly between pages\n\n        fig, ax = plt.subplots()\n        ax.text(0.5, 0.5, 'This is a text', fontproperties=FontProperties(\n            size=14, weight='bold', family=['DejaVu Sans']))\n        fig.savefig(io.BytesIO(), format='pdf')\n\n        fig, ax = plt.subplots()\n        ax.text(0.5, 0.5, 'This is another text', fontproperties=FontProperties(\n            size=14, weight='bold', family=['DejaVu Sans']))\n        fig.savefig(io.BytesIO(), format='pdf')\n\n        # Assert that the number of unique glyphs used is less than the total\n        # number of glyphs used across both pages\n\n"], "sample_559": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_821": ["def test_affinity_propagation_sparse_input():\n    # Test AffinityPropagation with sparse input\n    X = csr_matrix(np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]))\n    af = AffinityPropagation(affinity=\"precomputed\")\n    assert_raises(ValueError, af.fit, X)\n", "def test_affinity_propagation_sparse_input():\n\n    # Test affinity propagation with sparse input via affinity=\"precomputed\".\n\n    S = csr_matrix([[0, 1, 2], [1, 0, 3], [2, 3, 0]])\n    af = AffinityPropagation(affinity=\"precomputed\")\n    assert_no_warnings(af.fit, S)\n\n    # Test input validation\n    af = AffinityPropagation(affinity=\"precomputed\")\n    assert_raises(TypeError, af.fit, np.array([[1,2],[3,4]]))\n", "def test_affinity_propagation_sparse_affinity_matrix():\n    # Test AffinityPropagation with a sparse affinity matrix\n    X = np.array([[0, 0], [1, 1], [-2, -2]])\n    S = csr_matrix(np.dot(X, X.T))\n    af = AffinityPropagation(affinity=\"precomputed\")\n    af.fit(S)\n    assert_array_equal(S.toarray(), af.affinity_matrix_)\n\n", "    def test_affinity_propagation_sparse_input():\n        # Test AffinityPropagation with sparse input data\n\n        from scipy.sparse import csr_matrix\n\n        X_sparse = csr_matrix(X)\n\n        af = AffinityPropagation(affinity=\"precomputed\")\n        assert_raises(TypeError, af.fit, X_sparse)\n\n        af = AffinityPropagation()\n        af.fit(X_sparse)\n        labels_sparse = af.labels_\n\n        af = AffinityPropagation()\n        af.fit(X)\n        labels = af.labels_\n\n        assert_array_equal(labels, labels_sparse)\n", "    def test_affinity_propagation_sparse_input():\n        # Test AffinityPropagation with sparse input\n        S = csr_matrix([[0, 1, 0],\n                        [1, 0, 1],\n                        [0, 1, 0]])\n\n        af = AffinityPropagation(affinity='precomputed')\n        labels = af.fit_predict(S)\n        assert_array_equal(labels, [1, 0, 1])\n"], "sample_522": ["    def test_colorbar_labelpad():\n        fig, ax = plt.subplots()\n        pc = ax.pcolormesh(np.random.randn(10, 10))\n        cb = fig.colorbar(pc)\n        cb.set_label('Colorbar Label', labelpad=20)\n        assert cb.label.get_position()[0] == 20\n", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_334": ["    def test_attribute_subclassing(self):\n        class CustomForm(Form):\n            default_renderer = CustomRenderer\n\n        class SubForm(CustomForm):\n            pass\n\n        form = SubForm()\n        self.assertIsInstance(form.renderer, CustomRenderer)\n", "    def test_attribute_class_inheritance(self):\n        class BaseForm(Form):\n            default_renderer = CustomRenderer\n\n        class CustomForm(BaseForm):\n            pass\n\n        form = CustomForm()\n        self.assertIsInstance(form.renderer, CustomRenderer)\n", "    def test_attribute_class_overridden(self):\n        class Custom1(Form):\n            default_renderer = CustomRenderer\n\n        class Custom2(Custom1):\n            default_renderer = DjangoTemplates\n\n        form = Custom2()\n        self.assertEqual(form.renderer, Custom2.default_renderer)\n", "    def test_attribute_class_inheritance(self):\n        class BaseForm(Form):\n            default_renderer = CustomRenderer\n\n        class DerivedForm(BaseForm):\n            pass\n\n        form = DerivedForm()\n        self.assertIsInstance(form.renderer, DerivedForm.default_renderer)\n\n", "    def test_attribute_class_override(self):\n        class CustomForm(Form):\n            default_renderer = DjangoTemplates\n\n        class OverridingCustomForm(CustomForm):\n            default_renderer = CustomRenderer\n\n        form = OverridingCustomForm()\n        self.assertIsInstance(form.renderer, OverridingCustomForm.default_renderer)\n"], "sample_239": ["    def test_all_valid_with_non_form_errors(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-MAX_NUM_FORMS': '2',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset1.errors = {'non_field_errors': ['This is a non-form error']}\n        self.assertIs(all_valid((formset1, formset2)), False)\n        self.assertEqual(formset1._errors, {'non_field_errors': ['This is a non-form error']})\n        self.assertEqual(formset2._errors, {})\n", "    def test_empty_formset_with_initial_data(self):\n        \"\"\"Empty formsets with initial data should be valid.\"\"\"\n        data = {'form-TOTAL_FORMS': '0',\n                'form-INITIAL_FORMS': '0'}\n        formset = ChoiceFormSet(data, initial=[{'choice': 'Initial', 'votes': 1}])\n        self.assertTrue(formset.is_valid())\n", "    def test_all_valid_with_invalid_formset(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [{}, {'votes': ['This field is required.']}]\n        self.assertEqual(formset1._errors, expected_errors[0])\n        self.assertEqual(formset2._errors, expected_errors[1])\n", "    def test_all_valid_empty_formset(self):\n        data = {'form-TOTAL_FORMS': '0', 'form-INITIAL_FORMS': '0'}\n        formset = formset_factory(Choice)(data)\n        self.assertIs(all_valid((formset,)), True)\n", "    def test_all_valid_with_non_form_errors(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-MAX_NUM_FORMS': '2',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        formset1.non_form_errors().append(\"General error\") \n        formset2 = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset1, formset2)), False)\n        expected_errors = [{}, {}]\n        self.assertEqual(formset1._errors, expected_errors)\n        self.assertEqual(formset2._errors, expected_errors)\n"], "sample_392": ["    def test_isnull_key_with_key_transform(self):\n        obj = NullableJSONModel.objects.create(\n            value={\"nested\": {\"a\": 1, \"b\": None}}\n        )\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__nested__b__isnull=True),\n            [obj],\n        )\n", "    def test_key_transform_with_null_value(self):\n        obj = NullableJSONModel.objects.create(value={\"a\": None})\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__a__isnull=False,\n                \n                value__x=KeyTransform(\"a\", \"value\"),\n            ),\n            [obj],\n        )\n", "    def test_key_transform_ordering(self):\n        qs = (\n            NullableJSONModel.objects.values(\"value__d__0__name\")\n            .annotate(\n                key=KeyTransform(\"name\", KeyTransform(\"0\", \"value__d\")),\n            )\n            .order_by(\"key\")\n        )\n        self.assertSequenceEqual(list(qs), [{\"value__d__0__name\": \"y\"}, {\"value__d__0__name\": \"z\"}])\n", "    def test_key_transform_on_related_field(self):\n        related_obj = RelatedModel.objects.create()\n        self.objs[4].related_object = related_obj\n        self.objs[4].save()\n\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                related_object__value__has_key=\"j\"\n            ).values_list(\"id\"),\n            [self.objs[4].id],\n        )\n", "    def test_key_transform_with_null_value(self):\n        NullableJSONModel.objects.create(value=None)\n        self.assertEqual(\n            NullableJSONModel.objects.filter(\n                value__c=KeyTransform(\"c\", \"value\")\n            ).count(),\n            len(self.objs),\n        )\n"], "sample_381": ["    def test_custom_migration_name_with_dependency(self):\n        class Migration(migrations.Migration):\n            dependencies = [\n                ('another_app', '0001_initial'),\n            ]\n            operations = [\n                migrations.CreateModel('CustomModel', fields=[]),\n            ]\n\n        migration = Migration('0002_custom_migration', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'custom_migration')\n", "    def test_squashed_migration_with_multiple_operations(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Person', fields=[]),\n                migrations.AddField('Person', 'age', models.IntegerField()),\n                migrations.DeleteModel('Person'),\n            ]\n            \n                return True\n        \n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'squashed_person')\n\n", "    def test_squashed_migrations_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Person1', fields=[]),\n                migrations.CreateModel('Person2', fields=[])\n            ]\n\n        migration1 = Migration('0001_person1', 'test_app')\n        migration2 = Migration('0002_person2', 'test_app')\n        migration_squashed = Migration('0003_squashed_0001', 'test_app')\n        self.assertEqual(migration1.suggest_name(), 'person1')\n        self.assertEqual(migration2.suggest_name(), 'person2')\n        self.assertEqual(migration_squashed.suggest_name(), 'person1_person2')\n", "    def test_migration_name_from_operation_with_custom_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    'Person', fields=[], name='CustomPersonName'\n                ),\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'custompersonname')\n", "    def test_model_with_unique_together(self):\n        class Book(models.Model):\n            title = models.CharField(max_length=200)\n            author = models.ForeignKey('testapp.Author', models.CASCADE)\n\n            class Meta:\n                unique_together = ('title', 'author',)\n\n        class Author(models.Model):\n            name = models.CharField(max_length=200)\n\n        before = [ModelState('testapp', 'author', [\n            ('name', models.CharField(max_length=200)),\n        ])]\n        after = [ModelState('testapp', 'author', [\n            ('name', models.CharField(max_length=200)),\n        ]), ModelState('testapp', 'book', [\n            ('title', models.CharField(max_length=200)),\n            ('author', models.ForeignKey('testapp.Author', models.CASCADE)),\n        ], options={'unique_together': 'testapp.Book.unique_together'})]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['CreateModel'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='book')\n\n"], "sample_552": ["def test_figure_set_dpi_after_savefig():\n    fig, ax = plt.subplots()\n    fig.savefig(io.BytesIO(), format='png')\n    original_dpi = fig.dpi\n    fig.set_dpi(150)\n    fig.savefig(io.BytesIO(), format='png')\n    assert fig.dpi == 150\n", "compilation error", "compilation error", "    def test_savefig_metadata_fail(self, fmt, tmpdir):\n        fig = Figure()\n        with pytest.raises(ValueError, match=\"Metadata dict must have string keys\"):\n            fig.savefig(str(tmpdir.join(f\"test.{fmt}\")), format=fmt,\n                        metadata={1: 'value'})\n", "compilation error"], "sample_618": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_131": ["    def test_migrate_test_setting_true(self, mocked_migrate, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        saved_settings = copy.deepcopy(connection.settings_dict)\n        try:\n            connection.settings_dict['TEST']['MIGRATE'] = True\n            with mock.patch.object(creation, '_create_test_db'):\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n            mocked_migrate.assert_called_once_with(\n                verbosity=max(0, 0 - 1), interactive=False, database=connection.alias, run_syncdb=True\n            )\n        finally:\n            connection.settings_dict = saved_settings\n", "    def test_create_test_db_with_keepdb(self, mocked_migrate, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        with mock.patch.object(creation, '_create_test_db'):\n            creation.create_test_db(verbosity=0, autoclobber=True, serialize=False, keepdb=True)\n            mocked_migrate.assert_not_called()\n", "    def test_clone_test_db(self, mocked_migrate, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        with mock.patch.object(creation, '_clone_test_db'):\n            creation.clone_test_db(suffix='suffix', verbosity=0, autoclobber=True, keepdb=False)\n", "    def test_destroy(self, mocked_migrate, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        test_name = 'test_db'\n        creation.connection.settings_dict['NAME'] = test_name\n        with mock.patch.object(creation, '_destroy_test_db'):\n            creation.destroy_test_db(verbosity=0)\n            mocked_ensure_connection.assert_called_once_with()\n", "    def test_serialize_db_to_string_and_deserialize_db_from_string(self, mocked_migrate, mocked_ensure_connection):\n        creation = connection.creation_class(connection)\n        creation.create_test_db(verbosity=0, autoclobber=True, serialize=True)\n\n        # Serialize the database\n        serialized_data = creation.serialize_db_to_string()\n\n        # Clear the database\n        creation._execute_create_test_db(\n            creation._nodb_connection.cursor(), \n            {'dbname':\n            creation.connection.ops.quote_name(creation._get_test_db_name()),\n            'suffix': creation.sql_table_creation_suffix()},\n            keepdb=True\n        )\n\n        # Deserialize the database\n        creation.deserialize_db_from_string(serialized_data)\n\n        # Assert that the database was successfully deserialized\n        # (This would require comparing the serialized and deserialized data\n        #  which is complex and database-specific)\n        self.assertTrue(True) # Placeholder assertion\n\n"], "sample_1082": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_513": ["    def test_legend_title_font_properties():\n        fig, ax = plt.subplots()\n        ax.plot(range(10), label='test')\n        font_properties = FontProperties(family='serif', size=16, weight='bold')\n        leg = ax.legend(title='Aardvark', title_fontproperties=font_properties)\n\n        assert leg.get_title().get_fontfamily()[0] == 'serif'\n        assert leg.get_title().get_fontsize() == 16\n        assert leg.get_title().get_weight() == 'bold'\n", "    def test_legend_title_fontproperties_fontsize():\n        # test fontsize for title, when passing both fontproperties and fontsize\n        plt.plot(range(10))\n        with pytest.raises(ValueError):\n            plt.legend(title='Aardvark', title_fontproperties=FontProperties(\n                                      family='serif', size=22),\n                       title_fontsize=10)\n", "    def test_legend_bbox_to_anchor():\n        fig, ax = plt.subplots()\n        ax.plot([0, 1], [0, 1], label='line')\n        leg = ax.legend(bbox_to_anchor=(1, 1))\n        assert leg.get_bbox_patch().get_bbox().x1 == fig.bbox.xmax\n        assert leg.get_bbox_patch().get_bbox().y1 == fig.bbox.ymax\n", "def test_legend_bbox_to_anchor():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1], label='line')\n    leg = ax.legend(bbox_to_anchor=(1, 1))\n    assert leg.get_bbox_to_anchor().xy == (1, 1)\n", "    def test_legend_label_with_newlines():\n        # check that line breaks are handled correctly in legend labels\n        fig, ax = plt.subplots()\n        ax.plot([1, 2, 3], [1, 2, 3], label='This\\nis\\na\\nlabel')\n        leg = ax.legend()\n        assert leg.get_texts()[0].get_text() == 'This\\nis\\na\\nlabel'\n"], "sample_731": ["    def test_fetch_california_housing_data_home(tmpdir):\n        # Check if the data is loaded correctly from a given data home.\n\n        data_home = tmpdir.strpath\n        fetch_california_housing(data_home=data_home)\n        data = fetch_california_housing(data_home=data_home,\n                                        download_if_missing=False)\n        assert data is not None\n        assert isinstance(data, Bunch)\n\n        check_return_X_y(fetch_california_housing, data_home=data_home,\n                         skip_if_no_data=True)\n", "    def test_fetch_return_X_y(self):\n        data, target = fetch(return_X_y=True)\n        assert data.shape == (20640, 8)\n        assert target.shape == (20640,)\n", "compilation error", "    def test_fetch_california_housing_data_home(tmpdir):\n        data_home = tmpdir.strpath\n        data = fetch(data_home=data_home)\n\n        assert data.data.shape == (20640, 8)\n        assert data.target.shape == (20640,)\n        assert 'feature_names' in data\n        assert data.DESCR == __doc__\n", "    def test_fetch_california_housing_data_downloaded(tmp_path):\n        # Test with a temporary data_home.\n        fetch_california_housing(data_home=tmp_path)\n        data = fetch_california_housing(data_home=tmp_path, download_if_missing=False)\n        assert isinstance(data, Bunch)\n        assert data.data.shape == (20640, 8)\n        assert data.target.shape == (20640,)\n        assert len(data.feature_names) == 8\n\n        # Test with return_X_y=True\n        X, y = fetch_california_housing(data_home=tmp_path, return_X_y=True,\n                                        download_if_missing=False)\n        assert X.shape == (20640, 8)\n        assert y.shape == (20640,)\n"], "sample_600": ["def test_encode_unsigned_to_signed(dtype):\n    original_values = np.array([2**np.iinfo(np.dtype(dtype)).bits - 1], dtype=np.dtype(dtype))\n    original = xr.Variable((\"x\",), original_values, attrs={\"_Unsigned\": \"true\"})\n    coder = variables.UnsignedIntegerCoder()\n    encoded = coder.encode(original)\n    assert encoded.dtype == np.dtype(f\"i{np.iinfo(np.dtype(dtype)).bits}\")\n    assert encoded.values == original_values.astype(encoded.dtype)\n", "def test_UnsignedIntegerCoder_encode(dtype):\n    original_values = np.array([np.iinfo(np.dtype(dtype)).max], dtype=dtype)\n    original = xr.Variable((\"x\",), original_values, attrs={\"_Unsigned\": \"true\"})\n    coder = variables.UnsignedIntegerCoder()\n    encoded = coder.encode(original)\n    assert encoded.dtype == np.dtype(f\"i{np.iinfo(np.dtype(dtype)).bits}\")\n    assert encoded.values == original_values\n", "    def test_encode_unsigned_to_signed(self, dtype):\n        # test for #1519\n        original_values = np.array([1, 2, -1], dtype=dtype)\n        original = xr.Variable((\"x\",), original_values)\n        coder = variables.UnsignedIntegerCoder()\n\n        encoded = coder.encode(original)\n\n        assert encoded.dtype == np.dtype(f\"i{np.dtype(dtype).itemsize}\")\n        assert encoded.attrs.get(\"_Unsigned\") == \"true\" \n", "    def test_encode_unsigned_to_signed(self, dtype):\n        original = xr.Variable((\"x\",), [1, 2, 3], dims=(\"x\",))\n        original.attrs[\"_FillValue\"] = np.iinfo(np.dtype(dtype)).max\n        original = original.astype(dtype)\n\n        coder = variables.UnsignedIntegerCoder()\n        encoded = coder.encode(original)\n        assert encoded.dtype.kind == \"i\"\n", "def test_encode_unsigned_integer(dtype):\n    original = xr.Variable((\"x\",), np.array([1, 2, 3], dtype=dtype))\n    coder = variables.UnsignedIntegerCoder()\n    encoded = coder.encode(original)\n    assert encoded.dtype == np.dtype(f\"u{np.iinfo(original.dtype).bits}\")\n    assert_identical(encoded, original)\n"], "sample_377": ["    def test_sensitive_method_decorator_with_class_method(self):\n        class MyClass:\n            @sensitive_variables\n            @method_decorator(sensitive_variables)\n                pass\n\n        MyClass().a_view(HttpRequest(), \"password\")\n", "    def test_sensitive_post_parameters_accepts_method_decorator(self):\n        class MyClass:\n            @method_decorator(sensitive_post_parameters)\n                return HttpResponse()\n\n        MyClass().a_view(HttpRequest())\n\n", "    def test_sensitive_variables_with_kwargs(self):\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(\n                sensitive_kwargs_function_caller, check_for_POST_params=False\n            )\n\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(\n                sensitive_kwargs_function_caller, check_for_POST_params=False\n            )\n\n", "    def test_sensitive_variables_with_custom_exception_reporter_filter(self):\n        \"\"\"\n        It's possible to use @sensitive_variables with a custom\n        exception reporter filter.\n        \"\"\"\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(\n                sensitive_variables_with_custom_exception_reporter,\n                check_for_vars=False,\n            )\n\n        with self.settings(DEBUG=False):\n            self.verify_unsafe_response(\n                sensitive_variables_with_custom_exception_reporter,\n                check_for_vars=False,\n            )\n", "    def test_sensitive_variables_with_nested_functions(self):\n            @sensitive_variables\n                pass\n            inner_func(\"secret\")\n\n        with self.assertRaises(TypeError) as e:\n            outer_func(HttpRequest())\n        self.assertIn(\"sensitive_variables() must be called as a decorator\", str(e))\n\n"], "sample_781": ["    def test_forest_oob_score_after_warm_start(self):\n        # Test that oob_score is calculated correctly when warm_start is True\n        # and the model is refitted.\n\n        X, y = make_classification(n_samples=100, n_informative=3, random_state=1)\n        forest = RandomForestClassifier(n_estimators=10, oob_score=True,\n                                         warm_start=True, random_state=42)\n\n        forest.fit(X, y)\n        initial_oob_score = forest.oob_score_\n\n        forest.fit(X, y)  # Refits with warm start\n        final_oob_score = forest.oob_score_\n\n        assert final_oob_score >= initial_oob_score\n", "    def test_oob_score_predict_proba_consistency(self):\n        # Make sure oob_score and predict_proba agree for\n        # forests trained with bootstrap=True\n        X, y = datasets.make_classification(n_samples=100,\n                                            n_features=10,\n                                            n_informative=5,\n                                            random_state=0)\n        est = RandomForestClassifier(n_estimators=100,\n                                     bootstrap=True,\n                                     oob_score=True,\n                                     random_state=0)\n        est.fit(X, y)\n        oob_score = est.oob_score_\n        y_pred_proba = est.predict_proba(X)\n        # Average probabilities for each class\n        avg_probs = np.mean(y_pred_proba, axis=0)\n        # Average probability should match OOB score\n        assert_almost_equal(oob_score, np.max(avg_probs), decimal=2)\n", "compilation error", "def test_oob_score_is_not_none(name):\n    ForestClassifier = FOREST_CLASSIFIERS[name]\n    clf = ForestClassifier(oob_score=True, random_state=42, bootstrap=True)\n    clf.fit(iris.data, iris.target)\n    assert clf.oob_score_ is not None\n", "def test_forest_oob_score_regression():\n    # Check that oob_score is correctly computed for regression.\n    rng = np.random.RandomState(42)\n    X = rng.rand(100, 10)\n    y = rng.rand(100)\n    est = RandomForestRegressor(n_estimators=10, random_state=42,\n                                oob_score=True)\n    est.fit(X, y)\n    assert hasattr(est, 'oob_score_')\n    assert 0 <= est.oob_score_ <= 1\n"], "sample_1103": ["def test_issue_15907():\n    a = symbols('a')\n    assert (a/a).is_one\n    assert (a/(a+1)).subs(a, 0) == 0\n    assert (a/(a+1)).subs(a, 1) == Rational(1, 2)\n\n", "def test_issue_16676():\n    from sympy import diff\n    f = Function('f')\n    assert diff(f(x*y), x) == f(x*y) * y\n    assert diff(f(x*y), x, evaluate=False) == Derivative(f(x*y), x)\n", "compilation error", "compilation error", "compilation error"], "sample_345": ["    def test_should_stop(self):\n        self.reloader.should_stop = True\n        self.assertTrue(self.reloader.should_stop())\n", "    def test_notify_file_changed_ignores_directories(self):\n        with mock.patch.object(self.reloader, '_create_watch_entry') as mocked_entry:\n            self.reloader.notify_file_changed(self.tempdir)\n        self.assertFalse(mocked_entry.called)\n", "    def test_tick_waits_correctly(self, mocked_sleep):\n        ticker = self.reloader.tick()\n        next(ticker)\n        self.assertEqual(mocked_sleep.call_count, 1)\n        self.assertEqual(mocked_sleep.call_args[0][0], self.reloader.SLEEP_TIME)\n", "    def test_notify_file_changed_ignores_missing_files(self):\n        self.reloader.notify_file_changed(self.nonexistent_file)\n        self.assertFalse(self.reloader.should_stop)\n", "    def test_should_stop_with_no_watched_files(self):\n        self.reloader._watched_files = set()\n        self.assertTrue(self.reloader.should_stop())\n"], "sample_909": ["    def test_numpydocstring_empty_doc(self):\n        docstring = \"\"\n        actual = str(NumpyDocstring(docstring))\n        expected = \"\"\n        self.assertEqual(expected, actual)\n", "compilation error", "    def test_google_style_raises(self):\n        docstrings = [(\"\"\"", "    def test_see_also_with_underscores(self):\n        docstring = \"\"\"", "    def test_raises_types_with_underscores(self):\n        docstrings = [(\"\"\""], "sample_1195": ["compilation error", "compilation error", "def test_gamma_trace():\n    mu, nu, rho, sigma = tensor_indices(\"mu, nu, rho, sigma\", LorentzIndex)\n\n    assert gamma_trace(G(mu)*G(nu)) == 4*MetricTensor(mu, nu)  \n    assert gamma_trace(G(mu)*G(nu)*G(rho)*G(sigma)) == 4*(MetricTensor(mu, nu)*MetricTensor(rho, sigma) + MetricTensor(mu, rho)*MetricTensor(nu, sigma) + MetricTensor(mu, sigma)*MetricTensor(nu, rho))\n", "compilation error", "compilation error"], "sample_432": ["    def test_clear_selection_after_page_change(self):\n        from selenium.webdriver.common.by import By\n\n        Parent.objects.bulk_create([Parent(name=\"parent%d\" % i) for i in range(101)])\n        self.admin_login(username=\"super\", password=\"secret\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_changelist_parent_changelist\")\n        )\n\n        select_all = self.selenium.find_element(By.ID, \"action-toggle\")\n        select_all.click()\n        self.selenium.find_element(By.LINK_TEXT, \"2\").click()\n        self.assertEqual(\n            self.selenium.find_element(By.CSS_SELECTOR, \".action-counter\").text,\n            \"0 of 100 selected\",\n        )\n\n", "    def test_delete_selected_with_pending_unsaved_changes(self):\n        from selenium.webdriver.common.by import By\n        from selenium.webdriver.support.ui import Select\n\n        Parent.objects.create(name=\"parent\")\n\n        self.admin_login(username=\"super\", password=\"secret\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_changelist_parent_changelist\")\n        )\n\n        name_input = self.selenium.find_element(By.ID, \"id_form-0-name\")\n        name_input.clear()\n        name_input.send_keys(\"other name\")\n        Select(self.selenium.find_element(By.NAME, \"action\")).select_by_value(\n            \"delete_selected\"\n        )\n        # Click on \"Save\" button before applying the action.\n        self.selenium.find_element(By.NAME, \"_save\").click()\n        self.selenium.find_element(By.NAME, \"index\").click()\n        alert = self.selenium.switch_to.alert\n        try:\n            self.assertEqual(\n                alert.text,\n                \"You have selected an action, but you haven't saved your changes to individual fields yet. Please click OK to save. You\u2019ll need to re-run the action.\",\n            )\n        finally:\n            alert.dismiss()\n\n", "    def test_search_in_non_default_field(self):\n        Parent.objects.create(name=\"Parent 1\")\n        Parent.objects.create(description=\"Parent 2 description\")\n        self.admin_login(username=\"super\", password=\"secret\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_changelist_parent_changelist\")\n        )\n        search_box = self.selenium.find_element(By.ID, \"searchbar\")\n        search_box.send_keys(\"description\")\n        search_box_submit = self.selenium.find_element(By.CSS_SELECTOR, 'input[type=\"submit\"]')\n        search_box_submit.click()\n        self.assertEqual(self.selenium.find_elements(By.CSS_SELECTOR, \"tbody tr\").__len__(), 1)\n", "    def test_page_size_dropdown(self):\n        self.admin_login(username=\"super\", password=\"secret\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:auth_user_changelist\")\n        )\n        dropdown = self.selenium.find_element(By.ID, \"page_size\")\n        self.assertEqual(dropdown.get_attribute(\"value\"), \"100\")\n        dropdown.click()\n        options = dropdown.find_elements(By.TAG_NAME, \"option\")\n        self.assertEqual(\n            [option.text for option in options if option.is_displayed()],\n            [\"10\", \"20\", \"50\", \"100\", \"All\"],\n        )\n\n", "    def test_changelist_filters_are_applied_to_search(self):\n        from selenium.webdriver.common.by import By\n\n        for i in range(3):\n            Parent.objects.create(name=f\"parent_{i}\", description=\"test\")\n\n        self.admin_login(username=\"super\", password=\"secret\")\n        self.selenium.get(\n            self.live_server_url + reverse(\"admin:admin_changelist_parent_changelist\")\n        )\n        search_input = self.selenium.find_element(By.NAME, \"q\")\n        search_input.send_keys(\"test\")\n        search_input.submit()\n        self.assertEqual(\n            self.selenium.page_source,\n            '3 results are available'\n\n        )\n        self.selenium.find_element(By.ID, 'id_description').click()\n\n        self.selenium.find_element(By.ID, 'id_description').send_keys('test')\n        self.selenium.find_element(By.CSS_SELECTOR, '.content .submit-row .submit input').click()\n        self.assertEqual(\n            self.selenium.page_source,\n            '3 results are available'\n        )\n\n"], "sample_572": ["    def test_errorbar_type_validation(self):\n\n        with pytest.raises(TypeError):\n            EstimateAggregator(\"mean\", \"invalid\")\n\n        with pytest.raises(TypeError):\n            EstimateAggregator((1, 2, 3), \"sd\")\n\n", "    def test_bad_errorbar_spec(self):\n\n        with pytest.raises(ValueError):\n            EstimateAggregator(\"mean\", \"invalid\")\n", "    def test_no_errorbars(self, long_df):\n        agg = EstimateAggregator(\"mean\")\n        out = agg(long_df, \"x\")\n        assert out[\"x\"] == long_df[\"x\"].mean()\n        assert \"xmin\" not in out\n        assert \"xmax\" not in out\n", "    def test_invalid_errorbar_arg(self):\n\n        with pytest.raises(TypeError, match=\"`errorbar` must be a callable\"):\n            EstimateAggregator(\"mean\", \"invalid\")\n", "    def test_no_errorbars(self, long_df):\n        agg = EstimateAggregator(\"mean\")\n        out = agg(long_df, \"x\")\n        assert out[\"x\"] == long_df[\"x\"].mean()\n        assert \"xmin\" not in out\n        assert \"xmax\" not in out\n"], "sample_1125": ["def test_operator_commutator():\n    A = Operator('A')\n    B = Operator('B')\n    assert A._eval_commutator(B) is None\n    assert A._eval_commutator(A) == Integer(0)\n    assert A._eval_commutator(Dagger(A)) == Integer(0)\n", "def test_identity_operator():\n    I = IdentityOperator()\n    assert I * Operator('A') == Operator('A')\n    assert Operator('A') * I == Operator('A')\n    assert I.inverse() == I\n", "def test_operator_apply():\n    x = symbols('x')\n    O = Operator('O')\n    psi = Operator('psi')\n    assert O*psi == Mul(O, psi)\n", "def test_operator_commutator():\n   A = Operator('A')\n   B = Operator('B')\n   assert A._eval_commutator(B, **{}) is None\n   assert A.commutator(B) is None\n", "compilation error"], "sample_217": ["    def test_media_class_inheritance(self):\n        class BaseWidget(TextInput):\n            class Media:\n                css = {'all': ('base.css',)}\n\n        class MyWidget(BaseWidget):\n            class Media:\n                js = ('mywidget.js',)\n\n        w = MyWidget()\n        self.assertEqual(str(w.media), \"\"\"<link href=\"http://media.example.com/static/base.css\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">", "    def test_media_property_multi_parents(self):\n        class BaseWidget(TextInput):\n            class Media:\n                css = {\n                    'all': ('/base/css1',),\n                }\n\n        class MyWidget1(BaseWidget):\n            class Media:\n                css = {\n                    'all': ('/my/css1',),\n                }\n\n        class MyWidget2(BaseWidget):\n            class Media:\n                css = {\n                    'all': ('/my/css2',),\n                }\n\n        w1 = MyWidget1()\n        w2 = MyWidget2()\n        self.assertEqual(str(w1.media), \"\"\"<link href=\"/base/css1\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">", "    def test_media_property_with_inheritance(self):\n        class MyWidgetBase(TextInput):\n            class Media:\n                css = {'all': ('base.css',)}\n                js = ('base.js',)\n\n        class MyWidgetDerived(MyWidgetBase):\n                return super().media + Media(css={'print': ('derived.css',)})\n            media = property(_media)\n\n        w = MyWidgetDerived()\n        self.assertEqual(\n            str(w.media),\n            \"\"\"<link href=\"http://media.example.com/static/base.css\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">", "    def test_media_inheritance_duplicate_css_files(self):\n        class MyWidget1(TextInput):\n            class Media:\n                css = {\n                    'all': ('path/to/css1', '/path/to/css2')\n                }\n                js = ('/path/to/js1',)\n\n        class MyWidget2(MyWidget1):\n            class Media:\n                css = {\n                    'all': ('/path/to/css2', '/path/to/css3')\n                }\n\n        w2 = MyWidget2()\n        self.assertEqual(\n            str(w2.media),\n            \"\"\"<link href=\"http://media.example.com/static/path/to/css1\" type=\"text/css\" media=\"all\" rel=\"stylesheet\">", "    def test_media_media_type_order(self):\n        widget1 = Media(css={'screen': ['a.css'], 'all': ['b.css']})\n        widget2 = Media(css={'print': ['c.css'], 'all': ['d.css']})\n        widget3 = Media(css={'screen': ['e.css'], 'all': ['f.css']})\n        merged = widget1 + widget2 + widget3\n        self.assertEqual(merged._css, {'screen': ['a.css', 'e.css'], 'print': ['c.css'], 'all': ['b.css', 'd.css', 'f.css']})\n"], "sample_944": ["def test_restify_type_hints_ellipsis():\n    from typing import Ellipsis\n    assert restify(Ellipsis) == \"...\"\n", "def test_stringify_type_hints_special_form():\n    from typing import Generic, TypeVar\n\n    T = TypeVar('T')\n\n    class MyGeneric(Generic[T]):\n        pass\n\n    assert stringify(MyGeneric[int]) == 'tests.test_util_typing.MyGeneric[int]'\n", "def test_stringify_type_union_operator_nested():\n    assert stringify(List[int | None]) == \"List[Optional[int]]\"  # type: ignore\n    assert stringify(List[int | str]) == \"List[int | str]\"  # type: ignore\n    assert stringify(List[int | str | None]) == \"List[Optional[int | str]]\"  # type: ignore\n", "def test_stringify_type_union_operator_with_typevars():\n    T = TypeVar('T')\n    assert stringify(T | None) == \"Optional[T]\"  # type: ignore\n    assert stringify(str | T) == \"str | T\"  # type: ignore\n    assert stringify(T | str | None) == \"Optional[str | T]\"  # type: ignore\n", "    def test_stringify_complex_type_hints():\n        T = TypeVar('T')\n        U = TypeVar('U')\n        class Foo(Generic[T]):\n                pass\n        assert stringify(Foo[int]) == \"tests.test_util_typing.Foo[int]\"\n        assert stringify(Foo[List[str]]) == \"tests.test_util_typing.Foo[List[str]]\"\n        assert stringify(Foo[T]) == \"tests.test_util_typing.Foo[T]\"\n        assert stringify(Foo[Union[int, str]]) == \"tests.test_util_typing.Foo[Union[int, str]]\"\n        from typing import Type\n        assert stringify(Type[Foo[T]]) == \"Type[tests.test_util_typing.Foo[T]]\"\n"], "sample_984": ["compilation error", "def test_And():\n    assert str(And(x > 0, x < 1)) == 'And(x > 0, x < 1)'\n", "compilation error", "compilation error", "compilation error"], "sample_992": ["    def test_NumPyPrinter_Piecewise():\n        p = NumPyPrinter()\n        expr = Piecewise((x, x < 0), (y, x > 0), (0, True))\n        assert p.doprint(expr) == \"numpy.select([x < 0, x > 0, True], [x, y, 0], default=numpy.nan)\"\n", "    def test_NumPyPrinter_Piecewise():\n        p = NumPyPrinter()\n        expr = Piecewise((x**2, x > 0), (x, True))\n        assert p.doprint(expr) == 'numpy.select([x > 0], [x**2, x], default=numpy.nan)'\n\n", "    def test_NumPyPrinter_Piecewise():\n        p = NumPyPrinter()\n        expr = Piecewise((x, x < 1), (y, x >= 1))\n        assert p.doprint(expr) == 'numpy.select([x < 1], [x, y], default=numpy.nan)'\n", "    def test_NumPyPrinter():\n        p = NumPyPrinter()\n        expr = acos(x)\n        assert 'numpy' in p.module_imports\n        assert p.doprint(expr) == 'numpy.arccos(x)'\n        assert not any(m.startswith('scipy') for m in p.module_imports)\n        expr = Mod(x, 2)\n        assert p.doprint(expr) == 'numpy.mod(x, 2)'\n", "    def test_NumPyPrinter():\n        p = NumPyPrinter()\n        assert p.doprint(x + y) == 'x + y'\n        assert p.doprint(x**y) == 'x**y'\n        assert p.doprint(Mod(x, 2)) == 'numpy.mod(x, 2)'\n        assert p.doprint(And(x, y)) == 'numpy.logical_and(x, y)'\n        assert p.doprint(Or(x, y)) == 'numpy.logical_or(x, y)'\n        assert p.doprint(Not(x)) == 'numpy.logical_not(x)'\n\n"], "sample_192": ["compilation error", "    def test_all_valid_with_inital_data(self):\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '2',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n     }\n        ChoiceFormSet = formset_factory(Choice)\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset,)), True)\n", "    def test_valid_empty_formset(self):\n        ChoiceFormSet = formset_factory(Choice)\n        formset = ChoiceFormSet(data={}, auto_id=False, prefix='choices')\n        self.assertIs(all_valid((formset,)), True)\n        self.assertEqual(formset._errors, [{}])\n", "    def test_all_valid_with_empty_formsets(self):\n        data = {}\n        ChoiceFormSet = formset_factory(Choice)\n        formset1 = ChoiceFormSet(data, auto_id=False)\n        formset2 = ChoiceFormSet(data, auto_id=False)\n        self.assertIs(all_valid((formset1, formset2)), True)\n", "    def test_empty_formset_cleaned_data(self):\n        data = {'form-TOTAL_FORMS': '0', 'form-INITIAL_FORMS': '0'}\n        formset = ArticleFormSet(data)\n        self.assertEqual(formset.cleaned_data, [])\n"], "sample_306": ["    def test_iso8601_duration(self):\n        test_values = (\n            ('P1DT2H30M45S', timedelta(days=1, hours=2, minutes=30, seconds=45)),\n            ('P1DT2H30M45,5S', timedelta(days=1, hours=2, minutes=30, seconds=45, microseconds=500000)),\n            ('P1DT2H30M45.5S', timedelta(days=1, hours=2, minutes=30, seconds=45, microseconds=500000)),\n            ('PT2H30M45S', timedelta(hours=2, minutes=30, seconds=45)),\n            ('PT2H30M45,5S', timedelta(hours=2, minutes=30, seconds=45, microseconds=500000)),\n            ('PT2H30M45.5S', timedelta(hours=2, minutes=30, seconds=45, microseconds=500000)),\n            ('P-1DT2H30M45S', timedelta(days=-1, hours=2, minutes=30, seconds=45)),\n            ('P-1DT2H30M45,5S', timedelta(days=-1, hours=2, minutes=30, seconds=45, microseconds=500000)),\n            ('P-1DT2H30M45.5S', timedelta(days=-1, hours=2, minutes=30, seconds=45, microseconds=500000)),\n            ('P1D', timedelta(days=1)),\n            ('P-1D', timedelta(days=-1)),\n            ('PT1H', timedelta(hours=1)),\n            ('PT-1H', timedelta(hours=-1)),\n            ('PT1M', timedelta(minutes=1)),\n            ('PT-1M', timedelta(minutes=-1)),\n            ('PT1S', timedelta(seconds=1)),\n            ('PT-1S', timedelta(seconds=-1)),\n        )\n        for source,", "    def test_iso8601_duration_format(self):\n        test_values = (\n            ('P1DT2H3M4S', timedelta(days=1, hours=2, minutes=3, seconds=4)),\n            ('PT2H3M4S', timedelta(hours=2, minutes=3, seconds=4)),\n            ('P1D', timedelta(days=1)),\n            ('PT', timedelta(seconds=0)),\n            ('P1.5D', timedelta(days=1.5)),\n            ('P-1DT2H3M4S', timedelta(days=-1, hours=2, minutes=3, seconds=4)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n", "    def test_invalid_iso8601_format(self):\n        self.assertIsNone(parse_duration('P1DT2H'))\n        self.assertIsNone(parse_duration('PT1H1M'))\n        self.assertIsNone(parse_duration('P1D 2H'))\n        self.assertIsNone(parse_duration('PT1H 2M'))\n", "    def test_iso8601(self):\n        test_values = (\n            ('P1DT10H20M30.1S', timedelta(days=1, hours=10, minutes=20, seconds=30, milliseconds=100)),\n            ('PT10H20M30.1S', timedelta(hours=10, minutes=20, seconds=30, milliseconds=100)),\n            ('P1D', timedelta(days=1)),\n            ('PT10H', timedelta(hours=10)),\n            ('PT10M', timedelta(minutes=10)),\n            ('PT10S', timedelta(seconds=10)),\n            ('P1DT10H20M30', timedelta(days=1, hours=10, minutes=20, seconds=30)),\n            ('-P1DT10H20M30S', timedelta(days=-1, hours=-10, minutes=-20, seconds=-30)),\n            ('-PT10H20M30S', timedelta(hours=-10, minutes=-20, seconds=-30)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n", "    def test_iso_8601_duration(self):\n        test_values = (\n            ('P1DT12H30M', timedelta(days=1, hours=12, minutes=30)),\n            ('PT12H30M', timedelta(hours=12, minutes=30)),\n            ('P1D', timedelta(days=1)),\n            ('PT1H', timedelta(hours=1)),\n            ('PT1M', timedelta(minutes=1)),\n            ('PT1S', timedelta(seconds=1)),\n            ('P1DT12H30M10S', timedelta(days=1, hours=12, minutes=30, seconds=10)),\n            ('-P1DT12H30M', timedelta(days=-1, hours=-12, minutes=-30)),\n            ('P1D2H30M10.123S', timedelta(days=1, hours=2, minutes=30, seconds=10, microseconds=123000)),\n            ('P1D2H30M10,123S', timedelta(days=1, hours=2, minutes=30, seconds=10, microseconds=123000)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n"], "sample_1168": ["    def test_multiset_permutations_empty():\n        assert list(multiset_permutations([])) == [[]]\n        assert list(multiset_permutations(multiset([]))) == [[]]\n", "    def test_combinations_with_replacement():\n        assert list(combinations_with_replacement('abc', 2)) == \\\n            [('a', 'a'), ('a', 'b'), ('a', 'c'), ('b', 'b'), ('b', 'c'),\n             ('c', 'c')]\n        assert list(combinations_with_replacement([1, 2, 3], 2)) == \\\n            [(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)]\n        assert list(combinations_with_replacement(range(4), 3)) == [\n            (0, 0, 0), (0, 0, 1), (0, 0, 2), (0, 0, 3), (0, 1, 1), (0, 1, 2),\n            (0, 1, 3), (0, 2, 2), (0, 2, 3), (0, 3, 3), (1, 1, 1), (1, 1, 2),\n            (1, 1, 3), (1, 2, 2), (1, 2, 3), (1, 3, 3), (2, 2, 2), (2, 2, 3),\n            (2, 3, 3), (3, 3, 3)]\n", "compilation error", "    def test_cyclic_permutations():\n        assert list(cyclic_permutations('abc', 2)) == ['abc', 'bca', 'cab']\n        assert list(cyclic_permutations('abc', 3)) == ['abc', 'bca', 'cab']\n        assert list(cyclic_permutations([1, 2, 3], 2)) == [[1, 2, 3], [2, 3, 1], [3, 1, 2]]\n", "    def test_unrank_bell():\n        for n in range(1, 6):\n            for i in range(factorial(n)):\n                assert len(list(generate_bell(n))) == factorial(n)\n                p = Permutation(list(unrank_bell(n, i)))\n                assert p in generate_bell(n)\n                assert next(generate_bell(n, i)) == p\n"], "sample_400": ["    def test_custom_name_with_squashed(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\"Person\", fields=[]),\n            ]\n            name = \"custom_name\"\n\n        migration = Migration(\"0001_custom_name_squashed_0002_auto_20230425_1200\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"custom_name\")\n\n", "    def test_operation_with_custom_name(self):\n        class CustomOperation(migrations.Operation):\n                super().__init__()\n                self.name = name\n                return f\"Custom operation {self.name}\"\n\n        class Migration(migrations.Migration):\n            operations = [CustomOperation(\"my_custom_operation\")]\n\n        migration = Migration(\"0001_initial\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"my_custom_operation\")\n", "    def test_alter_unique_together(self):\n        changes = self.get_changes(\n            [\n                ModelState(\n                    \"testapp\",\n                    \"Book\",\n                    [\n                        (\"id\", models.AutoField(primary_key=True)),\n                        (\"title\", models.CharField(max_length=200)),\n                        (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n                    ],\n                ),\n                ModelState(\"testapp\", \"Author\", [(\"id\", models.AutoField(primary_key=True))]),\n            ],\n            [\n                ModelState(\n                    \"testapp\",\n                    \"Book\",\n                    [\n                        (\"id\", models.AutoField(primary_key=True)),\n                        (\n                            \"title\",\n                            models.CharField(unique=True, max_length=200),\n                        ),\n                        (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n                    ],\n                ),\n                ModelState(\"testapp\", \"Author\", [(\"id\", models.AutoField(primary_key=True))]),\n            ],\n        )\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n        self.assertOperationAttributes(\n            changes, \"testapp\", 0, 0, name=\"title\",\n            field_name=\"title\", unique=True\n        )\n\n", "    def test_operation_with_custom_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    \"Person\",\n                    fields=[],\n                    name=\"custom_person\",\n                ),\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"custom_person\")\n\n", "    def test_squashed_migration_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\"Person\", fields=[]),\n                migrations.CreateModel(\"Animal\", fields=[]),\n            ]\n            squashed = True\n\n        migration = Migration(\"0001_initial\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"squashed_initial\")\n"], "sample_138": ["    def test_manifest_strict_missing_entry(self):\n        missing_file_name = 'cached/missing.css'\n        configured_storage = storage.staticfiles_storage\n        self.assertNotIn(missing_file_name, configured_storage.hashed_files)\n\n        # File name not found in manifest\n        with self.assertRaisesMessage(ValueError, \"Missing staticfiles manifest entry for '%s'\" % missing_file_name):\n            self.hashed_file_path(missing_file_name)\n        \n        # File doesn't exist on disk\n        err_msg = \"The file '%s' could not be found with %r.\" % (missing_file_name, configured_storage._wrapped)\n        with self.assertRaisesMessage(ValueError, err_msg):\n            self.hashed_file_path(missing_file_name)\n", "    def test_post_process_relative_urls_in_css(self):\n        relpath = self.hashed_file_path('cached/styles.css')\n        self.assertEqual(relpath, 'cached/styles.a60c0e74834f.css')\n        with storage.staticfiles_storage.open(relpath) as relfile:\n            content = relfile.read()\n            self.assertIn(b'url(\"../another.css\")', content)\n", "    def test_relative_import_with_hashed_name(self):\n        relpath = self.hashed_file_path('cached/css/relative.css')\n        self.assertEqual(relpath, 'cached/css/relative.1234567890abcdef.css')\n        with storage.staticfiles_storage.open(relpath) as relfile:\n            content = relfile.read()\n            self.assertIn(b'url(\"img/relative.acae32e4532b.png\")', content)\n", "    def test_manifest_entry_ordering(self):\n        relpath1 = self.hashed_file_path('cached/styles.css')\n        relpath2 = self.hashed_file_path('cached/other.css')\n\n        with open(self._get_filename_path('styles.css'), 'w') as f:\n            f.write('@import url(\"%s\");' % relpath2)\n        with open(self._get_filename_path('other.css'), 'w') as f:\n            f.write('/* other.css content */')\n\n        self.run_collectstatic()\n\n        # Ensure that the hashed filenames are correct regardless of order\n        with storage.staticfiles_storage.open(relpath1) as relfile1:\n            content1 = relfile1.read()\n            self.assertIn(b'@import url(\"%s\");' % relpath2.encode(), content1)\n        with storage.staticfiles_storage.open(relpath2) as relfile2:\n            content2 = relfile2.read()\n            self.assertEqual(content2, b'/* other.css content */')\n", "    def test_manifest_version_updated_on_change(self):\n        original_version = storage.staticfiles_storage.manifest_version\n        # Modify a file to ensure the manifest version is updated.\n        filename = self.hashed_file_path('cached/styles.css')\n        with open(os.path.join(settings.STATIC_ROOT, filename), 'a') as f:\n            f.write(\"\\n/* Changed */\")\n        self.run_collectstatic(clear=True)\n        self.assertNotEqual(storage.staticfiles_storage.manifest_version, original_version)\n"], "sample_958": ["def test_domain_cpp_build_nested_templates(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"nested-template\")\n    assert len(ws) == 0\n", "    def check(spec, text, file):\n        pattern = '<li><p>%s<a .*?><code .*?><span .*?>%s</span></code></a></p></li>' % spec\n        res = re.search(pattern, text)\n        if not res:\n            print(\"Pattern\\n\\t%s\\nnot found in %s\" % (pattern, file))\n            assert False\n", "compilation error", "def test_domain_cpp_build_with_cpp_id_attributes(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"cpp-id-attributes\")\n    assert len(ws) == 0\n        actual = app.env.domaindata['cpp'].find_attr_id(targetType, 'id_attr')\n        assert actual == expected\n    check('class', 'id_attr')\n    check('struct', 'id_attr')\n    check('union', 'id_attr')\n    check('function', 'id_attr')\n    check('member', 'id_attr')\n    check('var', 'id_attr')\n    check('type', 'id_attr')\n    check('concept', 'id_attr')\n    check('enum', 'id_attr')\n    check('enumerator', 'id_attr')\n", "    def check(target):\n        class Config:\n            cpp_id_attributes = [\"id_attr\"]\n            cpp_paren_attributes = [\"paren_attr\"]\n        parser = DefinitionParser(target, location=None,\n                                  config=Config())\n        ast, isShorthand = parser.parse_xref_object()\n        parser.assert_end()\n\n\n\n"], "sample_337": ["    def test_csrf_cookie_secure_flag(self):\n        \"\"\"\n        The CSRF cookie should be flagged as Secure when HTTPS is used.\n        \"\"\"\n        req = self._get_request()\n        req._is_secure_override = True\n        mw = CsrfViewMiddleware(token_view)\n        mw.process_view(req, token_view, (), {})\n        resp = mw(req)\n        csrf_cookie = resp.cookies.get('csrfcookie')\n        self.assertTrue(csrf_cookie['secure'])\n", "    def test_csrf_cookie_not_set_in_session_on_post(self):\n        \"\"\"\n        If the CSRF cookie is not set in the session on a POST request, the\n        middleware should reject the request.\n\n        \"\"\"\n        req = self._get_POST_request_with_token()\n        mw = CsrfViewMiddleware(post_form_view)\n        self._check_referer_rejects(mw, req)\n        response = mw.process_view(req, post_form_view, (), {})\n        self.assertEqual(response.status_code, 403)\n        self.assertEqual(\n            response.reason_phrase,\n            'CSRF verification failed. Request aborted.',\n        )\n", "    def test_csrf_cookie_domain_with_empty_string(self):\n        \"\"\"\n        CSRF cookie domain can be set to an empty string.\n        \"\"\"\n        with self.settings(CSRF_COOKIE_DOMAIN=''):\n            req = self._get_request()\n            mw = CsrfViewMiddleware(token_view)\n            mw.process_view(req, token_view, (), {})\n            resp = mw(req)\n            csrf_cookie = resp.cookies.get(settings.CSRF_COOKIE_NAME)\n            self.assertIsNotNone(csrf_cookie)\n            self.assertEqual(csrf_cookie['domain'], \"\")\n", "    def test_http_good_referer_with_insecure_cookie(self):\n        \"\"\"\n        A POST HTTP request with a good referer should be accepted even\n        if CSRF_COOKIE_SECURE is False, as the cookie is stored in the session.\n        \"\"\"\n        req = self._get_POST_request_with_token()\n        req.META['HTTP_REFERER'] = 'http://www.example.com/'\n        mw = CsrfViewMiddleware(post_form_view)\n        response = mw.process_view(req, post_form_view, (), {})\n        self.assertIsNone(response)\n", "    def test_csrf_protection_disabled(self):\n        \"\"\"\n        If CSRF_COOKIE_NAME is set to None, CSRF protection is disabled.\n        \"\"\"\n        with self.settings(CSRF_COOKIE_NAME=None):\n            req = self._get_POST_request_with_token()\n            mw = CsrfViewMiddleware(post_form_view)\n            response = mw.process_view(req, post_form_view, (), {})\n            self.assertIsNone(response)\n"], "sample_609": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_388": ["    def test_inactive_user_email(self):\n        User.objects.create(username=\"inactive_user\", is_active=False, email=\"inactive@example.com\")\n        response = self.client.get(\"/remote_user/\", **{self.header: \"inactive_user\"})\n        self.assertTrue(response.context[\"user\"].is_anonymous) \n", "    def test_configure_user_called_twice_for_known_user(self):\n        \"\"\"\n        Ensure configure_user is called only once for known users.\n\n        Regression test for #29304.\n        \"\"\"\n        user = User.objects.create(username=\"knownuser\")\n        with mock.patch.object(CustomRemoteUserBackend, 'configure_user') as mock_configure_user:\n            response = self.client.get(\"/remote_user/\", **{self.header: \"knownuser\"})\n            self.assertEqual(mock_configure_user.call_count, 1)\n", "    def test_configure_user_called_twice(self):\n        \"\"\"\n        Tests that configure_user is called twice when a user is created and then\n        fetched again in the same request.\n\n        This tests that the backend doesn't accidentally overwrite changes\n        made by the first call to configure_user.\n        \"\"\"\n        # Create a user with a specific email address\n        User.objects.create(username=\"testuser\", email=\"test@example.com\")\n\n        @override_settings(\n            MIDDLEWARE={\"append\": \"django.contrib.auth.middleware.RemoteUserMiddleware\"},\n        )\n            response = self.client.get(\n                \"/remote_user/\",\n                **{self.header: \"testuser\"},\n            )\n            return response\n\n            user.email = request.META.get(RemoteUserTest.email_header, \"\")\n            return user\n\n        # Mock the configure_user method to track if it's called\n        with patch.object(\n            CustomRemoteUserBackend, \"configure_user\", mock_configure_user\n        ) as mock_method:\n            response = get_response(self.client)\n            # Ensure configure_user was called twice\n            self.assertEqual(mock_method.call_count, 2)\n            self.assertEqual(response.context[\"user\"].email, \"test@example.com\")\n", "    def test_user_can_authenticate(self):\n        \"\"\"\n        Test the user_can_authenticate method of the RemoteUserBackend.\n        \"\"\"\n        user = User.objects.create(username=\"knownuser\", is_active=False)\n        response = self.client.get(\"/remote_user/\", **{self.header: self.known_user})\n        self.assertTrue(response.context[\"user\"].is_anonymous)\n\n        user.is_active = True\n        user.save()\n        response = self.client.get(\"/remote_user/\", **{self.header: self.known_user})\n        self.assertEqual(response.context[\"user\"].username, \"knownuser\")\n", "    def test_auth_remote_user_header_missing_keys(self):\n        \"\"\"\n        Tests the case where required keys are missing from the REMOTE_USER header.\n\n        \"\"\"\n        response = self.client.get(\"/remote_user/\")\n        self.assertTrue(response.context[\"user\"].is_anonymous)\n\n"], "sample_511": ["    def test_colormaps():\n        # Test some basic colormap functionality\n        plt.figure()\n        plt.imshow([[1, 2], [3, 4]], cmap='viridis')\n        plt.colorbar()\n        \n        plt.figure()\n        plt.imshow([[1, 2], [3, 4]], cmap='nipy_spectral')\n        plt.colorbar()\n        \n        plt.figure()\n        plt.imshow([[1, 2], [3, 4]], cmap='magma')\n        plt.colorbar()\n", "def test_subplot_kwarg_warnings():\n    with pytest.warns(UserWarning,\n                       match=\"Passing kwargs to subplot() is deprecated\"):\n        plt.subplot(111, facecolor='red')\n    with pytest.warns(UserWarning,\n                       match=\"Passing kwargs to subplot() is deprecated\"):\n        plt.subplot(111, figsize=(5, 5))\n", "    def test_title(self):\n        fig, ax = plt.subplots()\n        ax.set_title('My Title', fontdict={'fontsize': 16})\n        assert ax.title.get_text() == 'My Title'\n        assert ax.title.get_fontsize() == 16\n\n", "    def test_get_plot_commands():\n        plot_commands = plt.get_plot_commands()\n        assert isinstance(plot_commands, list)\n        assert all(isinstance(cmd, str) for cmd in plot_commands)\n", "def test_figure_subplot_kwarg():\n    fig, ax = plt.subplots(1, 1, figsize=(5, 5), facecolor=\"red\")\n    assert ax.figure.get_figwidth() == 5\n    assert ax.figure.get_facecolor() == \"red\"\n"], "sample_785": ["compilation error", "    def test_time_series_split_errors():\n        X = np.zeros((10, 1))\n        with pytest.raises(ValueError, match='`n_splits` should be greater'):\n            TimeSeriesSplit(n_splits=0).split(X)\n        with pytest.raises(ValueError, match='`max_train_size` should be greater'):\n            TimeSeriesSplit(max_train_size=0).split(X)\n", "    def test_TimeSeriesSplit_with_n_jobs(self):\n        X = np.arange(100).reshape(10, 10)\n        tscv = TimeSeriesSplit(n_splits=5)\n        with Parallel(n_jobs=2) as parallel:\n            for _ in parallel(delayed(next)(tscv.split(X)) for _ in range(5)):\n                pass\n\n", "    def test_repeated_kfold_properties():\n        rkf = RepeatedKFold(n_splits=5, n_repeats=3)\n        assert rkf.n_splits == 15\n        assert rkf.get_n_splits() == 15\n", "    def test_parameter_validation_kfold():\n        kf = KFold(n_splits=5)\n        with pytest.raises(ValueError):\n            kf.split(X, y=y)\n        with pytest.raises(ValueError):\n            kf.split(X, groups=groups)\n\n"], "sample_1007": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1006": ["def test_binomial_series():\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True)\n\n    assert binomial(n, k).series(n, 0, 3) == \\\n        binomial(0, k) + n*binomial(1, k) + n**2/2*binomial(2, k) + O(n**3)\n\n", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_333": ["    def test_attribute_class_instance(self):\n        class CustomForm(Form):\n            default_renderer = CustomRenderer()\n\n        form = CustomForm()\n        self.assertEqual(form.renderer, CustomForm.default_renderer)\n", "    def test_attribute_subclassing(self):\n        class CustomForm(Form):\n            default_renderer = DjangoTemplates\n\n        class CustomSubForm(CustomForm):\n            pass\n\n        form = CustomSubForm()\n        self.assertIsInstance(form.renderer, CustomForm.default_renderer)\n", "    def test_attribute_class_subclassing(self):\n        class CustomForm(Form):\n            default_renderer = CustomRenderer\n\n        class AnotherCustomForm(CustomForm):\n            pass\n        \n        form = AnotherCustomForm()\n        self.assertIsInstance(form.renderer, AnotherCustomForm.default_renderer)\n\n", "    def test_attribute_class_instance(self):\n        class CustomForm(Form):\n            default_renderer = CustomRenderer\n\n        form = CustomForm()\n        form.default_renderer = DjangoTemplates()\n        self.assertIsInstance(form.renderer, DjangoTemplates)\n", "    def test_renderer_method(self):\n        class CustomForm(Form):\n            default_renderer = CustomRenderer\n\n                return 'Custom render output'\n\n        form = CustomForm()\n        self.assertEqual(form.as_p(), 'Custom render output')\n"], "sample_457": ["    def test_expressions_with_include(self):\n        msg = (\n            \"UniqueConstraint.include cannot be used with expressions. Use \"\n            \"django.contrib.postgres.indexes.OpClass() instead.\"\n        )\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                Lower(\"field\"),\n                include=[\"other_field\"],\n                name=\"test_func_include\",\n            )\n", "    def test_expression_with_include(self):\n        message = \"UniqueConstraint.include cannot be used with expressions.\"\n        with self.assertRaisesMessage(ValueError, message):\n            models.UniqueConstraint(\n                Lower(\"field\"),\n                name=\"test_func_include\",\n                include=[\"other_field\"],\n            )\n        \n", "    def test_include_covers_index(self):\n        UniqueConstraintInclude.objects.create(name=\"p1\", color=\"red\")\n        with self.assertRaises(IntegrityError):\n            UniqueConstraintInclude.objects.create(name=\"p1\", color=\"blue\")\n", "    def test_opclasses_database_constraint(self):\n        obj1 = UniqueConstraintOpclasses.objects.create(name=\"Test User 1\")\n        obj2 = UniqueConstraintOpclasses.objects.create(name=\"Test User 2\")\n        with self.assertRaises(IntegrityError):\n            UniqueConstraintOpclasses.objects.create(name=\"Test User 2\")\n", "    def test_unique_constraint_condition_with_foreign_key(self):\n        u = User.objects.create(username=\"user\")\n        UniqueConstraintForeignKey.objects.create(user=u, name=\"p1\")\n        with self.assertRaises(IntegrityError):\n            UniqueConstraintForeignKey.objects.create(user=u, name=\"p1\")\n"], "sample_1203": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1105": ["def test_matmul_empty_matrix():\n    assert MatMul(*[]).identity == GenericIdentity()\n    assert MatMul(*[]).shape == (1, 1)\n", "def test_matmul_mixed_scalar_matpow():\n    A_sym = MatrixSymbol(\"A\", 2, 2)\n    assert MatMul(2, MatPow(A_sym, 2)).doit() == 2*A_sym**2 \n", "compilation error", "def test_matmul_empty_list():\n    assert MatMul().doit() == 1\n", "def test_matmul_identity_preservation():\n    I = Identity(3)\n    assert MatMul(I, A).doit() == A\n    assert MatMul(I, A, I).doit() == A\n    assert MatMul(A, I).doit() == A\n    assert MatMul(A, I, I).doit() == A\n"], "sample_26": ["    def test_compimagehdu_from_fits(self):\n\n        hdu = fits.CompImageHDU.fromfile(self.data(\"comp.fits\"))\n        assert isinstance(hdu.compressed_data, bytes)\n\n", "    def test_compressed_integers_scaling(self, dtype):\n        \"\"\"Test that the various integer dtypes are correctly written and read with BSCALE/BZERO.\"\"\"\n        mid = np.iinfo(dtype).max // 2\n        data = np.arange(mid - 50, mid + 50, dtype=dtype)\n        testfile = self.temp(\"test.fits\")\n        hdu = fits.CompImageHDU(data=data)\n        hdu.header[\"BSCALE\"] = 2.0\n        hdu.header[\"BZERO\"] = 100\n        hdu.writeto(testfile, overwrite=True)\n        new = fits.getdata(testfile)\n        np.testing.assert_array_equal(\n            (data * 2.0) + 100, new\n        )\n", "def test_compressed_header_naxis():\n    hdu = fits.CompImageHDU(data=np.arange(100).reshape((10,10)))\n    assert hdu.header[\"NAXIS\"] == 2\n", "    def test_comp_hdu_uint64_scaling(self):\n        data = np.arange(100, dtype=np.uint64)\n        hdu = fits.CompImageHDU(data=data)\n        hdu.header[\"BZERO\"] = 100\n        hdu.header[\"BSCALE\"] = 2\n\n        with pytest.raises(TypeError):\n            hdu.scale(\"int32\", bzero=hdu.header[\"BZERO\"], bscale=hdu.header[\"BSCALE\"])\n\n        hdu.scale(\"uint64\", bzero=hdu.header[\"BZERO\"], bscale=hdu.header[\"BSCALE\"])\n        assert (hdu.data == (data * 2 + 100)).all()\n", "    def test_comp_hdu_write_readonly_compressed(self, tmp_path):\n        # Regression test to make sure that we can write out read-only compressed arrays (#5512)\n\n        x = np.array([1.0, 2.0, 3.0])\n        x.setflags(write=False)\n        ghdu = fits.CompImageHDU(data=x, compression_type=\"RICE_1\")\n        \n        filename = tmp_path / \"test3.fits\"\n\n        ghdu.writeto(filename)\n\n        with fits.open(filename) as hdulist:\n            assert_equal(hdulist[1].data, [1.0, 2.0, 3.0]) \n"], "sample_528": ["def test_use_list():\n    mpl.rcParams[PARAM] = 'gray'\n    with temp_style('test1', DUMMY_SETTINGS):\n        with temp_style('test2', {'axes.facecolor': 'blue'}):\n            with style.context(['test1', 'test2']):\n                assert mpl.rcParams[PARAM] == VALUE\n                assert mpl.rcParams['axes.facecolor'] == 'blue'\n", "def test_reload_library():\n    temp_file = 'test_style.%s' % STYLE_EXTENSION\n    temp_path = Path(TemporaryDirectory().name, temp_file)\n    temp_path.parent.mkdir()\n    temp_path.write_text(f'{PARAM}: {VALUE}', encoding='utf-8')\n    style.reload_library()\n    assert temp_path.stem in style.library\n    assert style.library[temp_path.stem][PARAM] == VALUE\n\n", "def test_style_contextmanager():\n    mpl.rcParams[PARAM] = 'gray'\n    style_ctx = style.context('test')\n    with style_ctx as context:\n    assert mpl.rcParams[PARAM] == VALUE\n    assert isinstance(context, contextlib.contextmanager)\n    assert mpl.rcParams[PARAM] == 'gray'\n", "    def test_style_path_in_library(tmpdir):\n        style_name = 'test_style'\n        path = Path(tmpdir, f'{style_name}.{STYLE_EXTENSION}')\n        path.write_text(f'{PARAM} : {VALUE}', encoding='utf-8')\n        style.reload_library()\n        assert style_name in style.library\n        assert style.library[style_name] == {PARAM: VALUE}\n\n", "def test_reload_library():\n    mpl.rcParams[PARAM] = 'gray'\n    with temp_style('test', DUMMY_SETTINGS):\n        mpl.style.reload_library()\n        with style.context('test'):\n            assert mpl.rcParams[PARAM] == VALUE\n"], "sample_469": ["    def test_alias_annotation_with_subquery(self):\n        qs = Book.objects.alias(\n            max_pubdate_alias=Subquery(Book.objects.filter(isbn=OuterRef(\"isbn\")).values(\"pubdate\").order_by(\"-pubdate\")[:1]),\n        ).annotate(max_pubdate=F(\"max_pubdate_alias\"))\n        self.assertIs(hasattr(qs.first(), \"max_pubdate_alias\"), False)\n        for book in qs:\n            with self.subTest(book=book):\n                self.assertEqual(book.max_pubdate, book.pubdate)\n", "    def test_alias_subquery(self):\n        inner_qs = Book.objects.alias(\n            rating_alias=F(\"rating\") - 1,\n        ).values(\"rating_alias\")\n        qs = Book.objects.filter(rating__in=Subquery(inner_qs.values(\"rating_alias\")))\n        self.assertCountEqual(qs, [self.b1, self.b2, self.b3, self.b4])\n", "    def test_alias_with_subquery(self):\n        qs = (\n            Book.objects.alias(\n                related_books_count=Subquery(\n                    Book.objects.filter(publisher=OuterRef(\"publisher\")).count()\n                )\n            )\n            .order_by(\"related_books_count\")\n        )\n        self.assertCountEqual(\n            qs,\n            [\n                self.b4,\n                self.b3,\n                self.b2,\n                self.b1,\n            ],\n        )\n\n", "    def test_alias_with_transform(self):\n        qs = Book.objects.alias(\n            rating_alias=F(\"rating\") * 2,\n        ).annotate(double_rating=Transform(\n            rating_alias, OutputField(DecimalField()),\n        ))\n        self.assertIs(hasattr(qs.first(), \"rating_alias\"), False)\n        for book in qs:\n            with self.subTest(book=book):\n                self.assertEqual(book.double_rating, book.rating * 2)\n\n", "    def test_alias_subquery_in_aggregate_values_chaining(self):\n        qs = (\n            Book.objects.annotate(pub_year=ExtractYear(\"pubdate\"))\n            .alias(\n                top_rated_alias=Subquery(\n                    Book.objects.filter(pubdate__year=OuterRef(\"pub_year\"))\n                    .order_by(\"-rating\")\n                    .values(\"rating\")[:1]\n                ),\n            )\n            .values(\"pub_year\", \"top_rated_alias\", \"rating\")\n            .annotate(\n                top_rating=F(\"top_rated_alias\")\n            )\n        )\n\n        self.assertCountEqual(\n            qs,\n            [\n                {\"pub_year\": 1991, \"top_rating\": 5.0, \"rating\": 5.0},\n                {\"pub_year\": 1995, \"top_rating\": 4.0, \"rating\": 4.0},\n                {\"pub_year\": 2007, \"top_rating\": 4.5, \"rating\": 4.5},\n                {\"pub_year\": 2008, \"top_rating\": 4.0, \"rating\": 4.0},\n            ],\n        )\n\n"], "sample_812": ["def test_recursion():\n    # Check recursion for deeply nested estimators\n\n    class NestedEstimator(BaseEstimator):\n\n            self.estimator = estimator\n        \n            return {'estimator': self.estimator}\n\n    nested_estimator = NestedEstimator(\n        NestedEstimator(\n            NestedEstimator(LogisticRegression())\n        )\n    )\n\n    expected = \"\"\"", "    def test_deeply_nested_changed_only():\n        # Render a deeply nested estimator with changed_only=True\n        set_config(print_changed_only=True)\n        rfe = RFE(RFE(RFE(RFE(RFE(RFE(RFE(LogisticRegression(C=999))))))))\n        expected = \"\"\"", "    def test_empty_vocabulary():\n        vectorizer = CountVectorizer(vocabulary={})\n        expected = \"\"\"", "def test_estimator_with_arrays_of_objects():\n    # Test pprint with estimator containing arrays of objects\n    class MyEstimator(BaseEstimator):\n            self.arr = arr\n\n            return self\n\n    arr = [object() for _ in range(10)]\n    estimator = MyEstimator(arr)\n    repr_ = estimator.__repr__()\n    # Assert that the array content is present\n    assert 'object()' in repr_\n", "def test_deeply_nested_with_changed_only():\n    # Render a deeply nested estimator with print_changed_only=True\n    set_config(print_changed_only=True)\n    rfe = RFE(RFE(RFE(RFE(RFE(RFE(RFE(LogisticRegression(C=99)))))))))\n    expected = \"\"\""], "sample_472": ["    def test_page_iteration(self):\n        paginator = Paginator(Article.objects.order_by(\"id\"), 5)\n        p = paginator.page(1)\n        for i, article in enumerate(p):\n            self.assertEqual(article, self.articles[i])\n", "    def test_paginate_with_custom_ordering(self):\n        \"\"\"\n        Test pagination when ordering is explicitly provided in the queryset.\n\n        This tests if pagination works correctly when a custom ordering is\n\n        applied to the queryset instead of relying on the default ordering.\n        \"\"\"\n        paginator = Paginator(Article.objects.order_by('-pub_date'), 5)\n        p = paginator.page(1)\n        self.assertEqual(\"<Page 1 of 1>\", str(p))\n        self.assertSequenceEqual(\n            p.object_list,\n            self.articles  # Should be in reverse chronological order\n        )\n\n", "    def test_paginator_iteration_empty_object_list(self):\n        paginator = Paginator([], 2)\n        page_iterator = iter(paginator)\n        with self.assertRaises(StopIteration):\n            next(page_iterator)\n        self.assertEqual(\n            [str(page) for page in iter(paginator)],\n            [\"<Page 1 of 1>\"],\n        )\n", "    def test_paginating_empty_queryset_returns_empty_page(self):\n        paginator = Paginator(Article.objects.none(), 5)\n        self.assertEqual(paginator.count, 0)\n        self.assertEqual(paginator.num_pages, 0)\n        page = paginator.page(1)\n        self.assertEqual(page.object_list, [])\n        self.assertFalse(page.has_next())\n        self.assertFalse(page.has_previous())\n        self.assertFalse(page.has_other_pages())\n", "    def test_get_elided_page_range_with_adjacent_pages(self):\n        paginator = Paginator(range(100), 10)\n        page_range = paginator.get_elided_page_range(5)\n        self.assertEqual(list(page_range), [1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\n"], "sample_142": ["    def test_nested_fields_with_duplicates(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            fields = ('title', ('title', 'album'))\n\n        errors = MyModelAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'fields' contains duplicate field(s).\",\n                obj=MyModelAdmin,\n                id='admin.E006'\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_duplicate_fields_in_nested_fieldsets(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            fieldsets = [\n                (None, {\n                    'fields': ['title', ('title', 'album')]\n                }),\n            ]\n\n        errors = MyModelAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"There are duplicate field(s) in 'fieldsets[0][1]'.\",\n                obj=MyModelAdmin,\n                id='admin.E012'\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_check_nested_fieldsets_for_duplicates(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            fieldsets = [\n                (None, {\n                    'fields': ['title', ('title', 'album')]\n                }),\n                (None, {\n                    'fields': ['title']\n                }),\n            ]\n\n        errors = MyModelAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"There are duplicate field(s) in 'fieldsets'.\",\n                obj=MyModelAdmin,\n                id='admin.E011'\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_fieldsets_with_readonly_fields(self):\n        \"\"\"\n        Regression test for #23768 - Ensure fieldsets with readonly fields are\n        correctly handled.\n        \"\"\"\n        class MyModelAdmin(admin.ModelAdmin):\n            readonly_fields = ('title',)\n            fieldsets = (\n                (None, {\n                    'fields': ('title', 'album')\n                }),\n            )\n\n        errors = MyModelAdmin(Song, AdminSite()).check()\n        self.assertEqual(errors, [])\n\n", "    def test_check_fieldsets_for_duplicate_names(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            fieldsets = [\n                ('Name1', {'fields': ['title']}),\n                ('Name1', {'fields': ['album']}),\n            ]\n\n        errors = MyModelAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"Duplicate fieldset names found: 'Name1'.\",\n                obj=MyModelAdmin,\n                id='admin.E011'\n            )\n        ]\n        self.assertEqual(errors, expected)\n"], "sample_554": ["    def test_text_get_text(self):\n        text = Text(0.5, 0.5, 'Hello world!')\n        assert text.get_text() == 'Hello world!'\n", "compilation error", "def test_text_antialiased_off_default_vs_manual_with_font(fig_test, fig_ref):\n    mpl.rcParams['font.family'] = 'fantasy'\n    fig_test.text(0.5, 0.5, '6 inches x 2 inches',\n                             antialiased=False)\n\n    mpl.rcParams['text.antialiased'] = False\n    fig_ref.text(0.5, 0.5, '6 inches x 2 inches')\n\n", "compilation error", "compilation error"], "sample_107": ["    def test_cleanse_setting_nested(self):\n        settings_dict = {'DATABASE': {'PASSWORD': 'not_so_secret'}}\n        cleansed_dict = cleanse_setting('DATABASE', settings_dict)\n        self.assertEqual(cleansed_dict['DATABASE']['PASSWORD'], CLEANSED_SUBSTITUTE)\n", "    def test_cleanse_setting_nested(self):\n        nested_dict = {'secrets': {'PASSWORD': 'secret_password'}}\n        cleansed_dict = cleanse_setting('secrets', nested_dict)\n        self.assertEqual(cleansed_dict['secrets']['PASSWORD'], CLEANSED_SUBSTITUTE)\n", "    def test_cleanse_setting_nested_dicts(self):\n        sensitive_data = {'secrets': {'api_key': '123', 'password': 'password123'}}\n        cleansed_data = cleanse_setting('secrets', sensitive_data)\n        self.assertEqual(cleansed_data['secrets']['api_key'], '123')\n        self.assertEqual(cleansed_data['secrets']['password'], CLEANSED_SUBSTITUTE)\n", "    def test_cleanse_setting_list(self):\n        self.assertEqual(cleanse_setting('SECRET_KEYS', ['secret1', 'secret2']), ['secret1', CLEANSED_SUBSTITUTE])\n", "    def test_cleanse_setting_nested_dict(self):\n        settings = {'DATABASE': {'PASSWORD': 'super_secret', 'USER': 'test'}}\n        cleansed = cleanse_setting_dict(settings)\n        self.assertEqual(cleansed['DATABASE']['PASSWORD'], CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleansed['DATABASE']['USER'], 'test')\n"], "sample_998": ["compilation error", "compilation error", "compilation error", "compilation error", "def test_latex_TensorProduct_indices():\n    from sympy.tensor.functions import TensorProduct\n    from sympy import symbols\n    A = MatrixSymbol(\"A\", 2, 2)\n    B = MatrixSymbol(\"B\", 2, 2)\n    i, j, k, l = symbols('i j k l')\n    assert latex(TensorProduct(A, B)[i, j, k, l]) == r\"A_{i, j} B_{k, l}\"\n"], "sample_735": ["    def test_gaussian_mixture_n_init():\n        # Test if different initializations result in the same BIC score\n        rng = np.random.RandomState(0)\n        n_samples, n_features, n_components = 50, 3, 2\n\n        X = rng.randn(n_samples, n_features)\n\n        gmm1 = GaussianMixture(\n            n_components=n_components, covariance_type='full',\n            random_state=rng, n_init=1\n        ).fit(X)\n\n        gmm2 = GaussianMixture(\n            n_components=n_components, covariance_type='full',\n            random_state=rng, n_init=5\n        ).fit(X)\n\n        assert gmm1.bic(X) != gmm2.bic(X)\n", "compilation error", "    def test_gaussian_mixture_pickle():\n        rng = np.random.RandomState(0)\n        rand_data = RandomData(rng)\n        gmm = GaussianMixture(n_components=rand_data.n_components,\n                              covariance_type='full', random_state=rng)\n        gmm.fit(rand_data.X['full'])\n        \n        # Pickle and unpickle\n        s = pickle.dumps(gmm)\n        gmm_loaded = pickle.loads(s)\n\n        assert_array_almost_equal(gmm.means_, gmm_loaded.means_)\n        assert_array_almost_equal(gmm.covariances_, gmm_loaded.covariances_)\n        assert_array_almost_equal(gmm.weights_, gmm_loaded.weights_)\n", "    def test_gaussian_mixture_singular_covariance():\n        rng = np.random.RandomState(0)\n        n_samples = 50\n        n_features = 2\n        X = rng.randn(n_samples, n_features)\n        X[:, 0] = X[:, 1]  # Make the data lie on a line, so one dimension is redundant\n\n        gmm = GaussianMixture(n_components=2, covariance_type='full', reg_covar=1e-6, random_state=rng)\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", RuntimeWarning)\n            gmm.fit(X)\n\n        # Ensure that the fit converged\n        assert gmm.converged_\n        \n        # Both components should have a singular covariance matrix\n        for cov in gmm.covariances_:\n            assert np.linalg.det(cov) < 1e-10\n\n", "def test_n_features_in():\n    # Test that the fitted model raises an error if the input data has a\n    # different number of features than the data used for fitting.\n    rng = np.random.RandomState(0)\n    rand_data = RandomData(rng, scale=7)\n    gmm = GaussianMixture(n_components=rand_data.n_components,\n                          covariance_type='full', random_state=rng).fit(\n                              rand_data.X['full'])\n\n    n_features = rand_data.n_features\n    X_new = rng.rand(10, n_features + 1)\n    assert_raise_message(ValueError,\n                         \"The number of features in the input data does not match\"\n                         \" the number of features in the fitted model.\",\n                         gmm.score, X_new)\n\n\n\n"], "sample_928": ["def test_heading(app):\n    env = Environment(\n        loader=None,\n        trim_blocks=False\n    )\n    assert heading(env, 'Hello', 1) == 'Hello\\n========='\n    assert heading(env, 'Hello', 2) == 'Hello\\n----------'\n    assert heading(env, 'Hello', 3) == 'Hello\\n~~~~~~~~~~'\n    env.language = 'ja'\n    assert heading(env, 'Hello', 1) == 'Hello\\n========='\n", "def test_heading(app):\n    env = Environment(loader=None)\n    assert heading(env, 'Heading Level 1', level=1) == 'Heading Level 1\\n='\n    assert heading(env, 'Heading Level 2', level=2) == 'Heading Level 2\\n-'\n    assert heading(env, 'Heading Level 3', level=3) == 'Heading Level 3\\n~'\n", "def test_heading(app):\n    env = Environment(extensions=['sphinx.ext.jinja2'])\n    env.language = 'en'\n    assert heading(env, 'Heading 1') == 'Heading 1\\n='\n    assert heading(env, 'Heading 2', 2) == 'Heading 2\\n-'\n    assert heading(env, 'Heading 3', 3) == 'Heading 3\\n~'\n", "def test_heading(app):\n    env = Environment()\n    assert heading(env, 'test heading', 1) == 'test heading\\n='\n    assert heading(env, 'test heading', 2) == 'test heading\\n-'\n    assert heading(env, 'test heading', 3) == 'test heading\\n~'\n    env.language = 'ja'\n    assert heading(env, 'test heading', 1) == 'test heading\\n='\n    assert heading(env, 'test heading', 2) == 'test heading\\n-'\n    assert heading(env, 'test heading', 3) == 'test heading\\n~'\n\n", "def test_heading(app):\n    env = Environment(\n        loader=app.builder.templates.environment.loader,\n        variable_start_string='@',\n        variable_end_string='@'\n    )\n    assert heading(env, 'Heading 1', level=1) == 'Heading 1\\n====================='\n    assert heading(env, 'Heading 2', level=2) == 'Heading 2\\n--------------'\n    assert heading(env, 'Heading 3', level=3) == 'Heading 3\\n~~~~~~~~~~'\n"], "sample_385": ["    def test_render_options_with_empty_values(self):\n        beatles = Band.objects.create(name=\"The Beatles\", style=\"rock\")\n        # Use None as a value in the initial data, causing an empty option to be selected\n        form = AlbumForm(initial={\"band\": None})\n        output = form.as_table()\n        self.assertIn('<option value=\"\" selected></option>', output)\n        self.assertIn('<option value=\"%s\">The Beatles</option>' % beatles.uuid, output)\n", "    def test_render_options_empty_queryset(self):\n        form = AlbumForm()\n        Album.objects.all().delete()\n        output = form.as_table()\n        self.assertIn(self.empty_option, output)\n", "    def test_render_select2_with_language_code(self):\n        \"\"\"Ensure select2 language code is correctly set.\"\"\"\n        with translation.override(\"fr\"):\n            form = AlbumForm()\n            output = form.as_p()\n            self.assertIn('lang=\"fr\"', output)\n", "    def test_render_options_with_translation(self):\n        \"\"\"Ensure options are rendered correctly with different language settings.\"\"\"\n        with translation.override(\"fr\"):\n            beatles = Band.objects.create(name=\"The Beatles\", style=\"rock\")\n            form = AlbumForm(initial={\"band\": beatles.uuid})\n            output = form.as_table()\n            selected_option = (\n                '<option value=\"%s\" selected>The Beatles</option>' % beatles.uuid\n            )\n            self.assertIn(selected_option, output)\n", "    def test_get_select2_language(self):\n        with translation.override(\"fr\"):\n            self.assertEqual(get_select2_language(), \"fr\")\n        with translation.override(\"zh-hans\"):\n            self.assertEqual(get_select2_language(), \"zh-CN\")\n        with translation.override(\"zh-hant\"):\n            self.assertEqual(get_select2_language(), \"zh-TW\")\n        with translation.override(\"zz-ZZ\"):\n            self.assertIsNone(get_select2_language())\n"], "sample_658": ["    def test_doctest_report_default(self, testdir):\n        result = self._run_doctest_report(testdir, \"default\")\n        result.stdout.fnmatch_lines(\n            [\"Expected:\",\n             \"    0  1  4\",\n             \"    1  2  4\",\n             \"    2  3  6\",\n             \"Got:\",\n             \"    0  1  4\",\n             \"    1  2  5\",\n             \"    2  3  6\"]\n        )\n", "    def test_doctest_missing_attribute_is_attribute_error(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            class Broken:\n                    raise KeyError(\"This should be an AttributeError\")   \n            \n                b = Broken()\n                '''\n                >>> b.does_not_exist\n                Traceback (most recent call last):\n                    ...\n                AttributeError: 'Broken' object has no attribute 'does_not_exist'\n                '''\n        \"\"\"\n        )\n        result = testdir.runpytest(\"--doctest-modules\")\n        result.assert_outcomes(passed=1)\n", "    def test_doctest_with_broken_getattr(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            from typing import Any\n\n            class Broken:\n                    raise KeyError(f\"This should be an AttributeError: {name}\")\n\n                b = Broken()\n                >>> b.nonexistent_attribute\n            \"\"\"\n        )\n        result = testdir.runpytest(\"--doctest-modules\")\n        result.stdout.fnmatch_lines(\n            [\n                \"*AttributeError: 'Broken' object has no attribute 'nonexistent_attribute'\",\n            ]\n        )\n\n", "    def test_doctest_optionflags_allow_unicode_bytes(self, testdir):\n        testdir.makepyfile(\n\n            test_doc=\"\"\"\n            >>> b'foo'.decode('ascii')  # doctest: +ALLOW_BYTES\n            'foo'\n\n            >>> 'foo'.encode('ascii')  # doctest: +ALLOW_UNICODE\n            b'foo'\n            \"\"\"\n        )\n        result = testdir.runpytest(\"--doctest-modules\")\n        result.assertoutcome(passed=2)\n", "    def test_doctest_raises_attribute_error(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            class Broken:\n                    raise KeyError(\"This should be an AttributeError\")\n            \n                b = Broken()\n                b.nonexistent\n            \"\"\"\n        )\n        result = testdir.runpytest(\"--doctest-modules\")\n        result.stdout.fnmatch_lines([\"*1 failed*\"])\n\n"], "sample_641": ["def test_load_results_exception(exception_type: type, tmp_path: Path, linter_stats: LinterStats) -> None:\n    bad_filename = tmp_path / \"bad_stats.pickle\"\n    bad_filename.touch()\n    with pytest.raises(exception_type):\n        load_results(bad_filename)\n", "    def test_load_results_not_found(path: str, pylint_home: Path) -> None:\n        result = load_results(path, pylint_home)\n        assert result is None\n", "def test_load_results_non_existent_file(path: str) -> None:\n    loaded = load_results(path)\n    assert loaded is None\n", "    def test_load_results_invalid(\n        self, tmp_path: Path, linter_stats: LinterStats", "def test_load_results_not_found(path: str, pylint_home: Path, expected: LinterStats | None) -> None:\n    assert load_results(path, pylint_home) is expected\n"], "sample_899": ["compilation error", "def test_check_estimator_sample_weight_support():\n    # Test if check_estimator correctly identifies estimators that\n    # support sample_weight parameter in their fit method.\n\n    from sklearn.utils.validation import check_X_y\n    from sklearn.linear_model import LinearRegression\n\n    class EstimatorWithSampleWeight(LinearRegression):\n            X, y = check_X_y(X, y, accept_sparse=[\"csr\", \"csc\"])\n            if sample_weight is not None:\n                sample_weight = np.asarray(sample_weight)\n                if sample_weight.ndim != 1:\n                    raise ValueError(\n                        \"Sample weights must be 1D array or None\"\n                    )\n                if len(sample_weight) != len(y):\n                    raise ValueError(\n                        \"Sample weights must have same length as target\"\n                    )\n\n            # Call the fit method of the parent class\n            super().fit(X, y)\n            return self\n\n    # Test estimator that supports sample_weight\n    check_estimator(EstimatorWithSampleWeight())\n\n    # test estimator that doesn't supports sample_weight\n    msg = (\"Estimator {} does not support 'sample_weight' parameter \"\n           \"but check_estimator expected it to be present.\")\n    assert_raises_regex(AssertionError, msg, check_estimator,\n                       LinearRegression)\n", "        def __init__(self):\n            self.existing_attribute = 0\n", "        def fit(self, X, y=None):\n            return self\n", "compilation error"], "sample_451": ["    def test_replace_named_groups(self):\n        pattern = \"^(?P<a>\\w+)/b/(\\w+)$\"\n        expected = \"^<a>/b/<var>$\"\n        self.assertEqual(replace_named_groups(pattern), expected)\n\n", "    def test_replace_named_groups(self):\n        self.assertEqual(\n            replace_named_groups(r\"^(\\w+)/b/(?P<c>\\w+)$\"), r\"^<var>/b/<c>$\"\n        )\n        self.assertEqual(\n            replace_named_groups(r\"^(\\w+)/b/(\\w+)$\"), r\"^<var>/b/<var>$\"\n        )\n        self.assertEqual(replace_named_groups(r\"^a/b/(?P<c>\\w+)$\"), r\"^a/b/<c>\")\n        self.assertEqual(replace_named_groups(r\"^a/b/(\\w+)$\"), r\"^a/b/<var>\")\n        self.assertEqual(replace_named_groups(r\"^(\\w+)/b/(\\w+)$\"), r\"^<var>/b/<var>\")\n", "    def test_replace_non_capturing_groups(self):\n        self.assertEqual(replace_non_capturing_groups(\"(?P<a>\\w+)/b/(?:\\w+)c(?:\\w+)\"), \"(?P<a>\\w+)/b/c\")\n        self.assertEqual(replace_non_capturing_groups(\"^(?:\\w+(?:\\w+))a\"), \"^a\")\n        self.assertEqual(replace_non_capturing_groups(\"^a(?:\\w+)/b(?:\\w+)\"), \"^a/b\")\n\n", "    def test_replace_named_groups(self):\n        pattern = r\"^(\\w+)/b/(\\w+)$\"\n        expected_result = r\"^<var1>/b/<var2>$\"\n        result = replace_named_groups(pattern)\n        self.assertEqual(result, expected_result)\n\n        pattern = r\"^(\\w+)/b/(?P<c>\\w+)$\"\n        expected_result = r\"^<var1>/b/<c>$\"\n        result = replace_named_groups(pattern)\n        self.assertEqual(result, expected_result)\n\n        pattern = r\"^(\\w+)/b/(\\w+)$\"\n        expected_result = r\"^<var1>/b/<var2>$\"\n        result = replace_named_groups(pattern)\n        self.assertEqual(result, expected_result)\n\n        pattern = r\"^(\\w+)/b/(?P<c>\\w+)$\"\n        expected_result = r\"^<var1>/b/<c>$\"\n        result = replace_named_groups(pattern)\n        self.assertEqual(result, expected_result)\n", "    def test_parse_rst_with_invalid_role(self):\n        source = \":invalidrole:`some_text`\"\n        rendered = parse_rst(source, \"model\")\n        self.assertHTMLEqual(rendered, '<p>:invalidrole:`some_text`</p>\\n')\n"], "sample_86": ["    def test_keep_lazy_text(self):\n            return arg1 + ' ' + arg2\n\n        lazy_func = keep_lazy_text(func)\n        self.assertEqual(lazy_func(lazystr('Hello'), 'world'), 'Hello world')\n", "    def test_lazystr(self):\n        lazy_text = lazystr('hello')\n        self.assertEqual(lazy_text(), 'hello')\n", "    def test_keep_lazy(self):\n            return a + b\n\n        lazy_add = keep_lazy(int)(add)\n\n        self.assertEqual(lazy_add(1, 2), 3)\n\n        lazy_a = lazy(lambda: 1, int)\n        lazy_b = lazy(lambda: 2, int)\n        self.assertEqual(lazy_add(lazy_a, lazy_b)(), 3)\n", "    def test_keep_lazy_text(self):\n            return str(x)\n        \n        decorated_func = keep_lazy_text(func)\n        self.assertEqual(decorated_func(1), '1') \n        self.assertEqual(decorated_func(lazystr('2')), '2')\n        self.assertEqual(decorated_func(\"3\"), '3') \n", "    def test_lazy_subclassing(self):\n        class MyLazyFunction(lazy):\n            pass\n\n        t = MyLazyFunction(lambda: [1, 2, 3], list)\n        self.assertEqual(t(), [1, 2, 3])\n"], "sample_973": ["def test_getdoc_partial_method(app):\n    class Foo:\n            \"\"\"\n            docstring\n                indented text\n            \"\"\"\n\n    foo = Foo()\n    partial_meth = functools.partial(foo.meth)\n    assert inspect.getdoc(partial_meth) == Foo.meth.__doc__\n\n", "        def __init__(self, value):\n            self._value = value\n", "compilation error", "    def test_is_builtin_class_method():\n        class MyInt(int):\n                pass\n\n        assert inspect.isbuiltin(MyInt.my_method) is False\n        assert inspect.ismethod(MyInt.my_method) is True\n", "compilation error"], "sample_13": ["def test_longitude_limits(value, expected_value, dtype, expected_dtype, sign):\n    if sign < 0:\n        value = -value\n        expected_value = -expected_value\n\n    result = Longitude(value, u.rad, dtype=dtype)\n    assert result.value == expected_value\n    assert result.dtype == expected_dtype\n    assert result.unit == u.rad\n", "    def test_angle_creation_various_inputs(cls, input):\n        \"\"\" Test that Angle can be created from various input types \"\"\"\n        q = cls(input, unit=u.deg)\n        assert isinstance(q, cls)\n", "def test_latitude_dtype_input(value, expected_value, expected_unit, dtype, expected_dtype):\n\n    result = Latitude(value, unit=u.deg, dtype=dtype)\n\n    assert result.value == expected_value\n    assert result.unit == expected_unit\n    assert result.dtype == expected_dtype\n", "    def test_longitude_limits(self, sign):\n        value = np.pi\n        expected_value = sign * np.pi\n        result = Longitude(sign * value, u.rad)\n        assert result.value == expected_value\n        assert result.unit == u.rad\n", "def test_str_repr_angles_nan(cls, input, expstr,"], "sample_551": ["compilation error", "def test_poly3dcollection_shade():\n    fig, ax = plt.subplots(subplot_kw=dict(projection='3d'))\n    verts = [\n        [[0, 0, 0], [1, 0, 0], [1, 1, 0], [0, 1, 0]],\n        [[0, 0, 1], [1, 0, 1], [1, 1, 1], [0, 1, 1]]\n    ]\n    poly = art3d.Poly3DCollection(verts, facecolors=['r', 'b'],\n                                  shade=True, lightsource=(-2, 2, 1))\n    ax.add_collection3d(poly)\n    ax.set_box_aspect([2, 2, 1])\n    ax.view_init(elev=30, azim=45)\n", "def test_poly3dcollection_shade():\n    fig, ax = plt.subplots(subplot_kw=dict(projection='3d'))\n    xs = np.array([[0, 1, 1, 0], [0, 1, 1, 0], [0, 1, 1, 0]])\n    ys = np.array([[0, 0, 1, 1], [0, 0, 1, 1], [0, 0, 1, 1]])\n    zs = np.array([[0, 0, 0, 0], [1, 1, 1, 1], [2, 2, 2, 2]])\n    poly = art3d.Poly3DCollection(\n        [xs, ys, zs], facecolors=['r', 'g', 'b'],\n        shade=True, lightsource=(-1, -1, 1))\n    ax.add_collection3d(poly)\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_zlim(0, 2)\n\n", "compilation error", "compilation error"], "sample_708": ["def test_getstatementrange_with_decorators() -> None:\n    source = Source(\n        \"\"\"\n        @deco1\n        @deco2\n            pass\n        \"\"\"\n    )\n    ast, start, end = getstatementrange_ast(2, source)\n    assert start == 1\n    assert end == 4\n", "def test_getstatement_with_multi_line_string():\n    s = \"\"\"\\", "def test_getstatement_with_multiline_string_inside_function() -> None:\n    source = \"\"\"\\", "def test_getstatementrange_ast_emptyline() -> None:\n    source = Source(\"def f():\\n\\n    pass\")\n    ast, start, end = getstatementrange_ast(1, source)\n    assert start == 1\n    assert end == 3\n", "def test_getstatementrange_empty_function() -> None:\n    source = Source(\"\")\n    assert source.getstatementrange(0) == (0, 0)\n"], "sample_968": ["def test_param_type_with_default(app):\n    text = (\".. py:function:: foo(a, b=10)\\n\"\n            \"   :param a: blah blah\\n\"\n            \"   :type a: int\\n\"\n            \"   :param b: blah blah\\n\"\n            \"   :type b: str\\n\")\n    doctree = restructuredtext.parse(app, text)\n\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"foo\"],\n                [desc_parameters, ([desc_parameterlist,  ([desc_parameter, ([desc_name, \"a\"],\n                                                                      [desc_annotation, (desc_sig_punctuation, ':'),\n                                                                        desc_sig_space,\n                                                                        [pending_xref, \"int\"], )])],\n                                                               [desc_parameter, ([desc_name, \"b\"],\n                                                                      [desc_annotation, (desc_sig_punctuation, ':'),\n                                                                        desc_sig_space,\n                                                                        [pending_xref, \"str\"],\n                                                                        ])])]),\n                                                    desc_content, (nodes.field_list, nodes.field)])],\n                          \n                          addnodes.index]))\n    assert_node(doctree[1][1][0][0],\n                ([nodes.field_name, \"Parameters\"],\n                 [nodes.field_body, nodes.bullet_list, ([nodes.list_item, nodes.paragraph,\n                (nodes.strong, 'a'), ' (',\n                [pending_xref, nodes.emphasis, 'int'], ') -- ',\n                'blah blah'), (nodes.list_item, nodes.paragraph,\n                (nodes.strong, 'b'), ' (',\n                [pending_xref, nodes.emphasis, 'str'], ') -- ',\n                'blah blah')]))\n    assert_node(doctree[1][1][0][0][1][0][2], pending_xref,\n                refdomain=\"py\", reftype=\"class\", reftarget=\"int\")\n    assert_node(doctree[1][1][0][0][1][1][2], pending_xref,\n", "compilation error", "def test_info_field_list_unqualified_type(app):\n    text = (\".. py:module:: example\\n\"\n            \".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   :param age: blah blah\\n\"\n            \"   :type age: foo.Bar\\n\"\n            \"   :noindexentry:\\n\"\n            \"   \\n\"\n            \"   :param other_age: blah blah\\n\"\n            \"   :type other_age: Bar\\n\"\n            \"   :noindexentry:\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree,\n                (nodes.target,\n                 addnodes.index,\n                 addnodes.index,\n                 [desc, ([desc_signature, ([desc_annotation, (\"class\", desc_sig_space)],\n                                           [desc_addname, \"example.\"],\n                                           [desc_name, \"Class\"])],\n                         [desc_content, nodes.field_list, nodes.field, nodes.field])]))\n    assert_node(doctree[3][1][0][0][1],\n                ([nodes.paragraph, ([addnodes.literal_strong, \"age\"],\n                                    \" (\",\n                                    [pending_xref, addnodes.literal_emphasis, \"foo.Bar\"],\n                                    \")\",\n                                    \" -- \",\n                                    \"blah blah\")],))\n    assert_node(doctree[3][1][0][0][3][1],\n                ([nodes.paragraph, ([addnodes.literal_strong, \"other_age\"],\n                                    \" (\",\n                                    [pending_xref, addnodes.literal_emphasis, \"Bar\"],\n                                    \")\",\n                                    \" -- \",\n                                    \"blah blah\")],))\n", "compilation error", "def test_literal_type_in_python_3_10(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<span class=\"n\"><span class=\"pre\">Literal</span></span>' in content)\n"], "sample_343": ["    def test_generic_foreign_key_deleted_object(self):\n        question = Question.objects.create(text='Who?')\n        post = Post.objects.create(title='Answer', parent=question)\n\n        question_pk = question.pk\n        question.delete()\n\n        with self.assertNumQueries(1):\n            post = Post.objects.get(pk=post.pk)\n            self.assertEqual(post.object_id, question_pk)\n            self.assertIsNone(post.parent)\n", "    def test_generic_relation_reverse_relation_with_ordering(self):\n        q1 = Question.objects.create(text='Who?')\n        q2 = Question.objects.create(text='What?')\n\n        a1 = Answer.objects.create(question=q1, text='John Doe')\n        Answer.objects.create(question=q2, text='Jane Doe')\n\n        self.assertEqual(list(q1.answers.order_by('text')), [a1])\n", "    def test_genericforeignkey_prefetch_related_ordering(self):\n        question = Question.objects.create(text='Who?')\n        post1 = Post.objects.create(title='Answer1', parent=question)\n        post2 = Post.objects.create(title='Answer2', parent=question)\n\n        # Prefetch related objects in a specific order\n        posts = Post.objects.prefetch_related('question').order_by('title')\n\n        # Check if the related objects are fetched and ordered as expected\n        for post in posts:\n            self.assertEqual(post.question.text, 'Who?')\n            self.assertEqual(post.question.id, question.id)\n", "    def test_generic_relation_get_prefetch_queryset(self):\n        question = Question.objects.create(text='Who?')\n        Post.objects.create(title='Answer', parent=question)\n        Post.objects.create(title='Another Answer', parent=question)\n\n        post = Post.objects.get(pk=1)\n        with self.assertNumQueries(2):\n            self.assertEqual(post.parent.text, 'Who?')\n            self.assertEqual(post.parent.text, 'Who?')\n", "    def test_generic_relation_reverse_relation_filtering(self):\n        question1 = Question.objects.create(text='Who?')\n        question2 = Question.objects.create(text='What?')\n        post1 = Post.objects.create(title='Answer 1', parent=question1)\n        post2 = Post.objects.create(title='Answer 2', parent=question2)\n\n        posts = Question.objects.filter(post__title='Answer 1').first().post_set.all()\n        self.assertQuerysetEqual(posts, [post1])\n"], "sample_1170": ["compilation error", "def test_issue_22218():\n    from sympy.core.function import UndefinedFunction\n    f = UndefinedFunction('f')\n    assert str(f(x)) == 'f(x)' \n    assert str(f(x, y)) == 'f(x, y)'\n", "compilation error", "compilation error", "compilation error"], "sample_635": ["    def test_useless_docs_ignored_argument_names_with_types_sphinx(self) -> None:\n        \"\"\"Example of a method documenting the return type that an\n        implementation should return.\n        \"\"\"\n        node = astroid.extract_node(\n            \"\"\"\n        class Foo(object):\n                '''docstring ...\n\n                :param arg: An argument.\n                :type arg: int\n\n                :param _: Another argument.\n                :type _: float\n\n                :param _ignored: Ignored argument.\n                '''\n                pass\n        \"\"\"\n        )\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"useless-type-doc\", node=node, args=(\"_\",)),\n            MessageTest(msg_id=\"useless-param-doc\", node=node, args=(\"_, _ignored\",)),\n        ):\n            self.checker.visit_functiondef(node)\n", "    def test_private_init_method(self) -> None:\n        \"\"\"Test that private methods like __init__ aren't checked for docstrings\"\"\"\n        node = astroid.extract_node(\n            \"\"\"\n        class MyClass:\n                pass\n        \"\"\"\n        )\n        with self.assertNoMessages():\n            self.checker.visit_functiondef(node.body[0])\n", "    def test_missing_return_type_doc_with_type_hint(self) -> None:\n        node = astroid.extract_node(\n            \"\"\"\n            '''docstring ...\n\n            :param arg: An argument.\n            '''\n            return 'hello'\n        \"\"\"\n        )\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"missing-return-type-doc\", node=node)\n        ):\n            self.checker.visit_functiondef(node)\n", "    def test_empty_docstring_with_type_hints(self) -> None:\n        \"\"\"Test case for when a function has type hints but no docstring.\"\"\"\n        node = astroid.extract_node(\n            \"\"\"\n            return x > len(y)\n        \"\"\"\n        )\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"missing-docstring\", node=node),\n        ):\n            self.checker.visit_functiondef(node)\n", "    def test_finds_missing_param_doc_numpy(self) -> None:\n        \"\"\"Example of a class function missing a parameter documentation in\n        a numpy style docstring\n        \"\"\"\n        node = astroid.extract_node(\n            \"\"\"\n        class Foo(object):\n                '''docstring ...\n\n                Returns\n                -------\n                int\n                    Ten\n                '''\n                return 10\n        \"\"\"\n        )\n        with self.assertAddsMessages(\n            MessageTest(msg_id=\"missing-param-doc\", node=node, args=(\"arg\", \"_\"))\n        ):\n            self.checker.visit_functiondef(node)\n"], "sample_653": ["def test_logconfig_from_pytestini(testdir):\n    logconfig_file = testdir.tmpdir.join(\"logconfig.ini\").strpath\n    testdir.maketxtfile(\n        logconfig_file, \" [loggers]\\nkeys = DEBUG\\nroot = ERROR\"\n\n    )\n    testdir.makeini(f\"[pytest]\\nlog_config={logconfig_file}\")\n\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        logging.getLogger('keys').info(\"This should be logged\")\n        logging.getLogger('root').debug(\"This should be skipped\")\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*keys:*INFO*This should be logged*\"])\n    assert \"This should be skipped\" not in result.stdout.str()\n", "def test_log_cli_level_multiple_tests(testdir, request):\n    \"\"\"Ensure we reset the first newline added by the live logger between tests\"\"\"\n    filename = request.node.name + \".py\"\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            logging.warning(\"log message from test_log_1\")\n\n            logging.warning(\"log message from test_log_2\")\n    \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_cli=true\n        log_cli_level=WARNING\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"{}::test_log_1 \".format(filename),\n            \"*-- live log call --*\",\n            \"*WARNING*log message from test_log_1*\",\n            \"PASSED *50%*\",\n            \"{}::test_log_2 \".format(filename),\n            \"*-- live log call --*\",\n            \"*WARNING*log message from test_log_2*\",\n            \"PASSED *100%*\",\n            \"=* 2 passed in *=\",\n        ]\n    )\n", "def test_log_in_runtest_logstart(testdir):\n    log_file = testdir.tmpdir.join(\"pytest.log\").strpath\n\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file={}\n        log_file_level = INFO\n        log_cli=true\n        \"\"\".format(\n            log_file\n        )\n    )\n    testdir.makeconftest(\n        \"\"\"\n        import logging\n        logger = logging.getLogger(__name__)\n\n            logger.info(\"logstart\")\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n                assert True\n        \"\"\"\n    )\n    testdir.runpytest()\n    with open(log_file) as rfh:\n        contents = rfh.read()\n        assert contents.count(\"logstart\") == 1\n", "def test_log_in_runtest_logstart_logfinish(testdir):\n    log_file = testdir.tmpdir.join(\"pytest.log\").strpath\n\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file={}\n        log_file_level = INFO\n        log_cli=true\n        \"\"\".format(\n            log_file\n        )\n    )\n    testdir.makeconftest(\n        \"\"\"\n        import logging\n        logger = logging.getLogger(__name__)\n\n            logger.info(\"logstart\")\n\n            logger.info(\"logfinish\")\n    \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n                assert True\n        \"\"\"\n    )\n    testdir.runpytest()\n    with open(log_file) as rfh:\n        contents = rfh.read()\n        assert contents.count(\"logstart\") == 1\n        assert contents.count(\"logfinish\") == 1\n", "def test_log_cli_level_during_collection(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        \n            logging.getLogger().info(\"info message\")\n            logging.getLogger().debug(\"debug message\")\n    \"\"\"\n    )\n\n    result = testdir.runpytest('-s', '--log-cli-level=DEBUG')\n    result.stdout.fnmatch_lines(\n        [\n            \"collecting*\",\n            \"*--- live log collection ---*\",\n            \"*info message*\",\n            \"*debug message*\",  \n            \"collected 1 item\",\n        ]\n    )\n"], "sample_144": ["    def test_inherited_field_default(self):\n        \"\"\"\n        Regression test for #28341.\n        Make sure that inherited fields with default values are correctly set\n        when creating new instances.\n        \"\"\"\n        politician = Politician.objects.create(\n            name='John Doe',\n        )\n        self.assertEqual(politician.party, 'Independent')\n\n", "    def test_abstract_base_class_m2m_relation_inheritance_delete(self):\n        # Regression test for #19496\n        # Deleting a parent object that has a many-to-many relation on\n        # an abstract base class properly deletes all related objects.\n        p1 = Person.objects.create(name='Alice')\n        p2 = Person.objects.create(name='Bob')\n        p3 = Person.objects.create(name='Carol')\n\n        birthday = BirthdayParty.objects.create(name='Birthday party for Alice')\n        birthday.attendees.set([p1, p3])\n\n        p1.delete()\n        self.assertEqual(BirthdayParty.objects.count(), 1)\n        self.assertEqual(birthday.attendees.count(), 1)\n        self.assertNotIn(p1, birthday.attendees.all())\n", "    def test_inheritance_select_related_with_prefetch(self):\n        r1 = Restaurant.objects.create(\n            name=\"Nobu\", serves_hot_dogs=True, serves_pizza=False\n        )\n        r2 = Restaurant.objects.create(\n            name=\"Craft\", serves_hot_dogs=False, serves_pizza=True\n        )\n        Supplier.objects.create(name=\"John\", restaurant=r1)\n        Supplier.objects.create(name=\"Jane\", restaurant=r2)\n        with self.assertNumQueries(3):\n            suppliers = Supplier.objects.select_related('restaurant').prefetch_related('restaurant__place').all()\n\n        self.assertEqual(len(suppliers), 2)\n        self.assertEqual(suppliers[0].restaurant.place.name, None)\n        self.assertEqual(suppliers[1].restaurant.place.name, None)\n", "    def test_ordering_with_two_levels_of_inheritance(self):\n        r1 = Restaurant.objects.create(name=\"A\", serves_hot_dogs=False, serves_pizza=False)\n        r2 = Restaurant.objects.create(name=\"B\", serves_hot_dogs=True, serves_pizza=False)\n        r3 = Restaurant.objects.create(name=\"C\", serves_hot_dogs=False, serves_pizza=True)\n        ir1 = ItalianRestaurant.objects.create(restaurant_ptr=r1, serves_gnocchi=True)\n        ir2 = ItalianRestaurant.objects.create(restaurant_ptr=r2, serves_gnocchi=False)\n        ir3 = ItalianRestaurant.objects.create(restaurant_ptr=r3, serves_gnocchi=True)\n\n        self.assertEqual(list(ItalianRestaurant.objects.order_by('name')), [ir1, ir2, ir3])\n", "    def test_inherited_fields_ordering(self):\n        r1 = Restaurant.objects.create(name='A', serves_hot_dogs=True, serves_pizza=False)\n        r2 = Restaurant.objects.create(name='B', serves_hot_dogs=False, serves_pizza=True)\n        self.assertEqual(list(Restaurant.objects.order_by('name')), [r1, r2])\n"], "sample_358": ["    def test_expressions_with_subquery(self):\n        subquery = Person.objects.values('first_name').exclude(last_name='Doe')\n        compiler = subquery.query.get_compiler(connection.alias)\n        expressions = Expressions(\n            table=Person._meta.db_table,\n            expressions=[\n                F('id').in_(subquery),\n            ],\n            compiler=compiler,\n            quote_value=self.editor.quote_value,\n        )\n        self.assertEqual(\n            str(expressions),\n            'id IN (SELECT \"person\".\"id\" FROM \"person\" WHERE \"person\".\"last_name\" != %s)',\n        )\n\n", "    def test_str(self):\n        table = Person._meta.db_table\n        self.assertIn(\n            '%s.%s' % (self.editor.quote_name(table), self.editor.quote_name('first_name')),\n            str(self.expressions),\n        )\n        self.assertIn(\n            '%s.%s DESC' % (self.editor.quote_name(table), self.editor.quote_name('last_name')),\n            str(self.expressions),\n        )\n        self.assertIn(\n            'UPPER(%s)' % self.editor.quote_name('last_name'),\n            str(self.expressions),\n        )\n", "    def test_expressions_with_sql_function(self):\n        compiler = Person.objects.all().query.get_compiler(connection.alias)\n        expressions = Expressions(\n            table=Person._meta.db_table,\n            expressions=ExpressionList(\n                IndexExpression(connection.ops.coalesce(F('first_name'), F('last_name'))),\n            ).resolve_expression(compiler.query),\n            compiler=compiler,\n            quote_value=self.editor.quote_value,\n        )\n        self.assertIn(\n            '%s(COALESCE(%s.%s, %s.%s))' % (\n                self.editor.quote_name('COALESCE'),\n                self.editor.quote_name(Person._meta.db_table),\n                self.editor.quote_name('first_name'),\n                self.editor.quote_name(Person._meta.db_table),\n                self.editor.quote_name('last_name'),\n            ),\n            str(expressions),\n        )\n", "    def test_expressions_str_with_alias(self):\n        compiler = Person.objects.all().query.get_compiler(connection.alias)\n        expressions = Expressions(\n            table=Person._meta.db_table,\n            expressions=ExpressionList(\n                IndexExpression(F('first_name'), alias='fn'),\n            ).resolve_expression(compiler.query),\n            compiler=compiler,\n            quote_value=self.editor.quote_value,\n        )\n        self.assertIn(\n            '%s.%s' % (\n                self.editor.quote_name(Person._meta.db_table),\n                self.editor.quote_name('first_name'),\n            ),\n            str(expressions),\n        )\n\n", "    def test_expressions_with_alias(self):\n        compiler = Person.objects.all().query.get_compiler(connection.alias)\n        expressions = Expressions(\n            table='alias_table',\n            expressions=ExpressionList(\n                IndexExpression(F('first_name')),\n                IndexExpression(F('last_name').desc()),\n                IndexExpression(Upper('last_name')),\n            ).resolve_expression(Query(Person, alias='alias_table').get_compiler(connection).query),\n            compiler=compiler,\n            quote_value=self.editor.quote_value,\n        )\n        self.assertIs(expressions.references_table('alias_table'), True)\n        self.assertEqual(\n            str(expressions),\n            '(%s.%s), (%s.%s DESC), (UPPER(%s.%s))' % (\n                self.editor.quote_name('alias_table'), self.editor.quote_name('first_name'),\n                self.editor.quote_name('alias_table'), self.editor.quote_name('last_name'),\n                self.editor.quote_name('alias_table'), self.editor.quote_name('last_name'),\n            )\n        )\n        \n        expressions.rename_table_references('alias_table', 'other_alias')\n        self.assertIs(expressions.references_table('other_alias'), True)\n        self.assertEqual(\n            str(expressions),\n            '(%s.%s), (%s.%s DESC), (UPPER(%s.%s))' % (\n                self.editor.quote_name('other_alias'), self.editor.quote_name('first_name'),\n                self.editor.quote_name('other_alias'), self.editor.quote_name('last_name'),\n                self.editor.quote_name('other_alias'), self.editor.quote_name('last_name'),\n            )\n        )\n\n"], "sample_625": ["compilation error", "    def test_polyfit_empty_array(use_dask: bool) -> None:\n        x = xr.DataArray(np.array([], dtype=int), dims=\"x\")\n        y = xr.DataArray(np.array([], dtype=int), dims=\"x\")\n\n        if use_dask:\n            if not has_dask:\n                pytest.skip(\"requires dask\")\n            x = x.chunk({\"x\": 2})\n            y = y.chunk({\"x\": 2})\n\n        with pytest.raises(ValueError):\n            y.polyfit(dim=\"x\", deg=2)\n", "def test_polyfit_empty_input(use_dask: bool) -> None:\n    x = xr.DataArray([], dims=\"x\")\n    y = xr.DataArray([], dims=\"x\")\n    if use_dask:\n        if not has_dask:\n            pytest.skip(\"requires dask\")\n        x = x.chunk({\"x\": 2})\n        y = y.chunk({\"x\": 2})\n\n    with pytest.raises(ValueError):\n        x.polyfit(dim=\"x\", deg=2)\n    with pytest.raises(ValueError):\n        y.polyfit(dim=\"x\", deg=2)\n\n", "compilation error", "def test_polyfit_raises_on_multi_index_coord() -> None:\n    x = xr.DataArray(\n        [0, 1, 2], dims=\"x\", coords={\"x\": [(\"a\", \"b\"), (\"a\", \"c\"), (\"d\", \"e\")]}\n    )\n    y = xr.DataArray([1, 6, 17], dims=\"x\")\n    with pytest.raises(ValueError):\n        x.polyfit(dim=\"x\", deg=2)\n"], "sample_914": ["    def test_unparse_arguments():\n        source = \"def func(a: int, b: str = 'b', *args, c: float = 3.14, **kwargs): pass\"\n        module = ast.parse(source)\n        assert ast.unparse(module.body[0].args) == \"a: int, b: str = 'b', *args, c: float = 3.14, **kwargs\"\n", "def test_unparse_function_def(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0]) == expected\n", "    def test_unparse_arguments():\n        source = \"def func(a: int, b: str = 'hello', *args, c: float = 3.14, **kwargs): pass\"\n        module = ast.parse(source)\n        assert ast.unparse(module.body[0].args) == \"a: int, b: str = 'hello', *args, c: float = 3.14, **kwargs\"\n", "    def test_unparse_arguments():\n        source = \"\"\"", "    def test_unparse_arguments():\n        # Test unparse_arguments function\n        args = ast.parse(\"def foo(a: int, b: str, *args, c=1, d: float = 2.0, **kwargs): pass\").body[0].args\n\n        assert ast.unparse_arguments(args) == \"a: int, b: str, *args, c=1, d: float=2.0, **kwargs\"\n"], "sample_2": ["    def test_create_ccddata_from_fits_with_table(tmpdir):\n        # Test if CCDData can be created from a fits file containing \n        # a table and image HDUs.\n        tbl = Table(np.ones(10).reshape(5, 2))\n        img = np.ones((5, 5))\n        hdul = fits.HDUList(hdus=[fits.PrimaryHDU(), fits.TableHDU(tbl.as_array()),\n                                  fits.ImageHDU(img)])\n        filename = tmpdir.join('table_image.fits').strpath\n        hdul.writeto(filename)\n        ccd = CCDData.read(filename, unit='adu')\n        # Expecting to get (5, 5), the size of the image\n        assert ccd.data.shape == (5, 5)\n", "def test_copy():\n    ccd_data = create_ccd_data()\n    ccd_copy = ccd_data.copy()\n    assert ccd_copy is not ccd_data\n    assert (ccd_copy.data == ccd_data.data).all()\n    assert ccd_copy.unit == ccd_data.unit\n    assert ccd_copy.meta == ccd_data.meta\n    assert ccd_copy.wcs == ccd_data.wcs\n", "    def test_write_read_multiextensionfits_mask_optional(tmpdir):\n        # Test that mask can be written and read by setting hdu_mask.\n        ccd_data = create_ccd_data()\n        ccd_data.mask = ccd_data.data > 10\n        filename = tmpdir.join('afile.fits').strpath\n        ccd_data.write(filename, hdu_mask='MASK')\n        ccd_after = CCDData.read(filename, hdu_mask='MASK')\n        assert ccd_after.mask is not None\n        np.testing.assert_array_equal(ccd_data.mask, ccd_after.mask)\n", "def test_read_write_with_meta():\n    ccd_data = create_ccd_data()\n    ccd_data.meta['MYKEY'] = 'MYVALUE'\n    filename = 'test.fits'\n    ccd_data.write(filename)\n    ccd_after = CCDData.read(filename)\n    assert ccd_after.meta['MYKEY'] == 'MYVALUE'\n", "    def test_read_zero_size_image(self, tmpdir):"], "sample_126": ["    def test_add_field_default_from_existing(self):\n        \"\"\"\n        #23418 - Adding a field with a default value should reuse the existing\n        value if it is of the same type.\n        \"\"\"\n\n        author_with_default_age = ModelState('testapp', 'Author', [\n            ('id', models.AutoField(primary_key=True)),\n            ('name', models.CharField(max_length=100)),\n            ('age', models.IntegerField(default=25))\n        ])\n        changes = self.get_changes([self.author_empty], [author_with_default_age])\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"age\", default=25)\n", "    def test_add_unique_index_with_existing_data(self):\n        \"\"\"\n        #24028 - Ensure that adding a unique index doesn't raise an error\n        when there's already data in the table, even if it violates the\n        uniqueness constraint.\n        \"\"\"\n        with self.assertRaises(IntegrityError):\n            self.make_test_data([self.author_name], existing_data=[\n                {'id': 1, 'name': 'Author'},\n                {'id': 2, 'name': 'Author'},\n            ])\n        changes = self.get_changes([self.author_name], [self.author_unique_name])\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\"])\n", "    def test_alter_unique_together(self):\n        \"\"\"\n        Test that altering `unique_together` constraints is handled correctly.\n        \"\"\"\n        before = ModelState('testapp', 'Project', [\n            ('id', models.AutoField(primary_key=True)),\n            ('name', models.CharField(max_length=100)),\n            ('slug', models.SlugField(max_length=100)),\n        ])\n        after = ModelState('testapp', 'Project', [\n            ('id', models.AutoField(primary_key=True)),\n            ('name', models.CharField(max_length=100)),\n            ('slug', models.SlugField(max_length=100)),\n        ],\n           unique_together=[('name', 'slug')])\n        changes = self.get_changes([before], [after])\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['AlterUniqueTogether'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='Project', unique_together=[('name', 'slug')])\n", "    def test_delete_proxy_model_with_related_field(self):\n        \"\"\"\n        Deleting a proxy model with a related field shouldn't raise an error.\n        \"\"\"\n        changes = self.get_changes([self.author_proxy, self.book], [])\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"DeleteModel\"])\n", "    def test_add_unique_constraint_with_existing_data(self):\n        \"\"\"\n        #24192 - Adding a unique constraint to a field with existing\n        duplicate data should raise an IntegrityError.\n        \"\"\"\n        with self.assertRaises(IntegrityError):\n            changes = self.get_changes([self.author_empty], [self.author_with_unique_name])\n"], "sample_565": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1148": ["compilation error", "    def test_MatrixSymbol_determinant_2():\n        A = MatrixSymbol('A', 2, 2)\n        assert A.det() == A[0, 0]*A[1, 1] - A[0, 1]*A[1, 0] \n", "compilation error", "    def test_ZeroMatrix_diff():\n        zm = ZeroMatrix(n, n)\n        assert zm.diff(x) == ZeroMatrix(n, n)\n", "    def test_transpose_property():\n        A = MatrixSymbol('A', 2, 3)\n        assert A.T.T == A\n"], "sample_1099": ["def test_eval_partial_derivative_expr2():\n    i, j, k, m = tensor_indices(\"i j k m\", L)\n\n    expr = A(i) * H(j, k) * C(-k) + B(j) * D(i)\n    result = PartialDerivative(expr, A(m))._perform_derivative()\n    assert result - H(j, k)*C(-k)*L.delta(i, -m) == 0\n", "compilation error", "    def test_eval_partial_derivative_expr2():\n    \n        # this is only some special expression\n        # tested: multiple tensor derivatives\n        base_expr2 = A(i)*H(-i, j)*B(k) + A(i)*C(-i)*D(j)*B(k)\n\n        tensor_derivative = PartialDerivative(base_expr2, H(m, n), B(p))._perform_derivative()\n\n        assert (tensor_derivative -  A(L_0)*L.metric(-L_0, -L_1)*L.delta(L_1, -m)*L.delta(j, -n)*B(-p)).expand() == 0\n\n", "def test_eval_partial_derivative_expr2():\n    tau = symbols(\"tau\")\n    # this is only some special expression\n    # tested: vector derivative\n    \n    base_expr2 = A(i)*B(i) + tau*H(i, j)*H(-i, j)\n\n    vector_derivative = PartialDerivative(base_expr2, A(k))._perform_derivative()\n\n    assert (vector_derivative - (B(k) + A(k)*L.delta(L_0, -k) * tau * H(L_0, j) * H(-L_0, j))) == 0 \n", "compilation error"], "sample_85": ["    def test_fast_delete_related_objects_with_no_reverse_relation(self):\n        t = Referrer.objects.create()\n        SecondReferrer.objects.create(referrer=t)\n        self.assertNumQueries(2, t.delete)\n", "    def test_fast_delete_no_reverse_relation(self):\n        c = Child.objects.create()\n        p = Parent.objects.create()\n        # 1 for self delete, 1 for fast delete of child qs\n        self.assertNumQueries(2, p.delete)\n        self.assertFalse(Parent.objects.exists())\n        self.assertFalse(Child.objects.exists())\n", "    def test_fast_delete_related_m2m(self):\n        m = M.objects.create()\n        r = R.objects.create()\n        mr = MR.objects.create(m=m, r=r)\n        self.assertNumQueries(1, mr.delete)\n        self.assertFalse(MR.objects.exists())\n        self.assertFalse(M.objects.filter(pk=m.pk).exists())\n        self.assertFalse(R.objects.filter(pk=r.pk).exists())\n", "    def test_fast_delete_m2m_no_reverse(self):\n        m = M.objects.create()\n        r = R.objects.create()\n        m.m2m.add(r)\n        # 1 to fast-delete m, no cascade to r\n        self.assertNumQueries(1, m.delete)\n", "    def test_fast_delete_with_custom_pk(self):\n        class CustomPKModel(models.Model):\n            custom_pk = models.AutoField(primary_key=True)\n            name = models.CharField(max_length=100)\n\n        CustomPKModel.objects.create(name='test')\n        # 1 query for the delete.\n        self.assertNumQueries(1, CustomPKModel.objects.all().delete)\n        self.assertFalse(CustomPKModel.objects.exists())\n"], "sample_818": ["    def test_spectral_clustering_n_init(self):\n        # Test that spectral clustering with n_init > 1 gives consistent results\n        X, y = make_blobs(n_samples=100, random_state=0,\n                          centers=[[1, 1], [-1, -1]], cluster_std=0.01)\n\n        model1 = SpectralClustering(n_clusters=2, n_init=5, random_state=0)\n        labels1 = model1.fit(X).labels_\n\n        model2 = SpectralClustering(n_clusters=2, n_init=5, random_state=0)\n        labels2 = model2.fit(X).labels_\n\n        assert_array_equal(labels1, labels2)\n", "    def test_spectral_clustering_with_precomputed_affinity_matrix(self):\n        #Test that SpectralClustering can handle precomputed affinity matrix.\n        X = np.array([[1, 1], [2, 1], [1, 0],\n                      [4, 7], [3, 5], [3, 6]])\n        affinity_matrix = pairwise_distances(X)\n        clustering = SpectralClustering(n_clusters=2, affinity='precomputed').fit(affinity_matrix)\n        assert len(np.unique(clustering.labels_)) == 2\n", "def test_spectral_clustering_n_init():\n    # Test that SpectralClustering with n_init > 1 gives consistent results\n    X, y = make_blobs(n_samples=50, random_state=0,\n                      centers=[[1, 1], [-1, -1]], cluster_std=0.01)\n\n    model = SpectralClustering(n_clusters=2, n_init=10, random_state=0)\n    labels1 = model.fit(X).labels_\n\n    model2 = SpectralClustering(n_clusters=2, n_init=5, random_state=0)\n    labels2 = model2.fit(X).labels_\n\n    assert adjusted_rand_score(labels1, labels2) == 1\n", "def test_spectral_clustering_connectivity():\n    # Test that spectral_clustering works with a connectivity matrix\n    X, y = make_blobs(n_samples=20, random_state=0,\n                      centers=[[1, 1], [-1, -1]], cluster_std=0.01)\n    connectivity = kneighbors_graph(X, n_neighbors=5, include_self=False)\n    labels = SpectralClustering(n_clusters=2, affinity='precomputed',\n                                random_state=0).fit(connectivity).labels_\n    assert adjusted_rand_score(y, labels) == 1\n\n", "def test_spectral_clustering_n_init():\n    X, y = make_blobs(n_samples=20, random_state=0,\n                      centers=[[1, 1], [-1, -1]], cluster_std=0.01)\n\n    sp = SpectralClustering(n_clusters=2, n_init=5, random_state=0)\n    labels = sp.fit(X).labels_\n    assert adjusted_rand_score(y, labels) == 1\n"], "sample_864": ["    def test_max_iter():\n        ms = MeanShift(bandwidth=1.2, max_iter=5)\n        ms.fit(X)\n        assert ms.n_iter_ <= 5\n", "    def test_get_bin_seeds_empty_data():\n        X = np.array([])\n        test_bins = get_bin_seeds(X, 1, 1)\n        assert_array_equal(test_bins, [])\n", "def test_max_iter():\n    # Test that the max_iter parameter is respected\n    ms = MeanShift(bandwidth=1.2, max_iter=2)\n    ms.fit(X)\n    assert ms.n_iter_ == 2\n", "    def test_get_bin_seeds_empty_case():\n        # Test get_bin_seeds when passed an empty array\n        X = np.array([])\n        test_bins = get_bin_seeds(X, 1)\n        assert len(test_bins) == 0\n", "    def test_max_iter():\n        ms = MeanShift(bandwidth=1.2, max_iter=5)\n        ms.fit(X)\n        assert ms.n_iter_ <= 5\n\n"], "sample_1111": ["def test_sqrt():\n    x = Symbol('x')\n    lines = [\n        '      1 |                                             .        ',\n        '        |                                            .         ',\n        '        |                                          .          ',\n        '        |                                         ..           ',\n        '        |                                     ....            ',\n        '        |                                   ...             ',\n        '        |                               .                 ',\n        '        |                              .                  ',\n        '        |.-------------------------------------------------',\n        '        |                           .                    ',\n        '        |                        ..                     ',\n        '        |                      ...                      ',\n        '        |                   ....                       ',\n        '        |                ...                          ',\n        '        |             ..                             ',\n        '        |          .                                      ',\n        '        |                              .                  ',\n        '    0.2 |_________________________________________________',\n        '         0                          0.5                        1'\n    ]\n    assert lines == list(textplot_str(sqrt(x), 0, 1))\n", "def test_horizontal_line():\n    x = Symbol('x')\n    lines = [\n        '      1 |_______________________________________________________',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '    0 |-------------------------------------------------------',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '     -1 |_______________________________________________________',\n        '         -1                         0                          1'\n    ]\n    assert lines == list(textplot_str(1, -1, 1))\n", "def test_constant():\n    x = Symbol('x')\n    lines = [\n        '      1 |_______________________________________________________',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '      0 |-------------------------------------------------------',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '        |                                                       ',\n        '     -1 |_______________________________________________________',\n        '         -1                         0                          1'\n    ]\n    assert lines == list(textplot_str(2, -1, 1))\n", "def test_constant():\n    x = Symbol('x')\n    lines = [\n        '      1 |_______________________________________________________',\n        '        |                .........................................',\n        '        |                .........................................',\n        '        |                .........................................',\n        '        |                .........................................',\n        '        |                .........................................',\n        '        |                .........................................',\n        '        |                .........................................',\n        '        |                .........................................',\n        '        |                .........................................',\n        '        |                .........................................',\n        '        |                .........................................',\n        '        |                .........................................',\n        '        |                .........................................',\n        '        |                .........................................',\n        '        |                .........................................',\n        '        |                .........................................',\n        '        |                .........................................',\n        '        |                .........................................',\n        '      0 |_______________________________________________________',\n        '         -1                         0                          1'\n    ]\n    assert lines == list(textplot_str(2, -1, 1))\n\n", "def test_no_x():\n    x = Symbol('x')\n    lines = [\n        '      1 |                ',\n        '        |                ',\n        '        |                ',\n        '        |                ',\n        '        |                ',\n        '        |                ',\n        '        |                ',\n        '        |                ',\n        '        |                ',\n        '      0 |-----------------',\n        '        |                ',\n        '        |                ',\n        '        |                ',\n        '        |                ',\n        '        |                ',\n        '        |                ',\n        '        |                ',\n        '        |                ',\n        '     -1 |_________________',\n        '         0                 1'\n    ]\n    assert lines == list(textplot_str(1, 0, 1))\n\n"], "sample_403": ["    def test_unapply_creates_empty_migration(self):\n        \"\"\"\n        Tests unapplying a migration creates an empty migration.\n        \"\"\"\n        operation = migrations.CreateModel(\n            \"Pony\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"pink\", models.IntegerField(default=1)),\n            ],\n        )\n        project_state = ProjectState()\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_unapply\", new_state)\n        # Test the database alteration\n        with connection.schema_editor() as editor:\n            operation.database_forwards(\"test_unapply\", editor, project_state, new_state)\n        with connection.schema_editor() as editor:\n            operation.database_backwards(\"test_unapply\", editor, new_state, project_state)\n        self.assertTableNotExists(\"test_unapply_pony\")\n", "    def test_references_field_when_field_is_none(self):\n        operation = FieldOperation(\"Model\", \"field\", None)\n        self.assertIs(operation.references_field(\"Model\", \"field\", \"migrations\"), False)\n\n", "    def test_alter_unique_together(self):\n        \"\"\"\n        Tests that AlterUniqueTogether operation correctly applies\n        unique_together constraints.\n\n        This also checks the backwards compatibility:\n        - unique_together should be removed when the operation is reversed.\n        \"\"\"\n        # Create a model with a default unique_together constraint\n        project_state = self.set_up_test_model(\"test_alteruniqu\")\n        operation = migrations.AlterUniqueTogether(\n            \"SomeModel\", unique_together=[(\"field\", \"other_field\")]\n        )\n        # Test the state alteration\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_alteruniqu\", new_state)\n        self.assertEqual(\n            new_state.models[\"test_alteruniqu\", \"somemodel\"].unique_together,\n            [[\"field\", \"other_field\"]],\n        )\n        # Test the database alteration\n        with connection.SchemaEditor() as editor:\n            operation.database_forwards(\"test_alteruniqu\", editor, project_state, new_state)\n        # Test the database reversal\n        with connection.SchemaEditor() as editor:\n            operation.database_backwards(\"test_alteruniqu\", editor, new_state, project_state)\n        self.assertEqual(\n            project_state.models[\"test_alteruniqu\", \"somemodel\"].unique_together,\n            [],\n        )\n\n", "    def test_non_field_references(self):\n        operation = FieldOperation(\"MoDel\", \"field\", models.BooleanField(default=False))\n        self.assertIs(operation.references_field(\"Other\", \"field\", \"migrations\"), False)\n", "    def test_references_field_with_choices(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.CharField(max_length=10, choices=[(\"A\", \"A\"), (\"B\", \"B\")]),\n        )\n        self.assertIs(operation.references_field(\"Model\", \"field\", \"migrations\"), True)\n        self.assertIs(operation.references_field(\"Other\", \"whatever\", \"migrations\"), False) \n"], "sample_484": ["    def test_expression(self):\n        authors = Author.objects.annotate(\n            name_part=Right(Lower(\"name\"), Value(5))\n        )\n        self.assertQuerySetEqual(\n            authors.order_by(\"name\"), [\"smith\", \"honda\"], lambda a: a.name_part\n        )\n", "    def test_negative_length(self):\n        with self.assertRaisesMessage(ValueError, \"'length' must be greater than 0\"):\n            Author.objects.annotate(raises=Right(\"name\", Value(-1)))\n", "    def test_empty_string(self):\n        Author.objects.create(name=\"\")\n        authors = Author.objects.annotate(name_part=Right(\"name\", 5))\n        self.assertQuerySetEqual(authors.order_by(\"name\"), [\"\"], lambda a: a.name_part)\n", "    def test_with_value(self):\n        authors = Author.objects.annotate(\n            name_part=Right(\"name\", Value(5))\n        )\n        self.assertQuerySetEqual(\n            authors.order_by(\"name\"), [\"Smith\", \"honda\"], lambda a: a.name_part\n        )\n", "    def test_string_length(self):\n        authors = Author.objects.annotate(name_part=Right(\"name\", Value(3)))\n        self.assertQuerySetEqual(\n            authors.order_by(\"name\"), [\"ith\", \"nda\"], lambda a: a.name_part\n        )\n"], "sample_669": ["def test_capturefd_closed_on_teardown():\n    \"\"\"Ensure that FDCapture instances are properly closed on teardown.\"\"\"\n    cap = StdCaptureFD(out=True, err=True, in_=True)\n    cap.start_capturing()\n    try:\n        os.write(1, b\"hello\\n\")\n    finally:\n        cap.stop_capturing()\n    assert cap.out._oldfd == 1\n\n", "def test_capture_multiple_stderr_writes(capsys):\n    sys.stderr.write(\"first line\\n\")\n    sys.stderr.write(\"second line\\n\")\n    _, err = capsys.readouterr()\n    assert err == \"first line\\nsecond line\\n\"\n", "def test_multiple_captures_same_stream(testdir, capsys):\n    testdir.makepyfile(\n        \"\"\"\n        import sys\n            print(\"hello\")\n            sys.stdout.write(\"world\")\n            captured = capsys.readouterr()\n            assert captured.out == \"helloworld\"\n        \n            print(\"spam\")\n            captured = capsys.readouterr()\n            assert captured.out == \"spam\"\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=2)\n", "def test_encodedfile_writelines_empty_strings(tmpfile: BinaryIO) -> None:\n    ef = capture.EncodedFile(tmpfile, encoding=\"utf-8\")\n    assert ef.writelines([\"\"]) is None\n    ef.flush()\n    tmpfile.seek(0)\n    assert tmpfile.read() == b\"\"\n    tmpfile.close()\n", "def test_capture_and_logging_with_setup_teardown(testdir, capsys):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import sys\n\n        logger = logging.getLogger(__name__)\n\n            logging.basicConfig(level='INFO')\n            print(\"setup\")\n\n            print(\"teardown\")\n            \n\n            print(\"hello\")\n            logger.info(\"info message\")\n            sys.stderr.write(\"error\\n\")\n        \n        \n        \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*setup*\",\n            \"hello\",\n            \"*teardown*\",\n        ]\n    )\n    result.stderr.fnmatch_lines([\"error\"])\n    assert \"info message\" in capsys.readouterr().out\n"], "sample_298": ["    def test_token_salt_change(self):\n        \"\"\"\n        Ensure using a different token salt prevents token validation.\n        \"\"\"\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        original_salt = p0.key_salt\n        p0.key_salt = 'different_salt'\n        tk1 = p0.make_token(user)\n        p0.key_salt = original_salt\n\n        self.assertFalse(p0.check_token(user, tk1))\n\n\n", "    def test_password_reset_token_generator_without_secret_key(self):\n        \"\"\"\n        Test that PasswordResetTokenGenerator raises ImproperlyConfigured if\n        SECRET_KEY is not set in settings.\n        \"\"\"\n        with override_settings(SECRET_KEY=None):\n            with self.assertRaises(ImproperlyConfigured):\n                PasswordResetTokenGenerator()\n", "    def test_password_reset_token_generator_algorithm(self):\n        \"\"\"Test that a custom algorithm can be used for token generation.\"\"\"\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        # Use a different algorithm for token generation\n        p0 = PasswordResetTokenGenerator()\n        p0.algorithm = 'sha512'\n        tk0 = p0.make_token(user)\n        # Token generated with 'sha512' should not be valid with the default 'sha256'\n        p1 = PasswordResetTokenGenerator()\n        self.assertIs(p1.check_token(user, tk0), False)\n        # Token generated with 'sha512' should be valid with the same algorithm\n        self.assertIs(p0.check_token(user, tk0), True)\n", "    def test_token_with_changed_password(self):\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        user.set_password('newtestpw')\n        user.save()\n        self.assertIs(p0.check_token(user, tk1), False)\n\n", "    def test_token_algorithm(self):\n        user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        tk1 = p0.make_token(user)\n        p1 = MockedPasswordResetTokenGenerator(datetime.now())\n        p1.algorithm = 'sha512'\n        tk2 = p1.make_token(user)\n        self.assertNotEqual(tk1, tk2)\n        self.assertIs(p0.check_token(user, tk1), True)\n        self.assertIs(p1.check_token(user, tk2), True)\n        self.assertIs(p0.check_token(user, tk2), False)\n        self.assertIs(p1.check_token(user, tk1), False)\n"], "sample_844": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_995": ["def test_numpy_to_float():\n    from sympy.utilities.pytest import skip\n    from sympy.external import import_module\n    np = import_module('numpy')\n    if not np:\n        skip('numpy not installed. Abort numpy tests.')\n\n        prec = np.finfo(npval).nmant + 1\n        x = Float(npval)\n        assert x._prec == prec\n        y = Float(ratval, precision=prec)\n        assert abs((x - y)/y) < 2**(-(prec + 1))\n\n    check_prec_and_relerr(np.float16(2/3), S(2)/3)\n    check_prec_and_relerr(np.float32(2/3), S(2)/3)\n    check_prec_and_relerr(np.float64(2/3), S(2)/3)\n    # extended precision, on some arch/compilers:\n    x = np.longdouble(2)/3\n    check_prec_and_relerr(x, S(2)/3)\n    y = Float(x, precision=10)\n    assert same_and_same_prec(y, Float(S(2)/3, precision=10))\n\n    raises(TypeError, lambda: Float(np.complex64(1+2j)))\n    raises(TypeError, lambda: Float(np.complex128(1+2j)))\n", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_529": ["compilation error", "compilation error", "    def test_legend_with_duplicate_labels():\n        fig, ax = plt.subplots()\n        ax.plot([1, 2], [1, 2], label='A')\n        ax.plot([2, 3], [2, 3], label='A')\n        with pytest.warns(UserWarning):\n            ax.legend()\n", "    def test_legend_handles_none():\n        fig, ax = plt.subplots()\n        ax.plot([1, 2], [3, 4], label='line')\n        leg = ax.legend(handles=None)\n        assert len(leg.legend_handles) == 0\n", "def test_legend_markersize_from_line2d():\n    # Test that markersize can be copied for legend lines\n    fig, ax = plt.subplots()\n    lines = [mlines.Line2D([0], [0], marker='o', markersize=i) for i in range(3, 6)]\n    labels = [\"foo\", \"bar\", \"xyzzy\"]\n    markersizes = [line.get_markersize() for line in lines]\n    legend = ax.legend(lines, labels)\n\n    new_markersizes = [line.get_markersize() for line in legend.get_lines()]\n    new_labels = [text.get_text() for text in legend.get_texts()]\n\n    assert markersizes == new_markersizes\n    assert labels == new_labels\n"], "sample_225": ["    def test_get_action(self):\n        self.site.add_action(delete_selected, 'my_delete_selected')\n        self.assertEqual(self.site.get_action('my_delete_selected'), delete_selected)\n\n", "    def test_get_actions(self):\n        self.site.add_action(lambda x: None, 'custom_action')\n        actions = list(self.site.actions)\n        self.assertEqual(len(actions), 2)\n        self.assertIn(('custom_action', self.site.get_action('custom_action')), actions)\n", "    def test_get_actions(self):\n        self.site.add_action(lambda: None, name='test_action')\n        actions = list(self.site.actions)\n        self.assertEqual(len(actions), 2)\n        self.assertIn(('test_action', self.site._actions['test_action']), actions)\n        self.assertIn(('delete_selected', delete_selected), actions)\n", "    def test_get_action(self):\n        self.site.add_action(delete_selected)\n        self.assertEqual(self.site.get_action('delete_selected'), delete_selected)\n", "    def test_get_action(self):\n        test_action = self.site._actions['delete_selected']\n        self.assertEqual(self.site.get_action('delete_selected'), test_action)\n"], "sample_1012": ["def test_pycode_user_functions():\n    class MyFunction(Expr):\n            return Expr.__new__(cls, arg)\n\n            return 'myfunc({})'.format(self.args[0])\n\n    from sympy.printing.pycode import PythonCodePrinter\n    printer = PythonCodePrinter()\n    printer.known_functions['myfunc'] = 'myfunc'\n    assert pycode(MyFunction(x)) == 'myfunc(x)'\n\n", "def test_scipy_sparse_matrix():\n    p = SciPyPrinter()\n\n    smat = SparseMatrix([[1, 0, 0], [0, 2, 0], [0, 0, 3]])\n    assert p.doprint(smat) == 'scipy.sparse.coo_matrix([1, 2, 3], ([0, 1, 2], [0, 1, 2]), shape=(3, 3))'\n", "compilation error", "compilation error", "def test_pycode_with_settings():\n\n    s1, s2 = symbols('x y')\n\n    assert pycode(s1 + s2, settings={'user_functions': {'add': 'my_add'}}) == 'my_add(x, y)'\n\n    assert pycode(s1 + s2, settings={'fully_qualified_modules': False}) == 'x + y'\n\n    assert pycode(s1 + s2, settings={'contract': True}) == 'x + y'\n\n"], "sample_322": ["    def test_minimize_rollbacks_circular(self):\n        \"\"\"Minimize rollbacks with circular dependencies.\"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_dependency(None, b1, a1)\n        graph.add_dependency(None, a1, b1)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            b1: b1_impl,\n        })\n\n        plan = executor.migration_plan({a1})\n\n        self.assertEqual(plan, [])\n", "    def test_minimize_rollbacks_multiple_app_dependencies(self):\n        \"\"\"\n        Minimize rollbacks when multiple apps depend on a shared migration.\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        c1_impl = FakeMigration('c1')\n        c1 = ('c', '1')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_node(c1, c1_impl)\n        graph.add_dependency(None, b1, a1)\n        graph.add_dependency(None, c1, a1)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            b1: b1_impl,\n            c1: c1_impl,\n        })\n\n\n\n        plan = executor.migration_plan({a1})\n\n        self.assertEqual(plan, [(c1_impl, True)])\n", "    def test_minimize_rollbacks_unapplied(self):\n        r\"\"\"\n        Minimize rollbacks when target migration is unapplied.\n\n        a: 1 <------ 3\n              \\   /\n        b:      2\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        a3_impl = FakeMigration('a3')\n        a3 = ('a', '3')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        b2_impl = FakeMigration('b2')\n        b2 = ('b', '2')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(a3, a3_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_node(b2, b2_impl)\n        graph.add_dependency(None, a2, a1)\n        graph.add_dependency(None, a3, a1)\n        graph.add_dependency(None, b2, a2)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {\n            a1: a1_impl,\n            b1: b1_impl,\n            a3: a3_impl,\n            b2: b2_impl,\n        })\n\n        plan = executor.migration_plan({a1})\n\n        should_be_rolled_back = [b2_impl, a3_impl]\n        exp = [(m, True) for m in should_be_rolled_back]\n        self.assertEqual(plan, exp)\n", "compilation error", "    def test_minimize_rollbacks_multi_app_branchy(self):\n        r\"\"\"\n        Minimize rollbacks with multiple apps and branchy dependencies.\n\n        a: 1 <---- 3 <--\\\n              \\ \\- 2 <--- 4\n               \\       \\\n        b:      \\- 1 <--- 5\n        c:          \\\n                    \\- 1\n\n        \"\"\"\n        a1_impl = FakeMigration('a1')\n        a1 = ('a', '1')\n        a2_impl = FakeMigration('a2')\n        a2 = ('a', '2')\n        a3_impl = FakeMigration('a3')\n        a3 = ('a', '3')\n        a4_impl = FakeMigration('a4')\n        a4 = ('a', '4')\n        b1_impl = FakeMigration('b1')\n        b1 = ('b', '1')\n        b5_impl = FakeMigration('b5')\n        b5 = ('b', '5')\n        c1_impl = FakeMigration('c1')\n        c1 = ('c', '1')\n        graph = MigrationGraph()\n        graph.add_node(a1, a1_impl)\n        graph.add_node(a2, a2_impl)\n        graph.add_node(a3, a3_impl)\n        graph.add_node(a4, a4_impl)\n        graph.add_node(b1, b1_impl)\n        graph.add_node(b5, b5_impl)\n        graph.add_node(c1, c1_impl)\n        graph.add_dependency(None, a2, a1)\n        graph.add_dependency(None, a3, a1)\n        graph.add_dependency(None, a4, a2)\n        graph.add_dependency(None, a4, a3)\n        graph.add_dependency(None, b5, b1)\n        graph.add_dependency(None, b1, a1)\n        graph.add_dependency(None, c1, b5)\n\n        executor = MigrationExecutor(None)\n        executor.loader = FakeLoader(graph, {"], "sample_816": ["    def test_vectorizer_analyze_error_handling(self, tmpdir):\n        # test if the analyzer function raises an error, it's correctly handled\n\n            raise ValueError(\"Analyzer error!\")\n\n        f = tmpdir.join(\"file.txt\")\n        f.write(\"sample content\\n\")\n\n        with pytest.raises(ValueError, match=\"Analyzer error!\"):\n            CountVectorizer(analyzer=analyzer).fit_transform([f])\n", "    def test_vectorizer_ngrams_empty_input(self):\n        for vec in [CountVectorizer(ngram_range=(1, 2)),\n                     TfidfVectorizer(ngram_range=(1, 2)),\n                     HashingVectorizer(ngram_range=(1, 2))]:\n            with pytest.raises(ValueError, match=\"Empty input data\"):\n                vec.fit([])\n", "    def test_tfidfvectorizer_empty_vocabulary(self):\n        vect = TfidfVectorizer(vocabulary=[])\n        # This used to raise a ValueError\n        with pytest.warns(UserWarning):\n            vect.fit_transform(JUNK_FOOD_DOCS)\n        assert len(vect.vocabulary_) == 0\n", "    def test_vectorizer_with_empty_documents(self):\n        vect = CountVectorizer()\n        X = vect.fit_transform(['', 'hello'])\n        assert_array_equal(X.toarray(), [[0], [1]])\n        assert vect.get_feature_names() == ['hello']\n", "    def test_vectorizer_unicode_error(self, tmpdir):\n        f = tmpdir.join(\"file.txt\")\n        f.write(\"\u00e4\u00f6\u00fc\")\n\n        with pytest.raises(UnicodeDecodeError):\n            CountVectorizer().fit_transform([f])\n"], "sample_1164": ["compilation error", "def test_qsympy():\n    hbar = sympify('h/2/pi')\n    x = symbols('x')\n    ket = JzKet(1, 1)\n    op = JzOp(Symbol('J'))*hbar\n\n    assert str(op) == 'J*hbar/2/pi'\n    ascii_str = \\", "def test_tensorproduct_basics():\n    tp1 = TensorProduct(JzKet(1, 0), JzKet(1, 1))\n    tp2 = TensorProduct(JzKet(1, 0), JzKet(1, 1), JzKet(1, 0))\n    assert tp1.rank == 2\n    assert tp2.rank == 3\n    assert tp1.terms == [(JzKet(1, 0), JzKet(1, 1))]\n    assert tp2.terms == [(JzKet(1, 0), JzKet(1, 1), JzKet(1, 0))]\n    assert str(tp1) == '|1,0>x|1,1>'\n    assert str(tp2) == '|1,0>x|1,1>x|1,0>'\n    assert tp1.get_factor(0) == JzKet(1, 0)\n\n", "compilation error", "def test_densityoperator():\n    rho = DensityOperator(\n        Ket(0), Ket(0), Ket(1), Ket(1), [0.5, 0.5], [0.5, 0.5]\n    )\n    assert str(rho) == '{0.5*|0><0| + 0.5*|1><1|}'\n    assert pretty(rho) == '{0.5*|0><0| + 0.5*|1><1|}'\n    assert upretty(rho) == '{0.5\u22c5\u27580\u27e9\u27e80| + 0.5\u22c5\u27581\u27e9\u27e81|}'\n    assert latex(rho) == r'0.5{\\left|0\\right\\rangle }{\\left\\langle 0\\right|} + 0.5{\\left|1\\right\\rangle }{\\left\\langle 1\\right|}'\n    sT(rho, \"DensityOperator(List(Ket(Integer(0)), Ket(Integer(1))), List(Ket(Integer(0)), Ket(Integer(1))), List(Float(0.5, 0), Float(0.5, 0)), List(Float(0.5, 0), Float(0.5, 0)))\")\n"], "sample_1204": ["    def test_coset_representative():\n        S = SymmetricGroup(4)\n        H = PermutationGroup(Permutation(1, 2))\n        coset = S.coset(H, Permutation(0,1))\n        assert coset.representative == Permutation(0,1)\n", "    def test_centralizer():\n        S = SymmetricGroup(4)\n        x = Permutation(1, 2)\n        C = S.centralizer(x)\n        assert C.is_subgroup(S)\n        assert all(g*x == x*g for g in C.generators)\n", "def test_is_primitive():\n    G = SymmetricGroup(4)\n    assert G.is_primitive\n    A = AlternatingGroup(4)\n    assert A.is_primitive\n    G = PermutationGroup(Permutation(0, 1, 2), Permutation(0, 2, 3))\n    assert G.is_primitive\n    G = PermutationGroup([Permutation(0, 1)])\n    assert not G.is_primitive\n    G = PermutationGroup([Permutation([0, 1, 2]), Permutation([0, 2, 1])])\n    assert G.is_primitive\n    G = PermutationGroup(Permutation(0, 1, 2, 3), Permutation(0, 2))\n    assert not G.is_primitive\n", "    def test_coset_representatives():\n        S = SymmetricGroup(4)\n        H = PermutationGroup([Permutation(1, 2)])\n        reps = S.coset_representatives(H)\n        assert len(reps) == S.index(H)\n        for rep in reps:\n            assert rep in S\n            assert rep.is_permutation\n        expected = [Permutation(size=4), Permutation(0, 1, size=4),\n                    Permutation(0, 2, size=4), Permutation(0, 3, size=4),\n                    Permutation(1, 2, size=4), Permutation(1, 3, size=4),\n                    Permutation(2, 3, size=4), Permutation(0, 1, 2, size=4),\n                    Permutation(0, 1, 3, size=4), Permutation(0, 2, 3, size=4),\n                    Permutation(1, 2, 3, size=4), Permutation(1, 3, 2, size=4),\n                    Permutation(2, 3, 1, size=4), Permutation(0, 3, 2, size=4),\n                    Permutation(0, 2, 1, size=4)]\n        for rep in reps:\n            assert rep in expected\n", "    def test_is_solvable():\n        G = SymmetricGroup(5)\n        assert G.is_solvable is False\n\n        G = AlternatingGroup(5)\n        assert G.is_solvable is True\n\n        G = DihedralGroup(6)\n        assert G.is_solvable is True\n\n        G = PermutationGroup([Permutation(1, 2, 3), Permutation(2, 3, 4)])\n        assert G.is_solvable is True\n\n        G = PermutationGroup([Permutation(1, 2, 3), Permutation(1, 3, 4)])\n        assert G.is_solvable is False\n\n\n"], "sample_424": ["    def test_field_operation_with_default(self):\n        operation = migrations.AlterField(\n            \"Model\",\n            \"field\",\n            models.IntegerField(default=10),\n        )\n        self.assertEqual(operation.describe(), \"Alter field 'field' on 'Model'.\")\n        self.assertEqual(operation.field.default, 10)\n", "    def test_runpython_allow_migrate_returns_true_when_migrate_is_true(self):\n            pass\n        operation = migrations.RunPython(inner_method, inner_method, allow_migrate=True)\n        self.assertTrue(operation.allow_migrate)\n", "    def test_representation(self):\n        operation = migrations.CreateModel(\n            \"Pony\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"pink\", models.IntegerField(default=1)),\n            ],\n        )\n        self.assertEqual(repr(operation), \"<CreateModel: Pony>\")\n", "    def test_references_field_related_name(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ForeignKey(\"Other\", models.CASCADE, related_name=\"relation\"),\n        )\n        self.assertIs(operation.references_field(\"Other\", \"relation\", \"migrations\"), True)\n        self.assertIs(\n            operation.references_field(\"Other\", \"whatever\", \"migrations\"), False\n        )\n", "    def test_references_field_by_through_to_fields(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ManyToManyField(\n                \"Other\",\n                through=\"Through\",\n                through_fields=[\"through_field\"],\n            ),\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"whatever\", \"migrations\"), True\n        )\n        self.assertIs(\n            operation.references_field(\"Through\", \"through_field\", \"migrations\"), True\n        )\n        self.assertIs(\n            operation.references_field(\"Missing\", \"whatever\", \"migrations\"), False\n        )\n\n"], "sample_959": ["    def test_domain_cpp_parse_template_args_complex(app):\n        text = (\".. cpp:function:: void f(typename T::type Arg)\\n\"\n                \".. cpp:function:: template <typename T> void g(T Arg)\\n\"\n                \".. cpp:function:: void h(typename T::template type Arg)\\n\")\n        doctree = restructuredtext.parse(app, text)\n        assert_node(doctree, (addnodes.index, desc, addnodes.index, desc, addnodes.index, desc))\n", "    def test_domain_cpp_parse_object_type(app, warning):\n        text = '.. cpp:object:: MyObject\\n'\n        doctree = restructuredtext.parse(app, text)\n        assert_node(doctree, desc)\n", "compilation error", "    def check(spec, text, file):\n        pattern = '<li><p>%s<a .*?><code .*?><span .*?>%s</span></code></a></p></li>' % spec\n        res = re.search(pattern, text)\n        if not res:\n            print(\"Pattern\\n\\t%s\\nnot found in %s\" % (pattern, file))\n            assert False", "def test_domain_cpp_build_visibility(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"visibility\")\n    assert len(ws) == 0\n"], "sample_56": ["    def test_ordering_in_fieldsets(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            fieldsets = [\n                (None, {\n                    'fields': ['title', 'album']\n                }),\n                (None, {\n                    'fields': ['title']\n                }),\n            ]\n            ordering = ['album']\n\n        errors = MyModelAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"'ordering' is specified in 'MyModelAdmin', but 'fieldsets' \"\n                \"also define fields. If you want to order fields within \"\n                \"fieldsets, specify them directly in the fieldsets.\",\n                obj=MyModelAdmin,\n                id='admin.E016'\n            )\n        ]\n        self.assertEqual(errors, expected)\n\n", "    def test_check_sublists_for_nested_duplicates(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            fields = ['state', ['state', ['state']]]\n\n        errors = MyModelAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"The value of 'fields' contains duplicate field(s).\",\n                obj=MyModelAdmin,\n                id='admin.E006'\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_check_duplicate_fieldsets(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            fieldsets = [\n                (None, {\n                    'fields': ['title', 'album']\n                }),\n                (None, {\n                    'fields': ['title', 'album']\n                }),\n            ]\n\n        errors = MyModelAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"There are duplicate fieldsets.\",\n                obj=MyModelAdmin,\n                id='admin.E011'\n            )\n        ]\n        self.assertEqual(errors, expected)\n", "    def test_extra_in_inline(self):\n        class SongInline(admin.TabularInline):\n            model = Song\n            extra = 1\n        \n        class AlbumAdmin(admin.ModelAdmin):\n            inlines = [SongInline]\n        \n        errors = AlbumAdmin(Album, AdminSite()).check()\n        self.assertEqual(errors, [])\n", "    def test_nested_fieldsets_duplicates(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            fieldsets = (\n                ('Main', {'fields': ['title', ('title', 'album')]}),\n            )\n\n        errors = MyModelAdmin(Song, AdminSite()).check()\n        expected = [\n            checks.Error(\n                \"There are duplicate field(s) in 'fieldsets[0][1]'.\",\n                obj=MyModelAdmin,\n                id='admin.E012'\n            )\n        ]\n        self.assertEqual(errors, expected)\n"], "sample_1092": ["def test_issue_13000_rewrite():\n    eq = x/(-4*x**2 + y**2).rewrite(\n        'y**2/(4*x**2 - x**2)',\n    )\n    cse_eq = cse(eq)[1][0]\n    assert cse_eq == eq\n\n\n\n", "def test_cse_with_complex_numbers():\n    z = symbols('z', complex=True)\n    eq = z**2 + 1\n    assert cse(eq) == ([], [eq])\n", "compilation error", "def test_issue_19641():\n    a, b = symbols('a b')\n    expr = a*b/(a*b + 1)\n    assert cse(expr) == ([], [expr])\n", "compilation error"], "sample_1047": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_121": ["    def test_unique_together_on_unmanaged_model(self):\n        class UnmanagedModel(models.Model):\n            name = models.CharField(max_length=100)\n            value = models.CharField(max_length=100)\n\n            class Meta:\n                managed = False\n                unique_together = ('name', 'value')\n\n        self.assertEqual(UnmanagedModel.check(), [])\n", "    def test_check_constraints_invalid_check(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                constraints = [models.CheckConstraint(check=1, name='invalid_check')]\n\n        self.assertEqual(Model.check(), [\n            Error(\n                \"CheckConstraint check must be a Q object.\",\n                obj=Model._meta.constraints[0],\n                id='models.E027',\n            ),\n        ])\n", "    def test_check_constraints_with_database_specific_functions(self):\n        class Model(models.Model):\n            age = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.CheckConstraint(\n                        check=models.Q(age__gte=models.F('other_field')), name='age_vs_other_field'\n                    )\n                ]\n        self.assertEqual(Model.check(), [\n            Error(\n                \"Database constraint 'age_vs_other_field' uses a database-specific function \"\n                \"('F') which Django doesn't support.\",\n                obj=Model,\n                id='models.E028',\n            ),\n        ])\n", "    def test_check_constraints_with_placeholder_in_check(self):\n        class Model(models.Model):\n            name = models.CharField(max_length=100)\n\n            class Meta:\n                constraints = [models.CheckConstraint(check=models.Q(name__startswith=models.F('placeholder')), name='starts_with_placeholder')]\n\n        errors = Model.check()\n        self.assertEqual(errors, [\n            Error(\n                \"The CheckConstraint 'starts_with_placeholder' uses a placeholder \"\n                \"in its check expression, which is not supported.\",\n                obj=Model,\n                id='models.E028',\n            )\n        ])\n", "    def test_unique_together_with_field_default(self):\n        class Model(models.Model):\n            name = models.CharField(max_length=100)\n            value = models.IntegerField(default=42)\n\n            class Meta:\n                unique_together = ('name', 'value')\n"], "sample_969": ["def test_stringify_type_hints_ForwardRef():\n    from typing import ForwardRef  # type: ignore\n    assert stringify(ForwardRef(\"myint\"), False) == \"myint\"\n    assert stringify(ForwardRef(\"myint\"), True) == \"myint\"\n", "def test_stringify_type_union_operator_in_type_hints():\n    assert stringify(Dict[str, int | None], False) == \"Dict[str, int | None]\"\n    assert stringify(Dict[str, int | None], True) == \"~typing.Dict[str, int | None]\"\n\n\n", "    def test_stringify_type_union_operator_nested():\n        assert stringify(Union[int, List[str | None]], False) == \"Union[int, List[str | None]]\"  # type: ignore\n        assert stringify(Union[int, List[str | None]], True) == \"~typing.Union[int, ~typing.List[~typing.Union[str, NoneType]]]\"  # type: ignore\n", "def test_stringify_type_hints_ForwardRef():\n    from typing import ForwardRef  # type: ignore\n    assert stringify(ForwardRef(\"myint\"), False) == \"myint\"\n    assert stringify(ForwardRef(\"myint\"), True) == \"myint\"\n", "def test_stringify_type_hints_ParamSpec():\n    from typing import ParamSpec  # type: ignore\n    P = ParamSpec('P')\n    assert stringify(Callable[P, int], False) == \"Callable[P, int]\"\n    assert stringify(Callable[P, int], True) == \"~typing.Callable[~typing.ParamSpec('P'), int]\"\n"], "sample_250": ["    def test_rfc5322_format(self):\n        now = datetime.now(get_default_timezone())\n        self.assertEqual(dateformat.format(now, 'r'), now.strftime('%a, %d %b %Y %H:%M:%S %z'))\n\n", "    def test_dateformat_with_format_string_containing_invalid_characters(self):\n        with self.assertRaises(ValueError):\n            dateformat.format(datetime(2023, 10, 26, 10, 0, 0), 'This is an invalid format string!@#$%^')\n", "    def test_dateformat_with_naive_datetime(self):\n        naive_dt = datetime(2023, 10, 26, 10, 30, 0)\n        with self.assertRaises(TypeError) as context:\n            dateformat.format(naive_dt, 'r')\n\n        self.assertIn(\n            \"The format for naive datetime objects may not contain \"\n            \"time-related format specifiers (found 'r').\",\n            str(context.exception),\n        )\n", "    def test_iso_week_number(self):\n        # Test ISO-8601 week number calculation\n        self.assertEqual(dateformat.format(datetime(2008, 12, 29), 'W'), '53')\n        self.assertEqual(dateformat.format(datetime(2009, 1, 5), 'W'), '1')\n", "    def test_strftime(self):\n        my_birthday = datetime(1979, 7, 8, 22, 00)\n        # Test that dateformat.format behaves the same as strftime\n        self.assertEqual(dateformat.format(my_birthday, '%Y'), my_birthday.strftime('%Y'))\n        self.assertEqual(dateformat.format(my_birthday, '%m'), my_birthday.strftime('%m'))\n        self.assertEqual(dateformat.format(my_birthday, '%d'), my_birthday.strftime('%d'))\n\n"], "sample_799": ["    def test_cross_val_score_with_sparse_input():\n        # Test cross_val_score with sparse input\n        X, y = make_classification(n_samples=100, random_state=0)\n        X_sparse = csr_matrix(X)\n        est = LogisticRegression()\n        scores = cross_val_score(est, X_sparse, y, cv=5)\n        assert scores.shape == (5,)\n", "def test_fit_and_score_pandas_index():\n    # check permutation_test_score with pandas dataframe\n    # and ensure index is preserved after scoring\n    types = [(MockDataFrame, MockDataFrame)]\n    try:\n        from pandas import Series, DataFrame\n        types.append((Series, DataFrame))\n    except ImportError:\n        pass\n    for TargetType, InputFeatureType in types:\n        # X dataframe, y series\n        iris = load_iris()\n        X, y = iris.data, iris.target\n        X_df = InputFeatureType(X, index=list('abc'))\n        y_ser = TargetType(y)\n        clf = CheckingClassifier(check_X=lambda x: isinstance(x, InputFeatureType)\n                               and x.index.equals(pd.Index(list('abc'))),\n                               check_y=lambda x: isinstance(x, TargetType))\n        permutation_test_score(clf, X_df, y_ser)\n\n", "compilation error", "    def test_cross_val_score_empty_y():\n        X = np.empty((10, 2))\n        y = np.array([])\n        clf = LogisticRegression()\n        with pytest.raises(ValueError):\n            cross_val_score(clf, X, y)\n\n", "    def failing_scorer(estimator, X, y):\n        raise ValueError"], "sample_459": ["    def test_integerfield_validates_zero_against_positive_choices(self):\n        f = models.PositiveIntegerField(choices=((1, 1),))\n        with self.assertRaises(ValidationError):\n            f.clean(\"0\", None)\n", "    def test_integerfield_cleans_valid_strings(self):\n        f = models.IntegerField()\n        self.assertEqual(f.clean(\"10\", None), 10)\n        self.assertEqual(f.clean(\"-10\", None), -10)\n", "    def test_integerfield_cleans_valid_float(self):\n        f = models.IntegerField()\n        self.assertEqual(f.clean(2.0, None), 2)\n", "    def test_integerfield_cleans_valid_float(self):\n        f = models.IntegerField()\n        self.assertEqual(f.clean(1.0, None), 1)\n", "    def test_integerfield_validates_out_of_range_against_choices(self):\n        f = models.IntegerField(choices=self.Choices.choices)\n        with self.assertRaises(ValidationError):\n            f.clean(\"2\", None)\n"], "sample_1095": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_822": ["compilation error", "compilation error", "def test_check_sparse_matrix_format():\n    # Ensures that check_pairwise_arrays raises an error if \n    # input is not in a sparse matrix format.\n\n    XA = np.resize(np.arange(40), (5, 8))\n    XB = np.resize(np.arange(32), (4, 8))\n\n    assert_raises(ValueError, check_pairwise_arrays, XA.tolist(),\n                  XB.tolist())\n", "compilation error", "    def test_pairwise_distances_chunked_invalid_working_memory():\n        # Test pairwise_distances_chunked with invalid working_memory input\n        rng = np.random.RandomState(0)\n        X = rng.random_sample((1000, 10))\n        assert_raises(ValueError, list, pairwise_distances_chunked(X,\n                working_memory=-1))\n"], "sample_985": ["compilation error", "compilation error", "def test_Max_Min_evalf():\n    x, y = symbols('x y')\n    assert Max(x, 2).evalf(subs={x: 1.5}) == 2\n    assert Min(x, 2).evalf(subs={x: 1.5}) == 1.5", "compilation error", "    def test_expand():\n        x, y = symbols('x y')\n        assert (x + y)**2.expand() == x**2 + 2*x*y + y**2\n        assert (x + y)**3.expand() == x**3 + 3*x**2*y + 3*x*y**2 + y**3\n\n        a, b, c = symbols('a b c')\n        assert (a + b + c)**2.expand() == a**2 + 2*a*b + 2*a*c + b**2 + 2*b*c + c**2\n\n        assert (x * y)**2.expand() == x**2 * y**2\n        assert (x * y)**3.expand() == x**3 * y**3\n"], "sample_481": ["    def test_join_unsafe_joiner(self):\n        var_list = [\"alpha\", \"beta & me\", \"gamma\"]\n        context = {\"var_list\": var_list, \"var_joiner\": mark_safe(\"<br/>\")}\n        output = self.engine.render_to_string(\"join_unsafe_joiner\", context)\n        expected_result = \"alpha<br/>beta & me<br/>gamma\"\n        self.assertEqual(output, expected_result)\n\n", "    def test_join_empty_list(self):\n        context = {\"empty_list\": []}\n        output = self.engine.render_to_string(\"join_empty_list\", context)\n        self.assertEqual(output, \"\")\n\n", "    def test_join09(self):\n        output = self.engine.render_to_string(\n            \"join09\", {\"a\": [\"alpha\", \"beta & me\"], \"var\": \" & \"}\n        )\n        self.assertEqual(output, \"ALPHA &amp; BETA &amp; ME\")\n", "    def test_join_empty_list(self):\n        output = self.engine.render_to_string(\"join_empty_list\", {\"a\": []})\n        self.assertEqual(output, \"\")\n\n", "    def test_join_string(self):\n        string = '<a>Hello World!</a>Beta & me<script>Hi!</script>'\n        context = {\"string\": string}\n        output = self.engine.render_to_string(\"join_string\", context)\n        expected_result = '<a>Hello World!</a>, Beta & me, <script>Hi!</script>'\n        self.assertEqual(output, expected_result)\n\n"], "sample_502": ["def test_subplot_kwarg_warning():\n    with pytest.warns(MatplotlibDeprecationWarning,\n                     match=r'Passing parameters other than projection to '\n                           r'subplot\\(\\) is deprecated'):\n        plt.subplot(111, facecolor='blue')\n", "    def test_polar_replace_axes():\n        ax1 = plt.subplot(projection='polar')\n        ax2 = plt.subplot(111)\n        assert ax1.name == 'polar'\n        assert ax2.name == 'rectilinear'\n        assert ax1 is not ax2\n        ax3 = plt.subplot(projection='polar')\n        assert ax1 is not ax3\n        assert ax1 not in plt.gcf().axes\n        assert ax3 is plt.gca()\n", "def test_polar_scatter():\n    fig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\n    sc = ax.scatter(np.arange(0, 2*np.pi, 0.1), np.ones_like(np.arange(0, 2*np.pi, 0.1)))\n    assert isinstance(sc, mpl.collections.PathCollection)\n", "compilation error", "compilation error"], "sample_474": ["    def test_mysql_utf8mb4(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\"SET NAMES utf8mb4\")\n        authors = Author.objects.annotate(first_initial=Left(\"name\", 1))\n        self.assertCountEqual(authors.filter(first_initial=Chr(ord(\"\u00e9\"))), [self.elena])\n        self.assertCountEqual(\n            authors.exclude(first_initial=Chr(ord(\"\u00e9\"))), [self.john, self.rhonda]\n        )\n\n", "    def test_null_input(self):\n        # Ensure that Chr handles NULL inputs gracefully\n        authors = Author.objects.annotate(first_initial=Left(\"name\", 1)).filter(\n            first_initial=Chr(0)\n        )\n        self.assertEqual(authors.count(), 0)\n", "    def test_null(self):\n        self.rhonda.name = None\n        self.rhonda.save()\n        authors = Author.objects.annotate(first_initial=Left(\"name\", 1))\n        self.assertCountEqual(authors.filter(first_initial=Chr(ord(\"J\"))), [self.john])\n        self.assertCountEqual(\n            authors.exclude(first_initial=Chr(ord(\"J\"))), [self.elena]\n        )\n        self.assertCountEqual(authors.filter(first_initial__isnull=True), [self.rhonda])\n", "    def test_empty_string(self):\n        authors = Author.objects.annotate(first_initial=Left(\"name\", 1))\n        self.assertCountEqual(authors.filter(first_initial=Chr(ord(\"\"))), [])\n", "    def test_null_handling(self):\n        self.john.alias = None\n        self.john.save()\n        authors = Author.objects.annotate(first_initial=Chr(F(\"alias\")[0]))\n        self.assertIsNone(authors.get(name=\"John Smith\").first_initial)\n"], "sample_1101": ["def test_schur_number():\n    assert SchurNumber(1) == 1\n    assert SchurNumber(2) == 4\n    assert SchurNumber(3) == 13\n    assert SchurNumber(4) == 44\n    assert SchurNumber(5).lower_bound() == 364\n    raises(ValueError, lambda: SchurNumber(S.Infinity))\n    raises(ValueError, lambda: SchurNumber(-1))\n    raises(ValueError, lambda: SchurNumber(Rational(1,2)))\n", "def test_SchurNumber():\n    assert SchurNumber(1) == 1\n    assert SchurNumber(2) == 4\n    assert SchurNumber(3) == 13\n    assert SchurNumber(4) == 44\n    raises(ValueError, lambda: SchurNumber(S.Infinity))\n    raises(ValueError, lambda: SchurNumber(0))\n    raises(ValueError, lambda: SchurNumber(-1))\n    raises(ValueError, lambda: SchurNumber(Rational(1, 2)))\n    assert SchurNumber(6).lower_bound() == 364\n", "def test_SchurNumber():\n    assert SchurNumber(1).lower_bound() == 1\n    assert SchurNumber(2).lower_bound() == 4\n    assert SchurNumber(3).lower_bound() == 13\n    assert SchurNumber(4).lower_bound() == 44\n    raises(ValueError, lambda: SchurNumber(-1))\n    raises(ValueError, lambda: SchurNumber(Rational(1,2)))\n    raises(ValueError, lambda: SchurNumber(S.Infinity))\n    assert SchurNumber(5) == SchurNumber(5) \n", "compilation error", "def test_SchurNumber():\n\n    raises(ValueError, lambda: SchurNumber(S.Infinity))\n    raises(ValueError, lambda: SchurNumber(Rational(-1, 2)))\n    raises(ValueError, lambda: SchurNumber(0))\n    assert SchurNumber(1).lower_bound() == 1\n    assert SchurNumber(2).lower_bound() == 3\n    assert SchurNumber(3).lower_bound() == 13\n    assert SchurNumber(4).lower_bound() == 44\n    assert SchurNumber(5).lower_bound() == 133\n    assert SchurNumber(1) == 1\n    assert SchurNumber(2) == 4\n    assert SchurNumber(3) == 13\n    assert SchurNumber(4) == 44\n\n"], "sample_1154": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_931": ["def test_pyclass_only_method(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:method:: meth\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'Class() (built-in class)', 'Class', '', None)])\n    assert 'Class' in domain.objects\n    assert domain.objects['Class'] == ('index', 'Class', 'class')\n", "def test_pyexception_no_module(app):\n    text = \".. py:exception:: MyException\"\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"exception \"],\n                                                    [desc_name, \"MyException\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], desc, desctype=\"exception\",\n                domain=\"py\", objtype=\"exception\", noindex=False)\n\n    assert 'MyException' in domain.objects\n    assert domain.objects['MyException'] == ('index', 'MyException', 'exception')\n", "    def test_pyexception_signature_no_module(app):\n        text = \".. py:exception:: IOError\"\n        doctree = restructuredtext.parse(app, text)\n        assert_node(doctree, (addnodes.index,\n                              [desc, ([desc_signature, ([desc_annotation, \"exception \"],\n                                                        [desc_name, \"IOError\"])],\n                                      desc_content)]))\n        assert_node(doctree[1], desc, desctype=\"exception\",\n                    domain=\"py\", objtype=\"exception\", noindex=False)\n", "def test_pyattribute_empty_type(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"      :value: ''\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'attr (Class attribute)', 'Class.attr', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_name, \"attr\"],\n                                                     [desc_annotation, \" = ''\"])],\n                                   [desc_content, ()]))\n    assert 'Class.attr' in domain.objects\n    assert domain.objects['Class.attr'] == ('index', 'Class.attr', 'attribute')\n", "compilation error"], "sample_27": ["    def test_fitsdiff_different_hdu_types(tmp_path):\n        path1 = tmp_path / \"test1.fits\"\n        path2 = tmp_path / \"test2.fits\"\n\n        hdulist = HDUList([PrimaryHDU(), ImageHDU(data=np.zeros(5))])\n        hdulist.writeto(path1)\n        hdulist[1] = BinTableHDU(data=np.zeros((1, 2)))\n        hdulist.writeto(path2)\n\n        diff = FITSDiff(path1, path2)\n        assert \"Extension HDU 1\" in diff.report()\n        assert \"Extension types differ\" in diff.report()\n", "def test_fitsdiff_different_extension_types(tmp_path):\n    \"\"\"\n    Test that FITSDiff handles files with extensions of different types correctly.\n    \"\"\"\n    path1 = tmp_path / \"test1.fits\"\n    path2 = tmp_path / \"test2.fits\"\n\n    hdulist1 = HDUList([PrimaryHDU(), ImageHDU(data=np.zeros(5))])\n    hdulist1.writeto(path1)\n    hdulist2 = HDUList([PrimaryHDU(), TableHDU(data=np.zeros((5, 2)))])\n    hdulist2.writeto(path2)\n\n    diff = FITSDiff(path1, path2)\n    assert not diff.identical\n    assert diff.diff_hdu_count == (2, 2)\n    assert len(diff.diff_hdus) == 1\n\n    hdudiff = diff.diff_hdus[0][1]\n    assert hdudiff.diff_extension_types == (\"IMAGE\", \"BINTABLE\")\n    assert \"Extension types differ\" in diff.report()\n", "    def test_headerdiff_comment_change(self):\n        ha = Header([(\"A\", 1, \"Comment A\"), (\"B\", 2, \"Comment B\")])\n        hb = ha.copy()\n        hb.comments[\"A\"] = \"Changed Comment A\"\n        diff = HeaderDiff(ha, hb)\n        assert not diff.identical\n        assert \"Headers contain differences\" in diff.report()\n        assert \"Comment for keyword 'A' differs\" in diff.report()\n", "    def test_fitsdiff_with_different_versions(tmp_path):\n        \"\"\"Make sure diff report shows version numbers when they are different\"\"\"\n        path1 = tmp_path / \"test1.fits\"\n        path2 = tmp_path / \"test2.fits\"\n\n        hdulist = HDUList([PrimaryHDU(), ImageHDU(data=np.zeros(5), name=\"SCI\", ver=1)])\n        hdulist.writeto(path1)\n        hdulist[1].ver = 2\n        hdulist.writeto(path2)\n\n        diff = FITSDiff(path1, path2)\n        assert \"Extension HDU 1 (SCI, 1):\" in diff.report()\n        assert \"Extension HDU 1 (SCI, 2):\" in diff.report()\n", "    def test_diff_with_empty_hdu(tmp_path):\n        path1 = tmp_path / \"test1.fits\"\n        path2 = tmp_path / \"test2.fits\"\n\n        hdulist1 = HDUList([PrimaryHDU(), ImageHDU(data=np.zeros(5))])\n        hdulist1.writeto(path1)\n\n        hdulist2 = HDUList([PrimaryHDU()])\n        hdulist2.writeto(path2)\n\n        diff = FITSDiff(path1, path2)\n        assert not diff.identical\n        assert \"Files contain different numbers of HDUs\" in diff.report()\n        assert \"a: 2\\n b: 1\" in diff.report() \n"], "sample_885": ["    def test_validate_params_with_default_values():\n       \"\"\"Check that the validation works correctly when default values are used in the function call.\"\"\"\n       @validate_params({\"a\": [int], \"b\": [str, \"default\"]})\n           pass\n\n       f(1)  # Should pass, b defaults to \"default\"\n       with pytest.raises(InvalidParameterError, match=\"The 'a' parameter\"):\n           f(\"wrong\")\n", "    def test_validate_params_default_values():\n        \"\"\"Check that default values are handled correctly when validating parameters.\"\"\"\n\n        @validate_params({\"a\": [int], \"b\": [int, 5]})\n            pass\n\n        f(1)  # b should default to 5 and be valid\n        f(1, b=10)  # explicitly setting b should be valid\n\n        with pytest.raises(InvalidParameterError, match=\"The 'a' parameter\"):\n            f(\"wrong\")\n", "    def test_validate_params_with_empty_constraints():\n        \"\"\"Check that validate_params works correctly with an empty constraint dictionary.\"\"\"\n        @validate_params({})\n            pass\n\n        f(1, 2)\n", "    def test_generate_valid_param_return_type(constraint):\n        \"\"\"Check that the type of the generated valid parameter matches the constraint.\"\"\"\n        value = generate_valid_param(constraint)\n        assert isinstance(value, type(generate_invalid_param_val(constraint)))\n", "    def test_validate_params_inherited_parameters():\n        \"\"\"Check that inherited parameters are validated even if not explicitly declared\n        in the subclass constructor.\"\"\"\n        est = ThirdPartyEstimator(b=[1, 2, 3])\n        with pytest.raises(\n            InvalidParameterError, match=\"The 'a' parameter of _Estimator must be\"\n        ):\n            est.fit()\n"], "sample_317": ["    def test_feed_generator_encoding(self):\n        response = self.client.get('/syndication/encoding/')\n        self.assertEqual(response['Content-Type'], 'application/rss+xml; charset=utf-8')\n", "    def test_feed_generator_feed_url_override(self):\n        response = self.client.get('/syndication/feedurl-override/')\n        doc = minidom.parseString(response.content)\n        self.assertEqual(doc.getElementsByTagName('atom:link')[0].attributes['href'].value, 'http://example.com/customfeedurl-override/') \n", "    def test_feed_generator_custom_classes(self):\n        \"\"\"\n        Make sure custom feed classes can be used.\n        \"\"\"\n        response = self.client.get('/syndication/custom_feed_classes/')\n        feed = minidom.parseString(response.content).firstChild\n\n        self.assertEqual(feed.nodeName, 'feed')\n        self.assertEqual(feed.getAttribute('xmlns'), 'http://www.w3.org/2005/Atom')\n        self.assertChildNodes(\n            feed,\n            ['title', 'subtitle', 'link', 'id', 'updated', 'entry', 'rights', 'category', 'author', 'custom_element']\n        )\n\n", "    def test_feed_generator_default(self):\n        \"\"\"\n        Test that DefaultFeed correctly selects the Rss201rev2Feed\n        as the default when no feed_generator is explicitly provided.\n        \"\"\"\n        with self.settings(FEED_GENERATOR_CLASS='django.contrib.syndication.views.DefaultFeed'):\n            response = self.client.get('/syndication/default/')\n            doc = minidom.parseString(response.content)\n            feed = doc.getElementsByTagName('rss')[0]\n            self.assertEqual(feed.getAttribute('version'), '2.0')\n\n", "    def test_feed_no_items(self):\n        \"\"\"\n        Test the structure of feeds when there are no items.\n        \"\"\"\n        response = self.client.get('/syndication/noitems/')\n        doc = minidom.parseString(response.content)\n        feed = doc.getElementsByTagName('rss')[0]\n        chan = feed.getElementsByTagName('channel')[0]\n\n        self.assertEqual(chan.getElementsByTagName('item'), [])\n"], "sample_592": ["    def test_short_data_repr_with_dask_array(self):\n        import dask.array as da\n        da_a = da.random.random((100, 100))\n\n        expected = dedent(\n            \"\"\"\\\n            [100 values with dtype=float64]\n            \"\"\"\n        )\n\n        actual = formatting.short_data_repr(da_a)\n        assert actual == expected\n", "    def test_format_items(self):\n        cases = [\n            (np.arange(4) * np.timedelta64(1, \"D\"), \"0 days 1 days 2 days 3 days\"),\n            (\n                np.arange(4) * np.timedelta64(3, \"h\"),\n                \"03:00:00 06:00:00 09:00:00 12:00:00\",\n            ),\n            (\n                np.arange(4) * np.timedelta64(500, \"ms\"),\n                \"00:00:00.500000 00:00:01 00:00:01.500000 00:00:02\",\n            ),\n            (pd.to_timedelta([\"NaT\", \"0s\", \"1s\", \"NaT\"]), \"NaT 00:00:00 00:00:01 NaT\"),\n            (\n                pd.to_timedelta([\"1 day 1 hour\", \"1 day\", \"0 hours\"]),\n                \"1 days 01:00:00 1 days 00:00:00 0 days 00:00:00\",\n            ),\n            ([1, 2, 3], \"1 2 3\"),\n        ]\n        for item, expected in cases:\n            actual = \" \".join(formatting.format_items(item))\n            assert expected == actual\n", "    def test_short_data_repr_with_large_dask_array(self):\n        import dask.array as da\n\n        arr = da.random.random((1000, 1000), chunks=(100, 100))\n        actual = formatting.short_data_repr(arr)\n        assert \"1000 values\" in actual\n        assert \"dtype=float64\" in actual\n\n", "    def test_short_data_repr_dask_array(self):\n        import dask.array as da\n\n        da_array = da.random.random((100, 100), chunks=(50, 50))\n        expected = \"[\\n    ... [100 values with dtype=float64]\\n]\"\n        actual = formatting.short_data_repr(da_array)\n        assert expected == actual\n", "    def test_summarize_attr_bytes(self):\n        expected = \"    key: b'key_value'\"\n        actual = formatting.summarize_attr(\"key\", b\"key_value\")\n        assert actual == expected\n"], "sample_714": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_773": ["    def test_logistic_regression_penalty_parameter(penalty):\n        # Test the penalty parameter in LogisticRegression\n\n        X, y = make_classification(n_samples=100, n_features=10,\n                                    random_state=42)\n\n        lr = LogisticRegression(penalty=penalty, solver='lbfgs',\n                                random_state=42)\n        lr.fit(X, y)\n        assert lr.penalty == penalty\n", "    def test_logistic_regression_coef_positive():\n        # Test if LogisticRegression with positive class weights produce\n        # positive coefficients.\n        X = np.array([[1, 2], [3, 4], [5, 6]]).reshape((-1, 2))\n        y = np.array([0, 1, 0])\n        weights = {1: 10}\n        lr = LogisticRegression(class_weight=weights, solver='lbfgs',\n                                random_state=42)\n        lr.fit(X, y)\n        assert np.all(lr.coef_[0] >= 0)\n\n", "    def test_LogisticRegressionCV_multiclass_warning(self):\n        # Check warning when multi_class is deprecated\n        X, y = make_classification(n_samples=200, n_classes=3,\n                                   random_state=0)\n\n        with pytest.warns(FutureWarning,\n                         match=\"The 'multi_class' parameter is deprecated\"):\n            LogisticRegressionCV(penalty='l2', solver='lbfgs',\n                                 multi_class='multinomial').fit(X, y)\n", "    def test_logistic_regression_warm_start_fit_intercept(self):\n        X, y = make_classification(n_samples=100, random_state=0)\n        lr = LogisticRegression(warm_start=True, fit_intercept=False)\n        lr.fit(X, y)\n        initial_coef = lr.coef_.copy()\n        lr.fit(X, y)\n        assert np.allclose(lr.coef_, initial_coef)\n        lr = LogisticRegression(warm_start=True, fit_intercept=True)\n        lr.fit(X, y)\n        initial_coef = lr.coef_.copy()\n        lr.fit(X, y)\n        assert np.allclose(lr.coef_, initial_coef)\n\n", "    def test_logistic_regression_predict_proba_ovr_consistency():\n        # Test that predict_proba for ovr is consistent with binary\n        # classification when only two classes are present.\n\n        X = iris.data\n        y = iris.target\n        y[y != 0] = 1\n\n        lr_ovr = LogisticRegression(multi_class='ovr', solver='lbfgs')\n        lr_bin = LogisticRegression(solver='lbfgs')\n\n        lr_ovr.fit(X, y)\n        lr_bin.fit(X, y)\n\n        assert_allclose(lr_ovr.predict_proba(X), lr_bin.predict_proba(X))\n"], "sample_904": ["def test_productionlist_with_empty_rule(app):\n    text = (\".. productionlist:: P2\\n\"\n            \"   A:   \\n\"\n            \"   B: `P1:B` `~P1:B`\\n\")\n    doctree = restructuredtext.parse(app, text)\n    refnodes = list(doctree.findall(pending_xref))\n    assert_node(refnodes[0], pending_xref, reftarget=\"P2:B\")\n    assert_node(refnodes[1], pending_xref, reftarget=\"P1:B\")\n    assert_node(refnodes[2], pending_xref, reftarget=\"P1:B\")\n", "def test_productionlist_empty(app):\n    text = \".. productionlist:: Empty\\n\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, nodes.section)\n    assert doctree[0].children == []\n", "def test_glossary_with_definition_list(app):\n    text = (\".. glossary::\\n\"\n            \"\\n\"\n            \"   term1\\n\"\n            \"       :definition:\\n\"\n            \"         definition list item 1\\n\"\n            \"         definition list item 2\\n\")\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (\n        [glossary, definition_list, ([definition_list_item, ([term, (\"term1\",\n                                                index)],\n                                [definition, ([nodes.paragraph, \"definition list item 1\"],\n                                               [nodes.paragraph, \"definition list item 2\"])])])],\n    ))\n", "def test_glossary_duplicate_term_different_classifier(app, warning):\n    text = (\".. glossary::\\n\"\n            \"\\n\"\n            \"   term1 : classifier1\\n\"\n            \"       description\\n\"\n            \"   term1 : classifier2\\n\"\n            \"       description\\n\"\n            )\n    restructuredtext.parse(app, text)\n    assert (\"case1.rst:4: WARNING: duplicate term description of term1, \"\n            \"other instance in case1\" in warning.getvalue())\n\n", "compilation error"], "sample_779": ["    def test_check_class_weight_balanced_linear_classifier():\n        # tests check_class_weight_balanced_linear_classifier \n        # with various input and parameter combinations\n\n        # check if balanced weights are properly handled\n\n        msg = \"Estimator does not implement a 'class_weight' parameter\"\n        assert_raises_regex(TypeError, msg, check_class_weight_balanced_linear_classifier,\n                                BaseBadClassifier())\n        check_class_weight_balanced_linear_classifier(\n            \"test\", BadBalancedWeightsClassifier())\n", "def test_check_class_weight_balanced_linear_classifier():\n    # Check that check_class_weight_balanced_linear_classifier\n    # raises an error when the estimator does not support 'balanced'\n    # class_weight\n    msg = (\"Estimator BadBalancedWeightsClassifier does not seem to support\"\n           \" 'balanced' class_weight.\")\n\n    assert_raises_regex(AssertionError, msg,\n                        check_class_weight_balanced_linear_classifier,\n                        BadBalancedWeightsClassifier)\n", "    def test_check_balanced_weights_classifier():\n        # Test that check_estimator correctly handles estimators with balanced\n        # class weights.\n        msg = (\"Estimator BadBalancedWeightsClassifier raises error when \"\n               \"balanced class_weight is passed to fit.\")\n        assert_raises_regex(AssertionError, msg,\n                            check_estimator,\n                            BadBalancedWeightsClassifier)\n", "def test_check_class_weight_balanced_linear_classifier():\n    # test that check_class_weight_balanced_linear_classifier\n    # works as expected\n    msg = (\"BadBalancedWeightsClassifier doesn't raise ValueError \"\n           \"when 'class_weight' is set to 'balanced' for a \"\n           \"multi-class classification problem.\")\n    assert_raises_regex(AssertionError, msg,\n                        check_class_weight_balanced_linear_classifier,\n                        BadBalancedWeightsClassifier)\n\n", "    def test_check_estimator_class_weight_balanced(self):\n        # Check that estimators with 'class_weight='balanced' in their\n        # __init__ are validated and raise an error if balanced class weights\n        # are modified\n        class_weight_options = ['balanced']\n        estimator = BadBalancedWeightsClassifier(class_weight='balanced')\n        msg = (\"Estimator BadBalancedWeightsClassifier doesn't \"\n               \"raise an error when 'class_weight' is set to 'balanced'\")\n        assert_raises_regex(AssertionError, msg,\n                            check_class_weight_balanced_linear_classifier,\n                            estimator, class_weight_options)\n"], "sample_232": ["    def test_key_transform_with_subquery(self):\n        subquery = JSONModel.objects.values(\n            'value__d__0',\n        ).annotate(\n            count=Count('value__d__0'),\n        ).filter(count=1)\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__d__0__in=Subquery(subquery),\n            ),\n            [self.objs[4]],\n        )\n", "    def test_key_transform_with_function_lookup(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(\n                value__foo=KeyTransform('foo', Function('Lower', value='value')),\n            ),\n            [self.objs[7]],\n        )\n", "    def test_key_transform_null_value(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__k__l=KeyTransform('l', NullableJSONModel.objects.filter(pk=OuterRef('pk')).values('value'))),\n            [],\n        )\n", "compilation error", "    def test_key_transform_length(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__baz__length=2),\n            [self.objs[7]],\n        )\n"], "sample_235": ["    def test_multiple_hooks_registered_in_single_transaction(self):\n        with transaction.atomic():\n            transaction.on_commit(lambda: self.notify(1))\n            transaction.on_commit(lambda: self.notify(2))\n            transaction.on_commit(lambda: self.notify(3))\n\n        self.assertDone([])\n", "    def test_hooks_in_nested_transactions(self):\n        with transaction.atomic():\n            self.do(1)\n            with transaction.atomic():\n                self.do(2)\n        self.assertDone([1, 2])\n\n", "    def test_on_commit_inside_savepoint(self):\n        with transaction.atomic():\n            with transaction.atomic():\n                self.do(1)\n                transaction.on_commit(lambda: self.notify(2))\n                self.assertNotified([])\n            self.assertDone([1, 2])\n\n", "    def test_hooks_are_cleared_after_savepoint_rollback(self):\n        with transaction.atomic():\n            self.do(1)\n            transaction.savepoint()\n            try:\n                self.do(2)\n                raise ForcedError()\n            except ForcedError:\n                pass\n            self.do(3)\n        self.assertDone([1, 3])\n\n", "    def test_notified_callback_on_commit_and_rollback_inside_atomic(self):\n            self.notify('inside_commit')\n\n        with transaction.atomic():\n            transaction.on_commit(on_commit)\n            try:\n                with transaction.atomic():\n                    transaction.on_commit(lambda: self.notify('inside_nested_atomic'))\n                    raise ForcedError()\n            except ForcedError:\n                pass\n            self.do(1)\n\n        self.assertDone([1])\n        self.assertNotified(['inside_commit'])\n\n"], "sample_837": ["def test_show_versions():\n    show_versions()\n", "def test_show_versions():\n    captured_output = io.StringIO()\n    sys.stdout = captured_output\n    show_versions()\n    sys.stdout = sys.__stdout__\n    output = captured_output.getvalue()\n    assert \"System:\" in output\n    assert \"BLAS:\" in output\n    assert \"Python deps:\" in output\n", "def test_show_versions():\n    show_versions()\n", "def test_show_versions():\n    show_versions()\n", "def test_show_versions():\n    show_versions()\n"], "sample_1129": ["compilation error", "def test_expm1():\n    from sympy import expm1\n\n    expr = expm1(x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'scipy.special.expm1(x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == 'numpy.expm1(x)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr) == 'mpmath.expm1(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == 'math.exp(x) - 1'\n", "def test_issue_20523():\n    from sympy import elliptic_k\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(elliptic_k(x)) == '  # Not supported in Python with NumPy:\\n  # elliptic_k\\nelliptic_k(x)'\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(elliptic_k(x)) == 'scipy.special.ellipk(x)'\n", "def test_expm1_log1p():\n    from sympy import expm1, log1p\n\n    expr1 = expm1(x)\n    expr2 = log1p(x)\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr1) == 'numpy.expm1(x)'\n    assert prntr.doprint(expr2) == 'numpy.log1p(x)'\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr1) == 'numpy.expm1(x)'\n    assert prntr.doprint(expr2) == 'numpy.log1p(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr1) == 'math.exp(x) - 1'\n    assert prntr.doprint(expr2) == 'math.log(x + 1)'\n\n", "compilation error"], "sample_1156": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_613": ["    def test_resample_datetime_overflow(self):\n        # https://github.com/pydata/xarray/issues/4569\n        times = pd.date_range('2000-01-01', freq='M', periods=12)\n        arr = DataArray(np.arange(12), coords={'time': times}, dims='time')\n        expected = arr.resample(time='3M').mean()\n\n        # This used to raise an OverflowError\n        actual = arr.resample(time='Y').mean()\n        assert_identical(expected, actual)\n", "    def test_ds_resample_apply_func_no_args(self):\n            return x.mean(\"time\")\n\n        times = pd.date_range(\"2000\", freq=\"D\", periods=3)\n        ds = xr.Dataset({\"foo\": (\"time\", [1.0, 1.0, 1.0]), \"time\": times})\n        expected = xr.Dataset({\"foo\": (\"time\", [1.0, 1.0, 1.0]), \"time\": times})\n        actual = ds.resample(time=\"D\").map(func)\n        assert_identical(expected, actual)\n", "compilation error", "    def test_resample_ds_apply_func_kwargs(self):\n            return arg1.mean(\"time\") + arg2 + kwargs[\"arg3\"]\n\n        times = pd.date_range(\"2000\", freq=\"D\", periods=3)\n        ds = xr.Dataset({\"foo\": (\"time\", [1.0, 1.0, 1.0]), \"time\": times})\n        expected = xr.Dataset({\"foo\": (\"time\", [3.0, 3.0, 3.0]), \"time\": times})\n        actual = ds.resample(time=\"D\").map(func, args=(1.0,), kwargs={\"arg3\": 1.0})\n        assert_identical(expected, actual)\n\n", "    def test_resample_with_timedelta_index(self):\n        times = pd.timedelta_range(\"0 days\", periods=10, freq=\"6H\")\n        ds = Dataset({\"foo\": (\"time\", np.arange(10)), \"time\": times})\n\n        actual = ds.resample(time=\"1D\").mean()\n        expected = ds.groupby(ds.time.dt.floor(\"D\")).mean()\n        assert_identical(expected, actual)\n"], "sample_711": ["def test_file_collect_with_symlinked_dirs(pytester: Pytester, tmp_path: Path) -> None:\n    \"\"\"Test that collection works when symlinks point to directories.\"\"\"\n    real_dir = tmp_path / \"real_dir\"\n    real_dir.mkdir()\n    (real_dir / \"test_file.py\").touch()\n    link_dir = tmp_path / \"link_dir\"\n    link_dir.symlink_to(real_dir)\n    \n    result = pytester.runpytest(str(tmp_path))\n    result.assert_outcomes(passed=1)\n    assert result.session.items[0].filename == str(real_dir / \"test_file.py\")\n", "def test_node_get_closest_marker_returns_correct_markers(pytester: Pytester) -> None:\n    p = pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.foo\n            pass\n\n        class TestClass:\n            @pytest.mark.bar\n                pass\n    \"\"\"\n    )\n    items = pytester.getitems(p)\n    assert items[0].get_closest_marker(\"foo\") is not None\n    assert items[1].get_closest_marker(\"bar\") is not None\n    assert items[0].get_closest_marker(\"bar\", default=\"default\") == \"default\"\n", "def test_Node_get_closest_marker(pytester: Pytester) -> None:\n    \"\"\"Tests that get_closest_marker correctly retrieves markers from parents.\"\"\"\n    pytest_file = pytester.makefile(\n        \".py\",\n        test_get_closest_marker=\"\"\"\n        import pytest\n\n        @pytest.mark.slow\n            pass\n\n        class TestClass:\n            @pytest.mark.fast\n                pass\n    \"\"\",\n    )\n    pytester.runpytest(\n        \"-v\",\n        pytest_file,\n        \"--markers\",\n        \"slow: requires_slow\",\n        \"--markers\",\n        \"fast: requires_fast\",\n    )\n    items = pytester.getnodes(pytest_file)\n\n    slow_marker = items[0].get_closest_marker(\"slow\")\n    assert slow_marker and slow_marker.name == \"slow\"\n\n\n", "compilation error", "def test_get_closest_marker_defaults_to_none(pytester: Pytester) -> None:\n    items = pytester.getitems(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.slow\n            pass\n    \"\"\"\n    )\n    assert items[0].get_closest_marker(\"fast\") is None\n\n"], "sample_1114": ["def test_issue_18081b():\n    assert imageset(Lambda(n, 2*pi*n), S.Integers).intersect(\n        imageset(Lambda(n, 3*pi*n), S.Integers)\n    ) == ImageSet(Lambda(n, 6*pi*n), S.Integers)\n", "compilation error", "def test_ComplexRegion_empty_set():\n    assert ComplexRegion(S.EmptySet).is_empty\n    assert ComplexRegion(S.EmptySet).measure == 0\n    assert ComplexRegion(Interval(1,2) - Interval(1,2)).is_empty\n", "def test_ComplexRegion_with_inf():\n    unit_disk = ComplexRegion(Interval(0, 1)*Interval(0, 2*S.Pi), polar=True)\n    assert unit_disk.contains(oo) == False\n    assert unit_disk.contains(-oo) == False\n    assert unit_disk.contains(I*oo) == False\n    assert unit_disk.contains(-I*oo) == False\n", "compilation error"], "sample_437": ["    def test_close_if_unusable_or_obsolete_age_limit(self):\n        self.patch_settings_dict(conn_health_checks=True)\n        self.assertIsNone(connection.connection)\n        # Newly created connections are considered healthy without performing\n        # the health check.\n        with patch.object(connection, \"is_usable\", side_effect=AssertionError):\n            self.run_query()\n\n        old_connection = connection.connection\n        # Simulate setting connection age limit.\n        connection.close_at = time.monotonic() + 1\n        # Simulate request_finished.\n        connection.close_if_unusable_or_obsolete()\n        # The connection is not closed because the age limit hasn't been reached\n        # yet, even though health checks are enabled.\n        self.assertIs(old_connection, connection.connection)\n\n        # Simulate time passing to exceed the age limit.\n        time.sleep(2)\n\n        # Simulate request_started.\n        connection.close_if_unusable_or_obsolete()\n        # The connection is closed because the age limit was reached.\n        self.assertIsNot(old_connection, connection.connection)\n\n\n\n", "    def test_on_commit_runs_after_health_check(self):\n        self.patch_settings_dict(conn_health_checks=True)\n        self.assertIsNone(connection.connection)\n        # Newly created connections are considered healthy without performing\n        # the health check.\n        with patch.object(connection, \"is_usable\", side_effect=AssertionError):\n            # Simulate outermost atomic block: changing autocommit for\n            # a connection.\n            connection.set_autocommit(False)\n            self.run_query()\n\n        old_connection = connection.connection\n        # Simulate connection health check failing.\n        with patch.object(\n            connection, \"is_usable\", return_value=False\n        ) as mocked_is_usable:\n            # Simulate on_commit callback execution.\n                self.assertEqual(connection.connection, old_connection)\n\n            connection.on_commit(on_commit_callback)\n            connection.commit()\n\n        self.assertEqual(mocked_is_usable.call_count, 1)\n", "    def test_transaction_management_with_health_checks(self):\n        self.patch_settings_dict(conn_health_checks=True)\n        self.assertIsNone(connection.connection)\n\n        # Simulate connection health check failing.\n        with patch.object(\n            connection, \"is_usable\", return_value=False\n        ) as mocked_is_usable:\n            with transaction.atomic():\n                self.run_query()\n\n        # A new connection is established within the atomic block.\n        self.assertEqual(mocked_is_usable.call_count, 1)\n        self.assertIsNotNone(connection.connection)\n\n        # Simulate connection health check succeeding.\n        with patch.object(connection, \"is_usable\", return_value=True):\n            with transaction.atomic():\n                self.run_query()\n        # The connection is reused.\n        self.assertEqual(mocked_is_usable.call_count, 1)\n\n", "    def test_health_checks_disabled_errors_occurred(self):\n        self.patch_settings_dict(conn_health_checks=False)\n        self.assertIsNone(connection.connection)\n        # Newly created connections are considered healthy without performing\n        # the health check.\n        with patch.object(connection, \"is_usable\", side_effect=AssertionError):\n            self.run_query()\n\n        old_connection = connection.connection\n        # Simulate errors_occurred.\n        connection.errors_occurred = True\n        # Simulate request_started (the connection is healthy).\n        connection.close_if_unusable_or_obsolete()\n        # Persistent connections are enabled.\n        self.assertIs(old_connection, connection.connection)\n        # No additional health checks after the one in\n        # close_if_unusable_or_obsolete() are executed during this \"request\"\n        # when running queries.\n        with patch.object(connection, \"is_usable\", side_effect=AssertionError):\n            self.run_query()\n", "    def test_set_autocommit_health_checks_disabled(self):\n        self.patch_settings_dict(conn_health_checks=False)\n        self.assertIsNone(connection.connection)\n        # Newly created connections are considered healthy without performing\n        # the health check.\n        with patch.object(connection, \"is_usable\", side_effect=AssertionError):\n            # Simulate outermost atomic block: changing autocommit for\n            # a connection.\n            connection.set_autocommit(False)\n            self.run_query()\n            connection.commit()\n            connection.set_autocommit(True)\n\n        old_connection = connection.connection\n        # Simulate request_finished.\n        connection.close_if_unusable_or_obsolete()\n        # Persistent connections are enabled.\n        self.assertIs(old_connection, connection.connection)\n        # Health checks are not performed.\n        with patch.object(connection, \"is_usable\", side_effect=AssertionError):\n            # Simulate outermost atomic block: changing autocommit for\n            # a connection.\n            connection.set_autocommit(False)\n            self.run_query()\n            connection.commit()\n            connection.set_autocommit(True)\n            # Health check wasn't performed and the connection is unchanged.\n            self.assertIs(old_connection, connection.connection)\n"], "sample_354": ["    def test_createsuperuser_command_suggested_username_with_database_option(self, mock_get_default_username):\n        mock_get_default_username.return_value = 'suggested_username'\n        new_io = StringIO()\n\n        @mock_inputs({'password': 'nopasswd', 'username': '', 'email': ''})\n            call_command(\n                'createsuperuser',\n                interactive=True,\n                database='other',\n                stdin=MockTTY(),\n                verbosity=0,\n                stdout=new_io,\n            )\n            command_output = new_io.getvalue().strip()\n            self.assertIn('suggested_username', command_output)\n        test(self)\n", "    def test_create_permissions_with_custom_model(self):\n        class CustomUser(models.Model):\n            username = models.CharField(max_length=150, unique=True)\n\n            class Meta:\n                permissions = (\n                    ('custom_permission', 'Custom Permission'),\n                )\n        \n        # Add the custom model to the auth app\n        self.app_config.models = {\n            'CustomUser': CustomUser\n        }\n\n        create_permissions(self.app_config,  verbosity=0)\n        self.assertEqual(Permission.objects.filter(codename='custom_permission').count(), 1)\n", "    def test_validate_password_against_required_fields_via_option_blank(self):\n        new_io = StringIO()\n        first_name = ''\n        entered_passwords = [\n            first_name, first_name,\n            'superduperunguessablepassword', 'superduperunguessablepassword',\n        ]\n\n            return entered_passwords.pop(0)\n\n        @mock_inputs({\n            'password': bad_then_good_password,\n            'bypass': 'n',\n        })\n            call_command(\n                'createsuperuser',\n                interactive=True,\n                first_name=first_name,\n                date_of_birth='1970-01-01',\n                email='joey@example.com',\n                stdin=MockTTY(),\n                stdout=new_io,\n                stderr=new_io,\n            )\n            self.assertEqual(\n                new_io.getvalue().strip(),\n                \"This password is too similar to the first name.\\n\"\n                'Superuser created successfully.'\n            )\n\n        test(self)\n", "    def test_create_permissions_with_custom_permissions(self):\n        Permission._meta.permissions = [\n            ('can_add_user', 'Add User'),\n        ]\n        create_permissions(self.app_config, verbosity=0)\n        self.assertTrue(Permission.objects.filter(codename='can_add_user').exists())\n", "    def test_createsuperuser_with_existing_user_in_other_db(self):\n        \"\"\"\n        Ensure no error arises when creating a superuser with the same username as\n        an existing user in a different database.\n        \"\"\"\n        User.objects.using('other').create_superuser(\n            username='existing_user',\n            email='existing@example.com',\n            password='password123',\n        )\n        new_io = StringIO()\n        call_command(\n            'createsuperuser',\n            interactive=False,\n            username='existing_user',\n            email='another_email@example.com',\n            database='default',\n            stdout=new_io,\n        )\n        output = new_io.getvalue().strip()\n        self.assertEqual(output, 'Superuser created successfully.')\n        self.assertEqual(User.objects.using('default').count(), 1)\n"], "sample_1014": ["def test_ndim_array_reshape_with_negative_dimensions():\n    a = ImmutableDenseNDimArray([1, 2, 3, 4, 5, 6], (2, 3))\n    with raises(ValueError):\n        a.reshape((-1, 2))\n\n    a = ImmutableDenseNDimArray([1, 2, 3, 4, 5, 6], (2, 3))\n    with raises(ValueError):\n        a.reshape((2, -1))\n", "def test_transpose():\n\n    a = ImmutableDenseNDimArray([1, 2, 3, 4], (2, 2))\n    at = a.transpose()\n    assert at == ImmutableDenseNDimArray([1, 3, 2, 4], (2, 2))\n\n    b = ImmutableSparseNDimArray([1, 2, 3, 4], (2, 2))\n    bt = b.transpose()\n    assert bt == ImmutableSparseNDimArray([1, 3, 2, 4], (2, 2))\n", "def test_free_symbols():\n        a = ImmutableDenseNDimArray([[Symbol('x'), Symbol('y')], [Symbol('z'), Symbol('w')]])\n        assert a.free_symbols == {Symbol('x'), Symbol('y'), Symbol('z'), Symbol('w')}\n\n        b = ImmutableSparseNDimArray([[Symbol('x'), Symbol('y')], [Symbol('z'), Symbol('w')]])\n        assert b.free_symbols == {Symbol('x'), Symbol('y'), Symbol('z'), Symbol('w')}\n\n        c = ImmutableDenseNDimArray([Symbol('x'), 2, 3, Symbol('w')])\n        assert c.free_symbols == {Symbol('x'), Symbol('w')}\n", "def test_reshape_with_symbolic_index():\n    from sympy.abc import i, j\n    M = ImmutableDenseNDimArray([[1, 2], [3, 4]])\n    assert M.reshape(4, 1)[i, j] == M[i // 4, i % 4]\n\n    M = ImmutableSparseNDimArray([[1, 2], [3, 4]])\n    assert M.reshape(4, 1)[i, j] == M[i // 4, i % 4]\n", "def test_indexing_assignment():\n    m = ImmutableDenseNDimArray([1, 2, 3, 4], (2, 2))\n    raises(TypeError, lambda: m[0, 0] = 5)\n\n    m = MutableDenseNDimArray([1, 2, 3, 4], (2, 2))\n    m[0, 0] = 5\n    assert m[0, 0] == 5\n    assert m == MutableDenseNDimArray([5, 2, 3, 4], (2, 2))\n\n    m = MutableDenseNDimArray([[1, 2], [3, 4]])\n    m[0, :] = [5, 6]\n    assert m == MutableDenseNDimArray([[5, 6], [3, 4]])\n    m[:, 1] = [7, 8]\n    assert m == MutableDenseNDimArray([[5, 7], [3, 8]])\n\n    raises(IndexError, lambda: m[2, 0] = 9)\n    raises(IndexError, lambda: m[0, 2] = 9)\n\n    ms = ImmutableSparseNDimArray([1, 2, 3, 4], (2, 2))\n    raises(TypeError, lambda: ms[0, 0] = 5)\n"], "sample_988": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_367": ["    def test_cache_page_decorator_middleware_compatibility(self):\n        \"\"\"\n        Ensure cache_page decorator plays nicely with other middleware.\n        \"\"\"\n        class CustomMiddleware:\n                self.get_response = get_response\n                response = self.get_response(request)\n                response['X-Custom-Header'] = 'Value'\n                return response\n\n        @cache_page(60)\n            return HttpResponse('Cached content')\n\n        middleware = CustomMiddleware\n\n        request = HttpRequest()\n        response = a_view(request)\n\n        self.assertIn('X-Custom-Header', response.headers)\n        self.assertEqual(response.headers['X-Custom-Header'], 'Value')\n", "    def test_cache_page_decorator_cache_alias(self):\n        @cache_page(60 * 15, cache='my_cache_alias')\n            return HttpResponse()\n        response = a_view(HttpRequest())\n        self.assertEqual(response.cache_alias, 'my_cache_alias')\n", "    def test_cache_control_decorator(self):\n        @cache_control(max_age=3600, must_revalidate=True, s_maxage=7200)\n            return HttpResponse()\n\n        response = view(HttpRequest())\n        self.assertEqual(response.headers['Cache-Control'], 'max-age=3600, must-revalidate, s-maxage=7200')\n", "    def test_cache_control_decorator_multiple_calls(self):\n        class MyClass:\n            @cache_control(max_age=100, private=True)\n                return HttpResponse()\n\n        request = HttpRequest()\n        response = MyClass().a_view(request)\n\n        self.assertEqual(response.headers['Cache-Control'], 'max-age=100, private')\n\n        response = MyClass().a_view(request)\n\n        self.assertEqual(response.headers['Cache-Control'], 'max-age=100, private')\n\n", "    def test_cache_control_decorator_attributes(self):\n        @cache_control(public=True, max_age=3600)\n            return HttpResponse()\n        response = a_view(HttpRequest())\n        self.assertEqual(response.headers['Cache-Control'], 'public, max-age=3600')\n"], "sample_462": ["    def test_choicefield_empty_values(self):\n        f = ChoiceField(choices=[(\"1\", \"One\"), (\"2\", \"Two\")], required=False, empty_values=(\"Choose...\",))\n        self.assertEqual('Choose...', f.clean('Choose...'))\n", "    def test_choicefield_empty_values(self):\n        f = ChoiceField(choices=[(\"1\", \"One\"), (\"2\", \"Two\")], required=False)\n        self.assertEqual(\"\", f.clean(\"\"))\n        self.assertEqual(None, f.clean(None))\n        self.assertEqual(\"1\", f.clean(\" 1 \"))\n", "    def test_choicefield_empty_values(self):\n        f = ChoiceField(choices=[(\"1\", \"One\"), (\"2\", \"Two\")], required=False,\n                       empty_values=['', 'None', None])\n        self.assertEqual(\"\", f.clean(\"\"))\n        self.assertEqual(\"\", f.clean(\"None\"))\n        self.assertEqual(\"\", f.clean(None))\n        self.assertEqual(\"1\", f.clean(\"1\"))\n", "    def test_choicefield_empty_value(self):\n        f = ChoiceField(choices=[(\"1\", \"One\"), (\"2\", \"Two\")], empty_value=\"Select\")\n        self.assertEqual(\"Select\", f.clean(\"Select\"))\n", "    def test_choicefield_invalid_choice(self):\n        f = ChoiceField(choices=[(\"1\", \"One\"), (\"2\", \"Two\")])\n        with self.assertRaisesMessage(\n            ValidationError, \"'Select a valid choice. 3 is not one of the available choices.'\"\n        ):\n            f.clean(\"3\")\n        self.assertEqual(f.error_messages['invalid_choice'], 'Select a valid choice. %(value)s is not one of the available choices.')\n"], "sample_1008": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_721": ["    def test_check_array_dtype_object_copy(self):\n        X = np.array([['a', 1], ['b', 2]], dtype=object)\n        # Should copy if dtype is object\n        assert_not_equal(check_array(X).flags.writeable, X.flags.writeable)\n\n", "    def test_check_array_ignore_dtype(self):\n        X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=np.object)\n        X_checked = check_array(X, dtype=np.float64, force_all_finite='allow-nan', warn_on_dtype=False)\n        assert_equal(X_checked.dtype, np.float64)\n        assert_warns(DataConversionWarning, check_array, X, dtype=np.float64, force_all_finite='allow-nan', warn_on_dtype=True)\n", "    def test_check_array_dtype_object_no_warning(self):\n        # Test that object dtype arrays are not converted to float\n        arr = np.array([[1, 2, 3], [4.0, 5, 6], [7, 8, 'a']], dtype=object)\n        result = check_array(arr)\n        assert_equal(result.dtype.kind, 'O')\n        assert_equal(result.tolist(), arr.tolist())\n\n", "    def test_check_array_large_sparse_matrix():\n        # Test that check_array can handle very large sparse matrices\n        # without crashing or taking too long.\n        size = 10000\n        density = 0.01\n        X = sp.rand(size, size, density=density)\n\n        assert_warns(UserWarning, check_array, X, accept_sparse=True)\n        assert_array_equal(check_array(X, accept_sparse=True).toarray(), X.toarray())\n", "compilation error"], "sample_769": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_703": ["def test_dollar_prefix(expr: str) -> None:\n    matcher = {\"true\": True, \"false\": False}.__getitem__\n    assert evaluate(expr, matcher)\n\n", "def test_leading_trailing_spacing(expr: str, expected: bool) -> None:\n    matcher = {\"test\": True, \"test2\": False}.__getitem__\n    assert evaluate(expr, matcher) == expected\n", "def test_dollar_in_ident(ident: str) -> None:\n    assert evaluate(ident, {ident: True}.__getitem__)\n", "    def test_invalid_idents(ident: str) -> None:\n        with pytest.raises(ParseError) as excinfo:\n            evaluate(ident, {ident: True}.__getitem__)\n        assert excinfo.value.column == len(ident) + 1\n        assert excinfo.value.message == \"expected not OR left parenthesis OR identifier; got end of input\"\n\n", "def test_unexpected_character() -> None:\n    with pytest.raises(ParseError) as excinfo:\n        evaluate(\"a$\", lambda ident: True)\n    assert excinfo.value.column == 2\n    assert excinfo.value.message == \"unexpected character \"$\"\n"], "sample_338": ["    def test_custom_operation_name(self):\n        class MyCustomOperation(migrations.Operation):\n                return 'Custom operation'\n\n        class Migration(migrations.Migration):\n            operations = [\n                MyCustomOperation(),\n                migrations.CreateModel('Person', fields=[]),\n            ]\n\n        migration = Migration('some_migration', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'custom_operation_person')\n", "    def test_custom_migration_name(self):\n        class Migration(migrations.Migration):\n            dependencies = [\n                ('otherapp', '0001_initial'),\n            ]\n            operations = [\n                migrations.CreateModel(\n                    'Book',\n                    fields=[\n                        ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                        ('title', models.CharField(max_length=200)),\n                        ('author', models.ForeignKey('testapp.Author', on_delete=models.CASCADE)),\n                    ],\n                    options={\n                        'verbose_name': 'Book',\n                        'verbose_name_plural': 'Books',\n                    },\n                ),\n            ]\n            name = 'custom_name'\n\n        migration = Migration('0002_custom_name', 'testapp')\n        self.assertEqual(migration.suggest_name(), 'custom_name')\n", "    def test_squashed_migration(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel('Person', fields=[]),\n                migrations.CreateModel('Animal', fields=[]),\n            ]\n            # We are going to simulate a squashed migration using the 'replaces' attribute\n            replaces = ['0001_initial']\n\n        migration = Migration('0002_some_name', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'person_animal')\n", "    def test_empty_migration(self):\n        class Migration(migrations.Migration):\n            operations = []\n\n        migration = Migration('0001_empty', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'empty')\n", "    def test_squashed_migration_suggests_name(self):\n        class Migration(migrations.Migration):\n            operations = [migrations.CreateModel('Person', fields=[]), migrations.AddField('Person', 'age', models.IntegerField(),)]\n            squashed = True\n\n        migration = Migration('0001_initial', 'test_app')\n        self.assertEqual(migration.suggest_name(), 'person')\n"], "sample_713": ["    def test_ridge_classifer_predict_proba(self):\n        # Test predict_proba for RidgeClassifier\n        X = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0],\n                      [1.0, 1.0], [1.0, 0.0]])\n        y = np.array([1, 1, 1, -1, -1])\n        clf = RidgeClassifier()\n        clf.fit(X, y)\n        probas = clf.predict_proba(X)\n        assert probas.shape == (len(X), 2)\n        assert np.allclose(np.sum(probas, axis=1), 1)\n", "compilation error", "    def test_ridge_warm_start(self):\n        X = np.random.randn(100, 10)\n        y = np.random.randn(100)\n\n        # First fit\n        ridge = Ridge(alpha=1.0, fit_intercept=True, warm_start=True)\n        ridge.fit(X, y)\n        coef1 = ridge.coef_.copy()\n\n        # Second fit with different alpha, should reuse previous solution\n        ridge.alpha = 2.0\n        ridge.fit(X, y)\n        coef2 = ridge.coef_.copy()\n\n        # Assert that the coefficients are different\n        assert not np.allclose(coef1, coef2)\n", "    def test_ridge_classifier_predict_proba(self):\n        X, y = make_classification(n_samples=100, n_features=10,\n                                   random_state=0)\n        clf = RidgeClassifier(solver='auto', alpha=1.0)\n        clf.fit(X, y)\n        proba = clf.predict_proba(X)\n        assert proba.shape == (100, 2)\n        assert_almost_equal(proba.sum(axis=1), np.ones(100))\n", "    def test_ridge_classifier_with_sample_weight(self):\n        X, y = make_classification(n_samples=100, n_features=10, random_state=0)\n        sample_weight = np.random.rand(len(y))\n\n        clf = RidgeClassifier()\n        clf.fit(X, y, sample_weight=sample_weight)\n\n        # Check that the classifier can predict\n        clf.predict(X)\n"], "sample_349": ["    def test_render_options_with_language_code(self):\n        with translation.override('de'):\n            beatles = Band.objects.create(name='The Beatles', style='rock')\n            form = AlbumForm(initial={'band': beatles.uuid})\n            output = form.as_table()\n            self.assertEqual(output, '<table class=\"form-table\"><tr><th><label for=\"id_band\">Band:</label></th><td><select name=\"band\" id=\"id_band\" class=\"my-class admin-autocomplete\" data-ajax--cache=\"true\" data-ajax--delay=\"250\" data-ajax--type=\"GET\" data-ajax--url=\"/autocomplete/\" data-theme=\"admin-autocomplete\" data-allow-clear=\"false\" data-app-label=\"admin_widgets\" data-field-name=\"band\" data-model-name=\"album\" data-placeholder=\"\" lang=\"de\"><option value=\"%s\" selected>The Beatles</option></select></td></tr></table>' % beatles.uuid)\n", "    def test_language_support(self):\n        with translation.override('de'):\n            form = AlbumForm()\n            attrs = form['band'].field.widget.get_context(name='my_field', value=None, attrs={})['widget']['attrs']\n            self.assertEqual(attrs['lang'], 'de')\n\n", "    def test_select2_translations(self):\n        with translation.override('es'):\n            form = AlbumForm()\n            attrs = form['band'].field.widget.get_context(name='band', value=None, attrs={})['widget']['attrs']\n            self.assertEqual(attrs['lang'], 'es')\n", "    def test_language_support(self):\n        with translation.override('fr'):\n            form = AlbumForm()\n            attrs = form['band'].field.widget.get_context(name='my_field', value=None, attrs={})['widget']['attrs']\n            self.assertEqual(attrs['lang'], 'fr')\n\n", "    def test_i18n(self):\n        with translation.override('fr'):  # French\n            form = AlbumForm()\n            attrs = form['band'].field.widget.get_context(name='my_field', value=None, attrs={})['widget']['attrs']\n            self.assertEqual(attrs['lang'], 'fr')\n"], "sample_190": ["    def test_related_field_ordering_with_select_related(self):\n        article = self.a1\n        self.assertEqual(\n            list(Article.objects.select_related('author').order_by('author__name')),\n            [article]\n        )\n", "    def test_isnull_lhs_exists(self):\n        qs = Article.objects.filter(\n            pk__isnull=Exists(Author.objects.filter(id=OuterRef('author_id')))\n        )\n        self.assertEqual(list(qs), list(Article.objects.all()))\n", "    def test_exact_query_rhs_with_selected_columns_null_value(self):\n        old_author = Author.objects.create(name='Author 1')\n        authors_max_ids = Author.objects.filter(\n            name='Author 1',\n        ).values(\n            'name',\n        ).annotate(\n            max_id=Max('id'),\n        ).values('max_id')\n        authors = Author.objects.filter(id=authors_max_ids[:1])\n        self.assertEqual(authors.get(), old_author)\n", "    def test_nested_outerref_rhs(self):\n        tag = Tag.objects.create(name=self.au1.alias)\n        tag.articles.add(self.a1)\n        qs = Tag.objects.annotate(\n            has_author_alias_match=Exists(\n                Article.objects.filter(author__alias=OuterRef('name'))\n            ),\n        )\n        self.assertEqual(qs.get(has_author_alias_match=True), tag)\n", "    def test_exact_query_rhs_with_selected_columns_multi_lookup(self):\n        newest_author = Author.objects.create(name='Author 2')\n        authors_max_ids = Author.objects.filter(\n            name='Author 2',\n        ).values(\n            'name',\n        ).annotate(\n            max_id=Max('id'),\n        ).values('max_id')\n        authors = Author.objects.filter(id__in=authors_max_ids[:1], name='Author 2')\n        self.assertEqual(authors.get(), newest_author)\n"], "sample_1001": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_582": ["    def test_cli_blueprint_registration(app, runner):\n        app.register_blueprint(custom)\n        app.register_blueprint(nested)\n        app.register_blueprint(merged)\n\n        result = runner.invoke(app.cli, [\"custom\"])\n        assert result.output == \"custom_result\\n\"\n\n        result = runner.invoke(app.cli, [\"customized\", \"nested\"])\n        assert result.output == \"nested_result\\n\"\n\n        result = runner.invoke(app.cli, [\"merged\"])\n        assert result.output == \"merged_result\\n\"\n\n        app.register_blueprint(late)\n        result = runner.invoke(app.cli, [\"late\"])\n        assert result.output == \"late_result\\n\"\n", "    def test_blueprint_cli(runner, app):\n        app.register_blueprint(custom)\n        app.register_blueprint(nested)\n        app.register_blueprint(merged)\n        app.register_blueprint(late, url_prefix='/late')\n\n        result = runner.invoke(app.cli, ['custom'])\n        assert result.exit_code == 0\n        assert result.output == \"custom_result\\n\"\n\n        result = runner.invoke(app.cli, ['nested', 'nested'])\n        assert result.exit_code == 0\n        assert result.output == \"nested_result\\n\"\n\n        result = runner.invoke(app.cli, ['merged', 'merged'])\n        assert result.exit_code == 0\n        assert result.output == \"merged_result\\n\"\n\n        result = runner.invoke(app.cli, ['late', 'late'])\n        assert result.exit_code == 0\n        assert result.output == \"late_result\\n\"\n\n", "compilation error", "def test_cli_blueprint_naming(app):\n    custom = Blueprint(\"custom\", __name__, cli_group=\"customized\")\n    nested = Blueprint(\"nested\", __name__)\n    @custom.cli.command(\"custom\")\n        click.echo(\"custom_result\")\n\n    @nested.cli.command(\"nested\")\n        click.echo(\"nested_result\")\n\n    app.register_blueprint(custom)\n    app.register_blueprint(nested)\n    runner = CliRunner()\n    result = runner.invoke(app.cli, [\"customized\", \"custom\"])\n    assert result.exit_code == 0\n    assert result.output == \"custom_result\\n\"\n\n    result = runner.invoke(app.cli, [\"nested\", \"nested\"])\n    assert result.exit_code == 0\n    assert result.output == \"nested_result\\n\"\n", "compilation error"], "sample_617": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1062": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_896": ["compilation error", "    def test_nmf_n_components_default_value_change_warning(self):\n        # Tests the warning when n_components is not provided and no init is given\n        rng = np.random.RandomState(0)\n        X = rng.random_sample((6, 5))\n        with pytest.warns(\n            FutureWarning,\n            match=\"The default value of `n_components` will change from\",\n        ):\n            NMF(random_state=0, tol=1e-6).fit(X)\n", "compilation error", "def test_nmf_init_error(Estimator):\n    rng = np.random.RandomState(0)\n    X = rng.random_sample((6, 5))\n    with pytest.raises(ValueError, match=\"Invalid init parameter\"):\n        Estimator(init=\"not_valid\").fit(X)\n", "compilation error"], "sample_433": ["    def test_rename_field_in_index_together(self):\n        initial_author = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n                (\"age\", models.IntegerField()),\n            ],\n            {\n                \"index_together\": {(\"name\", \"age\")},\n            },\n        )\n        author_renamed_field = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"author_name\", models.CharField(max_length=200)),\n                (\"age\", models.IntegerField()),\n            ],\n            {\n                \"index_together\": {(\"author_name\", \"age\")},\n            },\n        )\n        changes = self.get_changes([initial_author], [author_renamed_field])\n\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(\n            changes,\n            \"testapp\",\n            0,\n            [\"RenameField\", \"AlterIndexTogether\"],\n        )\n        self.assertOperationAttributes(\n            changes,\n            \"testapp\",\n            0,\n            1,\n            name=\"author\",\n            index_together={(\"author_name\", \"age\")},\n        )\n", "    def test_rename_model(self):\n        before = ModelState(\n            \"testapp\",\n            \"OldName\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"field\", models.CharField(max_length=200)),\n            ],\n        )\n        after = ModelState(\n            \"testapp\",\n            \"NewName\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"field\", models.CharField(max_length=200)),\n            ],\n        )\n        changes = self.get_changes([before], [after])\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(\n            changes,\n            \"testapp\",\n            0,\n            [\"RenameModel\"],\n        )\n        self.assertOperationAttributes(\n            changes,\n            \"testapp\",\n            0,\n            0,\n            old_name=\"OldName\",\n            new_name=\"NewName\",\n        )\n\n", "    def test_mixed_operations_and_none(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\"Person\", fields=[]),\n                migrations.RunSQL(\"SELECT 1 FROM person;\"),\n                migrations.DeleteModel(\"Animal\"),\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"person_delete_animal\")\n", "compilation error", "    def test_rename_field_in_index_together(self):\n        before = ModelState(\n            \"otherapp\",\n            \"Book\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n                (\"old_title\", models.CharField(max_length=200)),\n            ],\n            {\n                \"index_together\": {(\"author\", \"old_title\")},\n            },\n        )\n        after = ModelState(\n            \"otherapp\",\n            \"Book\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n                (\"title\", models.CharField(max_length=200)),\n            ],\n            {\n                \"index_together\": {(\"author\", \"title\")},\n            },\n        )\n        changes = self.get_changes([AutodetectorTests.author_empty, before],\n                                [AutodetectorTests.author_empty, after],\n                                MigrationQuestioner({\"ask_rename\": True}))\n        self.assertNumberMigrations(changes, \"otherapp\", 1)\n        self.assertOperationTypes(\n            changes,\n            \"otherapp\",\n            0,\n            [\"RenameField\", \"AlterIndexTogether\"],\n        )\n\n"], "sample_550": ["compilation error", "    def test_tool_trigger(self, tool):\n        with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n            plt.rcParams['toolbar'] = 'toolmanager'\n        fig, ax = plt.subplots()\n        tb = NavigationToolbar2(fig.canvas)\n        assert ax.get_navigate_mode() is None\n        tb.trigger_tool(tool)\n        assert ax.get_navigate_mode() == tool\n", "compilation error", "    def test_toolmanager_tool_registration(self):\n        with pytest.warns(UserWarning, match=_EXPECTED_WARNING_TOOLMANAGER):\n            plt.rcParams['toolbar'] = 'toolmanager'\n        fig = plt.gcf()\n        toolmanager = fig.canvas.manager.toolmanager\n\n        class MyTool(ToolBase):\n            name = 'MyTool'\n\n        toolmanager.add_tool(MyTool)\n\n        assert 'MyTool' in toolmanager.tools\n        assert isinstance(toolmanager.get_tool('MyTool'), MyTool)\n\n        toolmanager.remove_tool('MyTool')\n\n        assert 'MyTool' not in toolmanager.tools\n", "    def test_toolbar_navigate_mode(self, tool_name, expected_mode):\n        fig, ax = plt.subplots()\n        ax.figure.canvas.manager.toolmanager.trigger_tool(tool_name)\n        assert ax.get_navigate_mode() == expected_mode\n"], "sample_895": ["    def test_verbose_feature_names_out_false_empty(transformers, remainder, expected_names):\n        \"\"\"Check feature_names_out for verbose_feature_names_out=False\n\n        when transformer outputs are empty.\n        \"\"\"\n        pd = pytest.importorskip(\"pandas\")\n        df = pd.DataFrame([[1, 2, 3, 4]], columns=[\"a\", \"b\", \"c\", \"d\"])\n        ct = ColumnTransformer(\n            transformers,\n            remainder=remainder,\n            verbose_feature_names_out=False,\n        )\n        ct.fit(df)\n\n        names = ct.get_feature_names_out()\n        assert isinstance(names, np.ndarray)\n        assert names.dtype == object\n        assert_array_equal(names, expected_names)\n", "    def test_verbose_feature_names_out_false_with_duplicate_names(\n        transformers, remainder, expected_names", "    def test_column_transformer_set_output_with_sparse_transformer(remainder):\n        pd = pytest.importorskip(\"pandas\")\n        X = pd.DataFrame(\n            {\n                \"pet\": pd.Series([\"dog\", \"cat\", \"snake\"], dtype=\"category\"),\n                \"age\": [1.4, 2.1, 4.4],\n                \"color\": pd.Series([\"green\", \"blue\", \"red\"], dtype=\"object\"),\n                \"height\": [20, 40, 10],\n            }\n        )\n\n        ct = ColumnTransformer(\n            [\n                (\"color_encode\", OneHotEncoder(sparse_output=True), [\"color\"]),\n                (\"age\", StandardScaler(), [\"age\"]),\n            ],\n            remainder=remainder,\n            verbose_feature_names_out=False,\n        )\n\n        ct.set_output(transform=\"pandas\")\n        X_trans = ct.fit_transform(X)\n\n        assert isinstance(X_trans, pd.DataFrame)\n        assert_array_equal(X_trans.columns, ct.get_feature_names_out())\n", "compilation error", "    def test_column_transformer_with_sparse_output():\n        \"\"\"Check column transformer works with sparse output.\"\"\"\n        from scipy.sparse import csr_matrix\n\n        pd = pytest.importorskip(\"pandas\")\n\n        X = pd.DataFrame(\n            {\n                \"pet\": pd.Series([\"dog\", \"cat\", \"snake\"], dtype=\"category\"),\n                \"color\": pd.Series([\"green\", \"blue\", \"red\"], dtype=\"object\"),\n                \"age\": [1.4, 2.1, 4.4],\n            }\n        )\n        ct = ColumnTransformer(\n            [\n                (\n                    \"color_encode\",\n                    OneHotEncoder(sparse_output=True, dtype=\"int8\"),\n                    [\"color\"],\n                ),\n                (\"age\", StandardScaler(), [\"age\"]),\n            ],\n            remainder=\"drop\",\n            verbose_feature_names_out=False,\n        ).set_output(transform=\"pandas\")\n\n        X_trans = ct.fit_transform(X)\n\n        assert isinstance(X_trans, pd.DataFrame)\n        assert isinstance(X_trans.values, csr_matrix)\n"], "sample_544": ["    def test_resample_clip(fig_test, fig_ref, interp):\n        # Test if resampling clips values properly\n        data = np.array([[0, 1], [2, 3]], dtype=np.float64)\n        ax_test = fig_test.subplots()\n        ax_test.imshow(data, interpolation=interp, cmap=\"viridis\")\n        ax_ref = fig_ref.subplots()\n        ax_ref.imshow(data, interpolation=interp, cmap=\"viridis\")\n", "    def test_image_extent_and_origin(self, origin):\n        fig, ax = plt.subplots()\n        data = np.arange(4).reshape((2, 2))\n        im = ax.imshow(data, origin=origin, extent=(0, 2, 0, 2))\n        # Check that the image patch is correctly placed\n        assert im.get_extent() == (0, 2, 0, 2)\n        # Check that the pixel coordinates are consistent with the origin\n        if origin == 'upper':\n            assert im.get_pixel_aspect_ratio(0, 0) == 1\n        else:\n            assert im.get_pixel_aspect_ratio(0, 1) == 1\n\n\n", "def test_nearest_upsample():\n    fig, ax = plt.subplots()\n    arr = np.array([[1, 2], [3, 4]], dtype=np.float64)\n    img = ax.imshow(arr, interpolation='nearest', cmap='viridis')\n    ax.set_xlim(0, 3)\n    ax.set_ylim(0, 3)\n", "def test_imshow_complex():\n    # Test imshow with complex numbers, ensuring magnitude is displayed.\n    x = np.linspace(-5, 5, 100)\n    X, Y = np.meshgrid(x, x)\n    Z = X + 1j * Y\n    fig, ax = plt.subplots()\n    ax.imshow(Z)\n", "compilation error"], "sample_256": ["    def test_bug_23902_special_characters(self):\n        user = User.objects.get(username='testclient')\n        data = {'password1': 'p@$$w0rd', 'password2': 'p@$$w0rd'}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertTrue(form.is_valid())\n", "    def test_disabled_input_fields(self):\n        user = User.objects.get(username='testclient')\n        form = AdminPasswordChangeForm(user, {})\n        self.assertTrue(form.fields['password1'].widget.attrs.get('readonly'))\n        self.assertTrue(form.fields['password2'].widget.attrs.get('readonly'))\n", "    def test_custom_password_validator(self):\n        class CustomPasswordValidator(Validator):\n                if 'test' in password:\n                    raise ValidationError(_('The password cannot contain the word \"test\".'))\n\n        user = User.objects.get(username='testclient')\n        data = {'password1': 'testpassword', 'password2': 'testpassword'}\n        with self.assertRaisesMessage(ValidationError, 'The password cannot contain the word \"test\".'):\n            AdminPasswordChangeForm(user, data).full_clean()\n", "    def test_custom_user_model(self):\n        @register\n        class CustomUser(AbstractUser):\n            pass\n        with override_settings(AUTH_USER_MODEL='auth_tests.CustomUser'):\n            User = get_user_model()\n            user = User.objects.create_user(username='testuser', password='testpassword')\n            data = {'password1': 'newpassword', 'password2': 'newpassword'}\n            form = AdminPasswordChangeForm(user, data)\n            self.assertTrue(form.is_valid())\n            form.save()\n            self.assertTrue(user.check_password('newpassword'))\n\n", "    def test_no_password_provided(self):\n        user = User.objects.get(username='testclient')\n        data = {}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertIn(_(\"This field is required.\"), form.errors['password1'])\n        self.assertIn(_(\"This field is required.\"), form.errors['password2'])\n"], "sample_861": ["    def test_grid_search_n_jobs_validation():\n        # Test that n_jobs validation works as expected\n        clf = MockClassifier()\n        # Test that n_jobs is validated for both GridSearchCV\n        with pytest.raises(ValueError,\n                           match=\"n_jobs should be an integer\"):\n            GridSearchCV(clf, {'foo_param': [1, 2, 3]},\n                         n_jobs='invalid_n_jobs')\n\n        with pytest.raises(ValueError,\n                           match=\"n_jobs should be >= -1\"):\n            GridSearchCV(clf, {'foo_param': [1, 2, 3]}, n_jobs=-2)\n\n        # Test that negative n_jobs are handled correctly (use all CPUs)\n        gs_all_cpus = GridSearchCV(clf, {'foo_param': [1, 2, 3]},\n                                   n_jobs=-1)\n        gs_all_cpus.fit(X, y)\n\n        gs_one_job = GridSearchCV(clf, {'foo_param': [1, 2, 3]},\n                                  n_jobs=1)\n        gs_one_job.fit(X, y)\n\n", "    def test_grid_search_with_early_stopping(self):\n        # Test with early stopping based on validation score\n        X, y = make_classification(n_samples=100, n_features=20, random_state=0)\n        # Define a custom scorer that simulates early stopping based on val score\n            y_pred = estimator.predict(X)\n            # Simulate early stopping if validation accuracy is above a threshold\n            if np.mean(y_pred == y_true) > 0.8:\n                return float('inf')\n            else:\n                return 0\n        param_grid = {'C': [0.1, 1, 10]}\n        clf = LogisticRegression(random_state=0)\n        grid_search = GridSearchCV(clf, param_grid, scoring=early_stopping_scorer,\n                                 refit=False, cv=3)\n        grid_search.fit(X, y)\n        # Check if the grid search stopped early\n        assert grid_search.best_score_ == float('inf')\n", "    def test_gridsearchcv_n_jobs_negative():\n        # Check that n_jobs < 0 raises a ValueError\n\n        clf = LogisticRegression()\n        param_grid = {'C': [1, 10]}\n        with pytest.raises(ValueError, match='n_jobs must be >= 1'):\n            GridSearchCV(clf, param_grid, cv=2, n_jobs=-1).fit(X, y)\n\n", "def test_grid_search_with_precomputed_kernel():\n    # Check that GridSearchCV works with a precomputed kernel\n    X = np.random.randn(10, 10)\n    y = np.random.randint(0, 2, size=10)\n    kernel = np.dot(X, X.T)\n    clf = SVC(kernel='precomputed')\n    param_grid = {'C': [0.1, 1, 10]}\n\n    gs = GridSearchCV(clf, param_grid, cv=3)\n    gs.fit(kernel, y)\n\n    assert hasattr(gs, 'best_params_')\n    assert hasattr(gs, 'best_score_')\n\n", "    def test_grid_search_with_precomputed_kernel():\n        # Test GridSearchCV with precomputed kernels\n        X, y = make_classification(n_samples=50, random_state=0)\n        kernel_matrix = pairwise_kernels(X, metric='linear')\n        svm = SVC(kernel='precomputed')\n        grid_search = GridSearchCV(svm, {'C': [0.1, 1, 10]}, cv=3)\n\n        grid_search.fit(kernel_matrix, y)\n\n        assert grid_search.best_params_['C'] in [0.1, 1, 10]\n"], "sample_389": ["    def test_prefixed(self):\n        # Add SCRIPT_NAME prefix to relative paths.\n        self.set_script_name(\"/somesubpath\")\n        tests = (\n            \"/path/\",\n            \"path/\",\n            \"path\",\n        )\n        for setting in (\"MEDIA_URL\", \"STATIC_URL\"):\n            for path in tests:\n                new_settings = {setting: path}\n                with self.settings(**new_settings):\n                    for script_name in [\"/somesubpath\", \"/somesubpath/\", \"/\", \"\", None]:\n                        with self.subTest(script_name=script_name, **new_settings):\n                            try:\n                                self.set_script_name(script_name)\n                                expected = path if script_name is None else f\"{script_name}{path}\"\n                                self.assertEqual(getattr(settings, setting), expected)\n                            finally:\n                                clear_script_prefix()\n", "    def test_script_name_prefix(self):\n        tests = (\n            \"/path/\",\n            \"http://myhost.com/path/\",\n            \"http://myhost/path/\",\n            \"https://myhost/path/\",\n            None,\n        )\n        for setting in (\"MEDIA_URL\", \"STATIC_URL\"):\n            for path in tests:\n                new_settings = {setting: path}\n                with self.settings(**new_settings):\n                    self.set_script_name(\"/somesubpath\")\n                    expected = path if path is None else f\"/somesubpath{path}\"\n                    self.assertEqual(getattr(settings, setting), expected)\n", "    def test_prefixed(self):\n        # Add SCRIPT_NAME prefix to relative paths.\n        for setting in (\"MEDIA_URL\", \"STATIC_URL\"):\n            for script_name in [\"/somesubpath\", \"/somesubpath/\", \"/\", \"\"]:\n                with self.subTest(script_name=script_name, setting=setting):\n                    with self.settings(**{setting: \"path/\"}):\n                        self.set_script_name(script_name)\n                        self.assertEqual(getattr(settings, setting), script_name + \"path/\")\n                    clear_script_prefix()\n", "    def test_prefixed(self):\n        # Add SCRIPT_NAME prefix to relative paths\n        tests = (\n            \"/path/\",\n            \"/path\",\n            \"path/\",\n            \"path\",\n        )\n        for setting in (\"MEDIA_URL\", \"STATIC_URL\"):\n            for path in tests:\n                new_settings = {setting: path}\n                with self.settings(**new_settings):\n                    for script_name in [\"/somesubpath\", \"/somesubpath/\", \"/\", \"\", None]:\n                        with self.subTest(script_name=script_name, **new_settings):\n                            try:\n                                self.set_script_name(script_name)\n                                expected = f\"{script_name}{path}\" if script_name else path\n                                self.assertEqual(getattr(settings, setting), expected)\n                            finally:\n                                clear_script_prefix()\n", "    def test_prefixed_relative(self):\n        # Prefix SCRIPT_NAME to relative paths that don't start with '/'.\n\n        tests = (\n            \"path/\",\n            \"images/logo.png\",\n            \"css/style.css\",\n        )\n        for setting in (\"MEDIA_URL\", \"STATIC_URL\"):\n            for path in tests:\n                new_settings = {setting: path}\n                with self.settings(**new_settings):\n                    for script_name in [\"/somesubpath\", \"/somesubpath/\", \"/\", \"\", None]:\n                        with self.subTest(script_name=script_name, **new_settings):\n                            try:\n                                self.set_script_name(script_name)\n                                expected = script_name + path if script_name else path\n                                self.assertEqual(getattr(settings, setting), expected)\n                            finally:\n                                clear_script_prefix()\n"], "sample_588": ["    def test_auto_combine_join_exact(self):\n        objs = [Dataset({\"x\": [0], \"y\": [0]}), Dataset({\"x\": [1], \"y\": [1]})]\n        with pytest.raises(ValueError, match=\"indexes along dimension\"):\n            auto_combine(objs, join=\"exact\")\n", "    def test_auto_combine_merge_and_concat_with_fill_value(self):\n        # https://github.com/pydata/xarray/issues/2616\n        objs = [\n            Dataset({\"a\": (\"x\", [2, 3]), \"x\": [1, 2]}),\n            Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]}),\n        ]\n        with pytest.warns(FutureWarning, match=\"require both concatenation\"):\n            actual = auto_combine(objs, fill_value=0)\n        expected = Dataset({\"a\": ((\"t\", \"x\"), [[0, 2, 3], [1, 2, 0]])}, {\"x\": [0, 1, 2]})\n        assert_identical(expected, actual)\n", "    def test_auto_combine_with_empty_dataset(self):\n        objs = [Dataset({\"x\": [0]}), Dataset()]\n        with pytest.warns(FutureWarning, match=\"empty datasets\"):\n            auto_combine(objs)\n", "    def test_auto_combine_with_fill_value_and_coords(self):\n        datasets = [\n            Dataset({\"a\": (\"x\", [2, 3]), \"x\": [1, 2]}),\n            Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]}),\n        ]\n        with pytest.warns(FutureWarning, match=\"supplied have global\"):\n            auto_combine(datasets, fill_value=np.nan)\n", "    def test_combine_nested_fill_value_with_coords(self):\n        datasets = [\n            Dataset({\"a\": (\"x\", [2, 3]), \"x\": [1, 2]}),\n            Dataset({\"a\": (\"x\", [1, 2]), \"x\": [0, 1]}),\n        ]\n        expected = Dataset(\n            {\"a\": ((\"t\", \"x\"), [[np.nan, 2, 3], [1, 2, np.nan]])},\n            {\"x\": [0, 1, 2]},\n        )\n        actual = combine_nested(datasets, concat_dim=\"t\", fill_value=np.nan)\n        assert_identical(expected, actual)\n\n"], "sample_937": ["def test_unparse_function_with_annotation(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0]) == expected\n", "def test_unparse_typed_arguments(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0]) == expected\n", "def test_unparse_functiondef(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0]) == expected\n", "def test_unparse_arguments():\n    code = \"\"\"", "def test_function_definition(source, expected):\n    module = ast.parse(source)\n    assert ast.unparse(module.body[0]) == expected\n"], "sample_793": ["def test_iforest_predict_proba():\n    # toy sample (the last two samples are outliers)\n    X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1], [6, 3], [-4, 7]]\n\n    # Test IsolationForest\n    clf = IsolationForest(\n        behaviour=\"new\", random_state=rng, contamination=contamination\n    )\n    clf.fit(X)\n    probas = clf.predict_proba(X)\n    assert probas.shape == (len(X), 2)\n    assert np.allclose(probas.sum(axis=1), 1)\n", "def test_iforest_decision_function_out_of_bounds():\n    # Test if decision_function handles input with features out of bounds\n    X_train = np.array([[0, 1], [1, 2]])\n    X_test = np.array([[2, 1], [1, 3]])\n    clf = IsolationForest().fit(X_train)\n    decision_function_results = clf.decision_function(X_test)\n    assert_array_equal(decision_function_results.shape, (2,))\n    assert not np.isnan(decision_function_results).any() \n", "def test_iforest_predict_proba():\n    X_train = [[1, 1], [1, 2], [2, 1], [3, 3]]\n    clf = IsolationForest().fit(X_train)\n    proba = clf.predict_proba([[2., 2.]])\n    assert proba.shape == (1, 2)\n    assert proba[0][0] < proba[0][1]\n", "def test_iforest_large_n_samples():\n    rng = check_random_state(0)\n    X_train = rng.rand(10000, 10)\n\n    clf = IsolationForest(n_estimators=100, random_state=rng).fit(X_train)\n    assert clf.decision_function(X_train).shape[0] == X_train.shape[0]\n", "def test_iforest_contamination():\n    # Test the influence of contamination parameter on prediction\n\n    X_train = [[1, 1], [1, 2], [2, 1]]\n    X_test = [[2, 2], [3, 3]]\n\n    clf1 = IsolationForest(contamination=0.1).fit(X_train)\n    clf2 = IsolationForest(contamination=0.5).fit(X_train)\n\n    # with a lower contamination, model is more strict\n    assert np.sum(clf1.predict(X_test) == -1) > np.sum(clf2.predict(X_test) == -1)\n"], "sample_299": ["    def test_relative_path(self):\n        with self.settings(CACHES={\n            'default': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': 'cache',\n            },\n        }):\n            self.assertEqual(check_file_based_cache_is_absolute(None), [\n                Warning(\n                    \"Your 'default' cache LOCATION path is relative. Use an \"\n                    \"absolute path instead.\",\n                    id='caches.W003',\n                ),\n            ])\n\n", "    def test_relative_path(self):\n        with self.settings(CACHES={\n            'default': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': 'cache',\n            },\n        }):\n            self.assertEqual(check_file_based_cache_is_absolute(None), [\n                Warning(\n                    \"Your 'default' cache LOCATION path is relative. Use an \"\n                    \"absolute path instead.\",\n                    id='caches.W003',\n                ),\n            ])\n", "    def test_relative_path(self):\n        with self.settings(CACHES={\n            'default': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': 'cache',\n            },\n        }):\n            self.assertEqual(check_file_based_cache_is_absolute(None), [\n                Warning(\n                    \"Your 'default' cache LOCATION path is relative. Use an \"\n                    \"absolute path instead.\",\n                    id='caches.W003',\n                ),\n            ])\n", "    def test_relative_path(self):\n        with self.settings(CACHES={\n            'default': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': 'cache',\n            },\n        }):\n            self.assertEqual(check_file_based_cache_is_absolute(None), [\n                Warning(\n                    \"Your 'default' cache LOCATION path is relative. Use an \"\n                    \"absolute path instead.\",\n                    id='caches.W003',\n                ),\n            ])\n", "    def test_relative_path(self):\n        with self.settings(CACHES={\n            'default': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': 'cache',\n            },\n        }):\n            self.assertEqual(check_file_based_cache_is_absolute(None), [\n                Warning(\n                    \"Your 'default' cache LOCATION path is relative. Use an \"\n                    \"absolute path instead.\",\n                    id='caches.W003',\n                ),\n            ])\n"], "sample_956": ["def test_missing_reference_same_domain_duplicate(tempdir, app, status, warning):\n    inv_file = tempdir / 'inventory'\n    inv_file.write_bytes(inventory_v2)\n    set_config(app, {\n        'https://docs.python.org/': inv_file,\n    })\n\n    # load the inventory and check if it's done correctly\n    normalize_intersphinx_mapping(app, app.config)\n    load_mappings(app)\n\n    # check resolution when a target is found\n    rn = reference_check(app, 'py', 'func', 'module1.func', 'foo')\n    assert isinstance(rn, nodes.reference)\n    assert rn['refuri'] == 'https://docs.python.org/sub/foo.html#module1.func'\n    assert rn['reftitle'] == '(in foo v2.0)'\n    assert rn[0].astext() == 'foo'\n\n    # create a duplicate entry in the inventory\n    inv_file.write_bytes(b'\\nmodule1.func: https://docs.python.org/duplicate/foo.html#module1.func\\n')\n\n    # check that the first matching entry is used\n    rn = reference_check(app, 'py', 'func', 'module1.func', 'foo')\n    assert isinstance(rn, nodes.reference)\n    assert rn['refuri'] == 'https://docs.python.org/sub/foo.html#module1.func'\n    assert rn['reftitle'] == '(in foo v2.0)'\n", "def test_missing_reference_version(tempdir, app, status, warning):\n    inv_file = tempdir / 'inventory'\n    inv_file.write_bytes(inventory_v2)\n    set_config(app, {\n        'https://docs.python.org/': inv_file,\n    })\n\n    # load the inventory and check if it's done correctly\n    normalize_intersphinx_mapping(app, app.config)\n    load_mappings(app)\n\n    # check for versioned refs\n    rn = reference_check(app, 'py', 'mod', 'module1', 'foo', version='2.0')\n    assert rn['refuri'] == 'https://docs.python.org/foo.html#module-module1'\n    assert rn['reftitle'] == '(in foo v2.0)'\n    assert rn[0].astext() == 'foo'\n\n    # check for refs without version\n    rn = reference_check(app, 'py', 'mod', 'module1', 'foo', version=None)\n    assert rn['refuri'] == 'https://docs.python.org/foo.html#module-module1'\n    assert rn['reftitle'] == '(in foo v2.0)'\n    assert rn[0].astext() == 'foo'\n\n    # check for non-existent version\n    rn = reference_check(app, 'py', 'mod', 'module1', 'foo', version='3.0')\n    assert rn is None\n", "    def test_missing_reference_different_domain_ref_in_title(tempdir, app, status, warning):\n        inv_file = tempdir / 'inventory'\n        inv_file.write_bytes(inventory_v2)\n        set_config(app, {\n            'https://docs.python.org/': inv_file,\n        })\n\n        # load the inventory and check if it's done correctly\n        normalize_intersphinx_mapping(app, app.config)\n        load_mappings(app)\n\n        # no context data\n        kwargs = {}\n        node, contnode = fake_node('py', 'func', 'module1.func', 'a term', **kwargs)\n        rn = missing_reference(app, app.env, node, contnode)\n        assert rn is None\n\n        # different domain ref in title\n        kwargs = {'std:term': 'a term'}\n        node, contnode = fake_node('py', 'func', 'module1.func', 'a term', **kwargs)\n        rn = missing_reference(app, app.env, node, contnode)\n        assert rn is None\n", "def test_missing_reference_disabled_reftypes_with_versions(tempdir, app, status, warning):\n    inv_file = tempdir / 'inventory'\n    inv_file.write_bytes(inventory_v2)\n    set_config(app, {\n        'inv': ('https://docs.python.org/', inv_file),\n        'https://docs.other/': inv_file\n    })\n\n    # load the inventory and check if it's done correctly\n    normalize_intersphinx_mapping(app, app.config)\n    load_mappings(app)\n\n            if expected is None:\n                assert rn is None\n            else:\n                assert rn.astext() == expected\n\n        kwargs = {}\n        node, contnode = fake_node('std', 'term', 'a term', 'a term', **kwargs)\n        rn = missing_reference(app, app.env, node, contnode)\n        assert_(rn, 'a term' if term else None)\n\n        node, contnode = fake_node('std', 'term', 'inv:a term', 'a term', **kwargs)\n        rn = missing_reference(app, app.env, node, contnode)\n        assert_(rn, 'a term')\n\n        node, contnode = fake_node('std', 'doc', 'docname', 'docname', **kwargs)\n        rn = missing_reference(app, app.env, node, contnode)\n        assert_(rn, 'docname' if doc else None)\n\n        node, contnode = fake_node('std', 'doc', 'inv:docname', 'docname', **kwargs)\n        rn = missing_reference(app, app.env, node, contnode)\n        assert_(rn, 'docname')\n\n        # an arbitrary ref in another domain\n        node, contnode = fake_node('py', 'func', 'module1.func', 'func()', **kwargs)\n        rn = missing_reference(app, app.env, node, contnode)\n        assert_(rn, 'func()' if py else None)\n\n        node, contnode = fake_node('py',", "    def test_missing_reference_custom_domain(tempdir, app, status, warning):\n        inv_file = tempdir / 'inventory'\n        inv_file.write_bytes(inventory_v2)\n        set_config(app, {\n            'custom': ('https://docs.example.org/', inv_file),\n        })\n\n        # load the inventory and check if it's done correctly\n        normalize_intersphinx_mapping(app, app.config)\n        load_mappings(app)\n\n        # no context data\n        kwargs = {}\n        node, contnode = fake_node('custom', 'func', 'custom.func', 'custom.func()', **kwargs)\n        rn = missing_reference(app, app.env, node, contnode)\n        assert rn is None\n\n        # custom:module context helps to search objects\n        kwargs = {'custom:module': 'custom_module'}\n        node, contnode = fake_node('custom', 'func', 'custom.func', 'custom.func()', **kwargs)\n        rn = missing_reference(app, app.env, node, contnode)\n        assert rn.astext() == 'custom.func()'\n\n"], "sample_1076": ["def test_SymPyPrinter_print_IndexedBase():\n    p = SymPyPrinter()\n    assert p.doprint(p[0, 1]) == 'p[0, 1]'\n", "compilation error", "compilation error", "def test_SymPyPrinter_print_Piecewise():\n    p = SymPyPrinter()\n    expr = Piecewise((1, Eq(x, 0)), (x, True))\n    assert p.doprint(expr) == 'Piecewise((1, Eq(x, 0)), (x, True))'\n\n", "    def test_SymPyPrinter_Piecewise():\n        p = SymPyPrinter()\n        expr = Piecewise((1, Eq(x, 0)), (2, x>6))\n        assert p.doprint(expr) == 'Piecewise((1, Eq(x, 0)), (2, x > 6))'\n"], "sample_181": ["    def test_filtered_aggregate_with_exists(self):\n        q = Exists(Book.objects.filter(contact__name='test3', rating__gt=4))\n        aggs = Author.objects.aggregate(cnt=Count('pk', filter=q))\n        self.assertEqual(aggs['cnt'], 1)\n", "    def test_filtered_aggregate_on_conditional_annotate(self):\n        qs = Book.objects.annotate(\n            pub_year=Case(When(pubdate__year=2007, then=2007), default=2008),\n        ).annotate(\n            cnt=Count('pk', filter=Q(pub_year=2222)),\n        )\n        self.assertEqual(qs.get().cnt, 3)\n", "    def test_filtered_aggregate_on_annotate_with_exists(self):\n        exists_subquery = Exists(Book.objects.filter(contact=OuterRef('pk'), rating__gt=4))\n        aggs = Author.objects.annotate(has_highly_rated_book=exists_subquery).aggregate(\n            cnt=Count('pk', filter=Q(has_hishly_rated_book=True)),\n        )\n        self.assertEqual(aggs['cnt'], 2)\n", "    def test_filtered_aggregate_with_related_model_exists(self):\n        agg = Count('book', filter=Exists(Book.objects.filter(contact__name='test2')))\n        self.assertEqual(Author.objects.aggregate(cnt=agg)['cnt'], 1)\n", "    def test_filtered_count_with_exists(self):\n        qs1 = Book.objects.filter(contact__name='test')\n        qs2 = Book.objects.filter(contact__name='test')\n        qs1 = qs1.annotate(has_friend_books=Exists(Book.objects.filter(contact__in=qs2)))\n        self.assertEqual(qs1.count(), 1)\n"], "sample_941": ["def test_restify_type_hints_paramspec():\n    from typing import ParamSpec  # type: ignore\n    P = ParamSpec('P')\n    assert restify(List[P]) == \":class:`List`\\\\ [:obj:`tests.test_util_typing.P`]\"\n", "def test_stringify_type_variables():\n    assert stringify(T) == \"T\"\n    assert stringify(List[T]) == \"List[T]\"\n    assert stringify(Dict[str, T]) == \"Dict[str, T]\"\n", "    def test_restify_type_hints_newtype():\n        MyInt = NewType('MyInt', int)\n        assert restify(MyInt) == \":class:`MyInt`\"\n        assert restify(Optional[MyInt]) == \":obj:`Optional`\\\\ [:class:`MyInt`]\"\n", "def test_stringify_type_hints_empty_tuple():\n    assert stringify(Tuple[()]) == \"Tuple[]\"\n", "def test_restify_type_hints_GenericAlias():\n    from typing import GenericAlias\n    class MyGeneric(GenericAlias):\n        pass\n    assert restify(MyGeneric[int]) == \":class:`tests.test_util_typing.MyGeneric`\\\\ [:class:`int`]\"\n\n"], "sample_922": ["def test_pyattribute_no_type(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"      :value: ''\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'attr (Class attribute)', 'Class.attr', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_name, \"attr\"],\n                                                     [desc_annotation, \" = ''\"])],\n                                   [desc_content, ()]))\n    assert 'Class.attr' in domain.objects\n    assert domain.objects['Class.attr'] == ('index', 'Class.attr', 'attribute')\n", "def test_pyexception_signature_with_alias(app):\n    text = \".. py:exception:: MyException\\n\"\n    text += \".. py:exception:: AliasException (MyException)\\n\"\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert 'MyException' in domain.objects\n    assert domain.objects['MyException'] == ('index', 'MyException', 'exception')\n    assert 'AliasException' in domain.objects\n\n    assert domain.objects['AliasException'] == ('index', 'AliasException', 'exception')\n\n", "def test_pyclass_abstract(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:abstractmethod:: meth\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'meth() (Class method)', 'Class.meth', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_annotation, \"abstract \"],\n                                                     [desc_name, \"meth\"],\n                                                     [desc_parameterlist, ()])],\n                                   [desc_content, ()]))\n    assert 'Class.meth' in domain.objects\n    assert domain.objects['Class.meth'] == ('index', 'Class.meth', 'method')\n", "def test_pyattribute_with_default(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"      :type: int\\n\"\n            \"      :value: 42\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"class \"],\n                                                    [desc_name, \"Class\"])],\n                                  [desc_content, (addnodes.index,\n                                                  desc)])]))\n    assert_node(doctree[1][1][0], addnodes.index,\n                entries=[('single', 'attr (Class attribute)', 'Class.attr', '', None)])\n    assert_node(doctree[1][1][1], ([desc_signature, ([desc_name, \"attr\"],\n                                                     [desc_annotation, (\": \",\n                                                                        [pending_xref, \"int\"])],\n                                                     [desc_annotation, \" = 42\"])],\n                                   [desc_content, ()]))\n    assert 'Class.attr' in domain.objects\n    assert domain.objects['Class.attr'] == ('index', 'Class.attr', 'attribute')\n", "def test_pyexception_signature_with_module(app):\n    text = \".. py:exception:: module.MyException\\n\"\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"exception \"],\n                                                    [desc_addname, \"module.\"],\n                                                    [desc_name, \"MyException\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], desc, desctype=\"exception\",\n                domain=\"py\", objtype=\"exception\", noindex=False)\n\n    assert 'module.MyException' in domain.objects\n    assert domain.objects['module.MyException'] == ('index', 'module.MyException', 'exception')\n"], "sample_221": ["    def test_in_lookup_queryset_with_prefetches(self):\n        # Test prefetch_related with __in query against related objects.\n        groups = Group.objects.prefetch_related('event_set').filter(id__in=[self.e1.group_id])\n        self.assertSequenceEqual(groups, [self.e1.group])\n\n        dumped = pickle.dumps(groups)\n        reloaded = pickle.loads(dumped)\n        self.assertSequenceEqual(reloaded, [self.e1.group])\n\n\n\n", "    def test_pickle_prefetch_related_ordering(self):\n        g1 = Group.objects.create(name='g1')\n        g2 = Group.objects.create(name='g2')\n        Event.objects.create(title='e1', group=g1)\n        Event.objects.create(title='e2', group=g2)\n        events = Event.objects.prefetch_related('group').order_by('group__name')\n        events2 = pickle.loads(pickle.dumps(events))\n        self.assertEqual([e.title for e in events2], ['e1', 'e2'])\n", "    def test_pickle_prefetch_related_with_m2m_and_objects_filter(self):\n        g = Group.objects.create(name='foo')\n        m2m = M2MModel.objects.create()\n        m2m.groups.add(g)\n        m2ms = M2MModel.objects.prefetch_related('groups').filter(groups__name='foo')\n        m2ms2 = pickle.loads(pickle.dumps(m2ms))\n        self.assertSequenceEqual(m2ms2, [m2m])\n", "    def test_q_object_in_lookup_queryset_evaluation(self):\n        \"\"\"\n        Neither pickling nor unpickling a QuerySet.query with a Q object's\n        __in=inner_qs lookup should evaluate inner_qs.\n        \"\"\"\n        from django.db.models import Q\n\n        events = Event.objects.filter(Q(group__in=Group.objects.all()))\n\n        with self.assertNumQueries(0):\n            dumped = pickle.dumps(events.query)\n\n        with self.assertNumQueries(0):\n            reloaded = pickle.loads(dumped)\n            reloaded_events = Event.objects.none()\n            reloaded_events.query = reloaded\n\n        self.assertSequenceEqual(reloaded_events, [self.e1])\n\n", "    def test_pickle_select_related_with_default_manager_method(self):\n        g = Group.objects.create(name='foo')\n        Event.objects.create(title='event', group=g)\n        events = Event.objects.select_related('group', 'group__events')\n        events2 = pickle.loads(pickle.dumps(events))\n        g2 = list(events2)[0].group\n        self.assertEqual(g2.events.count(), 1)\n"], "sample_1042": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_740": ["def test_check_array_dtype_object_with_allow_nd():\n    X = np.array([[1, 2], [3, 4]], dtype=object)\n\n    # should raise a warning\n    assert_warns(DataConversionWarning, check_array, X, allow_nd=True,\n                 dtype=np.float64, warn_on_dtype=True)\n\n    # should raise warning if allow_nd is False\n    assert_warns(DataConversionWarning, check_array, X, allow_nd=False,\n                 dtype=np.float64, warn_on_dtype=True)\n", "compilation error", "compilation error", "compilation error", "    def test_check_consistent_length_with_pandas_index(self):\n        # Test check_consistent_length with pandas Index objects\n        import pandas as pd\n        index1 = pd.Index([1, 2, 3])\n        index2 = pd.Index([4, 5, 6])\n        index3 = pd.Index([7, 8])\n        assert_raises_regex(ValueError, \"inconsistent numbers of samples\",\n                            check_consistent_length, index1, index2, index3)\n        \n\n"], "sample_241": ["    def test_transform_output_field(self):\n        expr = ExpressionWrapper(\n            Case(\n                When(Q(age__gt=18), then=Value('adult')),\n                default=Value('child'),\n            ),\n            output_field=CharField(),\n        )\n        self.assertIsInstance(expr.output_field, CharField)\n", "    def test_expressionwrapper_with_alias(self):\n        expr = ExpressionWrapper(F('field'), output_field=IntegerField(), alias='field_alias')\n        self.assertEqual(expr.alias, 'field_alias')\n        self.assertEqual(str(expr), 'field_alias')\n", "    def test_invalid_output_field(self):\n        with self.assertRaises(ValueError):\n            ExpressionWrapper(Value(3), output_field='invalid')\n", "    def test_resolve_output_field_failure(self):\n        with self.assertRaisesMessage(FieldError, 'Cannot resolve expression type, unknown output_field'):\n            ExpressionWrapper(object()).output_field\n", "    def test_expression_wrapper_repr(self):\n        expr = ExpressionWrapper(F('col') + F('anothercol'), output_field=IntegerField())\n        self.assertEqual(repr(expr), \"ExpressionWrapper(F(col) + F(anothercol))\")\n"], "sample_627": ["    def test_concat_empty_datasets():\n        \"\"\"Test concatenation of empty datasets.\"\"\"\n        ds1 = Dataset()\n        ds2 = Dataset(coords={\"foo\": [1]})\n\n        actual = concat([ds1, ds2], dim=\"x\")\n        expected = Dataset(coords={\"foo\": [1], \"x\": [0]})\n        assert_identical(actual, expected)\n\n        ds1 = Dataset(coords={\"x\": []})\n        ds2 = Dataset({\"a\": (\"x\", [1])}, coords={\"x\": [1]})\n        with pytest.raises(ValueError, match=\"cannot concatenate\"):\n            concat([ds1, ds2], dim=\"x\")\n\n", "compilation error", "compilation error", "    def test_concat_error_inconsistent_coordinate_data(self):\n        ds1 = Dataset({\"a\": (\"x\", [1, 2])}, coords={\"x\": [0, 1]})\n        ds2 = Dataset({\"a\": (\"x\", [3, 4])}, coords={\"x\": [1, 2]})\n        with pytest.raises(ValueError, match=r\"Coordinates must be consistent\"):\n            concat([ds1, ds2], dim=\"x\")\n", "compilation error"], "sample_664": ["def test_funcargnames_deprecation(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            return 42\n\n            assert foo == 42\n        \"\"\"\n    )\n    result = testdir.runpytest(\"-v\")\n    result.assert_outcomes(passed=1)\n    result.stdout.fnmatch_lines(\n        [\n            \"*The `funcargnames` attribute was an alias for `fixturenames`*\",\n            \"*since pytest 2.3 - use the newer attribute instead.*\",\n        ]\n    )\n\n", "def test_funcargnames_deprecation(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            pass\n\n            pass\n        \"\"\"\n    )\n    result = testdir.runpytest_inprocess([], '--fixtures')\n    result.stdout.fnmatch_lines(\n        [\n            \"*funcargnames is deprecated*\",\n            \"*use fixturenames instead*\",\n        ]\n    )\n", "def test_fixture_positional_arguments_deprecation(testdir, fixturename, fixture_def):\n    \"\"\"Test that passing positional arguments to fixture definitions emits a deprecation warning.\"\"\"\n    testdir.makepyfile(\n        f\"\"\"\n        {fixture_def}\n\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines(\n        [f\"*Passing arguments to pytest.fixture() as positional arguments is deprecated - pass them as a keyword argument instead.*\"]\n    )\n", "def test_funcargnames_is_deprecated(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            pass\n\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stdout.fnmatch_lines([\"*The `funcargnames` attribute was an alias*\"])\n", "def test_fixture_positional_arguments(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture\n            pass\n        \n\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(failed=1)\n    result.stdout.fnmatch_lines(\n        [\"*Passing arguments to pytest.fixture() as positional arguments is deprecated - pass them\",\n         \"*as a keyword argument instead.*\"]\n    )\n\n"], "sample_396": ["    def test_ticket_24605_related_empty(self):\n        i5 = Individual.objects.create(alive=True)\n        self.assertSequenceEqual(\n            Individual.objects.filter(\n                Q(alive=False), Q(related_individual__isnull=True)\n            ),\n            [],\n        )\n", "    def test_ticket_24605_with_annotation(self):\n        \"\"\"\n        Subquery table names should be quoted even with annotations.\n        \"\"\"\n        i1 = Individual.objects.create(alive=True)\n        RelatedIndividual.objects.create(related=i1)\n        i2 = Individual.objects.create(alive=False)\n        RelatedIndividual.objects.create(related=i2)\n        i3 = Individual.objects.create(alive=True)\n        i4 = Individual.objects.create(alive=False)\n\n        self.assertSequenceEqual(\n            Individual.objects.annotate(\n                has_relation=Exists(RelatedIndividual.objects.filter(related=OuterRef('pk')))\n            ).filter(\n                alive=False\n            ).filter(\n                has_relation=False\n            ),\n            [i4],\n        )\n", "    def test_ticket_24605_complex(self):\n        i1 = Individual.objects.create(alive=True)\n        RelatedIndividual.objects.create(related=i1)\n        i2 = Individual.objects.create(alive=False)\n        RelatedIndividual.objects.create(related=i2)\n        i3 = Individual.objects.create(alive=True)\n        i4 = Individual.objects.create(alive=False)\n\n        RelatedIndividual.objects.create(related=i3)\n        self.assertSequenceEqual(\n            Individual.objects.filter(\n                Q(alive=True) & Q(related_individual__isnull=False),\n            ).order_by(\"pk\"),\n            [i1, i3],\n        )\n", "    def test_ticket_24605_with_select_related(self):\n        \"\"\"\n        Subquery table names should be quoted even with select_related.\n        \"\"\"\n        i1 = Individual.objects.create(alive=True)\n        RelatedIndividual.objects.create(related=i1)\n        i2 = Individual.objects.create(alive=False)\n        RelatedIndividual.objects.create(related=i2)\n        i3 = Individual.objects.create(alive=True)\n        i4 = Individual.objects.create(alive=False)\n\n        qs = Individual.objects.select_related('related_individual').filter(\n            Q(alive=False), Q(related_individual__isnull=True)\n        )\n        self.assertSequenceEqual(qs, [i4])\n", "    def test_ticket_24605_double_subquery(self):\n        i1 = Individual.objects.create(alive=True)\n        RelatedIndividual.objects.create(related=i1)\n        i2 = Individual.objects.create(alive=False)\n        RelatedIndividual.objects.create(related=i2)\n        i3 = Individual.objects.create(alive=True)\n        i4 = Individual.objects.create(alive=False)\n\n        qs = Individual.objects.filter(related_individual__isnull=True, alive=False)\n        self.assertSequenceEqual(qs, [i4])\n        \n"], "sample_967": ["def test_mathjax_custom_path(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    assert ('<script defer=\"defer\" src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\">'\n            '</script>' in content)\n\n", "def test_mathjax_path(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    assert '<script src=\"path/to/mathjax.js\"></script>' in content\n", "def test_mathjax_custom_path(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    assert 'custom/mathjax.js' in content\n", "def test_custom_mathjax_path(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    assert 'custom/mathjax.js' in content\n", "def test_mathjax_path_config(app, status, warning):\n    app.builder.build_all()\n\n    content = (app.outdir / 'index.html').read_text()\n    assert '<script src=\"some/other/path.js\"></script>' in content\n"], "sample_1207": ["    def test_issue_26067():\n        x = Symbol('x')\n        assert parse_expr('x**(3*y)', transformations=standard_transformations +\n                        (implicit_multiplication,)) == x**(3*y)\n", "    def test_issue_2516():\n        raises(TokenError, lambda: parse_expr('a (b'))\n        raises(TokenError, lambda: parse_expr('a ) b'))\n", "    def test_issue_2515_2():\n        raises(TokenError, lambda: parse_expr('(((('))\n        raises(TokenError, lambda: parse_expr('\"\"\"'))\n        raises(TokenError, lambda: parse_expr('1 + ((('))\n\n", "compilation error", "def test_issue_25807():\n    x = Symbol('x')\n    assert parse_expr('1/x/(x+1)') == 1/(x*(x+1))\n"], "sample_81": ["    def test_resolve_regex_pattern(self):\n        resolver = get_resolver()\n        match = resolver.resolve('/translated/')\n        self.assertEqual(match.func.__name__, 'translated_view')\n", "    def test_str_translated(self):\n        self.assertEqual(str(RoutePattern(_('translated/'))), 'translated/')\n", "    def test_resolve(self):\n        resolver = get_resolver()\n        match = resolver.resolve('/translated/')\n        self.assertEqual(match.func.__name__, 'translated_view')\n        self.assertEqual(match.app_names, [])\n        self.assertEqual(match.namespaces, [])\n        self.assertEqual(match.route, 'translated/')\n", "    def test_str_with_translation(self):\n        self.assertEqual(str(RoutePattern(_('translated/'))), '\u00fcbersetzt/')\n", "    def test_regex_pattern_translation(self):\n        pattern = RegexPattern(_('^translated/$'))\n        self.assertEqual(str(pattern), '^traduit/$')\n"], "sample_139": ["    def test_changelist_view_filter_select_multiple(self):\n        \"\"\"\n        Test filtering changelist view by selecting multiple options in a filter\n        dropdown.\n        \"\"\"\n        Child.objects.create(name='child 1', parent=Parent.objects.create(name='parent 1'))\n        Child.objects.create(name='child 2', parent=Parent.objects.create(name='parent 2'))\n        Child.objects.create(name='child 3', parent=Parent.objects.create(name='parent 1'))\n\n        self.login('super', 'secret')\n        self.selenium.get('/admin/admin_changelist/child/')\n\n        # Select multiple parents from the dropdown filter\n        parent_filter = self.selenium.find_element_by_id('id_parent')\n        parent_filter.send_keys('parent 1')\n        parent_filter.send_keys(Keys.ENTER)\n        parent_filter.send_keys('parent 2')\n        parent_filter.send_keys(Keys.ENTER)\n\n        # Wait for the results to update\n        self.wait_until_element_visible(self.selenium.find_element_by_css_selector('.changelist tr'))\n\n        # Assert that only the child objects matching the selected parents are visible\n        rows = self.selenium.find_elements_by_css_selector('.changelist tr')\n        self.assertEqual(len(rows), 3)\n", "    def test_search_by_related_field(self):\n        parent1 = Parent.objects.create(name='parent 1')\n        parent2 = Parent.objects.create(name='parent 2')\n        Child.objects.create(name='child 1', parent=parent1, age=10)\n        Child.objects.create(name='child 2', parent=parent2, age=20)\n        self.login('super', 'secret')\n        self.selenium.get('/admin/admin_changelist/child/')\n        search_box = self.selenium.find_element_by_name(\n            'q'\n        )\n        search_box.send_keys('parent 1')\n        search_box.submit()\n        self.assertEqual(self.selenium.find_elements_by_xpath('//tbody/tr'), 1)\n        self.assertEqual(\n            self.selenium.find_element_by_xpath('//tbody/tr/td[2]').text,\n            'child 1'\n        )\n", "    def test_add_permission_is_respected(self):\n        \"\"\"\n        When ModelAdmin.has_add_permission() returns False, the \"Add\" button\n        is not shown in the changelist view.\n        \"\"\"\n        class RestrictedEventAdmin(EventAdmin):\n                return False\n\n        custom_site = AdminSite(name='restricted')\n        custom_site.register(Event, RestrictedEventAdmin)\n\n        self.client.login(username='super', password='secret')\n        response = self.client.get('/restricted/event/')\n        self.assertContains(response, 'No matching entries found.')\n        self.assertNotContains(response, 'Add Event')\n", "    def test_filter_by_related_field_with_manytomany(self):\n        \"\"\"\n        Regression tests for ticket #18400: filter by related ManyToMany field\n        should work correctly.\n\n        This test case ensures that the filtering mechanism correctly handles\n        ManyToMany relationships when filtering the changelist.\n\n        \"\"\"\n        article = Article.objects.create(title=\"Article 1\", content=\"...\")\n        article.tags.add(Tag.objects.create(name=\"Technology\"), Tag.objects.create(name=\"News\"))\n        self.login('super')\n        self.selenium.get('%s/admin/blog/tag/' % self.live_server_url)\n        self.selenium.find_element_by_name('_changelist-filter').send_keys('Technology')\n        self.selenium.find_element_by_css_selector('button.button').click()\n        # Check if the filtered result contains the expected article\n        self.assertIn('Article 1', self.selenium.page_source)\n", "    def test_empty_list_filter_with_filter_horizontal(self):\n        child = self._create_superuser('child')\n        m = EmptyListFilterWithFilterHorizontalChildAdmin(Child, custom_site)\n        request = self._mocked_authenticated_request('/child/', child)\n        response = m.changelist_view(request)\n\n        self.assertEqual(response.context_data['cl'].filter_specs, [])\n\n        self.assertContains(response, 'Filter Horizontal')\n"], "sample_947": ["def test_cmacro(app):\n    text = \".. c:macro:: MY_MACRO(a, b)\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1], addnodes.desc, desctype=\"macro\",\n                domain=\"c\", objtype=\"macro\", noindex=False)\n\n    entry = _get_obj(app, 'MY_MACRO')\n    assert entry == ('index', 'c.MY_MACRO', 'macro')\n", "def test_build_c_macro_target(app, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"macro_target\")\n    assert len(ws) == 0\n    entries = extract_role_links(app, \"macro_target.html\")\n    assert entries == [\n        ('c.MACRO', 'MACRO', 'MACRO'),\n        ('c.MACRO_ARG', 'MACRO_ARG', 'MACRO_ARG'),\n    ]\n", "def test_cmacro(app):\n    text = \".. c:macro:: MY_MACRO\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1], addnodes.desc, desctype=\"macro\",\n                domain=\"c\", objtype=\"macro\", noindex=False)\n\n    entry = _get_obj(app, 'MY_MACRO')\n    assert entry == ('index', 'c.MY_MACRO', 'macro')\n", "def test_cmacro(app):\n    text = \".. c:macro:: MACRO(arg1, arg2)\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1], addnodes.desc, desctype=\"macro\",\n                domain=\"c\", objtype=\"macro\", noindex=False)\n\n    entry = _get_obj(app, 'MACRO')\n    assert entry == ('index', 'c.MACRO', 'macro')\n", "def test_cmacro(app):\n    text = \".. c:macro:: MACRO_NAME\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree[1], addnodes.desc, desctype=\"macro\",\n                domain=\"c\", objtype=\"macro\", noindex=False)\n\n    entry = _get_obj(app, 'MACRO_NAME')\n    assert entry == ('index', 'c.MACRO_NAME', 'macro')\n"], "sample_319": ["    def test_empty_operations(self):\n        class Migration(migrations.Migration):\n            operations = []\n\n        migration = Migration(\"0001_initial\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"auto_20231026_1656\")\n\n", "    def test_operation_with_invalid_chars_in_suggested_name_2(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.RenameField(\n                    model_name=\"Person\", old_name=\"full_name\", new_name=\"*full*name*\"\n                ),\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"person_full_name\")\n", "    def test_combined_operations_with_empty_create_models(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\"Person\", fields=[]),\n                migrations.CreateModel(\"Animal\", fields=[]),\n                migrations.RunSQL(\"SELECT 1 FROM person;\"),\n                migrations.RunSQL(\"SELECT 1 FROM animal;\"),\n                migrations.DeleteModel(\"Person\"),\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"person_animal_delete_person\")\n\n", "    def test_operation_with_invalid_chars_in_suggested_name_with_model_name(self):\n        class Migration(migrations.Migration):\n            operations = [\n                migrations.AddField(\n                    \"Person\",\n                    \"name\",\n                    models.CharField(max_length=200),\n                ),\n                migrations.AddConstraint(\n                    \"Person\",\n                    models.UniqueConstraint(\n                        fields=[\"name\"], name=\"person.name-*~unique!\"\n                    ),\n                ),\n            ]\n\n        migration = Migration(\"some_migration\", \"test_app\")\n        self.assertEqual(migration.suggest_name(), \"person_name_unique_\")\n", "    def test_alter_field_with_rename(self):\n        initial_author = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n                (\"age\", models.IntegerField()),\n            ],\n            {\n                \"index_together\": {(\"name\", \"age\")},\n            },\n        )\n        author_renamed_field = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"author_name\", models.CharField(max_length=200)),\n                (\"age\", models.IntegerField()),\n            ],\n            {\n                \"index_together\": {(\"author_name\", \"age\")},\n            },\n        )\n        changes = self.get_changes([initial_author], [author_renamed_field])\n\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(\n            changes,\n            \"testapp\",\n            0,\n            [\"RenameField\", \"AlterIndexTogether\"],\n        )\n        self.assertOperationAttributes(\n            changes,\n            \"testapp\",\n            0,\n            0,\n            model_name=\"author\",\n            old_name=\"name\",\n            new_name=\"author_name\",\n        )\n        self.assertOperationAttributes(\n            changes,\n            \"testapp\",\n            0,\n            1,\n            name=\"author\",\n            index_together={(\"author_name\", \"age\")},\n        )\n"], "sample_862": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_365": ["    def test_lazystr(self):\n        lazy_str = lazystr('hello world')\n        self.assertEqual(lazy_str(), 'hello world')\n\n", "    def test_classproperty_setter(self):\n        class Foo:\n            foo = classproperty(lambda cls: 123)\n\n            @foo.setter\n                cls.foo_attr = value\n\n        Foo.foo = 456\n        self.assertEqual(Foo.foo_attr, 456)\n", "    def test_simple_lazy_object_copy(self):\n        class MyObject:\n                self.value = value\n\n        original = MyObject(5)\n        lazy_obj = SimpleLazyObject(lambda: original)\n        copied_obj = copy.copy(lazy_obj)\n        self.assertEqual(copied_obj._setupfunc(), original)\n\n", "    def test_lazy_deepcopy(self):\n        original_object = {'key': 'value'}\n        lazy_obj = lazy(lambda: original_object, dict)\n        copied_obj = copy.deepcopy(lazy_obj())\n        self.assertEqual(copied_obj, original_object)\n", "    def test_keep_lazy_text(self):\n            return f'{a} {b}'\n        lazy_func = keep_lazy_text(my_func)\n\n        self.assertEqual(lazy_func(\"Hello\", lazystr(\"world\")), \"Hello world\")\n"], "sample_775": ["def test_empty_objects():\n    # Test handling of empty estimators, like empty pipelines\n\n    empty_pipeline = Pipeline([])\n    expected = \"\"\"Pipeline(memory=None,\n             steps=[])\"\"\"\n    expected = expected[1:]  # remove first \\n\n\n    assert empty_pipeline.__repr__() == expected\n\n\n\n", "compilation error", "    def test_tuple_param():\n        # Make sure tuples in parameters are printed correctly\n        pp = _EstimatorPrettyPrinter(indent=1, indent_at_name=True)\n\n        from sklearn.svm import LinearSVC\n        svc = LinearSVC(C=1.0, class_weight=None, dual=True,\n                        fit_intercept=True,\n                        intercept_scaling=1, loss='squared_hinge',\n                        max_iter=1000, multi_class='ovr', penalty='l2',\n                        random_state=None, tol=0.0001, verbose=0)\n\n        expected = \"\"\"LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n                  intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n                  multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n                  verbose=0)\"\"\"\n\n        expected = expected[1:]  # remove first \\n\n        assert pp.pformat(svc) == expected\n", "compilation error", "def test_changed_params_empty():\n    # Make sure _changed_params returns an empty dict\n    # when no params have changed\n    lr = LogisticRegression()\n    expected = {}\n    assert _changed_params(lr) == expected\n"], "sample_461": ["    def test_urlfield_widget_attrs(self):\n        f = URLField(attrs={'class': 'special', 'placeholder': 'Enter URL'})\n        expected = '<input type=\"url\" name=\"f\" id=\"id_f\" class=\"special\" placeholder=\"Enter URL\" required>'\n        self.assertWidgetRendersTo(f, expected)\n", "    def test_urlfield_with_initial_value(self):\n        f = URLField(initial=\"http://initial.example.com\")\n        self.assertEqual(f.value(), \"http://initial.example.com\")\n", "    def test_urlfield_with_initial_value(self):\n        f = URLField(initial=\"http://example.com\")\n        self.assertEqual(f.value(), \"http://example.com\")\n", "    def test_urlfield_unicode_input(self):\n        f = URLField()\n        self.assertEqual(f.clean(\"http://\u2603.net\"), \"http://\u2603.net\")\n\n", "    def test_urlfield_clean_empty_string_with_assume_scheme(self):\n        f = URLField(assume_scheme=\"https\")\n        self.assertEqual(f.clean(\"\"), \"\")\n"], "sample_11": ["def test_world_to_pixel_values_different_int_types():\n    int_sliced = SlicedLowLevelWCS(WCS_SPECTRAL_CUBE, np.s_[:, 0, :])\n    np64_sliced = SlicedLowLevelWCS(WCS_SPECTRAL_CUBE, np.s_[:, np.int64(0), :])\n    world_arrays = ([10, 20], [25])\n    for int_coord, np64_coord in zip(int_sliced.world_to_pixel_values(*world_arrays),\n                                     np64_sliced.world_to_pixel_values(*world_arrays)):\n        assert all(int_coord == np64_coord)\n", "def test_empty_slice():\n    wcs = WCS_SPECTRAL_CUBE\n    with pytest.raises(ValueError, match=\"Cannot slice WCS: the resulting WCS should have at least one pixel and one world dimension\"):\n        SlicedLowLevelWCS(wcs, np.s_[:, :, :][:, :, :])\n", "def test_pixel_bounds_after_slicing():\n    wcs = WCS_SPECTRAL_CUBE\n    sliced_wcs = SlicedLowLevelWCS(wcs, [slice(None), slice(1, 5), slice(None)])\n    assert sliced_wcs.pixel_bounds == [(-6, 6), (1, 4), (5, 15)] \n", "def test_sliced_low_level_wcs_with_non_standard_axis_names():\n    header = Header.fromstring(\"\"\"", "    def test_world_to_pixel_values_different_float_types():\n        float32_sliced = SlicedLowLevelWCS(WCS_SPECTRAL_CUBE, np.s_[:, 0, :])\n        world_coords = np.array([10., 20., 25.], dtype=np.float32)\n        pixel_coords_float32 = float32_sliced.world_to_pixel_values(*world_coords)\n        pixel_coords_float64 = float32_sliced.world_to_pixel_values(*world_coords.astype(np.float64))\n        assert np.allclose(pixel_coords_float32, pixel_coords_float64)\n\n\n"], "sample_274": ["    def test_modelchoicefield_empty_label(self):\n        # Test that empty_label works correctly\n        ChoiceModel.objects.create(pk=1, name='a')\n        ChoiceModel.objects.create(pk=2, name='b')\n        ChoiceModel.objects.create(pk=3, name='c')\n\n        f = ModelChoiceField(queryset=ChoiceModel.objects.all(), empty_label=\"--------\")\n        self.assertEqual(f.choices[0][1], \"--------\")\n        self.assertEqual(f.choices[1][1], \"a\")\n        self.assertEqual(f.choices[2][1], \"b\")\n        self.assertEqual(f.choices[3][1], \"c\")\n", "    def test_modelchoicefield_empty_label(self):\n        # Test that ModelChoiceField uses the empty_label if provided.\n        ChoiceModel.objects.create(pk=1, name='a')\n        e = {'required': 'REQUIRED'}\n        f = ModelChoiceField(queryset=ChoiceModel.objects.all(), empty_label='Choose one', error_messages=e)\n        self.assertFormErrors(['REQUIRED'], f.clean, '')\n        self.assertEqual(f.choices[0][1], 'Choose one')\n", "    def test_modelchoicefield_to_field_name(self):\n        e = {\n            'required': 'REQUIRED',\n            'invalid_choice': 'INVALID CHOICE',\n        }\n        f = ModelChoiceField(queryset=ChoiceModel.objects.all(), to_field_name='name', error_messages=e)\n        self.assertFormErrors(['REQUIRED'], f.clean, '')\n        self.assertFormErrors(['INVALID CHOICE'], f.clean, 'd')\n", "    def test_empty_modelchoicefield(self):\n        # Create choices for the model choice field tests below.\n        ChoiceModel.objects.create(pk=1, name='a')\n        ChoiceModel.objects.create(pk=2, name='b')\n        ChoiceModel.objects.create(pk=3, name='c')\n\n        e = {\n            'required': 'REQUIRED',\n            'invalid_choice': 'INVALID CHOICE',\n        }\n        f = ModelChoiceField(queryset=ChoiceModel.objects.none(), error_messages=e, empty_label=None)\n        self.assertFormErrors(['REQUIRED'], f.clean, '')\n", "    def test_modelchoicefield_empty_label(self):\n        # Test ModelChoiceField with empty_label\n\n        e = {\n            'required': 'REQUIRED',\n            'invalid_choice': 'INVALID CHOICE',\n        }\n        f = ModelChoiceField(queryset=ChoiceModel.objects.all(), empty_label='Select...', error_messages=e)\n        self.assertFormErrors(['REQUIRED'], f.clean, '')\n        self.assertFormErrors(['INVALID CHOICE'], f.clean, '4')\n\n        # Test ModelMultipleChoiceField with empty_label\n\n        e = {\n            'required': 'REQUIRED',\n            'invalid_choice': '%(value)s IS INVALID CHOICE',\n            'invalid_list': 'NOT A LIST OF VALUES',\n        }\n        f = ModelMultipleChoiceField(queryset=ChoiceModel.objects.all(), empty_label='Select...', error_messages=e)\n        self.assertFormErrors(['REQUIRED'], f.clean, '')\n        self.assertFormErrors(['NOT A LIST OF VALUES'], f.clean, '3')\n        self.assertFormErrors(['4 IS INVALID CHOICE'], f.clean, ['4'])\n\n"], "sample_566": ["    def test_add_subplot_no_axes_kwargs():\n        fig, ax = plt.subplots()\n        ax.plot([1, 2], [2, 1])\n\n        fig.add_subplot(1, 1, 1)\n", "    def test_subfigure_gridspec_layout(self):\n        fig = plt.figure()\n        gs = fig.add_gridspec(2, 2)\n        subfig = fig.add_subfigure(gs[0, :])\n\n        ax = subfig.add_subplot(1, 1, 1)\n        ax.plot([1, 2], [3, 4])\n\n        assert ax.get_position().x0 == 0\n        assert ax.get_position().y0 == 0\n        assert ax.get_position().x1 == 1\n        assert ax.get_position().y1 == 1\n", "    def test_subfigure_legend_title():\n        fig = plt.figure()\n        subfigs = fig.subfigures(1, 2)\n        ax1 = subfigs[0].subplots()\n        ax1.plot([1, 2, 3], [1, 2, 3], label='Line 1')\n        ax1.legend(title='Legend1')\n        ax2 = subfigs[1].subplots()\n        ax2.plot([1, 2, 3], [1, 2, 3], label='Line 2')\n        ax2.legend(title='Legend2')\n    \n", "    def test_subplot_mosaic_no_overlap(self):\n        fig, axs = plt.subplot_mosaic('AA', figsize=(3, 3))\n        assert len(axs) == 1\n        assert axs['A'].get_position().x0 == 0\n        assert axs['A'].get_position().y0 == 0\n        assert axs['A'].get_position().width == 1\n        assert axs['A'].get_position().height == 1\n\n", "compilation error"], "sample_191": ["    def test_snapshot_files_with_multiple_watchers(self):\n        self.reloader.watch_dir(self.tempdir, '*.py')\n        self.reloader.watch_dir(self.tempdir, '*.txt')\n        with mock.patch.object(self.reloader, 'watched_files') as mock_watched_files:\n            mock_watched_files.return_value = [self.existing_file]\n            snapshot = dict(self.reloader.snapshot_files())\n            self.assertIn(self.existing_file, snapshot)\n", "    def test_snapshot_files_returns_empty_dict_when_watched_files_empty(self, mocked_stat):\n        with mock.patch.object(self.reloader, 'watched_files', return_value=[]):\n            self.assertEqual(self.reloader.snapshot_files(), {})\n", "    def test_should_stop_returns_true_when_changed(self):\n        self.reloader._changed_files = {self.existing_file}\n        self.assertTrue(self.reloader.should_stop())\n", "    def test_watch_dir_with_non_existing_directory(self, mocked_modules, notify_mock):\n        self.reloader.watch_dir(self.nonexistent_file.parent, '*.py')\n        with self.tick_twice():\n            self.increment_mtime(self.existing_file)\n        self.assertEqual(notify_mock.call_count, 0)\n\n", "    def test_is_modified_returns_false_when_mtime_is_unchanged(self, mocked_time):\n        mocked_time.time.side_effect = [1, 1]\n        self.assertFalse(self.reloader.is_modified(self.existing_file))\n"], "sample_581": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_949": ["def test_man_pages_with_custom_template(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'man1' / 'index.1').read_text()\n    assert '.TH \"My Project 1.0\" \"January 1, 2023\" \"My Org\"' in content\n    assert '.SH NAME' in content\n    assert '.SH SYNOPSIS' in content\n    assert '.SH DESCRIPTION' in content\n", "def test_man_pages_with_unknown_docname(app, status, warning):\n    app.config['man_pages'] = [\n        ('unknown_docname', 'unknown', 'Unknown page', [], 1),\n        ('index', 'sphinxtests', 'Sphinx Tests', [], 1),\n    ]\n    app.build()\n    assert not (app.outdir / 'unknown.1').exists()\n    assert (app.outdir / 'sphinxtests.1').exists()\n    assert 'no \"man_pages\" config value found' not in warning.getvalue()\n    assert '\"man_pages\" config value references unknown document unknown_docname' in warning.getvalue()\n", "def test_manpage_show_urls(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'image-deprecation.1').read_text()\n    assert 'http://example.com/image.png' in content\n", "def test_image(app, status, warning):\n    app.builder.build_all()\n    content = (app.outdir / 'image.1').read_text()\n    assert '.PSPIC' in content\n    assert 'sphinx-logo.png' in content\n", "def test_image(app, status, warning):\n    app.build()\n    content = (app.outdir / 'man1' / 'image.1').read_text()\n    assert 'image: Sphinx logo' not in content\n\n"], "sample_246": ["    def test_custom_locale_dir(self):\n        management.call_command('makemessages', locale=[LOCALE], verbosity=0,\n                                locale_path='custom_locale')\n        self.assertTrue(os.path.exists(os.path.join('custom_locale', LOCALE, 'LC_MESSAGES', 'django.po')))\n", "    def test_custom_locale_dir(self):\n        with override_settings(LOCALE_PATHS=[self.test_dir]):\n            management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n            custom_locale_file = os.path.join(\n                self.test_dir, 'locale', LOCALE, 'LC_MESSAGES', 'django.po')\n            self.assertTrue(os.path.exists(custom_locale_file))\n", "    def test_custom_locale_layout(self):\n        \"\"\"Test custom locale paths setting.\"\"\"\n        with override_settings(LOCALE_PATHS=[os.path.join(self.test_dir, 'custom_locale')]):\n            management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n            custom_de_locale = os.path.join(\n                self.test_dir, 'custom_locale', 'de', 'LC_MESSAGES', 'django.po')\n            self.assertTrue(os.path.exists(custom_de_locale))\n", "    def test_custom_locale_in_app(self):\n        \"\"\"\n        * translations for an app with a custom locale folder are stored in that folder\n        * translations outside of that app are in LOCALE_PATHS[0]\n        \"\"\"\n        custom_locale_path = os.path.join(self.test_dir, 'app_with_locale', 'custom_locale')\n        with override_settings(LOCALE_PATHS=[custom_locale_path]):\n            management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n            app_de_locale = os.path.join(custom_locale_path, 'de', 'LC_MESSAGES', 'django.po')\n            project_de_locale = os.path.join(\n                self.test_dir, 'locale', 'de', 'LC_MESSAGES', 'django.po')\n            self.assertTrue(os.path.exists(app_de_locale))\n            self.assertFalse(os.path.exists(project_de_locale)) \n            \n            with open(app_de_locale) as fp:\n                po_contents = fp.read()\n                self.assertMsgId('This app has a custom locale directory', po_contents)\n", "    def test_project_locale_paths_existing_locales(self):\n        os.makedirs(os.path.join(self.test_dir, 'project_locale', 'de', 'LC_MESSAGES'))\n        with open(os.path.join(self.test_dir, 'project_locale', 'de', 'LC_MESSAGES', 'django.po'), 'w') as f:\n            f.write('# This PO file already exists\\n')\n        with override_settings(LOCALE_PATHS=[os.path.join(self.test_dir, 'project_locale')]):\n            management.call_command('makemessages', locale=[LOCALE], verbosity=0)\n            project_de_locale = os.path.join(\n                self.test_dir, 'project_locale', 'de', 'LC_MESSAGES', 'django.po')\n            self.assertTrue(os.path.exists(project_de_locale))\n            with open(project_de_locale, 'r') as f:\n                contents = f.read()\n            self.assertIn('# This PO file already exists\\n', contents)\n"], "sample_865": ["    def test_decision_tree_sparse_empty_leaf(self):\n        # test for empty leaves with sparse input\n        X = csc_matrix((5, 1))\n        y = np.array([0, 0, 1, 1, 0])\n        clf = DecisionTreeClassifier()\n        clf.fit(X, y)\n        assert clf.tree_.n_node_samples[clf.tree_.children_left == TREE_LEAF].all() >= 0\n", "    def test_max_depth_pruning():\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        y = np.array([0, 1, 0])\n\n        for clf in [DecisionTreeClassifier, ExtraTreeClassifier]:\n            tree = clf(max_depth=1, random_state=0).fit(X, y)\n            assert tree.tree_.max_depth == 1\n            tree = clf(max_depth=2, random_state=0).fit(X, y)\n            assert tree.tree_.max_depth == 2\n", "compilation error", "    def test_min_samples_split_multioutput():\n        # Check that min_samples_split works as intended with multi-output data\n        X = np.array([[0, 1], [1, 0], [0, 1], [1, 0]])\n        y = np.array([[1, 0], [0, 1], [1, 0], [0, 1]])\n        clf = DecisionTreeClassifier(min_samples_split=3)\n        clf.fit(X, y)\n        assert clf.tree_.node_count == 3\n", "    def test_predict_with_empty_leaf():\n        # Ensure prediction works even when a leaf node contains no samples\n        X = np.array([[0], [1], [2]])\n        y = np.array([0, 1, 0])\n        clf = DecisionTreeClassifier(random_state=0, max_depth=1)\n\n        # Bias the split to create an empty left child\n        X_train = np.array([[0.1], [1.1]])\n        y_train = np.array([0, 1])\n\n        clf.fit(X_train, y_train)\n        predictions = clf.predict(X)\n        assert_array_equal(predictions, [0, 1, 0])\n"], "sample_1159": ["compilation error", "def test_failing_assumptions_with_symbols():\n    x = Symbol('x', real=True, positive=True)\n    y = Symbol('y', real=True)\n    assert failing_assumptions(6*x + y, **x.assumptions0) == \\\n    {'real': None, 'imaginary': None, 'complex': None, 'hermitian': None,\n    'positive': None, 'nonpositive': None, 'nonnegative': None, 'nonzero': None,\n    'negative': None, 'zero': None, 'extended_real': None, 'finite': None,\n    'infinite': None, 'extended_negative': None, 'extended_nonnegative': None,\n    'extended_nonpositive': None, 'extended_nonzero': None,\n    'extended_positive': None }\n", "compilation error", "compilation error", "    def test_failing_assumptions_with_specific_symbols():\n        x = Symbol('x', real=False)\n        y = Symbol('y', positive=True)\n        assert failing_assumptions(x + y, **x.assumptions0) == \\"], "sample_965": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_151": ["    def test_delete_model_with_proxy(self):\n        \"\"\"\n        #23463 - Deleting a model that has a proxy should delete both.\n        \"\"\"\n        changes = self.get_changes([self.author_empty, self.author_proxy], [self.author_empty])\n        # Right number/type of migrations?\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"DeleteModel\", \"DeleteModel\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='AuthorProxy')\n        self.assertOperationAttributes(changes, 'testapp', 0, 1, name='Author')\n\n", "    def test_alter_unique_together(self):\n        \"\"\"\n        ALTER UNIQUE TOGETHER should be applied correctly.\n        \"\"\"\n        unique_together = ModelState(\n            'testapp', 'Book',\n            [\n                ('title', models.CharField(max_length=100)),\n                ('author', models.ForeignKey('testapp.Author', on_delete=models.CASCADE)),\n            ],\n            unique_together=[('title', 'author')]\n        )\n        non_unique = ModelState(\n            'testapp', 'Book',\n            [\n                ('title', models.CharField(max_length=100)),\n                ('author', models.ForeignKey('testapp.Author', on_delete=models.CASCADE)),\n            ]\n        )\n        changes = self.get_changes([non_unique], [unique_together])\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AlterUniqueTogether\"])\n        self.assertOperationAttributes(\n            changes, 'testapp', 0, 0, name='book', unique_together=[('title', 'author')]\n        )\n", "    def test_remove_unique_together(self):\n        \"\"\"\n        Removing a unique_together constraint should generate a RemoveConstraint\n        operation.\n        \"\"\"\n        model_state_with_unique = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=100)),\n            (\"age\", models.IntegerField()),\n        ], unique_together=[(\"name\", \"age\")])\n        model_state_without_unique = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=100)),\n            (\"age\", models.IntegerField()),\n        ])\n        changes = self.get_changes([model_state_with_unique], [model_state_without_unique])\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RemoveConstraint\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0,\n                                       name=\"Author\", constraints=[(\"name\", \"age\")])\n\n", "    def test_alter_field_no_change(self):\n        \"\"\"\n        #23981 - AlterField should not generate a migration if no changes are made.\n        \"\"\"\n        changes = self.get_changes([self.author_with_age], [self.author_with_age])\n        self.assertNumberMigrations(changes, 'testapp', 0)\n", "    def test_alter_unique_together_remove_and_add(self):\n        \"\"\"\n        #23447 - Test removing and adding unique_together constraint.\n        \"\"\"\n        before = ModelState('testapp', 'Book', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=100)),\n            ('author', models.ForeignKey('testapp.Author', models.CASCADE)),\n        ], unique_together=[('title', 'author')])\n        after = ModelState('testapp', 'Book', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=100)),\n            ('author', models.ForeignKey('testapp.Author', models.CASCADE)),\n        ], unique_together=[('title', 'author', 'id')])\n        changes = self.get_changes([before], [after])\n\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\"AlterUniqueTogether\"])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='Book', unique_together=[('title', 'author', 'id')])\n"], "sample_84": ["    def test_parse_http_date_invalid(self):\n        with self.assertRaises(ValueError):\n            parse_http_date('Invalid date string')\n", "    def test_parse_http_date_invalid(self):\n        with self.assertRaises(ValueError):\n            parse_http_date('invalid date')\n", "    def test_parse_http_date_safe(self):\n        self.assertIsNone(parse_http_date_safe('Totally invalid date string'))\n        self.assertIsNone(parse_http_date_safe('Sun Nov  6 08:49:37'))\n        self.assertEqual(parse_http_date_safe('Sun Nov  6 08:49:37 1994'), datetime(1994, 11, 6, 8, 49, 37))\n", "    def test_parse_http_date_bad_format(self):\n        with self.assertRaises(ValueError):\n            parse_http_date('invalid date string')\n", "    def test_parse_http_date_invalid(self):\n        with self.assertRaises(ValueError):\n            parse_http_date('Invalid date string')\n"], "sample_886": ["    def test__wrap_in_pandas_container_index_preservation():\n        \"\"\"Check index is preserved when input is a DataFrame.\"\"\"\n        pd = pytest.importorskip(\"pandas\")\n        X_df = pd.DataFrame([[1, 0, 3], [0, 0, 1]], index=['a','b'], columns=[\"f0\", \"f1\", \"f2\"])\n        new_df = _wrap_in_pandas_container(X_df)\n        assert_array_equal(new_df.index, X_df.index)\n", "def test__wrap_data_with_container():\n    pd = pytest.importorskip(\"pandas\")\n    X = np.asarray([[1, 0, 3], [0, 0, 1]])\n    est = EstimatorWithSetOutput().fit(X)\n    est.set_output(transform=\"pandas\")\n\n    # Test with no original input\n    wrapped_data = _wrap_data_with_container(\"transform\", X, None, est)\n    assert isinstance(wrapped_data, pd.DataFrame)\n\n    # Test with original input\n    original_input = pd.DataFrame(X, columns=[\"a\", \"b\", \"c\"])\n    wrapped_data = _wrap_data_with_container(\"transform\", X, original_input, est)\n    assert isinstance(wrapped_data, pd.DataFrame)\n    assert_array_equal(wrapped_data.columns, [\"X0\", \"X1\", \"X2\"])\n    assert_array_equal(wrapped_data.index, original_input.index)\n", "def test__wrap_data_with_container_preserves_original_input():\n        \"\"\"Check that the original input is not modified.\"\"\"\n        pd = pytest.importorskip(\"pandas\")\n        X = np.asarray([[1, 0, 3], [0, 0, 1]])\n        est = EstimatorWithSetOutput()\n        est.set_output(transform=\"pandas\")\n        X_wrapped = _wrap_data_with_container(\"transform\", X, X, est)\n        assert X_wrapped is not X\n        assert X_wrapped.shape == X.shape\n        assert isinstance(X_wrapped, pd.DataFrame)\n        assert (X_wrapped.to_numpy() == X).all()\n", "def test_set_output_index_preservation():\n   \"\"\"Check that set_output preserves the original index.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    est = EstimatorWithSetOutputIndex()\n    X = pd.DataFrame([[1, 0, 3], [0, 0, 1]], index=['a', 'b'])\n    est.fit(X)\n    est.set_output(transform=\"pandas\")\n    X_trans = est.transform(X)\n    assert_array_equal(X_trans.index, X.index)\n", "    def test_set_output_mixin_with_original_index():\n        \"\"\"Check the index is preserved when wrapping with pandas.\"\"\"\n        pd = pytest.importorskip(\"pandas\")\n        X = np.asarray([[1, 0, 3], [0, 0, 1]])\n        index = pd.Index(['a', 'b'])\n        X_df = pd.DataFrame(X, index=index)\n\n        est = EstimatorWithSetOutputIndex().fit(X_df)\n        est.set_output(transform=\"pandas\")\n        X_trans = est.transform(X_df)\n        assert_array_equal(X_trans.index, index)\n\n"], "sample_1056": ["    def test_custom_printed_object():\n        obj = CustomPrintedObject()\n        assert lambdarepr(obj) == 'lambda'\n        assert TensorflowPrinter().doprint(obj) == 'tensorflow'\n        assert NumPyPrinter().doprint(obj) == 'numpy'\n        assert NumExprPrinter().doprint(obj) == 'numexpr'\n", "def test_NumExprPrinter():\n    # Make sure that NumExprPrinter correctly calls evaluate\n    # for the numexpr library and raises a TypeError for unsupported function\n    npr = NumExprPrinter()\n    l = npr.doprint(x + y)\n    assert \"evaluate('\" in l\n    assert \"'x + y')\" in l\n\n    raises(TypeError, lambda: npr.doprint(sin(x).apply(lambda u: u**2)))\n", "def test_NumExprPrinter():\n    p = NumExprPrinter()\n    assert p.doprint(x + y) == \"x+y\"\n\n    assert p.doprint(sin(x)) == \"sin(x)\"\n\n    assert p.doprint(x**2) == \"x**2\"\n\n    assert p.doprint(sqrt(x)) == \"sqrt(x)\"\n\n    assert p.doprint(x*y) == \"x*y\"\n\n    assert p.doprint(x/y) == \"x/y\"\n\n    assert p.doprint(x > y) == \"(x > y)\"\n\n    assert p.doprint(x < y) == \"(x < y)\"\n\n    assert p.doprint(x == y) == \"(x == y)\"\n\n    assert p.doprint(x != y) == \"(x != y)\"\n\n\n\n    raises(TypeError, lambda: p.doprint(Matrix([[x, y]])))\n    raises(TypeError, lambda: p.doprint(Piecewise((x , x > 0))))\n    raises(TypeError, lambda: p.doprint(Sum(x**i, (i, 0, 5))))\n\n", "    def test_numexpr():\n        expr = x**2 + 2*x + 1\n        l = NumExprPrinter().doprint(expr)\n        assert eval(l) == expr.subs(x, 3)\n\n", "def test_lambdarepr_NumExprPrinter():\n    # Test basic NumExprPrinter functionality\n\n    nprinter = NumExprPrinter()\n\n    # Basic operations\n    assert nprinter.doprint(x + y) == \"(x + y)\"\n    assert nprinter.doprint(x * y) == \"(x * y)\"\n    assert nprinter.doprint(x**y) == \"((x)**(y))\"\n\n    # Trigonometric functions\n    assert nprinter.doprint(sin(x)) == \"sin(x)\"\n    assert nprinter.doprint(sqrt(x)) == \"sqrt(x)\"\n\n    # Boolean operations\n    assert nprinter.doprint(x > y) == \"(x > y)\"\n    assert nprinter.doprint(x and y) == \"(x and y)\"\n\n"], "sample_599": ["def test_UnsignedIntegerCoder():\n    original = xr.Variable((\"x\",), np.array([-1, 0, 10, 255], dtype='i1'))\n    coder = variables.UnsignedIntegerCoder()\n    encoded = coder.encode(original)\n    assert encoded.dtype == np.dtype('i1')\n    assert encoded.attrs['_Unsigned'] == 'true'\n    decoded = coder.decode(encoded)\n    assert_identical(decoded, original)\n", "def test_CFScaleOffsetCoder_encode_float16():\n    original = xr.Variable(\n        (\"x\",), np.arange(10, dtype=np.float16), encoding=dict(scale_factor=10)\n    )\n    coder = variables.CFScaleOffsetCoder()\n    encoded = coder.encode(original)\n    assert encoded.dtype == np.float32\n", "compilation error", "    def test_scaling_converts_to_float64(dtype):\n        original = xr.Variable(\n            (\"x\",), np.arange(10, dtype=dtype), encoding=dict(scale_factor=10)\n        )\n        coder = variables.CFScaleOffsetCoder()\n        encoded = coder.encode(original)\n        assert encoded.dtype == np.float64\n        roundtripped = coder.decode(encoded)\n        assert_identical(original, roundtripped)\n        assert roundtripped.dtype == np.float64\n", "compilation error"], "sample_683": ["   def test_capture_with_live_logging_no_stdout(testdir, capsys):\n        testdir.makepyfile(\n            \"\"\"\n            import logging\n            import sys\n\n            logger = logging.getLogger(__name__)\n\n                logging.info(\"something\")\n                print(\"hello\")\n                logging.info(\"something\")\n\n        \"\"\"\n        )\n        result = testdir.runpytest_subprocess(\"--log-cli-level=INFO\", \"--capture=no\")\n        assert result.ret == 0\n        out, err = capsys.readouterr()\n        assert \"hello\" in out\n        assert \"something\" in err\n", "    def test_global_capture_with_live_logging_and_failure(testdir):\n        testdir.makeconftest(\n            \"\"\"\n                if \"test_global\" in report.nodeid:\n                    if report.when == \"teardown\":\n                        with open(\"caplog\", \"w\") as f:\n                            f.write(report.caplog)\n                        with open(\"capstdout\", \"w\") as f:\n                            f.write(report.capstdout)\n        \"\"\"\n        )\n\n        testdir.makepyfile(\n            \"\"\"\n            import logging\n            import sys\n            import pytest\n\n            logger = logging.getLogger(__name__)\n\n            @pytest.fixture\n                print(\"fix setup\")\n                logging.info(\"fix setup\")\n                yield\n                logging.info(\"fix teardown\")\n                print(\"fix teardown\")\n\n                print(\"begin test\")\n                logging.info(\"something in test\")\n                assert False\n                print(\"end test\")\n        \"\"\"\n        )\n        result = testdir.runpytest_subprocess(\"--log-cli-level=INFO\")\n        assert result.ret == 1\n\n        with open(\"caplog\") as f:\n            caplog = f.read()\n\n        assert \"fix setup\" in caplog\n        assert \"something in test\" in caplog\n        assert \"fix teardown\" in caplog\n\n        with open(\"capstdout\") as f:\n            capstdout = f.read()\n\n        assert \"fix setup\" in capstdout\n        assert \"begin test\" in capstdout\n        assert \"end test\" not in capstdout\n        assert \"fix teardown\" in capstdout\n", "def test_capture_binary_output_with_encoding(testdir, capsys):\n    testdir.makepyfile(\n        \"\"\"\n        import sys\n\n            sys.stdout.buffer.write(b'\\\\xff\\\\xfe')\n        \"\"\"\n    )\n    result = testdir.runpytest()\n    assert result.ret == 0\n    out, err = capsys.readouterr()\n    assert out == \"\\xff\\xfe\"\n", "def test_capture_with_plugin_output(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n            logging.info(\"hello\")\n        \"\"\"\n    )\n    testdir.monkeypatch.setenv(\"PYTEST_DISABLE_PLUGIN_AUTOLOAD\", \"1\")\n    result = testdir.runpytest(\n        \"--log-cli-level=INFO\",\n        \"-p\",\n        \"pytest-capturelog\",\n    )\n    assert result.ret == 0\n\n", "    def test_multi_capture_resume_suspend(capsys):\n        cap = capture.MultiCapture()\n        cap.start_capturing()\n\n        # Print something to stdout and stderr\n        print(\"hello world\")\n        sys.stderr.write(\"error\\n\")\n\n        # Suspend capturing\n        cap.suspend()\n\n        # Print something again\n        print(\"this should not be captured\")\n        sys.stderr.write(\"this should not be captured either\\n\")\n\n        # Resume capturing\n        cap.resume()\n\n        # Print something more\n        print(\"back to normal\")\n        sys.stderr.write(\"back to error\\n\")\n        (out, err) = cap.readouterr()\n\n        assert out == \"hello world\\nback to normal\\n\"\n        assert err == \"error\\nback to error\\n\"\n"], "sample_942": ["def test_pyexception_with_module(app):\n    text = \".. py:exception:: exceptions.IOError\"\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_annotation, \"exception \"],\n                                                    [desc_addname, \"exceptions.\"],\n                                                    [desc_name, \"IOError\"])],\n                                  desc_content)]))\n    assert_node(doctree[1], desc, desctype=\"exception\",\n                domain=\"py\", objtype=\"exception\", noindex=False)\n    assert 'exceptions.IOError' in domain.objects\n    assert domain.objects['exceptions.IOError'] == ('index', 'exceptions.IOError', 'exception', False)\n\n", "def test_domain_specific_index(app):\n    text = (\".. py:function:: func1\\n\"\n            \".. py:function:: func2()\\n\"\n            \"   :domain-specific-index: mydomain\\n\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc, addnodes.index, desc))\n    assert_node(doctree[0], addnodes.index, entries=[('pair', 'built-in function; func1()', 'func1', '', None)])\n    assert 'func1' in domain.objects\n    assert_node(doctree[2], addnodes.index, entries=[])\n    assert 'func2' not in domain.objects\n\n", "compilation error", "def test_pymodule_index_duplicate(app):\n    text = (\".. py:module:: foo\\n\"\n            \".. py:module:: foo.bar\\n\"\n            \".. py:module:: foo.bar\\n\")\n    restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    assert index.generate() == (\n        [('f', [IndexEntry('foo', 0, 'index', 'module-foo', '', '', ''),\n                IndexEntry('foo.bar', 1, 'index', 'module-foo.bar', '', '', '')])],\n        False\n    )\n", "def test_pymodule_noindex(app):\n    text = \".. py:module:: module\\n\"\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index, desc))\n    assert_node(doctree[0], addnodes.index, entries=[('single', 'module', 'module', '', None)])\n\n"], "sample_680": ["    def test_importorskip_with_reason(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n            pytest.importorskip(\"doesnotexist\", reason=\"need this for foo\")\n                pass\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines(\n            [\"*SKIP*need this for foo*\", \"*1 skipped*\"]\n        )\n", "    def test_importorskip_not_a_module(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n                pytest.importorskip(\"not_a_module\", reason=\"reason\")\n                assert False\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines(\n            [\n                \"*reason*\",\n                \"*SKIPPED*\",\n            ]\n        ) \n", "    def test_importorskip_with_reason(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n                pytest.importorskip(\"doesnotexist\", reason=\"This module is optional\")\n                assert False\n\n            \n            \"\"\"\n        )\n        result = testdir.runpytest(\"-rs\")\n        result.stdout.fnmatch_lines(\n            [\n                \"*SKIP*test_import*\",\n                \"*This module is optional*\",\n            ]\n        )\n", "    def test_importorskip_conditional(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.skipif('doesnotexist' in sys.modules)\n                assert False\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines([\"*1 skipped*\"])\n\n", "    def test_importorskip_condition(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n            @pytest.mark.skipif(not pytest.importorskip(\"os.path\"), reason=\"os.path is not available\")\n                pass\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines([\"*1 passed*\"])\n        result = testdir.runpytest(\"-rs\")\n        result.stdout.fnmatch_lines([\"*1 skipped*\"])\n"], "sample_853": ["def test_transform_target_regressor_pipeline():\n    X, y = friedman\n    tt = Pipeline([('transformer', StandardScaler()),\n                   ('regressor', LinearRegression())])\n    reg = TransformedTargetRegressor(regressor=tt)\n    reg.fit(X, y)\n    reg.predict(X)\n\n", "def test_transform_target_regressor_pipeline():\n    X, y = friedman\n    pipe = Pipeline([\n        ('transformer', StandardScaler()),\n        ('regressor', LinearRegression())\n    ])\n    tt = TransformedTargetRegressor(regressor=pipe, func=np.log, inverse_func=np.exp)\n    tt.fit(X, y)\n    tt.predict(X)\n", "    def test_transform_target_regressor_pipeline():\n        X, y = friedman\n        regr = Pipeline([\n            ('transformer', StandardScaler()),\n            ('regressor', LinearRegression())\n        ])\n        tt = TransformedTargetRegressor(regressor=regr)\n        tt.fit(X, y)\n        y_pred = tt.predict(X)\n        assert y.shape == y_pred.shape\n", "def test_transform_target_regressor_pipeline():\n    X, y = friedman\n    regr = Pipeline([\n        ('transformer', StandardScaler()),\n        ('regressor', LinearRegression())\n    ])\n    tt = TransformedTargetRegressor(regressor=regr)\n    tt.fit(X, y)\n    tt.predict(X)\n", "def test_transform_target_regressor_pipeline():\n    X, y = friedman\n    preprocessor = Pipeline(\n        steps=[\n            ('scaler', StandardScaler()),\n            ('transformer', FunctionTransformer(func=np.log,\n                                                 inverse_func=np.exp,\n                                                 validate=True,\n                                                 check_inverse=False))\n        ])\n    regr = TransformedTargetRegressor(regressor=LinearRegression(),\n                                     transformer=preprocessor)\n\n    regr.fit(X, y)\n    y_pred = regr.predict(X)\n    assert y.shape == y_pred.shape\n"], "sample_1123": ["def test_ConditionSet_assumptions():\n    n = Symbol('n', negative=True)\n    assert ConditionSet(n, n < 1, S.Integers) == S.EmptySet\n    p = Symbol('p', positive=True)\n    assert ConditionSet(n, n < p, S.Integers) == S.EmptySet\n", "def test_ConditionSet_empty():\n    assert ConditionSet(x, S.false, S.Reals) == EmptySet\n    assert ConditionSet(x, S.false, FiniteSet(1,2,3)) == EmptySet\n    assert ConditionSet(x, x < 1, EmptySet) == EmptySet\n", "compilation error", "compilation error", "def test_ConditionSet_base_set_empty():\n    assert ConditionSet(x, x > 1, EmptySet) == EmptySet\n    assert ConditionSet(x, x < 1, EmptySet) == EmptySet\n    assert ConditionSet(x, True, EmptySet) == EmptySet\n    assert ConditionSet(x, False, EmptySet) == EmptySet\n"], "sample_1149": ["def test_S_as_sympify():\n    assert S(1) == 1\n    assert S(1.0) == 1\n    assert isinstance(S(\"1/2\"), Rational)\n", "def test_S_sympify():\n    assert S(1) == 1\n    assert S(1.0) == Rational(1, 1)\n    assert S('x') == Symbol('x')\n", "def test_S_creation():\n    assert S(1) == 1\n    assert S(1.0) == 1\n    assert S('1') == 1\n    assert isinstance(S('1/2'), Rational)\n", "def test_S_access():\n    assert S.Zero is S.Integer(0)\n    assert S.One is S.Integer(1)\n    assert S.Half is S.Rational(1, 2)\n\n    assert S(1) is S.One\n    assert S('1/2') is S.Half\n", "def test_singleton_access_before_definition():\n    with pytest.raises(AttributeError):\n        S.TestSingleton\n    class TestSingleton(Basic, metaclass=Singleton):\n        pass\n    assert TestSingleton() is S.TestSingleton\n"], "sample_148": ["    def test_lookup_field_related_object_with_get_absolute_url(self):\n        article = Article.objects.create(title=\"Test article\")\n        self.assertEqual(\n            lookup_field(\"get_absolute_url\", article)[2],\n            \"/article/%s/\" % article.id\n        )\n", "    def test_flatten_empty_fieldsets(self):\n        fieldsets = (\n            (None, {\n                'fields': ()\n            }),\n        )\n        self.assertEqual(flatten_fieldsets(fieldsets), [])\n\n\n\n", "    def test_construct_change_message(self):\n        form = forms.ModelForm(instance=Article(title='My Article'))\n        form.changed_data = {'title': 'My Updated Article'}\n        formset = None\n        add = False\n        expected_message = [{'changed': {'fields': ['title']}}]\n        self.assertEqual(construct_change_message(form, formset, add), expected_message)\n\n", "    def test_lookup_field_related_object(self):\n        \"\"\"\n        Regression test for #23546.\n        \"\"\"\n        article = Article.objects.create(title=\"Test Article\")\n        self.assertEqual(\n            lookup_field('location', article, return_attr=True),\n            ('location', 'location')\n        )\n", "compilation error"], "sample_1179": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_264": ["    def test_cookie_storage_deprecation(self):\n        \"\"\"\n        Tests that the cookie storage correctly handles deprecation warnings\n        and uses the new signing mechanism when the deprecated one is used.\n        \"\"\"\n        storage = self.storage_class(self.get_request())\n        messages = ['test', 'me']\n        set_cookie_data(storage, messages)\n        # The message contains what's expected.\n\n        self.assertEqual(list(storage), messages)\n", "    def test_empty_message_list(self):\n        storage = self.storage_class(self.get_request())\n        response = self.get_response()\n        storage.update(response)\n        self.assertEqual(self.stored_messages_count(storage, response), 0)\n", "    def test_not_finished_sentinel(self):\n        storage = self.get_storage()\n        response = self.get_response()\n        for i in range(5):\n            storage.add(constants.INFO, get_random_string(20))\n        storage.update(response)\n\n        # Check that some messages are left in storage\n        cookie_storing = self.stored_messages_count(storage, response)\n        self.assertEqual(cookie_storing, 4)\n\n        # Retrieve the remaining messages\n        self.assertEqual(len(list(storage)), 4)\n\n", "    def test_cookie_storage_deprecation(self):\n        storage = self.storage_class(self.get_request())\n        messages = ['test', 'me']\n        # Set initial data.\n        set_cookie_data(storage, messages, encode_empty=True)\n        self.assertEqual(list(storage), [])\n", "    def test_legacy_hash_decode_invalid(self):\n        storage = self.storage_class(self.get_request())\n        messages = ['this', 'that']\n        # Encode/decode a message using the pre-Django 3.1 hash.\n        encoder = MessageEncoder()\n        value = encoder.encode(messages)\n        encoded_messages = '%s$%s' % (storage._legacy_hash(value)[:-1], value)  # Truncate the hash\n        decoded_messages = storage._decode(encoded_messages)\n        self.assertIsNone(decoded_messages)\n\n"], "sample_167": ["    def test_naturaltime_with_timezone(self):\n        tz_one = get_fixed_timezone(-720)\n        tz_two = get_fixed_timezone(720)\n        now_one = datetime.datetime(2012, 3, 9, 1, 30, tzinfo=tz_one)\n        now_two = datetime.datetime(2012, 3, 9, 1, 30, tzinfo=tz_two)\n\n        test_list = [now_one, now_two]\n        result_list = ['now', 'now']\n        with translation.override('en'):\n            self.humanize_tester(test_list, result_list, 'naturaltime')\n\n", "    def test_apnumber_with_float(self):\n        test_list = [str(x) for x in range(1, 11)]\n        test_list.append('3.14')\n        test_list.append('1.0')\n        test_list.append(None)\n        result_list = ('one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', '3.14', '1', None)\n        with translation.override('en'):\n            self.humanize_tester(test_list, result_list, 'apnumber')\n\n", "    def test_naturaltime_with_timezone(self):\n        now = datetime.datetime.now(utc)\n        one_hour_later = now + datetime.timedelta(hours=1)\n        test_list = (\n            (now.replace(tzinfo=utc).isoformat(), 'now'),\n            (one_hour_later.replace(tzinfo=utc).isoformat(), 'an hour from now'),\n        )\n        with translation.override('en'):\n            self.humanize_tester(test_list, ['now', 'an hour from now'], 'naturaltime')\n", "    def test_naturaltime_with_timezone(self):\n        now = datetime.datetime(2012, 3, 9, 22, 30, tzinfo=get_fixed_timezone(480))\n        test_list = [\n            now,\n            now - datetime.timedelta(seconds=1),\n            now + datetime.timedelta(seconds=1),\n        ]\n        result_list = [\n            'now',\n            'a second ago',\n            'a second from now',\n        ]\n        with translation.override('en'):\n            self.humanize_tester(test_list, result_list, 'naturaltime') \n", "    def test_naturaltime_timezones(self):\n        with override_settings(USE_TZ=True, TIME_ZONE='America/Los_Angeles'):\n            now = datetime.datetime.now(utc).astimezone(get_fixed_timezone(-480))\n            result = humanize.naturaltime(now)\n            self.assertEqual(result, 'now')\n"], "sample_530": ["    def test_picking_no_pick(self, child_type):\n        if child_type == 'draw':\n            picking_child = DrawingArea(5, 5)\n            picking_child.add_artist(mpatches.Rectangle((0, 0), 5, 5, linewidth=0))\n        elif child_type == 'image':\n            im = np.ones((5, 5))\n            im[2, 2] = 0\n            picking_child = OffsetImage(im)\n        elif child_type == 'text':\n            picking_child = TextArea('\\N{Black Square}', textprops={'fontsize': 5})\n        else:\n            assert False, f'Unknown picking child type {child_type}'\n\n        fig, ax = plt.subplots()\n        ab = AnnotationBbox(picking_child, (0.5, 0.5), boxcoords='axes fraction')\n        ab.set_picker(False)\n        ax.add_artist(ab)\n\n        calls = []\n        fig.canvas.mpl_connect('pick_event', lambda event: calls.append(event))\n\n        # Annotation should *not* be picked by an event.\n        x, y = ax.transAxes.transform_point((0.5, 0.5))\n        fig.canvas.draw()\n        calls.clear()\n        MouseEvent(\n            \"button_press_event\", fig.canvas, x, y, MouseButton.LEFT)._process()\n        assert len(calls) == 0\n", "compilation error", "    def test_annotationbbox_no_offsetbox(self):\n        fig, ax = plt.subplots()\n        ax.plot([0, 1], [0, 1])\n        ab = AnnotationBbox(\n            \"Test\", (1, 1), xybox=(2, 2),\n            boxcoords='data',\n            arrowprops=dict(arrowstyle=\"->\"),\n        )\n        ax.add_artist(ab)\n        fig.canvas.draw()\n\n        # Check if the annotation is drawn correctly without an OffsetBox\n\n", "    def test_offsetbox_clip_children_with_artist(self):\n        # - create a plot\n        # - put an AnchoredOffsetbox with a child DrawingArea\n        #   at the center of the axes\n        # - give the DrawingArea a gray background\n        # - put a black line across the bounds of the DrawingArea\n        # - see that the black line is clipped to the edges of\n        #   the DrawingArea.\n        fig, ax = plt.subplots()\n        size = 100\n        da = DrawingArea(size, size, clip=True)\n        bg = mpatches.Rectangle((0, 0), size, size,\n                                facecolor='#CCCCCC',\n                                edgecolor='None',\n                                linewidth=0)\n        line = mlines.Line2D([-size*.5, size*1.5], [size/2, size/2],\n                             color='black',\n                             linewidth=10)\n        anchored_box = AnchoredOffsetbox(\n            loc='center',\n            child=da,\n            pad=0.,\n            frameon=False,\n            bbox_to_anchor=(.5, .5),\n            bbox_transform=ax.transAxes,\n            borderpad=0.)\n\n        da.add_artist(bg)\n        da.add_artist(line)\n        ax.add_artist(anchored_box)\n        ax.set_xlim((0, 1))\n        ax.set_ylim((0, 1))\n\n        fig.canvas.draw()\n        assert not fig.stale\n        da.clip_children = True\n        assert fig.stale\n", "def test_annotationbbox_zorder():\n    fig, ax = plt.subplots()\n    rect = mpatches.Rectangle((0.2, 0.2), 0.2, 0.2, facecolor='blue')\n    ax.add_patch(rect)\n\n    da = DrawingArea(20, 20, 0, 0, clip=True)\n    p = mpatches.Circle((-10, 30), 32)\n    da.add_artist(p)\n    ab = AnnotationBbox(da, [.5, .5], xybox=(-0.2, 0.5),\n                        xycoords='data',\n                        boxcoords=\"axes fraction\", box_alignment=(0., .5),\n                        arrowprops=dict(arrowstyle=\"->\"), zorder=10)\n    ax.add_artist(ab)\n\n    fig.canvas.draw()\n    renderer = fig.canvas.get_renderer()\n    # Check if the annotation is drawn on top of the rectangle\n    rect_order = rect.get_zorder()\n    ab_order = ab.get_zorder()\n    assert ab_order > rect_order\n"], "sample_39": ["compilation error", "    def test_wcs_to_header_empty_data(self):\n        w = wcs.WCS(naxis=2)\n        header = w.to_header(relax=True)\n        assert 'CDELT1' in header\n        assert 'CDELT2' in header\n", "compilation error", "compilation error", "    def test_no_wcs_exception():\n\n        with pytest.raises(ValueError) as excinfo:\n            w = wcs.WCS(naxis=2)\n        assert excinfo.value.args[0] == \"No WCS information found.\"\n"], "sample_1191": ["compilation error", "compilation error", "def test_hermite_normal_form_modulo_D():\n    A = DM([[12, 6, 4, 8], [3, 9, 6, 12], [2, 16, 14, 28], [20, 10, 10, 20]], ZZ)\n    D = ZZ(2*3*5*7)\n    hnf = hermite_normal_form(A, D=D).to_dense()\n\n    assert hnf.shape == (4, 4)\n\n    assert hnf[0, 0] != 0\n    assert hnf[1, 1] != 0\n    assert hnf[2, 2] != 0\n\n    assert all(hnf[i, j] % D == 0 for i in range(3) for j in range(3, 4))\n\n\n\n", "def test_hermite_normal_form():\n    A = DM([[12, 6, 4], [3, 9, 6], [2, 16, 14]], ZZ)\n    H = DM([[10, 0, 2], [0, 15, 3], [0, 0, 2]], ZZ)\n\n    assert hermite_normal_form(A).to_dense() == H\n\n    A = DM([[1, 2, 3], [4, 5, 6], [7, 8, 9]], ZZ)\n    H = DM([[1, 0, -1], [0, 1, 2], [0, 0, 0]], ZZ)\n    assert hermite_normal_form(A).to_dense() == H\n\n    A = DM([[1, 2], [3, 4]], ZZ)\n\n    raises(DMShapeError, lambda: hermite_normal_form(A, D=2, check_rank=True))\n", "def test_hermite_normal_form_modulo_D():\n    m = DM([[12, 6, 4], [3, 9, 6], [2, 16, 14]], ZZ)\n    D = 30\n    hnf = DM([[10, 0, 2], [0, 15, 3], [0, 0, 2]], ZZ)\n    assert hermite_normal_form(m, D=D).to_dense() == hnf\n    raises(DMDomainError, lambda: hermite_normal_form(m, D=ZZ(29)))\n    raises(DMShapeError, lambda: hermite_normal_form(m.transpose(), D=D))\n"], "sample_917": ["    def check_xref_tag(role, tag):\n        pattern = (r'{role}-role:.*?'\n                   r'<(?P<tag>{tag}) .*?>(.*?)</(?P=tag)>').format(role=role, tag=tag)\n        result = re.findall(pattern, output)\n        expect = '''\\", "    def check_classes(role, tag, expected_classes):\n        classes_set = classes(role, tag)\n        assert classes_set == set(expected_classes)\n", "    def check(role, tag, expect):\n        assert classes(role, tag) == expect, \\\n            \"Classes for role '{}' and tag '{}' not as expected\".format(role, tag)\n", "compilation error", "compilation error"], "sample_943": ["def test_pep_0420_enabled_no_subpackages(make_app, apidoc):\n    outdir = apidoc.outdir\n    assert (outdir / 'conf.py').isfile()\n    assert (outdir / 'a.rst').isfile()\n\n    app = make_app('text', srcdir=outdir)\n    app.build()\n    print(app._status.getvalue())\n    print(app._warning.getvalue())\n\n    builddir = outdir / '_build' / 'text'\n    assert (builddir / 'a.txt').isfile()\n", "def test_module_first(make_app, apidoc):\n    outdir = apidoc.outdir\n    assert (outdir / 'conf.py').isfile()\n    assert (outdir / 'index.rst').isfile()\n    \n    content = (outdir / 'index.rst').read_text() \n    assert \"Module contents\" in content\n", "def test_pep_0420_enabled_no_init(make_app, apidoc):\n    outdir = apidoc.outdir\n    assert (outdir / 'conf.py').isfile()\n    assert (outdir / 'c.rst').isfile()\n    assert not (outdir / 'x.rst').exists()\n\n    with open(outdir / 'c.rst') as f:\n        rst = f.read()\n        assert \"c package\" in rst\n        assert \"automodule:: c\" in rst\n        assert \"automodule:: c.d\" in rst\n\n    app = make_app('text', srcdir=outdir)\n    app.build()\n    print(app._status.getvalue())\n    print(app._warning.getvalue())\n", "def test_pep_0420_enabled_dot_in_package(make_app, apidoc):\n    outdir = apidoc.outdir\n    assert (outdir / 'conf.py').isfile()\n    assert (outdir / 'a.b.c.rst').isfile()\n    assert (outdir / 'a.b.e.rst').isfile()\n\n\n", "def test_custom_template(make_app, apidoc):\n    outdir = apidoc.outdir\n    assert (outdir / 'conf.py').isfile()\n    assert (outdir / 'index.rst').isfile()\n\n    with open(outdir / 'index.rst') as f:\n        rst = f.read()\n    assert \"This is a custom template\" in rst\n\n    app = make_app('text', srcdir=outdir)\n    app.build()\n    print(app._status.getvalue())\n    print(app._warning.getvalue())\n"], "sample_428": ["    def test_zero_decimal_with_grouping(self):\n        self.assertEqual(nformat(0, \".\", grouping=3, thousand_sep=\",\"), \"0\")\n        self.assertEqual(nformat(0.0, \".\", grouping=3, thousand_sep=\",\"), \"0\")\n        self.assertEqual(nformat(Decimal(\"0\"), \".\", grouping=3, thousand_sep=\",\"), \"0\")\n        self.assertEqual(nformat(Decimal(\"0.0\"), \".\", grouping=3, thousand_sep=\",\"), \"0\")\n", "    def test_large_number_scientific_notation():\n        self.assertEqual(\n            nformat(\n                Decimal(\"1.23456789012345678901234567890e+300\"),\n                \".\",\n                decimal_pos=3,\n            ),\n            \"1.235e+300\",\n        )\n        self.assertEqual(\n            nformat(\n                Decimal(\"1e-300\"), \".\", decimal_pos=3\n            ),\n            \"0.000\",\n        )\n\n", "    def test_format_decimal_with_custom_grouping(self):\n        self.assertEqual(\n            nformat(Decimal(\"1234567890\"), \".\", grouping=[3, 2, 0]),\n            \"1,234,567.890\",\n        )\n        self.assertEqual(\n            nformat(Decimal(\"1234567890\"), \".\", grouping=[3, 2, 0], decimal_pos=2),\n            \"1,234,567.89\",\n        )\n        self.assertEqual(\n            nformat(Decimal(\"1234567890\"), \".\", grouping=(3, 2, 0)),\n            \"1,234,567.890\",\n        )\n        self.assertEqual(\n            nformat(Decimal(\"1234567\"), \".\", grouping=[2, 1, 1]), \"12,34,567\"\n        )\n", "    def test_number_like_strings(self):\n        self.assertEqual(nformat(\"1,234\", \".\"), \"1234\")\n        self.assertEqual(nformat(\"1,234.56\", \".\"), \"1234.56\")\n        self.assertEqual(nformat(\"-1,234.56\", \".\"), \"-1234.56\")\n", "    def test_non_uniform_grouping(self):\n        self.assertEqual(nformat(1234567890, \".\", grouping=(3, 2, 0)), \"1,23,45,67890\")\n        self.assertEqual(nformat(1234567890, \".\", decimal_pos=2, grouping=(3, 2, 0)), \"1,23,45,67890.00\")\n        self.assertEqual(nformat(-1234567890, \".\", grouping=(3, 2, 0)), \"-1,23,45,67890\")\n        self.assertEqual(nformat(1234567.89, \".\", grouping=(3, 2, 0)), \"123,456.89\")\n        self.assertEqual(nformat(12345678901234567890, \".\", grouping=(3, 2, 0)), \"123,456,789,012,345,678,90\")\n"], "sample_431": ["    def test_refresh_on_proxy_model(self):\n        pm = ProxyArticle.objects.create(headline=\"Proxy Article Test\")\n        pm.headline = \"Updated Headline\"\n        pm.save()\n        pm.refresh_from_db()\n        self.assertEqual(pm.headline, \"Updated Headline\")\n", "    def test_refresh_m2m(self):\n        a = Article.objects.create(headline=\"Article 1\", pub_date=datetime.now())\n        b = Article.objects.create(headline=\"Article 2\", pub_date=datetime.now())\n        c = Article.objects.create(headline=\"Article 3\", pub_date=datetime.now())\n        tag1 = Tag.objects.create(name=\"Django\")\n        tag2 = Tag.objects.create(name=\"Python\")\n        a.tags.add(tag1, tag2)\n        b.tags.add(tag1)\n        a_copy = Article.objects.get(pk=a.pk)\n        a_copy.tags.add(Tag.objects.create(name=\"Testing\"))\n        with self.assertNumQueries(1):\n            a_copy.refresh_from_db()\n        self.assertCountEqual([t.name for t in a_copy.tags.all()], [\"Django\", \"Python\"])\n", "    def test_refresh_m2m_field(self):\n        a = Article.objects.create()\n        a.tags.add('django', 'python')\n        a.refresh_from_db()\n        self.assertEqual(set(a.tags.names()), {'django', 'python'})\n", "    def test_refresh_on_unsaved_instance(self):\n        a = Article(headline=\"New article\", pub_date=datetime.now())\n        with self.assertRaises(ValueError):\n            a.refresh_from_db()\n        \n", "    def test_save_empty_related_manager(self):\n        article = Article.objects.create()\n        article.featuredarticles.add()\n        article.save()\n        with self.assertNumQueries(1):\n            article.featuredarticles.clear()\n\n\n"], "sample_487": ["        def custom_permission_action2(modeladmin, request, queryset):\n            pass\n", "    def test_actions_valid_with_permission(self):\n        @admin.action(permissions=[\"custom_permission\"])\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (custom_permission_action,)\n\n                return True\n\n        self.assertIsValid(BandAdmin, Band)\n", "    def test_actions_not_callable(self):\n        class BandAdmin(ModelAdmin):\n            actions = \"not callable\"\n\n        self.assertIsInvalid(\n            BandAdmin,\n            Band,\n            \"The value of 'actions' must be a list or tuple of callables.\",\n            id=\"admin.E131\",\n        )\n", "    def test_actions_valid(self):\n        @admin.action(permissions=[\"can_edit\"])\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (edit_action,)\n            actions_on_top = True\n\n        self.assertIsValid(BandAdmin, Band)\n\n\n", "    def test_actions_with_different_names_are_valid(self):\n        @admin.action\n            pass\n\n        @admin.action\n            pass\n\n        class BandAdmin(ModelAdmin):\n            actions = (action1, action2)\n\n        self.assertIsValid(BandAdmin, Band)\n"], "sample_124": ["    def test_attribute_subclass(self):\n        class CustomForm(Form):\n            default_renderer = DjangoTemplates\n\n        class SubCustomForm(CustomForm):\n            pass\n\n        form = SubCustomForm()\n        self.assertTrue(isinstance(form.renderer, SubCustomForm.default_renderer))\n", "    def test_attribute_subclassing(self):\n        class CustomForm(Form):\n            pass\n\n        class SubCustomForm(CustomForm):\n            default_renderer = CustomRenderer()\n\n        form = SubCustomForm()\n        self.assertEqual(form.renderer, SubCustomForm.default_renderer)\n", "    def test_attribute_inheritance(self):\n        class BaseForm(Form):\n            default_renderer = CustomRenderer\n\n        class ChildForm(BaseForm):\n            pass\n\n        form = ChildForm()\n        self.assertTrue(isinstance(form.renderer, ChildForm.default_renderer))\n", "    def test_attribute_class_inheritance(self):\n        class ParentForm(Form):\n            default_renderer = CustomRenderer\n\n        class ChildForm(ParentForm):\n            pass\n\n        form = ChildForm()\n        self.assertTrue(isinstance(form.renderer, ParentForm.default_renderer)) \n", "    def test_attribute_subclassing(self):\n        class CustomForm(Form):\n            default_renderer = DjangoTemplates()\n\n        class SubCustomForm(CustomForm):\n            pass\n\n        form = SubCustomForm()\n        self.assertEqual(form.renderer, CustomForm.default_renderer)\n"], "sample_97": ["    def test_compare_snapshots_ignores_missing_files(self, mock_stat):\n        snapshot1 = {\n            self.existing_file: 'old_stats',\n        }\n        snapshot2 = {}\n        self.assertTrue(self.reloader.compare_snapshots(snapshot1, snapshot2))\n        mock_stat.assert_not_called()\n", "    def test_watch_dir_with_absolute_path(self):\n        self.reloader.watch_dir(str(self.tempdir), '*.py')\n        watched_files = list(self.reloader.watched_files())\n        self.assertIn(self.existing_file, watched_files)\n", "    def test_should_stop(self):\n        self.reloader.stop = True\n        self.assertTrue(self.reloader.should_stop())\n", "    def test_notify_file_changed_ignores_duplicates(self):\n        with mock.patch.object(self.reloader, 'should_stop', return_value=False):\n            self.reloader.notify_file_changed(self.existing_file)\n            self.reloader.notify_file_changed(self.existing_file)\n            self.assertEqual(self.reloader.changed_files, {self.existing_file: 1})\n", "    def test_should_stop(self, mocked_time):\n        mocked_time.time.side_effect = [1, 1.1, 1.5]\n        self.reloader.last_check = 0\n\n        self.assertFalse(self.reloader.should_stop())\n        self.assertFalse(self.reloader.should_stop())\n        self.assertTrue(self.reloader.should_stop())\n\n"], "sample_453": ["    def test_prepopulated_fields_js(self):\n        article = Article.objects.create(title=\"Test Article\", slug=\"test-article\")\n        request = self.request_factory.get(\n            reverse(\"admin:admin_views_article_change\", args=[article.pk])\n        )\n        request.user = self.superuser\n\n        admin = ArticleAdmin(Article, site)\n        # Set up a prepopulated field\n        admin.prepopulated_fields = {\"slug\": {\"field\": \"title\"}}\n\n        response = admin.change_view(request, str(article.pk))\n        response.render()\n        self.assertIn('\"prepopulated_fields\":', response.content.decode())\n        prepopulated_fields_json = json.loads(\n            response.context_data[\"prepopulated_fields_json\"]\n        )\n\n        self.assertEqual(\n            prepopulated_fields_json[0][\"id\"], \"#id_slug\"\n        )\n        self.assertEqual(prepopulated_fields_json[0][\"name\"], \"slug\")\n        self.assertEqual(prepopulated_fields_json[0][\"dependency_ids\"], [\"#id_title\"])\n        self.assertEqual(prepopulated_fields_json[0][\"dependency_list\"], [\"title\"])\n        self.assertEqual(prepopulated_fields_json[0][\"maxLength\"], 50)\n", "    def test_cell_count(self):\n        request = self.request_factory.get(\n            reverse(\"admin:admin_views_article_changelist\")\n        )\n        request.user = self.superuser\n        admin = ArticleAdmin(Article, site)\n        response = admin.changelist_view(request)\n        response.render()\n        inline_admin_formset = response.context_data[\"adminform\"].inline_admin_formsets[0]\n        cell_count = admin.cell_count(inline_admin_formset.forms[0])\n        self.assertEqual(cell_count, 5) \n", "    def test_date_hierarchy_format(self):\n        modeladmin = ModelAdmin(Question, site)\n        modeladmin.date_hierarchy = \"posted\"\n\n        posted_dates = (\n            datetime.date(2017, 10, 1),\n            datetime.date(2017, 10, 15),\n            datetime.date(2017, 12, 15),\n            datetime.date(2017, 12, 31),\n            datetime.date(2018, 2, 1),\n        )\n        Question.objects.bulk_create(\n            Question(question=\"q\", posted=posted) for posted in posted_dates\n        )\n\n        request = self.factory.get(\"/\", {\"year\": 2017, \"month\": 10})\n        request.user = self.superuser\n        changelist = modeladmin.get_changelist_instance(request)\n        spec = date_hierarchy(changelist)\n        self.assertEqual(spec['choices'][0]['label'], 'October')\n        self.assertEqual(spec['choices'][1]['label'], 'December')\n", "    def test_prepopulated_fields_js(self):\n        article = Article.objects.all()[0]\n        request = self.request_factory.get(\n            reverse(\"admin:admin_views_article_change\", args=[article.pk])\n        )\n        request.user = self.superuser\n        admin = ArticleAdmin(Article, site)\n        admin.prepopulated_fields = {\"slug\": {\"field\": \"title\"}}\n        response = admin.change_view(request, str(article.pk))\n        response.render()\n        self.assertContains(response, 'data-prepopulated-fields')\n        self.assertContains(response, '\"slug\": {\"id\": \"#id_slug')\n", "    def test_cell_count(self):\n        request = self.request_factory.get(\n            reverse(\"admin:admin_views_article_changelist\")\n        )\n        request.user = self.superuser\n        admin = ArticleAdmin(Article, site)\n        response = admin.changelist_view(request)\n        response.render()\n        # Check the cell count for an inline.\n        inline_admin_form = response.context_data[\"cl\"][\"formset\"].forms[0]\n        self.assertEqual(cell_count(inline_admin_form), 5)\n"], "sample_101": ["    def test_force_script_name(self):\n        \"\"\"\n        FORCE_SCRIPT_NAME is respected.\n        \"\"\"\n        application = get_wsgi_application()\n\n        environ = self.request_factory._base_environ(\n            PATH_INFO=\"/test/\",\n            CONTENT_TYPE=\"text/html; charset=utf-8\",\n            REQUEST_METHOD=\"GET\"\n        )\n\n        response_data = {}\n\n            response_data[\"status\"] = status\n            response_data[\"headers\"] = headers\n\n        response = application(environ, start_response)\n\n        self.assertEqual(response_data[\"status\"], \"200 OK\")\n        self.assertIn(b\"Content-Length: 16\", response)\n        self.assertIn(b\"Content-Type: text/html; charset=utf-8\", response)\n        self.assertIn(b\"/app/test/ Hello World!\", response)\n", "    def test_script_name_rewritten(self):\n        \"\"\"\n        Test that get_script_name returns the correct SCRIPT_NAME when mod_rewrite \n        has been used.\n        \"\"\"\n        environ = self.request_factory._base_environ(\n            SCRIPT_NAME='/app',\n            PATH_INFO='/path/to/resource',\n            SCRIPT_URL='/app/path/to/resource',\n            REQUEST_METHOD='GET'\n        )\n        application = get_internal_wsgi_application()\n        response_data = {}\n            response_data['status'] = status\n            response_data['headers'] = headers\n        response = application(environ, start_response)\n        request_path = environ.get('PATH_INFO')\n        self.assertEqual(request_path, '/path/to/resource')\n        self.assertEqual(environ.get('SCRIPT_NAME'), '/app')\n\n", "    def test_limited_stream(self):\n        application = get_wsgi_application()\n        environ = self.request_factory._base_environ(\n            PATH_INFO=\"/\",\n            CONTENT_TYPE=\"text/html; charset=utf-8\",\n            REQUEST_METHOD=\"POST\",\n            CONTENT_LENGTH=\"1024\"\n        )\n        environ['wsgi.input'] = BytesIO(b'x' * 1025)\n\n        response_data = {}\n\n            response_data[\"status\"] = status\n            response_data[\"headers\"] = headers\n\n        response = application(environ, start_response)\n\n        self.assertEqual(response_data[\"status\"], \"200 OK\")\n        self.assertEqual(len(response.read()), 1024)\n\n", "    def test_limited_stream(self):\n        content = b'This is a test stream. It will be truncated.'\n        limited_stream = LimitedStream(BytesIO(content), 20)\n        self.assertEqual(limited_stream.read(10), b'This is a t')\n        self.assertEqual(limited_stream.read(10), b'est stream.')\n        self.assertEqual(limited_stream.read(10), b'')\n        with self.assertRaises(ValueError):\n            limited_stream.read(1) \n", "    def test_get_script_name(self):\n        \"\"\"\n        get_script_name() returns the correct SCRIPT_NAME.\n        \"\"\"\n        environ = self.request_factory._base_environ(\n            SCRIPT_NAME=\"/app\",\n            PATH_INFO=\"/path/to/resource\"\n        )\n        self.assertEqual(get_script_name(environ), '/app')\n\n        environ = self.request_factory._base_environ(\n            SCRIPT_NAME=\"/app/\",\n            PATH_INFO=\"/path/to/resource\"\n        )\n        self.assertEqual(get_script_name(environ), '/app')\n\n        environ = self.request_factory._base_environ(\n\n            SCRIPT_URL=\"/app/path/to/resource\",\n            PATH_INFO=\"/path/to/resource\"\n        )\n        self.assertEqual(get_script_name(environ), '/app')\n\n        environ = self.request_factory._base_environ(\n            SCRIPT_URL=\"/app//path/to/resource\",\n            PATH_INFO=\"/path/to/resource\"\n        )\n        self.assertEqual(get_script_name(environ), '/app')\n\n        environ = self.request_factory._base_environ(\n            SCRIPT_NAME=\"/app\",\n            PATH_INFO=\"\",\n            REDIRECT_URL=\"/app/\"\n        )\n        self.assertEqual(get_script_name(environ), '/app')\n\n        environ = self.request_factory._base_environ(\n            SCRIPT_NAME=\"/app\",\n            PATH_INFO=\"/path/to/resource\",\n            REDIRECT_URL=\"/app/path/to/resource\"\n        )\n        self.assertEqual(get_script_name(environ), '/app')\n\n        settings.FORCE_SCRIPT_NAME = '/forced_script_name'\n        environ = self.request_factory._base_environ(\n            SCRIPT_NAME=\"/app\",\n            PATH_INFO=\"/path/to/resource\",\n        )\n        self.assertEqual(get_script_name(environ), '/forced_script_name')\n        del settings.FORCE_SCRIPT_NAME\n"], "sample_606": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_811": ["def test_pairwise_distances_chunked_empty():\n    # Test edge case where X or Y is empty.\n    X = np.array([])\n    Y = np.array([[1, 2], [3, 4]])\n    gen = pairwise_distances_chunked(X, Y, working_memory=1000)\n    assert isinstance(gen, GeneratorType)\n    result = list(gen)\n    assert result == []\n    X = np.array([[1, 2], [3, 4]])\n    Y = np.array([])\n    gen = pairwise_distances_chunked(X, Y, working_memory=1000)\n    assert isinstance(gen, GeneratorType)\n    result = list(gen)\n    assert result == []\n", "def test_pairwise_distances_chunked_empty():\n    # Test the pairwise_distances_chunked function with empty input arrays.\n    X = np.array([])\n    Y = np.array([])\n    gen = pairwise_distances_chunked(X, Y, working_memory=1)\n    assert_raises(StopIteration, next, gen)\n\n    # Test with only one array empty\n    X = np.array([[1, 2], [3, 4]])\n    Y = np.array([])\n    gen = pairwise_distances_chunked(X, Y, working_memory=1)\n    assert_raises(StopIteration, next, gen)\n\n    X = np.array([])\n    Y = np.array([[1, 2], [3, 4]])\n    gen = pairwise_distances_chunked(X, Y, working_memory=1)\n    assert_raises(StopIteration, next, gen)\n", "compilation error", "compilation error", "compilation error"], "sample_52": ["    def test_empty_queryset(self):\n        f = forms.ModelChoiceField(Category.objects.none())\n        self.assertEqual(len(f.choices), 1)\n        self.assertEqual(f.choices[0], ('', '---------'))\n", "    def test_to_field_name_error(self):\n        with self.assertRaises(FieldDoesNotExist):\n            forms.ModelChoiceField(Category.objects.all(), to_field_name='nonexistent_field')\n", "    def test_modelchoicefield_empty_queryset(self):\n        f = forms.ModelChoiceField(Category.objects.none())\n        self.assertEqual(len(f.choices), 1)\n        self.assertEqual(list(f.choices), [('', '---------')])\n        self.assertIsNone(f.clean(None))\n", "    def test_limit_choices_to(self):\n        class MyCategory(Category):\n            class Meta:\n                proxy = True\n\n        f = forms.ModelChoiceField(\n            Category.objects.all(),\n            limit_choices_to={'name__startswith': 'A'}\n        )\n        self.assertEqual(list(f.choices), [\n            ('', '---------'),\n            (self.c2.pk, 'A test'),\n        ])\n        f.limit_choices_to = {'name__startswith': 'E'}\n        self.assertEqual(list(f.choices), [\n            ('', '---------'),\n            (self.c1.pk, 'Entertainment'),\n        ])\n\n        # Test callable limit_choices_to\n            return queryset.filter(name__startswith='T')\n\n        f = forms.ModelChoiceField(\n            Category.objects.all(),\n            limit_choices_to=get_categories_starting_with_t,\n        )\n        self.assertEqual(list(f.choices), [\n\n\n        ])\n\n\n\n", "    def test_empty_queryset(self):\n        f = forms.ModelChoiceField(Category.objects.none())\n        self.assertEqual(f.choices, [('', '---------')])\n        with self.assertRaises(ValidationError):\n            f.clean('')\n        with self.assertRaises(ValidationError):\n            f.clean(None)\n"], "sample_339": ["    def test_inlineformset_factory_max_num_with_queryset(self):\n        author = Author.objects.create(name='Charles Baudelaire')\n        BookFormSet = inlineformset_factory(\n            Author,\n            Book,\n            fields='__all__',\n            max_num=2,\n            queryset=Book.objects.filter(author=author),\n        )\n        data = {\n            'book_set-TOTAL_FORMS': '3',\n            'book_set-INITIAL_FORMS': '0',\n            'book_set-MAX_NUM_FORMS': '0',\n            'book_set-0-title': 'Foo',\n            'book_set-0-author': str(author.id),\n            'book_set-1-title': 'Bar',\n            'book_set-1-author': str(author.id),\n            'book_set-2-title': 'Baz',\n            'book_set-2-author': str(author.id),\n        }\n        formset = BookFormSet(data, instance=author)\n        self.assertIs(formset.is_valid(), False)\n        self.assertEqual(len(formset.forms), 2)\n        self.assertEqual(formset.errors, [{}, {}, {'__all__': ['Please submit at most 2 forms.']}])\n\n", "    def test_modelformset_factory_empty_queryset(self):\n        AuthorFormSet = modelformset_factory(Author, fields='__all__')\n        formset = AuthorFormSet(queryset=Author.objects.none())\n        self.assertEqual(len(formset.forms), 0)\n", "    def test_inlineformset_factory_empty_queryset(self):\n        author = Author.objects.create(name='Charles Baudelaire')\n        BookFormSet = inlineformset_factory(\n            Author,\n            Book,\n            fields='__all__',\n            queryset=Book.objects.none(),\n        )\n        formset = BookFormSet(instance=author)\n        self.assertEqual(len(formset), 0)\n", "    def test_modelformset_factory_default_renderer(self):\n        BookFormSet = modelformset_factory(Book, fields='__all__')\n        formset = BookFormSet()\n        self.assertIsNotNone(formset.renderer)\n", "    def test_modelformset_factory_filter(self):\n        class PublishedAuthor(models.Model):\n            name = models.CharField(max_length=100)\n            published = models.BooleanField(default=False)\n\n        AuthorFormSet = modelformset_factory(PublishedAuthor, fields='__all__')\n        published = PublishedAuthor.objects.create(name='Charles Baudelaire', published=True)\n        unpublished = PublishedAuthor.objects.create(name='Walt Whitman', published=False)\n        data = {\n            'form-TOTAL_FORMS': '2',\n            'form-INITIAL_FORMS': '0',\n            'form-MAX_NUM_FORMS': '0',\n            'form-0-name': 'Arthur Rimbaud',\n            'form-0-published': True,\n            'form-1-name': 'Emily Dickinson',\n            'form-1-published': False,\n        }\n\n        formset = AuthorFormSet(data, queryset=PublishedAuthor.objects.filter(published=True))\n        self.assertEqual(len(formset.forms), 1)\n        self.assertEqual(formset.forms[0].instance, published)\n\n"], "sample_933": ["def test_gettext_uuid(app):\n    app.config.gettext_uuid = True\n    app.builder.build_all()\n\n    # Do messages end up in the correct location?\n    # top-level documents end up in a message catalog\n    assert (app.outdir / 'extapi.pot').isfile()\n    # directory items are grouped into sections\n    assert (app.outdir / 'subdir.pot').isfile()\n\n    catalog = (app.outdir / 'markup.pot').read_text()\n    # Check if UUIDs are present in msgid\n\n    for line in catalog.splitlines():\n      if 'msgid' in line:\n        assert \"uuid\" in line\n\n", "def test_gettext_uuid(app):\n    app.builder.build_all()\n\n    # Check if UUIDs are incorporated into the messages\n    pot = (app.outdir / 'index_entries.pot').read_text()\n    assert 'msgid \"\"' in pot  # Empty msgid for the initial UUID\n    # Make sure the UUID appears after the message\n    assert 'msgid \"index target section\"' in pot  \n    assert 'uuid \"uuid_hash\"' in pot \n\n", "def test_gettext_uuid_unique(app):\n    app.builder.build_all()\n\n    # Check if UUIDs are unique across messages\n    pot = (app.outdir / 'index_entries.pot').read_text()\n    msgids = re.findall(r'msgid \"(.*)\"', pot)\n    uuids = re.findall(r'uuid \"(.*)\"', pot)\n    assert len(set(uuids)) == len(uuids), 'Duplicate UUIDs found in pot file'\n", "def test_gettext_uuid(app):\n    app.builder.build_all()\n\n    catalog = (app.outdir / 'extapi.pot').read_text()\n    assert 'msgid \"something\"' in catalog\n\n    # Check if UUIDs are included\n    for line in catalog.splitlines():\n        if line.startswith('msgid'):\n            assert '(' in line\n\n", "def test_gettext_uuid(app):\n    app.builder.build_all()\n\n    # Do messages end up in the correct location?\n    # top-level documents end up in a message catalog\n    assert (app.outdir / 'extapi.pot').isfile()\n    # directory items are grouped into sections\n    assert (app.outdir / 'subdir.pot').isfile()\n\n    catalog = (app.outdir / 'extapi.pot').read_text()\n    assert 'msgid \"something\"' in catalog\n"], "sample_1074": ["def test_is_solvable():\n    # every abelian group is solvable\n    for i in (1, 2, 3):\n        C = CyclicGroup(i)\n        Ab = AbelianGroup(i, i + 2)\n        assert C.is_solvable\n        assert Ab.is_solvable\n    Ab = AbelianGroup(5, 7, 10)\n    assert Ab.is_solvable\n    # A_5 is not solvable\n    assert AlternatingGroup(5).is_solvable is False\n    # S_4 is solvable\n    assert SymmetricGroup(4).is_solvable is True\n\n", "compilation error", "compilation error", "    def test_center():\n        S = SymmetricGroup(4)\n        assert S.center().is_trivial\n        G = DihedralGroup(8)\n        assert G.center().order() == 2\n        A = AlternatingGroup(5)\n        assert A.center().is_trivial\n\n        G = PermutationGroup([Permutation(0,1,2), Permutation(0,2,3)])\n        assert G.center().order() == 3\n\n\n", "compilation error"], "sample_589": ["def test_interpolate_na_max_gap_time_specifier_limits(\n    da_time, transform", "def test_interpolate_na_max_gap_time_index_errors(da_time, time_range_func):\n    da_time[\"t\"] = time_range_func(\"2001-01-01\", freq=\"H\", periods=11)\n    with raises_regex(ValueError, r\"max_gap must be less than\"):\n        da_time.interpolate_na(\"t\", max_gap=pd.Timedelta(days=2))\n\n\n", "    def test_interpolate_na_max_gap_time_specifier_boundary(\n        da_time,", "    def test_interpolate_na_max_gap_raises_non_numeric(da_time):\n        da_time[\"t\"] = pd.date_range(\"2001-01-01\", freq=\"H\", periods=11)\n        da_time.data[3] = \"test\"\n        with raises_regex(TypeError, \"Incompatible types in\"):\n            da_time.interpolate_na(\"t\", max_gap=\"3H\")\n", "    def test_interpolate_na_max_gap_method(self, da_time, method, time_range_func):\n        da_time[\"t\"] = time_range_func(\"2001-01-01\", freq=\"H\", periods=11)\n        expected = da_time.copy(\n            data=[np.nan, 1, 2, np.nan, np.nan, 5, np.nan, np.nan, np.nan, np.nan, 10]\n        )\n        actual = da_time.interpolate_na(\"t\", method=method, max_gap=\"3H\")\n        assert_equal(actual, expected)\n"], "sample_906": ["compilation error", "compilation error", "compilation error", "def test_domain_cpp_build_paren_warn(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"paren-warn\")\n    assert len(ws) == 1\n    assert \"WARNING: cpp:function reference target not found: f()\" in ws[0]\n", "    def check(target):\n        class Config:\n            cpp_id_attributes = [\"id_attr\"]\n            cpp_paren_attributes = [\"paren_attr\"]\n        parser = DefinitionParser(target, location=None,\n                                  config=Config())\n        ast, isShorthand = parser.parse_xref_object()\n        parser.assert_end()"], "sample_536": ["def test_polygon_selector_empty_polygon(draw_bounding_box):\n    verts = []\n    event_sequence = []\n    check_polygon_selector(event_sequence, verts, 0,\n                           draw_bounding_box=draw_bounding_box)\n\n", "def test_polygon_selector_disconnect(draw_bounding_box):\n    ax = get_ax()\n\n    onselect = mock.Mock(spec=noop, return_value=None)\n\n    tool = widgets.PolygonSelector(ax, onselect, draw_bounding_box=draw_bounding_box)\n\n    tool.disconnect()\n\n    # Verify that the disconnect method does not raise any errors.\n\n    verts = [(50, 50), (150, 50), (50, 150)]\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n    for (etype, event_args) in event_sequence:\n        do_event(tool, etype, **event_args)\n    # The onselect function should not be called after disconnect.\n    assert onselect.call_count == 0\n", "def test_polygon_selector_update_verts_after_draw(fig_test, fig_ref, draw_bounding_box):\n    verts = [(0.1, 0.4), (0.5, 0.9), (0.3, 0.2)]\n    ax_test = fig_test.add_subplot()\n    ax_ref = fig_ref.add_subplot()\n    tool_test = widgets.PolygonSelector(\n        ax_test, onselect=noop, draw_bounding_box=draw_bounding_box)\n    tool_ref = widgets.PolygonSelector(\n        ax_ref, onselect=noop, draw_bounding_box=draw_bounding_box)\n\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n    for (etype, event_args) in event_sequence:\n        do_event(tool_test, etype, **event_args)\n        do_event(tool_ref, etype, **event_args)\n    tool_test.verts = [(0.2, 0.5), (0.6, 0.8), (0.4, 0.3)]\n    assert tool_test.verts == [(0.2, 0.5), (0.6, 0.8), (0.4, 0.3)]\n\n", "def test_polygon_selector_set_props_verts_setter(fig_test, fig_ref, draw_bounding_box):\n    verts = [(0.1, 0.4), (0.5, 0.9), (0.3, 0.2)]\n    ax_test = fig_test.add_subplot()\n\n    tool_test = widgets.PolygonSelector(\n        ax_test, onselect=noop, draw_bounding_box=draw_bounding_box,\n        props={'facecolor': 'r', 'fill': True})\n    tool_test.verts = verts\n    assert tool_test.verts == verts\n    \n    ax_ref = fig_ref.add_subplot()\n    tool_ref = widgets.PolygonSelector(\n        ax_ref, onselect=noop, draw_bounding_box=draw_bounding_box,\n        props={'facecolor': 'r', 'fill': True})\n    event_sequence = [\n        *polygon_place_vertex(*verts[0]),\n        *polygon_place_vertex(*verts[1]),\n        *polygon_place_vertex(*verts[2]),\n        *polygon_place_vertex(*verts[0]),\n    ]\n    for (etype, event_args) in event_sequence:\n        do_event(tool_ref, etype, **event_args)\n", "def test_polygon_selector_with_existing_verts(fig_test, fig_ref, draw_bounding_box):\n    verts = [(0.1, 0.4), (0.5, 0.9), (0.3, 0.2)]\n    ax_test = fig_test.add_subplot()\n\n    tool_test = widgets.PolygonSelector(\n        ax_test, onselect=noop, draw_bounding_box=draw_bounding_box)\n    tool_test.verts = verts\n    new_vert = (0.7, 0.6)\n    event_sequence = [\n        *polygon_place_vertex(*new_vert),\n        *polygon_place_vertex(*verts[0]),  \n    ]\n    for (etype, event_args) in event_sequence:\n        do_event(tool_test, etype, **event_args)\n\n    ax_ref = fig_ref.add_subplot()\n    tool_ref = widgets.PolygonSelector(\n        ax_ref, onselect=noop, draw_bounding_box=draw_bounding_box)\n    tool_ref.verts = verts + [new_vert]\n    assert tool_test.verts == tool_ref.verts\n"], "sample_1018": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1096": ["def test_IndexedBase_free_symbols():\n    i, j = symbols('i j', integer=True)\n    a = Symbol('a')\n    A = IndexedBase('A')\n    assert A.free_symbols == {A}\n    assert A[i].free_symbols == {A, i}\n    assert A[i, j].free_symbols == {A, i, j}\n    assert (A[i] + a).free_symbols == {A, i, a}\n\n\n", "compilation error", "compilation error", "def test_IndexedBase_getitem_slice():\n    i = symbols('i', integer=True)\n    A = IndexedBase('A')\n    assert A[i:i+2] == A[Slice(i, i + 2)]\n    assert A[:i] == A[Slice(None, i)]\n    assert A[i:] == A[Slice(i, None)]\n    assert A[:] == A[Slice(None, None)]\n    raises(IndexException, lambda: A[1:i])\n", "compilation error"], "sample_363": ["    def test_empty_value(self):\n        from selenium.webdriver.common.by import By\n        self.admin_login(username='super', password='secret', login_url='/')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_event_add'))\n\n        # Test that the rawid widget allows an empty value.\n        self.selenium.find_element(By.ID, 'id_main_band').clear()\n        self.selenium.find_element(By.XPATH, '//input[@value=\"Save\"]').click()\n\n        self.assertNoErrors()\n", "    def test_raw_id_filter(self):\n        from selenium.webdriver.common.by import By\n        self.school.students.set([self.lisa, self.peter])\n        self.school.alumni.set([self.lisa, self.peter])\n        self.admin_login(username='super', password='secret', login_url='/')\n        change_url = reverse('admin:admin_widgets_school_change', args=(self.school.id,))\n        self.selenium.get(self.live_server_url + change_url)\n\n        self.selenium.find_element(By.ID, 'id_students').send_keys('lisa')\n        self.wait_for_element_text('#id_students option', 'Lisa')\n        self.selenium.find_element(By.ID, 'id_students').send_keys(Keys.ENTER)\n\n        self.selenium.find_element(By.ID, 'id_alumni').send_keys('pet')\n        self.wait_for_element_text('#id_alumni option', 'Peter')\n\n        self.selenium.find_element(By.ID, 'id_alumni').send_keys(Keys.ENTER)\n\n        self.assertCountSeleniumElements('#id_students option', 1)\n        self.assertCountSeleniumElements('#id_alumni option', 1)\n\n", "    def test_search_field(self):\n        from selenium.webdriver.common.by import By\n        from selenium.webdriver.common.keys import Keys\n\n        self.admin_login(username='super', password='secret', login_url='/')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_event_add'))\n\n        # Open the popup window for the ForeignKey field\n        self.selenium.find_element(By.ID, 'lookup_id_venue').click()\n        self.wait_for_and_switch_to_popup()\n\n        # Search for a venue by name\n        search_input = self.selenium.find_element(By.ID, 'searchbar')\n        search_input.send_keys('Blue')\n        search_input.send_keys(Keys.ENTER)\n\n        # Verify that only venues with 'Blue' in their name are displayed\n        results = self.selenium.find_elements(By.CSS_SELECTOR, '.results li')\n        for result in results:\n            self.assertIn('Blue', result.text)\n\n", "    def test_raw_id_widget_for_non_existing_pk(self):\n        from selenium.webdriver.common.by import By\n        self.admin_login(username='super', password='secret', login_url='/')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_event_add'))\n\n        # Try to input a non-existing primary key\n        self.selenium.find_element(By.ID, 'id_main_band').send_keys('9999')\n        self.selenium.find_element(By.ID, 'id_main_band').submit()\n\n        # An error message should be displayed\n", "    def test_raw_id_widget_with_related_lookup(self):\n        from selenium.webdriver.common.by import By\n        self.admin_login(username='super', password='secret', login_url='/')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_widgets_event_add'))\n        \n        self.selenium.find_element(By.ID, 'lookup_id_venue').click()\n        self.wait_for_and_switch_to_popup()\n\n        # Select a venue from the list\n        venue_link = self.selenium.find_element(By.LINK_TEXT, 'Venue A')\n        venue_link.click()\n\n        self.selenium.switch_to.window(self.selenium.window_handles[0])\n        self.wait_for_value('#id_venue', '1')\n        \n        # Save and check that the venue is saved correctly\n        self.selenium.find_element(By.XPATH, '//input[@value=\"Save\"]').click()\n\n        self.wait_page_ready()\n\n        event = Event.objects.get(id=1)\n        self.assertEqual(event.venue_id, 1) \n"], "sample_413": ["    def test_template_tags_with_same_name_in_installed_apps_and_custom_libraries(self):\n        with self.settings(\n            INSTALLED_APPS=[\n                \"check_framework.template_test_apps.same_tags_app_1\",\n                \"check_framework.template_test_apps.same_tags_app_2\",\n            ],\n            TEMPLATES=[\n                {\n                    \"BACKEND\": \"django.template.backends.django.DjangoTemplates\",\n                    \"OPTIONS\": {\n                        \"libraries\": {\n                            \"same_tags\": \"check_framework.template_test_apps.same_tags_app_1.templatetags.same_tags\",\n                        },\n                    },\n                },\n            ],\n        ):\n            self.assertEqual(\n                check_for_template_tags_with_the_same_name(None),\n                [self.error_same_tags],\n            )\n", "    def test_template_tags_with_same_name_in_built_in_modules(self):\n        with self.settings(\n            TEMPLATES=[\n                self.get_settings(\n                    \"same_tags\", \"django.template.defaulttags.same_tags\"\n                ),\n                self.get_settings(\n                    \"same_tags\", \"check_framework.template_test_apps.same_tags_app_2.templatetags.same_tags\"\n                ),\n            ]\n        ):\n            self.assertEqual(\n                check_for_template_tags_with_the_same_name(None), []\n            )\n", "    def test_empty_templates(self):\n        with self.settings(TEMPLATES=[]):\n            self.assertEqual(check_for_template_tags_with_the_same_name(None), [])\n\n\n", "    def test_template_tags_with_same_name_in_default_and_custom(self):\n        with self.settings(\n            TEMPLATES=[\n                {\n                    \"BACKEND\": \"django.template.backends.django.DjangoTemplates\",\n                    \"OPTIONS\": {\n                        \"libraries\": {\n                            \"same_tags\": \"django.template.defaulttags\",\n                        },\n                    },\n                },\n                self.get_settings(\n                    \"same_tags\", \"same_tags_app_2.templatetags.same_tags\"\n                ),\n            ]\n        ):\n            self.assertEqual(\n                check_for_template_tags_with_the_same_name(None),\n                [self.error_same_tags],\n            )\n", "    def test_template_tags_with_different_module_names(self):\n        with self.settings(\n            TEMPLATES=[\n                self.get_settings(\n                    \"same_tags\", \"same_tags_app_1.templatetags.same_tags\"\n                ),\n                self.get_settings(\n                    \"different_tags\", \"same_tags_app_2.templatetags.same_tags\"\n                ),\n            ]\n        ):\n            self.assertEqual(\n                check_for_template_tags_with_the_same_name(None), []\n            )\n"], "sample_64": ["    def test_cookie_with_leading_spaces(self):\n        c = SimpleCookie()\n        c.load(' name=value')\n        self.assertEqual(c['name'].value, 'value')\n", "    def test_invalid_cookie_names(self):\n        \"\"\"Test handling of invalid cookie names.\"\"\"\n        c = SimpleCookie()\n        c['name with spaces'] = 'value'\n        self.assertIn('name with spaces', c)\n        self.assertEqual(c['name with spaces'].value, 'value')\n        c.load('invalid=value')\n        self.assertIn('invalid', c)\n        c.load('invalid;')\n        self.assertIn('invalid', c)\n", "    def test_cookie_expiration(self):\n        c = SimpleCookie()\n        c['name'] = 'val'\n        c['name']['expires'] = 1234567890\n        self.assertEqual(c['name']['expires'], 1234567890)\n        self.assertIn('Expires', c.output())\n", "    def test_cookie_max_age(self):\n        c = SimpleCookie()\n        c['name'] = 'value'\n        c['name']['max-age'] = 3600\n        self.assertEqual(c['name']['max-age'], 3600)\n", "    def test_secure_after_load(self):\n        c = SimpleCookie()\n        c.load(\"name=val\")\n        c['name']['secure'] = True\n        self.assertTrue(c['name']['secure'])\n"], "sample_203": ["    def test_decimal_validator_with_different_exponent(self):\n        class MyForm(forms.Form):\n            field = forms.DecimalField(\n                max_digits=5,\n                decimal_places=2,\n                validators=[validators.DecimalValidator(max_digits=5, decimal_places=2)],\n            )\n\n        tests = [\n            ('12345.67', True),\n            ('123456.7', False),\n            ('123.4567', False),\n            ('12345.678', False),\n            ('0.000123', True),\n\n        ]\n        for value, expected in tests:\n            with self.subTest(value=value):\n                form = MyForm({'field': value})\n                if expected:\n                    self.assertTrue(form.is_valid())\n                else:\n                    self.assertFalse(form.is_valid())\n", "    def test_file_extension_validator_with_allowed_extensions(self):\n        class MyForm(forms.Form):\n            field = forms.FileField(validators=[validators.FileExtensionValidator(allowed_extensions=['txt', 'pdf'])])\n\n        with self.subTest('valid extension'):\n            form = MyForm({'field': SimpleUploadedFile('test.txt', b'content')})\n            self.assertTrue(form.is_valid())\n\n        with self.subTest('invalid extension'):\n            form = MyForm({'field': SimpleUploadedFile('test.jpg', b'content')})\n            self.assertFalse(form.is_valid())\n            self.assertEqual(form.errors['field'][0], 'File extension \u201cjpg\u201d is not allowed. Allowed extensions are: txt, pdf.')\n\n", "    def test_file_extension_validator(self):\n        class MyForm(forms.Form):\n            file_field = forms.FileField(\n                validators=[validators.FileExtensionValidator(allowed_extensions=['txt', 'pdf'])]\n            )\n\n        with self.subTest('allowed_extension'):\n            form = MyForm({'file_field': SimpleUploadedFile('myfile.txt', b'content')})\n            self.assertTrue(form.is_valid())\n\n        with self.subTest('disallowed_extension'):\n            form = MyForm({'file_field': SimpleUploadedFile('myfile.jpg', b'content')})\n            self.assertFalse(form.is_valid())\n            self.assertEqual(form.errors['file_field'], ['File extension \u201cjpg\u201d is not allowed. Allowed extensions are: txt, pdf.'])\n\n\n", "    def test_fileextensionvalidator(self):\n        class MyForm(forms.Form):\n            field = forms.FileField(\n                validators=[validators.FileExtensionValidator(\n                    allowed_extensions=['txt'],\n                )],\n            )\n        form = MyForm({'field': SimpleUploadedFile('test.pdf', b'content')})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['field'], ['File extension \u201cpdf\u201d is not allowed. Allowed extensions are: txt.'])\n\n", "    def test_file_extension_validator(self):\n        class MyForm(forms.Form):\n            file_field = forms.FileField(validators=[validators.FileExtensionValidator(['txt', 'pdf'])])\n\n        with self.subTest('allowed extension'):\n            form = MyForm({'file_field': SimpleUploadedFile('test.txt', b'content')})\n            self.assertTrue(form.is_valid())\n        with self.subTest('disallowed extension'):\n            form = MyForm({'file_field': SimpleUploadedFile('test.jpg', b'content')})\n            self.assertFalse(form.is_valid())\n            self.assertEqual(form.errors['file_field'], ['File extension \u201cjpg\u201d is not allowed. Allowed extensions are: txt, pdf.'])\n"], "sample_1145": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1013": ["compilation error", "def test_lambdify_inspect_nested():\n    f = lambdify((x, y), x**2 + sin(y))\n    # Test that inspect.getsource works but don't hard-code implementation\n    # details\n    assert 'x**2 + sin(y)' in inspect.getsource(f)\n", "compilation error", "def test_lambdify_nested_function():\n    # Test lambdify with a nested function\n            return a * x\n\n        return lambdify(x, inner(x))\n    f = outer(2)\n    assert f(3) == 6\n", "compilation error"], "sample_874": ["def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n    assert_array_equal(feature_names_t, sel.get_feature_names_out(feature_names))\n    assert_array_equal(feature_names_t, sel.get_feature_names_out())\n    assert_array_equal(\n        np.array([\"x0\", \"x2\", \"x4\", \"x6\", \"x8\"]), sel.get_feature_names_out([])\n    )\n    with pytest.raises(ValueError):\n        sel.get_feature_names_out([\"a\", \"b\", \"c\", \"d\"])\n\n\n", "def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n    assert_array_equal(\n        sel.get_feature_names_out(input_features=feature_names),\n        np.array(feature_names)[sel.get_support(indices=True)],\n    )\n    \n    assert_array_equal(\n        sel.get_feature_names_out(),\n        np.array(feature_names)[sel.get_support(indices=True)],\n    )\n", "    def test_set_params():\n        sel = StepSelector(step=2)\n        # Check setting parameters with set_params\n        sel.set_params(step=3)\n        assert sel.step == 3\n", "def test_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n    assert_array_equal(sel.get_feature_names_out(input_features=feature_names),\n                       np.array(feature_names)[sel.get_support(indices=True)])\n\n", "    def test_get_feature_names_out(self):\n        sel = StepSelector()\n        sel.fit(X, y)\n        assert_array_equal(feature_names[::2], sel.get_feature_names_out())\n        assert_array_equal(\n            feature_names[::2], sel.get_feature_names_out(feature_names)\n        )\n\n\n"], "sample_1060": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_978": ["def test_repeated_degree_2():\n    d = 2\n    knots = [0, 0, 1, 1, 2, 3, 3, 4]\n    splines = bspline_basis_set(d, knots, x)\n    b0 = Piecewise((x**2/2, Interval(0, 1).contains(x)), (0, True))\n    b1 = Piecewise((Rational(1, 2) - x + x**2/2, Interval(0, 1).contains(x)),\n                   (0, True))\n    b2 = Piecewise((-1 + x, Interval(1, 2).contains(x)),\n                   (Rational(3, 2) - x + x**2/2, Interval(2, 3).contains(x)),\n                   (0, True))\n    b3 = Piecewise((Rational(-3, 2) + 3*x - x**2, Interval(1, 2).contains(x)),\n                   (Rational(9, 2) - 3*x + x**2/2, Interval(2, 3).contains(x)),\n                   (0, True))\n    b4 = Piecewise((Rational(-1, 2) + x - x**2/2, Interval(2, 3).contains(x)),\n                   (0, True))\n    assert splines[0] == b0\n    assert splines[1] == b1\n    assert splines[2] == b2\n    assert splines[3] == b3\n    assert splines[4] == b4\n", "    def test_degree_0_with_negative_knots():\n        d = 0\n        knots = [-2, -1, 0, 1, 2]\n        splines = bspline_basis_set(d, knots, x)\n        for i in range(len(splines)):\n            assert splines[i] == Piecewise((1, Interval(knots[i], knots[i + 1]).contains(x)),\n                                           (0, True))\n", "def test_basic_degree_negative():\n    d = -1\n    knots = range(5)\n    try:\n        bspline_basis_set(d, knots, x)\n    except ValueError as e:\n        assert str(e) == 'degree must be non-negative: -1'\n", "def test_bspline_basis_set_zero_knots():\n    d = 2\n    knots = [0]\n    splines = bspline_basis_set(d, knots, x)\n    assert len(splines)==0\n", "def test_bspline_basis_set_empty():\n    d = 1\n    knots = []\n    with pytest.raises(ValueError):\n        bspline_basis_set(d, knots, x)\n"], "sample_38": ["    def test_sip_with_alternate_key():\n        \"\"\"\n        Test SIP WCS initialization with an alternate key.\n        Relevant for #5443\n        \"\"\"\n        with fits.open(get_pkg_data_filename('data/sip.fits')) as f:\n            header = f[0].header\n\n        # Add a second WCS with a different key\n        header['CTYPE1B'] = \"RA---TAN-SIP\"\n        header['CTYPE2B'] = \"DEC--TAN-SIP\"\n        header['CUNIT1B'] = 'deg'\n        header['CUNIT2B'] = 'deg'\n        header['CRPIX1B'] = 1000\n        header['CRPIX2B'] = 500\n        header['CDELT1B'] = -0.00016666666666666666\n        header['CDELT2B'] = 0.00016666666666666666\n        header['CRVAL1B'] = 180.0\n        header['CRVAL2B'] = 0.0\n        header['PV2_1B'] = 1.0\n        header['PV2_3B'] = 220.\n\n        w = wcs.WCS(header, key='B')\n        assert (w.wcs.ctype == np.array(['RA---TAN-SIP', 'DEC--TAN-SIP'])).all()\n        assert (w.wcs.crpix == np.array([1000., 500.])).all()\n\n", "compilation error", "    def test_sip_header_without_key(self):\n        hdr = \"\"\"", "    def test_to_fits_2():\n        \"\"\"\n        Test to_fits() with a WCS object that doesn't have an associated\n        HDU.\n        \"\"\"\n        w = wcs.WCS(naxis=2)\n        w.wcs.crval = [1, 1]\n        w.wcs.cdelt = [0.1, 0.1]\n        w.wcs.crpix = [1, 1]\n        wfits = w.to_fits()\n        assert isinstance(wfits, fits.HDUList)\n        assert isinstance(wfits[0], fits.PrimaryHDU)\n", "    def test_no_wcs(self):\n        with pytest.raises(ValueError) as excinfo:\n            wcs.WCS(naxis=2)\n        assert excinfo.value.args[0] == \"Missing WCS information in header\"\n"], "sample_471": ["    def test_integerfield_invalid_input(self):\n        f = IntegerField()\n        with self.assertRaisesMessage(ValidationError, \"'Enter a whole number.'\"):\n            f.clean(\"abc\")\n        with self.assertRaisesMessage(ValidationError, \"'Enter a whole number.'\"):\n            f.clean(\"1.2.3\")\n        with self.assertRaisesMessage(ValidationError, \"'Enter a whole number.'\"):\n            f.clean(\"1,000\")\n        with self.assertRaisesMessage(ValidationError, \"'Enter a whole number.'\"):\n            f.clean(\"...\")\n", "    def test_integerfield_unicode_invalid_number(self):\n        f = IntegerField()\n        with self.assertRaisesMessage(ValidationError, \"'Enter a whole number.'\"):\n            f.clean(\"\uff15\uff10a\")\n", "    def test_integerfield_empty_string(self):\n        f = IntegerField()\n        self.assertEqual(f.clean(\"\"), \"\")\n", "compilation error", "    def test_integerfield_invalid_input(self):\n        f = IntegerField()\n        with self.assertRaisesMessage(ValidationError, \"'Enter a whole number.'\"):\n            f.clean('abc')\n"], "sample_331": ["    def test_iso8601_duration(self):\n        test_values = (\n            ('P1DT2H3M4S', timedelta(days=1, hours=2, minutes=3, seconds=4)),\n            ('PT2H3M4S', timedelta(hours=2, minutes=3, seconds=4)),\n            ('P2D', timedelta(days=2)),\n            ('PT30S', timedelta(seconds=30)),\n            ('-P1DT2H3M4S', timedelta(days=-1, hours=-2, minutes=-3, seconds=-4)),\n            ('P1.5D', timedelta(days=1.5)),\n            ('P1D5.5H', timedelta(days=1, hours=5.5)),\n            ('P1DT2.5H3M', timedelta(days=1, hours=2.5, minutes=3)),\n            ('PT2.5H3M4.5S', timedelta(hours=2.5, minutes=3, seconds=4.5)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n", "    def test_parse_duration_iso8601(self):\n        test_values = (\n            ('P1DT12H30M', timedelta(days=1, hours=12, minutes=30)),\n            ('P1D', timedelta(days=1)),\n            ('PT12H30M', timedelta(hours=12, minutes=30)),\n            ('PT30M', timedelta(minutes=30)),\n            ('PT30S', timedelta(seconds=30)),\n            ('P1DT12H30M15.5S', timedelta(days=1, hours=12, minutes=30, seconds=15.5)),\n            ('-P1D', timedelta(days=-1)),\n            ('-PT12H30M', timedelta(hours=-12, minutes=-30)),\n            ('-PT30M', timedelta(minutes=-30)),\n            ('-PT30S', timedelta(seconds=-30)),\n            ('-P1DT12H30M15.5S', timedelta(days=-1, hours=-12, minutes=-30, seconds=-15.5)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n\n", "    def test_invalid_duration(self):\n        invalid_inputs = (\n            'abc',\n            '15:30:61',\n            '25:30',\n            '15:30.abc',\n            '15:30,abc',\n            '15:30.1.2',\n            '-15:30.1.2',\n            '15 30:30',\n            '15:30 30',\n            '10:15:30Z',\n            '10:15:30+',\n\n        )\n        for source in invalid_inputs:\n            with self.subTest(source=source):\n                self.assertIsNone(parse_duration(source))\n", "    def test_iso8601_format(self):\n        test_values = (\n            ('P1DT2H3M4S', timedelta(days=1, hours=2, minutes=3, seconds=4)),\n            ('PT2H3M4S', timedelta(hours=2, minutes=3, seconds=4)),\n            ('P1D', timedelta(days=1)),\n            ('PT2H', timedelta(hours=2)),\n            ('PT3M', timedelta(minutes=3)),\n            ('PT4S', timedelta(seconds=4)),\n            ('-P1DT2H3M4S', timedelta(days=-1, hours=-2, minutes=-3, seconds=-4)),\n            ('P1DT2.5H3M4S', timedelta(days=1, hours=2, minutes=3, seconds=4)),\n            ('P1DT2.5H3M4.5S', timedelta(days=1, hours=2, minutes=3, seconds=4, microseconds=500000)),\n\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n", "    def test_iso8601_duration_format(self):\n        test_values = (\n            ('P1DT2H30M45S', timedelta(days=1, hours=2, minutes=30, seconds=45)),\n            ('PT2H30M45S', timedelta(hours=2, minutes=30, seconds=45)),\n            ('PT2H30M', timedelta(hours=2, minutes=30)),\n            ('P1D', timedelta(days=1)),\n            ('-P1DT2H30M45S', timedelta(days=-1, hours=-2, minutes=-30, seconds=-45)),\n            ('P1D2H30M45.123S', timedelta(days=1, hours=2, minutes=30, seconds=45, milliseconds=123000)),\n            ('P1D2H30M45,123S', timedelta(days=1, hours=2, minutes=30, seconds=45, milliseconds=123000)),\n        )\n        for source, expected in test_values:\n            with self.subTest(source=source):\n                self.assertEqual(parse_duration(source), expected)\n\n"], "sample_729": ["    def test_enet_fit_intercept_with_precompute(self):\n        # Test if fit_intercept is applied correctly when precompute is used.\n\n        n_samples = 20\n        n_features = 10\n\n        X, y, _, _ = build_dataset(n_samples=n_samples, n_features=n_features)\n        Gram = X.T @ X\n\n        # Fit with fit_intercept=True and precompute=True\n        clf_intercept = ElasticNet(fit_intercept=True, precompute=Gram)\n        clf_intercept.fit(X, y)\n\n        # Fit with fit_intercept=False and precompute=True\n        clf_no_intercept = ElasticNet(fit_intercept=False, precompute=Gram)\n        clf_no_intercept.fit(X, y)\n\n        # The coefficients should be different due to the intercept term\n        assert_not_almost_equal(clf_intercept.coef_, clf_no_intercept.coef_)\n\n        # Test that the intercept is correctly set when fit_intercept=True\n        assert clf_intercept.intercept_ != 0\n        assert clf_no_intercept.intercept_ == 0\n", "    def test_enet_with_sample_weights():\n        X, y, _, _ = build_dataset(n_samples=20, n_features=10)\n        sample_weight = np.random.rand(X.shape[0])\n        enet = ElasticNet(fit_intercept=True)\n        enet.fit(X, y, sample_weight=sample_weight)\n        assert_array_equal(enet.n_iter_, 1)  # should converge in one iteration\n", "    def test_enet_warm_start_with_sparse_input():\n        X, y, _, _ = build_dataset(n_samples=50, n_features=20)\n        sparse_X = sparse.csr_matrix(X)\n        \n        clf = ElasticNet(warm_start=True, alpha=0.5, max_iter=10)\n        clf.fit(sparse_X, y)\n        initial_coef = clf.coef_.copy()\n        \n        clf.fit(sparse_X, y)\n        assert_array_almost_equal(clf.coef_, initial_coef) \n", "def test_enet_with_sample_weight():\n    # Test sample weights for ElasticNet and MultiTaskElasticNet.\n    X, y, _, _ = build_dataset(n_samples=20, n_features=10)\n\n    sample_weight = np.ones(X.shape[0])\n    sample_weight[::2] *= 2\n    \n    # Test ElasticNet\n    clf = ElasticNet(random_state=42)\n    clf.fit(X, y, sample_weight=sample_weight)\n    coef_weights = clf.coef_\n\n    # Test MultiTaskElasticNet\n    multi_y = np.hstack((y[:, np.newaxis], y[:, np.newaxis]))\n    clf_multi = MultiTaskElasticNet(random_state=42)\n    clf_multi.fit(X, multi_y, sample_weight=sample_weight)\n    coef_weights_multi = clf_multi.coef_\n\n    # Check if coefficients are different when using sample weights.\n    clf_no_weight = ElasticNet(random_state=42)\n    clf_no_weight.fit(X, y)\n    assert_array_not_equal(coef_weights, clf_no_weight.coef_)\n    clf_multi_no_weight = MultiTaskElasticNet(random_state=42)\n    clf_multi_no_weight.fit(X, multi_y)\n    assert_array_not_equal(coef_weights_multi, clf_multi_no_weight.coef_)\n\n\n", "def test_enet_precompute_sparse_input():\n    X, y, _, _ = build_dataset(n_samples=20, n_features=10)\n    X_sparse = sparse.csr_matrix(X)\n\n    # Test both 'auto' and 'True' options for precompute\n    for precompute in ['auto', True]:\n        enet = ElasticNet(precompute=precompute)\n        enet.fit(X_sparse, y)\n        # Check that the Gram matrix is computed correctly\n        assert_array_equal(enet.GramMatrix, X_sparse.T.dot(X_sparse))\n\n"], "sample_858": ["def test_custom_estimator_names():\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    eclf1 = VotingClassifier(estimators=[\n        ('logreg', clf1), ('randfor', clf2)],\n        voting='soft').fit(X, y)\n    assert 'logreg' in eclf1.named_estimators\n    assert eclf1.named_estimators.logreg is eclf1.estimators[0][1]\n    assert 'randfor' in eclf1.named_estimators\n    assert eclf1.named_estimators.randfor is eclf1.estimators[1][1]\n", "def test_voting_classifier_with_empty_estimator_list():\n    \"\"\"Test VotingClassifier with an empty estimator list raises ValueError.\"\"\"\n    msg = \"At least one estimator is required.\"\n    with pytest.raises(ValueError, match=msg):\n        VotingClassifier().fit(X, y)\n", "def test_voting_classifier_sparse_input():\n    \"\"\"Check that VotingClassifier works with sparse input.\"\"\"\n    from scipy.sparse import csr_matrix\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    clf3 = GaussianNB()\n    eclf = VotingClassifier(estimators=[\n                            ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n                            voting='soft',\n                            weights=[1, 1, 1])\n\n    X_sparse = csr_matrix(X)\n    eclf.fit(X_sparse, y)\n\n    assert eclf.predict(X_sparse).shape == (len(y),)\n    assert eclf.predict_proba(X_sparse).shape == (len(y), 2)\n\n", "def test_voting_classifier_with_empty_estimator_list():\n    \"\"\"Check that VotingClassifier throws an error if estimator list is empty.\"\"\"\n    with pytest.raises(ValueError, match=\"Require at least one estimator\"):\n        VotingClassifier([]).fit(X, y)\n", "def test_voting_classifier_with_empty_estimator_list():\n    \"\"\"Check that VotingClassifier handles an empty list of estimators.\"\"\"\n    eclf = VotingClassifier(estimators=[])\n    msg = (\"VotingClassifier must have at least one estimator.\"\n           \" Got 0 estimators.\")\n    with pytest.raises(ValueError, match=msg):\n        eclf.fit(X, y)\n"], "sample_607": ["def test_guess_engine_none_available():\n    with pytest.raises(ValueError, match=r\"xarray is unable to open this file\"):\n        plugins.guess_engine(\"not-valid\")\n\n", "def test_list_engines_no_entrypoints():\n    engines = plugins.list_engines()\n\n    assert engines == {} \n", "def test_get_backend():\n    engines = plugins.build_engines([])\n    assert isinstance(plugins.get_backend(\"dummy\"), DummyBackendEntrypoint1)\n    assert isinstance(plugins.get_backend(DummyBackendEntrypoint1), DummyBackendEntrypoint1)\n    with pytest.raises(ValueError):\n        plugins.get_backend(\"invalid\")\n    with pytest.raises(TypeError):\n        plugins.get_backend(123)\n", "    def test_get_backend_string(dummy_pkg_entrypoint):\n        dummy_backend = plugins.get_backend(\"dummy\")\n        assert isinstance(dummy_backend, DummyBackendEntrypoint1)\n", "def test_no_installed_backends():\n    with pytest.raises(ValueError, match=r\"xarray is unable to open this file because it has no currently installed IO backends\"):\n        plugins.guess_engine(\"not-valid\")\n"], "sample_570": ["    def test_empty_data_ci(self):\n        agg = EstimateAggregator(\"mean\", \"ci\")\n        df = pd.DataFrame({'x': []})\n        out = agg(df, \"x\")\n        assert pd.isna(out[\"x\"])\n        assert pd.isna(out[\"xmin\"])\n        assert pd.isna(out[\"xmax\"])\n", "    def test_no_errorbars(self, long_df):\n\n        agg = EstimateAggregator(\"mean\", None)\n        out = agg(long_df, \"x\")\n        assert out[\"x\"] == long_df[\"x\"].mean()\n        assert \"xmin\" not in out\n        assert \"xmax\" not in out\n", "    def test_errorbar_level(self, long_df):\n\n        agg = EstimateAggregator(\"mean\", \"ci\", level=80)\n        out = agg(long_df, \"y\")\n        assert out[\"ymin\"] == np.percentile(long_df[\"y\"], 10)\n        assert out[\"ymax\"] == np.percentile(long_df[\"y\"], 90)\n", "    def test_no_errorbar(self, long_df):\n        agg = EstimateAggregator(\"mean\")\n        out = agg(long_df, \"y\")\n        assert out[\"y\"] == long_df[\"y\"].mean()\n        assert 'ymin' not in out\n        assert 'ymax' not in out\n", "    def test_empty_errorbars(self, long_df):\n\n        agg = EstimateAggregator(\"mean\", \"ci\")\n        out = agg(long_df[long_df[\"x\"] > 10], \"x\")\n        assert pd.isna(out[\"x\"])\n        assert pd.isna(out[\"xmin\"])\n        assert pd.isna(out[\"xmax\"])\n"], "sample_47": ["    def test_cleanse_setting_nested_dict(self):\n        settings = {'SECRET_KEY': 'super_secret', 'OTHER_DATA': {'NESTED_KEY': 'value'}}\n        expected = {'SECRET_KEY': CLEANSED_SUBSTITUTE, 'OTHER_DATA': {'NESTED_KEY': 'value'}}\n        cleansed_settings = cleanse_settings(settings)\n        self.assertEqual(cleansed_settings, expected)\n", "    def test_cleanse_setting_nested(self):\n        data = {'KEY': 'super_secret', 'OTHER': {'NESTED': 'value'}}\n        expected_data = {'KEY': CLEANSED_SUBSTITUTE, 'OTHER': {'NESTED': 'value'}}\n        cleansed_data = cleanse_setting('KEY', data)\n        self.assertEqual(cleansed_data, expected_data)\n", "    def test_cleanse_setting_unicode(self):\n        self.assertEqual(cleanse_setting('PASSWORD', '\u00fcber'), CLEANSED_SUBSTITUTE)\n", "    def test_cleanse_setting_dict(self):\n        data = {'password': 'super_secret', 'API_KEY': 'api_key_value'}\n        cleaned_data = cleanse_setting('PASSWORD', data)\n        for key, value in cleaned_data.items():\n            if key.lower() == 'password':\n                self.assertEqual(value, CLEANSED_SUBSTITUTE)\n            else:\n                self.assertEqual(value, data[key])\n\n", "    def test_cleanse_setting_nested_dict(self):\n        data = {'AUTH_TOKEN': 'secret', 'nested': {'PASSWORD': 'another secret'}}\n        cleaned_data = cleanse_settings(data)\n        self.assertEqual(cleaned_data['AUTH_TOKEN'], CLEANSED_SUBSTITUTE)\n        self.assertEqual(cleaned_data['nested']['PASSWORD'], CLEANSED_SUBSTITUTE)\n"], "sample_1053": ["compilation error", "compilation error", "    def test_issue_10841():\n        assert Float(1.1, 5) + Float(2.2, 4) == Float(3.3, 4)\n        assert Float(1.1, 4) + Float(2.2, 5) == Float(3.3, 4)\n", "compilation error", "compilation error"], "sample_642": ["def test_pylintrc_prioritization(tmp_path: Path, capsys: CaptureFixture) -> None:\n    \"\"\"Test prioritization of pylintrc files.\"\"\"\n    # pylint: disable=duplicate-code\n    chroot_path = tmp_path\n    testutils.create_files(\n        [\n            \"a/pylintrc\",\n            \"a/b/__init__.py\",\n            \"a/b/pylintrc\",\n            \"a/b/c/__init__.py\",\n            \"a/b/c/pylintrc\",\n            \"a/b/c/d/__init__.py\",\n            \"a/b/c/d/pylintrc\",\n        ]\n    )\n    with open(chroot_path / \"a\" / \"pylintrc\", \"w\") as f:\n        f.write(\"load-plugins = foo\")\n    with open(chroot_path / \"a\" / \"b\" / \"pylintrc\", \"w\") as f:\n        f.write(\"load-plugins = bar\")\n    with open(chroot_path / \"a\" / \"b\" / \"c\" / \"pylintrc\", \"w\") as f:\n        f.write(\"load-plugins = baz\")\n    with open(chroot_path / \"a\" / \"b\" / \"c\" / \"d\" / \"pylintrc\", \"w\") as f:\n        f.write(\"load-plugins = qux\")\n    with fake_home():\n        os.chdir(chroot_path / \"a\" / \"b\" / \"c\" / \"d\")\n        with pytest.raises(SystemExit):\n            Run([\"--verbose\"])\n        out = capsys.readouterr()\n        assert \"load-plugins = foo, bar, baz, qux\" in out.err  \n\n", "def test_rcfile_option(capsys: CaptureFixture) -> None:\n    \"\"\"Test that the --rcfile option takes precedence over the environment.\"\"\"\n    with fake_home():\n        os.environ[\"PYLINTRC\"] = os.path.join(tempfile.gettempdir(), \".pylintrc\")\n        with open(os.environ[\"PYLINTRC\"], \"w\") as f:\n            f.write(\"[MESSAGES CONTROL]\\n\")\n            f.write(\"disable=too-many-lines\")\n\n        with tempdir() as chroot:\n            chroot_path = Path(chroot)\n            testutils.create_files([\"a/b/c/d/__init__.py\"])\n            os.chdir(chroot_path / \"a/b/c\")\n            with pytest.raises(SystemExit):\n                Run([\"--rcfile\", \"pylintrc\"])\n            out = capsys.readouterr()\n            # This output only exists when launched in verbose mode\n            assert \"No config file found, using default configuration\" not in out.err\n\n", "def test_pylintrc_toml(capsys: CaptureFixture) -> None:\n    \"\"\"Test that we correctly load from a toml config.\"\"\"\n    with tempdir() as chroot:\n        with fake_home():\n            chroot_path = Path(chroot)\n            testutils.create_files(\n                [\n                    \"a/pylintrc\",\n                    \"a/pyproject.toml\",\n                    \"a/b/c/d/__init__.py\",\n                ]\n            )\n            with open(chroot_path / \"a\" / \"pylintrc\", \"w\", encoding=\"utf8\") as f:\n                f.write(\n                    \"\"\"", "def test_pylintrc_toml(tmp_path: Path) -> None:\n    \"\"\"Test that a TOML file is found and used.\"\"\"\n    with tempdir() as chroot:\n        with fake_home():\n            chroot_path = Path(chroot)\n            testutils.create_files([\"a/b/c/d/__init__.py\"])\n            toml_file = tmp_path / \"pyproject.toml\"\n            with open(toml_file, \"w\", encoding=\"utf8\") as f:\n                f.write(\n                    \"\"\"", "def test_init_hook(capsys: CaptureFixture) -> None:\n    \"\"\"Test that the --init-hook option executes correctly.\"\"\"\n    with tempdir() as chroot:\n        with fake_home():\n            chroot_path = Path(chroot)\n            os.chdir(chroot_path)\n            testutils.create_files([\"a/__init__.py\"])\n            with pytest.raises(SystemExit):\n                Run([\"a\", \"--init-hook\", 'print(\"Hello from init hook\")'])\n            out = capsys.readouterr()\n            assert \"Hello from init hook\" in out.err\n"], "sample_146": ["    def test_consistent_language_settings(self):\n        for tag in ['en', 'fr']:\n            with self.subTest(tag), self.settings(LANGUAGE_CODE=tag):\n                self.assertEqual(check_language_settings_consistent(None), [])\n", "    def test_consistent_language_settings(self):\n        with self.settings(LANGUAGE_CODE='en'):\n            self.assertEqual(check_language_settings_consistent(None), [])\n", "    def test_valid_language_settings(self):\n        with self.settings(LANGUAGE_CODE='en', LANGUAGES=[('en', 'English')]):\n            self.assertEqual(check_language_settings_consistent(None), [])\n\n", "    def test_consistent_language_settings(self):\n        for tag in ['en']:\n            with self.subTest(tag), self.settings(LANGUAGE_CODE=tag):\n                self.assertEqual(check_language_settings_consistent(None), [])\n", "    def test_consistent_language_settings(self):\n        self.assertEqual(check_language_settings_consistent(None), [])\n"], "sample_927": ["        def assert_consistent_classes(self, other):\n            assert self.name == other.name, \\\n                f'Inconsistent role names: expected {self.name}, got {other.name}'\n            assert self.classes == other.classes, \\\n                f'Inconsistent root element classes for role {self.name}:\\n' \\\n                f'Expected: {self.classes}\\nGot: {other.classes}'\n            for tag in set(self.content_classes.keys()).union(other.content_classes.keys()):\n                assert self.content_classes.get(tag, set()) == other.content_classes.get(tag, set()), \\\n                    f'Inconsistent classes for role {self.name}, tag {tag}:\\n' \\\n                    f'Expected: {self.content_classes.get(tag, set())}\\nGot: {other.content_classes.get(tag, set())}'\n", "compilation error", "    def test_xref_consistency():\n        #  Ensure the classes are consistent across different XRef roles within the same context.\n        func = RoleClasses('func', 'a', ['p'])\n        check('p', {'xref', 'fn'})\n\n        class_ = RoleClasses('class', 'a', ['p'])\n        check('p', {'xref', 'cl'})\n\n        var = RoleClasses('var', 'a', ['p'])\n        check('p', {'xref', 'var'})\n\n        # Function XRefs should be consistent across different contexts\n        func_in_p = RoleClasses('func', 'p', ['a'])\n        check('a', {'xref', 'fn'})\n\n        func_in_span = RoleClasses('func', 'span', ['a'])\n        check('a', {'xref', 'fn'})\n", "        def __str__(self):\n            return f'''{self.name}\n              classes: {self.classes}\n              content classes: {self.content_classes}'''\n", "        def check_content_classes(self, other):\n            \"\"\"Check that the content classes of two roles match.\"\"\"\n\n            for tag, content_classes in self.content_classes.items():\n                self.assertSetEqual(content_classes, other.content_classes[tag],\n                                    msg=f'Mismatch in classes for tag {tag} in roles {self.name} and {other.name}')\n"], "sample_1175": ["compilation error", "compilation error", "def test_issue_18406():\n    x = Symbol('x')\n    y = Symbol('y')\n    assert pretty(Piecewise((x, x > 1), (y, True), evaluate=False)) == \\\n    '\u23a7x   for x > 1\\n'\\\n    '\u23a8          \\n'\\\n    '\u23a9y   otherwise'\n    assert upretty(Piecewise((x, x > 1), (y, True), evaluate=False)) == \\\n    '\u23a7x   for x > 1\\n'\\\n    '\u23a8          \\n'\\\n    '\u23a9y   otherwise'\n\n\n\n", "compilation error", "compilation error"], "sample_901": ["def test_k_means_empty_cluster_relocated_sparse():\n    # check that empty clusters are correctly relocated when using sample\n    # weights with sparse input (#13486)\n    X = sp.csr_matrix([[1], [1]])\n    sample_weight = np.array([1.9, 0.1])\n    init = np.array([[1], [10]])\n\n    km = KMeans(n_clusters=2, init=init, n_init=1)\n    km.fit(X, sample_weight=sample_weight)\n\n    assert len(set(km.labels_)) == 2\n    assert_allclose(km.cluster_centers_, [[1], [10]])\n\n", "    def test_k_means_empty_cluster_partial_fit(self):\n        # Check that empty clusters are handled correctly\n        # during partial_fit.\n\n        # Create a dataset with two distinct clusters\n        X = np.array([[0, 0], [1, 1], [2, 2], [3, 3], [4, 4]])\n\n        # Initialize KMeans with 3 clusters\n        km = KMeans(n_clusters=3, random_state=0)\n\n        # Partially fit on the first 3 samples, creating an empty cluster\n        km.partial_fit(X[:3])\n\n        # Fit on the remaining 2 samples\n        km.partial_fit(X[3:])\n\n        # Check that there are no empty clusters\n        assert len(set(km.labels_)) == 3\n", "    def test_k_means_empty_cluster_relocated_minibatch():\n        # check that empty clusters are correctly relocated when using sample\n        # weights with MiniBatchKMeans (#13486)\n        X = np.array([[-1], [1]])\n        sample_weight = [1.9, 0.1]\n        init = np.array([[-1], [10]])\n\n        mbkm = MiniBatchKMeans(n_clusters=2, init=init, n_init=1)\n        mbkm.partial_fit(X, sample_weight=sample_weight)\n        mbkm.partial_fit(X, sample_weight=sample_weight)\n\n        assert len(set(mbkm.labels_)) == 2\n        assert_allclose(mbkm.cluster_centers_, [[-1], [1]])\n", "def test_k_means_empty_cluster_warning():\n    # Check that a warning is raised when a cluster is empty\n\n    X = np.array([[1, 2], [1.1, 2.2], [10, 10]])\n    kmeans = KMeans(n_clusters=3, random_state=42)\n\n    with pytest.warns(ConvergenceWarning):\n        kmeans.fit(X)\n", "compilation error"], "sample_303": ["    def test_runshell(self):\n        with mock.patch('subprocess.run') as mock_run:\n            self.client.runshell([])\n            mock_run.assert_called_once()\n", "    def test_runshell(self):\n        with mock.patch('subprocess.run') as mock_run:\n            self.client.runshell([])\n            mock_run.assert_called_once()\n", "    def test_runshell(self):\n        with mock.patch('subprocess.run') as mock_run:\n            self.client.runshell(parameters='some_parameters')\n            mock_run.assert_called_once_with(\n                mock.ANY, env=mock.ANY, check=True\n            )\n", "    def test_runshell(self):\n        with mock.patch('subprocess.run') as mock_run:\n            self.client.runshell(['some_parameters'])\n            mock_run.assert_called_once_with(\n                ['some_parameters'], env=None, check=True\n            )\n", "    def test_runshell(self):\n        with mock.patch('subprocess.run') as mock_run:\n            self.client.runshell([])\n            mock_run.assert_called_once_with(\n                [], env=None, check=True\n            )\n"], "sample_83": ["    def test_tag_function(self):\n            return Node()\n        self.library.tag_function(func)\n        self.assertEqual(self.library.tags[func.__name__], func)\n", "    def test_tag_function(self):\n        @self.library.tag_function\n            return Node()\n\n        self.assertEqual(self.library.tags['func'], func)\n", "    def test_tag_function(self):\n            return Node()\n        self.library.tag_function(func)\n        self.assertEqual(self.library.tags[func.__name__], func)\n", "    def test_tag_invalid(self):\n        msg = \"Unsupported arguments to Library.tag: (None, None)\"\n        with self.assertRaisesMessage(ValueError, msg):\n            self.library.tag(None, None)\n\n", "    def test_tag_invalid(self):\n        msg = \"Unsupported arguments to Library.tag: (None, '')\"\n        with self.assertRaisesMessage(ValueError, msg):\n            self.library.tag(None, '')\n"], "sample_149": ["    def test_permission_name_and_codename_length(self):\n        class Checked(models.Model):\n            class Meta:\n                verbose_name = 'Some model'\n                permissions = [\n                    ('some_long_permission_codename', 'Some permission'),\n                ]\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [])\n", "    def test_builtin_permission_name_max_length(self):\n        class Checked(models.Model):\n            class Meta:\n                verbose_name = 'some long verbose name'\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Error(\n                \"The verbose_name of model 'auth_tests.Checked' must be at most 160 \"\n                \"characters for its builtin permission names to be at most 255 \"\n                \"characters.\",\n                obj=Checked,\n                id='auth.E007',\n            ),\n        ])\n\n", "    def test_is_anonymous_property(self):\n        \"\"\"<User Model>.is_anonymous/is_authenticated must not be methods (property version).\"\"\"\n        class CustomUserWithBadIsAnonymousProperty(AbstractBaseUser):\n            username = models.CharField(max_length=30, unique=True)\n            USERNAME_FIELD = 'username'\n\n            @property\n                return True\n\n                return True\n        errors = checks.run_checks(app_configs=self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Critical(\n                '%s.is_anonymous must be an attribute or property rather than '\n                'a method. Ignoring this is a security issue as anonymous '\n                'users will be treated as authenticated!' % CustomUserWithBadIsAnonymousProperty,\n                obj=CustomUserWithBadIsAnonymousProperty,\n                id='auth.C009',\n            ),\n        ])\n", "    def test_user_model_missing_user_fields(self):\n        \"\"\"\n        Check that a custom user model without USERNAME_FIELD and REQUIRED_FIELDS\n        doesn't raise errors.\n        \"\"\"\n        class CustomUserWithoutUserFields(AbstractBaseUser):\n            date_of_birth = models.DateField()\n\n            USERNAME_FIELD = None\n            REQUIRED_FIELDS = []\n\n        errors = checks.run_checks(self.apps.get_app_configs())\n        self.assertEqual(errors, [])\n", "    def test_is_anonymous_is_authenticated_properties(self):\n        \"\"\"\n        <User Model>.is_anonymous/is_authenticated must not be methods.\n\n        This test case handles the scenario where is_anonymous and is_authenticated are\n        defined as properties.\n        \"\"\"\n\n        class CustomUser(AbstractBaseUser):\n            username = models.CharField(max_length=30, unique=True)\n            USERNAME_FIELD = 'username'\n\n            @property\n                return True\n\n            @property\n                return True\n\n        errors = checks.run_checks(app_configs=self.apps.get_app_configs())\n        self.assertEqual(errors, [\n            checks.Critical(\n                '%s.is_anonymous must be an attribute rather than '\n                'a method. Ignoring this is a security issue as anonymous '\n                'users will be treated as authenticated!' % CustomUser,\n                obj=CustomUser,\n                id='auth.C009',\n            ),\n            checks.Critical(\n                '%s.is_authenticated must be an attribute rather '\n                'than a method. Ignoring this is a security issue as anonymous '\n                'users will be treated as authenticated!' % CustomUser,\n                obj=CustomUser,\n                id='auth.C010',\n            ),\n        ])\n"], "sample_974": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_468": ["    def test_context_processors_are_called_in_correct_order(self):\n        request = self.request_factory.get(\"/\")\n        processor_calls = []\n\n            processor_calls.append(\"processor1\")\n            return {}\n\n            processor_calls.append(\"processor2\")\n            return {}\n\n        ctx = RequestContext(request, processors=[processor1, processor2])\n        \n        self.assertEqual(processor_calls, [\"processor1\", \"processor2\"])\n", "    def test_context_processors_order(self):\n        request = self.request_factory.get(\"/\")\n        ctx = RequestContext(\n            request,\n            processors=[context_process_returning_none, lambda r: {\"foo\": \"bar\"}],\n        )\n        self.assertEqual(ctx.dicts[3], {\"foo\": \"bar\"})\n", "    def test_context_processors_output(self):\n        request = self.request_factory.get(\"/\")\n\n            return {'test_key': 'test_value'}\n\n        context = RequestContext(request, processors=[processor])\n        self.assertEqual(context['test_key'], 'test_value')\n", "    def test_context_processor_return_type(self):\n        request = self.request_factory.get(\"/\")\n        with override_settings(TEMPLATE_CONTEXT_PROCESSORS=[context_process_returning_none]):\n            ctx = RequestContext(request, {})\n            self.assertEqual(len(ctx.dicts), 4)\n            self.assertEqual(ctx.dicts[-2], {})\n", "    def test_context_processors_order(self):\n        class ContextProcessor1:\n                return {\"processor1\": \"value1\"}\n\n        class ContextProcessor2:\n                return {\"processor2\": \"value2\"}\n\n        request = self.request_factory.get(\"/\")\n        ctx = RequestContext(\n            request, processors=[ContextProcessor1(), ContextProcessor2()],\n        )\n        self.assertEqual(ctx[\"processor1\"], \"value1\")\n        self.assertEqual(ctx[\"processor2\"], \"value2\")\n\n"], "sample_1107": ["    def test_minlex_default():\n        assert minlex([1, 2, 0], default=True) == (0, 1, 2)\n        assert minlex(\"aba\", default=True) == \"aab\"\n\n", "    def test_rgs_unrank():\n        for n in range(1, 6):\n            for i in range(RGS_enum(n)):\n                assert RGS_rank(RGS_unrank(i, n)) == i\n", "    def test_randomized_selection():\n        from sympy.utilities.iterables import randomized_selection\n        assert randomized_selection([1, 2, 3, 4, 5], 3) in [\n            [2, 5, 3], [3, 2, 5], [5, 3, 2], [2, 3, 5], [5, 2, 3],\n            [3, 5, 2], [1, 4, 5], [4, 1, 5], [5, 4, 1], [1, 5, 4],\n            [4, 5, 1], [5, 1, 4], [1, 2, 4], [2, 1, 4], [4, 2, 1],\n            [1, 4, 2], [4, 1, 2], [2, 4, 1], [1, 3, 4], [3, 1, 4],\n            [4, 3, 1], [1, 4, 3], [4, 1, 3], [3, 4, 1], [3, 2, 4],\n            [2, 3, 4], [4, 2, 3], [2, 4, 3], [4, 3, 2], [3, 4, 2]]\n", "    def test_permutations_with_replacement():\n        assert list(permutations_with_replacement('ab', 2)) == [\n            'aa', 'ab', 'ba', 'bb']\n        assert list(permutations_with_replacement([1, 2], 2)) == [\n            [1, 1], [1, 2], [2, 1], [2, 2]]\n", "    def test_multiset_permutations_empty():\n        assert list(multiset_permutations(multiset(), 2)) == []\n"], "sample_1136": ["compilation error", "compilation error", "compilation error", "    def test_issue_20034():\n        p = Poly(x**2 + 2*x*y + y**2, x, y)\n        assert p.as_expr() == x**2 + 2*x*y + y**2\n        assert p.subs(x, 1).as_expr() == 1 + 2*y + y**2\n\n", "compilation error"], "sample_36": ["compilation error", "compilation error", "def test_biweight_midcovariance_empty():\n    \"\"\"Test biweight_midcovariance with an empty array.\"\"\"\n    x = np.array([])\n    with pytest.raises(ValueError) as excinfo:\n        biweight_midcovariance(x)\n    assert \"Input array must have at least one element.\" in str(excinfo.value)\n", "def test_biweight_midcorrelation_constant():\n    x = np.ones(10)\n    y = 2 * np.ones(10)\n    assert_allclose(biweight_midcorrelation(x, y), 1.0)\n", "compilation error"], "sample_575": ["    def test_label_format_string(self, t):\n\n        s = Temporal().label(\"%Y-%m-%d\")\n        a = PseudoAxis(s._setup(t, Coordinate())._matplotlib_scale)\n        a.set_view_interval(10, 1000)\n        label, = a.major.formatter.format_ticks([100])\n        assert label == \"1972-09-27\"\n\n", "    def test_fill_with_order(self, x):\n        vs = {\"c\": True, \"a\": False, \"b\": True}\n        s = Nominal(order=[\"a\", \"b\", \"c\"])._setup(x, Fill())\n        assert_array_equal(s(x), [False, True, True, True])\n\n", "    def test_tick_every(self, t, x):\n\n        d = 50\n        ax = mpl.figure.Figure().subplots()\n        Temporal().tick(every=d)._setup(t, Coordinate(), ax.xaxis)\n        locator = ax.xaxis.get_major_locator()\n        assert np.allclose(np.diff(locator.maxticks.values()), d)\n", "    def test_tick_every(self, t, x):\n        days = pd.Timedelta(days=10)\n        ax = mpl.figure.Figure().subplots()\n        Temporal().tick(every=days)._setup(t, Coordinate(), ax.xaxis)\n        locator = ax.xaxis.get_major_locator()\n        assert set(locator.maxticks.values()) == {10}\n", "    def test_label_base(self, t, x):\n\n        s = Temporal(base=365)._setup(t, Coordinate(), ax.xaxis)\n        a = PseudoAxis(s._setup(t, Coordinate())._matplotlib_scale)\n        a.set_view_interval(10, 1000)\n        label, = a.major.formatter.format_ticks([100])\n        assert re.match(r\"^\\d+ yr$\", label)\n"], "sample_608": ["def test_diff_dataset_repr_with_different_attrs_and_coords() -> None:\n    ds_a = xr.Dataset(coords={\"x\": range(3)}, attrs={\"a\": 1})\n    ds_b = xr.Dataset(coords={\"x\": range(4)}, attrs={\"b\": 2})\n    expected = dedent(\n        \"\"\"\n        Left and right Dataset objects are not identical\n        Differing dimensions:\n            (x: 3) != (x: 4)\n        Differing coordinates:\n        L * x        (x) int64 0 1 2\n        R * x        (x) int64 0 1 2 3\n        Differing attributes:\n        L   a: 1\n        R   b: 2\n        \"\"\"\n    )\n    actual = formatting.diff_dataset_repr(ds_a, ds_b, \"identical\")\n    assert actual == expected\n", "    def test_diff_array_repr_with_missing_coords(self) -> None:\n        a = xr.DataArray(\n            np.arange(6).reshape(2, 3),\n            dims=(\"x\", \"y\"),\n            coords={\"x\": [0, 1]},\n        )\n        b = xr.DataArray(\n            np.arange(6).reshape(2, 3),\n            dims=(\"x\", \"y\"),\n            coords={\"y\": [0, 1, 2]},\n        )\n\n        expected = dedent(\n            \"\"\"\\\n        Left and right DataArray objects are not identical\n        Differing coordinates:\n        L * x        (x) int64 0 1\n        R * y        (y) int64 0 1 2\n        Coordinates only on the left object:\n          * x        (x) int64 0 1\n        Coordinates only on the right object:\n          * y        (y) int64 0 1 2\"\"\"\n        )\n        actual = formatting.diff_array_repr(a, b, \"identical\")\n        assert actual == expected\n", "    def test_format_item_datetime64(self) -> None:\n        actual = formatting.format_item(np.datetime64(\"2000-01-01\"))\n        expected = \"2000-01-01\"\n        assert expected == actual\n\n", "    def test_inline_variable_array_repr(self) -> None:\n        var = xr.Variable(\"x\", np.array([1, 2, 3]))\n        with pytest.raises(NotImplementedError):\n          formatting.inline_variable_array_repr(var)\n\n", "def test_diff_array_repr_with_identical_coords() -> None:\n    da_a = xr.DataArray(\n        np.array([1, 2, 3]),\n        dims=\"x\",\n        coords={\"x\": np.array([\"a\", \"b\", \"c\"])},\n    )\n\n    da_b = xr.DataArray(\n        np.array([1, 2, 3]), dims=\"x\", coords={\"x\": np.array([\"a\", \"b\", \"c\"])}\n    )\n\n    expected = \"Left and right DataArray objects are identical\"\n    actual = formatting.diff_array_repr(da_a, da_b, \"identical\")\n    assert actual == expected\n\n"], "sample_490": ["    def test_with_custom_error_message(self):\n        constraint = models.UniqueConstraint(\n            fields=[\"name\"], name=\"name_uniq\", violation_error_message=\"Custom message\"\n        )\n        msg = \"Custom message\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            constraint.validate(UniqueConstraintProduct, UniqueConstraintProduct(name=self.p1.name))\n", "    def test_violation_error_message(self):\n        UniqueConstraintViolationErrorMessage.objects.create(name=\"p1\", color=\"red\")\n        with self.assertRaisesMessage(\n            IntegrityError, \"custom error message\"\n        ):\n            UniqueConstraintViolationErrorMessage.objects.create(\n                name=\"p1\", color=\"blue\"\n            )\n", "    def test_unique_constraint_with_condition_and_expressions(self):\n        constraint = models.UniqueConstraint(\n            Lower(\"name\"),\n            condition=models.Q(color__isnull=True),\n            name=\"name_lower_without_color_uniq\"\n        )\n        obj1 = UniqueConstraintConditionProduct.objects.create(name=\"p1\", color=\"red\")\n        obj2 = UniqueConstraintConditionProduct.objects.create(name=\"p2\")\n        with self.assertRaises(IntegrityError):\n            UniqueConstraintConditionProduct.objects.create(name=obj1.name.lower(), color=None)\n        UniqueConstraintConditionProduct.objects.create(name=\"P3\", color=None)\n        \n", "    def test_validate_condition_with_exclude(self):\n        p1 = UniqueConstraintConditionProduct.objects.create(name=\"p1\")\n        constraint = UniqueConstraintConditionProduct._meta.constraints[0]\n        msg = \"Constraint \u201cname_without_color_uniq\u201d is violated.\"\n        with self.assertRaisesMessage(ValidationError, msg):\n            constraint.validate(\n                UniqueConstraintConditionProduct,\n                UniqueConstraintConditionProduct(name=p1.name, color=None),\n                exclude={\"name\"},\n            )\n", "    def test_opclasses_with_condition(self):\n        msg = (\n            \"UniqueConstraint.opclasses cannot be used with conditions. Use \"\n            \"django.contrib.postgres.indexes.OpClass() instead.\"\n        )\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                fields=[\"name\"],\n                condition=models.Q(color__isnull=True),\n                opclasses=[\"text_pattern_ops\"],\n                name=\"name_text_pattern_ops_unique\",\n            )\n"], "sample_65": ["    def test_javascript_gettext_with_context(self):\n        self.selenium.get(self.live_server_url + '/jsi18n_template/')\n\n        elem = self.selenium.find_element_by_id(\"pgettext_context\")\n        self.assertEqual(elem.text, \"Diese Nachricht sollte \u00fcbersetzt werden\")\n", "    def test_setlang_csrf_exempt(self):\n        with self.settings(CSRF_USE_SESSIONS=False):\n            post_data = {'language': 'es', 'next': '/'}\n            response = self.client.post('/i18n/setlang/', post_data, follow=True)\n            self.assertRedirects(response, '/')\n            self.assertEqual(self.client.cookies[settings.LANGUAGE_COOKIE_NAME].value, 'es')\n", "    def test_javascript_context_data(self):\n        self.selenium.get(self.live_server_url + '/jsi18n_context_data/')\n\n        elem = self.selenium.find_element_by_id(\"context_data\")\n        self.assertEqual(elem.text, \"This is a context data test\")\n\n", "    def test_javascript_gettext_fallback(self):\n        with self.settings(LANGUAGE_CODE='fr'):\n            self.selenium.get(self.live_server_url + '/jsi18n_template/')\n\n            elem = self.selenium.find_element_by_id(\"gettext\")\n            self.assertEqual(elem.text, \"Supprimer\")\n\n", "    def test_jsi18n_USE_I18N_False_fr(self):\n        response = self.client.get('/jsi18n/')\n        self.assertNotContains(\n            response,\n            '\"this color is to be translated\": \"cette couleur doit \u00eatre traduite\"'\n        )\n        self.assertContains(\n            response,\n            'django.pluralidx = function(count) { return (count == 1) ? 0 : 1; };'\n        )\n"], "sample_1100": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_737": ["    def test_vectorizer_empty_input():\n        for vec in [CountVectorizer(), TfidfVectorizer(), HashingVectorizer()]:\n            assert_raises(ValueError, vec.fit, [])\n            assert_raises(ValueError, vec.transform, [])\n", "    def test_vectorizer_empty_vocabulary():\n        vect = CountVectorizer(vocabulary=[])\n        assert_raises(ValueError, vect.fit, ALL_FOOD_DOCS)\n", "compilation error", "    def test_tfidfvectorizer_input_data_type():\n        # Test that TfidfVectorizer can handle different input data types\n        # such as lists, ndarray, and pandas Series of strings\n        data_types = [\n            ['This is a doc.', 'Another document.'],\n            np.array(['This is a doc.', 'Another document.']),\n            pd.Series(['This is a doc.', 'Another document.']),\n        ]\n\n        vect = TfidfVectorizer()\n        for data in data_types:\n            vect.fit(data)\n            transformed_data = vect.transform(data)\n            assert_equal(transformed_data.shape[0], len(data))\n\n", "    def test_vectorizer_empty_input():\n        for vec in [CountVectorizer(), TfidfVectorizer(), HashingVectorizer()]:\n            assert_raises(ValueError, vec.fit, [])\n            assert_raises(ValueError, vec.fit_transform, [])\n            assert_raises(ValueError, vec.transform, [])\n"], "sample_961": ["def test_info_field_list_empty_type(app, warning):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   :param str name: blah blah\\n\"\n            \"   :type name: \\n\")\n    doctree = restructuredtext.parse(app, text)\n\n    assert_node(doctree, (addnodes.index,\n                          [desc, (desc_signature,\n                                  [desc_content, nodes.field_list, nodes.field])]))\n    assert_node(doctree[1][1][0][0], ([nodes.field_name, \"Parameters\"],\n                                      [nodes.field_body, nodes.paragraph]))\n\n    # :param str name:\n    assert_node(doctree[1][1][0][0][1][0],\n                ([addnodes.literal_strong, \"name\"],\n                 \" (\",\n                 \")\",\n                 \" -- \",\n                 \"blah blah\"))\n", "def test_noindexentry_module_level(app):\n    text = (\".. py:module:: mymodule\\n\"\n            \".. py:function:: mymodule.f()\\n\"\n            \".. py:function:: mymodule.g()\\n\"\n            \"   :noindexentry:\\n\")\n    restructuredtext.parse(app, text)\n    index = PythonModuleIndex(app.env.get_domain('py'))\n    assert index.generate()[0] == (\n        'm',\n        [IndexEntry('mymodule', 0, 'index', 'module-mymodule', '', '', '')]\n    )\n", "def test_py_attribute_default_value(app):\n    text = (\".. py:class:: Class\\n\"\n            \"\\n\"\n            \"   .. py:attribute:: attr\\n\"\n            \"      :type: str\\n\"\n            \"      :default: 'hello'\")\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"attr\"],\n                                                     [desc_annotation, (\": \",\n                                                                        [pending_xref, \"str\"])],\n                                                     [desc_annotation, \" = 'hello'\"])],\n                                  [desc_content, ()]))\n    assert_node(doctree[1][1][0][1][1], pending_xref, **{\"py:class\": \"Class\"})\n", "def test_param_type_role(app, status, warning):\n    app.build()\n    content = (app.outdir / 'index.html').read_text()\n    assert '<span class=\"sig-param\">param</span><span class=\"sig-default\">: </span><span class=\"n\"><span class=\"pre\">int</span></span>' in content\n", "def test_special_method_signature(app):\n    text = \".. py:method:: __init__\\n\"\n    domain = app.env.get_domain('py')\n    doctree = restructuredtext.parse(app, text)\n    assert_node(doctree, (addnodes.index,\n                          [desc, ([desc_signature, ([desc_name, \"__init__\"]),\n                \t\t\t\tdesc_content)]))\n    assert domain.roles['meth'].name == \"meth\"\n"], "sample_688": ["    def test_collect_nested_packages_with_dupes(self, testdir):\n        \"\"\"Test importlib handling of nested packages with same names.\"\"\"\n        testdir.makepyfile(\n            **{\n                \"pkg1/a/foo.py\": \"def test_foo1(): pass\",\n                \"pkg1/b/foo.py\": \"def test_foo2(): pass\",\n                \"pkg2/a/foo.py\": \"def test_foo3(): pass\",\n            }\n        )\n        result = testdir.runpytest(\"-v\", \"--import-mode=importlib\", \"pkg1\", \"pkg2\")\n        result.stdout.fnmatch_lines(\n            [\n                \"pkg1/a/foo.py::test_foo1 *\",\n                \"pkg1/b/foo.py::test_foo2 *\",\n                \"pkg2/a/foo.py::test_foo3 *\",\n                \"* 3 passed in *\",\n            ]\n        )\n", "    def test_collect_init_with_no_tests(self, testdir):\n        testdir.makepyfile(\n            **{\n                \"tests/__init__.py\": \"\",\n            }\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines([\"collected 0 items / 1 error\", \"*1 error in*\"])\n", "    def test_collect_pyargs_with_testpaths(self, testdir, monkeypatch):\n        testmod = testdir.mkdir(\"testmod\")\n        # NOTE: __init__.py is not collected since it does not match python_files.\n        testmod.ensure(\"__init__.py\").write(\"def test_func(): pass\")\n        testmod.ensure(\"test_file.py\").write(\"def test_func(): pass\")\n\n        root = testdir.mkdir(\"root\")\n        root.ensure(\"pytest.ini\").write(\n            textwrap.dedent(\n                \"\"\"\n        [pytest]\n        addopts = --pyargs\n        testpaths = testmod", "    def test_collect_module_with_duplicate_name(self, testdir):\n        \"\"\"Ensure modules with duplicate names in different directories are collected correctly.\"\"\"\n        testdir.makepyfile(\n            \"module.py\": \"def test_module(): pass\",\n            \"tests/module.py\": \"def test_module_tests(): pass\",\n        )\n        result = testdir.runpytest(\"-v\", \"--import-mode=importlib\")\n        result.stdout.fnmatch_lines(\n            [\n                \"*module.py::test_module*\",\n                \"*tests/module.py::test_module_tests*\",\n                \"*2 passed in*\",\n            ]\n        )\n", "    def test_conftest_with_absolute_path_to_module(self, testdir):\n        tests_dir = testdir.tmpdir.join(\"tests\")\n        testdir.makepyfile(\n            **{\n                \"tests/conftest.py\": \"\"\"\n                    import sys\n                    sys.path.append(str(tests_dir.join(\"my_module\")))\n                \"\"\",\n                \"tests/my_module/foo.py\": \"\"\"\n                \"\"\",\n                \"tests/test_foo.py\": \"\"\"\n                    from my_module.foo import foo\n                        assert foo() == 42\n                \"\"\",\n            }\n        )\n        result = testdir.runpytest(\"-v\", \"--import-mode=importlib\")\n        result.stdout.fnmatch_lines([\"* 1 passed in *\"])\n"], "sample_405": ["    def test_references_field_with_null_references(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ForeignKey(\"Other\", models.SET_NULL, null=True),\n        )\n        self.assertIs(operation.references_field(\"Other\", \"id\", \"migrations\"), True)\n", "    def test_references_field_by_related_name(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ForeignKey(\"Other\", models.CASCADE, related_name=\"related_objects\"),\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"related_objects\", \"migrations\"), True\n        )\n", "    def test_references_field_by_related_name(self):\n        operation = FieldOperation(\n            \"Model\", \"field\", models.ForeignKey(\"Other\", models.CASCADE, related_name=\"related\")\n        )\n        self.assertIs(operation.references_field(\"Other\", \"related\", \"migrations\"), True)\n        self.assertIs(\n            operation.references_field(\"Other\", \"whatever\", \"migrations\"), False\n        )\n        self.assertIs(\n            operation.references_field(\"Missing\", \"whatever\", \"migrations\"), False\n        )\n", "    def test_references_field_by_manytomany_through_field(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ManyToManyField(\"Other\", through=\"Through\", through_fields=[\"through_field\"]),\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"whatever\", \"migrations\"), True\n        )\n        self.assertIs(\n            operation.references_field(\"Through\", \"through_field\", \"migrations\"), True\n        )\n        self.assertIs(\n            operation.references_field(\"Missing\", \"whatever\", \"migrations\"), False\n        )\n", "    def test_field_operation_delete_field(self):\n        operation = FieldOperation(\"Model\", \"field\", action=\"delete\")\n        self.assertEqual(operation.action, \"delete\")\n"], "sample_515": ["def test_colorbar_tick_placement():\n    fig, ax = plt.subplots()\n    im = ax.imshow([[0, 1], [2, 3]])\n    cb = fig.colorbar(im, extend='both')\n    cb.ax.tick_params(length=10)\n    # ensure that ticks are placed appropriately within the\n    # colorbar bounds\n    ticks = cb.ax.get_yticks()\n    tick_locations = cb.ax.yaxis.get_ticklocs()\n    assert np.all(tick_locations[tick_locations <= cb.ax.get_ylim()[1]])\n    assert np.all(tick_locations[tick_locations >= cb.ax.get_ylim()[0]])\n\n", "    def test_colorbar_label_fontproperties():\n        fig, ax = plt.subplots()\n        pc = ax.pcolormesh(np.random.randn(10, 10))\n        cb = fig.colorbar(pc, label='Test Label')\n        cb.set_label('New Label', fontdict={'fontsize': 14, 'color': 'red'})\n        assert cb.ax.get_ylabel() == 'New Label'\n        assert cb.ax.get_ylabel().get_fontsize() == 14\n        assert cb.ax.get_ylabel().get_color() == 'red'\n\n\n\n", "    def test_colorbar_mappable_update(self):\n        fig, ax = plt.subplots()\n        x = np.arange(10)\n        y = np.arange(10)\n        X, Y = np.meshgrid(x, y)\n        Z = np.sin(X) + np.cos(Y)\n        pc = ax.pcolormesh(X, Y, Z, cmap='viridis')\n        cb = fig.colorbar(pc)\n        pc.set_array(np.random.rand(10, 10))\n        fig.canvas.draw()\n        assert_allclose(cb.get_ticks(), cb.ax.get_yticks())\n\n", "    def test_colorbar_label_fontsize():\n        fig, ax = plt.subplots()\n        img = ax.imshow([[0, 1], [2, 3]])\n        cb = fig.colorbar(img, label=\"Intensity\")\n        cb.set_label(\"Intensity\", fontsize=16)\n        assert cb.ax.get_ylabel().get_fontsize() == 16\n", "def test_colorbar_set_label():\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh(np.random.randn(10, 10))\n    cb = fig.colorbar(pc)\n    cb.set_label('Test label')\n    assert cb.ax.get_ylabel() == 'Test label'\n\n"], "sample_728": ["    def test_make_moons_shuffle():\n        X1, y1 = make_moons(n_samples=100, noise=0.0, shuffle=False, random_state=0)\n        X2, y2 = make_moons(n_samples=100, noise=0.0, shuffle=True, random_state=0)\n        assert_not_equal(X1, X2)\n        assert_not_equal(y1, y2)\n", "    def test_make_checkerboard_error():\n        with pytest.raises(ValueError):\n            make_checkerboard(shape=(100, 100), n_clusters=(0, 5), \n                              shuffle=True, random_state=0)\n\n        with pytest.raises(ValueError):\n            make_checkerboard(shape=(100, 100), n_clusters=(5, 0), \n                              shuffle=True, random_state=0)\n", "def test_make_moons_n_samples():\n    n_samples = 100\n    X, y = make_moons(n_samples=n_samples, shuffle=False, noise=0.0)\n    assert_equal(X.shape[0], n_samples, \"Number of samples mismatch\")\n", "    def test_make_checkerboard_error():\n        with pytest.raises(ValueError):\n            make_checkerboard(shape=(100, 100), n_clusters=(2, 0))\n        with pytest.raises(ValueError):\n            make_checkerboard(shape=(100, 100), n_clusters=(0, 2)) \n", "def test_make_classification_hypercube_binary():\n    \"\"\"Test binary classification on a hypercube with balanced classes\n    and distinct clusters.\n    \"\"\"\n    X, y = make_classification(n_samples=100, n_features=2, n_informative=2,\n                               n_redundant=0, n_repeated=0, flip_y=0,\n                               class_sep=1e6, scale=1, shuffle=False,\n                               n_classes=2, random_state=0)\n\n    assert_equal(X.shape, (100, 2))\n    assert_equal(y.shape, (100,))\n    assert_true(np.all(np.unique(y) == np.array([0, 1])))\n\n    # Check that hypercube vertices are distinct\n    signs = np.sign(X)\n    unique_signs, counts = np.unique(signs, axis=0, return_counts=True)\n    assert_equal(unique_signs.shape[0], 4, \"Wrong number of unique signs\")\n    assert_equal(counts.sum(), len(X))\n    assert_equal(np.all(counts == 25), True, \"Classes are not balanced\")\n\n"], "sample_722": ["compilation error", "    def test_mini_batch_k_means_partial_fit_empty_batch(self):\n        mb_k_means = MiniBatchKMeans(n_clusters=3)\n        mb_k_means.partial_fit(X[:2])\n\n        # fit with an empty batch should not raise an error\n        mb_k_means.partial_fit(np.array([]))\n", "    def test_set_params_validation():\n        km = KMeans(n_clusters=3, init='k-means++', random_state=42)\n        params = {'n_clusters': 4, 'init': 'random', 'random_state': 13}\n        km.set_params(**params)\n        assert_equal(km.n_clusters, 4)\n        assert_equal(km.init, 'random')\n        assert_equal(km.random_state, 13)\n        assert_raises(ValueError, km.set_params, n_clusters=0)\n        assert_raises(ValueError, km.set_params, init='invalid_init')\n\n", "    def test_k_means_empty_cluster():\n        X = np.array([[1, 2], [1.1, 2.1], [1, 2.2], [10, 10]])\n        km = KMeans(n_clusters=4, random_state=42)\n        km.fit(X)\n        assert_equal(len(km.labels_), len(X))\n        assert_equal(km.cluster_centers_.shape, (4, 2))\n", "    def test_mini_batch_k_means_single_cluster():\n        mb_k_means = MiniBatchKMeans(n_clusters=1).fit(X)\n        assert_equal(mb_k_means.cluster_centers_.shape, (1, n_features))\n        assert_array_equal(mb_k_means.labels_, np.zeros(X.shape[0]))\n"], "sample_699": ["    def test_doctest_report_cdiff_with_empty_expected(self, pytester: Pytester):\n        pytester.makepyfile(\n            \"\"\"\n                '''\n                >>> foo()\n                \n                '''\n                print()\n            \"\"\"\n        )\n        result = self._run_doctest_report(pytester, \"cdiff\")\n        result.stdout.fnmatch_lines(\n            [\n                \"         \",\n                \"    --- 1,1 ----\"\n            ]\n        ) \n", "    def test_doctest_report_default(self, pytester: Pytester):\n        result = self._run_doctest_report(pytester, \"default\")\n        result.stdout.fnmatch_lines(\n            [\n                \"Expected:\",\n                \"       a  b\",\n                \"    0  1  4\",\n                \"    1  2  4\",\n                \"    2  3  6\",\n                \"Got:\",\n                \"       a  b\",\n                \"    0  1  4\",\n                \"    1  2  5\",\n                \"    2  3  6\",\n            ]\n        )\n", "def test_is_setup_py_handles_binary_files(tmp_path: Path) -> None:\n    setup_py = tmp_path.joinpath(\"setup.py\")\n    setup_py.write_bytes(b\"not_python_code\")\n    assert not _is_setup_py(setup_py)\n", "def test_is_setup_py_is_a_setup_py(tmp_path: Path, mod: str) -> None:\n    setup_py = tmp_path.joinpath(\"setup.py\")\n    setup_py.write_text(f'from {mod} import setup; setup(name=\"foo\")', \"utf-8\")\n    assert _is_setup_py(setup_py)\n", "    def test_doctest_report_no_differences(self, pytester: Pytester):\n        pytester.makepyfile(\n            \"\"\"\n                '''\n                >>> 1 + 1\n                2\n                '''\n                pass\n            \"\"\"\n        )\n        result = self._run_doctest_report(pytester, \"udiff\")\n        result.stdout.no_fnmatch_line(\"*\")\n"], "sample_455": ["    def test_expressions_with_include(self):\n        msg = (\n            \"UniqueConstraint.include cannot be used with expressions.\"\n        )\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                Lower(\"field\"),\n                name=\"test_func_include\",\n                include=[\"other_field\"],\n            )\n", "    def test_covering_index(self):\n        constraint = models.UniqueConstraint(\n            fields=[\"name\", \"color\"],\n            name=\"name_color_uniq_covering\",\n            include=[\"price\"],\n        )\n        self.assertTrue(constraint.covering)\n", "    def test_include_unique_constraint_with_condition(self):\n        UniqueConstraintIncludeCondition.objects.create(\n            name=\"p1\", shelf=\"front\"\n        )\n        with self.assertRaises(IntegrityError):\n            UniqueConstraintIncludeCondition.objects.create(\n                name=\"p1\", shelf=\"back\"\n            )\n\n", "    def test_condition_with_exclude(self):\n        constraint = UniqueConstraintConditionProduct._meta.constraints[0]\n        msg = \"Constraint \u201cname_without_color_uniq\u201d is violated.\"\n        with self.assertRaisesMessage(ValidationError, msg) as cm:\n            constraint.validate(\n                UniqueConstraintConditionProduct,\n                UniqueConstraintConditionProduct(name=self.p1.name, color=None),\n                exclude={\"name\"},\n            )\n        self.assertEqual(cm.exception.code, \"unique_together\")\n", "    def test_invalid_violation_error_code(self):\n        msg = \"UniqueConstraint.violation_error_code must be a string.\"\n        with self.assertRaisesMessage(ValueError, msg):\n            models.UniqueConstraint(\n                fields=[\"name\"],\n                name=\"name_invalid\",\n                violation_error_code=123,\n            )\n\n"], "sample_155": ["    def test_file_response_with_filename(self):\n        response = FileResponse(io.BytesIO(b'binary content'), filename='my_file.txt')\n        self.assertEqual(response['Content-Length'], '14')\n        self.assertEqual(response['Content-Type'], 'application/octet-stream')\n        self.assertEqual(response['Content-Disposition'], 'inline; filename=\"my_file.txt\"')\n        self.assertEqual(list(response), [b'binary content'])\n", "    def test_file_response_with_filename_override(self):\n        response = FileResponse(io.BytesIO(b'binary content'), filename='custom_filename.txt')\n        self.assertEqual(response['Content-Disposition'], 'attachment; filename=\"custom_filename.txt\"')\n        self.assertEqual(list(response), [b'binary content'])\n\n", "    def test_file_with_custom_filename(self):\n        with tempfile.NamedTemporaryFile() as tmp:\n            response = FileResponse(tmp, filename='test.txt')\n            self.assertEqual(response['Content-Disposition'], \"attachment; filename=\\\"test.txt\\\"\")\n", "    def test_file_response_with_filename_and_as_attachment(self):\n        response = FileResponse(io.BytesIO(b'binary content'), as_attachment=True, filename='my_file.txt')\n        self.assertEqual(response['Content-Length'], '14')\n        self.assertEqual(response['Content-Type'], 'application/octet-stream')\n        self.assertEqual(response['Content-Disposition'], 'attachment; filename=\"my_file.txt\"')\n        self.assertEqual(list(response), [b'binary content'])\n", "    def test_file_response_with_filename(self):\n        response = FileResponse(io.BytesIO(b'binary content'), filename='custom_name.txt')\n        self.assertEqual(response['Content-Disposition'], 'attachment; filename=\"custom_name.txt\"')\n        self.assertEqual(list(response), [b'binary content'])\n"], "sample_452": ["    def test_rename_references_model(self):\n        operation = RenameField(\"Model\", \"field\", \"new_field\")\n        self.assertIs(operation.references_model(\"Model\", \"migrations\"), True)\n        self.assertIs(operation.references_model(\"Other\", \"migrations\"), False)\n", "    def test_database_columns_ignore_swapped(self):\n        \"\"\"\n        Tests the database_columns property returns an empty list for swapped models.\n        \"\"\"\n        with override_settings(TEST_SWAP_MODEL=\"migrations.SomeFakeModel\"):\n            operation = migrations.CreateModel(\n                \"Pony\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"pink\", models.IntegerField(default=1)),\n                ],\n                options={\n                    \"swappable\": \"TEST_SWAP_MODEL\",\n                },\n            )\n            self.assertEqual(operation.database_columns, [])\n", "    def test_references_field_related_name(self):\n        operation = FieldOperation(\"Model\", \"field\", models.ForeignKey(\"Other\", models.CASCADE, related_name=\"related_field\"))\n        self.assertIs(operation.references_field(\"Model\", \"related_field\", \"migrations\"), True)\n        self.assertIs(operation.references_field(\"Other\", \"field\", \"migrations\"), True)\n        self.assertIs(operation.references_field(\"Missing\", \"whatever\", \"migrations\"), False)\n", "    def test_references_model_different_app_labels(self):\n        operation = FieldOperation(\n            \"app.Model\", \"field\", models.ForeignKey(\"other_app.Other\", models.CASCADE)\n        )\n        self.assertIs(operation.references_model(\"other_app.Other\", \"migrations\"), True)\n        self.assertIs(operation.references_model(\"app.Model\", \"migrations\"), True)\n        self.assertIs(operation.references_model(\"some.Other\", \"migrations\"), False)\n", "    def test_deconstruct_field_operation(self):\n        operation = FieldOperation(\n            \"MoDel\", \"field\", models.ForeignKey(\"Other\", models.CASCADE)\n        )\n        definition = operation.deconstruct()\n        self.assertEqual(definition[0], \"FieldOperation\")\n        self.assertEqual(definition[1], [])\n        self.assertEqual(\n            definition[2],\n            [\"model\", \"field\", models.ForeignKey(\"Other\", models.CASCADE)],\n        )\n"], "sample_466": ["    def test_serialize_complex_serializer(self):\n        self.assertSerializedEqual(ComplexSerializer(123))\n", "    def test_serialize_uuid_set(self):\n        uuids = {\n            uuid.UUID(\"5c859437-d061-4847-b3f7-e6b78852f8c8\"),\n            uuid.UUID(\"c7853ec1-2ea3-4359-b02d-b54e8f1bcee2\"),\n        }\n        self.assertSerializedResultEqual(\n            uuids,\n            (\n                \"{'5c859437-d061-4847-b3f7-e6b78852f8c8', 'c7853ec1-2ea3-4359-b02d-b54e8f1bcee2'}\",\n                {\"import uuid\"},\n            ),\n        )\n", "    def test_serialize_custom_field(self):\n        class CustomField(models.Field):\n            pass\n        field = CustomField()\n        self.assertSerializedEqual(field)\n", "    def test_serialize_class_with_bases(self):\n        class CustomModel(models.Model):\n            pass\n\n        class SubCustomModel(CustomModel):\n            pass\n\n        self.assertSerializedEqual(SubCustomModel)\n", "    def test_serialize_default_factory(self):\n            return \"dynamic value\"\n\n        class MyModel(models.Model):\n            field = models.CharField(default=my_factory)\n\n        field = MyModel._meta.get_field('field')\n        string = MigrationWriter.serialize(field)[0]\n        self.assertEqual(string, \"models.CharField(default=my_factory)\")\n"], "sample_140": ["    def test_sensitive_variables_with_kw_only_args(self):\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(sensitive_kwargs_only_function_caller)\n            self.verify_unsafe_email(sensitive_kwargs_only_function_caller)\n\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(sensitive_kwargs_only_function_caller, check_for_POST_params=False)\n            self.verify_safe_email(sensitive_kwargs_only_function_caller, check_for_POST_params=False)\n", "    def test_sensitive_variables_decorator_with_kwargs(self):\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(sensitive_kwargs_view, check_for_POST_params=False)\n\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(sensitive_kwargs_view, check_for_POST_params=False)\n", "    def test_sensitive_variables_with_nested_functions(self):\n            return sensitive_data\n        @sensitive_variables\n            sensitive_data = request.POST.get('password')\n            result = inner_function(sensitive_data)\n            return result\n\n        with self.settings(DEBUG=True):\n            request = self.rf.post('/some_url/', {'password': 'secretsauce'})\n            response = outer_function(request)\n            self.assertEqual(response, 'secretsauce')\n            self.assertLogs(\n                logger='django.views.debug',\n                level=logging.ERROR,\n                msg='Sensitive variable \"password\" accessed in view'\n            )\n\n        with self.settings(DEBUG=False):\n            request = self.rf.post('/some_url/', {'password': 'secretsauce'})\n            response = outer_function(request)\n            self.assertEqual(response, CLEANSED_SUBSTITUTE)\n\n", "    def test_sensitive_variables_and_sensitive_post_parameters(self):\n        with self.settings(DEBUG=True):\n            self.verify_safe_response(\n                both_sensitive_decorators_view, check_for_vars=False, check_for_POST_params=False\n            )\n", "    def test_sensitive_variables_with_args(self):\n            return render(request, 'some_template.html', {'arg1': arg1, 'arg2': arg2})\n\n        with self.settings(DEBUG=True):\n            response = self.client.get('/some_url/value1/value2', HTTP_X_REQUESTED_WITH='XMLHttpRequest')\n            self.assertEqual(response.status_code, 500)\n            self.assertIn('arg1 = value1', str(response.content))\n            self.assertIn('arg2 = value2', str(response.content))\n\n        with self.settings(DEBUG=False):\n            response = self.client.get('/some_url/value1/value2', HTTP_X_REQUESTED_WITH='XMLHttpRequest')\n            self.assertEqual(response.status_code, 500)\n            self.assertIn('arg1 = value1', str(response.content))\n            self.assertIn('arg2 = value2', str(response.content))\n"], "sample_623": ["    def test_request_chunks_incompatible_with_preferred_chunks(\n        self, shape, pref_chunks, chunks_to_fail", "    def test_no_preferred_chunks(self, shape, pref_chunks):\n        \"\"\"Do not modify chunks when no preferred chunks are provided.\"\"\"\n        initial = self.create_dataset(shape, pref_chunks)\n        chunks = {\"dim_0\": 3}\n        final = xr.open_dataset(\n            initial, engine=PassThroughBackendEntrypoint, chunks=chunks\n        )\n        self.check_dataset(initial, final, (3,))\n", "    def test_chunks_none(self, shape, pref_chunks, req_chunks):\n        \"\"\"Test that None in requested chunks uses preferred chunks.\"\"\"\n        initial = self.create_dataset(shape, pref_chunks)\n        final = xr.open_dataset(\n            initial,\n            engine=PassThroughBackendEntrypoint,\n            chunks=dict(zip(initial[self.var_name].dims, req_chunks)),\n        )\n        self.check_dataset(initial, final, explicit_chunks(pref_chunks, shape))\n", "    def test_no_chunks(self, shape):\n        \"\"\"Do not change the chunks when no chunks are provided.\"\"\"\n        initial = self.create_dataset(shape, (None,))\n        final = xr.open_dataset(\n            initial, engine=PassThroughBackendEntrypoint\n        )\n        self.check_dataset(initial, final, (None,))\n", "    def test_preferred_larger_than_shape(self, shape, pref_chunks, req_chunks, expected_chunks):\n        \"\"\"When preferred chunks are larger than the shape, use requested chunks.\"\"\"\n        initial = self.create_dataset(shape, pref_chunks)\n        final = xr.open_dataset(\n            initial,\n            engine=PassThroughBackendEntrypoint,\n            chunks=dict(zip(initial[self.var_name].dims, req_chunks)),\n        )\n        self.check_dataset(initial, final, explicit_chunks(expected_chunks, shape))\n\n"], "sample_320": ["    def test_references_field_by_related_name(self):\n        operation = FieldOperation(\n            \"Model\", \"field\", models.ForeignKey(\"Other\", models.CASCADE, related_name=\"related_things\")\n        )\n        self.assertIs(operation.references_field(\"Other\", \"related_things\", \"migrations\"), True)\n        self.assertIs(\n            operation.references_field(\"Other\", \"whatever\", \"migrations\"), False\n        )\n        self.assertIs(\n            operation.references_field(\"Missing\", \"whatever\", \"migrations\"), False\n        )\n\n", "    def test_references_field_many_to_many_field_names(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ManyToManyField(\"Other\", through=\"Through\", related_name=\"other_field\"),\n        )\n        self.assertIs(operation.references_field(\"Other\", \"other_field\", \"migrations\"), True)\n        self.assertIs(\n            operation.references_field(\"Through\", \"whatever\", \"migrations\"), True\n        )\n        self.assertIs(\n            operation.references_field(\"Missing\", \"whatever\", \"migrations\"), False\n        )\n", "    def test_deconstruct_with_constraints(self):\n        operation = migrations.AddField(\n            \"Model\", \"field\", models.CharField(max_length=100), unique=True\n        )\n        definition = operation.deconstruct()\n        self.assertEqual(\n            definition[2],\n            {\n                \"model_name\": \"Model\",\n                \"name\": \"field\",\n                \"field\": (\n                    \"django.db.models.fields.CharField\",\n                    {\"max_length\": 100},\n                ),\n                \"unique\": True,\n            },\n        )\n\n", "    def test_references_field_cascade(self):\n        operation = FieldOperation(\n            \"Model\",\n            \"field\",\n            models.ForeignKey(\n                \"Other\",\n                models.CASCADE,\n                on_delete=models.CASCADE,\n            ),\n        )\n        self.assertIs(operation.references_field(\"Other\", \"field\", \"migrations\"), True)\n        self.assertIs(\n            operation.references_field(\"Other\", \"whatever\", \"migrations\"), False\n        )\n        self.assertIs(\n            operation.references_field(\"Missing\", \"whatever\", \"migrations\"), False\n        )\n\n", "    def test_references_field_by_related_name(self):\n        operation = FieldOperation(\n            \"Model\", \"field\", models.ForeignKey(\"Other\", models.CASCADE, related_name=\"related\")\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"related\", \"migrations\"), True\n        )\n        self.assertIs(\n            operation.references_field(\"Other\", \"whatever\", \"migrations\"), False\n        )\n        self.assertIs(\n            operation.references_field(\"Missing\", \"whatever\", \"migrations\"), False\n        )\n"], "sample_693": ["    def test_do_cleanups_on_teardown_failure(pytester: PyTester) -> None:\n        testpath = pytester.makepyfile(\n            \"\"\"\n            import unittest\n        class MyTestCase(unittest.TestCase):\n            values = []\n                    self.values.append(1)\n                self.addCleanup(cleanup)\n                assert False\n                pass\n                pass\n            assert MyTestCase.values == [1, 1]", "    def test_do_cleanups_on_teardown_failure(pytester: Pytester) -> None:\n        testpath = pytester.makepyfile(\n            \"\"\"\n            import unittest\n            class MyTestCase(unittest.TestCase):\n                values = []\n                        self.values.append(1)\n                    self.addCleanup(cleanup)\n\n                    assert False\n                assert MyTestCase.values == [1]\n        \"\"\"\n        )\n        reprec = pytester.inline_run(testpath)\n        passed, skipped, failed = reprec.countoutcomes()\n        assert failed == 1\n        assert passed == 1\n", "    def test_do_cleanups_on_teardown_failure(pytester: Pytester) -> None:\n        testpath = pytester.makepyfile(\n            \"\"\"\n            import unittest\n            class MyTestCase(unittest.TestCase):\n                values = []\n                        self.values.append(1)\n                    self.addCleanup(cleanup)\n                    assert False\n                    pass\n                    pass\n                assert MyTestCase.values == [1, 1]\n        \"\"\"\n        )\n        reprec = pytester.inline_run(testpath)\n        passed, skipped, failed = reprec.countoutcomes()\n        assert failed == 2\n        assert passed == 1\n", "def test_do_cleanups_on_teardown_failure(pytester: Pytester) -> None:\n    testpath = pytester.makepyfile(\n        \"\"\"\n        import unittest\n        class MyTestCase(unittest.TestCase):\n            values = []\n                    self.values.append(1)\n                self.addCleanup(cleanup)\n                assert False\n                pass\n                pass\n            assert MyTestCase.values == [1, 1]\n    \"\"\"\n    )\n    reprec = pytester.inline_run(testpath)\n    passed, skipped, failed = reprec.countoutcomes()\n    assert failed == 2\n    assert passed == 1\n", "    def test_do_cleanups_on_teardown_failure(pytester: PyTester) -> None:\n        testpath = pytester.makepyfile(\n            \"\"\"\n            import unittest\n            class MyTestCase(unittest.TestCase):\n                values = []\n                        self.values.append(1)\n                    self.addCleanup(cleanup)\n                    pass\n                    assert False\n                assert MyTestCase.values == [1, 1]\n        \"\"\""], "sample_838": ["def test_column_transformer_remainder_with_sparse_output():\n    X = np.array([[0, 1, 2], [2, 4, 6]]).T\n    ct = make_column_transformer((StandardScaler(), [0, 1]),\n                                remainder=SparseMatrixTrans(),\n                                sparse_threshold=0.8)\n    X_trans = ct.fit_transform(X)\n    assert sparse.issparse(X_trans)\n\n\n", "compilation error", "def test_column_transformer_sparse_input():\n    X_sparse = sparse.csr_matrix([[0, 1, 2], [2, 4, 6], [8, 6, 4]]).T\n    ct = ColumnTransformer([('trans1', StandardScaler(), [0])],\n                           remainder='passthrough')\n\n    X_trans = ct.fit_transform(X_sparse)\n    assert sparse.issparse(X_trans)\n    assert X_trans.shape == (3, 3)\n    \n    ct = ColumnTransformer([('trans1', StandardScaler(), [0])],\n                           remainder=StandardScaler())\n    X_trans = ct.fit_transform(X_sparse)\n    assert sparse.issparse(X_trans)\n    assert X_trans.shape == (3, 3) \n", "compilation error", "compilation error"], "sample_797": ["    def test_power_transformer_sparse_input(self):\n        # Test PowerTransformer with sparse matrices input.\n\n        X = sparse.csr_matrix(X_2d)\n        pt = PowerTransformer()\n        X_trans = pt.fit_transform(X)\n        assert isinstance(X_trans, sparse.csr_matrix)\n        assert_array_almost_equal(pt.inverse_transform(X_trans), X.toarray())\n", "    def test_power_transformer_sparse_matrix(self):\n        # Test PowerTransformer with sparse matrices\n        X_sparse = sparse.csc_matrix(X_1col)\n        pt = PowerTransformer(method='yeo-johnson')\n        X_trans = pt.fit_transform(X_sparse)\n        assert isinstance(X_trans, sparse.csc_matrix)\n        assert (pt.inverse_transform(X_trans)\n                .toarray()\n                == X_sparse.toarray()).all()\n\n", "compilation error", "def test_power_transformer_sparse_input(method, standardize):\n    # Check that PowerTransformer works with sparse input data\n    X = X_2d\n    if method == 'box-cox':\n        X = np.abs(X)\n\n    X_sparse = sparse.csr_matrix(X)\n\n    pt = PowerTransformer(method, standardize)\n\n    X_trans = pt.fit_transform(X_sparse)\n    assert isinstance(X_trans, sparse.csr_matrix)\n\n    X_trans_dense = pt.transform(X_sparse.toarray())\n\n    assert_array_almost_equal(X_trans.toarray(), X_trans_dense) \n\n", "compilation error"], "sample_806": ["def test_gradient_boosting_oob_score_regression():\n    # Test out-of-bag score for regression\n\n    rng = np.random.RandomState(0)\n    X = rng.rand(100, 10)\n    y = rng.rand(100)\n\n    est = GradientBoostingRegressor(n_estimators=100, subsample=0.5,\n                                     random_state=0).fit(X, y)\n    score = est.score(X, y)\n    oob_score = est.oob_score_\n    assert oob_score is not None\n    assert 0 <= oob_score <= 1\n    assert abs(score - oob_score) < 0.1\n", "def test_gradient_boosting_warm_start_with_smaller_n_estimators():\n    # Test if warm start with smaller n_estimators continues training\n    X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)\n    est = GradientBoostingClassifier(n_estimators=10, max_depth=1,\n                                    warm_start=True, random_state=42)\n    est.fit(X, y)\n    initial_n_estimators = est.n_estimators\n    est.set_params(n_estimators=5)\n    est.fit(X, y)\n\n    assert est.n_estimators == initial_n_estimators\n", "    def test_gradient_boosting_multiclass_oob_predictions(self):\n        # Check that out-of-bag predictions work correctly for multiclass\n        # classification\n        X, y = make_classification(n_classes=3, random_state=0)\n        \n        clf = GradientBoostingClassifier(n_estimators=10, subsample=0.5,\n                                        oob_score=True, random_state=42)\n        clf.fit(X, y)\n        \n        # Check that oob_score_ is set\n        assert hasattr(clf, 'oob_score_')\n        \n        # Check that oob_predictions_ is set\n        assert hasattr(clf, 'oob_predictions_')\n        \n        # Check that oob_predictions_ has the correct shape\n        assert clf.oob_predictions_.shape == (len(y),)\n        \n        # Check that the number of unique predictions in oob_predictions_ \n        # is equal to the number of classes\n        assert len(np.unique(clf.oob_predictions_)) == 3\n", "def test_gradient_boosting_loss_kwargs():\n    # Check that GradientBoostingRegressor passes kwargs to the loss function.\n    # Regression test for #15143\n\n    X, y = make_regression()\n\n        return np.mean(np.square(y_true - y_pred) * kwargs['alpha'])\n\n    gb = GradientBoostingRegressor(loss=custom_loss, init=None)\n    gb.fit(X, y, alpha=2.0)\n\n    assert_allclose(gb.loss_(y, gb.predict(X), alpha=2.0),\n                    np.mean(np.square(y - gb.predict(X)) * 2.0))\n", "    def test_gradient_boosting_oob_score():\n        # Check that oob_score is calculated correctly.\n        X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)\n\n        for loss in ['ls', 'lad', 'huber', 'quantile']:\n            gb = GradientBoostingRegressor(loss=loss,\n                                           n_estimators=10, subsample=0.5,\n                                           random_state=0)\n            gb.fit(X, y)\n            assert gb.oob_score_ is not None\n\n        for loss in ['deviance', 'exponential']:\n            gb = GradientBoostingClassifier(loss=loss,\n                                            n_estimators=10, subsample=0.5,\n                                            random_state=0)\n            gb.fit(X, y)\n            assert gb.oob_score_ is not None\n"], "sample_503": ["def test_empty_line():\n    fig, ax = plt.subplots()\n    line = ax.plot([], [], \"r-\")[0]\n\n    assert line.get_data() == ([], [])\n\n    assert line.get_xdata() == []\n    assert line.get_ydata() == []\n\n    assert len(line.get_path().vertices) == 0\n", "def test_markersize_with_transform(fig_test, fig_ref):\n    ax_test = fig_test.add_subplot()\n    ax_ref = fig_ref.add_subplot()\n\n    t = mtransforms.Affine2D().scale(2).translate(5, 5)\n    ax_test.plot([0, 1], [0, 1], marker=\"o\", markersize=10, transform=t + ax_test.transData)\n    ax_ref.plot([0, 1], [0, 1], marker=\"o\", markersize=20)\n", "def test_marker_size_scaling(fig_test, fig_ref):\n    fig_test.subplots().plot([0, 1], marker='o', markersize=10, transform=fig_test.transFigure)\n    fig_ref.subplots().plot([0, 1], marker='o', markersize=10 * fig_ref.dpi / 72, transform=fig_ref.transFigure)\n", "def test_line_collection_colors():\n    fig, ax = plt.subplots()\n    x = np.arange(10)\n    y = np.arange(10)\n    colors = ['r', 'g', 'b'] * 4\n\n    # Create a LineCollection with specific colors for each line\n    lc = mlines.LineCollection(\n        [list(zip(x, y))]*12,\n        colors=colors,\n    )\n    ax.add_collection(lc)\n    ax.set_xlim(0, 9)\n    ax.set_ylim(0, 9)\n\n\n\n", "def test_line_collection(fig_test, fig_ref):\n    fig_test.subplots().add_collection(\n        mlines.LineCollection([[0, 1], [2, 3]], colors=['r', 'b'], linestyles=['-', '--'])\n    )\n    fig_ref.subplots().plot([0, 1], 'r-', [2, 3], 'b--')\n\n"], "sample_224": ["    def test_aggregation_subquery_annotation_ordering(self):\n        books_qs = Book.objects.annotate(\n            first_author_title=Subquery(\n                Author.objects.filter(pk=OuterRef('contact_id')).values('first_name')[:1],\n            )\n        ).order_by('first_author_title')\n        self.assertEqual(list(books_qs.values_list('first_author_title', flat=True)), [\n            'Adrian', 'Jeffrey', 'Jacob', 'Peter', 'Paul', 'Stuart', 'Wesley', 'Brad', 'James',\n        ])\n", "    def test_aggregation_subquery_annotation_aggregation(self):\n        books_qs = Book.objects.filter(\n            publisher=OuterRef('pk'),\n        ).values('contact__name').annotate(\n            avg_price=Avg('price'),\n        ).order_by(\n            'contact__name'\n        ).values('contact__name', 'avg_price')\n        publisher_qs = Publisher.objects.annotate(\n            avg_price=Subquery(books_qs),\n        ).values('name', 'avg_price')\n        self.assertEqual(list(publisher_qs), [\n            {'name': 'Apress', 'avg_price': Decimal('23.09')},\n            {'name': 'Jonno\\'s House of Books', 'avg_price': Decimal('82.80')},\n            {'name': 'Morgan Kaufmann', 'avg_price': Decimal('75.00')},\n            {'name': 'Prentice Hall', 'avg_price': Decimal('29.69')},\n            {'name': 'Sams', 'avg_price': Decimal('30.00')},\n        ])\n", "    def test_annotation_aggregate_count_star(self):\n        # Ensure that Count('*') can be used within an annotation\n        qs = Book.objects.annotate(\n            total_books=Count('*'),\n        ).filter(total_books__gt=1)\n        self.assertEqual(list(qs.values_list('pk', flat=True)), [\n            self.b1.pk,\n            self.b2.pk,\n            self.b3.pk,\n            self.b4.pk,\n            self.b5.pk,\n            self.b6.pk\n        ])\n", "    def test_aggregation_subquery_annotation_multivalued_same_alias(self):\n        \"\"\"\n        Subquery annotations must be properly aliased when used in the GROUP BY,\n        even if the alias is the same as a field in the outer query.\n        \"\"\"\n        subquery_qs = Book.objects.filter(\n            publisher=OuterRef('pk'),\n            price=Decimal('29.69'),\n        ).values('rating')\n        publisher_qs = Publisher.objects.filter(\n            book__contact__age__gt=20,\n            name=self.p1.name,\n        ).annotate(\n            rating=Subquery(subquery_qs),\n            contacts_count=Count('book__contact'),\n        ).values('rating', 'contacts_count').annotate(total_count=Count('rating'))\n        self.assertEqual(list(publisher_qs), [\n            {'rating': 4.0, 'contacts_count': 1, 'total_count': 2},\n        ])\n", "    def test_group_by_subquery_annotation_with_joins(self):\n        \"\"\"\n        Subquery annotations with joins are included in the GROUP BY if they\n        are grouped against.\n        \"\"\"\n        subquery_qs = Author.objects.filter(\n            book__publisher=OuterRef('pk'),\n            book__rating__gt=4,\n        ).annotate(\n            avg_age=Avg('age')\n        ).values('avg_age')\n        publisher_qs = Publisher.objects.annotate(\n            avg_author_age=Subquery(subquery_qs),\n        ).values('avg_author_age').annotate(total=Count('*')).order_by('avg_author_age')\n        self.assertEqual(dict(publisher_qs), {Decimal(\"36.5\"): 1, Decimal(\"39.5\"): 1, Decimal(\"41.0\"): 1, None: 2})\n"], "sample_601": ["def test_cftime_round_accessor(cftime_rounding_dataarray, cftime_date_type, use_dask):\n    import dask.array as da\n\n    freq = \"D\"\n    expected = xr.DataArray(\n        [\n            [cftime_date_type(1, 1, 1, 0), cftime_date_type(1, 1, 1, 0)],\n            [cftime_date_type(1, 1, 1, 0), cftime_date_type(1, 1, 2, 0)],\n        ],\n        name=\"round\",\n    )\n\n    if use_dask:\n        chunks = {\"dim_0\": 1}\n        # Currently a compute is done to inspect a single value of the array\n        # if it is of object dtype to check if it is a cftime.datetime (if not\n        # we raise an error when using the dt accessor).\n        with raise_if_dask_computes(max_computes=1):\n            result = cftime_rounding_dataarray.chunk(chunks).dt.round(freq)\n        expected = expected.chunk(chunks)\n        assert isinstance(result.data, da.Array)\n        assert result.chunks == expected.chunks\n    else:\n        result = cftime_rounding_dataarray.dt.round(freq)\n\n    assert_identical(result, expected)\n", "def test_cftime_accessor_method(cftime_rounding_dataarray, method, parameters):\n    dates = cftime_rounding_dataarray\n    expected = getattr(xr.coding.cftimeindex.CFTimeIndex(dates.values), method)(parameters)\n    actual = getattr(dates.dt, method)(parameters)\n    assert_array_equal(expected, actual)\n", "def test_cftime_accessor_method(cftime_rounding_dataarray, method, parameters):\n    dates = cftime_rounding_dataarray\n    expected = getattr(dates.to_datetimeindex(), method)(parameters)\n\n    result = getattr(dates.dt, method)(parameters)\n\n    assert_array_equal(expected.values, result.values)\n", "def test_cftime_round_accessor(cftime_rounding_dataarray, cftime_date_type, freq):\n    expected = xr.DataArray(\n        [\n            [cftime_date_type(1, 1, 1, 12), cftime_date_type(1, 1, 1, 12)],\n            [cftime_date_type(1, 1, 1, 12), cftime_date_type(1, 1, 2, 12)],\n        ],\n        name=\"round\",\n    )\n    result = cftime_rounding_dataarray.dt.round(freq)\n    assert_identical(result, expected)\n", "    def test_cftime_round_accessor(cftime_rounding_dataarray, cftime_date_type, freq):\n        expected = xr.DataArray(\n            [\n                [\n                    cftime_date_type(1, 1, 1, 12),\n                    cftime_date_type(1, 1, 2, 0),\n                ],\n                [\n                    cftime_date_type(1, 1, 2, 0),\n                    cftime_date_type(1, 1, 2, 0),\n                ],\n            ],\n            name=\"round\",\n        )\n        result = cftime_rounding_dataarray.dt.round(freq)\n        assert_identical(result, expected)\n\n"], "sample_254": ["    def test_inline_formset_error_message(self):\n        self.admin_login(username='super', password='secret')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_inlines_holder5_add'))\n        self.wait_until_visible('#id_dummy')\n        self.selenium.find_element_by_id('id_dummy').send_keys(1)\n        fields = ['id_inner5stacked_set-0-dummy', 'id_inner5tabular_set-0-dummy']\n        show_links = self.selenium.find_elements_by_link_text('SHOW')\n        for show_index, field_name in enumerate(fields):\n            show_links[show_index].click()\n            self.wait_until_visible('#' + field_name)\n            self.selenium.find_element_by_id(field_name).send_keys(1)\n\n        self.selenium.find_element_by_xpath('//input[@value=\"Save\"]').click()\n        error_messages = self.selenium.find_elements_by_css_selector('.errorlist li')\n        self.assertEqual(len(error_messages), 2)\n        self.assertEqual(error_messages[0].text, 'This field is required.')\n        self.assertEqual(error_messages[1].text, 'This field is required.')\n", "    def test_inline_formset_empty_initial_data(self):\n        self.admin_login(username='super', password='secret')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_inlines_holder6_add'))\n        self.wait_until_visible('#id_dummy')\n        fields = ['id_inner6stacked_set-0-dummy', 'id_inner6tabular_set-0-dummy']\n\n        for field_name in fields:\n            self.wait_until_visible('#' + field_name)\n            self.assertEqual(self.selenium.find_element_by_id(field_name).get_attribute('value'), '')\n        self.selenium.find_element_by_xpath('//input[@value=\"Save\"]').click()\n        for field_name in fields:\n            self.wait_until_visible('#' + field_name)\n            self.assertEqual(self.selenium.find_element_by_id(field_name).get_attribute('value'), '')\n\n", "    def test_inline_formset_error_empty_input(self):\n        self.admin_login(username='super', password='secret')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_inlines_holder5_add'))\n        self.wait_until_visible('#id_dummy')\n        self.selenium.find_element_by_id('id_dummy').send_keys(1)\n        fields = ['id_inner5stacked_set-0-dummy', 'id_inner5tabular_set-0-dummy']\n        show_links = self.selenium.find_elements_by_link_text('SHOW')\n        for show_index, field_name in enumerate(fields):\n            show_links[show_index].click()\n            self.wait_until_visible('#' + field_name)\n        self.selenium.find_element_by_xpath('//input[@value=\"Save\"]').click()\n        # Test the red border around inputs by css selectors\n        stacked_selectors = ['.errors input', '.errors select', '.errors textarea']\n        for selector in stacked_selectors:\n            self.assertBorder(\n                self.selenium.find_element_by_css_selector(selector),\n                '1px solid #ba2121',\n            )\n        tabular_selectors = [\n            'td ul.errorlist + input', 'td ul.errorlist + select', 'td ul.errorlist + textarea'\n        ]\n        for selector in tabular_selectors:\n            self.assertBorder(\n                self.selenium.find_element_by_css_selector(selector),\n                '1px solid #ba2121',\n            )\n", "    def test_inline_formset_error_empty_formset(self):\n        self.admin_login(username='super', password='secret')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_inlines_emptyformset_add'))\n        self.selenium.find_element_by_xpath('//input[@value=\"Save\"]').click()\n        self.assertEqual(len(self.selenium.find_elements_by_css_selector(\".errorlist\")), 1)\n\n", "    def test_inline_formset_error_message(self):\n        self.admin_login(username='super', password='secret')\n        self.selenium.get(self.live_server_url + reverse('admin:admin_inlines_holder5_add'))\n        self.wait_until_visible('#id_dummy')\n        self.selenium.find_element_by_id('id_dummy').send_keys(1)\n        fields = ['id_inner5stacked_set-0-dummy', 'id_inner5tabular_set-0-dummy']\n        show_links = self.selenium.find_elements_by_link_text('SHOW')\n        for show_index, field_name in enumerate(fields):\n            show_links[show_index].click()\n            self.wait_until_visible('#' + field_name)\n            self.selenium.find_element_by_id(field_name).send_keys(1)\n        self.selenium.find_element_by_xpath('//input[@value=\"Save\"]').click()\n        \n        error_message = self.selenium.find_element_by_css_selector('#inner5stacked_set-group .errorlist li').text\n        expected_message = 'This field is required.'\n        self.assertEqual(error_message, expected_message)\n        \n        error_message = self.selenium.find_element_by_css_selector('#inner5tabular_set-group .errorlist li').text\n        self.assertEqual(error_message, expected_message)\n"], "sample_873": ["def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n    assert_array_equal(feature_names_t, sel.get_feature_names_out())\n    assert_array_equal(\n        feature_names_t, sel.get_feature_names_out(input_features=feature_names)\n    )\n    # Test with wrong shape input_features\n    with pytest.raises(ValueError):\n        sel.get_feature_names_out(input_features=[1, 2, 3])\n\n\n\n", "def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n\n    names_out = sel.get_feature_names_out(input_features=feature_names)\n    assert_array_equal(feature_names_t, names_out)\n\n    names_out = sel.get_feature_names_out()\n    assert_array_equal(feature_names_t, names_out)\n\n    with pytest.raises(ValueError, match=\"Input features are missing\"):\n        sel.get_feature_names_out()\n", "def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n    assert_array_equal(feature_names_t, sel.get_feature_names_out(feature_names))\n    assert_array_equal(feature_names_t, sel.get_feature_names_out())\n    assert_array_equal([], sel.get_feature_names_out(None))\n", "def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n    feature_names_t = sel.get_feature_names_out(feature_names)\n\n    assert_array_equal(feature_names_t, feature_names_inv)\n", "def test_get_feature_names_out():\n    sel = StepSelector()\n    sel.fit(X, y)\n\n    assert_array_equal(feature_names_t, sel.get_feature_names_out(feature_names))\n    assert_array_equal(\n        feature_names_t, sel.get_feature_names_out(input_features=feature_names)\n    )\n    assert_array_equal(\n        feature_names_t, sel.get_feature_names_out(input_features=np.array(feature_names))\n    )\n\n    with pytest.raises(ValueError):\n        sel.get_feature_names_out(input_features=[\"a\", \"b\", \"c\"])\n\n    sel.fit(X, y)\n    assert_array_equal(sel.get_feature_names_out(), feature_names_t)\n\n    # Test with feature_names_in_ being None\n    sel.fit(X, y)\n    sel._feature_names_in = None\n    assert_array_equal(\n        sel.get_feature_names_out(), np.array([\"x0\", \"x2\", \"x4\", \"x6\", \"x8\"])\n    )\n\n\n\n"], "sample_1131": ["def test_loggamma():\n    from sympy import loggamma\n\n    expr = loggamma(x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'scipy.special.loggamma(x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == 'math.lgamma(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == 'math.lgamma(x)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr) == 'mpmath.loggamma(x)'\n", "compilation error", "def test_loggamma():\n    from sympy import loggamma\n\n    expr = loggamma(x)\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(expr) == 'scipy.special.loggamma(x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(expr) == 'math.lgamma(x)'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(expr) == 'math.lgamma(x)'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(expr) == 'mpmath.loggamma(x)'\n", "compilation error", "def test_cosm1():\n    from sympy import cosm1\n\n    prntr = SciPyPrinter()\n    assert prntr.doprint(cosm1(x)) == 'scipy.special.cosm1(x)'\n\n    prntr = NumPyPrinter()\n    assert prntr.doprint(cosm1(x)) == 'numpy.cos(x) - 1'\n\n    prntr = PythonCodePrinter()\n    assert prntr.doprint(cosm1(x)) == 'math.cos(x) - 1'\n\n    prntr = MpmathPrinter()\n    assert prntr.doprint(cosm1(x)) == 'mpmath.cos(x) - 1'\n"], "sample_425": ["    def test_serialize_custom_field(self):\n        class CustomField(models.Field):\n                super().__init__(*args, **kwargs)\n                self.custom_attr = 'value'\n\n                return 'migrations.test_writer.CustomField', [], {'custom_attr': self.custom_attr}\n\n        custom_field = CustomField(custom_attr='custom_value')\n        serialized = MigrationWriter.serialize(custom_field)\n        self.assertEqual(serialized[0], \"migrations.test_writer.CustomField(custom_attr='custom_value')\")\n        self.assertEqual(serialized[1], {\"from django.db import models\"})\n\n", "    def test_serialize_nested_class_instances(self):\n        class Outer:\n            class Inner:\n                pass\n\n        outer = Outer()\n        outer.inner = Outer.Inner()\n        self.assertSerializedEqual(outer.inner)\n        self.assertSerializedResultEqual(\n            outer.inner,\n            (\n                \"migrations.test_writer.Outer.Inner()\",\n                {\"import migrations.test_writer\"},\n            ),\n        )\n\n", "    def test_serialize_decimal(self):\n        self.assertSerializedEqual(Decimal(\"1.23\"))\n        self.assertSerializedEqual(Decimal(\"0\"))\n        self.assertSerializedEqual(Decimal(\"-1.23\"))\n", "    def test_serialize_custom_fields(self):\n        class MyCustomField(models.Field):\n                super().__init__(*args, **kwargs)\n\n        class MyModel(models.Model):\n            my_field = MyCustomField()\n\n        migration = migrations.Migration(\"0001_initial\", \"testapp\")\n        writer = MigrationWriter(migration)\n        writer.add_model(MyModel)\n        output = writer.as_string()\n        self.assertIn('MyCustomField', output)\n", "    def test_serialize_foreign_key(self):\n        class OtherModel(models.Model):\n            pass\n\n        self.assertSerializedEqual(\n            models.ForeignKey(OtherModel, on_delete=models.CASCADE)\n        )\n"], "sample_1190": ["compilation error", "compilation error", "compilation error", "compilation error", "def test_issue_23994():\n    x = Symbol('x')\n    # Test dimension handling in the presence of symbols\n    assert SI._collect_factor_and_dimension(x*meter) == (x, length)\n    assert SI._collect_factor_and_dimension(meter/x) == (1/x, 1/length)\n"], "sample_1071": ["def test_check_dimensions():\n    raises(ValueError, lambda: check_dimensions(meter + kilogram))\n    raises(ValueError, lambda: check_dimensions(meter + 5))\n    assert check_dimensions(meter) == meter\n    assert check_dimensions(2*meter) == 2*meter\n    assert check_dimensions(meter + meter) == 2*meter\n    assert check_dimensions(meter + 2*meter) == 3*meter\n    assert check_dimensions(meter + 2) == meter + 2\n", "compilation error", "def test_convert_to_units_with_symbols():\n    x = symbols('x')\n    assert convert_to(x * kilometer, meter) == x * 1000 * meter\n    assert convert_to(x * kilometer / hour, meter / second) == x * 1000 * meter / (3600 * second)\n    assert convert_to(x * kilogram * meter / second**2, newton) == x * newton\n", "def test_check_dimensions():\n    assert check_dimensions(meter + kilometer) == meter + kilometer\n    assert check_dimensions(meter + 2*kilometer) == meter + 2*kilometer\n    raises(ValueError, lambda: check_dimensions(meter + second))\n    raises(ValueError, lambda: check_dimensions(2 * meter + second))\n    # Check that constants are handled correctly:\n    assert check_dimensions(3*meter + 4*kilometer) == 7*meter\n    assert check_dimensions(2 * meter + 3*meter) == 5*meter\n    # Test that the function correctly handles nested additions\n    assert check_dimensions((meter + kilometer) + kilometer) == 2 * kilometer + meter\n    assert check_dimensions(meter + (kilometer + meter)) == 2 * kilometer + meter\n    assert check_dimensions((meter + kilometer) + (2*meter + kilometer)) == 4*meter + 2*kilometer\n\n", "compilation error"], "sample_583": ["compilation error", "    def test_create_mask_out_of_bounds():\n        indexer = indexing.OuterIndexer((np.array([0, 5]),))\n        with pytest.raises(IndexError):\n            indexing.create_mask(indexer, (3,))\n\n        indexer = indexing.VectorizedIndexer((np.array([0, 5]),))\n        with pytest.raises(IndexError):\n            indexing.create_mask(indexer, (3,))\n", "compilation error", "def test_outer_indexer_consistency_with_broadcast_indexes_basic():\n        if isinstance(x, np.ndarray) and x.dtype.kind == 'b':\n            x = x.nonzero()[0]\n        return x\n\n    original = np.random.rand(10, 20, 30)\n    v = Variable(['i', 'j', 'k'], original)\n    I = ReturnItem()  # noqa: E741  # allow ambiguous name\n    # test orthogonally applied indexers\n    indexers = [I[:], 0, -2, I[:3], np.array([0, 1, 2, 3]), np.array([0]),\n                np.arange(10) < 5]\n    for i, j, k in itertools.product(indexers, repeat=3):\n        if isinstance(j, np.ndarray) and j.dtype.kind == 'b':\n            j = np.arange(20) < 4\n        if isinstance(k, np.ndarray) and k.dtype.kind == 'b':\n            k = np.arange(30) < 8\n        _, expected, new_order = v._broadcast_indexes_basic((i, j, k))\n        expected_data = nputils.NumpyVIndexAdapter(v.data)[expected.tuple]\n        if new_order:\n            old_order = range(len(new_order))\n            expected_data = np.moveaxis(expected_data, old_order,\n                                      new_order)\n\n        outer_index = indexing.OuterIndexer((nonzero(i), nonzero(j),\n                                         nonzero(k)))\n        actual = indexing._outer_to_numpy_indexer(outer_index, v.shape)\n        actual_data = v.data[actual]\n        np.testing.", "compilation error"], "sample_911": ["    def test_definition_parser_templates_qualified_name(self):\n        check('class', 'template <typename T> class A {', {1: 'A', 2: '1A'})\n        check('member', 'template <typename T> A::A(T t) : t_(t) {}', {1: 'A__A', 2: '1A'})\n        check('function', 'template <typename T> void A::f(T t) { }', {1: 'f', 2: '1fv'})\n        check('class', 'template <typename T> class A<T>::B {', {1: 'B', 2: '1B'})\n        check('member', 'template <typename T> A<T>::B::B(T t) : t_(t) {}', {1: 'B__B', 2: '1B'})\n        check('function', 'template <typename T> void A<T>::B::f(T t) { }', {1: 'f', 2: '1fv'})\n", "    def check_link_target(target_type, role, text, file):\n        pattern = r'<a .*? href=\"{}/(?P<target>{})\\.html\".*?>[\\w\\s]+</a>'.format(role, target_type)\n        res = re.search(pattern, text)\n        if not res:\n            print(f\"Pattern\\n\\t{pattern}\\nnot found in {file}\")\n            assert False\n        return res.group('target')\n", "def test_build_domain_cpp_backslash_in_param_type(app, status, warning):\n    app.builder.build_all()\n    ws = filter_warnings(warning, \"backslash-param\")\n    assert len(ws) == 0\n\n", "    def check_target_existence(target, file):\n        pattern = '<p><a .*?href=\"#id{}\"\\s*'.format(target)\n        result = re.search(pattern, output)\n        expect = '''\\", "compilation error"], "sample_1109": ["compilation error", "def test_issue_19866():\n    assert floor(x + E) == floor(x) + floor(E)\n    assert ceiling(x + E) == ceiling(x) + ceiling(E)\n", "compilation error", "compilation error", "compilation error"], "sample_545": ["compilation error", "compilation error", "def test_figure_remove_axes():\n    fig, ax = plt.subplots()\n    fig.remove_axes(ax)\n    assert len(fig.axes) == 0\n\n", "compilation error", "compilation error"]}, "raw_scored_generations": {"sample_4": [false, false, false, false, false], "sample_5": [false, false, false, false, false], "sample_3": [true, false, true, false, false], "sample_0": [false, false, false, false, false], "sample_2": [true, true, true, true, false], "sample_1": [false, false, false, false, false], "sample_6": [false, false, true, false, true], "sample_7": [true, true, true, true, false], "sample_8": [false, true, false, false, false], "sample_9": [false, false, false, false, false], "sample_10": [true, false, true, false, false], "sample_11": [true, false, false, false, true], "sample_12": [true, true, true, true, false], "sample_13": [false, true, false, true, false], "sample_14": [false, false, false, true, true], "sample_15": [true, true, true, true, true], "sample_16": [false, false, false, false, true], "sample_17": [false, false, false, false, false], "sample_18": [false, false, true, false, false], "sample_19": [false, false, true, false, true], "sample_20": [false, true, true, true, true], "sample_21": [true, false, true, false, false], "sample_22": [false, false, true, false, false], "sample_23": [true, true, false, false, false], "sample_24": [false, false, false, false, false], "sample_25": [false, true, true, true, true], "sample_26": [true, true, true, true, true], "sample_27": [true, false, true, true, true], "sample_28": [true, true, true, true, true], "sample_29": [false, false, false, false, false], "sample_30": [true, true, false, false, false], "sample_31": [false, false, false, false, false], "sample_32": [false, true, false, false, false], "sample_33": [false, false, false, false, false], "sample_34": [false, false, false, false, false], "sample_35": [false, false, false, false, false], "sample_36": [false, false, false, false, false], "sample_37": [true, true, false, true, false], "sample_38": [true, false, false, true, true], "sample_39": [false, true, false, false, true], "sample_40": [true, false, false, true, false], "sample_41": [true, false, false, false, false], "sample_42": [false, false, false, false, false], "sample_43": [false, false, true, true, true], "sample_44": [false, true, false, false, false], "sample_45": [true, false, false, false, false], "sample_46": [true, false, false, false, false], "sample_47": [false, false, true, false, false], "sample_48": [false, false, false, false, false], "sample_49": [false, false, true, false, false], "sample_50": [true, true, true, true, true], "sample_51": [true, false, false, false, false], "sample_52": [false, false, false, false, false], "sample_54": [true, true, false, false, false], "sample_53": [false, false, false, true, true], "sample_55": [false, false, false, true, false], "sample_58": [true, false, false, true, true], "sample_56": [false, false, false, true, true], "sample_57": [false, false, false, true, false], "sample_59": [false, false, false, true, false], "sample_60": [false, false, false, false, false], "sample_61": [true, true, true, true, true], "sample_62": [false, false, false, false, false], "sample_63": [false, false, false, false, false], "sample_64": [true, false, false, true, true], "sample_65": [true, true, true, true, true], "sample_67": [false, true, false, false, false], "sample_66": [false, false, false, false, false], "sample_68": [false, false, false, false, false], "sample_69": [false, false, false, false, false], "sample_70": [false, false, true, false, false], "sample_71": [true, false, false, true, false], "sample_72": [false, false, false, false, false], "sample_73": [true, false, false, false, false], "sample_75": [true, false, false, false, false], "sample_74": [false, false, true, false, true], "sample_76": [true, true, true, false, true], "sample_77": [false, false, false, false, false], "sample_78": [false, false, true, false, false], "sample_79": [false, false, false, false, false], "sample_80": [true, false, false, true, false], "sample_82": [false, false, true, false, false], "sample_81": [false, true, false, false, false], "sample_83": [false, false, false, false, true], "sample_85": [false, false, false, false, false], "sample_84": [true, true, false, true, true], "sample_86": [false, false, false, false, false], "sample_88": [false, false, true, false, false], "sample_87": [false, false, false, false, false], "sample_89": [false, false, false, false, false], "sample_90": [false, false, false, false, false], "sample_91": [false, false, false, false, true], "sample_92": [false, false, false, false, true], "sample_93": [false, false, false, false, false], "sample_94": [false, false, false, false, true], "sample_95": [false, false, false, false, false], "sample_98": [false, false, false, false, false], "sample_96": [false, false, false, false, false], "sample_99": [false, false, false, false, false], "sample_97": [false, true, false, false, false], "sample_100": [false, false, false, false, false], "sample_102": [true, true, true, false, true], "sample_101": [false, false, false, false, false], "sample_103": [false, false, false, false, false], "sample_104": [false, false, false, false, false], "sample_107": [true, false, false, false, false], "sample_106": [false, false, false, false, false], "sample_105": [false, false, false, false, false], "sample_108": [false, false, false, false, false], "sample_109": [true, true, false, true, true], "sample_111": [true, true, true, true, true], "sample_110": [true, false, false, false, false], "sample_112": [false, false, false, false, false], "sample_113": [false, false, false, false, false], "sample_114": [false, false, false, false, false], "sample_115": [false, false, false, false, false], "sample_116": [false, false, false, false, false], "sample_117": [false, true, false, false, false], "sample_118": [false, false, false, false, false], "sample_119": [false, false, true, false, false], "sample_120": [false, false, false, false, false], "sample_121": [true, false, false, false, false], "sample_122": [false, false, false, false, false], "sample_123": [false, false, true, false, true], "sample_124": [true, true, true, true, true], "sample_125": [true, true, false, false, true], "sample_126": [false, false, false, false, false], "sample_127": [false, false, true, false, false], "sample_128": [true, true, true, true, true], "sample_129": [true, false, true, true, true], "sample_130": [false, false, true, true, true], "sample_131": [false, false, false, false, false], "sample_132": [false, false, false, false, false], "sample_133": [true, true, true, true, true], "sample_135": [false, true, false, false, false], "sample_134": [false, false, false, false, false], "sample_136": [false, false, false, false, false], "sample_139": [true, true, false, true, true], "sample_137": [false, false, false, false, false], "sample_138": [false, false, false, false, false], "sample_140": [false, false, false, false, false], "sample_141": [false, false, false, true, false], "sample_142": [true, true, false, true, false], "sample_143": [false, false, false, false, false], "sample_144": [false, true, false, true, true], "sample_145": [false, false, false, false, false], "sample_146": [true, true, true, true, true], "sample_147": [true, false, false, true, false], "sample_148": [false, true, false, false, false], "sample_151": [false, false, false, false, false], "sample_149": [true, false, false, true, false], "sample_152": [true, false, true, false, false], "sample_150": [false, false, false, false, false], "sample_153": [false, false, false, false, false], "sample_154": [false, false, false, false, false], "sample_155": [false, false, false, false, false], "sample_156": [false, true, false, true, false], "sample_157": [false, false, false, true, false], "sample_158": [true, true, false, false, false], "sample_159": [false, false, false, true, true], "sample_160": [false, false, true, false, false], "sample_161": [false, true, false, false, false], "sample_162": [true, true, true, true, true], "sample_163": [false, false, true, false, true], "sample_164": [false, true, false, false, false], "sample_165": [false, false, false, false, false], "sample_166": [true, true, true, true, true], "sample_167": [false, false, false, false, true], "sample_168": [false, false, true, false, false], "sample_169": [true, false, false, false, false], "sample_171": [true, true, true, true, true], "sample_170": [false, false, false, false, false], "sample_172": [false, true, true, true, true], "sample_173": [false, false, false, false, false], "sample_174": [false, false, false, false, false], "sample_175": [false, false, false, false, false], "sample_176": [true, false, false, false, false], "sample_177": [false, false, true, false, true], "sample_178": [false, false, false, false, false], "sample_180": [false, false, false, false, false], "sample_179": [false, false, false, false, false], "sample_182": [true, false, false, false, false], "sample_181": [false, false, false, false, false], "sample_183": [false, false, false, false, false], "sample_184": [false, false, false, false, false], "sample_185": [true, false, false, false, false], "sample_186": [true, false, false, false, true], "sample_187": [true, false, false, false, false], "sample_188": [false, false, false, false, false], "sample_189": [false, false, false, false, false], "sample_190": [false, false, true, true, true], "sample_191": [true, false, false, false, false], "sample_192": [false, true, false, false, true], "sample_193": [false, true, false, true, true], "sample_194": [false, false, false, false, false], "sample_195": [false, false, false, false, false], "sample_196": [false, false, false, false, false], "sample_198": [false, false, false, true, false], "sample_197": [true, true, false, true, false], "sample_199": [false, false, false, true, false], "sample_200": [false, false, false, false, false], "sample_201": [false, false, true, true, true], "sample_202": [true, true, true, false, true], "sample_203": [false, false, false, false, false], "sample_204": [false, false, false, false, false], "sample_205": [false, false, false, false, false], "sample_206": [false, false, false, false, false], "sample_207": [false, false, false, true, false], "sample_208": [false, false, false, false, false], "sample_209": [false, true, false, false, false], "sample_210": [false, false, false, false, false], "sample_211": [false, false, false, false, false], "sample_213": [false, false, true, false, true], "sample_212": [false, false, false, false, false], "sample_214": [true, false, false, false, false], "sample_215": [false, false, false, false, false], "sample_216": [false, true, false, false, true], "sample_217": [false, false, false, false, true], "sample_218": [false, false, false, false, false], "sample_219": [true, false, false, true, false], "sample_220": [true, false, false, true, true], "sample_221": [true, false, true, true, false], "sample_222": [true, false, true, true, true], "sample_223": [false, false, false, false, true], "sample_224": [false, false, false, false, false], "sample_225": [true, true, true, true, true], "sample_226": [false, false, false, false, false], "sample_227": [false, false, false, false, false], "sample_228": [true, false, true, false, true], "sample_229": [false, false, true, false, true], "sample_230": [true, false, false, false, true], "sample_231": [false, false, false, false, false], "sample_232": [false, false, true, false, false], "sample_233": [false, true, true, true, true], "sample_234": [false, false, true, false, false], "sample_235": [false, true, false, false, false], "sample_236": [false, false, false, true, false], "sample_237": [true, true, false, true, false], "sample_238": [true, false, false, false, true], "sample_239": [false, true, false, true, true], "sample_240": [true, true, true, true, false], "sample_241": [true, false, false, false, true], "sample_242": [false, false, false, false, false], "sample_243": [false, true, false, false, false], "sample_244": [false, true, true, false, true], "sample_245": [true, true, true, true, true], "sample_246": [true, true, true, true, true], "sample_247": [true, false, false, true, true], "sample_248": [false, false, false, false, false], "sample_249": [true, false, false, false, false], "sample_250": [true, false, false, false, false], "sample_251": [false, true, true, false, false], "sample_252": [false, false, true, false, false], "sample_253": [false, false, false, false, true], "sample_254": [true, true, true, true, true], "sample_256": [true, false, false, false, true], "sample_255": [false, false, false, false, false], "sample_257": [false, false, false, true, false], "sample_258": [false, false, false, false, false], "sample_259": [false, false, false, true, false], "sample_260": [true, true, true, false, false], "sample_261": [false, false, true, false, true], "sample_262": [false, false, false, false, false], "sample_263": [false, false, false, false, false], "sample_264": [true, true, false, false, true], "sample_265": [false, false, false, false, false], "sample_266": [false, false, false, false, false], "sample_267": [false, false, false, false, false], "sample_268": [false, false, false, false, false], "sample_269": [true, true, true, true, true], "sample_270": [false, false, false, false, false], "sample_271": [false, false, false, false, false], "sample_272": [false, false, false, false, true], "sample_273": [false, false, false, false, true], "sample_274": [false, false, true, true, false], "sample_275": [false, false, false, false, false], "sample_276": [false, false, false, false, false], "sample_277": [true, false, false, true, false], "sample_278": [true, true, false, false, false], "sample_279": [false, false, false, false, false], "sample_280": [false, false, false, false, false], "sample_281": [true, true, true, true, false], "sample_282": [true, false, false, true, true], "sample_283": [false, true, false, true, false], "sample_284": [false, false, true, false, true], "sample_285": [false, false, false, true, false], "sample_286": [false, false, false, false, false], "sample_287": [true, false, false, false, false], "sample_288": [false, true, false, false, false], "sample_289": [true, true, true, true, true], "sample_290": [true, false, true, true, false], "sample_291": [false, false, true, true, true], "sample_292": [false, false, false, false, false], "sample_293": [false, true, false, false, false], "sample_294": [false, false, false, false, false], "sample_295": [false, false, false, true, false], "sample_296": [false, true, true, true, true], "sample_297": [false, false, false, false, false], "sample_298": [true, false, true, true, true], "sample_299": [true, true, true, true, true], "sample_300": [false, false, false, true, false], "sample_301": [false, false, false, false, false], "sample_302": [false, false, false, false, false], "sample_303": [false, false, false, false, false], "sample_304": [false, false, true, false, true], "sample_305": [false, true, true, false, false], "sample_306": [false, false, false, false, false], "sample_307": [false, true, true, true, false], "sample_308": [true, false, false, true, true], "sample_309": [true, true, true, false, true], "sample_310": [false, false, false, true, false], "sample_312": [true, false, true, false, false], "sample_311": [false, false, false, true, false], "sample_313": [false, false, true, false, false], "sample_314": [true, true, false, true, true], "sample_315": [true, true, false, true, false], "sample_316": [false, false, false, false, false], "sample_317": [false, false, false, false, false], "sample_318": [false, false, false, false, false], "sample_319": [false, false, false, false, false], "sample_320": [false, true, false, false, false], "sample_321": [false, false, false, false, false], "sample_322": [true, false, true, false, false], "sample_323": [false, true, false, false, false], "sample_324": [false, false, false, false, false], "sample_325": [false, true, true, true, true], "sample_326": [false, false, false, false, false], "sample_327": [false, false, true, false, false], "sample_328": [false, false, false, false, false], "sample_329": [false, false, false, false, false], "sample_330": [true, true, false, false, true], "sample_331": [false, true, false, false, false], "sample_332": [false, false, false, false, true], "sample_333": [true, true, true, true, false], "sample_334": [true, true, false, true, true], "sample_335": [false, false, false, false, false], "sample_336": [false, false, false, false, false], "sample_337": [false, false, false, true, true], "sample_338": [false, false, true, false, false], "sample_339": [false, false, false, true, false], "sample_340": [false, true, false, false, false], "sample_341": [true, true, false, true, false], "sample_342": [false, false, true, true, true], "sample_343": [false, false, false, false, false], "sample_344": [true, true, false, false, false], "sample_345": [false, false, false, false, false], "sample_346": [false, false, false, false, false], "sample_347": [true, true, false, false, true], "sample_348": [false, false, false, false, false], "sample_349": [false, false, false, false, false], "sample_350": [false, false, true, true, true], "sample_351": [false, false, false, false, false], "sample_352": [false, false, true, false, true], "sample_353": [false, false, false, false, false], "sample_354": [false, false, false, true, false], "sample_355": [false, false, false, false, false], "sample_356": [false, false, false, false, false], "sample_357": [false, false, false, false, true], "sample_358": [false, false, false, true, false], "sample_359": [false, false, true, false, true], "sample_360": [false, false, false, false, false], "sample_361": [false, true, false, true, true], "sample_362": [false, false, false, false, false], "sample_363": [true, true, true, false, true], "sample_364": [false, false, false, false, false], "sample_365": [false, false, false, false, false], "sample_366": [false, false, false, true, true], "sample_367": [false, false, false, false, false], "sample_368": [false, false, false, false, false], "sample_369": [false, false, false, false, false], "sample_371": [true, false, false, false, false], "sample_370": [false, false, false, false, false], "sample_372": [false, false, false, false, false], "sample_373": [false, false, false, false, false], "sample_374": [false, false, false, false, false], "sample_375": [false, false, false, false, false], "sample_376": [true, false, false, true, true], "sample_377": [false, false, false, false, false], "sample_378": [true, false, true, false, true], "sample_379": [true, true, true, false, false], "sample_380": [true, false, false, true, false], "sample_381": [false, false, false, false, false], "sample_382": [false, false, false, false, false], "sample_383": [true, true, true, true, true], "sample_384": [true, false, false, true, false], "sample_385": [false, false, false, true, false], "sample_386": [true, false, true, true, false], "sample_387": [true, true, true, true, true], "sample_388": [false, false, false, false, true], "sample_389": [false, false, false, false, false], "sample_390": [true, true, false, true, true], "sample_391": [false, true, true, false, false], "sample_392": [false, false, false, false, false], "sample_393": [true, true, true, true, true], "sample_394": [true, false, false, true, false], "sample_395": [false, true, false, false, false], "sample_396": [true, true, true, false, true], "sample_397": [false, false, false, false, false], "sample_398": [true, false, false, false, false], "sample_399": [false, false, false, false, false], "sample_400": [false, false, false, false, false], "sample_401": [false, false, false, false, true], "sample_402": [false, false, false, false, false], "sample_403": [false, false, false, true, true], "sample_404": [true, true, true, true, true], "sample_405": [true, true, false, true, false], "sample_406": [true, false, false, false, false], "sample_407": [false, false, false, false, false], "sample_408": [true, false, false, false, false], "sample_409": [true, true, false, true, false], "sample_410": [false, false, false, false, false], "sample_411": [false, false, false, false, false], "sample_412": [true, true, true, false, true], "sample_413": [true, false, true, false, true], "sample_414": [true, true, true, true, true], "sample_415": [false, false, false, false, false], "sample_416": [false, false, false, false, false], "sample_417": [true, false, false, false, false], "sample_418": [false, false, false, false, false], "sample_419": [false, true, false, false, true], "sample_420": [false, false, false, false, true], "sample_421": [false, false, false, false, false], "sample_422": [false, true, false, false, false], "sample_423": [false, true, true, false, false], "sample_424": [false, false, false, false, true], "sample_425": [false, false, false, false, false], "sample_426": [true, false, true, false, false], "sample_427": [false, false, false, true, true], "sample_428": [false, false, false, false, false], "sample_429": [true, true, false, true, false], "sample_430": [false, true, false, false, false], "sample_431": [false, false, false, false, false], "sample_432": [true, true, true, true, true], "sample_433": [false, false, false, false, false], "sample_434": [false, false, false, false, false], "sample_435": [true, false, false, false, false], "sample_436": [false, true, false, true, false], "sample_437": [false, false, false, false, false], "sample_438": [false, false, false, false, false], "sample_439": [false, false, false, true, true], "sample_440": [false, false, false, false, false], "sample_441": [true, true, true, false, false], "sample_442": [false, true, false, true, true], "sample_443": [false, false, true, false, false], "sample_444": [true, false, false, true, false], "sample_445": [true, true, false, false, false], "sample_446": [true, true, false, false, false], "sample_447": [true, true, true, false, false], "sample_448": [false, false, false, false, false], "sample_449": [false, false, false, false, false], "sample_450": [false, false, false, false, false], "sample_451": [false, false, false, false, false], "sample_453": [false, false, false, false, false], "sample_452": [false, false, false, false, false], "sample_454": [false, false, false, false, false], "sample_455": [false, false, false, false, false], "sample_456": [true, true, true, false, false], "sample_457": [false, false, false, false, false], "sample_458": [true, true, false, true, false], "sample_459": [true, true, true, true, true], "sample_460": [false, false, false, false, false], "sample_461": [false, false, false, true, false], "sample_462": [false, false, false, false, true], "sample_463": [false, true, false, false, false], "sample_464": [true, false, true, false, true], "sample_465": [false, false, false, false, false], "sample_466": [false, false, false, false, false], "sample_467": [true, false, false, false, false], "sample_469": [true, false, false, false, false], "sample_468": [false, false, false, true, false], "sample_470": [false, false, false, false, false], "sample_471": [true, true, false, false, true], "sample_472": [true, false, false, false, true], "sample_473": [true, true, false, false, false], "sample_474": [false, true, false, false, false], "sample_475": [false, false, false, false, false], "sample_476": [false, false, false, false, false], "sample_477": [false, false, false, false, false], "sample_478": [false, false, false, false, false], "sample_479": [false, false, true, true, true], "sample_480": [false, false, false, false, false], "sample_481": [false, false, false, false, false], "sample_482": [false, false, false, false, false], "sample_483": [false, false, true, false, true], "sample_484": [true, false, false, true, true], "sample_485": [true, false, true, false, false], "sample_486": [false, false, false, false, false], "sample_487": [false, false, false, false, false], "sample_488": [false, false, false, false, false], "sample_489": [false, false, false, false, false], "sample_490": [false, false, false, false, false], "sample_491": [true, false, true, true, false], "sample_492": [false, true, false, false, false], "sample_493": [false, false, false, false, false], "sample_494": [false, false, false, false, false], "sample_495": [false, false, true, false, true], "sample_496": [false, false, true, false, false], "sample_497": [true, true, true, true, true], "sample_498": [false, false, true, false, true], "sample_499": [true, true, true, false, true], "sample_500": [false, false, false, false, false], "sample_501": [false, true, true, false, true], "sample_502": [false, true, true, false, false], "sample_503": [false, false, false, false, false], "sample_504": [false, false, false, false, false], "sample_505": [false, false, false, false, true], "sample_506": [false, false, false, false, false], "sample_507": [true, false, true, true, true], "sample_508": [true, false, false, true, false], "sample_509": [true, false, false, false, false], "sample_510": [true, false, false, false, false], "sample_511": [false, false, true, true, false], "sample_512": [true, true, false, false, false], "sample_513": [true, true, true, false, true], "sample_514": [false, false, false, true, false], "sample_515": [false, true, true, true, true], "sample_516": [true, false, false, false, false], "sample_517": [false, false, false, false, true], "sample_518": [false, true, true, false, true], "sample_519": [true, true, false, false, true], "sample_520": [false, false, false, false, true], "sample_521": [false, true, false, true, false], "sample_522": [true, false, false, false, false], "sample_523": [false, false, false, true, true], "sample_524": [false, false, false, false, true], "sample_525": [false, false, true, false, false], "sample_526": [false, false, false, true, false], "sample_527": [false, true, false, true, true], "sample_528": [true, false, false, true, true], "sample_529": [false, false, false, true, true], "sample_530": [true, false, false, true, true], "sample_531": [false, false, true, false, false], "sample_532": [false, false, false, true, false], "sample_533": [false, false, false, true, false], "sample_534": [true, false, false, false, false], "sample_535": [false, false, false, false, false], "sample_536": [false, false, false, false, false], "sample_537": [true, true, true, true, true], "sample_538": [true, false, true, true, false], "sample_539": [false, false, false, false, false], "sample_540": [false, false, false, false, false], "sample_541": [false, false, false, false, false], "sample_542": [false, false, false, true, true], "sample_543": [false, false, true, false, false], "sample_544": [false, true, false, false, false], "sample_545": [false, false, false, false, false], "sample_546": [true, false, false, false, true], "sample_547": [false, true, false, false, false], "sample_548": [false, true, true, true, false], "sample_549": [false, false, false, true, true], "sample_550": [false, true, false, true, true], "sample_551": [false, false, false, false, false], "sample_552": [true, false, false, true, false], "sample_553": [false, false, false, false, false], "sample_554": [true, false, false, false, false], "sample_555": [false, false, false, false, false], "sample_556": [false, false, false, false, false], "sample_557": [false, true, true, true, false], "sample_558": [false, false, true, false, false], "sample_559": [false, false, false, false, false], "sample_560": [false, false, false, false, false], "sample_561": [false, false, true, true, false], "sample_562": [true, true, true, false, false], "sample_563": [false, true, false, false, false], "sample_564": [false, false, false, false, false], "sample_565": [false, false, false, false, false], "sample_566": [false, true, false, true, false], "sample_567": [false, false, false, false, false], "sample_568": [false, false, false, false, false], "sample_569": [false, true, false, false, false], "sample_570": [true, false, false, false, false], "sample_571": [true, false, false, false, false], "sample_572": [false, true, false, false, false], "sample_573": [true, true, false, false, true], "sample_574": [false, false, false, false, false], "sample_575": [false, false, false, false, false], "sample_576": [false, false, true, true, false], "sample_577": [false, false, true, false, true], "sample_578": [false, false, true, false, false], "sample_579": [false, true, false, false, false], "sample_580": [false, false, false, false, false], "sample_581": [false, false, false, false, false], "sample_582": [true, true, false, false, false], "sample_583": [false, true, false, false, false], "sample_584": [true, true, false, false, false], "sample_585": [false, true, true, true, false], "sample_586": [false, true, true, false, false], "sample_587": [false, true, false, true, false], "sample_588": [false, false, false, false, true], "sample_589": [false, false, false, true, true], "sample_590": [false, true, true, false, false], "sample_591": [true, true, false, true, false], "sample_592": [true, true, true, true, true], "sample_593": [false, false, false, true, false], "sample_594": [true, true, true, true, true], "sample_595": [true, false, false, false, false], "sample_596": [true, true, true, true, true], "sample_597": [false, false, false, false, true], "sample_598": [true, true, false, true, true], "sample_599": [false, true, false, true, false], "sample_600": [false, false, true, true, false], "sample_601": [false, false, false, false, true], "sample_602": [true, true, false, false, false], "sample_603": [false, false, false, false, true], "sample_604": [false, true, true, true, true], "sample_605": [false, true, false, false, false], "sample_606": [false, false, false, false, false], "sample_607": [false, false, false, true, false], "sample_608": [false, true, true, true, false], "sample_609": [false, false, false, false, false], "sample_610": [false, false, true, false, false], "sample_611": [false, true, false, false, false], "sample_612": [false, false, false, false, false], "sample_613": [false, false, false, false, false], "sample_614": [true, false, true, false, true], "sample_615": [false, true, false, false, false], "sample_616": [false, false, false, false, false], "sample_617": [false, false, false, false, false], "sample_618": [false, false, false, false, false], "sample_619": [true, true, false, false, true], "sample_620": [true, false, true, false, true], "sample_621": [false, false, false, true, true], "sample_622": [true, true, true, true, true], "sample_623": [false, false, false, false, false], "sample_624": [false, false, false, false, false], "sample_625": [false, true, false, false, false], "sample_626": [true, true, false, true, true], "sample_627": [true, false, false, true, false], "sample_628": [false, true, true, false, true], "sample_629": [false, false, true, false, false], "sample_630": [false, false, false, false, false], "sample_631": [true, false, false, false, true], "sample_632": [false, false, false, false, false], "sample_633": [false, false, false, false, true], "sample_634": [false, false, false, false, false], "sample_635": [false, false, false, false, false], "sample_636": [false, false, false, false, false], "sample_637": [false, true, false, false, true], "sample_638": [false, false, false, false, false], "sample_639": [false, false, false, false, false], "sample_640": [false, true, false, false, false], "sample_641": [false, true, false, false, false], "sample_642": [false, true, false, false, false], "sample_643": [false, false, false, false, false], "sample_644": [false, false, false, false, false], "sample_645": [false, true, false, true, true], "sample_646": [true, false, true, true, true], "sample_647": [false, true, false, false, false], "sample_648": [true, false, true, true, true], "sample_649": [true, true, false, false, true], "sample_650": [true, true, false, true, false], "sample_651": [false, false, false, false, false], "sample_652": [true, true, false, true, false], "sample_653": [false, false, false, false, false], "sample_654": [true, true, true, true, true], "sample_655": [true, false, true, false, true], "sample_656": [false, true, false, true, false], "sample_657": [true, true, true, true, true], "sample_658": [false, true, true, true, true], "sample_659": [false, true, false, false, false], "sample_660": [false, true, true, true, true], "sample_661": [false, false, true, false, false], "sample_662": [false, false, false, false, false], "sample_663": [false, false, false, false, true], "sample_664": [false, false, false, false, false], "sample_665": [false, false, false, true, false], "sample_666": [false, false, false, false, false], "sample_667": [true, false, true, true, true], "sample_668": [false, false, false, false, false], "sample_669": [false, true, false, true, false], "sample_670": [false, false, true, true, false], "sample_671": [false, true, true, true, false], "sample_672": [true, true, false, false, false], "sample_673": [false, false, false, false, false], "sample_674": [false, false, false, false, true], "sample_675": [false, false, false, true, false], "sample_676": [false, false, false, true, false], "sample_677": [false, false, false, false, false], "sample_678": [false, true, false, false, false], "sample_679": [false, true, true, false, false], "sample_680": [false, true, true, true, false], "sample_681": [false, false, false, true, true], "sample_682": [false, false, true, true, true], "sample_683": [false, true, false, false, true], "sample_684": [false, true, true, false, true], "sample_685": [false, false, true, false, true], "sample_686": [false, false, false, false, false], "sample_687": [false, false, false, false, false], "sample_688": [false, false, false, false, false], "sample_689": [false, false, false, false, false], "sample_690": [false, true, false, true, true], "sample_691": [false, false, false, false, true], "sample_692": [true, false, true, true, false], "sample_693": [false, true, true, false, false], "sample_694": [false, false, false, true, false], "sample_695": [false, false, false, false, false], "sample_696": [false, false, false, false, false], "sample_697": [true, false, true, true, false], "sample_698": [false, false, true, false, true], "sample_699": [false, false, true, false, false], "sample_700": [true, false, true, true, true], "sample_701": [false, false, false, false, false], "sample_702": [false, true, false, true, true], "sample_703": [false, false, false, true, false], "sample_704": [false, false, false, true, false], "sample_705": [false, false, false, false, false], "sample_706": [false, false, false, false, false], "sample_707": [false, false, true, false, false], "sample_708": [false, false, false, false, false], "sample_709": [false, false, false, false, false], "sample_710": [false, false, true, false, false], "sample_711": [false, false, false, false, false], "sample_712": [false, false, true, true, false], "sample_713": [true, false, true, true, false], "sample_714": [false, false, false, false, false], "sample_715": [true, true, true, false, false], "sample_716": [true, true, true, true, true], "sample_717": [true, true, true, true, true], "sample_718": [false, false, false, false, false], "sample_719": [true, false, true, false, true], "sample_720": [false, false, false, false, false], "sample_721": [true, true, true, true, false], "sample_722": [false, false, true, true, true], "sample_723": [false, false, false, true, true], "sample_724": [true, true, false, true, false], "sample_725": [true, true, true, false, true], "sample_726": [false, false, false, false, false], "sample_727": [true, false, false, false, false], "sample_728": [true, true, true, true, true], "sample_729": [true, true, true, false, false], "sample_730": [false, true, false, false, false], "sample_731": [true, true, false, true, true], "sample_732": [false, false, false, false, false], "sample_733": [true, true, true, true, true], "sample_734": [true, false, false, true, true], "sample_735": [true, false, true, true, false], "sample_736": [true, true, true, false, true], "sample_737": [true, true, false, true, true], "sample_738": [true, true, true, true, true], "sample_739": [false, false, false, true, false], "sample_740": [true, false, false, false, true], "sample_741": [true, false, false, false, true], "sample_742": [true, true, true, false, true], "sample_743": [true, true, true, false, true], "sample_744": [false, false, false, false, false], "sample_745": [false, false, true, false, false], "sample_746": [false, false, false, false, false], "sample_747": [false, false, false, false, false], "sample_748": [true, true, false, false, true], "sample_749": [false, true, false, false, false], "sample_750": [true, true, true, true, true], "sample_751": [true, true, true, false, true], "sample_752": [false, true, false, false, true], "sample_753": [true, true, true, true, false], "sample_754": [false, false, true, false, true], "sample_755": [true, true, true, true, false], "sample_756": [true, false, false, false, true], "sample_757": [false, true, false, false, false], "sample_758": [false, false, false, false, false], "sample_759": [false, false, false, false, false], "sample_760": [true, false, false, false, true], "sample_761": [true, true, true, true, true], "sample_762": [true, true, true, true, true], "sample_763": [false, false, false, false, false], "sample_764": [false, false, false, false, false], "sample_765": [false, false, false, false, false], "sample_766": [true, false, true, true, true], "sample_767": [false, false, false, false, false], "sample_768": [true, false, true, true, false], "sample_769": [false, false, false, false, false], "sample_770": [true, true, true, false, true], "sample_771": [false, true, false, false, true], "sample_772": [true, false, true, false, false], "sample_773": [true, true, false, true, true], "sample_774": [false, false, false, false, false], "sample_775": [false, false, true, false, false], "sample_776": [false, false, true, false, false], "sample_777": [false, false, false, true, false], "sample_778": [true, true, true, true, true], "sample_779": [true, false, true, false, true], "sample_780": [false, false, true, true, false], "sample_781": [true, true, false, false, false], "sample_782": [false, false, false, false, false], "sample_783": [true, true, false, true, true], "sample_784": [false, false, true, false, true], "sample_785": [false, true, false, true, true], "sample_786": [true, true, false, true, false], "sample_787": [false, false, false, false, false], "sample_788": [false, true, false, false, false], "sample_789": [false, true, false, false, true], "sample_790": [false, true, false, true, true], "sample_791": [false, false, true, false, false], "sample_792": [true, false, false, false, false], "sample_793": [false, true, false, true, false], "sample_794": [true, true, true, false, false], "sample_795": [false, false, true, false, false], "sample_796": [false, false, false, false, true], "sample_797": [true, true, false, false, false], "sample_798": [false, true, false, true, true], "sample_799": [true, false, false, true, true], "sample_800": [true, false, true, false, false], "sample_801": [false, false, false, true, false], "sample_802": [false, false, false, false, false], "sample_803": [false, true, false, true, false], "sample_804": [false, false, false, false, false], "sample_805": [false, false, false, false, false], "sample_806": [false, false, true, false, true], "sample_807": [true, false, false, true, false], "sample_808": [true, false, false, false, false], "sample_809": [true, true, false, true, false], "sample_810": [false, true, false, false, false], "sample_811": [false, false, false, false, false], "sample_812": [false, false, false, false, false], "sample_813": [false, true, false, false, false], "sample_814": [false, true, false, true, false], "sample_815": [false, false, false, false, false], "sample_816": [false, true, true, true, true], "sample_817": [false, false, true, true, false], "sample_818": [true, true, true, false, true], "sample_819": [false, false, false, false, false], "sample_820": [false, false, false, true, false], "sample_821": [false, false, false, true, true], "sample_822": [false, false, false, false, true], "sample_823": [false, false, false, false, false], "sample_824": [false, true, true, true, false], "sample_825": [false, false, false, false, false], "sample_826": [false, false, false, false, false], "sample_827": [false, false, false, false, false], "sample_828": [true, false, false, false, false], "sample_829": [true, false, true, true, true], "sample_830": [false, true, false, false, false], "sample_831": [false, false, false, true, true], "sample_832": [false, false, false, false, false], "sample_833": [true, false, false, true, true], "sample_834": [true, true, false, true, true], "sample_835": [true, true, true, false, true], "sample_836": [true, true, false, false, false], "sample_837": [false, false, false, false, false], "sample_838": [false, false, false, false, false], "sample_839": [false, false, true, false, true], "sample_840": [false, false, false, true, false], "sample_841": [true, true, false, true, false], "sample_842": [false, true, false, false, true], "sample_843": [false, true, false, false, true], "sample_844": [false, false, false, false, false], "sample_845": [true, false, true, false, true], "sample_846": [false, false, false, true, false], "sample_847": [false, true, false, true, true], "sample_848": [false, false, true, true, false], "sample_849": [false, false, false, false, false], "sample_850": [false, true, true, false, true], "sample_851": [false, false, false, true, false], "sample_852": [true, true, true, false, false], "sample_853": [false, false, true, false, false], "sample_854": [false, false, false, false, false], "sample_855": [true, true, true, true, false], "sample_856": [false, false, false, false, false], "sample_857": [true, true, true, false, false], "sample_858": [true, false, false, false, false], "sample_859": [false, false, false, true, true], "sample_860": [false, false, false, false, false], "sample_861": [true, false, true, true, true], "sample_862": [false, false, false, false, false], "sample_863": [false, true, true, false, true], "sample_864": [true, true, true, true, true], "sample_865": [true, true, false, true, true], "sample_866": [false, true, true, true, true], "sample_867": [false, false, true, false, true], "sample_868": [false, false, false, false, true], "sample_869": [false, false, false, false, false], "sample_870": [false, false, true, true, true], "sample_871": [false, true, false, false, false], "sample_872": [true, true, true, true, true], "sample_873": [false, false, false, false, false], "sample_874": [false, false, true, true, true], "sample_875": [false, false, false, true, false], "sample_876": [false, false, true, false, false], "sample_877": [true, true, false, true, false], "sample_878": [true, false, false, true, false], "sample_879": [true, false, true, false, true], "sample_880": [false, false, true, false, true], "sample_881": [false, false, true, false, false], "sample_882": [true, false, false, true, true], "sample_883": [false, false, true, true, false], "sample_884": [false, false, true, false, false], "sample_885": [false, false, false, true, true], "sample_886": [true, false, false, false, true], "sample_887": [false, false, false, false, true], "sample_888": [true, true, true, true, true], "sample_889": [true, false, true, false, true], "sample_890": [false, false, false, true, false], "sample_891": [true, false, true, false, false], "sample_892": [true, false, true, true, false], "sample_893": [true, true, true, true, true], "sample_894": [false, false, false, false, false], "sample_895": [true, false, true, false, true], "sample_896": [false, false, false, false, false], "sample_897": [false, false, false, false, false], "sample_898": [true, false, false, false, false], "sample_899": [false, false, false, false, false], "sample_900": [false, true, true, false, false], "sample_901": [false, true, true, false, false], "sample_902": [false, true, true, false, false], "sample_903": [false, false, true, true, true], "sample_904": [false, false, false, false, false], "sample_905": [false, false, false, true, false], "sample_906": [false, false, false, false, true], "sample_907": [true, false, false, false, false], "sample_908": [true, false, false, true, true], "sample_909": [true, false, false, false, false], "sample_910": [false, true, true, true, true], "sample_911": [false, true, true, false, false], "sample_912": [true, false, true, false, false], "sample_913": [false, false, false, true, true], "sample_914": [true, false, true, false, true], "sample_915": [true, true, false, true, true], "sample_916": [false, false, true, true, true], "sample_917": [false, true, true, false, false], "sample_918": [true, true, true, true, true], "sample_919": [true, false, false, false, false], "sample_920": [false, false, false, false, false], "sample_921": [true, false, true, true, true], "sample_922": [true, true, false, true, true], "sample_923": [false, true, false, false, false], "sample_924": [false, true, true, true, false], "sample_925": [true, false, false, false, false], "sample_926": [true, false, true, false, true], "sample_927": [true, false, false, false, true], "sample_928": [false, false, false, false, false], "sample_929": [false, false, false, true, false], "sample_930": [false, false, false, false, false], "sample_931": [false, true, true, true, false], "sample_932": [false, false, true, true, true], "sample_933": [false, false, false, false, false], "sample_934": [true, true, false, true, false], "sample_935": [true, true, false, true, false], "sample_936": [false, false, false, true, false], "sample_937": [false, false, false, false, false], "sample_938": [false, false, false, true, false], "sample_939": [false, false, false, false, false], "sample_940": [true, false, false, true, false], "sample_941": [false, true, true, false, false], "sample_942": [true, false, false, false, false], "sample_943": [false, false, false, false, false], "sample_944": [false, false, false, false, true], "sample_945": [false, false, false, false, true], "sample_946": [false, false, false, false, true], "sample_947": [true, false, true, false, false], "sample_948": [true, true, false, false, false], "sample_949": [false, false, false, false, false], "sample_950": [true, false, false, false, false], "sample_951": [false, false, false, false, false], "sample_952": [true, false, false, false, false], "sample_953": [false, false, false, true, false], "sample_954": [false, false, false, false, false], "sample_955": [false, false, false, true, false], "sample_956": [true, false, true, false, true], "sample_957": [true, true, false, true, true], "sample_958": [true, true, false, false, false], "sample_959": [true, true, false, false, false], "sample_960": [false, false, false, false, true], "sample_961": [false, false, false, false, false], "sample_962": [true, false, true, false, true], "sample_963": [false, true, false, true, false], "sample_964": [false, false, false, false, false], "sample_965": [false, false, false, false, false], "sample_966": [false, false, false, false, false], "sample_967": [false, false, false, false, false], "sample_968": [false, false, false, false, false], "sample_969": [true, false, true, true, false], "sample_970": [false, false, false, false, true], "sample_971": [false, false, false, false, true], "sample_972": [true, true, true, true, false], "sample_973": [false, false, false, true, false], "sample_974": [false, false, false, false, false], "sample_975": [false, false, false, false, false], "sample_976": [false, false, false, false, true], "sample_977": [false, true, true, false, true], "sample_978": [false, true, false, true, false], "sample_979": [false, false, false, true, false], "sample_980": [false, false, false, false, false], "sample_981": [false, true, false, true, false], "sample_982": [false, false, false, false, false], "sample_983": [false, false, false, true, false], "sample_984": [false, false, false, false, false], "sample_985": [false, false, true, false, false], "sample_986": [false, true, false, false, false], "sample_987": [false, false, false, true, false], "sample_988": [false, false, false, false, false], "sample_989": [false, false, false, false, true], "sample_990": [false, false, false, false, false], "sample_991": [false, false, false, false, false], "sample_992": [true, true, true, true, true], "sample_993": [false, false, true, true, false], "sample_994": [false, true, false, false, false], "sample_995": [true, false, false, false, false], "sample_996": [false, false, false, false, false], "sample_997": [false, false, true, false, true], "sample_998": [false, false, false, false, true], "sample_999": [false, false, false, false, false], "sample_1000": [false, true, false, true, true], "sample_1001": [false, false, false, false, false], "sample_1002": [false, true, false, false, false], "sample_1003": [true, true, false, false, true], "sample_1004": [false, false, true, true, true], "sample_1005": [false, false, false, false, false], "sample_1006": [false, false, false, false, false], "sample_1007": [false, false, false, false, false], "sample_1008": [false, false, false, false, false], "sample_1009": [false, false, false, false, false], "sample_1010": [false, false, false, false, false], "sample_1011": [false, false, false, false, true], "sample_1012": [false, true, false, false, false], "sample_1013": [false, true, false, false, false], "sample_1014": [false, true, true, false, false], "sample_1015": [false, false, false, false, false], "sample_1016": [false, false, false, true, true], "sample_1017": [false, true, false, false, false], "sample_1018": [false, false, false, false, false], "sample_1019": [false, false, false, false, false], "sample_1020": [true, true, true, false, true], "sample_1021": [false, true, false, false, false], "sample_1022": [false, false, false, true, false], "sample_1023": [false, false, false, false, false], "sample_1024": [false, false, false, false, false], "sample_1025": [false, false, false, false, false], "sample_1026": [true, false, false, false, false], "sample_1027": [false, false, false, false, false], "sample_1028": [false, false, false, false, false], "sample_1029": [false, false, false, false, false], "sample_1030": [false, false, false, false, false], "sample_1031": [false, false, false, false, false], "sample_1032": [false, false, false, false, false], "sample_1033": [false, false, false, false, false], "sample_1034": [true, true, false, false, true], "sample_1035": [false, false, false, false, false], "sample_1036": [false, false, false, true, false], "sample_1037": [false, false, false, false, true], "sample_1038": [false, false, false, false, true], "sample_1039": [false, false, false, false, false], "sample_1040": [false, false, false, false, false], "sample_1041": [true, true, false, true, false], "sample_1042": [false, false, false, false, false], "sample_1043": [false, true, false, false, true], "sample_1044": [false, false, false, false, false], "sample_1045": [false, false, false, false, false], "sample_1046": [true, false, false, false, false], "sample_1047": [false, false, false, false, false], "sample_1048": [false, false, false, false, false], "sample_1049": [false, false, false, false, false], "sample_1050": [false, false, false, false, false], "sample_1051": [true, true, false, true, false], "sample_1052": [false, false, false, false, false], "sample_1053": [false, false, true, false, false], "sample_1054": [false, false, false, false, false], "sample_1055": [false, true, false, true, false], "sample_1056": [true, false, false, true, false], "sample_1057": [false, false, false, false, false], "sample_1058": [false, false, false, false, false], "sample_1059": [false, false, false, false, false], "sample_1060": [false, false, false, false, false], "sample_1061": [false, false, false, true, false], "sample_1062": [false, false, false, false, false], "sample_1063": [false, false, true, false, true], "sample_1064": [false, true, false, false, false], "sample_1065": [false, false, false, false, false], "sample_1066": [false, false, false, false, false], "sample_1067": [false, false, false, false, false], "sample_1068": [false, false, true, false, false], "sample_1069": [false, false, false, false, false], "sample_1070": [false, false, false, false, false], "sample_1071": [false, false, true, false, false], "sample_1072": [false, false, false, false, false], "sample_1073": [false, false, false, false, false], "sample_1074": [true, false, false, true, false], "sample_1075": [false, false, false, true, false], "sample_1076": [false, false, false, false, true], "sample_1077": [false, false, false, false, false], "sample_1078": [false, false, false, false, false], "sample_1079": [true, false, true, true, true], "sample_1080": [false, false, false, false, false], "sample_1081": [false, false, false, false, false], "sample_1082": [false, false, false, false, false], "sample_1083": [false, false, false, false, false], "sample_1084": [false, false, false, false, false], "sample_1085": [false, false, false, false, false], "sample_1086": [false, false, false, false, false], "sample_1087": [true, false, false, true, false], "sample_1088": [false, false, false, false, false], "sample_1089": [false, false, false, false, false], "sample_1090": [false, false, false, false, false], "sample_1091": [false, false, false, false, false], "sample_1092": [true, true, false, false, false], "sample_1093": [true, false, true, false, false], "sample_1094": [false, false, false, false, false], "sample_1095": [false, false, false, false, false], "sample_1096": [false, false, false, false, false], "sample_1097": [false, true, true, false, false], "sample_1098": [false, true, false, false, false], "sample_1099": [false, false, true, false, false], "sample_1100": [false, false, false, false, false], "sample_1101": [false, false, false, false, false], "sample_1102": [false, false, false, false, false], "sample_1103": [false, false, false, false, false], "sample_1104": [false, false, false, false, false], "sample_1105": [false, true, false, false, false], "sample_1106": [false, false, false, false, false], "sample_1107": [true, true, true, true, true], "sample_1108": [true, true, false, false, true], "sample_1109": [false, false, false, false, false], "sample_1110": [false, false, false, false, false], "sample_1111": [false, false, false, false, false], "sample_1112": [false, true, false, false, true], "sample_1113": [false, false, false, false, true], "sample_1114": [false, false, false, true, false], "sample_1115": [false, false, false, false, false], "sample_1116": [false, false, false, true, false], "sample_1117": [false, false, false, false, false], "sample_1118": [false, false, false, false, true], "sample_1119": [false, true, false, false, false], "sample_1120": [false, false, false, false, false], "sample_1121": [false, false, false, false, false], "sample_1122": [false, false, false, false, false], "sample_1123": [false, true, false, false, true], "sample_1124": [true, true, false, false, false], "sample_1125": [false, true, true, false, false], "sample_1126": [true, true, true, true, true], "sample_1127": [false, true, false, true, true], "sample_1128": [true, true, true, true, true], "sample_1129": [false, false, false, false, false], "sample_1130": [false, false, false, false, false], "sample_1131": [false, false, false, false, false], "sample_1132": [false, true, false, true, false], "sample_1133": [false, true, true, true, true], "sample_1134": [false, true, false, false, false], "sample_1135": [false, false, false, false, false], "sample_1136": [false, false, false, true, false], "sample_1137": [true, false, false, false, false], "sample_1138": [false, false, false, false, false], "sample_1139": [false, false, false, false, false], "sample_1140": [false, false, false, false, false], "sample_1141": [false, false, true, false, false], "sample_1142": [false, false, true, false, true], "sample_1143": [false, false, false, false, false], "sample_1144": [false, false, true, false, false], "sample_1145": [false, false, false, false, false], "sample_1146": [false, false, false, false, false], "sample_1147": [false, false, false, false, false], "sample_1148": [false, true, false, true, true], "sample_1149": [true, false, true, false, false], "sample_1150": [false, false, false, false, false], "sample_1151": [false, false, false, false, false], "sample_1152": [false, false, false, false, false], "sample_1153": [false, true, false, false, true], "sample_1154": [false, false, false, false, false], "sample_1155": [false, false, false, false, false], "sample_1156": [false, false, false, false, false], "sample_1157": [true, true, false, true, true], "sample_1158": [false, false, false, false, true], "sample_1159": [false, false, false, false, false], "sample_1160": [false, false, false, false, false], "sample_1161": [true, false, false, false, false], "sample_1162": [false, false, false, false, false], "sample_1163": [false, false, true, false, false], "sample_1164": [false, false, false, false, false], "sample_1165": [false, false, false, false, false], "sample_1166": [false, true, false, false, false], "sample_1167": [false, false, false, false, false], "sample_1168": [true, true, false, true, true], "sample_1169": [true, false, false, false, true], "sample_1170": [false, true, false, false, false], "sample_1171": [false, false, false, false, false], "sample_1172": [false, false, false, false, false], "sample_1173": [true, true, true, false, false], "sample_1174": [false, false, false, false, false], "sample_1175": [false, false, false, false, false], "sample_1176": [false, false, false, false, false], "sample_1177": [false, false, false, false, false], "sample_1178": [false, false, false, true, false], "sample_1179": [false, false, false, false, false], "sample_1180": [false, false, false, false, false], "sample_1181": [false, true, true, true, true], "sample_1182": [false, false, false, false, false], "sample_1183": [false, false, false, false, false], "sample_1184": [true, false, false, true, true], "sample_1185": [true, false, true, false, true], "sample_1186": [true, true, false, true, true], "sample_1187": [true, true, true, false, false], "sample_1188": [false, false, false, false, false], "sample_1189": [true, true, true, true, false], "sample_1190": [false, false, false, false, false], "sample_1191": [false, false, false, false, false], "sample_1192": [false, false, false, false, false], "sample_1193": [false, false, false, false, false], "sample_1194": [false, true, false, false, false], "sample_1195": [false, false, false, false, false], "sample_1196": [false, true, true, false, false], "sample_1197": [false, false, false, false, false], "sample_1198": [true, false, false, true, false], "sample_1199": [false, false, false, false, false], "sample_1200": [true, false, false, false, false], "sample_1201": [false, false, false, false, false], "sample_1202": [false, false, false, false, false], "sample_1203": [false, false, false, false, false], "sample_1204": [true, true, false, true, true], "sample_1205": [false, false, false, false, false], "sample_1206": [false, false, false, false, false], "sample_1207": [true, true, true, false, true], "sample_1208": [false, false, false, false, false], "sample_1209": [false, true, true, true, true]}}